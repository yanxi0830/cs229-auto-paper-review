{"conference": "ICLR 2017 conference submission", "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "From my original comments:\n\nThe results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n----\n\nThe authors reply still does not convince me.\n\nI still think that the same technique should be applied on more challenging scenarios."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "After discussion, the reviewers unanimously recommend accepting the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Neural Nets for embedded devices", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Experimental results look reasonable, validated on 3 tasks. \nReferences could be improved, for example I would rather see\nRumelhart's paper cited for back-propagation than the Deep Learning book.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. \nThe paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.\nThe paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. \nThis explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.\nAlgorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.\nUsing random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016 (modified: 13 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "17 Dec 2016", "TITLE": "related work", "IS_META_REVIEW": false, "comments": "A related work: ", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Baseline results...", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "From my original comments:\n\nThe results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n----\n\nThe authors reply still does not convince me.\n\nI still think that the same technique should be applied on more challenging scenarios.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Results better but not good baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Request for more numbers", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "From my original comments:\n\nThe results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n----\n\nThe authors reply still does not convince me.\n\nI still think that the same technique should be applied on more challenging scenarios."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "After discussion, the reviewers unanimously recommend accepting the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Neural Nets for embedded devices", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Experimental results look reasonable, validated on 3 tasks. \nReferences could be improved, for example I would rather see\nRumelhart's paper cited for back-propagation than the Deep Learning book.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. \nThe paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.\nThe paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. \nThis explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.\nAlgorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.\nUsing random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016 (modified: 13 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "17 Dec 2016", "TITLE": "related work", "IS_META_REVIEW": false, "comments": "A related work: ", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Baseline results...", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "From my original comments:\n\nThe results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n----\n\nThe authors reply still does not convince me.\n\nI still think that the same technique should be applied on more challenging scenarios.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Results better but not good baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Request for more numbers", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Arash Ardakani, Carlo Condo, Warren J. Gross", "accepted": true, "id": "487"}