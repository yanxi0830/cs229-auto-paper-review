{"conference": "ICLR 2017 conference submission", "title": "Exploring LOTS in Deep Neural Networks", "abstract": "Deep neural networks have recently demonstrated excellent performance on various tasks. Despite recent advances, our understanding of these learning models is still incomplete, at least, as their unexpected vulnerability to imperceptibly small, non-random perturbations revealed. The existence of these so-called adversarial examples presents a serious problem of the application of vulnerable machine learning models. In this paper, we introduce the layerwise origin-target synthesis (LOTS) that can serve multiple purposes. First, we can use it as a visualization technique that gives us insights into the function of any intermediate feature layer by showing the notion of a particular input in deep neural networks. Second, our approach can be applied to assess the invariance of the learned features captured at any layer with respect to the class of the particular input. Finally, we can also utilize LOTS as a general way of producing a vast amount of diverse adversarial examples that can be used for training to further improve the robustness of machine learning models and their performance as well.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input\u2019s representation and difference vectors between representations of two inputs, where the inputs are of different classes.\n\nIn the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it\u2019s an interesting computation to perform, the value of the visualizations is not very clear.\n\nIn the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).\n\nThe paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.\n\nA few suggestions for improvement:\n - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn\u2019t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.\n - How many layers are in the networks used in the paper, and what is their internal structure? This isn\u2019t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.\n - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)\n - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.\n - 4.2 states \u201cIn summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.\u201d I don\u2019t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.\n\n\n\n1/19/17 UPDATE AFTER REBUTTAL:\nGiven that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper studies the effects of modifying intermediate representations arising in deep convolutional networks, with the purpose of visualizing the role of specific neurons, and also to construct adversarial examples. The paper presents experiments on MNIST as well as faces. \n \n The reviewers agreed that, while this contribution presents an interesting framework, it lacks comparisons with existing methods, and the description of the method lacks sufficient rigor. In light of the discussions and the current state of the submission, the AC recommends rejection. \n \n Since the final scores of the reviewers might suggest otherwise, please let me explain my recommendation. \n \n The main contribution of this paper seems to be essentially a fast alternative to the method proposed in 'Adversarial Manipulation of Deep Representations', by Sabour et al, ICLR'16, although the lack of rigor and clarity in the presentation of section 3 makes this assessment uncertain. The most likely 'interpretation' of Eq (3) suggests that eta(x_o, x_t) = nabla_{x_o}( || f^(l)_w(x_t) - f^(l)_w(x_o) ||^2), which is simply one step of gradient descent of the method described in Sabour et al. One reviewer actually asked for clarification on this point on Dec. 26th, but the problem seems to be still present in the current manuscript. \n \n More generally, visualization and adversarial methods based on backpropagation of some form of distance measured in feature space towards the pixel space are not new; they can be traced back to Simoncelli & Portilla '99. \n Fast approximations based on simply stopping the gradient descent after one iteration do not constitute enough novelty. \n \n Another instance of lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure. It is not defined anywhere in the text, and the authors should not expect the reader to know its definition beforehand. \n \n Besides these issues, the paper does not contribute to the state-of-the-art of adversarial training nor feature visualization, mostly because its experiments are limited to mnist and face datasets. Since the main contribution of the paper is empirical, more emphasis should be made to present experiments on larger, more numerous datasets.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "07 Jan 2017", "TITLE": "Revision addressing comments and suggestions", "IS_META_REVIEW": false, "comments": "Our revised paper reflects to comments and suggestions for improvement.\n\n(1) We have added quantitative results of large-scale experiments comparing our LOTS approach to other adversarial example generation techniques such as the fast gradient sign (FGS) method, fast gradient value (FGV) method, and hot/cold (HC) approach. We have analyzed these techniques with respect to the quality of the produced images, and have also evaluated the effect of those samples when they are used for adversarial training.\n(2) We have clarified the motivation and advantage of using PASS over L-2 and L-inf norms with respect to measuring adversarial quality.\n(3) We have provided details about the internal structure of the tested network architectures, e.g., by denoting the position of layers in figures.\n(4) We have reviewed and simplified the whole paper to enhance readability and, last but not least, to allow us presenting the new results.\n\nThank you for all your comments and suggestions!", "OTHER_KEYS": "Andras Rozsa"}, {"TITLE": "Great start; recommended as workshop paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input\u2019s representation and difference vectors between representations of two inputs, where the inputs are of different classes.\n\nIn the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it\u2019s an interesting computation to perform, the value of the visualizations is not very clear.\n\nIn the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).\n\nThe paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.\n\nA few suggestions for improvement:\n - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn\u2019t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.\n - How many layers are in the networks used in the paper, and what is their internal structure? This isn\u2019t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.\n - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)\n - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.\n - 4.2 states \u201cIn summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.\u201d I don\u2019t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.\n\n\n\n1/19/17 UPDATE AFTER REBUTTAL:\nGiven that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "26 Dec 2016", "TITLE": "Question about eq 3 and 4", "IS_META_REVIEW": false, "comments": "Hi,\n\nI have a question about eq. 3 and 4.\n\nAs far as I understood from the beginning of section 3, f_{w}^{l} (x) is essentially activations at layer l, which means that value of f_{w}^{l} (x) is in R^{d_l} space where d_l is dimensionality of output of layer l.\nIn eq. (3) and (4) you \\eta compute gradient of f_{w}^{l} (x) over input x, which I would expect to be Jacobian matrix with size d_l*n (where n - dimensionality of input x). So looking at eq (3) and (4) I would expect that \\eta and x has different dimensionality. At the same time in section 4 you add s*\\eta to x.\nCould you explain this discrepancy in dimensionality and how \\eta should be calculated?\nMaybe you meant that numerator of eq (3) and (4) contain norm of f_{w}^{l} instead of it's value?\n\nThanks,\nAlex", "OTHER_KEYS": "Alexey Kurakin"}, {"TITLE": "Exciting new method to generate adversarial examples and study robustness, less interesting analyses", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.\n\nThe approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.\n\nPro: The presented approach is definitely sound, interesting and original. \nCon: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.\n\nIt would definitely improve the paper if it would present some interesting conclusions based on the new ideas.\n\n\n\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting but somewhat incomplete analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments:\n\nIt\u2019s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don\u2019t see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition.\n\nThe authors use the PASS score through the paper, but only given an intuition + citation for it. I think it\u2019s worth explaining what it actually does, in a sentence or two.\n\nThe PASS score seems to have some, but not complete, correlation with L_2, L_\\{infty} or visual estimation of how \u201cgood\u201d the adversarial examples are. I am not sure what the take-home message from all these numbers is.\n\n\u201cIn general, LOTS cannot produce high quality adversarial examples at the lower layers\u201d (sec 5.2) seems false for MNIST, no?\n\nI would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it\u2019s hard to judge the merits of this work in vacuum).\n\n-----\n\nEDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "11 Dec 2016", "TITLE": "Plotting or experimental bug?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Quantitative comparison of perturbation sizes with other methods", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "small questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input\u2019s representation and difference vectors between representations of two inputs, where the inputs are of different classes.\n\nIn the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it\u2019s an interesting computation to perform, the value of the visualizations is not very clear.\n\nIn the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).\n\nThe paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.\n\nA few suggestions for improvement:\n - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn\u2019t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.\n - How many layers are in the networks used in the paper, and what is their internal structure? This isn\u2019t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.\n - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)\n - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.\n - 4.2 states \u201cIn summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.\u201d I don\u2019t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.\n\n\n\n1/19/17 UPDATE AFTER REBUTTAL:\nGiven that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper studies the effects of modifying intermediate representations arising in deep convolutional networks, with the purpose of visualizing the role of specific neurons, and also to construct adversarial examples. The paper presents experiments on MNIST as well as faces. \n \n The reviewers agreed that, while this contribution presents an interesting framework, it lacks comparisons with existing methods, and the description of the method lacks sufficient rigor. In light of the discussions and the current state of the submission, the AC recommends rejection. \n \n Since the final scores of the reviewers might suggest otherwise, please let me explain my recommendation. \n \n The main contribution of this paper seems to be essentially a fast alternative to the method proposed in 'Adversarial Manipulation of Deep Representations', by Sabour et al, ICLR'16, although the lack of rigor and clarity in the presentation of section 3 makes this assessment uncertain. The most likely 'interpretation' of Eq (3) suggests that eta(x_o, x_t) = nabla_{x_o}( || f^(l)_w(x_t) - f^(l)_w(x_o) ||^2), which is simply one step of gradient descent of the method described in Sabour et al. One reviewer actually asked for clarification on this point on Dec. 26th, but the problem seems to be still present in the current manuscript. \n \n More generally, visualization and adversarial methods based on backpropagation of some form of distance measured in feature space towards the pixel space are not new; they can be traced back to Simoncelli & Portilla '99. \n Fast approximations based on simply stopping the gradient descent after one iteration do not constitute enough novelty. \n \n Another instance of lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure. It is not defined anywhere in the text, and the authors should not expect the reader to know its definition beforehand. \n \n Besides these issues, the paper does not contribute to the state-of-the-art of adversarial training nor feature visualization, mostly because its experiments are limited to mnist and face datasets. Since the main contribution of the paper is empirical, more emphasis should be made to present experiments on larger, more numerous datasets.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "07 Jan 2017", "TITLE": "Revision addressing comments and suggestions", "IS_META_REVIEW": false, "comments": "Our revised paper reflects to comments and suggestions for improvement.\n\n(1) We have added quantitative results of large-scale experiments comparing our LOTS approach to other adversarial example generation techniques such as the fast gradient sign (FGS) method, fast gradient value (FGV) method, and hot/cold (HC) approach. We have analyzed these techniques with respect to the quality of the produced images, and have also evaluated the effect of those samples when they are used for adversarial training.\n(2) We have clarified the motivation and advantage of using PASS over L-2 and L-inf norms with respect to measuring adversarial quality.\n(3) We have provided details about the internal structure of the tested network architectures, e.g., by denoting the position of layers in figures.\n(4) We have reviewed and simplified the whole paper to enhance readability and, last but not least, to allow us presenting the new results.\n\nThank you for all your comments and suggestions!", "OTHER_KEYS": "Andras Rozsa"}, {"TITLE": "Great start; recommended as workshop paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input\u2019s representation and difference vectors between representations of two inputs, where the inputs are of different classes.\n\nIn the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it\u2019s an interesting computation to perform, the value of the visualizations is not very clear.\n\nIn the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).\n\nThe paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.\n\nA few suggestions for improvement:\n - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn\u2019t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.\n - How many layers are in the networks used in the paper, and what is their internal structure? This isn\u2019t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.\n - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)\n - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.\n - 4.2 states \u201cIn summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.\u201d I don\u2019t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.\n\n\n\n1/19/17 UPDATE AFTER REBUTTAL:\nGiven that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "26 Dec 2016", "TITLE": "Question about eq 3 and 4", "IS_META_REVIEW": false, "comments": "Hi,\n\nI have a question about eq. 3 and 4.\n\nAs far as I understood from the beginning of section 3, f_{w}^{l} (x) is essentially activations at layer l, which means that value of f_{w}^{l} (x) is in R^{d_l} space where d_l is dimensionality of output of layer l.\nIn eq. (3) and (4) you \\eta compute gradient of f_{w}^{l} (x) over input x, which I would expect to be Jacobian matrix with size d_l*n (where n - dimensionality of input x). So looking at eq (3) and (4) I would expect that \\eta and x has different dimensionality. At the same time in section 4 you add s*\\eta to x.\nCould you explain this discrepancy in dimensionality and how \\eta should be calculated?\nMaybe you meant that numerator of eq (3) and (4) contain norm of f_{w}^{l} instead of it's value?\n\nThanks,\nAlex", "OTHER_KEYS": "Alexey Kurakin"}, {"TITLE": "Exciting new method to generate adversarial examples and study robustness, less interesting analyses", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.\n\nThe approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.\n\nPro: The presented approach is definitely sound, interesting and original. \nCon: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.\n\nIt would definitely improve the paper if it would present some interesting conclusions based on the new ideas.\n\n\n\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting but somewhat incomplete analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments:\n\nIt\u2019s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don\u2019t see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition.\n\nThe authors use the PASS score through the paper, but only given an intuition + citation for it. I think it\u2019s worth explaining what it actually does, in a sentence or two.\n\nThe PASS score seems to have some, but not complete, correlation with L_2, L_\\{infty} or visual estimation of how \u201cgood\u201d the adversarial examples are. I am not sure what the take-home message from all these numbers is.\n\n\u201cIn general, LOTS cannot produce high quality adversarial examples at the lower layers\u201d (sec 5.2) seems false for MNIST, no?\n\nI would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it\u2019s hard to judge the merits of this work in vacuum).\n\n-----\n\nEDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "11 Dec 2016", "TITLE": "Plotting or experimental bug?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Quantitative comparison of perturbation sizes with other methods", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "small questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Andras Rozsa, Manuel Gunther, Terrance E. Boult", "accepted": false, "id": "634"}