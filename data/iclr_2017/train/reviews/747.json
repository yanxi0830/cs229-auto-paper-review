{"conference": "ICLR 2017 conference submission", "title": "Gradients of Counterfactuals", "abstract": "Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs.  We propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score.   Best of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. \n\nOne particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?\n\nWith regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.\n\nFinally, does this model use batch normalization?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "11 Jan 2017", "TITLE": "New material added to the paper", "IS_META_REVIEW": false, "comments": "We added two new sections (2.5 and 2.6) to the paper. Section 2.5\nproposes two very desirable axioms for attribution methods,\nand uses them to rule out other attribution methods from the literature.\nSection 2.6 proposes a full axiomatization under which our method is\nunique.\n\nIt is possible that sections 2.3-2.7 may constitute a shorter 6-page\nself-contained paper with the title --- \"attributions using interior\ngradients\".\n", "OTHER_KEYS": "Ankur Taly"}, {"DATE": "22 Dec 2016", "TITLE": "Response to AnonReviewer2 and AnonReviewer3", "IS_META_REVIEW": false, "comments": "We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: ", "OTHER_KEYS": "Ankur Taly"}, {"TITLE": "review: lacking experimental comparison to prior work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.\n\nWhile motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an unnecessarily grandiose name for literally, a scaled image. \n\nThe authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.\n\nWhile the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more \u201cimportance\u201d on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy.\n\nUltimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.\n\nConsidering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. \u201cGradients of Coutnerfactuals\u201d sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Scaling input samples.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. \n\nOne particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?\n\nWith regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.\n\nFinally, does this model use batch normalization?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "comparison to prior work", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. \n\nOne particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?\n\nWith regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.\n\nFinally, does this model use batch normalization?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "11 Jan 2017", "TITLE": "New material added to the paper", "IS_META_REVIEW": false, "comments": "We added two new sections (2.5 and 2.6) to the paper. Section 2.5\nproposes two very desirable axioms for attribution methods,\nand uses them to rule out other attribution methods from the literature.\nSection 2.6 proposes a full axiomatization under which our method is\nunique.\n\nIt is possible that sections 2.3-2.7 may constitute a shorter 6-page\nself-contained paper with the title --- \"attributions using interior\ngradients\".\n", "OTHER_KEYS": "Ankur Taly"}, {"DATE": "22 Dec 2016", "TITLE": "Response to AnonReviewer2 and AnonReviewer3", "IS_META_REVIEW": false, "comments": "We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: ", "OTHER_KEYS": "Ankur Taly"}, {"TITLE": "review: lacking experimental comparison to prior work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.\n\nWhile motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an unnecessarily grandiose name for literally, a scaled image. \n\nThe authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.\n\nWhile the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more \u201cimportance\u201d on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy.\n\nUltimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.\n\nConsidering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. \u201cGradients of Coutnerfactuals\u201d sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Scaling input samples.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. \n\nOne particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?\n\nWith regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.\n\nFinally, does this model use batch normalization?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "comparison to prior work", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Mukund Sundararajan, Ankur Taly, Qiqi Yan", "accepted": false, "id": "747"}