{"conference": "ICLR 2017 conference submission", "title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlthough the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. \nAlso the performance on the SQuAD dataset is good. \nI would recommend this paper to be accepted."}, {"DATE": "14 Feb 2017", "TITLE": "Update", "IS_META_REVIEW": false, "comments": "We sincerely thank all reviewers for your feedback and comments! We have updated our paper, making minor adjustments such as fixing typos. We have also included some additional ablation studies, requested during the review process, in the appendix.", "OTHER_KEYS": "Victor Zhong"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that this is a good piece of work that should be accepted to ICLR. Authors are encouraged to incorporate reviewer feedback to further strengthen the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Implementation details", "IS_META_REVIEW": false, "comments": "Thanks a lot for a great paper. Could you please provide some implementation details ?\n1. What is the sentinel initialized with ? Is is just a 0 filled vector ? \n2. What are the initial start / end values that you feed to LSTM state before the first iteration ?\n3. What initialization values you use for W's and bias ?\n4. In the ablation study, you evaluated two layers of MLP. Is that applied on each word individually ? Or on the whole sequence ? In other words, the first layer matrix dimension is [2l, 2l] or [2l*m, 2l*m] ?\n5. You mention using dropout. Is is used only on the question/document encodding ? And what amount ? Do you also use L2 regularization ?\n\nThanks.", "OTHER_KEYS": "Radu Kopetz"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "\nPaper Summary: \nThe paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models.\n\nPaper Strengths: \n-- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations.\n-- The paper also presents ablation study of the proposed model which shows the importance of their design choices.\n-- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension.\n-- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task.\n-- The proposed model achieves state-of-the-art result on SQuAD dataset.\n-- The model architecture has been clearly described.\n\nPaper Weaknesses / Future Thoughts: \n-- The paper provides model's performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations?\n-- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc?\n\nPreliminary Evaluation: \nNovel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting model but need some more analyses", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.\n\nStrengths:\n\n1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.\n\n2. The proposed model architecture is novel and the design choices made seem reasonable.\n\n3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.\n\n4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.\n\nWeaknesses/Questions/Suggestions:\n\n1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.\n\n2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?\n\n3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).\n\n4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n\n5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.\n\n6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it.\n\nReview Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Official Review", "comments": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlthough the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. \nAlso the performance on the SQuAD dataset is good. \nI would recommend this paper to be accepted.\n\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Clarification questions", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"TITLE": "escape the local maxima", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"TITLE": "Objective Function", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"DATE": "27 Nov 2016", "TITLE": "A few questions about vocabulary size", "IS_META_REVIEW": false, "comments": "Hi,\n\nA sentence from Section 4 reads: \"We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out-of-vocabulary words to zero.\" Suppose GloVe vectors' vocabulary size is k (k ~ 2.2 million). Does this mean that the vocabulary size used in this paper is exactly 1 more than GloVe's vocabulary size (k+1)? Does this mean that effectively every out of vocabulary word is replaced with an ", "OTHER_KEYS": "Sahil Sharma"}, {"DATE": "10 Nov 2016", "TITLE": "Ablation study needed", "IS_META_REVIEW": false, "comments": "The paper introduces several interesting mechanisms for answer extraction in a QA setup. However, I find it really difficult to see where the actual benefits come from compared to related architectures like the work of Wang et al. 2016, which was also submitted to ICLR. I believe much of the improvements stem from the dynamic pointer decoder which itself contains various sub-architectures. For the sake of comparability, I would be interested to see how performances would change when:\n\n- not using co-attention but only attention on the question for each context token, similar to Wang et al.\n- exchanging the rather large Highway Maxout Network by something simpler, e.g. omitting m1 and m2 completely to get directly to the output, lowering pool-sizes because they seem rather large as well or using a simple MLP instead of the HMN. The intuition behind using Maxout sounds appealing but it has not been verified in the paper.\n- allowing only one iteration for the pointer decoder (i.e., direct prediction)\n\nThanks for considering these suggestions.", "OTHER_KEYS": "Dirk Weissenborn"}, {"IS_META_REVIEW": true, "comments": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlthough the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. \nAlso the performance on the SQuAD dataset is good. \nI would recommend this paper to be accepted."}, {"DATE": "14 Feb 2017", "TITLE": "Update", "IS_META_REVIEW": false, "comments": "We sincerely thank all reviewers for your feedback and comments! We have updated our paper, making minor adjustments such as fixing typos. We have also included some additional ablation studies, requested during the review process, in the appendix.", "OTHER_KEYS": "Victor Zhong"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that this is a good piece of work that should be accepted to ICLR. Authors are encouraged to incorporate reviewer feedback to further strengthen the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Implementation details", "IS_META_REVIEW": false, "comments": "Thanks a lot for a great paper. Could you please provide some implementation details ?\n1. What is the sentinel initialized with ? Is is just a 0 filled vector ? \n2. What are the initial start / end values that you feed to LSTM state before the first iteration ?\n3. What initialization values you use for W's and bias ?\n4. In the ablation study, you evaluated two layers of MLP. Is that applied on each word individually ? Or on the whole sequence ? In other words, the first layer matrix dimension is [2l, 2l] or [2l*m, 2l*m] ?\n5. You mention using dropout. Is is used only on the question/document encodding ? And what amount ? Do you also use L2 regularization ?\n\nThanks.", "OTHER_KEYS": "Radu Kopetz"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "\nPaper Summary: \nThe paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models.\n\nPaper Strengths: \n-- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations.\n-- The paper also presents ablation study of the proposed model which shows the importance of their design choices.\n-- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension.\n-- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task.\n-- The proposed model achieves state-of-the-art result on SQuAD dataset.\n-- The model architecture has been clearly described.\n\nPaper Weaknesses / Future Thoughts: \n-- The paper provides model's performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations?\n-- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc?\n\nPreliminary Evaluation: \nNovel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting model but need some more analyses", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.\n\nStrengths:\n\n1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.\n\n2. The proposed model architecture is novel and the design choices made seem reasonable.\n\n3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.\n\n4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.\n\nWeaknesses/Questions/Suggestions:\n\n1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.\n\n2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?\n\n3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).\n\n4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n\n5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.\n\n6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it.\n\nReview Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Official Review", "comments": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlthough the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. \nAlso the performance on the SQuAD dataset is good. \nI would recommend this paper to be accepted.\n\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Clarification questions", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"TITLE": "escape the local maxima", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"TITLE": "Objective Function", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"DATE": "27 Nov 2016", "TITLE": "A few questions about vocabulary size", "IS_META_REVIEW": false, "comments": "Hi,\n\nA sentence from Section 4 reads: \"We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out-of-vocabulary words to zero.\" Suppose GloVe vectors' vocabulary size is k (k ~ 2.2 million). Does this mean that the vocabulary size used in this paper is exactly 1 more than GloVe's vocabulary size (k+1)? Does this mean that effectively every out of vocabulary word is replaced with an ", "OTHER_KEYS": "Sahil Sharma"}, {"DATE": "10 Nov 2016", "TITLE": "Ablation study needed", "IS_META_REVIEW": false, "comments": "The paper introduces several interesting mechanisms for answer extraction in a QA setup. However, I find it really difficult to see where the actual benefits come from compared to related architectures like the work of Wang et al. 2016, which was also submitted to ICLR. I believe much of the improvements stem from the dynamic pointer decoder which itself contains various sub-architectures. For the sake of comparability, I would be interested to see how performances would change when:\n\n- not using co-attention but only attention on the question for each context token, similar to Wang et al.\n- exchanging the rather large Highway Maxout Network by something simpler, e.g. omitting m1 and m2 completely to get directly to the output, lowering pool-sizes because they seem rather large as well or using a simple MLP instead of the HMN. The intuition behind using Maxout sounds appealing but it has not been verified in the paper.\n- allowing only one iteration for the pointer decoder (i.e., direct prediction)\n\nThanks for considering these suggestions.", "OTHER_KEYS": "Dirk Weissenborn"}], "authors": "Caiming Xiong, Victor Zhong, Richard Socher", "accepted": true, "id": "388"}