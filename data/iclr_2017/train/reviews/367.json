{"conference": "ICLR 2017 conference submission", "title": "Optimal Binary Autoencoding with Pairwise Correlations", "abstract": "We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data. \nThe paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.\n \nGiven the weights of the \u2018min-max\u2019decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.\n\nThe paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.\n\n\nclarity:\n\n-The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2). \n\n\nsignificance\n\nOverall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.\n\n- what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.\n\n- what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?\n\n- Have you tried on more complex datasets such as cifar?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers (weakly) support the acceptance of the paper. I also think that binary neural networks model is an important direction to explore.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. \n\nThe experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing. ", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "The author attacks the problem of shallow binary autoencoders using a minmax game approach. The algorithm, though simple, appears to be very effective. The paper is well written and has sound analyses. Although the work does not extend to deep networks immediately, its connections with other popular minmax approaches (eg GANs) could be fruitful in the future.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 2}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data. \nThe paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.\n \nGiven the weights of the \u2018min-max\u2019decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.\n\nThe paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.\n\n\nclarity:\n\n-The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2). \n\n\nsignificance\n\nOverall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.\n\n- what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.\n\n- what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?\n\n- Have you tried on more complex datasets such as cifar?", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IS_META_REVIEW": true, "comments": "The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data. \nThe paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.\n \nGiven the weights of the \u2018min-max\u2019decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.\n\nThe paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.\n\n\nclarity:\n\n-The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2). \n\n\nsignificance\n\nOverall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.\n\n- what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.\n\n- what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?\n\n- Have you tried on more complex datasets such as cifar?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers (weakly) support the acceptance of the paper. I also think that binary neural networks model is an important direction to explore.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. \n\nThe experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing. ", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "The author attacks the problem of shallow binary autoencoders using a minmax game approach. The algorithm, though simple, appears to be very effective. The paper is well written and has sound analyses. Although the work does not extend to deep networks immediately, its connections with other popular minmax approaches (eg GANs) could be fruitful in the future.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 2}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data. \nThe paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.\n \nGiven the weights of the \u2018min-max\u2019decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.\n\nThe paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.\n\n\nclarity:\n\n-The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2). \n\n\nsignificance\n\nOverall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.\n\n- what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.\n\n- what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?\n\n- Have you tried on more complex datasets such as cifar?", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}], "authors": "Akshay Balsubramani", "accepted": true, "id": "367"}