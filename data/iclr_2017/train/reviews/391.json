{"conference": "ICLR 2017 conference submission", "title": "Exploring Sparsity in Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8\u00d7 and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse GEMMs. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2\u00d7 to 7\u00d7.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Here is a summary of the reviews:\n \n Strengths\n Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)\n Networks themselves are of a very large size (R3)\n Computational gains are substantial (R3, R4)\n Paper is clear (R1)\n \n Weaknesses\n Experiments are all done on a private dataset (R3)\n No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method\n No comparison to distillation techniques (R1)\n Paper doesn't present much novelty in terms of ideas (R3)\n \n The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a \"clear reject\" updated the score to 6 (just above acceptance).\n \n Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Revision with larger GRU model", "IS_META_REVIEW": false, "comments": "Based on the feedback from one of the reviewers, we have trained a larger sparse GRU model. This model is 2.2% worse than the GRU Dense baseline while 3.5 times faster than the dense GRU model. This larger GRU network recoups most of the loss in performance due to pruning. We believe that we can match the Dense GRU baseline performance by slightly reducing the sparsity of the network or increasing the number of hidden units. We thank the reviewer for this helpful suggestion. I have uploaded a new revision of this paper with this result. ", "OTHER_KEYS": "Sharan Narang"}, {"DATE": "14 Jan 2017", "TITLE": "New Revision", "IS_META_REVIEW": false, "comments": "I have uploaded a new revision which includes results using the pruning method proposed in \"Exploiting sparseness in deep neural networks for large vocabulary speech recognition\" by Yu et. al. The results show that gradual pruning used in our approach performs better (in terms of accuracy) than hard pruning. We thank the reviewers for the suggestion to compare our method with this pruning approach. ", "OTHER_KEYS": "Sharan Narang"}, {"DATE": "17 Dec 2016", "TITLE": "structurally sparse RNNs", "IS_META_REVIEW": false, "comments": "The paper is well motivated to explore the sparsity in RNNs after lots of works on sparse CNNs.\nIn general, methods in CNNs could be generalized to RNNs. It would be more comprehensive to compare the proposed method with those methods used in CNNs. For the speedup part, it would be more convictive to compare with structurally-sparse DNNs, which are recently proposed in CNNs:\n(1) ", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Review: Exploring Sparsity in Recurrent Neural Networks", "MEANINGFUL_COMPARISON": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn\u2019t motivate the details of the method very well. It\u2019s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don\u2019t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n\n[1] Jaeger, H. (2001). The \u201cecho state\u201d approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "sparsity vs accuracy", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. \n\nPros:\nProposes a pruning method that doesn\u2019t need re-training and doesn\u2019t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.\n\nCons & Questions:\nJudiciously choosing hyper parameters for different models and different applications wouldn\u2019t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)\n\nIn table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can\u2019t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)\n\nWhy sparsity for table3 and table5 are different? In text: \u201caverage sparsity of 88%\u201d but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016 (modified: 22 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Much better connection to prior work, but the baselines could still be stronger", "comments": "Updated review: 18 Jan. 2017\n\nThanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012.  The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the \"RNN Sparse 1760\" result in Table 3.\n\nI have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.\n\nThis paper has three main contributions.  (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training.  (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model.  (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.\n\nPros\n+ The paper is mostly clear and easy to understand.\n+ The paper tackles an important, practical problem in deep learning:  how to successfully deploy models at the lowest possible computational and memory cost.\n\nCons\n- As a second baseline, this paper should compare to \"distillation\" approaches (e.g., ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "13 Dec 2016 (modified: 18 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "When weights are added again, which value do they initially have?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 4, "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Why not use a teacher-student baseline?", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "29 Nov 2016"}, {"DATE": "29 Nov 2016", "TITLE": "Training time", "IS_META_REVIEW": false, "comments": "In introduction: \"... unlike previous approaches\nsuch as in Han et al. (2015). State of the art results in speech recognition generally require between\ndays and weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable.\"\n\nBut, according to Han et al. (2015), \"Huffman coding doesn\u2019t require training and is implemented\noffline after all the fine-tuning is finished.\"\n\nBoth yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.\nWhere does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "29 Nov 2016", "TITLE": "Have tried on LSTM?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Here is a summary of the reviews:\n \n Strengths\n Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)\n Networks themselves are of a very large size (R3)\n Computational gains are substantial (R3, R4)\n Paper is clear (R1)\n \n Weaknesses\n Experiments are all done on a private dataset (R3)\n No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method\n No comparison to distillation techniques (R1)\n Paper doesn't present much novelty in terms of ideas (R3)\n \n The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a \"clear reject\" updated the score to 6 (just above acceptance).\n \n Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Revision with larger GRU model", "IS_META_REVIEW": false, "comments": "Based on the feedback from one of the reviewers, we have trained a larger sparse GRU model. This model is 2.2% worse than the GRU Dense baseline while 3.5 times faster than the dense GRU model. This larger GRU network recoups most of the loss in performance due to pruning. We believe that we can match the Dense GRU baseline performance by slightly reducing the sparsity of the network or increasing the number of hidden units. We thank the reviewer for this helpful suggestion. I have uploaded a new revision of this paper with this result. ", "OTHER_KEYS": "Sharan Narang"}, {"DATE": "14 Jan 2017", "TITLE": "New Revision", "IS_META_REVIEW": false, "comments": "I have uploaded a new revision which includes results using the pruning method proposed in \"Exploiting sparseness in deep neural networks for large vocabulary speech recognition\" by Yu et. al. The results show that gradual pruning used in our approach performs better (in terms of accuracy) than hard pruning. We thank the reviewers for the suggestion to compare our method with this pruning approach. ", "OTHER_KEYS": "Sharan Narang"}, {"DATE": "17 Dec 2016", "TITLE": "structurally sparse RNNs", "IS_META_REVIEW": false, "comments": "The paper is well motivated to explore the sparsity in RNNs after lots of works on sparse CNNs.\nIn general, methods in CNNs could be generalized to RNNs. It would be more comprehensive to compare the proposed method with those methods used in CNNs. For the speedup part, it would be more convictive to compare with structurally-sparse DNNs, which are recently proposed in CNNs:\n(1) ", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Review: Exploring Sparsity in Recurrent Neural Networks", "MEANINGFUL_COMPARISON": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn\u2019t motivate the details of the method very well. It\u2019s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don\u2019t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n\n[1] Jaeger, H. (2001). The \u201cecho state\u201d approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "sparsity vs accuracy", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. \n\nPros:\nProposes a pruning method that doesn\u2019t need re-training and doesn\u2019t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.\n\nCons & Questions:\nJudiciously choosing hyper parameters for different models and different applications wouldn\u2019t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)\n\nIn table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can\u2019t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)\n\nWhy sparsity for table3 and table5 are different? In text: \u201caverage sparsity of 88%\u201d but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016 (modified: 22 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Much better connection to prior work, but the baselines could still be stronger", "comments": "Updated review: 18 Jan. 2017\n\nThanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012.  The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the \"RNN Sparse 1760\" result in Table 3.\n\nI have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.\n\nThis paper has three main contributions.  (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training.  (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model.  (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.\n\nPros\n+ The paper is mostly clear and easy to understand.\n+ The paper tackles an important, practical problem in deep learning:  how to successfully deploy models at the lowest possible computational and memory cost.\n\nCons\n- As a second baseline, this paper should compare to \"distillation\" approaches (e.g., ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "13 Dec 2016 (modified: 18 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "When weights are added again, which value do they initially have?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 4, "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Why not use a teacher-student baseline?", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "29 Nov 2016"}, {"DATE": "29 Nov 2016", "TITLE": "Training time", "IS_META_REVIEW": false, "comments": "In introduction: \"... unlike previous approaches\nsuch as in Han et al. (2015). State of the art results in speech recognition generally require between\ndays and weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable.\"\n\nBut, according to Han et al. (2015), \"Huffman coding doesn\u2019t require training and is implemented\noffline after all the fine-tuning is finished.\"\n\nBoth yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.\nWhere does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "29 Nov 2016", "TITLE": "Have tried on LSTM?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}], "authors": "Sharan Narang, Greg Diamos, Shubho Sengupta, Erich Elsen", "accepted": true, "id": "391"}