{"conference": "ICLR 2017 conference submission", "title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper studies efficient signal representations to perform bioacoustic classification based on CNNs. Contrary to image classification, where most useful information can be extracted with spatially localized kernels, bioacoustic signatures are more localized in the frequency domain, requiring to rethink the design of convolutional architectures. The authors propose to enforce the lower layers of the architecture with chirplet transforms, which are localized in the time-frequency plane as wavelets, but with time-varying central frequency. They present preliminary numerical experiments showing promising improvements over existing baselines. \n \n The reviewers found interest in the method, but raised concerns on the relatively narrow scope of the method, as well as the clarity and rigor of the presentation. Whereas the first concern is up to debate, I agree that the paper currently suffers from poor english which affects its clarity. \n \n Despite these concerns, the AC finds the contribution useful in the broader context of inductive bias and injecting priors in neural networks. This is an example where the inductive priors that work well on images (localized convolutions rather than generic fully connected layers) are not sufficient unless given massive amounts of data. The AC thus recommends rejection, but invites the contribution to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Chirplet transforms for small data tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A work to try to structure a deep network", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Pros: \n- Introduction of a nice filter banks and its implementation\n- Good numerical results\n- Refinement of the representation via back propagation, and a demonstration that it speeds up learning\n\nCons:\n- The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper. However, a source code would be great!\n- The link with a scattering transform is not clear\n- Sometimes (as mentionned in some of my comments), the writing could be improved.\n\nFrom a personal point of view, I also believe the negative points I mention can be easily removed.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors advocate use of chirplets as a basis for modeling audio signals.  They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).  The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).\n\nWhile the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.  Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).  I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "01 Dec 2016", "TITLE": "Design of the Chirplets", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper studies efficient signal representations to perform bioacoustic classification based on CNNs. Contrary to image classification, where most useful information can be extracted with spatially localized kernels, bioacoustic signatures are more localized in the frequency domain, requiring to rethink the design of convolutional architectures. The authors propose to enforce the lower layers of the architecture with chirplet transforms, which are localized in the time-frequency plane as wavelets, but with time-varying central frequency. They present preliminary numerical experiments showing promising improvements over existing baselines. \n \n The reviewers found interest in the method, but raised concerns on the relatively narrow scope of the method, as well as the clarity and rigor of the presentation. Whereas the first concern is up to debate, I agree that the paper currently suffers from poor english which affects its clarity. \n \n Despite these concerns, the AC finds the contribution useful in the broader context of inductive bias and injecting priors in neural networks. This is an example where the inductive priors that work well on images (localized convolutions rather than generic fully connected layers) are not sufficient unless given massive amounts of data. The AC thus recommends rejection, but invites the contribution to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Chirplet transforms for small data tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A work to try to structure a deep network", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Pros: \n- Introduction of a nice filter banks and its implementation\n- Good numerical results\n- Refinement of the representation via back propagation, and a demonstration that it speeds up learning\n\nCons:\n- The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper. However, a source code would be great!\n- The link with a scattering transform is not clear\n- Sometimes (as mentionned in some of my comments), the writing could be improved.\n\nFrom a personal point of view, I also believe the negative points I mention can be easily removed.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors advocate use of chirplets as a basis for modeling audio signals.  They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).  The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).\n\nWhile the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.  Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).  I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "01 Dec 2016", "TITLE": "Design of the Chirplets", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Herve Glotin, Julien Ricard, Randall Balestriero", "accepted": false, "id": "521"}