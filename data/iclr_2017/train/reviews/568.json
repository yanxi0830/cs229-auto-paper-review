{"conference": "ICLR 2017 conference submission", "title": "Character-aware Attention Residual Network for Sentence Representation", "abstract": "Text classification in general is a well studied area. However, classifying short and noisy text remains challenging. Feature sparsity is a major issue. The quality of document representation here has a great impact on the classification accuracy. Existing methods represent text using bag-of-word model, with TFIDF or other weighting schemes. Recently word embedding and even document embedding are proposed to represent text. The purpose is to capture features at both word level and sentence level. However, the character level information are usually ignored. In this paper, we take word morphology and word semantic meaning into consideration, which are represented by character-aware embedding and word distributed embedding. By concatenating both character-level and word distributed embedding together and arranging words in order, a sentence representation matrix could be obtained. To overcome data sparsity problem of short text, sentence representation vector is then derived based on different views from sentence representation matrix. The various views contributes to the construction of an enriched sentence embedding. We employ a residual network on the sentence embedding to get a consistent and refined sentence representation. Evaluated on a few short text datasets, our model outperforms state-of-the-art models.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper introduces some interesting architectural ideas for character-aware sequence modelling. However, as pointed out by reviewers and from my own reading of the paper, this paper fails badly on the evaluation front. First, some of the evaluation tasks are poorly defined (e.g. question task). Second, the tasks look fairly simple, whereas there are \"standard\" tasks such as language modelling datasets (one of the reviewers suggests TREC, but other datasets such as NANT, PTB, or even the Billion Word Corpus) which could be used here. Finally, the benchmarks presented against are weak. There are several character-aware language models which obtain robust results on LM data which could readily be adapted to sentence representation learning, eg. Ling et al. 2016, or Chung et al. 2016, which should have been compared against. The authors should look at the evaluations in these papers and consider them for a future version of this paper. As it stands, I cannot recommend acceptance in its current form.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a new model for sentence classification. \n\nPros:\n- Some interesting architecture choices in the network.\n\nCons:\n- No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not.\n- No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Need more explanation about network architecture", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a new neural network model for sentence representation. This new model is inspired by the success of residual network in Computer Vision and some observation of word morphology in Natural Language Processing. Although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.\n\nTo be specific:\n\n- I was confused by the contribution of this paper: character-aware word embedding or residual network or both?\n- The claim of using residual network in section 3.3 seems pretty thin, since it ignores some fundamental difference between image representation and sentence representation. Even though the results show that adding residual network could help, I was still not be convinced. Is there any explanation about what is captured in the residual component from the perspective of sentence modeling?\n- This paper combines several components in the classification framework, including character-aware model for word embedding, residual network and attention weight in Type 1 feature. I would like to see the contribution from each of them to the final performance, while in Table 3 I only saw one of them. Is it possible to add more results on the ablation test?\n- In equation (5), what is the meaning of $i$ in $G_i$?\n- The citation format is impropriate\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A paper that needs more work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "02 Dec 2016", "TITLE": "Evaluation on sentiment analysis", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "15 Nov 2016", "TITLE": "duplicate paper", "IS_META_REVIEW": false, "comments": "Hi Authors,\n\nYou seem to have submitted two of the same paper? Pls advise which is the correct one\n\nCharacter-aware Attention Residual Network for Sentence Representation\nXin Zheng, Zhenzhou Wu\n5 Nov 2016\n\nCHARACTER-AWARE RESIDUAL NETWORK FOR SENTENCE REPRESENTATION\nXin Zheng, Zhenzhou Wu\n4 Nov 2016\n", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper introduces some interesting architectural ideas for character-aware sequence modelling. However, as pointed out by reviewers and from my own reading of the paper, this paper fails badly on the evaluation front. First, some of the evaluation tasks are poorly defined (e.g. question task). Second, the tasks look fairly simple, whereas there are \"standard\" tasks such as language modelling datasets (one of the reviewers suggests TREC, but other datasets such as NANT, PTB, or even the Billion Word Corpus) which could be used here. Finally, the benchmarks presented against are weak. There are several character-aware language models which obtain robust results on LM data which could readily be adapted to sentence representation learning, eg. Ling et al. 2016, or Chung et al. 2016, which should have been compared against. The authors should look at the evaluations in these papers and consider them for a future version of this paper. As it stands, I cannot recommend acceptance in its current form.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a new model for sentence classification. \n\nPros:\n- Some interesting architecture choices in the network.\n\nCons:\n- No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not.\n- No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Need more explanation about network architecture", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a new neural network model for sentence representation. This new model is inspired by the success of residual network in Computer Vision and some observation of word morphology in Natural Language Processing. Although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.\n\nTo be specific:\n\n- I was confused by the contribution of this paper: character-aware word embedding or residual network or both?\n- The claim of using residual network in section 3.3 seems pretty thin, since it ignores some fundamental difference between image representation and sentence representation. Even though the results show that adding residual network could help, I was still not be convinced. Is there any explanation about what is captured in the residual component from the perspective of sentence modeling?\n- This paper combines several components in the classification framework, including character-aware model for word embedding, residual network and attention weight in Type 1 feature. I would like to see the contribution from each of them to the final performance, while in Table 3 I only saw one of them. Is it possible to add more results on the ablation test?\n- In equation (5), what is the meaning of $i$ in $G_i$?\n- The citation format is impropriate\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A paper that needs more work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "02 Dec 2016", "TITLE": "Evaluation on sentiment analysis", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "15 Nov 2016", "TITLE": "duplicate paper", "IS_META_REVIEW": false, "comments": "Hi Authors,\n\nYou seem to have submitted two of the same paper? Pls advise which is the correct one\n\nCharacter-aware Attention Residual Network for Sentence Representation\nXin Zheng, Zhenzhou Wu\n5 Nov 2016\n\nCHARACTER-AWARE RESIDUAL NETWORK FOR SENTENCE REPRESENTATION\nXin Zheng, Zhenzhou Wu\n4 Nov 2016\n", "OTHER_KEYS": "Tara N Sainath"}], "authors": "Xin Zheng, Zhenzhou Wu", "accepted": false, "id": "568"}