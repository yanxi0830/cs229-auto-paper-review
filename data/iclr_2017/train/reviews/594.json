{"conference": "ICLR 2017 conference submission", "title": "Low-rank passthrough neural networks", "abstract": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these \"Passthrough Networks\" architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers seem to agree that the framework presented is not very novel, something I agree with.\n The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Third revision", "IS_META_REVIEW": false, "comments": "I added additional results on the memory tasks and language modeling task.\n\nFor the memory task, as suggested by the reviewers, I compare against uRNN models with approx. the same number of parameters as the LRD-GRU. This uRNN, much larger than the one used in the original uRNN paper, converges quickly on the simple memory task but overfits (training was terminated by early stopping in the reported graph).\n\nOn the variable sequence length task (Danihelka 2016) it trains faster (with somewhat larger oscillations) than the LRD-GRU, but on the variable lag task (Henaff 2016) it fails.\n\nI've also considered a variant of the LRD-GRU which I tried only on these two subtasks. In this variant I add weight normalization (Salimans 2016) and a weight max-row-norm constraint in order to reduce the amounts of time that NaN recovery triggers. With this modification, NaN recovery is unnecessary, and the models train much faster. In fact, it performs on par with the uRNN even on the (Danihelka 2016) task.\n\nFor the language modelling task, I did experiments with a LSTM baseline and a low-rank plus diagonal LSTM, while trying to use the same setup of (Graves 2013). I still can't replicate those results (the code is online at ", "OTHER_KEYS": "Antonio Valerio Miceli Barone"}, {"DATE": "06 Jan 2017", "TITLE": "From an interested reader: Feedback", "IS_META_REVIEW": false, "comments": "I would like to give some feedback on an area the reviewers did not touch on, presentational shortcomings. I think those would also have to be improved for an acceptance of the paper.\n\nComments:\n\nThe authors say \"Classical physical systems, however, consist of spatially separated parts with primarily local interactions, long-distance interactions are possible but they tend to be limited by propagation delays, bandwidth and noise. Therefore it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them\" However, the diagonal + low-rank model assigns the majority of its parameters to the low-rank matrix, which is NOT local. Hence, one cannot say that the authors adhere to these physical constraints.\n\nIn section 3.1, the authors claim that 99.13% is the state-of-the-art on MNIST. LeCun achieved a lower error rate in 1998! Do you mean the state-of-the-art on randomly permuted MNIST?\n\nAlso, in section 3.1, you fail to compare against the same network without low-rank factorization. Also you do not mention how many parameters your model saved compared to a full-rank model. Overall, I think section 3.1 should just be omitted, as it contains too little information to be useful.\n\nI agree with reviewer three that the statement \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" makes it sound as if the framework is a contribution, which it isn't. (It is on page 2 of the original highway paper!) Also the framework is so general one almost cannot call it a framework. (We define a general function in terms of 3 new general functions.) Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place. Any neural network or indeed any method that somehow involves matrices can have its matrices replaced with low-rank + symmetric form. I think the authors have the believe that passthrough networks do not alter the state as much per layer as other networks, and thus are more likely to have a wasteful parameter budget. However, this is just a belief and not experimentally validated. In summary, section 2 is more confusing than helpful. I would immediately start by mentioning the model you are investigating (low-rank + symmetric) and then briefly explain why you think it makes particular sense to apply it to GRU / Highway networks.\n\nThe graphs in Figure 2 are two large. One-curve graphs do not need one sixth of a page. While this is obviously just a presentation shortcoming, it nevertheless sends the message that you didn't have enough to say to fill the page space with words. Also, if you are going to use the entire page for the six graphs, at least make the axis labels of a font size comparable to that of the main text.\n\nIn section 3.2.1 and 3.2.2, you have to present your results in a table and not just in the text, as you do in section 3.2.3 and 3.2.4. If you make Figure 2 smaller (see previous paragraph), you will have plenty of space for this.\n\nDetails such as learning rate and mini-batch size should either be in the appendix or in a seperate section, not intermixed with the experimental results. That makes it harder to read.\n\nI can't find any experiment where diagonal without symmetric model outperformed the diagonal plus symmetric model. If there is no such experiment, the diagonal without symmetric model should be omitted from the paper or relegated to a side note as it adds virtually no value, as the vast majority of parameters are tied up in the diagonal matrix.\n\nIn the conclusion, you say that your approach is \"orthogonal\" to the convolutional parametrizations explored by He et. al. How is that? He et al. experiment with a low-rank structure (they call it bottleneck) between each skip connection. That seems very similar to what you do. In any case, if I use the low-rank layout from He et al., it would be reasonable to think that using your low-rank layout in addition would yield at most diminishing returns, hence the methods are not orthogonal. Also, you talk about the convolutional parametrization in Srivastava et al in the same sentence. As far as I can tell, that paper neither dealt with convlutional nets nor with low-rank factorizations.\n\nYou include a lot of very basic references in your papers, i.e. multiple references on \"neural networks were successful\", multiple on the vanishing gradient problem, etc. I think you don't need that many \"obvious\" references. (This is a very minor point.) On the other hand, I think you are missing some more important references that have looked at low-rank or related methods for neural nets, such as:\n\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc' Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156.\n2013.\n\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with\nlow rank expansions. arXiv preprint arXiv:1405.3866, 2014.\n\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure\nwithin convolutional networks for efficient evaluation. In Advances in Neural Information Processing\nSystems, pages 1269\u20131277. 2014.", "OTHER_KEYS": "George Philipp"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network.\n\nAuthor also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.\n\nAn empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). \n\nHowever, there are few problems with the evaluation:\n\n- In the highway network experiment, the author does not compare with a baseline.\nWe can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?.\n\n- It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.\nAlso the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.\n\n-Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size.\n\n- It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.\n\n- it is not clear when to use low-rank or low-rank + diagonal from the experiments.\n\nOverall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "my review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.\nThat said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016"}, {"TITLE": "Exploring a solid idea, but results are not convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks.\n\nThe paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)\n\nAside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "No question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Prereview question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers seem to agree that the framework presented is not very novel, something I agree with.\n The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Third revision", "IS_META_REVIEW": false, "comments": "I added additional results on the memory tasks and language modeling task.\n\nFor the memory task, as suggested by the reviewers, I compare against uRNN models with approx. the same number of parameters as the LRD-GRU. This uRNN, much larger than the one used in the original uRNN paper, converges quickly on the simple memory task but overfits (training was terminated by early stopping in the reported graph).\n\nOn the variable sequence length task (Danihelka 2016) it trains faster (with somewhat larger oscillations) than the LRD-GRU, but on the variable lag task (Henaff 2016) it fails.\n\nI've also considered a variant of the LRD-GRU which I tried only on these two subtasks. In this variant I add weight normalization (Salimans 2016) and a weight max-row-norm constraint in order to reduce the amounts of time that NaN recovery triggers. With this modification, NaN recovery is unnecessary, and the models train much faster. In fact, it performs on par with the uRNN even on the (Danihelka 2016) task.\n\nFor the language modelling task, I did experiments with a LSTM baseline and a low-rank plus diagonal LSTM, while trying to use the same setup of (Graves 2013). I still can't replicate those results (the code is online at ", "OTHER_KEYS": "Antonio Valerio Miceli Barone"}, {"DATE": "06 Jan 2017", "TITLE": "From an interested reader: Feedback", "IS_META_REVIEW": false, "comments": "I would like to give some feedback on an area the reviewers did not touch on, presentational shortcomings. I think those would also have to be improved for an acceptance of the paper.\n\nComments:\n\nThe authors say \"Classical physical systems, however, consist of spatially separated parts with primarily local interactions, long-distance interactions are possible but they tend to be limited by propagation delays, bandwidth and noise. Therefore it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them\" However, the diagonal + low-rank model assigns the majority of its parameters to the low-rank matrix, which is NOT local. Hence, one cannot say that the authors adhere to these physical constraints.\n\nIn section 3.1, the authors claim that 99.13% is the state-of-the-art on MNIST. LeCun achieved a lower error rate in 1998! Do you mean the state-of-the-art on randomly permuted MNIST?\n\nAlso, in section 3.1, you fail to compare against the same network without low-rank factorization. Also you do not mention how many parameters your model saved compared to a full-rank model. Overall, I think section 3.1 should just be omitted, as it contains too little information to be useful.\n\nI agree with reviewer three that the statement \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" makes it sound as if the framework is a contribution, which it isn't. (It is on page 2 of the original highway paper!) Also the framework is so general one almost cannot call it a framework. (We define a general function in terms of 3 new general functions.) Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place. Any neural network or indeed any method that somehow involves matrices can have its matrices replaced with low-rank + symmetric form. I think the authors have the believe that passthrough networks do not alter the state as much per layer as other networks, and thus are more likely to have a wasteful parameter budget. However, this is just a belief and not experimentally validated. In summary, section 2 is more confusing than helpful. I would immediately start by mentioning the model you are investigating (low-rank + symmetric) and then briefly explain why you think it makes particular sense to apply it to GRU / Highway networks.\n\nThe graphs in Figure 2 are two large. One-curve graphs do not need one sixth of a page. While this is obviously just a presentation shortcoming, it nevertheless sends the message that you didn't have enough to say to fill the page space with words. Also, if you are going to use the entire page for the six graphs, at least make the axis labels of a font size comparable to that of the main text.\n\nIn section 3.2.1 and 3.2.2, you have to present your results in a table and not just in the text, as you do in section 3.2.3 and 3.2.4. If you make Figure 2 smaller (see previous paragraph), you will have plenty of space for this.\n\nDetails such as learning rate and mini-batch size should either be in the appendix or in a seperate section, not intermixed with the experimental results. That makes it harder to read.\n\nI can't find any experiment where diagonal without symmetric model outperformed the diagonal plus symmetric model. If there is no such experiment, the diagonal without symmetric model should be omitted from the paper or relegated to a side note as it adds virtually no value, as the vast majority of parameters are tied up in the diagonal matrix.\n\nIn the conclusion, you say that your approach is \"orthogonal\" to the convolutional parametrizations explored by He et. al. How is that? He et al. experiment with a low-rank structure (they call it bottleneck) between each skip connection. That seems very similar to what you do. In any case, if I use the low-rank layout from He et al., it would be reasonable to think that using your low-rank layout in addition would yield at most diminishing returns, hence the methods are not orthogonal. Also, you talk about the convolutional parametrization in Srivastava et al in the same sentence. As far as I can tell, that paper neither dealt with convlutional nets nor with low-rank factorizations.\n\nYou include a lot of very basic references in your papers, i.e. multiple references on \"neural networks were successful\", multiple on the vanishing gradient problem, etc. I think you don't need that many \"obvious\" references. (This is a very minor point.) On the other hand, I think you are missing some more important references that have looked at low-rank or related methods for neural nets, such as:\n\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc' Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156.\n2013.\n\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with\nlow rank expansions. arXiv preprint arXiv:1405.3866, 2014.\n\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure\nwithin convolutional networks for efficient evaluation. In Advances in Neural Information Processing\nSystems, pages 1269\u20131277. 2014.", "OTHER_KEYS": "George Philipp"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network.\n\nAuthor also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.\n\nAn empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). \n\nHowever, there are few problems with the evaluation:\n\n- In the highway network experiment, the author does not compare with a baseline.\nWe can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?.\n\n- It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.\nAlso the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.\n\n-Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size.\n\n- It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.\n\n- it is not clear when to use low-rank or low-rank + diagonal from the experiments.\n\nOverall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "my review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.\nThat said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016"}, {"TITLE": "Exploring a solid idea, but results are not convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks.\n\nThe paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)\n\nAside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "No question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Prereview question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Antonio Valerio Miceli Barone", "accepted": false, "id": "594"}