{"conference": "ICLR 2017 conference submission", "title": "PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING", "abstract": "We propose a learning method to quantify human intention. Generally, a human being will imagine several potential actions for a given scene, but only one of these actions will subsequently be taken. This makes it difficult to quantify human intentions. To solve this problem, we apply competitive learning to human behavior prediction as supervised learning. In our approach, competitive learning generates several outputs that are then associated with several potential situations imagined by a human. We applied the proposed method to human driving behavior and extracted three potential driving patterns. Results showed a squared error is reduced to 1/25 that of a conventional method . We also found that competitive learning can distinguish valid data from disturbance data in order to train a model.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.\nIt is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?\nThe competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Unclear whether the experimental comparison is fair", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThis paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.\n\n It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.\n\nIt seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? \n\nA large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver \u2018intentions\u2019. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly.\n\nMore generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. \n\nThe competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.\n\nThere is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach.\n\nThe graphical model of the driving process depicted in figure 1 seems nonsensical. If e is observed then all variables are known given the dependencies shown. Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time. It should be that s_t depends on d_(t-1). Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid.\n\nOdd to have a paragraph break in abstract\n\nFigure 1 caption should include a brief explanation of the variables shown\n", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.\nIt is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?\nThe competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016", "TITLE": "Details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.\nIt is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?\nThe competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Unclear whether the experimental comparison is fair", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThis paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.\n\n It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.\n\nIt seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? \n\nA large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver \u2018intentions\u2019. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly.\n\nMore generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. \n\nThe competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.\n\nThere is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach.\n\nThe graphical model of the driving process depicted in figure 1 seems nonsensical. If e is observed then all variables are known given the dependencies shown. Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time. It should be that s_t depends on d_(t-1). Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid.\n\nOdd to have a paragraph break in abstract\n\nFigure 1 caption should include a brief explanation of the variables shown\n", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.\nIt is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?\nThe competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016", "TITLE": "Details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Masayoshi Ishikawa, Mariko Okude, Takehisa Nishida & Kazuo Muto", "accepted": false, "id": "785"}