{"conference": "ICLR 2017 conference submission", "title": "Decomposing Motion and Content for Natural Video Sequence Prediction", "abstract": "We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the pro- posed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatio-temporal dynamics for pixel-level future prediction in natural videos.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Here is a summary of the reviews and discussion:\n \n Strengths\n Idea of decoupling motion and content is interesting in the context of future frame prediction (R3, R2, R1)\n Quantitative and Qualitative results are good (R1)\n \n Weaknesses\n Novelty in light of previous multi-stream networks (R3, R2)\n Not clear if this kind of decoupling works well or is of broader interest beyond future frame prediction (R3) AC comment: I don\u00d5t know if this is too serious a concern; to me the problem seems important enough -- making an advancement that improves a single relevant problem is a contribution\n Separation of motion and content already prevalent in other applications, e.g., pose estimation (R2)\n UCF-101 results are not so convincing (R3)\n Clarity could be improved, not written with general audience in mind (R2)\n Concern about static bias in the UCF dataset of the learned representations (R1, R2, R3) AC comment: The authors ran significant additional experiments to respond to this point\n \n The authors provided a comprehensive rebuttal which convinced R1 to upgrade their score. After engaging R2 and R3 in discussion, both said that the paper had improved in its subsequent revisions and would be happy to see the paper pass. The AC agrees with the opinion of the reviewers that the paper should be accepted as a poster.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We appreciate insightful and constructive comments by reviewers.\n\nWe have updated the results on UCF101 dataset. We found a better hyper-parameter setting for \\beta (previously it was 0.01, and now it is 0.001 for all of our networks and baselines). We also changed the qualitative results to videos with large motion as determined by the motion quantities measure suggested by Reviewer1. In addition, we added grids and plot high-amplitude optical flows to help visualize the motion and comparison. Finally, we have updated the videos in the project website for easier comparison of the generated videos; these videos show that our method also works quite well compared to the baseline methods for videos with large motions.\n\nPlease check our project website: ", "OTHER_KEYS": "Ruben Villegas"}, {"TITLE": "well-executed but limited novelty and impact", "RECOMMENDATION_UNOFFICIAL": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.\n\nThe idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.\n\nWhile results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset.  The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question.\n\nOverall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "comments": "The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.\n\nThe paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.\n\nDetails :\n\nThe paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.\n\nExample : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are \"additional\" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.\n\nEquation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.\n\nThe residual nature of the layer should be made more apparent in equation (3).\n\nThere are several typos, absence of articles and prepositions (\"of\" etc.). The paper should be reread carefully.\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016 (modified: 26 Jan 2017)", "TITLE": "Latest revision", "IS_META_REVIEW": false, "comments": "Latest revision to our Motion Content network paper contains additional evaluation on moving parts of the image determined by the optical flow magnitude between the previous and target groundtruth image.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Interesting architecture for an important problem, but requires additional experiments.", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "VGG", "comments": "You are training VGG size networks on quite small datasets, and you seem to use the same architecture for all datasets. I didn't find any information on pre-training. Didn't you have any issues with overfitting?", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "CLARITY": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Independence", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "CLARITY": 4}, {"TITLE": "static quality of MCNet on UFC-101", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 2, "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "04 Dec 2016"}, {"TITLE": "Pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 3, "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016"}, {"DATE": "13 Nov 2016", "TITLE": "Missing relevant reference and comparison?", "IS_META_REVIEW": false, "comments": "This work is similar to the paper published in ICLRw2016 ", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Here is a summary of the reviews and discussion:\n \n Strengths\n Idea of decoupling motion and content is interesting in the context of future frame prediction (R3, R2, R1)\n Quantitative and Qualitative results are good (R1)\n \n Weaknesses\n Novelty in light of previous multi-stream networks (R3, R2)\n Not clear if this kind of decoupling works well or is of broader interest beyond future frame prediction (R3) AC comment: I don\u00d5t know if this is too serious a concern; to me the problem seems important enough -- making an advancement that improves a single relevant problem is a contribution\n Separation of motion and content already prevalent in other applications, e.g., pose estimation (R2)\n UCF-101 results are not so convincing (R3)\n Clarity could be improved, not written with general audience in mind (R2)\n Concern about static bias in the UCF dataset of the learned representations (R1, R2, R3) AC comment: The authors ran significant additional experiments to respond to this point\n \n The authors provided a comprehensive rebuttal which convinced R1 to upgrade their score. After engaging R2 and R3 in discussion, both said that the paper had improved in its subsequent revisions and would be happy to see the paper pass. The AC agrees with the opinion of the reviewers that the paper should be accepted as a poster.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We appreciate insightful and constructive comments by reviewers.\n\nWe have updated the results on UCF101 dataset. We found a better hyper-parameter setting for \\beta (previously it was 0.01, and now it is 0.001 for all of our networks and baselines). We also changed the qualitative results to videos with large motion as determined by the motion quantities measure suggested by Reviewer1. In addition, we added grids and plot high-amplitude optical flows to help visualize the motion and comparison. Finally, we have updated the videos in the project website for easier comparison of the generated videos; these videos show that our method also works quite well compared to the baseline methods for videos with large motions.\n\nPlease check our project website: ", "OTHER_KEYS": "Ruben Villegas"}, {"TITLE": "well-executed but limited novelty and impact", "RECOMMENDATION_UNOFFICIAL": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.\n\nThe idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.\n\nWhile results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset.  The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question.\n\nOverall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "comments": "The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.\n\nThe paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.\n\nDetails :\n\nThe paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.\n\nExample : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are \"additional\" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.\n\nEquation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.\n\nThe residual nature of the layer should be made more apparent in equation (3).\n\nThere are several typos, absence of articles and prepositions (\"of\" etc.). The paper should be reread carefully.\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016 (modified: 26 Jan 2017)", "TITLE": "Latest revision", "IS_META_REVIEW": false, "comments": "Latest revision to our Motion Content network paper contains additional evaluation on moving parts of the image determined by the optical flow magnitude between the previous and target groundtruth image.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Interesting architecture for an important problem, but requires additional experiments.", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "VGG", "comments": "You are training VGG size networks on quite small datasets, and you seem to use the same architecture for all datasets. I didn't find any information on pre-training. Didn't you have any issues with overfitting?", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "CLARITY": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Independence", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "CLARITY": 4}, {"TITLE": "static quality of MCNet on UFC-101", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 2, "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "04 Dec 2016"}, {"TITLE": "Pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 3, "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016"}, {"DATE": "13 Nov 2016", "TITLE": "Missing relevant reference and comparison?", "IS_META_REVIEW": false, "comments": "This work is similar to the paper published in ICLRw2016 ", "OTHER_KEYS": "(anonymous)"}], "authors": "Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee", "accepted": true, "id": "357"}