{"conference": "ICLR 2017 conference submission", "title": "Adaptive Feature Abstraction for Translating Video to Language", "abstract": "Previous models for video captioning often use the output from a specific layer of a Convolutional Neural Network (CNN) as video representations, preventing them from modeling rich, varying context-dependent semantics in video descriptions. In this paper, we propose a new approach to generating adaptive spatiotemporal representations  of videos for a captioning task.  For this purpose, novel attention mechanisms with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature ``abstraction''), as well as local spatiotemporal regions of the feature maps at each layer. Our approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT.  Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Reviewers feel the work is well executed and that the model makes sense, but two of the reviewers were not convinced that the proposed method contains enough novelty in light of prior work. The comparison of the soft vs hard attention model variations is perhaps one of the more novel aspects of the work; however, the degree of novelty within these formulations and the insights obtained from their comparison were not perceived as being enough to warrant higher ratings. We would like to invite the authors to submit this paper to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Authors' final rebuttal", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for the critical comments.\n\nAll the reviewers agree that our experimental results convincingly support our hypothesis. The disagreement is that whether our paper is suitable for ICLR and whether our contribution is important to know by the representation learning community.\n\nWe made the following changes in the pdf file revision addressing the reviewers\u2019 concerns:\n\n1. We updated the abstract, emphasizing that the goal of our paper is not only about presenting another architecture for video captioning, but also about new video representations that are suitable for the video captioning task. Attention mechanisms are just technical means to achieve this goal.\n\n2. We updated the introduction, adding three technical challenges to overcome to use adaptive spatiotemporal feature representations with dynamic feature abstraction for the captioning task. We also explained why na\u00efve approaches such as MLP/max(average)-pooling did not meet our requirements.\n\n3. We updated the related work including the hypercolumn representation.\n\n4. We put additional experimental results comparing our approach ASTAR to hypercolumns, MLP, and max/average-pooling in Table 1 on Page 8. Our results clearly demonstrate the significance of dynamically selecting a specific level. The hypercolumn representation without level selection has much worse performance than our method.\n\n5. We updated Figure 1 and moved Figure 2 below it to emphasize our contributions following the review comments.\n\n6. We added the reference of Yao et al., ICCV 2015.\n\nIn summary, we believe that our contributions are clear and important to know by the representation learning community. This line of thinking might influence other researchers to perform additional research on classification and other tasks. It also inspires us to design new deep architectures to efficiently learn and effectively utilize different levels of feature representations in a dynamic fashion.", "OTHER_KEYS": "Martin Renqiang Min"}, {"TITLE": "state-of-the-art results but too incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder. Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT. While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN.\n\nThe work is well presented and the experiments clearly show the benefit of attention over multiple layers. However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR. Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "solid work but lack of novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Clear and efficient space+time+feature attention mechanisms for video captioning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "1) Summary\n\nThis paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels.\n\n2) Contributions\n\n+ Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions).\n+ Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms.\n+ Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case.\n\n3) Suggestions for improvement\n\nHypercolumns comparison:\nAs mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "related work and broader impact", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "22 Nov 2016 (modified: 05 Dec 2016)", "TITLE": "missing citations in related work", "IS_META_REVIEW": false, "comments": "You should also probably cite\n", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Reviewers feel the work is well executed and that the model makes sense, but two of the reviewers were not convinced that the proposed method contains enough novelty in light of prior work. The comparison of the soft vs hard attention model variations is perhaps one of the more novel aspects of the work; however, the degree of novelty within these formulations and the insights obtained from their comparison were not perceived as being enough to warrant higher ratings. We would like to invite the authors to submit this paper to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Authors' final rebuttal", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for the critical comments.\n\nAll the reviewers agree that our experimental results convincingly support our hypothesis. The disagreement is that whether our paper is suitable for ICLR and whether our contribution is important to know by the representation learning community.\n\nWe made the following changes in the pdf file revision addressing the reviewers\u2019 concerns:\n\n1. We updated the abstract, emphasizing that the goal of our paper is not only about presenting another architecture for video captioning, but also about new video representations that are suitable for the video captioning task. Attention mechanisms are just technical means to achieve this goal.\n\n2. We updated the introduction, adding three technical challenges to overcome to use adaptive spatiotemporal feature representations with dynamic feature abstraction for the captioning task. We also explained why na\u00efve approaches such as MLP/max(average)-pooling did not meet our requirements.\n\n3. We updated the related work including the hypercolumn representation.\n\n4. We put additional experimental results comparing our approach ASTAR to hypercolumns, MLP, and max/average-pooling in Table 1 on Page 8. Our results clearly demonstrate the significance of dynamically selecting a specific level. The hypercolumn representation without level selection has much worse performance than our method.\n\n5. We updated Figure 1 and moved Figure 2 below it to emphasize our contributions following the review comments.\n\n6. We added the reference of Yao et al., ICCV 2015.\n\nIn summary, we believe that our contributions are clear and important to know by the representation learning community. This line of thinking might influence other researchers to perform additional research on classification and other tasks. It also inspires us to design new deep architectures to efficiently learn and effectively utilize different levels of feature representations in a dynamic fashion.", "OTHER_KEYS": "Martin Renqiang Min"}, {"TITLE": "state-of-the-art results but too incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder. Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT. While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN.\n\nThe work is well presented and the experiments clearly show the benefit of attention over multiple layers. However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR. Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "solid work but lack of novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Clear and efficient space+time+feature attention mechanisms for video captioning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "1) Summary\n\nThis paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels.\n\n2) Contributions\n\n+ Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions).\n+ Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms.\n+ Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case.\n\n3) Suggestions for improvement\n\nHypercolumns comparison:\nAs mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "related work and broader impact", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "22 Nov 2016 (modified: 05 Dec 2016)", "TITLE": "missing citations in related work", "IS_META_REVIEW": false, "comments": "You should also probably cite\n", "OTHER_KEYS": "(anonymous)"}], "authors": "Yunchen Pu, Martin Renqiang Min, Zhe Gan, Lawrence Carin", "accepted": false, "id": "535"}