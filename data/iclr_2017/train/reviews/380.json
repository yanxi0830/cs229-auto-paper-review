{"conference": "ICLR 2017 conference submission", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviews clearly support acceptance of the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "A mathematically elegant extension of GANs to approximate density estimation", "comments": "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data.\n\nI would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature.\n\nAlso, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 \"Finally, let's whose discriminative power\". I'm not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "missing details on architecture and chosen energy function definition", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "16 Dec 2016", "CLARITY": 5}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting well written paper on improving the stability of discriminators in GANs.", "comments": "The authors present a method for changing the objective of generative adversarial networks such that the discriminator accurately recovers density information about the underlying data distribution. In the course of deriving the changed objective they prove that stability of the discriminator is not guaranteed in the standard GAN setup but can be recovered via an additional entropy regularization term.\n\nThe paper is clearly written, including the theoretical derivation. The derivation of the additional regularization term seems valid and is well explained. The experiments also empirically seem to support the claim that the proposed changed objective results in a \"better\" discriminator. There are only a few issues with the paper in its current form:\n- The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN. As a result it took me several passes through the paper to understand why the results don't hold for a standard GAN. I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below).\n- The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training. You also don't make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment. It might be worth-while mentioning this early in the paper.\n- The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets. I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance. For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data). Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories).\n- Main complaint: It is completely unclear what the generator and discriminators look like for the experiments. You mention that code will be available soon but I feel like a short description at least of the form of the energy used should also appear in the paper somewhere (perhaps in the appendix).\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"DATE": "14 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "Interesting papers. \nGood explanation using a fair amount of math.\n\nI have some questions. \n\n1. In the appendix, you compare your algorithm to f-GAN. However, It would be more natural that you compare it with GMMN because the formulation of maximum mean discrepancy is more similar to  (1).\n\n2. You claim that f-GAN including the usual GAN is too restrictive.  Would that statement be applied to GMMN? \nFrom my understanding, GAN is not restrictive because the discriminator is updated once or twice continuously and the obtained ratio is not optimal. This early stopping is a kind of regularization that prevents the discriminator from overfitting. In reality, GANs work using that stopping rule. However, GMMN would be restrictive because such a stopping rule cannot be used.\n\n\n[1] ", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?\n\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IS_META_REVIEW": true, "comments": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviews clearly support acceptance of the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "A mathematically elegant extension of GANs to approximate density estimation", "comments": "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data.\n\nI would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature.\n\nAlso, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 \"Finally, let's whose discriminative power\". I'm not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "missing details on architecture and chosen energy function definition", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "16 Dec 2016", "CLARITY": 5}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting well written paper on improving the stability of discriminators in GANs.", "comments": "The authors present a method for changing the objective of generative adversarial networks such that the discriminator accurately recovers density information about the underlying data distribution. In the course of deriving the changed objective they prove that stability of the discriminator is not guaranteed in the standard GAN setup but can be recovered via an additional entropy regularization term.\n\nThe paper is clearly written, including the theoretical derivation. The derivation of the additional regularization term seems valid and is well explained. The experiments also empirically seem to support the claim that the proposed changed objective results in a \"better\" discriminator. There are only a few issues with the paper in its current form:\n- The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN. As a result it took me several passes through the paper to understand why the results don't hold for a standard GAN. I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below).\n- The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training. You also don't make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment. It might be worth-while mentioning this early in the paper.\n- The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets. I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance. For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data). Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories).\n- Main complaint: It is completely unclear what the generator and discriminators look like for the experiments. You mention that code will be available soon but I feel like a short description at least of the form of the energy used should also appear in the paper somewhere (perhaps in the appendix).\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"DATE": "14 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "Interesting papers. \nGood explanation using a fair amount of math.\n\nI have some questions. \n\n1. In the appendix, you compare your algorithm to f-GAN. However, It would be more natural that you compare it with GMMN because the formulation of maximum mean discrepancy is more similar to  (1).\n\n2. You claim that f-GAN including the usual GAN is too restrictive.  Would that statement be applied to GMMN? \nFrom my understanding, GAN is not restrictive because the discriminator is updated once or twice continuously and the obtained ratio is not optimal. This early stopping is a kind of regularization that prevents the discriminator from overfitting. In reality, GANs work using that stopping rule. However, GMMN would be restrictive because such a stopping rule cannot be used.\n\n\n[1] ", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?\n\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}], "authors": "Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, Aaron Courville", "accepted": true, "id": "380"}