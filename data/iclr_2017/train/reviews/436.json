{"conference": "ICLR 2017 conference submission", "title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. \nOn the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. \n\nOverall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. \nThe intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper applies a previously introduced method (from ICLR '16) to the challenging question answering dataset (wikitables). The results are strong and quite close to the performance obtained by a semantic parser. There reviewers generally agree that this is an interesting and promising direction / results. The application of the neural programmer to this dataset required model modifications which are reasonable though quite straightforward, so, in that respect, the work is incremental. Still, achieving strong results on this moderately sized dataset with an expressive \n model is far from trivial. Though the approach, as has been discussed, does not directly generalize to QA with large knowledge bases (as well as other end-to-end differentiable methods for the QA task proposed so far), it is an important step forward and the task is already realistic and important.\n \n Pros\n \n + interesting direction\n + strong results on a interesting dataset\n \n Cons\n - incremental, the model is largely the same as in the previous paper", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "Questions regarding oracle and model design", "IS_META_REVIEW": false, "comments": "I have some questions about your paper:\n\n1. Why is the oracle performance only about 50%? Is it limited by the type of operations you provide? Also is the oracle also only run for 4 timesteps? (If so would increasing the number of timesteps improve the oracle?)\n\n2. In the paper that first introduces your neural programmer model (\"Neural Programmer: Inducing Latent Programs with Gradient Descent\"), you incorporate \"logic\" operations (\"or\" and \"and\"), why did you not include them as operations for this model?", "OTHER_KEYS": "Kelly Zhang"}, {"DATE": "24 Dec 2016", "TITLE": "Update from the authors", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for the constructive feedback. We performed more experiments and added significant new material to the paper. To summarize:\n1) We open-sourced the implementation of our model: ", "OTHER_KEYS": "Arvind Neelakantan"}, {"TITLE": "An interesting paper for a rather hard problem. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. \n\nI feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). \n\nI do have a few objections though. \n\n-- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the \"row selector\", the \"scalar answer\" and the \"lookup answer\" etc. \n\n-- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. \nOn the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. \n\nOverall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. \nThe intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A paper on a challenging task", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role.\n\nIt'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What's the headroom and oracle number with the current approach?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "05 Dec 2016", "TITLE": "Model questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "03 Dec 2016", "TITLE": "clarify the definition of the lookup answer loss", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. \nOn the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. \n\nOverall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. \nThe intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper applies a previously introduced method (from ICLR '16) to the challenging question answering dataset (wikitables). The results are strong and quite close to the performance obtained by a semantic parser. There reviewers generally agree that this is an interesting and promising direction / results. The application of the neural programmer to this dataset required model modifications which are reasonable though quite straightforward, so, in that respect, the work is incremental. Still, achieving strong results on this moderately sized dataset with an expressive \n model is far from trivial. Though the approach, as has been discussed, does not directly generalize to QA with large knowledge bases (as well as other end-to-end differentiable methods for the QA task proposed so far), it is an important step forward and the task is already realistic and important.\n \n Pros\n \n + interesting direction\n + strong results on a interesting dataset\n \n Cons\n - incremental, the model is largely the same as in the previous paper", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "Questions regarding oracle and model design", "IS_META_REVIEW": false, "comments": "I have some questions about your paper:\n\n1. Why is the oracle performance only about 50%? Is it limited by the type of operations you provide? Also is the oracle also only run for 4 timesteps? (If so would increasing the number of timesteps improve the oracle?)\n\n2. In the paper that first introduces your neural programmer model (\"Neural Programmer: Inducing Latent Programs with Gradient Descent\"), you incorporate \"logic\" operations (\"or\" and \"and\"), why did you not include them as operations for this model?", "OTHER_KEYS": "Kelly Zhang"}, {"DATE": "24 Dec 2016", "TITLE": "Update from the authors", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for the constructive feedback. We performed more experiments and added significant new material to the paper. To summarize:\n1) We open-sourced the implementation of our model: ", "OTHER_KEYS": "Arvind Neelakantan"}, {"TITLE": "An interesting paper for a rather hard problem. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. \n\nI feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). \n\nI do have a few objections though. \n\n-- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the \"row selector\", the \"scalar answer\" and the \"lookup answer\" etc. \n\n-- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. \nOn the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. \n\nOverall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. \nThe intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A paper on a challenging task", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role.\n\nIt'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What's the headroom and oracle number with the current approach?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "05 Dec 2016", "TITLE": "Model questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "03 Dec 2016", "TITLE": "clarify the definition of the lookup answer loss", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Arvind Neelakantan, Quoc V. Le, Martin Abadi, Andrew McCallum, Dario Amodei", "accepted": true, "id": "436"}