{"conference": "ICLR 2017 conference submission", "title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers unanimously recommended accepting the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Jan 2017", "TITLE": "conv parameters", "IS_META_REVIEW": false, "comments": "Did the author try the method on conv layers? How is the result? On resnet-152, fc layers have only 4% parameters. Thank you very much!\nComment: a related work of learning structurally sparse DNN: ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "28 Jan 2017", "TITLE": "timit", "IS_META_REVIEW": false, "comments": "I didn't read the paper in detail, but for the TIMIT results the authors reported 23.x PER, this is actually quite poor and quite off from SOTA. SOTA is around 16.5 PER and even end-to-end methods (w/o HMMs) can achieve ~17.6% (see Alex Graves CTC/RNN transducer paper and Jan Chorowski's Attention paper).", "OTHER_KEYS": "(anonymous)"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Very good paper. Extremely easy to read and understand. Exciting ideas. Very good results. a few typos.", "comments": "The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.\n\nThis density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.\n\nRegularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.\n\nAs weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.\n\nThe training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.\n\nThe experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.\n\n\nThe paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.\n\nThe result tables are a bit confusing unfortunately.\n\nminor issues:\n\np1\nenglish mistake: \u201cwhile networks *that* consist of convolutional layers\u201d.\n\np6-p7\nTable 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:\n In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.\nI assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "22 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.\n\nThe approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?\n\nI think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. \n\nA quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?\n\nPreliminary rating:\nI think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.\n\nMinor notes:\nPlease resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.", "IS_ANNOTATED": true, "TITLE": "Good results", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Lot of Aspects to this Algorithm", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016 (modified: 16 Dec 2016)"}, {"TITLE": "Compression and Regularisation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "TITLE": "Better results than state of the art", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers unanimously recommended accepting the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Jan 2017", "TITLE": "conv parameters", "IS_META_REVIEW": false, "comments": "Did the author try the method on conv layers? How is the result? On resnet-152, fc layers have only 4% parameters. Thank you very much!\nComment: a related work of learning structurally sparse DNN: ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "28 Jan 2017", "TITLE": "timit", "IS_META_REVIEW": false, "comments": "I didn't read the paper in detail, but for the TIMIT results the authors reported 23.x PER, this is actually quite poor and quite off from SOTA. SOTA is around 16.5 PER and even end-to-end methods (w/o HMMs) can achieve ~17.6% (see Alex Graves CTC/RNN transducer paper and Jan Chorowski's Attention paper).", "OTHER_KEYS": "(anonymous)"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Very good paper. Extremely easy to read and understand. Exciting ideas. Very good results. a few typos.", "comments": "The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.\n\nThis density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.\n\nRegularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.\n\nAs weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.\n\nThe training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.\n\nThe experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.\n\n\nThe paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.\n\nThe result tables are a bit confusing unfortunately.\n\nminor issues:\n\np1\nenglish mistake: \u201cwhile networks *that* consist of convolutional layers\u201d.\n\np6-p7\nTable 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:\n In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.\nI assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "22 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.\n\nThe approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?\n\nI think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. \n\nA quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?\n\nPreliminary rating:\nI think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.\n\nMinor notes:\nPlease resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.", "IS_ANNOTATED": true, "TITLE": "Good results", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Lot of Aspects to this Algorithm", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016 (modified: 16 Dec 2016)"}, {"TITLE": "Compression and Regularisation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "TITLE": "Better results than state of the art", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}], "authors": "Shengjie Wang, Haoran Cai, Jeff Bilmes, William Noble", "accepted": true, "id": "365"}