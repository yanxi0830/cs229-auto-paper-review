{"conference": "ICLR 2017 conference submission", "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning", "abstract": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where twp agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of ``analogy making,'' or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "pros:\n - tackles a fundamental problem of interest to many\n - novel approach\n \n cons:\n - originally not evaluated against some reasonable benchmarks. Note: now added or addressed\n - little theoretical development cf MDP theory\n - some remaining questions about the necessity (and ability) to find good time alignments\n \n I personally found the ideas to be quite compelling, and believe that this is likely to inspire future work.\n The experiments represent interesting scenarios for transfer, with the caveat that they are just in simulation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Note to AC and Reviewers", "IS_META_REVIEW": false, "comments": "Dear Reviewers and Area Chair,\nThe reviewers for the paper provided a positive evaluation, with suggestions for additional experiments providing comparisons with several prior methods, some clarifications, and brought up the important problem of addressing time-based correspondence alignment. We have edited the paper to include additional experiments that address these issues, and therefore believe that the reviewer concerns about the work have been addressed. We discuss the specifics below.\n\nAnonReviewer 1 Comments: \n\u201cCompared to previous work (Ammar et al. 2015)\u201d\n- Performed this comparison and added the results into the Figures 5 and 8. We found that our method performed significantly better than this prior work. \n\u201cIs it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\u201d\n- We introduced an EM style algorithm which removes this assumption. Description of this method are in Section 3.3.2, and results in Figures 5 and 8. \n\u201cmore comparison to other transfer methods, including those listed in Sec.2, would be very valuable\u201d\n- We have implemented several more baselines: KCCA, Unsupervised Manifold Alignment as suggested by Bou Ammar et al, Direct mappings, random projections, CCA in Figures 5 and 8.\n\nAnonReviewer3 Comments: \n\u201cpreferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA\u201d, \u201ccomparisons to related approaches is not very up to date\u201d \n-Performed comparisons to Kernel CCA and Manifold Alignment using Unsupervised Manifold Alignment (Wang and Mahadevan, Bou Ammar et al) and added to our experiments in Figures 5 and 8. \n\nAnonReviewer4 Comments: \n\u201cA limitation of the paper is that the authors suppose that time alignment is trivial [...] could be dealt with through subsampling, dynamic time warping, or learning a matching function\u201d \n-We introduce a new EM style algorithm alternating between feature learning and dynamic time warping. Description is in Section 3.3.2 and results in Figures 5 and 8. \n\u201cAnother baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\"\u201d\n-We added this comparison in Figures 5 and 8.\n\u201cat least a much bigger sample budget should be tested [...] control for the fact that the embeddings were trained with more iterations in the case of doing transfer\u201d \n- We ran the baseline with a significantly higher sample budget as shown in Table 1. The poor performance is likely due to not enough guided exploration happening without good reward shaping for the baseline. \n\u201cproblem of learning invariant feature spaces is also linked to metric learning [...] no parallel is drawn with Multi-Task learning in ML \u201d\n-The additional references suggested have been added in the Related Work (Section 2).\n", "OTHER_KEYS": "Coline Manon Devin"}, {"DATE": "17 Jan 2017", "TITLE": "Note to reviewers", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have addressed the concerns presented in the reviews, including comparisons to several prior methods and a method for automatically determining alignment, as detailed in the individual reviewer responses. If you have suggestions for additional comparisons, we would be happy to add them in the final version. As such, we hope you will consider adjusting your reviews accordingly. \n\nRegards\nAbhishek, Coline, Yuxuan, Pieter, Sergey", "OTHER_KEYS": "Coline Manon Devin"}, {"DATE": "10 Jan 2017", "TITLE": "Summary of Updates (Jan 10)", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their comments. Most comments asked for additional comparisons and learned state correspondences. \nIn response to reviewer comments, we have added the following to our paper:\n\n1. Learning state correspondences by using an alternating optimization to jointly assign the state pairs in P and learn the embedding functions f and g. This significantly relaxes the assumption that states can be paired by time-step and provides better performance in experiment 1. See section 3.3.2 for a description of this method and figures 5 and 7a for results.\n\n2. All of the comparisons suggested by the reviewers: kernel-CCA, unsupervised manifold alignment (Ammar et al. 2015), and random projections. Results are in Figures 5 and 7a.\n\n3. Connections to metric learning and multitask learning in Section 2.\n\nThe comparisons have been run on experiments 1 and 2 and will be added to experiment 3 for the final version.", "OTHER_KEYS": "Coline Manon Devin"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.\n\nOne limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).\n\nGeneral remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\" (the embedding functions on the two domains), to check that the bad performance of the \"no transfer\" version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal \\alpha.\n\nThe experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no \"CCA\" nor \"direct mapping\" results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?\n\nOverall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 3, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Transfer learning in RL using a nonlinear CCA like approach ", "comments": "\nThis paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent. The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA. \n\n\nThe paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links. For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA. A three layer neural net is used to construct the latent feature spaces. \n\nThe problem of transfer in RL is extremely important, and receives less attention than it should. This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent. This is a promising approach. However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic. There is little by way of theoretical development of the ideas using MDP theory. \n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "comments": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated.", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "Summary of Author Responses", "IS_META_REVIEW": false, "comments": "To address the reviewers\u2019 questions, we have made the following changes:\n-We added comparisons of our method to using canonical correlation analysis to find the embedding functions F and G (see Figures 5 and 8a)\n-We added comparisons of our method to directly predicting the target states from the source states (see Figures 5, 8a)\n-We have clarified the description of P in Sec 4.1\n-We have added two previous approaches to the related work section.\n", "OTHER_KEYS": "Coline Manon Devin"}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Questions", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"IMPACT": 3, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Comparison to other work on feature space construction ", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "pros:\n - tackles a fundamental problem of interest to many\n - novel approach\n \n cons:\n - originally not evaluated against some reasonable benchmarks. Note: now added or addressed\n - little theoretical development cf MDP theory\n - some remaining questions about the necessity (and ability) to find good time alignments\n \n I personally found the ideas to be quite compelling, and believe that this is likely to inspire future work.\n The experiments represent interesting scenarios for transfer, with the caveat that they are just in simulation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Note to AC and Reviewers", "IS_META_REVIEW": false, "comments": "Dear Reviewers and Area Chair,\nThe reviewers for the paper provided a positive evaluation, with suggestions for additional experiments providing comparisons with several prior methods, some clarifications, and brought up the important problem of addressing time-based correspondence alignment. We have edited the paper to include additional experiments that address these issues, and therefore believe that the reviewer concerns about the work have been addressed. We discuss the specifics below.\n\nAnonReviewer 1 Comments: \n\u201cCompared to previous work (Ammar et al. 2015)\u201d\n- Performed this comparison and added the results into the Figures 5 and 8. We found that our method performed significantly better than this prior work. \n\u201cIs it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\u201d\n- We introduced an EM style algorithm which removes this assumption. Description of this method are in Section 3.3.2, and results in Figures 5 and 8. \n\u201cmore comparison to other transfer methods, including those listed in Sec.2, would be very valuable\u201d\n- We have implemented several more baselines: KCCA, Unsupervised Manifold Alignment as suggested by Bou Ammar et al, Direct mappings, random projections, CCA in Figures 5 and 8.\n\nAnonReviewer3 Comments: \n\u201cpreferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA\u201d, \u201ccomparisons to related approaches is not very up to date\u201d \n-Performed comparisons to Kernel CCA and Manifold Alignment using Unsupervised Manifold Alignment (Wang and Mahadevan, Bou Ammar et al) and added to our experiments in Figures 5 and 8. \n\nAnonReviewer4 Comments: \n\u201cA limitation of the paper is that the authors suppose that time alignment is trivial [...] could be dealt with through subsampling, dynamic time warping, or learning a matching function\u201d \n-We introduce a new EM style algorithm alternating between feature learning and dynamic time warping. Description is in Section 3.3.2 and results in Figures 5 and 8. \n\u201cAnother baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\"\u201d\n-We added this comparison in Figures 5 and 8.\n\u201cat least a much bigger sample budget should be tested [...] control for the fact that the embeddings were trained with more iterations in the case of doing transfer\u201d \n- We ran the baseline with a significantly higher sample budget as shown in Table 1. The poor performance is likely due to not enough guided exploration happening without good reward shaping for the baseline. \n\u201cproblem of learning invariant feature spaces is also linked to metric learning [...] no parallel is drawn with Multi-Task learning in ML \u201d\n-The additional references suggested have been added in the Related Work (Section 2).\n", "OTHER_KEYS": "Coline Manon Devin"}, {"DATE": "17 Jan 2017", "TITLE": "Note to reviewers", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have addressed the concerns presented in the reviews, including comparisons to several prior methods and a method for automatically determining alignment, as detailed in the individual reviewer responses. If you have suggestions for additional comparisons, we would be happy to add them in the final version. As such, we hope you will consider adjusting your reviews accordingly. \n\nRegards\nAbhishek, Coline, Yuxuan, Pieter, Sergey", "OTHER_KEYS": "Coline Manon Devin"}, {"DATE": "10 Jan 2017", "TITLE": "Summary of Updates (Jan 10)", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their comments. Most comments asked for additional comparisons and learned state correspondences. \nIn response to reviewer comments, we have added the following to our paper:\n\n1. Learning state correspondences by using an alternating optimization to jointly assign the state pairs in P and learn the embedding functions f and g. This significantly relaxes the assumption that states can be paired by time-step and provides better performance in experiment 1. See section 3.3.2 for a description of this method and figures 5 and 7a for results.\n\n2. All of the comparisons suggested by the reviewers: kernel-CCA, unsupervised manifold alignment (Ammar et al. 2015), and random projections. Results are in Figures 5 and 7a.\n\n3. Connections to metric learning and multitask learning in Section 2.\n\nThe comparisons have been run on experiments 1 and 2 and will be added to experiment 3 for the final version.", "OTHER_KEYS": "Coline Manon Devin"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.\n\nOne limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).\n\nGeneral remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\" (the embedding functions on the two domains), to check that the bad performance of the \"no transfer\" version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal \\alpha.\n\nThe experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no \"CCA\" nor \"direct mapping\" results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?\n\nOverall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 3, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Transfer learning in RL using a nonlinear CCA like approach ", "comments": "\nThis paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent. The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA. \n\n\nThe paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links. For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA. A three layer neural net is used to construct the latent feature spaces. \n\nThe problem of transfer in RL is extremely important, and receives less attention than it should. This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent. This is a promising approach. However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic. There is little by way of theoretical development of the ideas using MDP theory. \n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "comments": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated.", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "Summary of Author Responses", "IS_META_REVIEW": false, "comments": "To address the reviewers\u2019 questions, we have made the following changes:\n-We added comparisons of our method to using canonical correlation analysis to find the embedding functions F and G (see Figures 5 and 8a)\n-We added comparisons of our method to directly predicting the target states from the source states (see Figures 5, 8a)\n-We have clarified the description of P in Sec 4.1\n-We have added two previous approaches to the related work section.\n", "OTHER_KEYS": "Coline Manon Devin"}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Questions", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"IMPACT": 3, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Comparison to other work on feature space construction ", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 5}], "authors": "Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, Sergey Levine", "accepted": true, "id": "331"}