{"conference": "ICLR 2017 conference submission", "title": "Lie-Access Neural Turing Machines", "abstract": "External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "*** Paper Summary ***\n\nThis paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.\n\n*** Review Summary ***\n\nThis paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. \n\n*** Detailed Review ***\n\nThe paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a Lie-(group) access neural turing machine (LANTM) architecture, and demonstrates it's utility on several problems. \n \n Pros:\n Reviewers agree that this is an interesting and clearly-presented idea.\n Overall, the paper is clearly written and presents original ideas.\n It is likely to inspire further work into more effective generalizations of NTMs.\n \n Cons:\n The true impact and capabilities of these architectures are not yet clear, although it is argued that the same can be said for NTMs. \n \n The paper has been revised to address some NTM features (sharpening) that were not included in the original version.\n The purpose and precise definition of the invNorm have also been fixed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Sharpening results.", "IS_META_REVIEW": false, "comments": "We have finished results adding sharpening to the RAM/tape model. Weight sharpening only confers small advantage over vanilla on the copy, bigram flip, and double tasks, but deteriorates performance on all other tasks. These results will appear in the final version of the paper.", "OTHER_KEYS": "Greg Yang"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. \nThis allows to place memory elements as points on a manifold, while still allowing training with backpropagation.\nIt's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes.\n\nPros:\n- novel and interesting idea for memory access\n- nicely written\n \nCons:\n- need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory)                                 \n- not clear if this really works better than standard NTM (compared only to simplified version)\n- not clear if this is useful in practice (no comparison on real tasks)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "mathematically elegant, limited impact", "comments": "The Neural Turing Machine and related \u201cexternal memory models\u201d have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.\n\nThe NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which \u201csoftly\u201d shifts the head, allowing the machine to read and write sequences. Since this soft shift typically \u201csmears\u201d the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.\n\nThe premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.\n\nThis is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.\n\nOverall, the paper is well communicated and a novel idea.\n\nThe primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.\n\nThe baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.\n\nMinor issues:\nFootnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.\nFigures on page 8 are difficult to follow.\n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "APPROPRIATENESS": 2, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting new ", "comments": "The paper proposes a new memory access scheme based on Lie group actions for NTMs.\n\nPros:\n* Well written\n* Novel addressing scheme as an extension to NTM.\n* Seems to work slightly better than normal NTMs.\n* Some interesting theory about the novel addressing scheme based on Lie groups.\n\nCons:\n* In the results, the LANTM only seems to be slightly better than the normal NTM.\n* The result tables are a bit confusing.\n* No source code available.\n* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.\n* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.\n* No tests on real-world tasks, only some toy tasks.\n* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "*** Paper Summary ***\n\nThis paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.\n\n*** Review Summary ***\n\nThis paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. \n\n*** Detailed Review ***\n\nThe paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "difference to NTM or DNTM, details", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Figure 2 difficult to make sense of", "comments": "I struggle to understand figure 2, despite the length of the caption. Perhaps labelling the images themselves a bit more clearly.", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "APPROPRIATENESS": 2}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Motivation for this InvNorm", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "APPROPRIATENESS": 2}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "*** Paper Summary ***\n\nThis paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.\n\n*** Review Summary ***\n\nThis paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. \n\n*** Detailed Review ***\n\nThe paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a Lie-(group) access neural turing machine (LANTM) architecture, and demonstrates it's utility on several problems. \n \n Pros:\n Reviewers agree that this is an interesting and clearly-presented idea.\n Overall, the paper is clearly written and presents original ideas.\n It is likely to inspire further work into more effective generalizations of NTMs.\n \n Cons:\n The true impact and capabilities of these architectures are not yet clear, although it is argued that the same can be said for NTMs. \n \n The paper has been revised to address some NTM features (sharpening) that were not included in the original version.\n The purpose and precise definition of the invNorm have also been fixed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Sharpening results.", "IS_META_REVIEW": false, "comments": "We have finished results adding sharpening to the RAM/tape model. Weight sharpening only confers small advantage over vanilla on the copy, bigram flip, and double tasks, but deteriorates performance on all other tasks. These results will appear in the final version of the paper.", "OTHER_KEYS": "Greg Yang"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. \nThis allows to place memory elements as points on a manifold, while still allowing training with backpropagation.\nIt's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes.\n\nPros:\n- novel and interesting idea for memory access\n- nicely written\n \nCons:\n- need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory)                                 \n- not clear if this really works better than standard NTM (compared only to simplified version)\n- not clear if this is useful in practice (no comparison on real tasks)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "mathematically elegant, limited impact", "comments": "The Neural Turing Machine and related \u201cexternal memory models\u201d have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.\n\nThe NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which \u201csoftly\u201d shifts the head, allowing the machine to read and write sequences. Since this soft shift typically \u201csmears\u201d the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.\n\nThe premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.\n\nThis is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.\n\nOverall, the paper is well communicated and a novel idea.\n\nThe primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.\n\nThe baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.\n\nMinor issues:\nFootnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.\nFigures on page 8 are difficult to follow.\n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "APPROPRIATENESS": 2, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting new ", "comments": "The paper proposes a new memory access scheme based on Lie group actions for NTMs.\n\nPros:\n* Well written\n* Novel addressing scheme as an extension to NTM.\n* Seems to work slightly better than normal NTMs.\n* Some interesting theory about the novel addressing scheme based on Lie groups.\n\nCons:\n* In the results, the LANTM only seems to be slightly better than the normal NTM.\n* The result tables are a bit confusing.\n* No source code available.\n* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.\n* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.\n* No tests on real-world tasks, only some toy tasks.\n* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "*** Paper Summary ***\n\nThis paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.\n\n*** Review Summary ***\n\nThis paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. \n\n*** Detailed Review ***\n\nThe paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "difference to NTM or DNTM, details", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Figure 2 difficult to make sense of", "comments": "I struggle to understand figure 2, despite the length of the caption. Perhaps labelling the images themselves a bit more clearly.", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "APPROPRIATENESS": 2}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Motivation for this InvNorm", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "APPROPRIATENESS": 2}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}], "authors": "Greg Yang, Alexander Rush", "accepted": true, "id": "403"}