{"conference": "ICLR 2017 conference submission", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a simple but effective approach for pruning ConvNet filters with extensive evaluation using several architectures on ImageNet and CIFAR-10.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "21 Jan 2017", "TITLE": "Pruning criterion", "IS_META_REVIEW": false, "comments": "Just some feedback on your idea:\n\nThe criteria for determining a filter that is not important was the focus of my ICLR16 paper: RandomOut: Using a convolutional gradient norm to win The Filter Lottery ", "OTHER_KEYS": "Joseph Paul Cohen"}, {"DATE": "12 Jan 2017", "TITLE": "Thanks", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their comments and suggestions. We have added new experiments based on reviewer suggestions (and will continue to add more as necessary). The new experiments from Hao in Appendix A report fprop times for the pruned networks and random pruning results that address Reviewer 4's question about random pruning and Reviewer 2's concerns.", "OTHER_KEYS": "Asim Kadav"}, {"TITLE": "Good idea, well thought through and decently tested", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The idea of \"pruning where it matters\" is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too.\n\nExtra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (\\mathbb symbol - two thumbs up!).\n\nThe experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case?\n\nMy question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the \"min sum of weights\" criterion compares to other approaches.\nHow does it compare to other pruning criteria? Is it better than \"pruning at random\"?\n\nOverall, I liked the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "03 Jan 2017 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Pruning Filters for Efficient ConvNets", "comments": "This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines. \nThe results are a 10% improvements in ResNet-like and ImageNet, which may be also achieved with better design of networks. New networks should have been also compared, but this we know it is time-consuming.\nA good paper with some useful results.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.\n\nPros:\n- Impressively retains accuracy on popular models on ImageNet and Cifar10\n\nCons:\n- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.\n- There is no direct comparison to the multitude of other pruning and speedup methods.\n- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.\n\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Simple idea with good experiments; transfer learning results would improve it", "comments": "This paper proposes a very simple idea (prune low-weight filters from ConvNets) in order to reduce FLOPs and memory consumption. The proposed method is experimented on with VGG-16 and ResNets on CIFAR10 and ImageNet.\n\nPros:\n- Creates *structured* sparsity, which automatically improves performance without changing the underlying convolution implementation\n- Very simple to implement\n\nCons:\n- No evaluation of how pruning impacts transfer learning\n\nI'm generally positive about this work. While the main idea is almost trivial, I am not aware of any other papers that propose exactly the same idea and show a good set of experimental results. Therefore I'm inclined to accept it. The only major downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much interest in the tasks of CIFAR10 or even ImageNet. Instead, the main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect filter pruning (or any other kind of pruning) to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nNit: Fig 2 title says VGG-16 in (b) and VGG_BN in (c). Are these the same models?", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a simple but effective approach for pruning ConvNet filters with extensive evaluation using several architectures on ImageNet and CIFAR-10.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "21 Jan 2017", "TITLE": "Pruning criterion", "IS_META_REVIEW": false, "comments": "Just some feedback on your idea:\n\nThe criteria for determining a filter that is not important was the focus of my ICLR16 paper: RandomOut: Using a convolutional gradient norm to win The Filter Lottery ", "OTHER_KEYS": "Joseph Paul Cohen"}, {"DATE": "12 Jan 2017", "TITLE": "Thanks", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their comments and suggestions. We have added new experiments based on reviewer suggestions (and will continue to add more as necessary). The new experiments from Hao in Appendix A report fprop times for the pruned networks and random pruning results that address Reviewer 4's question about random pruning and Reviewer 2's concerns.", "OTHER_KEYS": "Asim Kadav"}, {"TITLE": "Good idea, well thought through and decently tested", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The idea of \"pruning where it matters\" is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too.\n\nExtra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (\\mathbb symbol - two thumbs up!).\n\nThe experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case?\n\nMy question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the \"min sum of weights\" criterion compares to other approaches.\nHow does it compare to other pruning criteria? Is it better than \"pruning at random\"?\n\nOverall, I liked the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "03 Jan 2017 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Pruning Filters for Efficient ConvNets", "comments": "This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines. \nThe results are a 10% improvements in ResNet-like and ImageNet, which may be also achieved with better design of networks. New networks should have been also compared, but this we know it is time-consuming.\nA good paper with some useful results.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.\n\nPros:\n- Impressively retains accuracy on popular models on ImageNet and Cifar10\n\nCons:\n- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.\n- There is no direct comparison to the multitude of other pruning and speedup methods.\n- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.\n\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Simple idea with good experiments; transfer learning results would improve it", "comments": "This paper proposes a very simple idea (prune low-weight filters from ConvNets) in order to reduce FLOPs and memory consumption. The proposed method is experimented on with VGG-16 and ResNets on CIFAR10 and ImageNet.\n\nPros:\n- Creates *structured* sparsity, which automatically improves performance without changing the underlying convolution implementation\n- Very simple to implement\n\nCons:\n- No evaluation of how pruning impacts transfer learning\n\nI'm generally positive about this work. While the main idea is almost trivial, I am not aware of any other papers that propose exactly the same idea and show a good set of experimental results. Therefore I'm inclined to accept it. The only major downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much interest in the tasks of CIFAR10 or even ImageNet. Instead, the main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect filter pruning (or any other kind of pruning) to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nNit: Fig 2 title says VGG-16 in (b) and VGG_BN in (c). Are these the same models?", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}], "authors": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf", "accepted": true, "id": "324"}