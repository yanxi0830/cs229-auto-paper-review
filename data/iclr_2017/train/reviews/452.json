{"conference": "ICLR 2017 conference submission", "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning", "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate  by enabling temporal abstractions in the action space and implicitly enabling planning through sequences of repetitive macro-actions.   We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The basic idea of this paper is simple: run RL over an action space that models both the actions and the number of times they are repeated. It's a simple idea, but seems to work really well on a pretty substantial variety of domains, and it can be easily adapted to many different settings. In several settings, the improvement using this approach are dramatic. I think this is an obvious accept: a simple addition to existing RL algorithms that can often perform much better.\n \n Pros:\n + Simple and intuitive approach, easy to implement\n + Extensive evaluation, showing very good performance\n \n Cons:\n - Sometimes unclear _why_ certain domains benefit so much from this", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Revision in response to reviewer comments and questions", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for asking interesting questions and pointing out important flaws in the paper. We have uploaded a revised version of the paper that we believe addresses the questions raised. Major features of the revision are:\n\n1. We have added results on 2 more Atari 2600 games: Enduro and Q-bert. FiGAR seems to improve performance rather dramatically on Enduro with the FiGAR agent being close to 100 times better than the baseline A3C agent. (Note that the baseline agent performs very poorly according to the published results as well)\n\n2. In response to AnonReviewer3\u2019s comment about skipping intermediate frames, we have added Appendix F (page 23) by conducting experiments on what happens when FiGAR does not discard any intermediate frames (during evaluation phase). The general pattern seems to be that for games wherein lower action repetition is preferred, gains are made in terms of improved gameplay performance. However, for 24 out of 33 games the performance becomes worse, which depicts the importance of the temporal abstractions learnt by the action repetition part of the policy (\\pi_{\\theta_{x}}). This does not address the reviewer\u2019s question completely since at train time we still skip all the frames, as suggested by the action repetition policy. We have added a small discussion on future works section (section 6, page 10) which could potentially address this comment.\n\n3. In response to AnonReviewer3\u2019s suggestion to turn table1 into a bar graph we have done so (Figure 3, page 8) and it indeed does look much better.\n\n4. In response to AnonReviewer3\u2019s suggestion to compare directly to STRAW we have added Table 5 (Appendix A, page 14) which contains performance of STRAW models on all games which we have also experimented with. The general conclusion seems to be that in some games STRAW does better and in some games FiGAR does better.\n\n5. In response to AnonReviewer4\u2019s comment, we conducted experiments on shared representations for the FiGAR-TRPO agent. Appendix G (page 24) contains the results of the experiments. In general we observe that FiGAR-TRPO with shared representations does marginally better than FiGAR-TRPO, but not much better. The performance goes down on some tasks and improves on others. The average action repetition rate of the best policies learnt improves.\n\n6. In response to AnonReviewer4\u2019s comment on SMDPs we have added the relevant discussion to related works section (page 3).\n\n7. In response to AnonReviewer2\u2019s comment on the confusing nature of FiGAR-DDPG section, we have rewritten the section. It is hopefully clearer now.\n \n8. In response to AnonReviewer2\u2019s comment on the confusing notation \u2018r\u2019 for action repetition we have completely changed the notation for action repetition to the letter \u2018w\u2019.\n\n9. In response to AnonReviewer2\u2019s comment on  the potential weakness of the FiGAR framework, we have added a discussion on the shortcomings of the FiGAR in section 6 (page 10).\n\n10.We have corrected several typos as pointed out by the reviewers. \n", "OTHER_KEYS": "Sahil Sharma"}, {"TITLE": "Simple but effective idea with a very thorough evaluation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.\n\nA few comments/questions:\n- Table 1 could be easier to interpret as a figure of histograms.\n- Figure 3 could be easier to interpret as a table.\n- How was the subset of Atari games selected?\n- The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores.\n- It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.\n- FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.\n\nOverall, this is a nice simple addition to deep RL algorithms that many people will probably start using.\n\n--------------------\n\nI'm increasing my score to 8 based on the rebuttal and the revised paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "18 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).\n\nHere are some comments and questions, for improving the paper:\n\nThe introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.\n\nIn the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)\n\nExperiments:\nCan you provide error bars on the experimental results? (from running multiple random seeds)\n\nIt would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.\n\nThe TRPO evaluation is different from the results reported in Duan et al. ICML \u201916. Why not use the same benchmark?\n\nVideos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?\n\nHow many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).\n\nCan the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.\n\nMinor comments:\n-- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.\n-- \u201cidea of deciding when necessary\u201d - seems like it would be better to say \u201cidea of only deciding when necessary\"\n-- \"spaces.Durugkar et al.\u201d \u2014 missing a space.\n-- \u201cR={4}\u201d \u2014 why 4? Could you use a letter to indicate a constant instead? (or a different notation)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016 (modified: 16 Dec 2016)", "TITLE": "some questions and comments", "IS_META_REVIEW": false, "comments": "Hi, the main idea is quite interesting. I was curious about the following. My primary question is Q1, and others are predominantly comments.\n\nQ1: After learning is complete, did you try forward propagating through the network to find actions for every time-step as opposed to repeating actions? Concretely, if at t=5, action suggested by the network is a_3 with a repetition of 4, instead of sticking with a_3 for times t={5,6,7,8} perform action a_3 for just t=5, and forward prop through the policy again at t=6.\n\nI understand that the goal is to explore temporal abstractions, but for all the problems considered in this paper, a forward prop is not expensive at all. Hence, there is no computational bottle neck forcing action repetition during test-time. It is understandable that repeating actions speeds up training. However, at test time, the performance can potentially improve by not repeating. This idea is quite popular in variants of Receding Horizon Control and MCTS.\n\n2: Can you share hyper-parameter settings of section 5.2? How many iterations of TRPO was run, and how many trajectory samples per iteration? The performance on Ant-v1 task is too low for both TRPO and FIGAR. Running for more iterations and initializing the network better (smaller weights) might improve performance significantly for both. It might be informative to share the learning curves comparing FIGAR with TRPO. With the current results, it is a stretch to say that FIGAR \"outperforms\" TRPO.\n\n3: Considering that the action repetition is 1 for a majority of MuJoCo tasks (discounting Ant) and TORCS, why do you expect FIGAR to perform better? Also, the FIGAR policies seem to have more parameters than baselines they are compared against -- is this true? Have you compared to baselines with equal number of parameters?", "OTHER_KEYS": "Aravind Rajeswaran"}, {"DATE": "03 Dec 2016", "TITLE": "A few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "Intermediate frames", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The basic idea of this paper is simple: run RL over an action space that models both the actions and the number of times they are repeated. It's a simple idea, but seems to work really well on a pretty substantial variety of domains, and it can be easily adapted to many different settings. In several settings, the improvement using this approach are dramatic. I think this is an obvious accept: a simple addition to existing RL algorithms that can often perform much better.\n \n Pros:\n + Simple and intuitive approach, easy to implement\n + Extensive evaluation, showing very good performance\n \n Cons:\n - Sometimes unclear _why_ certain domains benefit so much from this", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Revision in response to reviewer comments and questions", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for asking interesting questions and pointing out important flaws in the paper. We have uploaded a revised version of the paper that we believe addresses the questions raised. Major features of the revision are:\n\n1. We have added results on 2 more Atari 2600 games: Enduro and Q-bert. FiGAR seems to improve performance rather dramatically on Enduro with the FiGAR agent being close to 100 times better than the baseline A3C agent. (Note that the baseline agent performs very poorly according to the published results as well)\n\n2. In response to AnonReviewer3\u2019s comment about skipping intermediate frames, we have added Appendix F (page 23) by conducting experiments on what happens when FiGAR does not discard any intermediate frames (during evaluation phase). The general pattern seems to be that for games wherein lower action repetition is preferred, gains are made in terms of improved gameplay performance. However, for 24 out of 33 games the performance becomes worse, which depicts the importance of the temporal abstractions learnt by the action repetition part of the policy (\\pi_{\\theta_{x}}). This does not address the reviewer\u2019s question completely since at train time we still skip all the frames, as suggested by the action repetition policy. We have added a small discussion on future works section (section 6, page 10) which could potentially address this comment.\n\n3. In response to AnonReviewer3\u2019s suggestion to turn table1 into a bar graph we have done so (Figure 3, page 8) and it indeed does look much better.\n\n4. In response to AnonReviewer3\u2019s suggestion to compare directly to STRAW we have added Table 5 (Appendix A, page 14) which contains performance of STRAW models on all games which we have also experimented with. The general conclusion seems to be that in some games STRAW does better and in some games FiGAR does better.\n\n5. In response to AnonReviewer4\u2019s comment, we conducted experiments on shared representations for the FiGAR-TRPO agent. Appendix G (page 24) contains the results of the experiments. In general we observe that FiGAR-TRPO with shared representations does marginally better than FiGAR-TRPO, but not much better. The performance goes down on some tasks and improves on others. The average action repetition rate of the best policies learnt improves.\n\n6. In response to AnonReviewer4\u2019s comment on SMDPs we have added the relevant discussion to related works section (page 3).\n\n7. In response to AnonReviewer2\u2019s comment on the confusing nature of FiGAR-DDPG section, we have rewritten the section. It is hopefully clearer now.\n \n8. In response to AnonReviewer2\u2019s comment on the confusing notation \u2018r\u2019 for action repetition we have completely changed the notation for action repetition to the letter \u2018w\u2019.\n\n9. In response to AnonReviewer2\u2019s comment on  the potential weakness of the FiGAR framework, we have added a discussion on the shortcomings of the FiGAR in section 6 (page 10).\n\n10.We have corrected several typos as pointed out by the reviewers. \n", "OTHER_KEYS": "Sahil Sharma"}, {"TITLE": "Simple but effective idea with a very thorough evaluation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.\n\nA few comments/questions:\n- Table 1 could be easier to interpret as a figure of histograms.\n- Figure 3 could be easier to interpret as a table.\n- How was the subset of Atari games selected?\n- The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores.\n- It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.\n- FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.\n\nOverall, this is a nice simple addition to deep RL algorithms that many people will probably start using.\n\n--------------------\n\nI'm increasing my score to 8 based on the rebuttal and the revised paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "18 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).\n\nHere are some comments and questions, for improving the paper:\n\nThe introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.\n\nIn the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)\n\nExperiments:\nCan you provide error bars on the experimental results? (from running multiple random seeds)\n\nIt would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.\n\nThe TRPO evaluation is different from the results reported in Duan et al. ICML \u201916. Why not use the same benchmark?\n\nVideos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?\n\nHow many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).\n\nCan the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.\n\nMinor comments:\n-- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.\n-- \u201cidea of deciding when necessary\u201d - seems like it would be better to say \u201cidea of only deciding when necessary\"\n-- \"spaces.Durugkar et al.\u201d \u2014 missing a space.\n-- \u201cR={4}\u201d \u2014 why 4? Could you use a letter to indicate a constant instead? (or a different notation)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016 (modified: 16 Dec 2016)", "TITLE": "some questions and comments", "IS_META_REVIEW": false, "comments": "Hi, the main idea is quite interesting. I was curious about the following. My primary question is Q1, and others are predominantly comments.\n\nQ1: After learning is complete, did you try forward propagating through the network to find actions for every time-step as opposed to repeating actions? Concretely, if at t=5, action suggested by the network is a_3 with a repetition of 4, instead of sticking with a_3 for times t={5,6,7,8} perform action a_3 for just t=5, and forward prop through the policy again at t=6.\n\nI understand that the goal is to explore temporal abstractions, but for all the problems considered in this paper, a forward prop is not expensive at all. Hence, there is no computational bottle neck forcing action repetition during test-time. It is understandable that repeating actions speeds up training. However, at test time, the performance can potentially improve by not repeating. This idea is quite popular in variants of Receding Horizon Control and MCTS.\n\n2: Can you share hyper-parameter settings of section 5.2? How many iterations of TRPO was run, and how many trajectory samples per iteration? The performance on Ant-v1 task is too low for both TRPO and FIGAR. Running for more iterations and initializing the network better (smaller weights) might improve performance significantly for both. It might be informative to share the learning curves comparing FIGAR with TRPO. With the current results, it is a stretch to say that FIGAR \"outperforms\" TRPO.\n\n3: Considering that the action repetition is 1 for a majority of MuJoCo tasks (discounting Ant) and TORCS, why do you expect FIGAR to perform better? Also, the FIGAR policies seem to have more parameters than baselines they are compared against -- is this true? Have you compared to baselines with equal number of parameters?", "OTHER_KEYS": "Aravind Rajeswaran"}, {"DATE": "03 Dec 2016", "TITLE": "A few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "Intermediate frames", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Sahil Sharma, Aravind S. Lakshminarayanan, Balaraman Ravindran", "accepted": true, "id": "452"}