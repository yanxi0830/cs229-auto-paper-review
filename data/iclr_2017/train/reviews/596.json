{"conference": "ICLR 2017 conference submission", "title": "Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory", "abstract": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "[Summary]\nThis paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.\n\nThe experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.\n\nThe paper is well-written and it is easy to follow.\n\n[Major comments]\nI actually do like the idea and am also impressed that this model can work well.\nThe main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.\n\nOne key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don\u2019t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?\n\nThe RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?\n\nI appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.\n\n[Minor comments]\nI am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper develops a new shared memory based model for doing inference in knowledge bases. The work shows strong empirical results, and potentially could be impactful. However the reviewers felt that the work was not completely convincing without more analysis into the mechanisms of the system itself. \n \n Pros:\n - Quality: The reviewers like the experimental results of this work, praising them as \"strikingly good\", but giving the caveat the dataset used is now a bit old for this task. \n \n Mixed:\n - Clarity: Some reviewers found the work to be fairly well-written although there was mixed opinions about the exposition. Details of the model could be better explained, as could the development of the model\n \n Cons:\n - Quality: The main criticism is not feeling that the methodology is motivated for this task. Multiple reviewers claim there is \"little analysis about how it works\". Or that was \"hard to see\" how this would help. All reviewers are in agreement, that the paper should explore more deeply what shared memory is adding to this task, and introduce the approach better in this regard.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "22 Dec 2016", "TITLE": "Report the performance of IRNs with different memory sizes and inference steps on FB15K", "IS_META_REVIEW": false, "comments": "Thanks all reviewers for your great feedback and comments. We have updated the paper to address the major comments for adding analysis in the KB experiments. We add Table 2 and a corresponding paragraph to analyze the performance with different termination steps and memory sizes. (Note that for T_max =1, it is the case where IRNs do not use the shared memory.)  We found the number of times IRNs access the shared memory is critical for the performance, so IRNs cannot achieve the same level of performance without using shared memory. \nIn response to reviewer 2, regarding the shortest path experiments on KB and models with dynamic memory size, we will investigate these directions for future work. \nWe will update our paper to improve the introduction section and figure 1, and add more analysis on the termination steps.\n", "OTHER_KEYS": "yelong shen"}, {"TITLE": "Interesting paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset. \n\nPros:\n\n- The proposed approach demonstrates strong performance on FB15k dataset. \n\n- The idea of using shared memory for knowledge base completion is new and interesting. \n\n- The proposed approach is general and can be applied in various tasks. \n\nCons:\n\n- There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task. \n\n- The introduction section can be improved. Specifically, the authors should motivate \"shared memory\" more in the introduction and how it different from existing methods that using \"unshared memory\" for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion.  The concept of shared memory and search controller only make sense to me after reading through section 2. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.\n\n\nThe paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.\n\n* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:\n  - What are the performance without the shared memory? And when its size is grown? \n  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.\n  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.\n  - What is the proportion of examples for which the prediction changed along several inference iterations?\n\n* A value of \\lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?\n\n* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\n[Summary]\nThis paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.\n\nThe experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.\n\nThe paper is well-written and it is easy to follow.\n\n[Major comments]\nI actually do like the idea and am also impressed that this model can work well.\nThe main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.\n\nOne key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don\u2019t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?\n\nThe RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?\n\nI appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.\n\n[Minor comments]\nI am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "20 Nov 2016", "TITLE": "comment on the shared memory", "IS_META_REVIEW": false, "comments": "Can you explain in detail how you construct the shared memory for each training sample ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "09 Nov 2016", "TITLE": "Comment on shared memory", "IS_META_REVIEW": false, "comments": "The idea of shared memory in context of memory augmented neural networks is not novel. Neural Semantic Encoders previously introduced shared and multiple memory accesses [1, 2]. Please discuss the connection between Implicit ReasoNet and Neural Semantic Encoders in your manuscript.\n\nThanks,\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\" arXiv preprint arXiv:1610.06454 (2016).", "OTHER_KEYS": "Tsendsuren Munkhdalai"}, {"IS_META_REVIEW": true, "comments": "[Summary]\nThis paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.\n\nThe experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.\n\nThe paper is well-written and it is easy to follow.\n\n[Major comments]\nI actually do like the idea and am also impressed that this model can work well.\nThe main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.\n\nOne key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don\u2019t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?\n\nThe RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?\n\nI appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.\n\n[Minor comments]\nI am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper develops a new shared memory based model for doing inference in knowledge bases. The work shows strong empirical results, and potentially could be impactful. However the reviewers felt that the work was not completely convincing without more analysis into the mechanisms of the system itself. \n \n Pros:\n - Quality: The reviewers like the experimental results of this work, praising them as \"strikingly good\", but giving the caveat the dataset used is now a bit old for this task. \n \n Mixed:\n - Clarity: Some reviewers found the work to be fairly well-written although there was mixed opinions about the exposition. Details of the model could be better explained, as could the development of the model\n \n Cons:\n - Quality: The main criticism is not feeling that the methodology is motivated for this task. Multiple reviewers claim there is \"little analysis about how it works\". Or that was \"hard to see\" how this would help. All reviewers are in agreement, that the paper should explore more deeply what shared memory is adding to this task, and introduce the approach better in this regard.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "22 Dec 2016", "TITLE": "Report the performance of IRNs with different memory sizes and inference steps on FB15K", "IS_META_REVIEW": false, "comments": "Thanks all reviewers for your great feedback and comments. We have updated the paper to address the major comments for adding analysis in the KB experiments. We add Table 2 and a corresponding paragraph to analyze the performance with different termination steps and memory sizes. (Note that for T_max =1, it is the case where IRNs do not use the shared memory.)  We found the number of times IRNs access the shared memory is critical for the performance, so IRNs cannot achieve the same level of performance without using shared memory. \nIn response to reviewer 2, regarding the shortest path experiments on KB and models with dynamic memory size, we will investigate these directions for future work. \nWe will update our paper to improve the introduction section and figure 1, and add more analysis on the termination steps.\n", "OTHER_KEYS": "yelong shen"}, {"TITLE": "Interesting paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset. \n\nPros:\n\n- The proposed approach demonstrates strong performance on FB15k dataset. \n\n- The idea of using shared memory for knowledge base completion is new and interesting. \n\n- The proposed approach is general and can be applied in various tasks. \n\nCons:\n\n- There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task. \n\n- The introduction section can be improved. Specifically, the authors should motivate \"shared memory\" more in the introduction and how it different from existing methods that using \"unshared memory\" for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion.  The concept of shared memory and search controller only make sense to me after reading through section 2. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.\n\n\nThe paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.\n\n* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:\n  - What are the performance without the shared memory? And when its size is grown? \n  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.\n  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.\n  - What is the proportion of examples for which the prediction changed along several inference iterations?\n\n* A value of \\lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?\n\n* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\n[Summary]\nThis paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.\n\nThe experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.\n\nThe paper is well-written and it is easy to follow.\n\n[Major comments]\nI actually do like the idea and am also impressed that this model can work well.\nThe main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.\n\nOne key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don\u2019t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?\n\nThe RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?\n\nI appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.\n\n[Minor comments]\nI am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "20 Nov 2016", "TITLE": "comment on the shared memory", "IS_META_REVIEW": false, "comments": "Can you explain in detail how you construct the shared memory for each training sample ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "09 Nov 2016", "TITLE": "Comment on shared memory", "IS_META_REVIEW": false, "comments": "The idea of shared memory in context of memory augmented neural networks is not novel. Neural Semantic Encoders previously introduced shared and multiple memory accesses [1, 2]. Please discuss the connection between Implicit ReasoNet and Neural Semantic Encoders in your manuscript.\n\nThanks,\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\" arXiv preprint arXiv:1610.06454 (2016).", "OTHER_KEYS": "Tsendsuren Munkhdalai"}], "authors": "Yelong Shen*, Po-Sen Huang*, Ming-Wei Chang, Jianfeng Gao", "accepted": false, "id": "596"}