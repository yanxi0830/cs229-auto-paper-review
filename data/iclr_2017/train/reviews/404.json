{"conference": "ICLR 2017 conference submission", "title": "Quasi-Recurrent Neural Networks", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep\u2019s computation on the previous timestep\u2019s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.\n \n Pros:\n - Fast model\n - Good results\n \n Cons:\n - Because of its strong relationship to other models, the novelty is incremental.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "15 Jan 2017", "TITLE": "Response to official reviews, plus results on copy and addition tasks", "IS_META_REVIEW": false, "comments": "Reviewer 1 asks, \"Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\"\n\nYes. Nal Kalchbrenner released results for the ByteNet on the same IWSLT dataset we used in this paper in his slides for NIPS ", "OTHER_KEYS": "James Bradbury"}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in\nsequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition\nmatrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). \n\nAs discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.\n\nThe authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. \n\nWhile I feel that the contribution is somewhat incremental, I recommend acceptance. \n", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Questions", "IS_META_REVIEW": false, "DATE": "17 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice paper", "comments": "This paper introduces a novel RNN architecture named QRNN.\n\nQNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.\nConsequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.\n\nVarious extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.\n\nAuthors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). \n\nOverall the paper is an enjoyable read and the proposed approach is interesting,\nPros:\n- Address an important problem\n- Nice empirical evaluation showing the benefit of their approach\n- Demonstrate up to 16x speed-up relatively to a LSTM\nCons:\n- Somewhat incremental novelty compared to (Balduzizi et al., 2016)\n\nFew specific questions:\n- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  \n- How does the i-fo-ifo pooling perform comparatively? \n- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Strong, but should compare with PixelCNNs", "OTHER_KEYS": "(anonymous)", "comments": "The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures. Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps. Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models. The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances.\n\n\nMajor comment\n=============\nQRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?\n\n\nMinor comments\n==============\n1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. \n\n2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. \n\n3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.\n\n4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.\n\nSentiment classification\n------------------------\n5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.\n\n6. What was the convolutional filter size?\n\n7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?\n\n8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors\u2019 claim that neurons are interpretable even more convincing.\n\n\nLanguage modeling\n-----------------\n9. What was the size of the training, test, and validation set?\n\n10. What was the convolutional filter size, denoted as \u2018k\u2019?\n\n11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?\n\n12. The authors should show learning curves for a models with and without zone-out.\n \nTranslation\n-----------\n13. What was the size of the training, test, and validation set?\n\n14. How does translation performance depend on k?\n\n\n \n \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "good.", "comments": "\nThis paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.\n\n- I would encourage the authors to improve the explanation of the model. \n- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. \n- Otherwise the experiments seem adequate and I enjoyed this paper.\n\nThis could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "a better TLDR", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.\n \n Pros:\n - Fast model\n - Good results\n \n Cons:\n - Because of its strong relationship to other models, the novelty is incremental.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "15 Jan 2017", "TITLE": "Response to official reviews, plus results on copy and addition tasks", "IS_META_REVIEW": false, "comments": "Reviewer 1 asks, \"Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\"\n\nYes. Nal Kalchbrenner released results for the ByteNet on the same IWSLT dataset we used in this paper in his slides for NIPS ", "OTHER_KEYS": "James Bradbury"}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in\nsequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition\nmatrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). \n\nAs discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.\n\nThe authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. \n\nWhile I feel that the contribution is somewhat incremental, I recommend acceptance. \n", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Questions", "IS_META_REVIEW": false, "DATE": "17 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice paper", "comments": "This paper introduces a novel RNN architecture named QRNN.\n\nQNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.\nConsequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.\n\nVarious extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.\n\nAuthors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). \n\nOverall the paper is an enjoyable read and the proposed approach is interesting,\nPros:\n- Address an important problem\n- Nice empirical evaluation showing the benefit of their approach\n- Demonstrate up to 16x speed-up relatively to a LSTM\nCons:\n- Somewhat incremental novelty compared to (Balduzizi et al., 2016)\n\nFew specific questions:\n- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  \n- How does the i-fo-ifo pooling perform comparatively? \n- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Strong, but should compare with PixelCNNs", "OTHER_KEYS": "(anonymous)", "comments": "The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures. Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps. Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models. The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances.\n\n\nMajor comment\n=============\nQRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?\n\n\nMinor comments\n==============\n1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. \n\n2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. \n\n3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.\n\n4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.\n\nSentiment classification\n------------------------\n5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.\n\n6. What was the convolutional filter size?\n\n7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?\n\n8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors\u2019 claim that neurons are interpretable even more convincing.\n\n\nLanguage modeling\n-----------------\n9. What was the size of the training, test, and validation set?\n\n10. What was the convolutional filter size, denoted as \u2018k\u2019?\n\n11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?\n\n12. The authors should show learning curves for a models with and without zone-out.\n \nTranslation\n-----------\n13. What was the size of the training, test, and validation set?\n\n14. How does translation performance depend on k?\n\n\n \n \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "good.", "comments": "\nThis paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.\n\n- I would encourage the authors to improve the explanation of the model. \n- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. \n- Otherwise the experiments seem adequate and I enjoyed this paper.\n\nThis could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "a better TLDR", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher", "accepted": true, "id": "404"}