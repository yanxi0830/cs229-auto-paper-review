{"conference": "ICLR 2017 conference submission", "title": "Recurrent Hidden Semi-Markov Model", "abstract": "Segmentation and labeling of high dimensional time series data has wide applications in behavior understanding and medical diagnosis. Due to the difficulty in obtaining the label information for high dimensional data, realizing this objective in an unsupervised way is highly desirable. Hidden Semi-Markov Model (HSMM) is a classical tool for this problem. However, existing HSMM and its variants has simple conditional assumptions of observations, thus the ability to capture the nonlinear and complex dynamics within segments is limited. To tackle this limitation, we propose to incorporate the Recurrent Neural Network (RNN) to model the generative process in HSMM, resulting the Recurrent HSMM (R-HSMM). To accelerate the inference while preserving accuracy, we designed a structure encoding function to mimic the exact inference. By generalizing the penalty method to distribution space, we are able to train the model and the encoding function simultaneously. Empirical results show that the proposed R-HSMM achieves the state-of-the-art performances on both synthetic and real-world datasets.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.\n \n Where this paper really falls down is on originality. In particular, in the last two years there have been related works that aren't cited (and unfortunately weren't mentioned by the reviewers) that produce similar models. In particular, Johnson et al's 2016 NIPS paper develops almost the same inference strategy in almost the same model class. \n \n ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Paper revision", "IS_META_REVIEW": false, "comments": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\nSpecifically, we added a baseline CRF-autoencoder, and did the quantitative comparison across all datasets. We\u2019ve also added a large dataset, which contains 2750 sequences, where each sequence has ~1.5k 4-D observations.\n\nFor more details about the dataset (called PN-FULL in the paper) and comparison with new baseline (called CRF-AE in the paper),  please check our revised pdf.", "OTHER_KEYS": "Hanjun Dai"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Good method for HSMM estimation", "comments": "This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. \n\nThat being said, I think two aspects may need further improvement:\n(1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification).\n(2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "Putting the score for now, will post the full review tomorrow.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Novel model for temporal data", "comments": "This paper presents a novel model for unsupervised segmentation and classification of time series data.  A recurrent hidden semi-markov model is proposed.  This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.  Each latent class has its own RNN for modeling observations for that category.  Further, an efficient training procedure based on a variational approximation.  Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.\n\nThis is an interesting and novel paper.  The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.  The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.  The method is technically sound and demonstrated to be effective.\n\nIt would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).  CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.  Regardless, I think the merits of using RNNs for the class-specific generative models are clear.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "10 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 5, "TITLE": "RNN performance", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"SUBSTANCE": 3, "TITLE": "conditional random fields", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.\n \n Where this paper really falls down is on originality. In particular, in the last two years there have been related works that aren't cited (and unfortunately weren't mentioned by the reviewers) that produce similar models. In particular, Johnson et al's 2016 NIPS paper develops almost the same inference strategy in almost the same model class. \n \n ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Paper revision", "IS_META_REVIEW": false, "comments": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\nSpecifically, we added a baseline CRF-autoencoder, and did the quantitative comparison across all datasets. We\u2019ve also added a large dataset, which contains 2750 sequences, where each sequence has ~1.5k 4-D observations.\n\nFor more details about the dataset (called PN-FULL in the paper) and comparison with new baseline (called CRF-AE in the paper),  please check our revised pdf.", "OTHER_KEYS": "Hanjun Dai"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Good method for HSMM estimation", "comments": "This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. \n\nThat being said, I think two aspects may need further improvement:\n(1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification).\n(2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "Putting the score for now, will post the full review tomorrow.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Novel model for temporal data", "comments": "This paper presents a novel model for unsupervised segmentation and classification of time series data.  A recurrent hidden semi-markov model is proposed.  This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.  Each latent class has its own RNN for modeling observations for that category.  Further, an efficient training procedure based on a variational approximation.  Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.\n\nThis is an interesting and novel paper.  The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.  The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.  The method is technically sound and demonstrated to be effective.\n\nIt would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).  CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.  Regardless, I think the merits of using RNNs for the class-specific generative models are clear.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "10 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 5, "TITLE": "RNN performance", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"SUBSTANCE": 3, "TITLE": "conditional random fields", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "Hanjun Dai, Bo Dai, Yan-Ming Zhang, Shuang Li, Le Song", "accepted": true, "id": "421"}