{"conference": "ICLR 2017 conference submission", "title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors propose to extend the \u201cstandard\u201d attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.\n\nThe paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the \u201cstructured attention\u201d models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the \u201csimple\u201d attention models, but the techniques are nonetheless interesting.\n\nMain comments:\n1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I\u2019m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.\n2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?\n\nMinor comments:\n1. Typographical error: Equation 1: \u201cp(z | x, q\u201d \u2192 \u201cp(z | x, q)\u201d\n2. Section 3.3: \u201cPast work has demonstrated that the techniques necessary for this approach, \u2026 \u201d \u2192  \u201cPast work has demonstrated the techniques necessary for this approach, \u2026 \u201d"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair shares the reviewer's opinion and thinks that this is a very solid paper that deserves to be presented at ICLR. The idea is novel, well described and backed up by solid experiments (that show some empirical gains).", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "review", "comments": "This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network.\n\nUnder this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.\n\nThe experiments of the paper prove the usefulness of the model in various level \u2014 seq2seq and tree structure etc. I think it\u2019s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model.\n\nIn sum, I think this is a solid contribution and the approach will benefit the research in other problems.\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 1, "SUBSTANCE": 1, "RECOMMENDATION_UNOFFICIAL": 1, "MEANINGFUL_COMPARISON": 2, "comments": "This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.\n\nThe paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. \n\nThis seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?\n\nMinor comments/typos:\n- last paragraph of sec 1: \"standard attention attention\"\n- third paragraph of sec 3.2: \"the on log-potentials\"\n- sec 4.1, Results: \"... as it has no information about the source ordering\" -- what do you mean here?", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "solid paper", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose to extend the \u201cstandard\u201d attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.\n\nThe paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the \u201cstructured attention\u201d models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the \u201csimple\u201d attention models, but the techniques are nonetheless interesting.\n\nMain comments:\n1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I\u2019m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.\n2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?\n\nMinor comments:\n1. Typographical error: Equation 1: \u201cp(z | x, q\u201d \u2192 \u201cp(z | x, q)\u201d\n2. Section 3.3: \u201cPast work has demonstrated that the techniques necessary for this approach, \u2026 \u201d \u2192  \u201cPast work has demonstrated the techniques necessary for this approach, \u2026 \u201d\n", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "Interesting, clearly-written paper which proposes to extend attention over latent structures. However, experimental results on two real-world tasks only show small improvements over the baseline \"simple\" attention system.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 1, "SUBSTANCE": 1, "MEANINGFUL_COMPARISON": 2, "RECOMMENDATION_UNOFFICIAL": 1, "comments": "", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "A couple minor questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "Clique Structure in Syntactic Tree Selection Example", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 3}, {"TITLE": "questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "The authors propose to extend the \u201cstandard\u201d attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.\n\nThe paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the \u201cstructured attention\u201d models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the \u201csimple\u201d attention models, but the techniques are nonetheless interesting.\n\nMain comments:\n1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I\u2019m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.\n2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?\n\nMinor comments:\n1. Typographical error: Equation 1: \u201cp(z | x, q\u201d \u2192 \u201cp(z | x, q)\u201d\n2. Section 3.3: \u201cPast work has demonstrated that the techniques necessary for this approach, \u2026 \u201d \u2192  \u201cPast work has demonstrated the techniques necessary for this approach, \u2026 \u201d"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair shares the reviewer's opinion and thinks that this is a very solid paper that deserves to be presented at ICLR. The idea is novel, well described and backed up by solid experiments (that show some empirical gains).", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "review", "comments": "This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network.\n\nUnder this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.\n\nThe experiments of the paper prove the usefulness of the model in various level \u2014 seq2seq and tree structure etc. I think it\u2019s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model.\n\nIn sum, I think this is a solid contribution and the approach will benefit the research in other problems.\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 1, "SUBSTANCE": 1, "RECOMMENDATION_UNOFFICIAL": 1, "MEANINGFUL_COMPARISON": 2, "comments": "This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.\n\nThe paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. \n\nThis seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?\n\nMinor comments/typos:\n- last paragraph of sec 1: \"standard attention attention\"\n- third paragraph of sec 3.2: \"the on log-potentials\"\n- sec 4.1, Results: \"... as it has no information about the source ordering\" -- what do you mean here?", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "solid paper", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose to extend the \u201cstandard\u201d attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.\n\nThe paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the \u201cstructured attention\u201d models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the \u201csimple\u201d attention models, but the techniques are nonetheless interesting.\n\nMain comments:\n1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I\u2019m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.\n2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?\n\nMinor comments:\n1. Typographical error: Equation 1: \u201cp(z | x, q\u201d \u2192 \u201cp(z | x, q)\u201d\n2. Section 3.3: \u201cPast work has demonstrated that the techniques necessary for this approach, \u2026 \u201d \u2192  \u201cPast work has demonstrated the techniques necessary for this approach, \u2026 \u201d\n", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "Interesting, clearly-written paper which proposes to extend attention over latent structures. However, experimental results on two real-world tasks only show small improvements over the baseline \"simple\" attention system.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 1, "SUBSTANCE": 1, "MEANINGFUL_COMPARISON": 2, "RECOMMENDATION_UNOFFICIAL": 1, "comments": "", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "A couple minor questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "TITLE": "Clique Structure in Syntactic Tree Selection Example", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 3}, {"TITLE": "questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush", "accepted": true, "id": "393"}