{"conference": "ICLR 2017 conference submission", "title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts from partially observed knowledge bases. Recently, it was shown that some classes of embeddings can also be exploited to perform a simplified form of rule mining. By interpreting logical conjunction as a form of composition between re- lation embeddings, simplified logical theories can be mined directly in the space of latent representations. In this paper, we present a method to mine full-fledged logical theories, which are significantly more expressive, by casting the semantics of the logical operators to the space of the embeddings. In order to extract relevant rules in the space of relation compositions we borrow sparse reconstruction pro- cedures from the field of compressed sensing. Our empirical analysis showcases the advantages of our approach.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.\n\nThe solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. \n\n- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.\n\n- If the goal is to extract useful rules (for other applications), it is necessary to compare it to \u201cgraph random walk\u201d ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers concur that the paper is well written and the topic is interesting, but that the authors have not put sufficient effort into motivating their approach and evaluating it. The baselines seem too simple, the evaluation is incomplete. It is furthermore disappointing that the authors not only did not respond to the reviews, but did not respond to the pre-review questions. There is little in this review process that would support the paper being accepted, and therefore I concur with the reviewers' opinion and support rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.\n\nThe solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. \n\n- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.\n\n- If the goal is to extract useful rules (for other applications), it is necessary to compare it to \u201cgraph random walk\u201d (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).\n\nThe paper is nicely written. \nBut its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?\n- If it is for better performance on link prediction then the paper does not show this. The experiments do not compare FRM against the performance of the original vector space model.\n- If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.\n\nOther remarks:\n- The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.\n- The scalability of the rule miner is a big drawback that should be addressed.\n- Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation. The learned rules are bad for both cases.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "unconclusive experiments and missing theoretical motivation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL. \n\nThe experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:\n\n1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?\n\n2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?\n\n3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.\n\nThe solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. \n\n- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.\n\n- If the goal is to extract useful rules (for other applications), it is necessary to compare it to \u201cgraph random walk\u201d ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers concur that the paper is well written and the topic is interesting, but that the authors have not put sufficient effort into motivating their approach and evaluating it. The baselines seem too simple, the evaluation is incomplete. It is furthermore disappointing that the authors not only did not respond to the reviews, but did not respond to the pre-review questions. There is little in this review process that would support the paper being accepted, and therefore I concur with the reviewers' opinion and support rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.\n\nThe solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. \n\n- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.\n\n- If the goal is to extract useful rules (for other applications), it is necessary to compare it to \u201cgraph random walk\u201d (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).\n\nThe paper is nicely written. \nBut its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?\n- If it is for better performance on link prediction then the paper does not show this. The experiments do not compare FRM against the performance of the original vector space model.\n- If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.\n\nOther remarks:\n- The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.\n- The scalability of the rule miner is a big drawback that should be addressed.\n- Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation. The learned rules are bad for both cases.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "unconclusive experiments and missing theoretical motivation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL. \n\nThe experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:\n\n1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?\n\n2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?\n\n3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Stefano Teso, Andrea Passerini", "accepted": false, "id": "637"}