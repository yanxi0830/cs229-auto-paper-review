{"conference": "ICLR 2017 conference submission", "title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted \"key-frames\" of the provided expert trajectories.\n\nThe paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:\n1) Although the recursive splitting approach for extracting the \"key-frames\" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:\n   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? \n   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?\n2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.\n3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.\n\nMinor issues:\n- Page 1: \"make use of ideas about imitation\" reads a bit awkwardly\n- Page 3: \"We use the Inception network pre-trained ImageNet\" -> pre-trained for ImageNet classification\n- Page 4: the definition of the transition function for the stochastic case seems broken\n- Page 6: \"efficient enough to evaluate\" a bit strangely written sentence\n\nAdditional comments rather than real issues:\n- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. \n- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted \"key-frames\", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Quality, Clarity:\n \n The work is well motivated and clearly written -- no issues there.\n \n Originality, Significance:\n \n The idea is simple and well motivated, i.e., the learning of reward functions based on feature selection from identified subtasks in videos.\n \n pros:\n - the problem is difficult and relevant: good solutions would have impact\n \n cons:\n - the benefit with respect to other baselines for various choices, although the latest version does contain updated baselines\n - the influence of the initial controller on the results\n - the work may gain better appreciation at a robotics conference\n \n I am very much on the fence for this paper.\n It straddles a number of recent advances in video segmentation, robotics, and RL, which makes the specific technical contributions harder to identify. I do think that a robotics conference would be appreciative of the work, but better learning of reward functions is surely a bottleneck and therefore of interest to ICLR.\n Given the lukewarm support for this paper by reviewers, the PCs decided not to accept the paper, but invite the authors to present it in the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Simple well motivated approach, but requires better references and comparisons to existing methods", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper tries to present a first step towards solving the difficult problem of \"learning from limited number of demonstrations\". The paper tries to present 3 contributions towards this effort:\n1. unsupervised segmentation of videos to identify intermediate steps in a process\n2. define reward function based on feature selection for each sub-task\n\nPros:\n+ The paper is a first attempt to solve a very challenging problem, where a robot is taught real-world tasks with very few visual demonstrations and without further retraining.\n+ The method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.\n+ As demonstrated in Fig. 3, the reward functions could be more interpretable and correlate with transitions between subtasks.\n+ Breaking a video into subtasks helps a video demonstration-based method achieve comparable performance with a method which requires full instrumentation for complex real-world tasks like door opening.\n\nCons:\n1. Unsupervised video segmentation can serve as a good starting point to identify subtasks. However, there are multiple prior works in this domain which need to be referenced and compared with. Particularly, video shot detection and shot segmentation works try to identify abrupt change in video to break it into visually diverse shots. These methods could be easily augmented with CNN-features.\n(Note that there are multiple papers in this domain, eg. refer to survey in Yuan et al. Trans. on Circuits and Systems for video tech. 2007)\n\n2. The authors claim that they did not find it necessary to identify commonalities across demonstrations. This limits the scope of the problem drastically and requires the demonstrations to follow very specific set of constraints. Again, it is to be noted that there is past literature (video co-segmentation, eg. Tang et al. ECCV'14) which uses these commonalities to perform unsupervised video segmentation.\n\n3. The unsupervised temporal video segmentation approach in the paper is only compared to a very simple random baseline for a few sample videos. However, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.\n\n4. The authors hypothesize that \"sparse independent features exists which can discriminate a wide range of unseen inputs\" and encode this intuition through the feature selection strategy. Again, the validity of the hypothesis is not experimentally well demonstrated. For instance, comparison to a simple linear classifier for subtasks would have been useful.\n\nOverall, the paper presents a simple approach based on the idea that recognizing sub-goals in an unsupervised fashion would help in learning from few visual demonstrations. This is well motivated as a first-step towards a difficult task. However, the methods and claims presented in the paper need to be analyzed and compared with better baselines.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice idea but more baselines are needed.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a novel method to learn vision feature as intermediate rewards to guide the robot training in the real world. Since there are only a few sequences of human demonstrations, the paper first segments the sequences into fragments so that the features are roughly invariant on the corresponding fragments across sequences, then clusters and finds most discriminative features on those fragments, and uses them as the reward function. The features are from pre-trained deep models.\n\nThe idea is simple and seems quite effective in picking the right reward functions. Fig. 6 is a good comparison (although it could be better with error bars). However, some baselines are not strong, in particular vision related baselines. For example, the random reward (\"simply outputs true or false\") in Tbl. 2 seems quite arbitrary and may not serve as a good baseline (but its performance is still not that bad, surprisingly.). A better baseline would be to use random/simpler feature extraction on the image, e.g., binning features and simply picking the most frequent ones, which might not be as discriminative as the proposed feature. I wonder whether a simpler vision-based approach would lead to a similarly performed reward function. If so, then these delicate steps (segment, etc) altogether.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "See review", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"TITLE": "Well writen paper on a simple idea for extracting reward functions in real-world scenarios. Is missing some baselines.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted \"key-frames\" of the provided expert trajectories.\n\nThe paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:\n1) Although the recursive splitting approach for extracting the \"key-frames\" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:\n   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? \n   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?\n2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.\n3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.\n\nMinor issues:\n- Page 1: \"make use of ideas about imitation\" reads a bit awkwardly\n- Page 3: \"We use the Inception network pre-trained ImageNet\" -> pre-trained for ImageNet classification\n- Page 4: the definition of the transition function for the stochastic case seems broken\n- Page 6: \"efficient enough to evaluate\" a bit strangely written sentence\n\nAdditional comments rather than real issues:\n- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. \n- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted \"key-frames\", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "06 Dec 2016", "TITLE": "Clarifications of unsupervised video segmentation algorithm", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "More explanation and ablation analysis.", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IS_META_REVIEW": true, "comments": "The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted \"key-frames\" of the provided expert trajectories.\n\nThe paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:\n1) Although the recursive splitting approach for extracting the \"key-frames\" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:\n   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? \n   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?\n2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.\n3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.\n\nMinor issues:\n- Page 1: \"make use of ideas about imitation\" reads a bit awkwardly\n- Page 3: \"We use the Inception network pre-trained ImageNet\" -> pre-trained for ImageNet classification\n- Page 4: the definition of the transition function for the stochastic case seems broken\n- Page 6: \"efficient enough to evaluate\" a bit strangely written sentence\n\nAdditional comments rather than real issues:\n- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. \n- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted \"key-frames\", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Quality, Clarity:\n \n The work is well motivated and clearly written -- no issues there.\n \n Originality, Significance:\n \n The idea is simple and well motivated, i.e., the learning of reward functions based on feature selection from identified subtasks in videos.\n \n pros:\n - the problem is difficult and relevant: good solutions would have impact\n \n cons:\n - the benefit with respect to other baselines for various choices, although the latest version does contain updated baselines\n - the influence of the initial controller on the results\n - the work may gain better appreciation at a robotics conference\n \n I am very much on the fence for this paper.\n It straddles a number of recent advances in video segmentation, robotics, and RL, which makes the specific technical contributions harder to identify. I do think that a robotics conference would be appreciative of the work, but better learning of reward functions is surely a bottleneck and therefore of interest to ICLR.\n Given the lukewarm support for this paper by reviewers, the PCs decided not to accept the paper, but invite the authors to present it in the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Simple well motivated approach, but requires better references and comparisons to existing methods", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper tries to present a first step towards solving the difficult problem of \"learning from limited number of demonstrations\". The paper tries to present 3 contributions towards this effort:\n1. unsupervised segmentation of videos to identify intermediate steps in a process\n2. define reward function based on feature selection for each sub-task\n\nPros:\n+ The paper is a first attempt to solve a very challenging problem, where a robot is taught real-world tasks with very few visual demonstrations and without further retraining.\n+ The method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.\n+ As demonstrated in Fig. 3, the reward functions could be more interpretable and correlate with transitions between subtasks.\n+ Breaking a video into subtasks helps a video demonstration-based method achieve comparable performance with a method which requires full instrumentation for complex real-world tasks like door opening.\n\nCons:\n1. Unsupervised video segmentation can serve as a good starting point to identify subtasks. However, there are multiple prior works in this domain which need to be referenced and compared with. Particularly, video shot detection and shot segmentation works try to identify abrupt change in video to break it into visually diverse shots. These methods could be easily augmented with CNN-features.\n(Note that there are multiple papers in this domain, eg. refer to survey in Yuan et al. Trans. on Circuits and Systems for video tech. 2007)\n\n2. The authors claim that they did not find it necessary to identify commonalities across demonstrations. This limits the scope of the problem drastically and requires the demonstrations to follow very specific set of constraints. Again, it is to be noted that there is past literature (video co-segmentation, eg. Tang et al. ECCV'14) which uses these commonalities to perform unsupervised video segmentation.\n\n3. The unsupervised temporal video segmentation approach in the paper is only compared to a very simple random baseline for a few sample videos. However, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.\n\n4. The authors hypothesize that \"sparse independent features exists which can discriminate a wide range of unseen inputs\" and encode this intuition through the feature selection strategy. Again, the validity of the hypothesis is not experimentally well demonstrated. For instance, comparison to a simple linear classifier for subtasks would have been useful.\n\nOverall, the paper presents a simple approach based on the idea that recognizing sub-goals in an unsupervised fashion would help in learning from few visual demonstrations. This is well motivated as a first-step towards a difficult task. However, the methods and claims presented in the paper need to be analyzed and compared with better baselines.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice idea but more baselines are needed.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a novel method to learn vision feature as intermediate rewards to guide the robot training in the real world. Since there are only a few sequences of human demonstrations, the paper first segments the sequences into fragments so that the features are roughly invariant on the corresponding fragments across sequences, then clusters and finds most discriminative features on those fragments, and uses them as the reward function. The features are from pre-trained deep models.\n\nThe idea is simple and seems quite effective in picking the right reward functions. Fig. 6 is a good comparison (although it could be better with error bars). However, some baselines are not strong, in particular vision related baselines. For example, the random reward (\"simply outputs true or false\") in Tbl. 2 seems quite arbitrary and may not serve as a good baseline (but its performance is still not that bad, surprisingly.). A better baseline would be to use random/simpler feature extraction on the image, e.g., binning features and simply picking the most frequent ones, which might not be as discriminative as the proposed feature. I wonder whether a simpler vision-based approach would lead to a similarly performed reward function. If so, then these delicate steps (segment, etc) altogether.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "See review", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"TITLE": "Well writen paper on a simple idea for extracting reward functions in real-world scenarios. Is missing some baselines.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted \"key-frames\" of the provided expert trajectories.\n\nThe paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:\n1) Although the recursive splitting approach for extracting the \"key-frames\" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:\n   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? \n   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?\n2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.\n3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.\n\nMinor issues:\n- Page 1: \"make use of ideas about imitation\" reads a bit awkwardly\n- Page 3: \"We use the Inception network pre-trained ImageNet\" -> pre-trained for ImageNet classification\n- Page 4: the definition of the transition function for the stochastic case seems broken\n- Page 6: \"efficient enough to evaluate\" a bit strangely written sentence\n\nAdditional comments rather than real issues:\n- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. \n- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted \"key-frames\", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "06 Dec 2016", "TITLE": "Clarifications of unsupervised video segmentation algorithm", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "More explanation and ablation analysis.", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}], "authors": "Pierre Sermanet, Kelvin Xu, Sergey Levine", "accepted": false, "id": "504"}