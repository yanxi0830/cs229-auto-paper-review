{"conference": "ICLR 2017 conference submission", "title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.\n\n\nPros:\n\n- significant speed improvements through dynamic batching\n- source code provided\n\n\nCons:\n\n- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context\n- presentation/vizualisation can be improved"}, {"DATE": "07 Feb 2017", "TITLE": "uploaded minor revision", "IS_META_REVIEW": false, "comments": "Adds \\iclrfinalcopy and a link to the github repo which is now live", "OTHER_KEYS": "Moshe Looks"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers viewed the paper favorably as a nice/helpful contribution to the implementation of this important class of methods.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "minor revisions in response to reviewer comments", "IS_META_REVIEW": false, "comments": "I've just uploaded a minor revision in response to reviewer comments, in particular clarifying the intent of sections 3 and 3.5.1 with new/revised initial paragraphs. This pushed us a bit over the page limit so I moved the code example from 3.4 into an appendix per AnonReviewer1's suggestion. Thanks again AnonReviewers for your time and attention, I think this version is certainly an improvement over the original draft.", "OTHER_KEYS": "Moshe Looks"}, {"TITLE": "Description of a promising software package.", "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it.\n\nThey show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN.\n\nIn the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1.\n\nThe reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.\n\nThe presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (\"A combinator library for neural networks\") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the \"Stanford Sentiment Treebank\" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble \"[...] variant sets a new state-of-the-art on both subtasks\" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.\n\nUpdate on Jan. 17th:\nafter the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016 (modified: 17 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "A new method to optimize computation graphs", "comments": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.\n\n\nPros:\n\n- significant speed improvements through dynamic batching\n- source code provided\n\n\nCons:\n\n- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context\n- presentation/vizualisation can be improved ", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Scaling of dynamic batching", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.\n\n\nPros:\n\n- significant speed improvements through dynamic batching\n- source code provided\n\n\nCons:\n\n- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context\n- presentation/vizualisation can be improved"}, {"DATE": "07 Feb 2017", "TITLE": "uploaded minor revision", "IS_META_REVIEW": false, "comments": "Adds \\iclrfinalcopy and a link to the github repo which is now live", "OTHER_KEYS": "Moshe Looks"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers viewed the paper favorably as a nice/helpful contribution to the implementation of this important class of methods.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "minor revisions in response to reviewer comments", "IS_META_REVIEW": false, "comments": "I've just uploaded a minor revision in response to reviewer comments, in particular clarifying the intent of sections 3 and 3.5.1 with new/revised initial paragraphs. This pushed us a bit over the page limit so I moved the code example from 3.4 into an appendix per AnonReviewer1's suggestion. Thanks again AnonReviewers for your time and attention, I think this version is certainly an improvement over the original draft.", "OTHER_KEYS": "Moshe Looks"}, {"TITLE": "Description of a promising software package.", "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it.\n\nThey show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN.\n\nIn the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1.\n\nThe reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.\n\nThe presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (\"A combinator library for neural networks\") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the \"Stanford Sentiment Treebank\" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble \"[...] variant sets a new state-of-the-art on both subtasks\" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.\n\nUpdate on Jan. 17th:\nafter the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016 (modified: 17 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "A new method to optimize computation graphs", "comments": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.\n\n\nPros:\n\n- significant speed improvements through dynamic batching\n- source code provided\n\n\nCons:\n\n- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context\n- presentation/vizualisation can be improved ", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Scaling of dynamic batching", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}], "authors": "Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, Peter Norvig", "accepted": true, "id": "379"}