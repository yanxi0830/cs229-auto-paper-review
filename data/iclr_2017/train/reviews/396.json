{"conference": "ICLR 2017 conference submission", "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation", "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with conventional gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than baseline GANs.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.\n\nI would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. \n- Is the mask a binary mask or a alpha blending mask?\n- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.\n\nThe proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).\n\nFig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the \"niceness\" of the decomposition appears to be largely irrelevant.\n\nFurthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.\n\n- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as \"advertised\".\n\nRegarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.\n\nI assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes a layered approach to image generation, ie starting by generating the background first, followed by generating the foreground objects. All three reviewers are positive, although not enthusiastic. The idea is nice, and the results are reasonable. Accept as poster. For the camera ready, the AC suggests making the generated images in the results larger, to allow the readers to fully appreciate their quality.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "26 Jan 2017", "TITLE": "Follow up", "IS_META_REVIEW": false, "comments": "Dear Reviewers, \n\nThanks again for your feedback. We posted responses and updated our paper last week.\n\nIt would be great if you can let us know your updated thoughts or if you have any additional questions we can help address.\n\nAs a recap, among other things, we have included additional quantitative and qualitative results to further demonstrate the role of the learnt masks and transformations (including experiments on an additional dataset).\n\nThanks,\nAuthors", "OTHER_KEYS": "Jianwei Yang"}, {"DATE": "18 Jan 2017", "TITLE": "Summary on the updates", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their insightful comments. We have conducted supplementary experiments and incorporated comments into our updated submission. The major changes are:\n\n1. Additional qualitative evidence for the role of affine transformation (Section 6.7): Our submission already contained full generation results on MNIST-ONE, and intermediate decomposition results on CUB-200 and CIFAR-10 to compare our model with and without the transformation. We have now added full generation results on CUB-200 and CIFAR-10, as well as results on LFW. We have placed images generated by the model without transformation next to the those generated by the model with transformation for easier inspection (Fig 20)\n\n2. Quantitative evidence for the role of affine transformation (Section 6.9): We also have quantitative analysis of the learned transformation parameters that confirms that the model does in fact rely on the transformation (Fig 22)\n\n3. Importance of modeling shapes (Section 6.8): We establish importance of the mask generator through an ablation of our model without it (Fig 21).\n\nWe also conducted several additional experiments and analyses that are described in our responses to reviewers below.\n\nPlease note that the sections in Appendix have been reordered to increase readability.\n", "OTHER_KEYS": "Jianwei Yang"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "review", "comments": "The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image. This is obviously a reasonable approach there is clearly a foreground object. However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer. How would the proposed method deal with such situations?\n\nOverall, this is a reasonable work that approaches an important problem from a new angle. Yet, I think sizable efforts remain needed to make it a generic methodology. ", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "26 Dec 2016"}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.\n\nThe segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit.\n\nIt would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "TITLE": "a figure-ground shape aware GAN mode for image generation", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Layerwise image generation.", "comments": "The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.\n\nI would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. \n- Is the mask a binary mask or a alpha blending mask?\n- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.\n\nThe proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).\n\nFig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the \"niceness\" of the decomposition appears to be largely irrelevant.\n\nFurthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.\n\n- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as \"advertised\".\n\nRegarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.\n\nI assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 3, "comments": "", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "TITLE": "category specific and category agnostic models, clarity", "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"DATE": "10 Nov 2016", "TITLE": "More comparison needed with \"Generating images part by part with composite generative model\"", "IS_META_REVIEW": false, "comments": "I'm the main author of the \"Generating images part by part with composite generative model\".\n\nI was impressed with the idea of using the spatial transformer networks, and new evaluation metrics. \nEven though our objective was general, if our model use two generators, it gives similar results with this paper.\nOur model is also similar to the LR-GAN if the spatial transformer networks is replaced to ordinary generators.\nI would have liked to see it compared with our model in Related Works.\n\nPlease check second version of our paper that will be released next week.\n\nThanks,\n", "OTHER_KEYS": "Hanock Kwak"}, {"IS_META_REVIEW": true, "comments": "The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.\n\nI would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. \n- Is the mask a binary mask or a alpha blending mask?\n- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.\n\nThe proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).\n\nFig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the \"niceness\" of the decomposition appears to be largely irrelevant.\n\nFurthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.\n\n- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as \"advertised\".\n\nRegarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.\n\nI assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes a layered approach to image generation, ie starting by generating the background first, followed by generating the foreground objects. All three reviewers are positive, although not enthusiastic. The idea is nice, and the results are reasonable. Accept as poster. For the camera ready, the AC suggests making the generated images in the results larger, to allow the readers to fully appreciate their quality.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "26 Jan 2017", "TITLE": "Follow up", "IS_META_REVIEW": false, "comments": "Dear Reviewers, \n\nThanks again for your feedback. We posted responses and updated our paper last week.\n\nIt would be great if you can let us know your updated thoughts or if you have any additional questions we can help address.\n\nAs a recap, among other things, we have included additional quantitative and qualitative results to further demonstrate the role of the learnt masks and transformations (including experiments on an additional dataset).\n\nThanks,\nAuthors", "OTHER_KEYS": "Jianwei Yang"}, {"DATE": "18 Jan 2017", "TITLE": "Summary on the updates", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their insightful comments. We have conducted supplementary experiments and incorporated comments into our updated submission. The major changes are:\n\n1. Additional qualitative evidence for the role of affine transformation (Section 6.7): Our submission already contained full generation results on MNIST-ONE, and intermediate decomposition results on CUB-200 and CIFAR-10 to compare our model with and without the transformation. We have now added full generation results on CUB-200 and CIFAR-10, as well as results on LFW. We have placed images generated by the model without transformation next to the those generated by the model with transformation for easier inspection (Fig 20)\n\n2. Quantitative evidence for the role of affine transformation (Section 6.9): We also have quantitative analysis of the learned transformation parameters that confirms that the model does in fact rely on the transformation (Fig 22)\n\n3. Importance of modeling shapes (Section 6.8): We establish importance of the mask generator through an ablation of our model without it (Fig 21).\n\nWe also conducted several additional experiments and analyses that are described in our responses to reviewers below.\n\nPlease note that the sections in Appendix have been reordered to increase readability.\n", "OTHER_KEYS": "Jianwei Yang"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "review", "comments": "The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image. This is obviously a reasonable approach there is clearly a foreground object. However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer. How would the proposed method deal with such situations?\n\nOverall, this is a reasonable work that approaches an important problem from a new angle. Yet, I think sizable efforts remain needed to make it a generic methodology. ", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "26 Dec 2016"}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.\n\nThe segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit.\n\nIt would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "TITLE": "a figure-ground shape aware GAN mode for image generation", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Layerwise image generation.", "comments": "The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.\n\nI would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. \n- Is the mask a binary mask or a alpha blending mask?\n- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.\n\nThe proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).\n\nFig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the \"niceness\" of the decomposition appears to be largely irrelevant.\n\nFurthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.\n\n- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as \"advertised\".\n\nRegarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.\n\nI assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 3, "comments": "", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "TITLE": "category specific and category agnostic models, clarity", "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"DATE": "10 Nov 2016", "TITLE": "More comparison needed with \"Generating images part by part with composite generative model\"", "IS_META_REVIEW": false, "comments": "I'm the main author of the \"Generating images part by part with composite generative model\".\n\nI was impressed with the idea of using the spatial transformer networks, and new evaluation metrics. \nEven though our objective was general, if our model use two generators, it gives similar results with this paper.\nOur model is also similar to the LR-GAN if the spatial transformer networks is replaced to ordinary generators.\nI would have liked to see it compared with our model in Related Works.\n\nPlease check second version of our paper that will be released next week.\n\nThanks,\n", "OTHER_KEYS": "Hanock Kwak"}], "authors": "Jianwei Yang, Anitha Kannan, Dhruv Batra, Devi Parikh", "accepted": true, "id": "396"}