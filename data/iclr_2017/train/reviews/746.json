{"conference": "ICLR 2017 conference submission", "title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs.  Rather than studying this as a black-box discrete regression problem with no assumption whatsoever  on the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning.   This principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.  As a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "I was holding off on this review hoping to get the missing details from the code at"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Author Response to third review?", "IS_META_REVIEW": false, "comments": "Dear authors,\n\ndo you plan to address the third reviewer's comments? Your responses could help bring some more clarity and improve the confidence for the final decision...\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Promising idea but hard-to-reproduce in current state", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I was holding off on this review hoping to get the missing details from the code at ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Nice problem statement, nut too immature to publish", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.\n\nMy main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.\n\nI found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn't make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.\n\nHighlights:\n+ Proof-of-concept of a partially-trainable implementation of the important \u201cdivide and conquer\u201d paradigm\n++ Explicit reasoning about complexity of induced programs\n- The solution isn\u2019t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem\n- The writing style pictures the method as very general, but falls back on very low level details specific to each task\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting but extremely difficult read", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.\n\nThe paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.\n\nInstead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.\n\nBoth split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.\n\nIn general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "05 Dec 2016", "TITLE": "Please clearly state what parts of the models are trained", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Backpropagation through the sampling operation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "I was holding off on this review hoping to get the missing details from the code at"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Author Response to third review?", "IS_META_REVIEW": false, "comments": "Dear authors,\n\ndo you plan to address the third reviewer's comments? Your responses could help bring some more clarity and improve the confidence for the final decision...\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Promising idea but hard-to-reproduce in current state", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I was holding off on this review hoping to get the missing details from the code at ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Nice problem statement, nut too immature to publish", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.\n\nMy main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.\n\nI found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn't make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.\n\nHighlights:\n+ Proof-of-concept of a partially-trainable implementation of the important \u201cdivide and conquer\u201d paradigm\n++ Explicit reasoning about complexity of induced programs\n- The solution isn\u2019t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem\n- The writing style pictures the method as very general, but falls back on very low level details specific to each task\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting but extremely difficult read", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.\n\nThe paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.\n\nInstead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.\n\nBoth split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.\n\nIn general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "05 Dec 2016", "TITLE": "Please clearly state what parts of the models are trained", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Backpropagation through the sampling operation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}], "authors": "Alex Nowak, Joan Bruna", "accepted": false, "id": "746"}