{"conference": "ICLR 2017 conference submission", "title": "L-SR1: A Second Order Optimization Method for Deep Learning", "abstract": "We describe L-SR1, a new second order method to train deep neural networks. Second order methods hold great promise for distributed training of deep networks. Unfortunately, they have not proven practical. Two significant barriers to their success are inappropriate handling of saddle points, and poor conditioning of the Hessian. L-SR1 is a practical second order method that addresses these concerns. We provide experimental results showing that L-SR1 performs at least as well as Nesterov's Accelerated Gradient Descent, on the MNIST and CIFAR10 datasets. For the CIFAR10 dataset, we see competitive performance on shallow networks like LeNet5, as well as on deeper networks like residual networks. Furthermore, we perform an experimental analysis of L-SR1 with respect to its hyper-parameters to gain greater intuition. Finally, we outline the potential usefulness of L-SR1 in distributed training of deep neural networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an interesting approach, in that (unlike many second-order methods) SR1 updates can potentially take advantage of negative curvature in the Hessian. However, all reviewers had some significant concerns about the utility of the method. In particular, reviewers were concerned that the method does not show a significant gain over the Adam algorithm (which is simpler/cheaper and easier to implement). The public reviewer also points out that there are many existing quasi-Newton methods designed for DL, so it is up to the authors to compared to at least one of these. For these reasons I'm recommending rejection at this time.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting work, but not ready to be published", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "O(mn)?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Address better optimization at saddle points with symmetric rank-one method which does not guarantee pos. def. update matrix, vs. BFGS approach. Investigating this optimization with limited memory version or SR1", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "complexity", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "19 Nov 2016", "TITLE": "A few comments", "IS_META_REVIEW": false, "comments": "This is a very interesting paper and the results look interesting. The paper is missing references for a large body of stochastic quasi-Newton methods and at times, is a bit misleading with its claims. \n\nReferences:\nA Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (ICML, 2016): ", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an interesting approach, in that (unlike many second-order methods) SR1 updates can potentially take advantage of negative curvature in the Hessian. However, all reviewers had some significant concerns about the utility of the method. In particular, reviewers were concerned that the method does not show a significant gain over the Adam algorithm (which is simpler/cheaper and easier to implement). The public reviewer also points out that there are many existing quasi-Newton methods designed for DL, so it is up to the authors to compared to at least one of these. For these reasons I'm recommending rejection at this time.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting work, but not ready to be published", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "O(mn)?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Address better optimization at saddle points with symmetric rank-one method which does not guarantee pos. def. update matrix, vs. BFGS approach. Investigating this optimization with limited memory version or SR1", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "complexity", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "19 Nov 2016", "TITLE": "A few comments", "IS_META_REVIEW": false, "comments": "This is a very interesting paper and the results look interesting. The paper is missing references for a large body of stochastic quasi-Newton methods and at times, is a bit misleading with its claims. \n\nReferences:\nA Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (ICML, 2016): ", "OTHER_KEYS": "(anonymous)"}], "authors": "Vivek Ramamurthy, Nigel Duffy", "accepted": false, "id": "615"}