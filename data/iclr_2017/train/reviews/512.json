{"conference": "ICLR 2017 conference submission", "title": "Nonparametrically Learning Activation Functions in Deep Neural Nets", "abstract": "We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justification for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. \u201c \u2026 the interval [-L+T, L+T] \u2026\u201d should be \u201c \u2026 the interval [-L+T, L-T] \u2026 \u201c ?\n- Pg 2., Equation for f(x), should it be \u201c (-L+T) i \\pi x / L \u201c in both sin and cos terms, or without \u201c x \u201c ?\n- Theorem 4.2 \u201c \u2026 some algorithm \\eps-uniformly stable \u2026\u201d remove the word \u201calgorithm\u201d\n- Theorem 4.5.  SGM undefined"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors propose a nonparametric regression approach to learn the activation functions in deep neural networks. The proposed theoretical analysis, based on stability arguments, is quite interesting. Experiments on MNIST and CIFAR-10 illustrate the potential of the approach. \n \n Reviewers were somewhat positive, but preliminary empirical evidence on small datasets makes this contribution better suited for the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Dec 2016", "TITLE": "Theorem 4.7", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "strong submission, interesting from both theoretical and experimental point of view", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper provides a principled framework for nonparametrically learning activation\nfunctions in deep neural networks. A theoretical justification for authors' choice of \nnonparametric activation functions is given. \nTheoretical results are satisfactory but I particularly like the experimental setup\nwhere their methods are tested on image\nrecognition datasets and achieve up to a 15% relative increase in test performance\ncompared to the baseline.\nWell-written paper and novel theoretical techniques. \nThe intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the\nAppendix clarifies everything.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Good paper, room for more  empirical study", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:\n\nThe paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.\n\nsignificance:\n\nThe paper introduces a nice idea, and present nice experimental results. however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper. \n \nA more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. \n\n\nComments: \n\n- Are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?\n\n- If all weights are tied across units and layers. One question that would be interesting to study , if there is an optimal non linearity. \n\n- How different is the non linearity learned if the hidden units are normalized or un-normalized.  In other words how does the non linearity change if you use or don't use batch normalization? \n\n- Does normalization affect  the conclusion that polynomial basis fail? \n\n\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. \u201c \u2026 the interval [-L+T, L+T] \u2026\u201d should be \u201c \u2026 the interval [-L+T, L-T] \u2026 \u201c ?\n- Pg 2., Equation for f(x), should it be \u201c (-L+T) i \\pi x / L \u201c in both sin and cos terms, or without \u201c x \u201c ?\n- Theorem 4.2 \u201c \u2026 some algorithm \\eps-uniformly stable \u2026\u201d remove the word \u201calgorithm\u201d\n- Theorem 4.5.  SGM undefined", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "learnt non-linearities vs network capacity", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Effect of non linearity and basis expansion choice", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. \u201c \u2026 the interval [-L+T, L+T] \u2026\u201d should be \u201c \u2026 the interval [-L+T, L-T] \u2026 \u201c ?\n- Pg 2., Equation for f(x), should it be \u201c (-L+T) i \\pi x / L \u201c in both sin and cos terms, or without \u201c x \u201c ?\n- Theorem 4.2 \u201c \u2026 some algorithm \\eps-uniformly stable \u2026\u201d remove the word \u201calgorithm\u201d\n- Theorem 4.5.  SGM undefined"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors propose a nonparametric regression approach to learn the activation functions in deep neural networks. The proposed theoretical analysis, based on stability arguments, is quite interesting. Experiments on MNIST and CIFAR-10 illustrate the potential of the approach. \n \n Reviewers were somewhat positive, but preliminary empirical evidence on small datasets makes this contribution better suited for the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Dec 2016", "TITLE": "Theorem 4.7", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "strong submission, interesting from both theoretical and experimental point of view", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper provides a principled framework for nonparametrically learning activation\nfunctions in deep neural networks. A theoretical justification for authors' choice of \nnonparametric activation functions is given. \nTheoretical results are satisfactory but I particularly like the experimental setup\nwhere their methods are tested on image\nrecognition datasets and achieve up to a 15% relative increase in test performance\ncompared to the baseline.\nWell-written paper and novel theoretical techniques. \nThe intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the\nAppendix clarifies everything.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Good paper, room for more  empirical study", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:\n\nThe paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.\n\nsignificance:\n\nThe paper introduces a nice idea, and present nice experimental results. however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper. \n \nA more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. \n\n\nComments: \n\n- Are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?\n\n- If all weights are tied across units and layers. One question that would be interesting to study , if there is an optimal non linearity. \n\n- How different is the non linearity learned if the hidden units are normalized or un-normalized.  In other words how does the non linearity change if you use or don't use batch normalization? \n\n- Does normalization affect  the conclusion that polynomial basis fail? \n\n\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. \u201c \u2026 the interval [-L+T, L+T] \u2026\u201d should be \u201c \u2026 the interval [-L+T, L-T] \u2026 \u201c ?\n- Pg 2., Equation for f(x), should it be \u201c (-L+T) i \\pi x / L \u201c in both sin and cos terms, or without \u201c x \u201c ?\n- Theorem 4.2 \u201c \u2026 some algorithm \\eps-uniformly stable \u2026\u201d remove the word \u201calgorithm\u201d\n- Theorem 4.5.  SGM undefined", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "learnt non-linearities vs network capacity", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Effect of non linearity and basis expansion choice", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Carson Eisenach, Zhaoran Wang, Han Liu", "accepted": false, "id": "512"}