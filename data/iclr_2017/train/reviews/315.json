{"conference": "ICLR 2017 conference submission", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviews (including the public one) were extremely positive, and this sheds light on a universal engineering issue that arises in fitting non-convex models. I think the community will benefit a lot from the insights here.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Analysis of large batch training", "comments": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"DATE": "17 Dec 2016", "TITLE": "Good paper", "IS_META_REVIEW": false, "comments": "I think that this is a great empirical exploration of long-held folk wisdom in the Deep Learning community - that using larger minibatches makes generalization error worse.  \n\nThe paper does a good of explaining why phenomenon occurs, by analyzing the \"sharpness\" of the loss function for large-batch and small-batch trained models.  \n\nSome other connections that I think would be interesting to explore: \n  -If you use very small minibatches, does generalization get even better (perhaps at the expense of very slow training).  \n  -Can other forms of noise injection compensate for the use of a larger minibatch?  For example, if I inject increasing amounts of noise into the gradients or the parameters with larger batches, does this close the gap with small-batch training?  \n  -The motivation for using smaller minibatches here seems closely related to the motivation for adversarial examples (ensuring that loss is relatively flat in a large region around data points).  ", "OTHER_KEYS": "Alex Lamb"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Little novelty but valuable empirical evidence", "comments": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. \n\nPros and Cons:\nAlthough there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. \n\nSignificance:\nI think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.\n\nComments:\nEarlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.\n\n\n\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 10, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good paper", "comments": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "local minima", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"TITLE": "Contant \"1\" in equation (3)?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviews (including the public one) were extremely positive, and this sheds light on a universal engineering issue that arises in fitting non-convex models. I think the community will benefit a lot from the insights here.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Analysis of large batch training", "comments": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"DATE": "17 Dec 2016", "TITLE": "Good paper", "IS_META_REVIEW": false, "comments": "I think that this is a great empirical exploration of long-held folk wisdom in the Deep Learning community - that using larger minibatches makes generalization error worse.  \n\nThe paper does a good of explaining why phenomenon occurs, by analyzing the \"sharpness\" of the loss function for large-batch and small-batch trained models.  \n\nSome other connections that I think would be interesting to explore: \n  -If you use very small minibatches, does generalization get even better (perhaps at the expense of very slow training).  \n  -Can other forms of noise injection compensate for the use of a larger minibatch?  For example, if I inject increasing amounts of noise into the gradients or the parameters with larger batches, does this close the gap with small-batch training?  \n  -The motivation for using smaller minibatches here seems closely related to the motivation for adversarial examples (ensuring that loss is relatively flat in a large region around data points).  ", "OTHER_KEYS": "Alex Lamb"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Little novelty but valuable empirical evidence", "comments": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. \n\nPros and Cons:\nAlthough there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. \n\nSignificance:\nI think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.\n\nComments:\nEarlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.\n\n\n\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 10, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good paper", "comments": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "local minima", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"TITLE": "Contant \"1\" in equation (3)?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang", "accepted": true, "id": "315"}