{"conference": "ICLR 2017 conference submission", "title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Reviewers agree that this a high-quality and interesting paper exploring important connections between widely used RL algorithms. It has the potential to be an impactful paper, with the most positive comment noting that it \"will likely influence a broad swath of RL\". \n \n Pros:\n - The main concepts of the paper came through clearly, and all reviewers felt the idea was interesting and novel.\n - The empirical part of the paper was convincing and \"empirical section is very well explained\" \n \n Cons:\n - There and some concerns about the writing and notation. None of these were too critical though, and the authors responded \n - Reviewers asked for some more comparisons with alternative formulations.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "17 Jan 2017", "TITLE": "Thankyou to the reviewers", "IS_META_REVIEW": false, "comments": "Thankyou to the three reviewers, and apologies for the delay in responding. We have updated the paper in response to your comments. Further responses below.", "OTHER_KEYS": "Brendan ODonoghue"}, {"DATE": "27 Dec 2016", "TITLE": "No questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "An excellent paper pointing out connections between existing algorithms, which also leads to new algorithms with excellent results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.\nOne minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. \nOverall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.  \n\nPresentation: \nAlthough that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network \u2013 for me that would be a more intuitive exposition of the new algorithm and findings. \n\nSmall concern in general case derivation: \nSection 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.\n\nResults:\nA comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)\n\nOverall: strong paper, good theoretical insights. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting links between policy-based and value-based methods", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016", "TITLE": "Discounted weighting of states?", "IS_META_REVIEW": false, "comments": "When initially reading the paper I assumed you just forgot to mention in section 2 that the distribution d^pi used a discounted weighted of states, for the policy gradient theorem from Sutton et al (1999) to hold (see above eq. 2 in that paper), however it seems like no such discounting is being applied when actually doing the updates in practice. Obviously in the tabular case it does not matter, but could it be a problem with DQNs? (which value of gamma is being used by the way?)", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "09 Dec 2016", "TITLE": "Page 6, last paragraph (a small doubt)", "IS_META_REVIEW": false, "comments": "The authors write at the page 6, last paragraph \"Under standard policy gradient the bellman residual will be small, then it follows that adding a term that reduces that error should not make much difference at the fixed point.\" \n\nThis point is not clear. Let's consider the standard policy gradient without the entropy-bonus. In this case, I was wondering if there is a way to determine the fixed-policy? I did the same maths inspired by the paper but I did not reach to any concluding result.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "08 Dec 2016", "TITLE": "A few more questions", "IS_META_REVIEW": false, "comments": "Hi,\n\nCould you please clarify the following:\n  1. It sounds like alpha is kept fixed in experiments and results are reported for the \"exploratory\" policy (softmax with temperature alpha) rather than the greedy one. Is that correct and if yes, why?\n  2. In both the grid world and Atari, is the critic estimate of Q(s, a) obtained by summing the (discounted) observed rewards for up to t_max timesteps after taking action a in state s, plus the (discounted) estimated V(last observed state)? (= as in A3C)\n  3. For the Q-learning step are you freezing the target network and clipping the error as in the Nature DQN paper? If not, why?\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "05 Dec 2016", "TITLE": "Independence assumption", "IS_META_REVIEW": false, "comments": "Hi,\n\nIn 3.2 you consider the case where \"the measure in the expectation (is) independent of theta\". However this is not the case in practice since both the state and action distributions depend on theta. Could you please comment on how this affects the proposed interpretation? I believe it would be important to explain it in the paper.\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "From eq. 3 to eq. 4", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Reviewers agree that this a high-quality and interesting paper exploring important connections between widely used RL algorithms. It has the potential to be an impactful paper, with the most positive comment noting that it \"will likely influence a broad swath of RL\". \n \n Pros:\n - The main concepts of the paper came through clearly, and all reviewers felt the idea was interesting and novel.\n - The empirical part of the paper was convincing and \"empirical section is very well explained\" \n \n Cons:\n - There and some concerns about the writing and notation. None of these were too critical though, and the authors responded \n - Reviewers asked for some more comparisons with alternative formulations.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "17 Jan 2017", "TITLE": "Thankyou to the reviewers", "IS_META_REVIEW": false, "comments": "Thankyou to the three reviewers, and apologies for the delay in responding. We have updated the paper in response to your comments. Further responses below.", "OTHER_KEYS": "Brendan ODonoghue"}, {"DATE": "27 Dec 2016", "TITLE": "No questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "An excellent paper pointing out connections between existing algorithms, which also leads to new algorithms with excellent results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.\nOne minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. \nOverall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.  \n\nPresentation: \nAlthough that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network \u2013 for me that would be a more intuitive exposition of the new algorithm and findings. \n\nSmall concern in general case derivation: \nSection 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.\n\nResults:\nA comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)\n\nOverall: strong paper, good theoretical insights. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting links between policy-based and value-based methods", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016", "TITLE": "Discounted weighting of states?", "IS_META_REVIEW": false, "comments": "When initially reading the paper I assumed you just forgot to mention in section 2 that the distribution d^pi used a discounted weighted of states, for the policy gradient theorem from Sutton et al (1999) to hold (see above eq. 2 in that paper), however it seems like no such discounting is being applied when actually doing the updates in practice. Obviously in the tabular case it does not matter, but could it be a problem with DQNs? (which value of gamma is being used by the way?)", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "09 Dec 2016", "TITLE": "Page 6, last paragraph (a small doubt)", "IS_META_REVIEW": false, "comments": "The authors write at the page 6, last paragraph \"Under standard policy gradient the bellman residual will be small, then it follows that adding a term that reduces that error should not make much difference at the fixed point.\" \n\nThis point is not clear. Let's consider the standard policy gradient without the entropy-bonus. In this case, I was wondering if there is a way to determine the fixed-policy? I did the same maths inspired by the paper but I did not reach to any concluding result.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "08 Dec 2016", "TITLE": "A few more questions", "IS_META_REVIEW": false, "comments": "Hi,\n\nCould you please clarify the following:\n  1. It sounds like alpha is kept fixed in experiments and results are reported for the \"exploratory\" policy (softmax with temperature alpha) rather than the greedy one. Is that correct and if yes, why?\n  2. In both the grid world and Atari, is the critic estimate of Q(s, a) obtained by summing the (discounted) observed rewards for up to t_max timesteps after taking action a in state s, plus the (discounted) estimated V(last observed state)? (= as in A3C)\n  3. For the Q-learning step are you freezing the target network and clipping the error as in the Nature DQN paper? If not, why?\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "05 Dec 2016", "TITLE": "Independence assumption", "IS_META_REVIEW": false, "comments": "Hi,\n\nIn 3.2 you consider the case where \"the measure in the expectation (is) independent of theta\". However this is not the case in practice since both the state and action distributions depend on theta. Could you please comment on how this affects the proposed interpretation? I believe it would be important to explain it in the paper.\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "From eq. 3 to eq. 4", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}], "authors": "Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, Volodymyr Mnih", "accepted": true, "id": "432"}