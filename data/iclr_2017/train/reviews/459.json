{"conference": "ICLR 2017 conference submission", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.\n\nThe framework is nice and appealing. \n\nHowever, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning\" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication. \n \n Pros:\n - Reviewers in general found the paper clear and well written.\n - Multi-task learning in deep models is of interest to the community\n - The approach is sensible and the experiments show that it seems to work\n \n Cons:\n - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental\n - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods\n - Not all reviewers were convinced by the experiments\n - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper\n \n The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Jan 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "\nDear readers and reviewers,\n\nWe would like to thank all the reviewers that helped us to improve this submission. Based on their comments, we have revised the submission.\n\nThe main changed parts are:\n\n1. We discuss some related work in multi-task learning from speech recognition community. (Sec 2, Comment from Reviewer 3).\n\n2. We add a new section in related work to discuss the literature in the line of constructing a MTL-like problem via clustering training data. (Sec 2, Pre-review question from Reviewer2)\n\n3. We give more details about initialisation, particularly how to make the random initialisation condition work in practice. (Sec 4, Comment from Reviewer 2)\n\nFinally, we add two more experiments. Due to page limits, they are placed in the appendix.\n\nA) We compare our methods with some classic (shallow, matrix-based) MTL methods using both classic and CNN features. (Appendix A, Comment from Reviewer 1)\n\nB) We add details about model size in terms of number of parameters, and demonstrate what happens if we make an almost equal sized UD-MTL competitor. (Appendix B, Pre-review question from Reviewer2)", "OTHER_KEYS": "Yongxin Yang"}, {"TITLE": "Comparison with other standard MTL methods is missing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.\n\nThe framework is nice and appealing. \n\nHowever, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning\" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. \n\nAs mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this. \n\nOne question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-de\ufb01ned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.\n\nAlthough factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used. \n\nBTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g., \n\nHuang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "model size and cluster adaptive training ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.\n\nThe framework is nice and appealing. \n\nHowever, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning\" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication. \n \n Pros:\n - Reviewers in general found the paper clear and well written.\n - Multi-task learning in deep models is of interest to the community\n - The approach is sensible and the experiments show that it seems to work\n \n Cons:\n - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental\n - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods\n - Not all reviewers were convinced by the experiments\n - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper\n \n The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Jan 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "\nDear readers and reviewers,\n\nWe would like to thank all the reviewers that helped us to improve this submission. Based on their comments, we have revised the submission.\n\nThe main changed parts are:\n\n1. We discuss some related work in multi-task learning from speech recognition community. (Sec 2, Comment from Reviewer 3).\n\n2. We add a new section in related work to discuss the literature in the line of constructing a MTL-like problem via clustering training data. (Sec 2, Pre-review question from Reviewer2)\n\n3. We give more details about initialisation, particularly how to make the random initialisation condition work in practice. (Sec 4, Comment from Reviewer 2)\n\nFinally, we add two more experiments. Due to page limits, they are placed in the appendix.\n\nA) We compare our methods with some classic (shallow, matrix-based) MTL methods using both classic and CNN features. (Appendix A, Comment from Reviewer 1)\n\nB) We add details about model size in terms of number of parameters, and demonstrate what happens if we make an almost equal sized UD-MTL competitor. (Appendix B, Pre-review question from Reviewer2)", "OTHER_KEYS": "Yongxin Yang"}, {"TITLE": "Comparison with other standard MTL methods is missing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.\n\nThe framework is nice and appealing. \n\nHowever, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning\" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. \n\nAs mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this. \n\nOne question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-de\ufb01ned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.\n\nAlthough factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used. \n\nBTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g., \n\nHuang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "model size and cluster adaptive training ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Yongxin Yang, Timothy M. Hospedales", "accepted": true, "id": "459"}