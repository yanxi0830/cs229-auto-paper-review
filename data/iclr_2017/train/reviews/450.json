{"conference": "ICLR 2017 conference submission", "title": "Revisiting Classifier Two-Sample Tests", "abstract": "The goal of two-sample tests is to assess whether two samples, $S_P \\sim P^n$ and $S_Q \\sim Q^m$, are drawn from the same distribution.  Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the $n$ examples in $S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a negative label. If the null hypothesis ``$P = Q$'' is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level.  As we will show, such \\emph{Classifier Two-Sample Tests} (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where $P$ and $Q$ differ.  The goal of this paper is to establish the properties, performance, and uses of C2ST.  First, we analyze their main theoretical properties.  Second, we compare their performance against a variety of state-of-the-art alternatives.  Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs).  Fourth, we showcase the novel application of GANs together with C2ST for causal discovery.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "## Paper summary\n\nThe paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.\n\nA theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.\n\n## Review summary \n\nThe paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.\n\n## Major comments / questions \n\nMy main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.\n\n* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: \"... the random variable n_te \\hat{t} follows a Binomial(nte, p)...\" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.\n\n* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. \n\nAnyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper is a valuable contribution to the literature.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Updated revision", "IS_META_REVIEW": false, "comments": "Dear all,\n\nThank you very much for your insightful feedback.\n\nWe have tried our best to incorporate all your suggestions into a revised version of our manuscript.\n\nPlease let us know how we could improve our submission further.\n\nSincerely,\nThe authors.", "OTHER_KEYS": "David Lopez-Paz"}, {"TITLE": "a very interesting framework; could improve clarity; significance to ICLR?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I would like first to apologize for the delay.\n\nSummary: A framework for two-samples statistical test using binary\nclassification is proposed. It allows multi-dimensional sample testing and\nan interpretability that other tests lack. A theoritical analysis is\nprovided and various empirical tests reported.\n\nA very interesting approach. I have however two main concerns.\n\nThe clarity of the presentation is obscured by too much content. It would\nbe more interesting if the presentation could be somewhat\nself-contained. You could consider making 2 papers out of this paper.\n\nSeriously, you cram a lot of experiments in this paper. But the setting\nof the experiments is not really explained. We are supposed to have read\nJitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All \nthis is okay but reduces your public to a very few.\n\nFor example, if I am not mistaken, you never explained what SCF is, despite\nthe fact that its performances are reported. \n\nAs a second point, given also that the number of submissions to this conference are exploding,\nI would like to challenge you with the following question:\n\nWhy is this work significant to the representation learning community?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A generally useful and interesting approach to 2-sample tests", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.  For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.  As such, a straightforward threshold can be derived for any classifier.  Finding a more powerful test then amounts to training a better classifier.  One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.\n\n+ The approach is sound and very general\n+ The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have\n\n- The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.  For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.  Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well\n * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)\n * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.\n\nArthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful.  If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.  The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "14 Dec 2016", "TITLE": "Discussion of variance for \"testing as classification\"", "IS_META_REVIEW": false, "comments": "We are writing both to provide a perspective on the proposed \u201cclassification approach\u201d to testing, and to expand on the reviews of AnonReviewer3 and AnonReviewer1, which suggested considering the relation between the MMD and classification. We discuss this link with reference to some earlier work, and then look into the broader implications for testing using classification error.\n\n\nFirst, the MMD can indeed be thought of as a classifier. In Section 2 of the paper:\n\n\nSriperumbudur, B., Fukumizu, K., Gretton, A., Lanckriet, G., and Schoelkopf, B., Kernel choice and classifiability for RKHS embeddings of probability distributions, NIPS 2009.\n\n\nwe show that the MMD is the negative of the optimal risk corresponding to a linear loss function, associated with the kernel classifier. The \u201cwitness function\u201d of the MMD can then be thought of as the decision function for that classifier, returning labels +1 for positive values (class P), and -1 for negative values (class Q). See figure 1 in Gretton et al (2012a) for an illustration of the witness function.\n\n\nThis is a particularly useful result, since it allows the direct comparison between two alternative tests:\n\n\n1) the original MMD test, based on the asymptotic distribution of the norm of the witness function under the null distribution (obtained in practice by permutation)\n\n\n2)  the proposed test strategy, based on the expected chance level performance of this classifier on a held-out test set, where the test threshold is set to the appropriate quantile of the binomial distribution. \n\n\nWe look at the 2-D blobs dataset from the paper of Chwialkowski et al. (NIPS 2015), for 1000 samples, and the same kernel bandwidth for each approach. The results are:\n\n\nOriginal MMD test: 85% true positives\nClassification-based approach: 63% true positives.\n\n\nThis can easily be understood, for two reasons: first, binomial variables have relatively high variance, hence the resulting test may be conservative. A more direct way to detect chance level would be to measure that the decision boundary is as \u201cflat\u201d as one would expect if there were no differences in the distributions generating the samples, as done for the original MMD test. Second, a classification test uses half its samples to create a decision function, and the other half for testing, whereas the MMD test uses the entire sample for testing.\n\n\nAll this being said, the approach proposed by the paper remains a very interesting one, and well worth publishing. When a classifier is highly complex and/or expensive to train, it may not be possible to cheaply obtain a suitable test threshold based on the decision function.  The proposed binomial test threshold might then be the only way to construct a test. In this case, the gain in test sensitivity by using a powerful classifier could be more important the loss in power due to the two issues raised in the paragraph above.\n\n\nArthur Gretton, Wittawat Jitkrittum", "OTHER_KEYS": "Arthur Gretton"}, {"TITLE": "a review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\n\n## Paper summary\n\nThe paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.\n\nA theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.\n\n## Review summary \n\nThe paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.\n\n## Major comments / questions \n\nMy main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.\n\n* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: \"... the random variable n_te \\hat{t} follows a Binomial(nte, p)...\" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.\n\n* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. \n\nAnyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Is this work similar to the previous work?", "IS_META_REVIEW": false, "comments": "Is this work similar to the following work [1], [2]?\nThe papers [1], [2] propose an algorithm of doing a two sample test using density ratio.\nDensity ratio estimation is essentially same as the class probability estimation [3]. \nThat means that the algorithm of [1], [2] is a two sample test using classifiers.\nI'm sorry if I made a mistake.\nThank you. \n\n[1] Kanamori et.al, 2012\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "04 Dec 2016", "TITLE": "Hyper-parameters and more", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "comparison to MMD", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Theoretical questions on classifier two-sample test", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "04 Nov 2016", "TITLE": "Evaluating generative models with classifiers", "IS_META_REVIEW": false, "comments": "This is interesting work. Another reference for Section 5 of this paper: the use of binary classifiers to evaluate generative models was also used in Bowman et al. 2016 (", "OTHER_KEYS": "Luke Vilnis"}, {"IS_META_REVIEW": true, "comments": "## Paper summary\n\nThe paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.\n\nA theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.\n\n## Review summary \n\nThe paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.\n\n## Major comments / questions \n\nMy main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.\n\n* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: \"... the random variable n_te \\hat{t} follows a Binomial(nte, p)...\" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.\n\n* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. \n\nAnyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper is a valuable contribution to the literature.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Updated revision", "IS_META_REVIEW": false, "comments": "Dear all,\n\nThank you very much for your insightful feedback.\n\nWe have tried our best to incorporate all your suggestions into a revised version of our manuscript.\n\nPlease let us know how we could improve our submission further.\n\nSincerely,\nThe authors.", "OTHER_KEYS": "David Lopez-Paz"}, {"TITLE": "a very interesting framework; could improve clarity; significance to ICLR?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I would like first to apologize for the delay.\n\nSummary: A framework for two-samples statistical test using binary\nclassification is proposed. It allows multi-dimensional sample testing and\nan interpretability that other tests lack. A theoritical analysis is\nprovided and various empirical tests reported.\n\nA very interesting approach. I have however two main concerns.\n\nThe clarity of the presentation is obscured by too much content. It would\nbe more interesting if the presentation could be somewhat\nself-contained. You could consider making 2 papers out of this paper.\n\nSeriously, you cram a lot of experiments in this paper. But the setting\nof the experiments is not really explained. We are supposed to have read\nJitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All \nthis is okay but reduces your public to a very few.\n\nFor example, if I am not mistaken, you never explained what SCF is, despite\nthe fact that its performances are reported. \n\nAs a second point, given also that the number of submissions to this conference are exploding,\nI would like to challenge you with the following question:\n\nWhy is this work significant to the representation learning community?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A generally useful and interesting approach to 2-sample tests", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.  For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.  As such, a straightforward threshold can be derived for any classifier.  Finding a more powerful test then amounts to training a better classifier.  One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.\n\n+ The approach is sound and very general\n+ The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have\n\n- The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.  For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.  Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well\n * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)\n * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.\n\nArthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful.  If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.  The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "14 Dec 2016", "TITLE": "Discussion of variance for \"testing as classification\"", "IS_META_REVIEW": false, "comments": "We are writing both to provide a perspective on the proposed \u201cclassification approach\u201d to testing, and to expand on the reviews of AnonReviewer3 and AnonReviewer1, which suggested considering the relation between the MMD and classification. We discuss this link with reference to some earlier work, and then look into the broader implications for testing using classification error.\n\n\nFirst, the MMD can indeed be thought of as a classifier. In Section 2 of the paper:\n\n\nSriperumbudur, B., Fukumizu, K., Gretton, A., Lanckriet, G., and Schoelkopf, B., Kernel choice and classifiability for RKHS embeddings of probability distributions, NIPS 2009.\n\n\nwe show that the MMD is the negative of the optimal risk corresponding to a linear loss function, associated with the kernel classifier. The \u201cwitness function\u201d of the MMD can then be thought of as the decision function for that classifier, returning labels +1 for positive values (class P), and -1 for negative values (class Q). See figure 1 in Gretton et al (2012a) for an illustration of the witness function.\n\n\nThis is a particularly useful result, since it allows the direct comparison between two alternative tests:\n\n\n1) the original MMD test, based on the asymptotic distribution of the norm of the witness function under the null distribution (obtained in practice by permutation)\n\n\n2)  the proposed test strategy, based on the expected chance level performance of this classifier on a held-out test set, where the test threshold is set to the appropriate quantile of the binomial distribution. \n\n\nWe look at the 2-D blobs dataset from the paper of Chwialkowski et al. (NIPS 2015), for 1000 samples, and the same kernel bandwidth for each approach. The results are:\n\n\nOriginal MMD test: 85% true positives\nClassification-based approach: 63% true positives.\n\n\nThis can easily be understood, for two reasons: first, binomial variables have relatively high variance, hence the resulting test may be conservative. A more direct way to detect chance level would be to measure that the decision boundary is as \u201cflat\u201d as one would expect if there were no differences in the distributions generating the samples, as done for the original MMD test. Second, a classification test uses half its samples to create a decision function, and the other half for testing, whereas the MMD test uses the entire sample for testing.\n\n\nAll this being said, the approach proposed by the paper remains a very interesting one, and well worth publishing. When a classifier is highly complex and/or expensive to train, it may not be possible to cheaply obtain a suitable test threshold based on the decision function.  The proposed binomial test threshold might then be the only way to construct a test. In this case, the gain in test sensitivity by using a powerful classifier could be more important the loss in power due to the two issues raised in the paragraph above.\n\n\nArthur Gretton, Wittawat Jitkrittum", "OTHER_KEYS": "Arthur Gretton"}, {"TITLE": "a review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\n\n## Paper summary\n\nThe paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.\n\nA theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.\n\n## Review summary \n\nThe paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.\n\n## Major comments / questions \n\nMy main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.\n\n* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: \"... the random variable n_te \\hat{t} follows a Binomial(nte, p)...\" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.\n\n* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. \n\nAnyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Is this work similar to the previous work?", "IS_META_REVIEW": false, "comments": "Is this work similar to the following work [1], [2]?\nThe papers [1], [2] propose an algorithm of doing a two sample test using density ratio.\nDensity ratio estimation is essentially same as the class probability estimation [3]. \nThat means that the algorithm of [1], [2] is a two sample test using classifiers.\nI'm sorry if I made a mistake.\nThank you. \n\n[1] Kanamori et.al, 2012\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "04 Dec 2016", "TITLE": "Hyper-parameters and more", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "comparison to MMD", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Theoretical questions on classifier two-sample test", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "04 Nov 2016", "TITLE": "Evaluating generative models with classifiers", "IS_META_REVIEW": false, "comments": "This is interesting work. Another reference for Section 5 of this paper: the use of binary classifiers to evaluate generative models was also used in Bowman et al. 2016 (", "OTHER_KEYS": "Luke Vilnis"}], "authors": "David Lopez-Paz, Maxime Oquab", "accepted": true, "id": "450"}