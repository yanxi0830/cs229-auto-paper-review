{"conference": "ICLR 2017 conference submission", "title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving", "abstract": "Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of simple baseline machine learning models suited for the tasks (including logistic regression convolutional neural networks and recurrent neural networks). The results of our baseline models show the promise of applying machine learning to HOL theorem proving.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.\n\nIt would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.\n \n As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no \"overlap\" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting dataset, reasonable first steps", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. \n\nI am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2):\n-\tTerms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.  \n-\tSection 2 describes how the data was splits into train and test set. One thing which is unclear is \u2013 can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? \n\n\nIt also unclear how the deep learning models are applied. Let\u2019s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector \u2013 and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? \n\nMy another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. \n\nDid the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? \n\nOverall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer \u2013 it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning \u2013 but that\u2019s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it\u2019s not possible for me to judge how valuable is this dataset. From the references, it seems like it\u2019s drawn from a set of benchmark conjectures/proofs used in the ITP community \u2013 so its possibly a good dataset. \n\nMy current rating is a weak reject, but if the authors address my concerns I would change to an accept.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "23 Dec 2016 (modified: 22 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Great start, interested in future work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.\n\nIt would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "06 Dec 2016", "TITLE": "When is this applicable?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "The dataset extraction method", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.\n\nIt would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.\n \n As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no \"overlap\" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting dataset, reasonable first steps", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. \n\nI am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2):\n-\tTerms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.  \n-\tSection 2 describes how the data was splits into train and test set. One thing which is unclear is \u2013 can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? \n\n\nIt also unclear how the deep learning models are applied. Let\u2019s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector \u2013 and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? \n\nMy another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. \n\nDid the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? \n\nOverall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer \u2013 it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning \u2013 but that\u2019s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it\u2019s not possible for me to judge how valuable is this dataset. From the references, it seems like it\u2019s drawn from a set of benchmark conjectures/proofs used in the ITP community \u2013 so its possibly a good dataset. \n\nMy current rating is a weak reject, but if the authors address my concerns I would change to an accept.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "23 Dec 2016 (modified: 22 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Great start, interested in future work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.\n\nIt would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "06 Dec 2016", "TITLE": "When is this applicable?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "The dataset extraction method", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Cezary Kaliszyk, Fran\u00e7ois Chollet, Christian Szegedy", "accepted": true, "id": "494"}