{"conference": "ICLR 2017 conference submission", "title": "Revisiting Denoising Auto-Encoders", "abstract": "Denoising auto-encoders (DAE)s were proposed as a simple yet powerful way to obtain representations in an unsupervised manner by learning a map that approximates the clean inputs from their corrupted versions. However, the original objective function proposed for DAEs does not guarantee that denoising happens only at the encoding stages. We argue that a better representation can be obtained if the encoder is forced to carry out most of the denoising effort. Here, we propose a simple modification to the DAE's objective function that accomplishes the above goal.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The proposed modification of the denoising autoencoder objective is interesting. Shifting some of the burden to the encoder has potential, but the authors need to to show that this burden cannot be transferred to the decoder; they propose some ways of doing this in their response, but these should be explored. And more convincing, larger-scale results are needed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Incremental with too little empirical evidence and insufficiently developed info-theoretic argument.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn\u2019t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.  What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).\n\nOn the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Incremental work without thorough empirical results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The work introduced a new form of regularization for denoising autoencoders, which explicitly enforces robustness in the encoding phrase w.r.t. input perturbation. The author motivates the regularization term as minimizing the conditional entropy of the encoding given the input. The modifier denoising autoencoders is evaluated on some synthetic datasets as well as MNIST, along with regular auto-encoders and denoising autoencoders. The work is fairly similar to several existing extensions to auto-encoders, e.g., contractive auto encoders, which the author did not include in the comparison.   The experiment section needs more polishing. More details should be provided to help understand the figures in the section. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Well motivated, if incremental improvement", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "paper update", "IS_META_REVIEW": false, "comments": "An updated version of the paper taking into consideration the reviewers comments has been uploaded", "OTHER_KEYS": "Luis Gonzalo Sanchez Giraldo"}, {"DATE": "09 Dec 2016", "TITLE": "Contractive autoencoders", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "04 Dec 2016", "TITLE": "Details on autoencoder architecture", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "interpretation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The proposed modification of the denoising autoencoder objective is interesting. Shifting some of the burden to the encoder has potential, but the authors need to to show that this burden cannot be transferred to the decoder; they propose some ways of doing this in their response, but these should be explored. And more convincing, larger-scale results are needed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Incremental with too little empirical evidence and insufficiently developed info-theoretic argument.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn\u2019t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.  What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).\n\nOn the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Incremental work without thorough empirical results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The work introduced a new form of regularization for denoising autoencoders, which explicitly enforces robustness in the encoding phrase w.r.t. input perturbation. The author motivates the regularization term as minimizing the conditional entropy of the encoding given the input. The modifier denoising autoencoders is evaluated on some synthetic datasets as well as MNIST, along with regular auto-encoders and denoising autoencoders. The work is fairly similar to several existing extensions to auto-encoders, e.g., contractive auto encoders, which the author did not include in the comparison.   The experiment section needs more polishing. More details should be provided to help understand the figures in the section. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Well motivated, if incremental improvement", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "paper update", "IS_META_REVIEW": false, "comments": "An updated version of the paper taking into consideration the reviewers comments has been uploaded", "OTHER_KEYS": "Luis Gonzalo Sanchez Giraldo"}, {"DATE": "09 Dec 2016", "TITLE": "Contractive autoencoders", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "04 Dec 2016", "TITLE": "Details on autoencoder architecture", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "interpretation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Luis Gonzalo Sanchez Giraldo", "accepted": false, "id": "550"}