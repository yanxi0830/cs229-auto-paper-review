{"conference": "ICLR 2017 conference submission", "title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.\n--------------\nThis paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.\nI think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.\n\nI have the following suggestions/concerns about the paper:\n\n1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.\n\n2) A similar idea is also presented in a concurrent ICLR submission \"Variational Lossy Autoencoder\". It would be interesting to have a discussion included in the paper and compare these works.\n\n3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.\n\n4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.\n\nI will gladly raise the score if the authors address my concerns."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper provides a solution that combines best of latent variable models and auto-regressive models. The concept is executed well and will make a positive contribution to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "comments": "UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.\n--------------\nThis paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.\nI think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.\n\nI have the following suggestions/concerns about the paper:\n\n1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.\n\n2) A similar idea is also presented in a concurrent ICLR submission \"Variational Lossy Autoencoder\". It would be interesting to have a discussion included in the paper and compare these works.\n\n3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.\n\n4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.\n\nI will gladly raise the score if the authors address my concerns.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper combines a hierarchical Variational Autoencoder with PixelCNNs to model the distribution of natural images. \nThey report good (although not state of the art) likelihoods on natural images and briefly start to explore what information is encoded by the latent representations in the hierarchical VAE.\n\nI believe that combining the PixelCNN with a VAE, as was already suggested in the PixelCNN paper, is an important and interesting contribution. \nThe encoding of high-, mid- and low-level variations at the different latent stages is interesting but seems not terribly surprising, since the size of the image regions the latent variables model is also at the corresponding scale. Showing that the PixelCNN improves the latent representation of the VAE with regard to some interesting task would be a much stronger result. \nAlso, while the paper claims, that combining the PixelCNN with the VAE reduces the number of computationally expensive autoregressive layers, it remains unclear how much more efficient their whole model is than an PixelCNN with comparable likelihood.\n\nIn general, I find the clarity of the presentation wanting. For example, I agree with reviewer1 that the exact structure of their model remains unclear from the paper and would be difficult to reproduce. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Nice paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "All in all this is a nice paper.\n\nI think the model is quite clever, attempting to get the best of latent variable models and auto-regressive models. The implementation and specific architecture choices (as discussed in the pre-review) also seem reasonable.\nOn the experimental side, I would have liked to see something more than NLL measurements and samples - maybe show this is useful for other tasks such as classification?\n\nThough I don't think this is a huge leap forward this is certainly a nice paper and I recoemmend acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "some questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "14 Dec 2016"}, {"DATE": "09 Dec 2016", "TITLE": "Model architectures and questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "07 Nov 2016 (modified: 08 Nov 2016)", "TITLE": "Good paper", "IS_META_REVIEW": false, "comments": "This is a very interesting paper. It combines the best of both world: PixelCNN and VAE. State-of-the-art LL and visually appealing images.\n\nBelow are some comments and suggestions.\n\n1. In van dern Oord et al. 2016b they use an auto-encoder architecture with conditional PixelCNN as the decoder. It is not a variational one, though. It would be interesting to give a comparison between the model in this paper and their model. Are there any other differences, besides the KLD loss on latent variables and the (optional) hierarchical extension?\n\n2. The introduction of latent varibles z definitely helps modeling global structure, however it also breaks the chain rule so the exact likelihood cannnot be derived.\n\n3. How is the quality of representations learned by the encoder? Will the PixelCNN decoder help the encoder learn better representations, compared to Kingma et. al., \"Semi-supervised learning with deep generative models\"?\n\n3. Also, it would be great if you can show some reconstruction results, in addition to generated samples.\n\nBut overall I like this paper very much.", "OTHER_KEYS": "Xun Huang"}, {"IS_META_REVIEW": true, "comments": "UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.\n--------------\nThis paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.\nI think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.\n\nI have the following suggestions/concerns about the paper:\n\n1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.\n\n2) A similar idea is also presented in a concurrent ICLR submission \"Variational Lossy Autoencoder\". It would be interesting to have a discussion included in the paper and compare these works.\n\n3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.\n\n4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.\n\nI will gladly raise the score if the authors address my concerns."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper provides a solution that combines best of latent variable models and auto-regressive models. The concept is executed well and will make a positive contribution to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "comments": "UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.\n--------------\nThis paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.\nI think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.\n\nI have the following suggestions/concerns about the paper:\n\n1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.\n\n2) A similar idea is also presented in a concurrent ICLR submission \"Variational Lossy Autoencoder\". It would be interesting to have a discussion included in the paper and compare these works.\n\n3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.\n\n4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.\n\nI will gladly raise the score if the authors address my concerns.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper combines a hierarchical Variational Autoencoder with PixelCNNs to model the distribution of natural images. \nThey report good (although not state of the art) likelihoods on natural images and briefly start to explore what information is encoded by the latent representations in the hierarchical VAE.\n\nI believe that combining the PixelCNN with a VAE, as was already suggested in the PixelCNN paper, is an important and interesting contribution. \nThe encoding of high-, mid- and low-level variations at the different latent stages is interesting but seems not terribly surprising, since the size of the image regions the latent variables model is also at the corresponding scale. Showing that the PixelCNN improves the latent representation of the VAE with regard to some interesting task would be a much stronger result. \nAlso, while the paper claims, that combining the PixelCNN with the VAE reduces the number of computationally expensive autoregressive layers, it remains unclear how much more efficient their whole model is than an PixelCNN with comparable likelihood.\n\nIn general, I find the clarity of the presentation wanting. For example, I agree with reviewer1 that the exact structure of their model remains unclear from the paper and would be difficult to reproduce. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Nice paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "All in all this is a nice paper.\n\nI think the model is quite clever, attempting to get the best of latent variable models and auto-regressive models. The implementation and specific architecture choices (as discussed in the pre-review) also seem reasonable.\nOn the experimental side, I would have liked to see something more than NLL measurements and samples - maybe show this is useful for other tasks such as classification?\n\nThough I don't think this is a huge leap forward this is certainly a nice paper and I recoemmend acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "some questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "14 Dec 2016"}, {"DATE": "09 Dec 2016", "TITLE": "Model architectures and questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "07 Nov 2016 (modified: 08 Nov 2016)", "TITLE": "Good paper", "IS_META_REVIEW": false, "comments": "This is a very interesting paper. It combines the best of both world: PixelCNN and VAE. State-of-the-art LL and visually appealing images.\n\nBelow are some comments and suggestions.\n\n1. In van dern Oord et al. 2016b they use an auto-encoder architecture with conditional PixelCNN as the decoder. It is not a variational one, though. It would be interesting to give a comparison between the model in this paper and their model. Are there any other differences, besides the KLD loss on latent variables and the (optional) hierarchical extension?\n\n2. The introduction of latent varibles z definitely helps modeling global structure, however it also breaks the chain rule so the exact likelihood cannnot be derived.\n\n3. How is the quality of representations learned by the encoder? Will the PixelCNN decoder help the encoder learn better representations, compared to Kingma et. al., \"Semi-supervised learning with deep generative models\"?\n\n3. Also, it would be great if you can show some reconstruction results, in addition to generated samples.\n\nBut overall I like this paper very much.", "OTHER_KEYS": "Xun Huang"}], "authors": "Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, Aaron Courville", "accepted": true, "id": "353"}