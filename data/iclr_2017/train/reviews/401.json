{"conference": "ICLR 2017 conference submission", "title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.  We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "EDIT: Updated score. See additional comment.\n\nI quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.\n\nHowever the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:\n\n- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.\n\n- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.\n\n- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.\n\n- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.\n\n- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does \"didn't work\" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.\n\n- A question about jump points: \nThe I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.\n\nSince the main idea is very interesting, I will be happy to update my score if the above concerns are addressed."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Interesting paper and clear accept. Not recommended for an oral presentation because of weaknesses in the empirical contribution that make the significance of the results unclear.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Feb 2017", "TITLE": "Updates on Inception V1 Results", "IS_META_REVIEW": false, "comments": "We have revised the paper with updated results on experiments with Inception V1 network, which continues to show promise.", "OTHER_KEYS": "Balaji Krishnamurthy"}, {"TITLE": "Review", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. ", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "RECOMMENDATION_UNOFFICIAL": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper reads well and the idea is new.\nSadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing. \nThe training of the introspection network could have been described in more detail. \nAlso, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments. That would have made the results more convincing.\nDue to the novelty of the idea, I recommend the paper. I would increase the rating if an updated draft addresses the mentioned issues.", "IS_ANNOTATED": true, "TITLE": "novel idea but requires more details / experimentation", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Valuable insight but needs careful analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "EDIT: Updated score. See additional comment.\n\nI quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.\n\nHowever the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:\n\n- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.\n\n- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.\n\n- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.\n\n- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.\n\n- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does \"didn't work\" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.\n\n- A question about jump points: \nThe I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.\n\nSince the main idea is very interesting, I will be happy to update my score if the above concerns are addressed. ", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 4, "comments": "", "IS_ANNOTATED": true, "TITLE": "Experimenting with hyperparameters, batch norm", "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"DATE": "03 Dec 2016", "TITLE": "Simiar questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"TITLE": "Questions about the experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 3, "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "EDIT: Updated score. See additional comment.\n\nI quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.\n\nHowever the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:\n\n- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.\n\n- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.\n\n- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.\n\n- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.\n\n- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does \"didn't work\" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.\n\n- A question about jump points: \nThe I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.\n\nSince the main idea is very interesting, I will be happy to update my score if the above concerns are addressed."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Interesting paper and clear accept. Not recommended for an oral presentation because of weaknesses in the empirical contribution that make the significance of the results unclear.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Feb 2017", "TITLE": "Updates on Inception V1 Results", "IS_META_REVIEW": false, "comments": "We have revised the paper with updated results on experiments with Inception V1 network, which continues to show promise.", "OTHER_KEYS": "Balaji Krishnamurthy"}, {"TITLE": "Review", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. ", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "RECOMMENDATION_UNOFFICIAL": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper reads well and the idea is new.\nSadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing. \nThe training of the introspection network could have been described in more detail. \nAlso, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments. That would have made the results more convincing.\nDue to the novelty of the idea, I recommend the paper. I would increase the rating if an updated draft addresses the mentioned issues.", "IS_ANNOTATED": true, "TITLE": "novel idea but requires more details / experimentation", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Valuable insight but needs careful analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "EDIT: Updated score. See additional comment.\n\nI quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.\n\nHowever the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:\n\n- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.\n\n- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.\n\n- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.\n\n- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.\n\n- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does \"didn't work\" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.\n\n- A question about jump points: \nThe I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.\n\nSince the main idea is very interesting, I will be happy to update my score if the above concerns are addressed. ", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 4, "comments": "", "IS_ANNOTATED": true, "TITLE": "Experimenting with hyperparameters, batch norm", "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"DATE": "03 Dec 2016", "TITLE": "Simiar questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"TITLE": "Questions about the experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 3, "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}], "authors": "Abhishek Sinha, Aahitagni Mukherjee, Mausoom Sarkar, Balaji Krishnamurthy", "accepted": true, "id": "401"}