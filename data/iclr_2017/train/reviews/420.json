{"conference": "ICLR 2017 conference submission", "title": "Frustratingly Short Attention Spans in Neural Language Modeling", "abstract": "Current language modeling architectures often use recurrent neural networks. Recently, various methods for incorporating differentiable memory into these architectures have been proposed. When predicting the next token, these models query information from a memory of the recent history and thus can facilitate learning mid- and long-range dependencies. However, conventional attention models produce a single output vector per time step that is used for predicting the next token as well as the key and value of a differentiable memory of the history of tokens. In this paper, we propose a key-value attention mechanism that produces separate representations for the key and value of a memory, and for a representation that encodes the next-word distribution. This usage of past memories outperforms existing memory-augmented neural language models on two corpora. Yet, we found that it mainly utilizes past memory only of the previous five representations. This led to the unexpected main finding that a much simpler model which simply uses a concatenation of output representations from the previous three-time steps is on par with more sophisticated memory-augmented neural language models.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it\u2019s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn\u2019t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I\u2019m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size."}, {"DATE": "17 Feb 2017", "TITLE": "Nice read!", "IS_META_REVIEW": false, "comments": "(I read the paper after knowing it got accepted to ICLR 2017)\n\nThis paper is quite illuminating to me, as it shows that language modeling may not be the right task to show whether a model has the ability to hold and use long-term information. However, it is up for debate whether that is because of the specific datasets used in the paper.", "OTHER_KEYS": "Xiang Zhang"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Reviewers found this paper to be a rigorous and \"thorough experimental analysis\" of context-length in language modelingv through the lens of an \"interesting extension to standard attention mechanism\". The paper reopens and makes more problematic widely accepted but rarely verified claims of the importance of long-term dependency.\n \n Pros:\n - \"Well-explained\" and clear presentation\n - Use of an \"inventive baseline\" in the form a ngram rnn \n - Use a impactful corpus for long-term language modeling\n \n Cons: \n - Several of the ideas have been explored previously.\n - Some open questions about the soundness of parameters (rev 1)\n - Requests for deeper analysis on data sets released with the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 4, "SUBSTANCE": 4, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors\u2019 responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "MEANINGFUL_COMPARISON": 1, "RECOMMENDATION_UNOFFICIAL": 1, "comments": "This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.\n\nThe authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named \"George Abbot\", yet \"Abbot\" isn't mentioned again until the next sentence 40 tokens later, and then the next \"Abbot\" is 15 tokens from there. Most gaps between occurrences of \"Abbot\" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.\n\nThis is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review: Frustratingly Short Attention Spans in Neural Language Modeling", "comments": "The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it\u2019s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn\u2019t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I\u2019m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size.\n", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Questions re: BPTT length, Wikipedia corpus, and CBT results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 1, "comments": "", "RECOMMENDATION_UNOFFICIAL": 1, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 2}, {"IMPACT": 4, "SUBSTANCE": 4, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "Few questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 3}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "How is h_t exactly computed and did you compare with higher dimensional context vectors?", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it\u2019s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn\u2019t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I\u2019m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size."}, {"DATE": "17 Feb 2017", "TITLE": "Nice read!", "IS_META_REVIEW": false, "comments": "(I read the paper after knowing it got accepted to ICLR 2017)\n\nThis paper is quite illuminating to me, as it shows that language modeling may not be the right task to show whether a model has the ability to hold and use long-term information. However, it is up for debate whether that is because of the specific datasets used in the paper.", "OTHER_KEYS": "Xiang Zhang"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Reviewers found this paper to be a rigorous and \"thorough experimental analysis\" of context-length in language modelingv through the lens of an \"interesting extension to standard attention mechanism\". The paper reopens and makes more problematic widely accepted but rarely verified claims of the importance of long-term dependency.\n \n Pros:\n - \"Well-explained\" and clear presentation\n - Use of an \"inventive baseline\" in the form a ngram rnn \n - Use a impactful corpus for long-term language modeling\n \n Cons: \n - Several of the ideas have been explored previously.\n - Some open questions about the soundness of parameters (rev 1)\n - Requests for deeper analysis on data sets released with the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 4, "SUBSTANCE": 4, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors\u2019 responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "MEANINGFUL_COMPARISON": 1, "RECOMMENDATION_UNOFFICIAL": 1, "comments": "This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.\n\nThe authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named \"George Abbot\", yet \"Abbot\" isn't mentioned again until the next sentence 40 tokens later, and then the next \"Abbot\" is 15 tokens from there. Most gaps between occurrences of \"Abbot\" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.\n\nThis is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review: Frustratingly Short Attention Spans in Neural Language Modeling", "comments": "The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it\u2019s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn\u2019t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I\u2019m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size.\n", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Questions re: BPTT length, Wikipedia corpus, and CBT results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 1, "comments": "", "RECOMMENDATION_UNOFFICIAL": 1, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 2}, {"IMPACT": 4, "SUBSTANCE": 4, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "Few questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 3}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "How is h_t exactly computed and did you compare with higher dimensional context vectors?", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}], "authors": "Micha\u0142 Daniluk, Tim Rockt\u00e4schel, Johannes Welbl, Sebastian Riedel", "accepted": true, "id": "420"}