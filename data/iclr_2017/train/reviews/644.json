{"conference": "ICLR 2017 conference submission", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models", "abstract": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top B candidates. This tends to result in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. We observe that our method not only improved diversity but also finds better top 1 solutions by controlling for the exploration and exploitation of the search space. Moreover, these gains are achieved with minimal computational or memory overhead com- pared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation, conversation and visual question generation using both standard quantitative metrics and qualitative human studies. We find that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.\n\nOn the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.\n\nThe novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.\n\nThe diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.\n\nThe experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).\n\n---------------\nupdate: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "09 Jan 2017", "TITLE": "Revision of submission with additional baseline comparison and discussions ", "IS_META_REVIEW": false, "comments": "Following suggestions from Reviewer 2, we have revised our submission to include comparison to the modified beam search objective proposed by Wu et al., 2016 (Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation ). Further details are provided in the paper and in the reply to Reviewer-2\u2019s comments. We thank the reviewer for this suggestion. \n\nWe also conduct additional experiments that study the correlation of SPICE with caption length and the variation of oracle accuracy with beam budget. While the first experiment suggests that longer sequences do not necessarily result in better scoring captions, the latter experiment shows that DBS utilizes the beam budget efficiently \u2014 obtaining higher oracle accuracies at much lower beam budgets compared to other decoding techniques. \n", "OTHER_KEYS": "Ashwin Kalyan Vijayakumar"}, {"TITLE": "good problem - but results are somewhat unclear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\nThe paper addresses an important problem - namely on how to improve diversity in responses. It is applaudable that the authors show results on several tasks showing the applicability across different problems. \n\nIn my view there are two weaknesses at this point\n\n1) the improvements (for essentially all tasks) seem rather minor and do not really fit the overall claim of the paper\n\n2) the approach seems quite ad hoc and it unclear to me if this is something that will and should be widely adopted. Having said this the gist of the proposed solution seems interesting but somewhat premature. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "potentially interesting idea but lacking comparisons against other classic search techniques beyond simple beam search", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\n\n[ Summary ]\n\nThis paper presents a new modified beam search algorithm that promotes diverse beam candidates. It is a well known problem \u2014with both RNNs and also non-neural language models\u2014 that beam search tends to generate beam candidates that are very similar with each other, which can cause two separate but related problems: (1) search error: beam search may not be able to discover a globally optimal solution as they can easily fall out of the beam early on, (2) simple, common, non-diverse output: the resulting output text tends to be generic and common.\n\nThis paper aims to address the second problem (2) by modifying the search objective function itself so that there is a distinct term that scores diversity among the beam candidates. In other words, the goal of the presented algorithm is not to reduce the search error of the original objective function. In contrast, stack decoding and future cost estimation, common practices in phrase-based SMT, aim to address the search error problem.\n\n[ Merits ]\n\nI think the Diverse Beam Search (DBS) algorithm proposed by the authors has some merits. It may be useful when we cannot rely on traditional beam search on the original objective function either because the trained model is not strong enough, or because of the search error, or because the objective itself does not align with the goal of the application.\n\n[ Weaknesses ]\n\nIt is however not entirely clear how the proposed method compares against more traditional approaches like stack decoding and future cost estimation, on tasks like machine translation, as the authors compare their algorithm mainly against L&J\u2019s diverse LM models and simple beam search.\n\nIn fact, modification to the objective function has been applied even in the neural MT context. For example, see equation (14) in page 12 of the following paper:\n\n\"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\" (", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "a relatively new problem, but proposed seems to be too simplistic", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.\n\nOn the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.\n\nThe novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.\n\nThe diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.\n\nThe experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).\n\n---------------\nupdate: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 26 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "Li & Jurafsky comparison", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "no question at this point", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "29 Nov 2016", "TITLE": "experiment comparison", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.\n\nOn the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.\n\nThe novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.\n\nThe diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.\n\nThe experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).\n\n---------------\nupdate: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "09 Jan 2017", "TITLE": "Revision of submission with additional baseline comparison and discussions ", "IS_META_REVIEW": false, "comments": "Following suggestions from Reviewer 2, we have revised our submission to include comparison to the modified beam search objective proposed by Wu et al., 2016 (Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation ). Further details are provided in the paper and in the reply to Reviewer-2\u2019s comments. We thank the reviewer for this suggestion. \n\nWe also conduct additional experiments that study the correlation of SPICE with caption length and the variation of oracle accuracy with beam budget. While the first experiment suggests that longer sequences do not necessarily result in better scoring captions, the latter experiment shows that DBS utilizes the beam budget efficiently \u2014 obtaining higher oracle accuracies at much lower beam budgets compared to other decoding techniques. \n", "OTHER_KEYS": "Ashwin Kalyan Vijayakumar"}, {"TITLE": "good problem - but results are somewhat unclear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\nThe paper addresses an important problem - namely on how to improve diversity in responses. It is applaudable that the authors show results on several tasks showing the applicability across different problems. \n\nIn my view there are two weaknesses at this point\n\n1) the improvements (for essentially all tasks) seem rather minor and do not really fit the overall claim of the paper\n\n2) the approach seems quite ad hoc and it unclear to me if this is something that will and should be widely adopted. Having said this the gist of the proposed solution seems interesting but somewhat premature. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "potentially interesting idea but lacking comparisons against other classic search techniques beyond simple beam search", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\n\n[ Summary ]\n\nThis paper presents a new modified beam search algorithm that promotes diverse beam candidates. It is a well known problem \u2014with both RNNs and also non-neural language models\u2014 that beam search tends to generate beam candidates that are very similar with each other, which can cause two separate but related problems: (1) search error: beam search may not be able to discover a globally optimal solution as they can easily fall out of the beam early on, (2) simple, common, non-diverse output: the resulting output text tends to be generic and common.\n\nThis paper aims to address the second problem (2) by modifying the search objective function itself so that there is a distinct term that scores diversity among the beam candidates. In other words, the goal of the presented algorithm is not to reduce the search error of the original objective function. In contrast, stack decoding and future cost estimation, common practices in phrase-based SMT, aim to address the search error problem.\n\n[ Merits ]\n\nI think the Diverse Beam Search (DBS) algorithm proposed by the authors has some merits. It may be useful when we cannot rely on traditional beam search on the original objective function either because the trained model is not strong enough, or because of the search error, or because the objective itself does not align with the goal of the application.\n\n[ Weaknesses ]\n\nIt is however not entirely clear how the proposed method compares against more traditional approaches like stack decoding and future cost estimation, on tasks like machine translation, as the authors compare their algorithm mainly against L&J\u2019s diverse LM models and simple beam search.\n\nIn fact, modification to the objective function has been applied even in the neural MT context. For example, see equation (14) in page 12 of the following paper:\n\n\"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\" (", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "a relatively new problem, but proposed seems to be too simplistic", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.\n\nOn the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.\n\nThe novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.\n\nThe diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.\n\nThe experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).\n\n---------------\nupdate: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 26 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "Li & Jurafsky comparison", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "no question at this point", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "29 Nov 2016", "TITLE": "experiment comparison", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Ashwin K Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, Dhruv Batra", "accepted": false, "id": "644"}