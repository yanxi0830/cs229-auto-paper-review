{"conference": "ICLR 2017 conference submission", "title": "Information Dropout: learning optimal representations through noise", "abstract": "We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the \"information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper\". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "Paper update: new experiments", "IS_META_REVIEW": false, "comments": "Following the suggestions of the reviewers, we updated the paper with new experiments and plots.\n\nFirst, to empirically validate that, by increasing the value of the parameter \\beta, we can obtain representations that are increasingly minimal and invariant while remaining discriminative (sufficient), we created a new dataset, called \u2018Occluded CIFAR\u2019. The experiment in Sec. 6 shows precisely this effect, thus validating the theoretical intuition. Indeed, by increasing \\beta, we also prevent overfitting and the overall quality of the representation actually improves.\n\nIn Figure 5 in Appendix D, we added a comparison between Information Dropout and binary dropout using the same settings as [Springenberg et al., 2014]. For both methods we obtain a slightly better testing error than the original paper and, as also observed in the previous experiments, Information Dropout performs comparably or better than dropout.\n\nIn Figure 6, we plot the amount of information flowing through the dropout layers of a CNN as the number of filters varies. This plots supports some of the theoretical intuitions, and we show empirically that information dropout automatically selects a lower noise level for smaller networks, and that the units in the higher layers contain on average more information relative to the task than units in the bottom layers.", "OTHER_KEYS": "Alessandro Achille"}, {"DATE": "03 Jan 2017", "TITLE": "More details on experiments and further tests", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n", "OTHER_KEYS": "Alessandro Achille"}, {"TITLE": "Insightful theoretical derivation, experiments can be improved.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting theory, but experimental results not (yet) very convincing, unfortunately.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting idea, but not very convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Paper update", "IS_META_REVIEW": false, "comments": "A personal communication asked whether there are cases in which a stochastic representation of the data can obtain a better value of the IB Lagrangian than any deterministic representation; in response to this, we added a remark in Section 3 saying that this indeed can happen. \n\nIn response to a question by the reviewer, we added to Section 2 a few examples of nuisances that act as a group on the data.\n\nWe updated the MNIST and CIFAR experiments: all the qualitative results are the same as before, but we slightly changed the hyperparameters and the optimization method to provide a more accurate and fairer comparison between the algorithms.\n\nFinally, we added an appendix to fill a gap in the narrative between Equation (2),  where the two distributions in the KL term were the actual prior and posterior of z, and Section 4, where we assume an approximated prior whose parameters are learned independently. Specifically, we show that if the approximated prior of the activations is chosen to be factorized, as we do, then our loss function differs from the actual IB Lagrangian by the total correlation of z. As a consequence, our approximation is correct when the components of z are mutually independent, and the loss function we use actually encourages this independence.\n\nWe would like to thank all the people that gave us early feedback on the paper.", "OTHER_KEYS": "Alessandro Achille"}, {"DATE": "03 Dec 2016", "TITLE": "Nuisances as group actions.", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the \"information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper\". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "Paper update: new experiments", "IS_META_REVIEW": false, "comments": "Following the suggestions of the reviewers, we updated the paper with new experiments and plots.\n\nFirst, to empirically validate that, by increasing the value of the parameter \\beta, we can obtain representations that are increasingly minimal and invariant while remaining discriminative (sufficient), we created a new dataset, called \u2018Occluded CIFAR\u2019. The experiment in Sec. 6 shows precisely this effect, thus validating the theoretical intuition. Indeed, by increasing \\beta, we also prevent overfitting and the overall quality of the representation actually improves.\n\nIn Figure 5 in Appendix D, we added a comparison between Information Dropout and binary dropout using the same settings as [Springenberg et al., 2014]. For both methods we obtain a slightly better testing error than the original paper and, as also observed in the previous experiments, Information Dropout performs comparably or better than dropout.\n\nIn Figure 6, we plot the amount of information flowing through the dropout layers of a CNN as the number of filters varies. This plots supports some of the theoretical intuitions, and we show empirically that information dropout automatically selects a lower noise level for smaller networks, and that the units in the higher layers contain on average more information relative to the task than units in the bottom layers.", "OTHER_KEYS": "Alessandro Achille"}, {"DATE": "03 Jan 2017", "TITLE": "More details on experiments and further tests", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n", "OTHER_KEYS": "Alessandro Achille"}, {"TITLE": "Insightful theoretical derivation, experiments can be improved.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting theory, but experimental results not (yet) very convincing, unfortunately.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting idea, but not very convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Paper update", "IS_META_REVIEW": false, "comments": "A personal communication asked whether there are cases in which a stochastic representation of the data can obtain a better value of the IB Lagrangian than any deterministic representation; in response to this, we added a remark in Section 3 saying that this indeed can happen. \n\nIn response to a question by the reviewer, we added to Section 2 a few examples of nuisances that act as a group on the data.\n\nWe updated the MNIST and CIFAR experiments: all the qualitative results are the same as before, but we slightly changed the hyperparameters and the optimization method to provide a more accurate and fairer comparison between the algorithms.\n\nFinally, we added an appendix to fill a gap in the narrative between Equation (2),  where the two distributions in the KL term were the actual prior and posterior of z, and Section 4, where we assume an approximated prior whose parameters are learned independently. Specifically, we show that if the approximated prior of the activations is chosen to be factorized, as we do, then our loss function differs from the actual IB Lagrangian by the total correlation of z. As a consequence, our approximation is correct when the components of z are mutually independent, and the loss function we use actually encourages this independence.\n\nWe would like to thank all the people that gave us early feedback on the paper.", "OTHER_KEYS": "Alessandro Achille"}, {"DATE": "03 Dec 2016", "TITLE": "Nuisances as group actions.", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Alessandro Achille, Stefano Soatto", "accepted": false, "id": "726"}