{"conference": "ICLR 2017 conference submission", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "abstract": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. \n\nDetailed comments:\n \n-Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. \n\n-Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?\n\n-Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section\n\n-Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. \n\n-Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.\n\nTypos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work presents a method for reducing the target vocabulary for abstractive summarization by employing a read-again copy mechanism. Reviewers felt that the paper was lacking in several regards particularly focusing on issues clarity and originality. \n \n Issues raised:\n - Several reviewers focused on clarity/writing issues of the work, highlighting inconsistencies of notation, justifications, and extraneous material. They recommend a rewrite of the work. \n - The question of originality and novelty is important. The copy mechanism has now been invented many times. Reviewers argue that simply applying this to summary is not enough, or at least not the focus of ICLR. The same question is posed about self-attention.\n - In terms of quality, there are concerns about comparisons to more recent baselines and getting the experiments to run correctly on Gigaword.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Extra Experiment for Copy Mechanism", "IS_META_REVIEW": false, "comments": "In the paper, we've shown that our copy mechanism can reduce the decoder size. Here, we show that it can also help to reduce the encoder vocabulary size. We fixed the decoder size as 15K, and we can see that with copy mechanism we are able to reduce encoder vocabulary size from 110k to 15k without loss of performance, while model without copy has severe decreasing. The reason is that our copy mechanism is able to extract an rare word\u2019s embedding from its context, and thus it is enough to learn and store the high-frequency words\u2019 embeddings in our model.\n\n\t    Rouge1\t\tRouge2\t\t    RougeL\nsize\t    nocopy    copy\tnocopy\tcopy    nocopy\t    copy\n5k\t    21.8\t    26.2\t9.8\t\t12.0\t    21.6\t    25.0\n15k\t    23.8\t    27.8\t10.7\t\t12.5\t    22.5\t    26.0\n30k\t    23.8\t    27.5\t10.7\t\t12.6\t    22.3\t    26.0\n110k    25.3         27.4     11.8\t        12.6\t    23.7\t    25.7\n", "OTHER_KEYS": "Wenyuan Zeng"}, {"DATE": "07 Jan 2017", "TITLE": "Authors: Please post rebuttal", "IS_META_REVIEW": false, "comments": "Reviewers are currently in discussion. Please post a rebuttal to any comments or questions in their reviews.\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Review for the Efficient Summarization with Read-Again ...", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.\n\nContributions:\nThe main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.\n\nWriting:\nThe text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. \n\nPros:\n- The proposed model is a simple extension to the model to the model proposed in [2] for summarization.\n- The results are better than the baselines.\n\nCons:\n- The improvements are not that large.\n- Justifications are not strong enough.\n- The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal.\n- The paper is very application oriented.\n\nQuestion:\n- How does the training speed when compared to the regular LSTM?\n\nSome Criticisms:\n\nA similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn\u2019t consider the application of that on the summarization task a significant contribution.  The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage.\nAs authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from \u201cread-again\u201d mechanism or the use of \u201cpointing\u201d. \nThe paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.\nIt is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are \nThe writing of this paper needs more work. In general, it is not very well-written. \n\nMinor comments:\n\nSome of the corrections that I would recommend fixing,\n\nOn page 4: \u201c\u2026 better than a single value \u2026 \u201d \u2014> \u201c\u2026 scalar gating \u2026\u201d\nOn page 4: \u201c\u2026 single value lacks the ability to model the variances among these dimensions.\u201d \u2014> \u201c\u2026 scalar gating couldn\u2019t capture the \u2026.\u201d\nOn page 6: \u201c \u2026 where h_0^2 and h_0^'2 are initial zero vectors \u2026 \u201c \u2014> \u201c\u2026 h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence \u2026\"\n\nThere are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2.\n\nBetter naming of the models in Table 1 is needed.\nThe location of Table 1 is a bit off.\n\n[1] Zaremba, Wojciech, and Ilya Sutskever. \"Reinforcement learning neural Turing machines.\" arXiv preprint arXiv:1505.00521 362 (2015). \n[2] Gulcehre, Caglar, et al. \"Pointing the Unknown Words.\" arXiv preprint arXiv:1603.08148 (2016).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Solid incremental work but need better writing and more experiments to be more convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms.\n\n1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to  re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM.\n \n2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance.\n \nThis paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, \n- how much computational costs does the 2nd pass reading add to the end-to-end system? \n- How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed?\n- There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Official Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. \n\nDetailed comments:\n \n-Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. \n\n-Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?\n\n-Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section\n\n-Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. \n\n-Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.\n\nTypos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "A few questions regarding to the model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Copy Mechanism", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "12 Nov 2016", "TITLE": "can you please comment on how p_t is used in the copy mechanisem", "IS_META_REVIEW": false, "comments": "Please see ", "OTHER_KEYS": "Ehud Ben-Reuven"}, {"IS_META_REVIEW": true, "comments": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. \n\nDetailed comments:\n \n-Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. \n\n-Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?\n\n-Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section\n\n-Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. \n\n-Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.\n\nTypos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work presents a method for reducing the target vocabulary for abstractive summarization by employing a read-again copy mechanism. Reviewers felt that the paper was lacking in several regards particularly focusing on issues clarity and originality. \n \n Issues raised:\n - Several reviewers focused on clarity/writing issues of the work, highlighting inconsistencies of notation, justifications, and extraneous material. They recommend a rewrite of the work. \n - The question of originality and novelty is important. The copy mechanism has now been invented many times. Reviewers argue that simply applying this to summary is not enough, or at least not the focus of ICLR. The same question is posed about self-attention.\n - In terms of quality, there are concerns about comparisons to more recent baselines and getting the experiments to run correctly on Gigaword.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Extra Experiment for Copy Mechanism", "IS_META_REVIEW": false, "comments": "In the paper, we've shown that our copy mechanism can reduce the decoder size. Here, we show that it can also help to reduce the encoder vocabulary size. We fixed the decoder size as 15K, and we can see that with copy mechanism we are able to reduce encoder vocabulary size from 110k to 15k without loss of performance, while model without copy has severe decreasing. The reason is that our copy mechanism is able to extract an rare word\u2019s embedding from its context, and thus it is enough to learn and store the high-frequency words\u2019 embeddings in our model.\n\n\t    Rouge1\t\tRouge2\t\t    RougeL\nsize\t    nocopy    copy\tnocopy\tcopy    nocopy\t    copy\n5k\t    21.8\t    26.2\t9.8\t\t12.0\t    21.6\t    25.0\n15k\t    23.8\t    27.8\t10.7\t\t12.5\t    22.5\t    26.0\n30k\t    23.8\t    27.5\t10.7\t\t12.6\t    22.3\t    26.0\n110k    25.3         27.4     11.8\t        12.6\t    23.7\t    25.7\n", "OTHER_KEYS": "Wenyuan Zeng"}, {"DATE": "07 Jan 2017", "TITLE": "Authors: Please post rebuttal", "IS_META_REVIEW": false, "comments": "Reviewers are currently in discussion. Please post a rebuttal to any comments or questions in their reviews.\n\nThanks!", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Review for the Efficient Summarization with Read-Again ...", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.\n\nContributions:\nThe main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.\n\nWriting:\nThe text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. \n\nPros:\n- The proposed model is a simple extension to the model to the model proposed in [2] for summarization.\n- The results are better than the baselines.\n\nCons:\n- The improvements are not that large.\n- Justifications are not strong enough.\n- The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal.\n- The paper is very application oriented.\n\nQuestion:\n- How does the training speed when compared to the regular LSTM?\n\nSome Criticisms:\n\nA similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn\u2019t consider the application of that on the summarization task a significant contribution.  The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage.\nAs authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from \u201cread-again\u201d mechanism or the use of \u201cpointing\u201d. \nThe paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.\nIt is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are \nThe writing of this paper needs more work. In general, it is not very well-written. \n\nMinor comments:\n\nSome of the corrections that I would recommend fixing,\n\nOn page 4: \u201c\u2026 better than a single value \u2026 \u201d \u2014> \u201c\u2026 scalar gating \u2026\u201d\nOn page 4: \u201c\u2026 single value lacks the ability to model the variances among these dimensions.\u201d \u2014> \u201c\u2026 scalar gating couldn\u2019t capture the \u2026.\u201d\nOn page 6: \u201c \u2026 where h_0^2 and h_0^'2 are initial zero vectors \u2026 \u201c \u2014> \u201c\u2026 h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence \u2026\"\n\nThere are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2.\n\nBetter naming of the models in Table 1 is needed.\nThe location of Table 1 is a bit off.\n\n[1] Zaremba, Wojciech, and Ilya Sutskever. \"Reinforcement learning neural Turing machines.\" arXiv preprint arXiv:1505.00521 362 (2015). \n[2] Gulcehre, Caglar, et al. \"Pointing the Unknown Words.\" arXiv preprint arXiv:1603.08148 (2016).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Solid incremental work but need better writing and more experiments to be more convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms.\n\n1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to  re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM.\n \n2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance.\n \nThis paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, \n- how much computational costs does the 2nd pass reading add to the end-to-end system? \n- How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed?\n- There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Official Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. \n\nDetailed comments:\n \n-Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. \n\n-Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?\n\n-Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section\n\n-Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. \n\n-Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.\n\nTypos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "A few questions regarding to the model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Copy Mechanism", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "12 Nov 2016", "TITLE": "can you please comment on how p_t is used in the copy mechanisem", "IS_META_REVIEW": false, "comments": "Please see ", "OTHER_KEYS": "Ehud Ben-Reuven"}], "authors": "Wenyuan Zeng, Wenjie Luo, Sanja Fidler, Raquel Urtasun", "accepted": false, "id": "630"}