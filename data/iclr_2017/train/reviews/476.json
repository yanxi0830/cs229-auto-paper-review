{"conference": "ICLR 2017 conference submission", "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers unanimously recommend accepting this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Experimental paper with interesting results. Well written. Solid experiments. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Description.\nThis paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. \n\nStrong  points.\n- The experiments are carefully done with thorough selection of hyperparameters. \n- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).\n- The paper is well and clearly written.\n\nWeak points:\n- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?\n\nOriginality:\n- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.\n\nQuality:\n- The experiments are well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The results go against some of the conclusions from previous work, so should be published and discussed.\n\nOverall:\nExperimental paper with interesting results. Well written. Solid experiments. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Experimental comparison of shallow, deep, and (non)-convolutional architectures with a fixed parameter budget", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. \nTo this end the authors conducted a series of experiments on the CIFAR10 dataset.\nThey find that there is a significant performance gap between the two approaches, in favour of deep CNNs. \nThe experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.\nThey also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.\n\nMy take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.\n\nAn interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper.\n\nIt was not quite clear to me why were the experiments were limited to use  30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.\n\nThe authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment. \nCould the authors argue why they think this to be the case? \nOne could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture. \nSince MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.\nThis issue could for example be examined on ImageNet when varying the amount of training data.\nAlso, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.\n\nExperiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Careful study proving, to the extent possible, a fascinating point", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "11 Dec 2016", "TITLE": "deep, but not too deep ?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "01 Dec 2016", "TITLE": "Regularization effect of distillation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers unanimously recommend accepting this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Experimental paper with interesting results. Well written. Solid experiments. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Description.\nThis paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. \n\nStrong  points.\n- The experiments are carefully done with thorough selection of hyperparameters. \n- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).\n- The paper is well and clearly written.\n\nWeak points:\n- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?\n\nOriginality:\n- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.\n\nQuality:\n- The experiments are well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The results go against some of the conclusions from previous work, so should be published and discussed.\n\nOverall:\nExperimental paper with interesting results. Well written. Solid experiments. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Experimental comparison of shallow, deep, and (non)-convolutional architectures with a fixed parameter budget", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. \nTo this end the authors conducted a series of experiments on the CIFAR10 dataset.\nThey find that there is a significant performance gap between the two approaches, in favour of deep CNNs. \nThe experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.\nThey also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.\n\nMy take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.\n\nAn interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper.\n\nIt was not quite clear to me why were the experiments were limited to use  30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.\n\nThe authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment. \nCould the authors argue why they think this to be the case? \nOne could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture. \nSince MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.\nThis issue could for example be examined on ImageNet when varying the amount of training data.\nAlso, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.\n\nExperiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Careful study proving, to the extent possible, a fascinating point", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "11 Dec 2016", "TITLE": "deep, but not too deep ?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "01 Dec 2016", "TITLE": "Regularization effect of distillation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson, Rich Caruana", "accepted": true, "id": "476"}