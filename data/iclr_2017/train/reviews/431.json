{"conference": "ICLR 2017 conference submission", "title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers were consistent in their praise of the paper. They asked for newer architectures, e.g. ResNet, DenseNet. The authors released an update with a Caffe converter which provides access to a wide range of CNNs and residual networks (ResNet-50 and DenseNet examples are provided). This seems like an incredibly useful tool and very glad it is open source. Paper is a clear accept.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for reading and commenting on the paper! \n\nAnonReviewer1: \u201cIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet\u201d\n\nResponse: \nWe released a converter to port Caffe model specs to Paleo, and thus Paleo now supports a wide range of CNNs and residual networks.  Our GitHub repository provides several examples, including ResNet-50 and DenseNet via the Caffe converter. Details can be found in our open source repository (", "OTHER_KEYS": "Hang Qi"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. \nThe paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature.\nFurthermore, their code is open-source and the live demo is looking good. \nThe authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.\nIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "No question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"TITLE": "Final review: Sound paper but a very simple model, few experiments at start but more added.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.\n\nThe ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.\n\nMore experiments were added, so I'm updating my score.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Technically sound. Only useful under the assumption that the code is released.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "05 Dec 2016 (modified: 06 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "Clarification - Table 2", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "28 Nov 2016", "TITLE": "Any plans to open-source the model?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers were consistent in their praise of the paper. They asked for newer architectures, e.g. ResNet, DenseNet. The authors released an update with a Caffe converter which provides access to a wide range of CNNs and residual networks (ResNet-50 and DenseNet examples are provided). This seems like an incredibly useful tool and very glad it is open source. Paper is a clear accept.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for reading and commenting on the paper! \n\nAnonReviewer1: \u201cIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet\u201d\n\nResponse: \nWe released a converter to port Caffe model specs to Paleo, and thus Paleo now supports a wide range of CNNs and residual networks.  Our GitHub repository provides several examples, including ResNet-50 and DenseNet via the Caffe converter. Details can be found in our open source repository (", "OTHER_KEYS": "Hang Qi"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. \nThe paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature.\nFurthermore, their code is open-source and the live demo is looking good. \nThe authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.\nIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "No question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"TITLE": "Final review: Sound paper but a very simple model, few experiments at start but more added.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.\n\nThe ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.\n\nMore experiments were added, so I'm updating my score.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Technically sound. Only useful under the assumption that the code is released.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "05 Dec 2016 (modified: 06 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "Clarification - Table 2", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "28 Nov 2016", "TITLE": "Any plans to open-source the model?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Hang Qi, Evan R. Sparks, Ameet Talwalkar", "accepted": true, "id": "431"}