{"conference": "ICLR 2017 conference submission", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks, which builds an ensemble of Residual Networks by growing the member network at each round of boosting. The proposed approach combines recent developements in Residual Networks - a method for creating very deep networks by including a shortcut layer between different groups of layers - with the Deep Incremental Boosting, which has been proposed as a methodology to train fast ensembles of networks of increasing depth through the use of boosting. We demonstrate that the synergy of Residual Networks and Deep Incremental Boosting has better potential than simply boosting a Residual Network of fixed structure or using the equivalent Deep Incremental Boosting without the shortcut layers.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "The contribution is incremental with no impressive comparison results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.\n\nThe method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. \n\nThe empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.\n\nIn addition, it is not clear how sensitive the boosting to the selection of injection point.\n\nThis paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.\n\nPros\n-provides some preliminary results for boosting of Res Nets\nCons\n-not sufficiently novel: an incremental approach \n-empirical analysis is not satisfactory", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Lack of comparison", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting ideas, unconvincing execution, lack of comparisons to the literature", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.\n\nThe main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.\n\nIt'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "04 Dec 2016", "TITLE": "comparisons and setup", "IS_META_REVIEW": false, "comments": "- Can you give the details of the experiment setup e.g. parameters to be tuned, algorithm to train at each step of boosting etc? Also can you give the details of networks architecture and references? \n- Can you elaborate on comparison to state of resNet variants, dense convolutional network?\n- Can you give also comparison on training time?\n- Do you have any result on Imagenet?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Terminology and comparisons", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "The contribution is incremental with no impressive comparison results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.\n\nThe method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. \n\nThe empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.\n\nIn addition, it is not clear how sensitive the boosting to the selection of injection point.\n\nThis paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.\n\nPros\n-provides some preliminary results for boosting of Res Nets\nCons\n-not sufficiently novel: an incremental approach \n-empirical analysis is not satisfactory", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Lack of comparison", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting ideas, unconvincing execution, lack of comparisons to the literature", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.\n\nThe main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.\n\nIt'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "04 Dec 2016", "TITLE": "comparisons and setup", "IS_META_REVIEW": false, "comments": "- Can you give the details of the experiment setup e.g. parameters to be tuned, algorithm to train at each step of boosting etc? Also can you give the details of networks architecture and references? \n- Can you elaborate on comparison to state of resNet variants, dense convolutional network?\n- Can you give also comparison on training time?\n- Do you have any result on Imagenet?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Terminology and comparisons", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Alan Mosca, George D. Magoulas", "accepted": false, "id": "683"}