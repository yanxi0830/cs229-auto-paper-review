{"conference": "ICLR 2017 conference submission", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.\n\nAs the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance."}, {"TITLE": "Very interesting results", "OTHER_KEYS": "Jeremy Noring", "comments": "Two things I'd like to see.\n\n1) Specifics about the JPEG and JPEG2000 implementations used, and how they were configured.  One major weakness I see in many papers is they do not include specific encoders and configuration used in comparisons.  Without knowing this, it's hard to know if the comparison was done with a suitably strong JPEG implementation that was properly configured, for example.\n\n2) The comparison to JPEG2000 is unfortunately not that interesting, since that codec does not have widespread usage and likely never will.  A better comparison would be with WebP performance.  Or, even better, both.\n\nVery nice results.  Is a software implementation of this available to play with?", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Feb 2017", "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is one of the two top papers in my stack and I recommend it for oral presentation. The reviewers were particularly careful and knowledgable of the topic.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "Revised paper uploaded", "IS_META_REVIEW": false, "comments": "We have uploaded a revised version of the paper.  Major changes are as follows:\n\n* Introduction/Discussion: we\u2019ve made some adjustments to the text, further motivating the use of MSE (instead of perceptual error), emphasizing that the addition of uniform noise is used only for optimization (all compression results are based on quantized and entropy-coded values), and elaborating on the visual appearance of the compressed results.\n\n* Images: we\u2019ve now run tests on additional images, including the \u201cclassic\u201d examples {Barbara, Lena, Mandrill, Peppers}, as well as more of our own photographs.  All examples have been added to the online set, which has been consolidated at ", "OTHER_KEYS": "Johannes Ball\u00e9"}, {"TITLE": "Very good paper", "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is the most convincing paper on image compression with deep neural networks that I have read so far. The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework. The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG). I would expect this paper to have a very good impact. \n\nYes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions. I think it is important to clearly state how NN compare to best previous methods. From the submitted version, I still don't know how both categories of methods are positioned. ", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "03 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Great progress performance-wise but missing details", "comments": "This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding. In addition, the paper discusses the approach\u2019s relationship to variational autoencoders.\n\nGiven that the approach to rate-distortion optimization has already been published, the novelty of this submission is arguably not very high (correct me if I missed a new trick). In some ways, this paper even represents a step backward, since earlier work optimized for a perceptual metric where here MSE is used. However, the results are a visible improvement over JPEG 2000, and I don\u2019t know of any other learned encoding which has been shown to achieve this level of performance. The paper is very well written.\n\nEquation 10 appears to be wrong and I believe the partition function should depend on g_s(y; theta). This would mean that the approach is not equivalent to a VAE for non-Euclidean metrics.\n\nWhat was the reason for optimizing MSE rather than a perceptual metric as in previous work? Given the author\u2019s backgrounds, it is surprising that even the evaluation was only performed in terms of PSNR.\n\nWhat is the contribution of adaptive entropy coding versus the effect of deeper encoders and decoders? This seems like an important piece of information, so it would be interesting to see the performance without adaptation as in the previous paper. More detail on the adaptive coder and its effects should be provided, and I will be happy to give a higher score when the authors do.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "28 Dec 2016 (modified: 11 Jan 2017)", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Official review", "comments": "This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of \"rate\" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.\n\nThe authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific \"form\" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A good paper with an interesting premise, some novel methods and good results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.\n\nAs the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "12 Dec 2016", "TITLE": "Some minor questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Optimizing different tradeoff vs distortion for different rates", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"TITLE": "Comparison and details", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 1, "comments": "", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.\n\nAs the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance."}, {"TITLE": "Very interesting results", "OTHER_KEYS": "Jeremy Noring", "comments": "Two things I'd like to see.\n\n1) Specifics about the JPEG and JPEG2000 implementations used, and how they were configured.  One major weakness I see in many papers is they do not include specific encoders and configuration used in comparisons.  Without knowing this, it's hard to know if the comparison was done with a suitably strong JPEG implementation that was properly configured, for example.\n\n2) The comparison to JPEG2000 is unfortunately not that interesting, since that codec does not have widespread usage and likely never will.  A better comparison would be with WebP performance.  Or, even better, both.\n\nVery nice results.  Is a software implementation of this available to play with?", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Feb 2017", "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is one of the two top papers in my stack and I recommend it for oral presentation. The reviewers were particularly careful and knowledgable of the topic.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "Revised paper uploaded", "IS_META_REVIEW": false, "comments": "We have uploaded a revised version of the paper.  Major changes are as follows:\n\n* Introduction/Discussion: we\u2019ve made some adjustments to the text, further motivating the use of MSE (instead of perceptual error), emphasizing that the addition of uniform noise is used only for optimization (all compression results are based on quantized and entropy-coded values), and elaborating on the visual appearance of the compressed results.\n\n* Images: we\u2019ve now run tests on additional images, including the \u201cclassic\u201d examples {Barbara, Lena, Mandrill, Peppers}, as well as more of our own photographs.  All examples have been added to the online set, which has been consolidated at ", "OTHER_KEYS": "Johannes Ball\u00e9"}, {"TITLE": "Very good paper", "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is the most convincing paper on image compression with deep neural networks that I have read so far. The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework. The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG). I would expect this paper to have a very good impact. \n\nYes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions. I think it is important to clearly state how NN compare to best previous methods. From the submitted version, I still don't know how both categories of methods are positioned. ", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "03 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Great progress performance-wise but missing details", "comments": "This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding. In addition, the paper discusses the approach\u2019s relationship to variational autoencoders.\n\nGiven that the approach to rate-distortion optimization has already been published, the novelty of this submission is arguably not very high (correct me if I missed a new trick). In some ways, this paper even represents a step backward, since earlier work optimized for a perceptual metric where here MSE is used. However, the results are a visible improvement over JPEG 2000, and I don\u2019t know of any other learned encoding which has been shown to achieve this level of performance. The paper is very well written.\n\nEquation 10 appears to be wrong and I believe the partition function should depend on g_s(y; theta). This would mean that the approach is not equivalent to a VAE for non-Euclidean metrics.\n\nWhat was the reason for optimizing MSE rather than a perceptual metric as in previous work? Given the author\u2019s backgrounds, it is surprising that even the evaluation was only performed in terms of PSNR.\n\nWhat is the contribution of adaptive entropy coding versus the effect of deeper encoders and decoders? This seems like an important piece of information, so it would be interesting to see the performance without adaptation as in the previous paper. More detail on the adaptive coder and its effects should be provided, and I will be happy to give a higher score when the authors do.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "28 Dec 2016 (modified: 11 Jan 2017)", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Official review", "comments": "This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of \"rate\" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.\n\nThe authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific \"form\" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.\n", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A good paper with an interesting premise, some novel methods and good results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.\n\nAs the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "12 Dec 2016", "TITLE": "Some minor questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Optimizing different tradeoff vs distortion for different rates", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"TITLE": "Comparison and details", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 1, "comments": "", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "Johannes Ball\u00e9, Valero Laparra, Eero P. Simoncelli", "accepted": true, "id": "305"}