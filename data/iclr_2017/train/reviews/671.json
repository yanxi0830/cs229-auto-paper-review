{"conference": "ICLR 2017 conference submission", "title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability \"incorporate dynamic recurrent connections through the definition of the transition system\" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple \"software engineering with no inherent \"free things\" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are \"reimplementations of things in the literature\". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google\u2019s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe \u201craison d\u2019etre,\u201d in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework \u2014 emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I\u2019d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6", "comments": "Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "02 Dec 2016", "TITLE": "Limitation of the framework", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6"}, {"IS_META_REVIEW": true, "comments": "Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability \"incorporate dynamic recurrent connections through the definition of the transition system\" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple \"software engineering with no inherent \"free things\" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are \"reimplementations of things in the literature\". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google\u2019s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe \u201craison d\u2019etre,\u201d in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework \u2014 emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I\u2019d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6", "comments": "Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "02 Dec 2016", "TITLE": "Limitation of the framework", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6"}], "authors": "Lingpeng Kong, Chris Alberti, Daniel Andor, Ivan Bogatyy, David Weiss", "accepted": false, "id": "671"}