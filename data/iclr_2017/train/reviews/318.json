{"conference": "ICLR 2017 conference submission", "title": "Learning Graphical State Transitions", "abstract": "Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Dec 2016", "TITLE": "Loss function requires strong supervision", "IS_META_REVIEW": false, "comments": "Section 4.1 Supervision mentioned that the proposed method requires additional supervision by providing the correct graph at each timestep. \n\nI have some questions about whether this is practical: \n\n1. In some applications, human may or may not be able to provide the \"correct graph\" at each timestep. What if the human supervisor can only provide suboptimal graphs during training? How would that affect the results?\n\n2. What was the result of not providing such intermediate supervision? Was the result slightly worse than the current result, or significantly different?\n\n3. Such additional supervision would also require more \"work\" by the human. How can the amount of work be quantified, so the reader understands the implications of having to provide additional supervision?\n\n4. I would appreciate if the need of such strong supervision was mentioned earlier in the paper, perhaps during the introduction or literature review, to give the reader some \"warning\". Some literature review/comparison of other approaches that require strong supervision would also be appreciated.\n\nThanks for the good work, I enjoyed reading it!\n\n\n", "OTHER_KEYS": "David Liu"}, {"DATE": "21 Dec 2016", "TITLE": "Revision uploaded", "IS_META_REVIEW": false, "comments": "The paper has been updated with the following changes:\n\n- Fixed typo in the equations given in B.2 and B.2.1.\n- Added a reference to work done by Giles et al. in Section 6.\n- Clarified the operation of direct reference in Sections 3 and 4.\n- Added a link to the source code for the model (in Appendix B).\n- Minor wording changes in the Abstract, Introduction, and Sections 3 and 4.", "OTHER_KEYS": "Daniel D. Johnson"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Architecture which allows to learn graph->graph tasks,  improves state of the art on babi", "comments": "The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.\n\nThis idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.\n\nThe preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "TITLE": "Complex implementation of a differentiable memory as a graph with promising preliminary results.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "19 Dec 2016 (modified: 20 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.\n\nThe approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "13 Dec 2016", "TITLE": "Revision uploaded", "IS_META_REVIEW": false, "comments": "Based on the reviewer comments, I have uploaded a new revision of the paper, with the following changes:\n\n- Added figures depicting the differentiable graph format and each of the graph transformations\n- Added a section comparing the GGT-NN model to existing works\n- Included baselines for the bAbI tasks, with discussion of the relative performance of the GGT-NN model\n- Clarified the feasibility of alternative network configurations (relative to Algorithm 1)\n- Moved implementation details of the graph transformations into the appendix\n- Clearly separated and simplified the background information section\n- Clarified the training procedure for the GGT-NN model\n\nThank you very much for your feedback.", "OTHER_KEYS": "Daniel D. Johnson"}, {"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "TITLE": "Inference vs. learning", "IS_META_REVIEW": false, "DATE": "12 Dec 2016", "CLARITY": 5}, {"DATE": "03 Dec 2016", "TITLE": "Hard paper...", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Cool paper!", "IS_META_REVIEW": false, "comments": "Great job with the paper. I love the idea.\n\nIt seems the main weakness is the lack of broader context. Specifically,\n\nBaselines: \n- it'd be nice to see bAbI results from other papers in Table 1, to quickly get a sense for how your results compare\n- have you thought about baseline methods for the experiments in 4.2?\n- are there any other tasks like bAbI where you can compare GGT-NN to existing results from the literature?\n\nRelated work:\n- I'd appreciate a related work section with discussion of the similarities and differences to other models with more or less structured memory representations including, e.g., Memory Networks, Hierarchical Attentive Memory, and Differentiable Neural Computers (DNC).\n\nFinally, have you considered training your model with a mixture of strong and weak supervision? Maybe you could get away with just a few instances labeled with the strong supervision described in 3.1, with the rest labeled only by the correct answer?", "OTHER_KEYS": "Daniel Tarlow"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Dec 2016", "TITLE": "Loss function requires strong supervision", "IS_META_REVIEW": false, "comments": "Section 4.1 Supervision mentioned that the proposed method requires additional supervision by providing the correct graph at each timestep. \n\nI have some questions about whether this is practical: \n\n1. In some applications, human may or may not be able to provide the \"correct graph\" at each timestep. What if the human supervisor can only provide suboptimal graphs during training? How would that affect the results?\n\n2. What was the result of not providing such intermediate supervision? Was the result slightly worse than the current result, or significantly different?\n\n3. Such additional supervision would also require more \"work\" by the human. How can the amount of work be quantified, so the reader understands the implications of having to provide additional supervision?\n\n4. I would appreciate if the need of such strong supervision was mentioned earlier in the paper, perhaps during the introduction or literature review, to give the reader some \"warning\". Some literature review/comparison of other approaches that require strong supervision would also be appreciated.\n\nThanks for the good work, I enjoyed reading it!\n\n\n", "OTHER_KEYS": "David Liu"}, {"DATE": "21 Dec 2016", "TITLE": "Revision uploaded", "IS_META_REVIEW": false, "comments": "The paper has been updated with the following changes:\n\n- Fixed typo in the equations given in B.2 and B.2.1.\n- Added a reference to work done by Giles et al. in Section 6.\n- Clarified the operation of direct reference in Sections 3 and 4.\n- Added a link to the source code for the model (in Appendix B).\n- Minor wording changes in the Abstract, Introduction, and Sections 3 and 4.", "OTHER_KEYS": "Daniel D. Johnson"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Architecture which allows to learn graph->graph tasks,  improves state of the art on babi", "comments": "The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.\n\nThis idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.\n\nThe preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "TITLE": "Complex implementation of a differentiable memory as a graph with promising preliminary results.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "19 Dec 2016 (modified: 20 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.\n\nThe approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "13 Dec 2016", "TITLE": "Revision uploaded", "IS_META_REVIEW": false, "comments": "Based on the reviewer comments, I have uploaded a new revision of the paper, with the following changes:\n\n- Added figures depicting the differentiable graph format and each of the graph transformations\n- Added a section comparing the GGT-NN model to existing works\n- Included baselines for the bAbI tasks, with discussion of the relative performance of the GGT-NN model\n- Clarified the feasibility of alternative network configurations (relative to Algorithm 1)\n- Moved implementation details of the graph transformations into the appendix\n- Clearly separated and simplified the background information section\n- Clarified the training procedure for the GGT-NN model\n\nThank you very much for your feedback.", "OTHER_KEYS": "Daniel D. Johnson"}, {"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "TITLE": "Inference vs. learning", "IS_META_REVIEW": false, "DATE": "12 Dec 2016", "CLARITY": 5}, {"DATE": "03 Dec 2016", "TITLE": "Hard paper...", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Cool paper!", "IS_META_REVIEW": false, "comments": "Great job with the paper. I love the idea.\n\nIt seems the main weakness is the lack of broader context. Specifically,\n\nBaselines: \n- it'd be nice to see bAbI results from other papers in Table 1, to quickly get a sense for how your results compare\n- have you thought about baseline methods for the experiments in 4.2?\n- are there any other tasks like bAbI where you can compare GGT-NN to existing results from the literature?\n\nRelated work:\n- I'd appreciate a related work section with discussion of the similarities and differences to other models with more or less structured memory representations including, e.g., Memory Networks, Hierarchical Attentive Memory, and Differentiable Neural Computers (DNC).\n\nFinally, have you considered training your model with a mixture of strong and weak supervision? Maybe you could get away with just a few instances labeled with the strong supervision described in 3.1, with the rest labeled only by the correct answer?", "OTHER_KEYS": "Daniel Tarlow"}], "authors": "Daniel D. Johnson", "accepted": true, "id": "318"}