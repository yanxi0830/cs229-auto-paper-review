{"conference": "ICLR 2017 conference submission", "title": "The loss surface of residual networks: Ensembles and the role of batch normalization", "abstract": "Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensemble are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network\u2019s depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Summary:\nIn this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.\n\nClarity:\nThis paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.\n\nSpecific Comments:\n- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)\n\n- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)\n\n- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets. \n \n However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.\n \n I thus cannot recommend acceptance of this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "final evaluation", "IS_META_REVIEW": false, "comments": "Authors' responses did not make me change my mind. I still think it is a clear rejection paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "promising insightful results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThis paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets.\n\nThe paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. \n\nIt is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper.\n\nThis work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "20 Dec 2016", "TITLE": "path-independence assumptions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "interesting extension of the result of Choromanska et al. but too incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "18 Dec 2016", "TITLE": "Title: revised version", "IS_META_REVIEW": false, "comments": "We just revised our manuscript based on the very meaningful discussions we had with the reviewing team. For convenience, the changes are marked in red.\n\nIn addition to incorporating everything we have promised so far and adding discussion based on the reviewers\u2019 suggestions, we were able to relax the assumptions used for the results of Sec.  4. This change addresses a concern raised by AnonReviewer3 and was done by using the assumption-free expression for the output of the network in Eq. 9 in order to compute the loss in Lemma 3.\n\nOverall, believe that what can be considered as the main results of our manuscript, i.e., the dynamic behavior during the training of ResNets, is now essentially assumption free and is also well supported experimentally. We thank the reviewers for their crucial role in improving the manuscript.\n", "OTHER_KEYS": "Etai Littwin"}, {"TITLE": "Interesting theoretical analysis (with new supporting experiments) but presented in a slightly confusing fashion.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:\nIn this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.\n\nClarity:\nThis paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.\n\nSpecific Comments:\n- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)\n\n- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)\n\n- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "16 Dec 2016", "TITLE": "More runs", "IS_META_REVIEW": false, "comments": "We looked back at the comment of AnonReviewer3. We are the first to analyze the dynamic behavior of ResNets during training and point to a novel phenomenon, which we analyze theoretically. We reveal both the underlying reason for this phenomenon and its profound effect on the training process. However, we might not have demonstrated convincingly enough the existence of this phenomenon on conventional datasets. We therefore ran a few last minute experiments to show exactly this.\n\nWe took the ResNet code of ", "OTHER_KEYS": "Lior Wolf"}, {"DATE": "07 Dec 2016", "TITLE": "Question on equation (8) gamma_r ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "04 Dec 2016", "TITLE": "assumptions and claims", "IS_META_REVIEW": false, "comments": "-  The issue regarding unrealistic assumptions of spin glass analysis for landscape of neural networks was posed as an open problem in COLT 2015.[1] Can you discuss the effect of this problem for your analysis?\n\n- Regarding assumption that minimal of (12) should hold, is this assumption is realistic or not?\n\n-Choromanska(AISTATS 2015) supports the claims based on theoretical results with many empirical results however you do not provide such analysis. Can you support your claims with reasonable empirical results?\n\n- What is the empirical setup for Figure 1?\n\n-Fractal net paper on arxiv claim that residuals are incidental. Can you elaborate on that based on your framework? What about densely connected conv networks(huang,2016 )? \n\n[1] A. Choromanska, Y. LeCun, G. Ben Arous, Open Problem: The landscape of the loss surfaces of multilayer networks, in the Conference on Learning Theory (COLT), Open Problems, 2015\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "Summary:\nIn this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.\n\nClarity:\nThis paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.\n\nSpecific Comments:\n- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)\n\n- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)\n\n- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets. \n \n However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.\n \n I thus cannot recommend acceptance of this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "final evaluation", "IS_META_REVIEW": false, "comments": "Authors' responses did not make me change my mind. I still think it is a clear rejection paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "promising insightful results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThis paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets.\n\nThe paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. \n\nIt is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper.\n\nThis work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "20 Dec 2016", "TITLE": "path-independence assumptions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "interesting extension of the result of Choromanska et al. but too incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "18 Dec 2016", "TITLE": "Title: revised version", "IS_META_REVIEW": false, "comments": "We just revised our manuscript based on the very meaningful discussions we had with the reviewing team. For convenience, the changes are marked in red.\n\nIn addition to incorporating everything we have promised so far and adding discussion based on the reviewers\u2019 suggestions, we were able to relax the assumptions used for the results of Sec.  4. This change addresses a concern raised by AnonReviewer3 and was done by using the assumption-free expression for the output of the network in Eq. 9 in order to compute the loss in Lemma 3.\n\nOverall, believe that what can be considered as the main results of our manuscript, i.e., the dynamic behavior during the training of ResNets, is now essentially assumption free and is also well supported experimentally. We thank the reviewers for their crucial role in improving the manuscript.\n", "OTHER_KEYS": "Etai Littwin"}, {"TITLE": "Interesting theoretical analysis (with new supporting experiments) but presented in a slightly confusing fashion.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:\nIn this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.\n\nClarity:\nThis paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.\n\nSpecific Comments:\n- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)\n\n- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)\n\n- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "16 Dec 2016", "TITLE": "More runs", "IS_META_REVIEW": false, "comments": "We looked back at the comment of AnonReviewer3. We are the first to analyze the dynamic behavior of ResNets during training and point to a novel phenomenon, which we analyze theoretically. We reveal both the underlying reason for this phenomenon and its profound effect on the training process. However, we might not have demonstrated convincingly enough the existence of this phenomenon on conventional datasets. We therefore ran a few last minute experiments to show exactly this.\n\nWe took the ResNet code of ", "OTHER_KEYS": "Lior Wolf"}, {"DATE": "07 Dec 2016", "TITLE": "Question on equation (8) gamma_r ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "04 Dec 2016", "TITLE": "assumptions and claims", "IS_META_REVIEW": false, "comments": "-  The issue regarding unrealistic assumptions of spin glass analysis for landscape of neural networks was posed as an open problem in COLT 2015.[1] Can you discuss the effect of this problem for your analysis?\n\n- Regarding assumption that minimal of (12) should hold, is this assumption is realistic or not?\n\n-Choromanska(AISTATS 2015) supports the claims based on theoretical results with many empirical results however you do not provide such analysis. Can you support your claims with reasonable empirical results?\n\n- What is the empirical setup for Figure 1?\n\n-Fractal net paper on arxiv claim that residuals are incidental. Can you elaborate on that based on your framework? What about densely connected conv networks(huang,2016 )? \n\n[1] A. Choromanska, Y. LeCun, G. Ben Arous, Open Problem: The landscape of the loss surfaces of multilayer networks, in the Conference on Learning Theory (COLT), Open Problems, 2015\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Etai Littwin, Lior Wolf", "accepted": false, "id": "622"}