{"conference": "ICLR 2017 conference submission", "title": "Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU", "abstract": "We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.\n\nI applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.\n\nA disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper suggests modifications to A3C - mainly by adding a system of queues to batch data - to obtain higher learning throughput on a GPU. The paper gives a solid systems analysis of the algorithm and performance. Although the method does not necessarily lead to higher Atari performance, it is a valuable and relevant contribution to the field, since it provides a stable, fast, open-source implementation of the currently best-performing deep RL algorithm.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "17 Jan 2017 (modified: 20 Jan 2017)", "TITLE": "Note for reviewers", "IS_META_REVIEW": false, "comments": "Please note we just updated the latest version of the paper with the latest version of Fig. 5.\nThis is now coherent with results reported in Figs. 6/7/8, where we achieve higher convergence rate by using a different learning rate.\n\nPlease also notice the code has been officially released on Github: ", "OTHER_KEYS": "iuri frosio"}, {"DATE": "14 Jan 2017", "TITLE": "Answer to reviewers", "IS_META_REVIEW": false, "comments": "We first thank all the reviewers for the interesting comments they provided. All three reviewers agree on the potential importance of the proposed algorithm, but they also suggest that more experimental data are needed to demonstrate that GA3C achieves results in the same ballpark as the original A3C. This turns out to be particularly important in the light of the fact that the policy lag in GA3C can potentially introduce instabilities, as noted by Reviewer #1.\n\nWe have included a better description of this in Section 4.4 of the revised paper, as well as analyzed it in a newly added Section 5.3, while providing additional experiments to verify the game performance of GA3C and its stability. As pointed out by the reviewers and already highlighted in the paper, establishing a validation protocol that allows comparing the final scores achieved by GA3C with the scores achieved by other training algorithms is complex or even infeasible, as many implementation details are not shared, as well as validation protocols are poorly reproducible. Beyond that, the final score (still reported in Table 3) tells only part of the story, and therefore we espouse the idea shared by Reviewer #1 and #3 of reporting learning curves as a function of the time. The newly added Fig. 6 and 7 show the learning curves for GA3C and several games as a function of the time. The reader can easily compare these graphs with those reported in Mnih\u2019s paper, as well as in other venues mentioned by the reviewers (e.g. github). The score vs. time curves show a similar dynamic for GA3C and A3C, although it is also important mentioning that curves reported in Mnih\u2019s paper represent the average of the top 5 curves in a set of 50 with various learning rates, which we are unable to match due limitations in available computational resources. Since systems, system loads, environments and hyper-parameters are so widely varying, we also believe that releasing the GA3C code (which we plan to do in the next days) will allow other researchers to systematically evaluate the performances of GA3C and facilitate a fair comparison with new techniques.\n\nReviewer #1\u2019s main concern is about instabilities introduced in GA3C. The training curves reported in the paper indeed show a few instabilities occurring in the training process, but these represent a common phenomenon in RL. Beyond analyzing in Section 4.4 how the policy lag in GA3C can generate these instabilities, we now show experimentally that such instabilities do not affect some games (e.g. Pong), while they can invalidate the learning process in other ones (e.g. Breakout), especially in the case of a large learning rate (Fig. 7 and 8). We found that properly tuning the algorithm hyper-parameters (and in particular the learning rate) dramatically limits the effect of instabilities on the learning process. GA3C would likely benefit from a thorough learning rate search as presented in Mnih\u2019s paper over a set of 50 different learning rates for A3C. Beyond this, we also show how batching on training data (see the newly introduced Section 5.3, and Fig. 7 and 8 in particular) can be used to limit the effect of these instabilities, at the price of a slower convergence rate.\n\nSpecific answers to the individual reviewers follow.\n\n===============================\nReviewer 1\n===============================\n\nWe agree with the reviewer that instabilities may affect the learning process. This is indeed a commonly observed phenomenon in RL. Given the fact that GA3C and A3C are indeed (slightly) different algorithms (as now clearly explained in Section 4.4 of the paper), we followed the reviewer\u2019s suggestion and performed a thorough evaluation of the instabilities of the learning process on several games. Experimental results are reported in Section 5.3 and in Fig. 7 and 8 in particular. These show that, for some games (e.g. Pong), the delayed computation of the gradient terms in GA3C (see Section 4.4) does not introduce convergence issues, that are however present in other games (e.g. Breakout). However, a proper selection of the algorithm hyper-parameters, and of the learning rate in particular, is sufficient to achieve results that are comparable to those achieved by A3C (see Table 3). Quite interestingly, the optimal learning rate for GA3C results to be smaller than the one reported in Mnih\u2019s paper for A3C (as it can be seen in Fig. 6) - this somehow confirms that A3C and GA3C are indeed slightly different algorithms, and GA3C requires careful parameter tuning to obtain optimal results. We also demonstrated that batching of the training data can be used to limit the instabilities, at the price of a slower convergence rate.\n\n===============================\nReviewer 2\n===============================\n\nFollowing the suggestion of the other two reviewers, and sharing the conviction that replicating the Deep Mind evaluation protocol is non-trivial, we now report in the paper more training curves for several games (Fig. 6) as a function of the training time. This allows the reader to perform a direct comparison between our training curves and the ones reported in Mnih\u2019s paper, independently from the hardware system used for training. Although, the reader should be careful about this comparison since Mnih\u2019s graphs show top 5 out of 50 runs which is hard to compare with the small number of runs considered in Fig. 6. These curves demonstrate that training time is comparable while achieving a similar training score. The analysis of the theoretical discrepancies between A3C and GA3C (Section 4.4) and its experimental evaluation in terms of instabilities (Section 5.3) should also serve to clarify that, once the hyper-parameters are properly set, GA3C is comparable to A3C in terms of performance.\nAs for the question about the protocol used for evaluation the score of our trained agents, we now better specify in the caption of table 3 and in the text that we are reporting the average score over 30 different runs for the best agent we trained with GA3C. A direct comparison using the protocol followed in Mnih\u2019s paper is not feasible, as the authors did not share the \u201chuman start condition\u201d for each game.\n\n===============================\nReviewer 3\n===============================\n\nWe agree with the reviewer that showing score vs. time plot would be more informative than reporting only the final score as in Table 3. Therefore, we added Fig. 6 to the paper, showing that, with the proper selection of the hyper-parameters, the dynamic of the learning curves of GA3C is similar to that shown in the original A3C paper by Mnih et al. We believe that other, more precise comparisons in terms of learning speed require an optimized implementation as well as a broad hyper-parameter search (in the style of the original A3C plot, showing top 5 results among a set of 50 learners trained with different learning rates), which is out of the scope for this paper. We also decided to keep Table 3 in the paper - although, as explicitly mentioned in the paper, this information is approximate, it gives an idea that GA3C and A3C achieves final score in the same ballpark.\nWe also suggest that the contribution of our paper is not limited to the implementation of GA3C, but it also includes the analysis of the computational aspects of GA3C and other RL algorithms that can be implemented through a similar architecture. This contribution stands independently from the potential stability issues for GA3C, while the proposed architecture (and the freely available code) can be modified or extended to address these issues, improve the convergence speed, or attack real world problems.", "OTHER_KEYS": "iuri frosio"}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors' own implementation of A3C as well as to published reference scores.\n\nThe paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see. \n\nI appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduce a variant of A3C model where while agents run on multiple cores on CPU the model computations which is the computationally intensive part is passed to the GPU. And they perform various analysis to show the gained speedup.\n\nThanks the authors for the replying to the questions and adjusting the paper to make it more clear.\nIt's an interesting modification the the original algorithm. And section 5 does a through analysis of gpu utilization on different configurations.\nThe main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities. Stability is a very important issue in RL, and also the most successful algorithms should be able to achieve good results in various domains. I do understand the computational resource limitation, especially in academia if in fact this work was done outside Nvidia.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting work, well motivated.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.\n\nI applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.\n\nA disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "07 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "30 Nov 2016", "TITLE": "1-Model explanation 2-Instability plots", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "29 Nov 2016", "TITLE": "Any comment on the CPU utilization?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Nov 2016", "TITLE": "Great improvement on V. Minh's initial Work ", "IS_META_REVIEW": false, "comments": "Look forward to testing the hybrid CPU/GPU version.  ", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.\n\nI applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.\n\nA disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper suggests modifications to A3C - mainly by adding a system of queues to batch data - to obtain higher learning throughput on a GPU. The paper gives a solid systems analysis of the algorithm and performance. Although the method does not necessarily lead to higher Atari performance, it is a valuable and relevant contribution to the field, since it provides a stable, fast, open-source implementation of the currently best-performing deep RL algorithm.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "17 Jan 2017 (modified: 20 Jan 2017)", "TITLE": "Note for reviewers", "IS_META_REVIEW": false, "comments": "Please note we just updated the latest version of the paper with the latest version of Fig. 5.\nThis is now coherent with results reported in Figs. 6/7/8, where we achieve higher convergence rate by using a different learning rate.\n\nPlease also notice the code has been officially released on Github: ", "OTHER_KEYS": "iuri frosio"}, {"DATE": "14 Jan 2017", "TITLE": "Answer to reviewers", "IS_META_REVIEW": false, "comments": "We first thank all the reviewers for the interesting comments they provided. All three reviewers agree on the potential importance of the proposed algorithm, but they also suggest that more experimental data are needed to demonstrate that GA3C achieves results in the same ballpark as the original A3C. This turns out to be particularly important in the light of the fact that the policy lag in GA3C can potentially introduce instabilities, as noted by Reviewer #1.\n\nWe have included a better description of this in Section 4.4 of the revised paper, as well as analyzed it in a newly added Section 5.3, while providing additional experiments to verify the game performance of GA3C and its stability. As pointed out by the reviewers and already highlighted in the paper, establishing a validation protocol that allows comparing the final scores achieved by GA3C with the scores achieved by other training algorithms is complex or even infeasible, as many implementation details are not shared, as well as validation protocols are poorly reproducible. Beyond that, the final score (still reported in Table 3) tells only part of the story, and therefore we espouse the idea shared by Reviewer #1 and #3 of reporting learning curves as a function of the time. The newly added Fig. 6 and 7 show the learning curves for GA3C and several games as a function of the time. The reader can easily compare these graphs with those reported in Mnih\u2019s paper, as well as in other venues mentioned by the reviewers (e.g. github). The score vs. time curves show a similar dynamic for GA3C and A3C, although it is also important mentioning that curves reported in Mnih\u2019s paper represent the average of the top 5 curves in a set of 50 with various learning rates, which we are unable to match due limitations in available computational resources. Since systems, system loads, environments and hyper-parameters are so widely varying, we also believe that releasing the GA3C code (which we plan to do in the next days) will allow other researchers to systematically evaluate the performances of GA3C and facilitate a fair comparison with new techniques.\n\nReviewer #1\u2019s main concern is about instabilities introduced in GA3C. The training curves reported in the paper indeed show a few instabilities occurring in the training process, but these represent a common phenomenon in RL. Beyond analyzing in Section 4.4 how the policy lag in GA3C can generate these instabilities, we now show experimentally that such instabilities do not affect some games (e.g. Pong), while they can invalidate the learning process in other ones (e.g. Breakout), especially in the case of a large learning rate (Fig. 7 and 8). We found that properly tuning the algorithm hyper-parameters (and in particular the learning rate) dramatically limits the effect of instabilities on the learning process. GA3C would likely benefit from a thorough learning rate search as presented in Mnih\u2019s paper over a set of 50 different learning rates for A3C. Beyond this, we also show how batching on training data (see the newly introduced Section 5.3, and Fig. 7 and 8 in particular) can be used to limit the effect of these instabilities, at the price of a slower convergence rate.\n\nSpecific answers to the individual reviewers follow.\n\n===============================\nReviewer 1\n===============================\n\nWe agree with the reviewer that instabilities may affect the learning process. This is indeed a commonly observed phenomenon in RL. Given the fact that GA3C and A3C are indeed (slightly) different algorithms (as now clearly explained in Section 4.4 of the paper), we followed the reviewer\u2019s suggestion and performed a thorough evaluation of the instabilities of the learning process on several games. Experimental results are reported in Section 5.3 and in Fig. 7 and 8 in particular. These show that, for some games (e.g. Pong), the delayed computation of the gradient terms in GA3C (see Section 4.4) does not introduce convergence issues, that are however present in other games (e.g. Breakout). However, a proper selection of the algorithm hyper-parameters, and of the learning rate in particular, is sufficient to achieve results that are comparable to those achieved by A3C (see Table 3). Quite interestingly, the optimal learning rate for GA3C results to be smaller than the one reported in Mnih\u2019s paper for A3C (as it can be seen in Fig. 6) - this somehow confirms that A3C and GA3C are indeed slightly different algorithms, and GA3C requires careful parameter tuning to obtain optimal results. We also demonstrated that batching of the training data can be used to limit the instabilities, at the price of a slower convergence rate.\n\n===============================\nReviewer 2\n===============================\n\nFollowing the suggestion of the other two reviewers, and sharing the conviction that replicating the Deep Mind evaluation protocol is non-trivial, we now report in the paper more training curves for several games (Fig. 6) as a function of the training time. This allows the reader to perform a direct comparison between our training curves and the ones reported in Mnih\u2019s paper, independently from the hardware system used for training. Although, the reader should be careful about this comparison since Mnih\u2019s graphs show top 5 out of 50 runs which is hard to compare with the small number of runs considered in Fig. 6. These curves demonstrate that training time is comparable while achieving a similar training score. The analysis of the theoretical discrepancies between A3C and GA3C (Section 4.4) and its experimental evaluation in terms of instabilities (Section 5.3) should also serve to clarify that, once the hyper-parameters are properly set, GA3C is comparable to A3C in terms of performance.\nAs for the question about the protocol used for evaluation the score of our trained agents, we now better specify in the caption of table 3 and in the text that we are reporting the average score over 30 different runs for the best agent we trained with GA3C. A direct comparison using the protocol followed in Mnih\u2019s paper is not feasible, as the authors did not share the \u201chuman start condition\u201d for each game.\n\n===============================\nReviewer 3\n===============================\n\nWe agree with the reviewer that showing score vs. time plot would be more informative than reporting only the final score as in Table 3. Therefore, we added Fig. 6 to the paper, showing that, with the proper selection of the hyper-parameters, the dynamic of the learning curves of GA3C is similar to that shown in the original A3C paper by Mnih et al. We believe that other, more precise comparisons in terms of learning speed require an optimized implementation as well as a broad hyper-parameter search (in the style of the original A3C plot, showing top 5 results among a set of 50 learners trained with different learning rates), which is out of the scope for this paper. We also decided to keep Table 3 in the paper - although, as explicitly mentioned in the paper, this information is approximate, it gives an idea that GA3C and A3C achieves final score in the same ballpark.\nWe also suggest that the contribution of our paper is not limited to the implementation of GA3C, but it also includes the analysis of the computational aspects of GA3C and other RL algorithms that can be implemented through a similar architecture. This contribution stands independently from the potential stability issues for GA3C, while the proposed architecture (and the freely available code) can be modified or extended to address these issues, improve the convergence speed, or attack real world problems.", "OTHER_KEYS": "iuri frosio"}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors' own implementation of A3C as well as to published reference scores.\n\nThe paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see. \n\nI appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduce a variant of A3C model where while agents run on multiple cores on CPU the model computations which is the computationally intensive part is passed to the GPU. And they perform various analysis to show the gained speedup.\n\nThanks the authors for the replying to the questions and adjusting the paper to make it more clear.\nIt's an interesting modification the the original algorithm. And section 5 does a through analysis of gpu utilization on different configurations.\nThe main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities. Stability is a very important issue in RL, and also the most successful algorithms should be able to achieve good results in various domains. I do understand the computational resource limitation, especially in academia if in fact this work was done outside Nvidia.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting work, well motivated.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.\n\nI applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.\n\nA disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "07 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "30 Nov 2016", "TITLE": "1-Model explanation 2-Instability plots", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "29 Nov 2016", "TITLE": "Any comment on the CPU utilization?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Nov 2016", "TITLE": "Great improvement on V. Minh's initial Work ", "IS_META_REVIEW": false, "comments": "Look forward to testing the hybrid CPU/GPU version.  ", "OTHER_KEYS": "(anonymous)"}], "authors": "Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, Jan Kautz", "accepted": true, "id": "437"}