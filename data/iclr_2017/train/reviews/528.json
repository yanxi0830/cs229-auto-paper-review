{"conference": "ICLR 2017 conference submission", "title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Quality, Clarity: There is no consensus on this, with the readers having varying backgrounds, and one reviewer commenting that they found it to be unreadable. \n \n Originality, Significance:\n  The reviews are mixed on this, with the high score (7) acknowledging a lack of expertise on program induction.\n The paper is based on the published TerpreT system, and some think that it marginal and contradictory with respect to the TerpreT paper. In the rebuttal, point (3) from the authors points to the need to better understand gradient-based program search, even if it is not always better. This leaves me torn about a decision on this paper, although currently it does not have strong support from the most knowledgeable reviewers.\n That said, due to the originality of this work, the PCs are inclined to invite this work to be presented as a workshop contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "17 Jan 2017", "TITLE": "General response", "IS_META_REVIEW": false, "comments": "Thank you to all the reviewers for their comments. We believe that these\nare the primary points raised by the reviewers:\n\n1. A comparison with a neural programming technique which does not\n   generate code would be a valuable addition to our experiments.\n\n   We did not include a comparison to a network which does not\n   generate source code because because they usually require\n   substantially more training data than the 5 input-output examples\n   we provide to have any success at generalization.\n   While it would be interesting to show this effect in experiments,\n   the wide variety of different models and training strategies,\n   the custom structure of list data and list-aware objective function in our work,\n   together with the lack of released standard implementations,\n   makes it unclear what neural programming baseline (and with what\n   training regime) would be appropriate.\n\n2. The tasks our network can learn are simple. In particular, we do\n   not consider sorting or merging problems.\n\n   Although the tasks that we considered are simple, they are more complex\n   than the tasks which many other neural programming approaches can handle,\n   particularly as we test for perfect generalization.\n\n   The approaches to learning to sort do so using program traces,\n   which provide much stronger supervision than the input-output examples\n   that we use [1], or use specialized memory representations that simplify\n   sorting [2]. \n\n3. This paper does not conclusively show that gradient-based\n   evaluators are appropriate for program induction.\n\n   This paper certainly does not contradict the findings of the\n   original TerpreT paper that discrete solvers are good backends for\n   program induction. Our motivation in this paper is to improve gradient-based\n   program search, and to understand the effect of different design choices\n   that arise when building differentiable interpreters. Even if gradient descent\n   isn't currently the best method for program induction, we believe it merits\n   further study. It is a very new idea, and it's feasible to us that seemingly\n   subtle design decisions could make a big difference in its performance (indeed,\n   this is one take-away from our experiments). Further, since gradient descent\n   is very different from alternatives, improving its performance may enable \n   new uses of program synthesis such as jointly inducing programs and training\n   neural network subcomponents as in [3], using SGD to scale up to large data\n   sets, or giving new ways of thinking about noisy data in program synthesis.\n\n   Finally, the recommendations for the design of such evaluators\n   discussed in this paper are not necessarily restricted to TerpreT-based models,\n   and we believe that our design recommendations apply to related\n   neural architectures that try to learn algorithmic patterns.\n\n4. How will this model generalize to programs that can't be solved\n   using the prefix-loop-suffix structure?\n\n   Defining new program structures is simple, and our current\n   implementation allows the optimizer to choose between several\n   program structures. More loops could be added if desired, and more\n   looping schemes could be added to extend the class of programs\n   which can be learned.\n\n5. TerpreT is not yet publicly available.\n\n   TerpreT is nearly ready for public release. Approval for open-sourcing\n   under the MIT license has been obtained, and we are currently in the\n   process of documenting the source code to publish it (with all models\n   used in this paper).\n\n\nReferences:\n[1] Scott Reed and Nando de Freitas. Neural Programmer-Interpreters.\n    In ICLR 2016.\n[2] Marcin Andrychowicz, Karol Kurach. Learning Efficient Algorithms\n    with Hierarchical Attentive Memory. ", "OTHER_KEYS": "Marc Brockschmidt"}, {"TITLE": "very interesting but probably too derivative from earlier already published work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).  \n\nAs mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.  To be clear I think the TerpreT paper deserves a large amount of attention.  It is truly inspiring.\n\nThis paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction.   This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper.  \n\nMy opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.  On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same so I think, unfortunately, it's below threshold.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper discusses a range of modelling choices for designing differentiable programming languages. Authors propose 4 recommendations that are then tested on a set of 13 algorithmic tasks for lists, such as \"length of the list\", \"return k-th element from the list\", etc. The solutions are learnt from input/output example pairs (5 for training, 25 for test).\n\nThe main difference between this work and differentiable architectures, like NTM, Neural GPU, NRAM, etc. is the fact that here the authors aim at automatically producing code that solves the given task.\n\nMy main concern are experiments - it would be nice to see a comparison to some of the neural networks mentioned in related work. Also, it would be useful to see how this model is doing on typical problems used by mentioned neural architectures (problems such as \"sorting\", \"merging\", \"adding\"). I'm wondering how this is going to generalize to other types of programs that can't be solved with prefix-loop-suffix structure.\n\nIt is also concerning that although  1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: A+L model that has \u201cloop\u201d fails to solve \u201clist length\u201d task in 84% of the runs).\n\n\nPro:\n- generates code rather than black-box neural architecture\n- nice that it can learn from very few examples\n\nCons:\n- weak results, works only for very simple tasks, missing comparison to neural architectures", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents design decisions of TerpreT [1] and experiments about learning simple loop programs and list manipulation tasks. The TerpreT line of work (is one of those which) bridges the gap between the programming languages (PL) and machine learning (ML) communities. Contrasted to the recent interest of the ML community for program induction, the focus here is on using the design of the programming language to reduce the search space. Namely, here, they used the structure of the control flow (if-then-else, foreach, zipWithi, and foldli \"templates\"), immutable data (no reuse of a \"neural\" memory), and types (they tried penalizing ill-typedness, and restricting the search only to well-typed programs, which works better). My bird eye view would be that this stands in between \"make everything continuous and perform gradient descent\" (ML) and \"discretize all the things and perform structured and heuristics-guided combinatorial search\" (PL).\n\nI liked that they have a relevant baseline (\\lambda^2), but I wished that they also included a fully neural network program synthesis baseline. Admittedly, it would not succeed except on the simplest tasks, but I think some of their experimental tasks are simple enough for \"non-generating code \" NNs to succeed on.\n\nI wished that TerpreT was available, and the code to reproduce these experiments too.\n\nI wonder if/how the (otherwise very interesting!) recommendations for the design of programming languages to perform gradient descent based-inductive programming would hold/perform on harder task than these loops. Even though these tasks are already interesting and challenging, I wonder how much of these tasks biased the search for good subset of constraints (e.g. those for structuring the control flow).\n\nOverall, I think that the paper is good enough to appear at ICLR, but I am no expert in program induction / synthesis.\n\nWriting:\n - The paper is at times hard to follow. For instance, the naming scheme of the model variants could be summarized in a table (with boolean information about the features it embeds).\n - Introduction: \"basis modern computing\" -> of\n - Page 3, training objective: \"minimize the cross-entropy between the distribution in the output register r_R^{(T)} and a point distribution with all probability mass on the correct output value\" -> if you want to cater to the ML community at large, I think that it is better to say that you treat the output of r_R^{(T)} as a classification problem with the correct output value (you can give details and say exactly which type of criterion/loss, cross-entropy, you use).\n\n\n[1] \"TerpreT: A Probabilistic Programming Language for Program Induction\", Gaunt et al. 2016", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Not sure if very interesting to the ICLR community", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.\n\nI must say i\u2019m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn\u2019t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.\n\nPros: \nUseful, well executed, novel study.\nCons:\nLow on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Weak accept", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Comparison with neural network models", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Just an application of TerpreT?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "30 Nov 2016", "TITLE": "Comparison with the memory model of Graves et al 16", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Quality, Clarity: There is no consensus on this, with the readers having varying backgrounds, and one reviewer commenting that they found it to be unreadable. \n \n Originality, Significance:\n  The reviews are mixed on this, with the high score (7) acknowledging a lack of expertise on program induction.\n The paper is based on the published TerpreT system, and some think that it marginal and contradictory with respect to the TerpreT paper. In the rebuttal, point (3) from the authors points to the need to better understand gradient-based program search, even if it is not always better. This leaves me torn about a decision on this paper, although currently it does not have strong support from the most knowledgeable reviewers.\n That said, due to the originality of this work, the PCs are inclined to invite this work to be presented as a workshop contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "17 Jan 2017", "TITLE": "General response", "IS_META_REVIEW": false, "comments": "Thank you to all the reviewers for their comments. We believe that these\nare the primary points raised by the reviewers:\n\n1. A comparison with a neural programming technique which does not\n   generate code would be a valuable addition to our experiments.\n\n   We did not include a comparison to a network which does not\n   generate source code because because they usually require\n   substantially more training data than the 5 input-output examples\n   we provide to have any success at generalization.\n   While it would be interesting to show this effect in experiments,\n   the wide variety of different models and training strategies,\n   the custom structure of list data and list-aware objective function in our work,\n   together with the lack of released standard implementations,\n   makes it unclear what neural programming baseline (and with what\n   training regime) would be appropriate.\n\n2. The tasks our network can learn are simple. In particular, we do\n   not consider sorting or merging problems.\n\n   Although the tasks that we considered are simple, they are more complex\n   than the tasks which many other neural programming approaches can handle,\n   particularly as we test for perfect generalization.\n\n   The approaches to learning to sort do so using program traces,\n   which provide much stronger supervision than the input-output examples\n   that we use [1], or use specialized memory representations that simplify\n   sorting [2]. \n\n3. This paper does not conclusively show that gradient-based\n   evaluators are appropriate for program induction.\n\n   This paper certainly does not contradict the findings of the\n   original TerpreT paper that discrete solvers are good backends for\n   program induction. Our motivation in this paper is to improve gradient-based\n   program search, and to understand the effect of different design choices\n   that arise when building differentiable interpreters. Even if gradient descent\n   isn't currently the best method for program induction, we believe it merits\n   further study. It is a very new idea, and it's feasible to us that seemingly\n   subtle design decisions could make a big difference in its performance (indeed,\n   this is one take-away from our experiments). Further, since gradient descent\n   is very different from alternatives, improving its performance may enable \n   new uses of program synthesis such as jointly inducing programs and training\n   neural network subcomponents as in [3], using SGD to scale up to large data\n   sets, or giving new ways of thinking about noisy data in program synthesis.\n\n   Finally, the recommendations for the design of such evaluators\n   discussed in this paper are not necessarily restricted to TerpreT-based models,\n   and we believe that our design recommendations apply to related\n   neural architectures that try to learn algorithmic patterns.\n\n4. How will this model generalize to programs that can't be solved\n   using the prefix-loop-suffix structure?\n\n   Defining new program structures is simple, and our current\n   implementation allows the optimizer to choose between several\n   program structures. More loops could be added if desired, and more\n   looping schemes could be added to extend the class of programs\n   which can be learned.\n\n5. TerpreT is not yet publicly available.\n\n   TerpreT is nearly ready for public release. Approval for open-sourcing\n   under the MIT license has been obtained, and we are currently in the\n   process of documenting the source code to publish it (with all models\n   used in this paper).\n\n\nReferences:\n[1] Scott Reed and Nando de Freitas. Neural Programmer-Interpreters.\n    In ICLR 2016.\n[2] Marcin Andrychowicz, Karol Kurach. Learning Efficient Algorithms\n    with Hierarchical Attentive Memory. ", "OTHER_KEYS": "Marc Brockschmidt"}, {"TITLE": "very interesting but probably too derivative from earlier already published work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).  \n\nAs mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.  To be clear I think the TerpreT paper deserves a large amount of attention.  It is truly inspiring.\n\nThis paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction.   This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper.  \n\nMy opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.  On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same so I think, unfortunately, it's below threshold.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper discusses a range of modelling choices for designing differentiable programming languages. Authors propose 4 recommendations that are then tested on a set of 13 algorithmic tasks for lists, such as \"length of the list\", \"return k-th element from the list\", etc. The solutions are learnt from input/output example pairs (5 for training, 25 for test).\n\nThe main difference between this work and differentiable architectures, like NTM, Neural GPU, NRAM, etc. is the fact that here the authors aim at automatically producing code that solves the given task.\n\nMy main concern are experiments - it would be nice to see a comparison to some of the neural networks mentioned in related work. Also, it would be useful to see how this model is doing on typical problems used by mentioned neural architectures (problems such as \"sorting\", \"merging\", \"adding\"). I'm wondering how this is going to generalize to other types of programs that can't be solved with prefix-loop-suffix structure.\n\nIt is also concerning that although  1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: A+L model that has \u201cloop\u201d fails to solve \u201clist length\u201d task in 84% of the runs).\n\n\nPro:\n- generates code rather than black-box neural architecture\n- nice that it can learn from very few examples\n\nCons:\n- weak results, works only for very simple tasks, missing comparison to neural architectures", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents design decisions of TerpreT [1] and experiments about learning simple loop programs and list manipulation tasks. The TerpreT line of work (is one of those which) bridges the gap between the programming languages (PL) and machine learning (ML) communities. Contrasted to the recent interest of the ML community for program induction, the focus here is on using the design of the programming language to reduce the search space. Namely, here, they used the structure of the control flow (if-then-else, foreach, zipWithi, and foldli \"templates\"), immutable data (no reuse of a \"neural\" memory), and types (they tried penalizing ill-typedness, and restricting the search only to well-typed programs, which works better). My bird eye view would be that this stands in between \"make everything continuous and perform gradient descent\" (ML) and \"discretize all the things and perform structured and heuristics-guided combinatorial search\" (PL).\n\nI liked that they have a relevant baseline (\\lambda^2), but I wished that they also included a fully neural network program synthesis baseline. Admittedly, it would not succeed except on the simplest tasks, but I think some of their experimental tasks are simple enough for \"non-generating code \" NNs to succeed on.\n\nI wished that TerpreT was available, and the code to reproduce these experiments too.\n\nI wonder if/how the (otherwise very interesting!) recommendations for the design of programming languages to perform gradient descent based-inductive programming would hold/perform on harder task than these loops. Even though these tasks are already interesting and challenging, I wonder how much of these tasks biased the search for good subset of constraints (e.g. those for structuring the control flow).\n\nOverall, I think that the paper is good enough to appear at ICLR, but I am no expert in program induction / synthesis.\n\nWriting:\n - The paper is at times hard to follow. For instance, the naming scheme of the model variants could be summarized in a table (with boolean information about the features it embeds).\n - Introduction: \"basis modern computing\" -> of\n - Page 3, training objective: \"minimize the cross-entropy between the distribution in the output register r_R^{(T)} and a point distribution with all probability mass on the correct output value\" -> if you want to cater to the ML community at large, I think that it is better to say that you treat the output of r_R^{(T)} as a classification problem with the correct output value (you can give details and say exactly which type of criterion/loss, cross-entropy, you use).\n\n\n[1] \"TerpreT: A Probabilistic Programming Language for Program Induction\", Gaunt et al. 2016", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Not sure if very interesting to the ICLR community", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.\n\nI must say i\u2019m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn\u2019t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.\n\nPros: \nUseful, well executed, novel study.\nCons:\nLow on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Weak accept", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Comparison with neural network models", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Just an application of TerpreT?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "30 Nov 2016", "TITLE": "Comparison with the memory model of Graves et al 16", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "John K. Feser, Marc Brockschmidt, Alexander L. Gaunt, Daniel Tarlow", "accepted": false, "id": "528"}