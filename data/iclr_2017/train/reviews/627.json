{"conference": "ICLR 2017 conference submission", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. \n\nThe dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.\n\nBesides, I have problems with the presentation of this paper.\n(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. \n(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq. (3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).\n(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma. So how \\mu' and \\sigma' are being used ?\n(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.\n\nOther things: no explanations for Figure 2 & 3. There's a missing \\pi symbol in Appendix A before the KL derivation."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair agrees with the reviewers that this paper is not of sufficient quality for ICLR. The experimental results are weak (there might be even be some issues with the experimental methodology) and it is not at all clear whether the translation model benefits from the image data. The authors did not address the final reviews.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Promising research direction but not quite there", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. \n\nAs pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "There are major issues", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences. \n\nThe idea seems to be to use a latent variable model and condition it on the image. In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference. That said, the approach appears flawed, because the image is not really used for translation.\n\nExperimental results are weak. If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline. In the view of the overall variance of the results, these improvements can not be considered significant. \n\nThe qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.\n\nOverall, there are major issues with both the approach and the execution of the paper.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Unclear motivation & unconvincing results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. \n\nThe dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.\n\nBesides, I have problems with the presentation of this paper.\n(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. \n(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq. (3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).\n(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma. So how \\mu' and \\sigma' are being used ?\n(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.\n\nOther things: no explanations for Figure 2 & 3. There's a missing \\pi symbol in Appendix A before the KL derivation.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "General comments", "IS_META_REVIEW": false, "comments": "1. Just for information, regarding your claim \"We also present the first translation task with which one uses a parallel corpus and images in training, while using a source corpus in translating\", there's a recent paper called \"Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot\" which also proposes the very same idea (", "OTHER_KEYS": "(anonymous)"}, {"DATE": "02 Dec 2016", "TITLE": "Latent representations", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Development and test set scores", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. \n\nThe dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.\n\nBesides, I have problems with the presentation of this paper.\n(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. \n(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq. (3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).\n(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma. So how \\mu' and \\sigma' are being used ?\n(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.\n\nOther things: no explanations for Figure 2 & 3. There's a missing \\pi symbol in Appendix A before the KL derivation."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair agrees with the reviewers that this paper is not of sufficient quality for ICLR. The experimental results are weak (there might be even be some issues with the experimental methodology) and it is not at all clear whether the translation model benefits from the image data. The authors did not address the final reviews.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Promising research direction but not quite there", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. \n\nAs pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "There are major issues", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences. \n\nThe idea seems to be to use a latent variable model and condition it on the image. In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference. That said, the approach appears flawed, because the image is not really used for translation.\n\nExperimental results are weak. If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline. In the view of the overall variance of the results, these improvements can not be considered significant. \n\nThe qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.\n\nOverall, there are major issues with both the approach and the execution of the paper.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Unclear motivation & unconvincing results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. \n\nThe dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.\n\nBesides, I have problems with the presentation of this paper.\n(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. \n(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq. (3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).\n(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma. So how \\mu' and \\sigma' are being used ?\n(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.\n\nOther things: no explanations for Figure 2 & 3. There's a missing \\pi symbol in Appendix A before the KL derivation.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "General comments", "IS_META_REVIEW": false, "comments": "1. Just for information, regarding your claim \"We also present the first translation task with which one uses a parallel corpus and images in training, while using a source corpus in translating\", there's a recent paper called \"Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot\" which also proposes the very same idea (", "OTHER_KEYS": "(anonymous)"}, {"DATE": "02 Dec 2016", "TITLE": "Latent representations", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Development and test set scores", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Joji Toyama, Masanori Misono, Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo", "accepted": false, "id": "627"}