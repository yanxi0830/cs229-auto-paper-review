{"conference": "ICLR 2017 conference submission", "title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least quasiconvex\u2014 activation functions. While sinusoidal activation functions have been successfully used for specific applications, they remain largely ignored and regarded as difficult to train. In this paper we formally characterize why these networks can indeed often be difficult to train even in very simple scenarios, and describe how the presence of infinitely many and shallow local minima emerges from the architecture. We also provide an explanation to the good performance achieved on a typical classification task, by showing that for several network architectures the presence of the periodic cycles is largely ignored when the learning is successful. Finally, we show that there are non-trivial tasks\u2014such as learning algorithms\u2014where networks using sinusoidal activations can learn faster than more established monotonic functions.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers unanimously recommend rejecting the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Comment to all reviewers", "IS_META_REVIEW": false, "comments": "Thank you to all the reviewers for the helpful comments. Probably due to a lack of clarity of the paper, some details in the proposed summaries differ from the main points we tried to convey. Let us then attempt to clarify what the point of the paper is supposed to be, both here and in the manuscript.\n\nThe main point of the paper is to analyze the effect of sine as an activation function, how it affects the representation learned and what consequences it has on learning.\u00a0\n\nThe main contributions of the paper are:\nO1 - a proof that confirms the claim that networks with sine might easily get stuck into local minima, obtained by analytically characterizing the loss surface.\nO2 - evidence showing that when training is successful for typical datasets (as it's the case for most previous works using sine presented in Section 2) the network is actually not relying on the periodicity of the function. The network relies almost exclusively on the central part of the function, that is monotonic and similar to a tanh. Evidence for this is presented on MNIST and in the new version on Reuters too.\nO3 - there might still be room for sine as activation function in certain artificial/algorithmic tasks where sine is intuitively beneficial, since there are at least two simple tasks (i.e. sum and diff) where sin can outperform a standard RNN baseline using tanh.\n\nWe are trying to make these points more clear and explicit in the paper.\n", "OTHER_KEYS": "Giambattista Parascandolo"}, {"TITLE": "Nice preliminary theoretical results of using sin activations, but more evidence needed", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:\nIn this paper, the authors explore the advantages/disadvantages of using a sin activation function.\nThey first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\nThey then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\nThey then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nPros:\nThe closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\nCons: \nSeems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Mainly theoretical idea with insufficient evidence of being practical", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.\nThis change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. \nThe experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "11 Dec 2016", "TITLE": "Details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IS_META_REVIEW": true, "comments": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers unanimously recommend rejecting the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Comment to all reviewers", "IS_META_REVIEW": false, "comments": "Thank you to all the reviewers for the helpful comments. Probably due to a lack of clarity of the paper, some details in the proposed summaries differ from the main points we tried to convey. Let us then attempt to clarify what the point of the paper is supposed to be, both here and in the manuscript.\n\nThe main point of the paper is to analyze the effect of sine as an activation function, how it affects the representation learned and what consequences it has on learning.\u00a0\n\nThe main contributions of the paper are:\nO1 - a proof that confirms the claim that networks with sine might easily get stuck into local minima, obtained by analytically characterizing the loss surface.\nO2 - evidence showing that when training is successful for typical datasets (as it's the case for most previous works using sine presented in Section 2) the network is actually not relying on the periodicity of the function. The network relies almost exclusively on the central part of the function, that is monotonic and similar to a tanh. Evidence for this is presented on MNIST and in the new version on Reuters too.\nO3 - there might still be room for sine as activation function in certain artificial/algorithmic tasks where sine is intuitively beneficial, since there are at least two simple tasks (i.e. sum and diff) where sin can outperform a standard RNN baseline using tanh.\n\nWe are trying to make these points more clear and explicit in the paper.\n", "OTHER_KEYS": "Giambattista Parascandolo"}, {"TITLE": "Nice preliminary theoretical results of using sin activations, but more evidence needed", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:\nIn this paper, the authors explore the advantages/disadvantages of using a sin activation function.\nThey first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\nThey then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\nThey then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nPros:\nThe closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\nCons: \nSeems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Mainly theoretical idea with insufficient evidence of being practical", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.\nThis change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. \nThe experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "11 Dec 2016", "TITLE": "Details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}], "authors": "Giambattista Parascandolo, Heikki Huttunen, Tuomas Virtanen", "accepted": false, "id": "608"}