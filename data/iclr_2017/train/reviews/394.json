{"conference": "ICLR 2017 conference submission", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.\n\nThis is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.\n \n Pros:\n - Improved results\n - Very simple method\n \n Cons:\n - Almost the best results (aside from Variational Dropout)", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 2, "SUBSTANCE": 2, "RECOMMENDATION_UNOFFICIAL": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d.\n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Incremental improvement, not convincing enough", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 12 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper Summary\nThis paper proposes a variant of dropout, applicable to RNNs, in which the state\nof a unit is randomly retained, as opposed to being set to zero. This provides\nnoise which gives the regularization effect, but also prevents loss of\ninformation over time, in fact making it easier to send gradients back because\nthey can flow right through the identity connections without attenuation.\nExperiments show that this model works quite well. It is still worse that\nvariational dropout on Penn Tree bank language modeling task, but given the\nsimplicity of the idea it is likely to become widely useful.\n\nStrengths\n- Simple idea that works well.\n- Detailed experiments help understand the effects of the zoneout probabilities\n  and validate its applicability to different tasks/domains.\n\nWeaknesses\n- Does not beat variational dropout (but maybe better hyper-parameter tuning\n  will help).\n\nQuality\nThe experimental design and writeup is high quality.\n\nClarity\nThe paper clear and well written, experimental details seem adequate.\n\nOriginality\nThe proposed idea is novel.\n\nSignificance\nThis paper will be of interest to anyone working with RNNs (which is a large\ngroup of people!).\n\nMinor suggestion-\n- As the authors mention - Zoneout has two things working for it - the noise and\n  the ability to pass gradients back without decay. It might help to tease apart\nthe contribution from these two factors. For example, if we use a fixed\nmask over the unrolled network (different at each time step) instead of resampling\nit again for every training case, it would tell us how much help comes from the\nidentity connections alone.", "ORIGINALITY": 1, "IS_ANNOTATED": true, "TITLE": "Simple idea, well executed.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "18 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.\n\nThis is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 2, "SUBSTANCE": 2, "RECOMMENDATION_UNOFFICIAL": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "What hypothesis do the experiments test?", "IS_META_REVIEW": false, "DATE": "10 Dec 2016"}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 1, "IS_ANNOTATED": true, "TITLE": "Better Regularization or Optimization ?", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Clarification re: performance of zoneout vs variational LSTM and char-level PTB", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "CLARITY": 2}, {"IS_META_REVIEW": true, "comments": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.\n\nThis is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.\n \n Pros:\n - Improved results\n - Very simple method\n \n Cons:\n - Almost the best results (aside from Variational Dropout)", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 2, "SUBSTANCE": 2, "RECOMMENDATION_UNOFFICIAL": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d.\n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Incremental improvement, not convincing enough", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 12 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper Summary\nThis paper proposes a variant of dropout, applicable to RNNs, in which the state\nof a unit is randomly retained, as opposed to being set to zero. This provides\nnoise which gives the regularization effect, but also prevents loss of\ninformation over time, in fact making it easier to send gradients back because\nthey can flow right through the identity connections without attenuation.\nExperiments show that this model works quite well. It is still worse that\nvariational dropout on Penn Tree bank language modeling task, but given the\nsimplicity of the idea it is likely to become widely useful.\n\nStrengths\n- Simple idea that works well.\n- Detailed experiments help understand the effects of the zoneout probabilities\n  and validate its applicability to different tasks/domains.\n\nWeaknesses\n- Does not beat variational dropout (but maybe better hyper-parameter tuning\n  will help).\n\nQuality\nThe experimental design and writeup is high quality.\n\nClarity\nThe paper clear and well written, experimental details seem adequate.\n\nOriginality\nThe proposed idea is novel.\n\nSignificance\nThis paper will be of interest to anyone working with RNNs (which is a large\ngroup of people!).\n\nMinor suggestion-\n- As the authors mention - Zoneout has two things working for it - the noise and\n  the ability to pass gradients back without decay. It might help to tease apart\nthe contribution from these two factors. For example, if we use a fixed\nmask over the unrolled network (different at each time step) instead of resampling\nit again for every training case, it would tell us how much help comes from the\nidentity connections alone.", "ORIGINALITY": 1, "IS_ANNOTATED": true, "TITLE": "Simple idea, well executed.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "18 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.\n\nThis is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 2, "SUBSTANCE": 2, "RECOMMENDATION_UNOFFICIAL": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "What hypothesis do the experiments test?", "IS_META_REVIEW": false, "DATE": "10 Dec 2016"}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 1, "IS_ANNOTATED": true, "TITLE": "Better Regularization or Optimization ?", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IMPACT": 2, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Clarification re: performance of zoneout vs variational LSTM and char-level PTB", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "CLARITY": 2}], "authors": "David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh  Goyal, Yoshua Bengio, Aaron Courville, Christopher Pal", "accepted": true, "id": "394"}