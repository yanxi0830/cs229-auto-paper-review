{"conference": "ICLR 2017 conference submission", "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "A new update", "IS_META_REVIEW": false, "comments": "We have clarified some of the questions on paragraph 2 on page 4, on paragraph 2 on page 6 and on paragraph 2 on page 15, respectively. Thanks!", "OTHER_KEYS": "Wentao Huang"}, {"DATE": "13 Jan 2017", "TITLE": "Paper updated", "IS_META_REVIEW": false, "comments": "We would like to thank the reviewers for their conscientious review and comments and for providing us with valuable feedback. We already updated the paper based on the reviewer comments.\n\nThanks!\n\nWentao", "OTHER_KEYS": "Wentao Huang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form. Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.\n\nWhile the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve. For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy. The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.\n\nA few questions that the authors may want to clarify:\n1. Page 4, last paragraph: \"from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)\". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true?\n2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that?\n3. At the end of page 9: \"we will discuss how to get optimal solution of C for two specific cases\". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper proposes a hierarchical infomax method. My comments are as follows: \n\n(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding. Therefore, it is not easy for readers to follow the paper. The authors should make this paper as compact as possible while maintaining the important message. \n\n(2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R). However, it is unclear why maximizing I(X;\\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\\breve{Y}) is an \u201cupper\u201d bound of I(X;R) (When it is difficult to directly maximize a function, people often maximize some tractable \u201clower\u201d bound of it).\n\nMinor comments:\n(1) If (2.11) is approximation of (2.8), \u201c\\approx\u201d should be used. \n\n(2) Why K_1 instead of N in Eq.(2.11)?\n\n(3) In Eq.(2.12), H(X) should disappear?\n\n(4) Can you divide Section 3 into subsections?\n\n", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review of \"information theoretic framework\"", "comments": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Nov 2016", "TITLE": "Some background on this paper", "IS_META_REVIEW": false, "comments": "My main research interests are in computational neuroscience, information theory, machine learning and deep learning. For those of us who engage in AI related research, we all want to learn from the human brain how to process information to achieve intelligence. For example, deep learning is now said to be brain-like, but also some researchers say that nothing to do with the neuroscience. But one thing is for sure, and that is deep nets learning the brain processing information with hierarchical structure. However, the current advances in neuroscience is of limited role on the inspiration for AI research. \n\nHow do we learn from the brain, learn what? We think we should learn the basic design principles for the information processing in our nervous system. What are the fundamental principles to guide the brain to design these complex structures and neural coding. \nIn fact, the principle on energy and information provides a fundamental constraint for the actual nervous systems. The efficient coding hypothesis proposed by Horace Barlow provides a good description of this principle. But how to model this hypothesis, we will encounter great challenges. Because a direct calculation of Shannon's mutual information (MI) is generally a very difficult thing in many cases. \n\nWe first solve the problem of effective approximation for evaluating MI in the context of neural population coding, especially for high-dimensional inputs. Then an fast and robust unsupervised learning algorithm is developed in this paper. With our methods, we have explained some interesting phenomena in neuroscience, which were previously unsolvable in other ways. \nIn machine learning and deep learning, this paper is a more fundamental part  that provides the basis for our other works. We have also got a lot of interesting results from other related works we have done or are doing. For example, we use our method to train CNN, RNN and generative model, etc. We can unify supervised learning and unsupervised learning and can completely without the backpropagation algorithm for supervised learning of deep nets. We can also prove that the cost function of SVM (Hinge loss) is a special case of our neural population infomax. For the current deep nets, we need the big data for training, and in our framework, the small data we also can achieve good results.\n\nThe above is a brief introduction to the background of this paper. Since there is no similar work on this paper before, so there may be relatively more formulas and derivations and not very easy to follow. \n\nThanks!\n", "OTHER_KEYS": "Wentao Huang"}, {"IS_META_REVIEW": true, "comments": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "A new update", "IS_META_REVIEW": false, "comments": "We have clarified some of the questions on paragraph 2 on page 4, on paragraph 2 on page 6 and on paragraph 2 on page 15, respectively. Thanks!", "OTHER_KEYS": "Wentao Huang"}, {"DATE": "13 Jan 2017", "TITLE": "Paper updated", "IS_META_REVIEW": false, "comments": "We would like to thank the reviewers for their conscientious review and comments and for providing us with valuable feedback. We already updated the paper based on the reviewer comments.\n\nThanks!\n\nWentao", "OTHER_KEYS": "Wentao Huang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form. Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.\n\nWhile the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve. For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy. The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.\n\nA few questions that the authors may want to clarify:\n1. Page 4, last paragraph: \"from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)\". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true?\n2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that?\n3. At the end of page 9: \"we will discuss how to get optimal solution of C for two specific cases\". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper proposes a hierarchical infomax method. My comments are as follows: \n\n(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding. Therefore, it is not easy for readers to follow the paper. The authors should make this paper as compact as possible while maintaining the important message. \n\n(2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R). However, it is unclear why maximizing I(X;\\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\\breve{Y}) is an \u201cupper\u201d bound of I(X;R) (When it is difficult to directly maximize a function, people often maximize some tractable \u201clower\u201d bound of it).\n\nMinor comments:\n(1) If (2.11) is approximation of (2.8), \u201c\\approx\u201d should be used. \n\n(2) Why K_1 instead of N in Eq.(2.11)?\n\n(3) In Eq.(2.12), H(X) should disappear?\n\n(4) Can you divide Section 3 into subsections?\n\n", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review of \"information theoretic framework\"", "comments": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Nov 2016", "TITLE": "Some background on this paper", "IS_META_REVIEW": false, "comments": "My main research interests are in computational neuroscience, information theory, machine learning and deep learning. For those of us who engage in AI related research, we all want to learn from the human brain how to process information to achieve intelligence. For example, deep learning is now said to be brain-like, but also some researchers say that nothing to do with the neuroscience. But one thing is for sure, and that is deep nets learning the brain processing information with hierarchical structure. However, the current advances in neuroscience is of limited role on the inspiration for AI research. \n\nHow do we learn from the brain, learn what? We think we should learn the basic design principles for the information processing in our nervous system. What are the fundamental principles to guide the brain to design these complex structures and neural coding. \nIn fact, the principle on energy and information provides a fundamental constraint for the actual nervous systems. The efficient coding hypothesis proposed by Horace Barlow provides a good description of this principle. But how to model this hypothesis, we will encounter great challenges. Because a direct calculation of Shannon's mutual information (MI) is generally a very difficult thing in many cases. \n\nWe first solve the problem of effective approximation for evaluating MI in the context of neural population coding, especially for high-dimensional inputs. Then an fast and robust unsupervised learning algorithm is developed in this paper. With our methods, we have explained some interesting phenomena in neuroscience, which were previously unsolvable in other ways. \nIn machine learning and deep learning, this paper is a more fundamental part  that provides the basis for our other works. We have also got a lot of interesting results from other related works we have done or are doing. For example, we use our method to train CNN, RNN and generative model, etc. We can unify supervised learning and unsupervised learning and can completely without the backpropagation algorithm for supervised learning of deep nets. We can also prove that the cost function of SVM (Hinge loss) is a special case of our neural population infomax. For the current deep nets, we need the big data for training, and in our framework, the small data we also can achieve good results.\n\nThe above is a brief introduction to the background of this paper. Since there is no similar work on this paper before, so there may be relatively more formulas and derivations and not very easy to follow. \n\nThanks!\n", "OTHER_KEYS": "Wentao Huang"}], "authors": "Wentao Huang, Kechen Zhang", "accepted": true, "id": "335"}