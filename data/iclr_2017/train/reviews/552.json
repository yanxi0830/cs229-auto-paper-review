{"conference": "ICLR 2017 conference submission", "title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Paper has an interesting idea, but isn't quite justified, as pointed out by R2. Very minimal experiments are presented in the paper.\n \n pros:\n - interesting idea\n \n cons:\n - insufficient experiments with no real world problems.\n - no rebuttal either :(.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "not ready yet", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. \n\nMy own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. \n\nI would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   \n\nI think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.\n\nSome questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?  Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "my review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "06 Dec 2016", "TITLE": "the pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "30 Nov 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Paper has an interesting idea, but isn't quite justified, as pointed out by R2. Very minimal experiments are presented in the paper.\n \n pros:\n - interesting idea\n \n cons:\n - insufficient experiments with no real world problems.\n - no rebuttal either :(.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "not ready yet", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. \n\nMy own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. \n\nI would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   \n\nI think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.\n\nSome questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?  Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "my review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "06 Dec 2016", "TITLE": "the pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "30 Nov 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Zoe McCarthy, Andrew Bai, Xi Chen, Pieter Abbeel", "accepted": false, "id": "552"}