{"conference": "ICLR 2017 conference submission", "title": "Learning Identity Mappings with Residual Gates", "abstract": "We propose a layer augmentation technique that adds shortcut connections with a linear gating mechanism, and can be applied to almost any network model. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Highway Neural Networks and Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. In our experiments, augmented plain networks -- which can be interpreted as simplified Highway Neural Networks -- outperform ResNets, raising new questions on how shortcut connections should be designed. We also evaluate our model on CIFAR-10 and CIFAR-100 using augmented Wide ResNets, achieving 3.65% and 18.27% test error, respectively.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). \n\nUsing an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. \n\nAlthough good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Feb 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "We have also added a section (3.3) that connects our technique to the Unrolled Iterative Estimation interpretation of Highway and ResNets. We further elaborate on how the learned k values are indicators of the abstraction jumps in representations defined in Greff et al, and analyze the k values for both experiments (MNIST and CIFAR) under this perspective.", "OTHER_KEYS": "Pedro Savarese"}, {"DATE": "03 Feb 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the last revision of our work, we have performed the following main changes:\n\n- Added 4 extra experiments comparing Wide ResNet and their augmented counterparts (Table 5) on CIFAR-100. With this we hope to provide stronger indications of the generality and advantages of our technique.\n\nWe are currently running Gated PlainNets on CIFAR-100.\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"DATE": "30 Jan 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the last revision of our work, we have performed the following main changes:\n\n- Added 6 extra experiments comparing Wide ResNet and their augmented counterparts (Table 4). With this we hope to provide stronger indications of the generality and advantages of our technique.\n\n\nWe are currently running the same 6 models on CIFAR-100 to add them to the table. Are have also run Gated PlainNets (u = g(k)f(x) + (1 - g(k))x) on CIFAR-10, as suggested by reviewer 3, and will add results after running on CIFAR-100 as well.\n\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"DATE": "24 Jan 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the last revision of our work, we have performed the following main changes:\n\n- Added results with u = g(k)f(x) + (1 - g(k))x (Gated Plain Networks), along with some intuitions of why it outperformed non-augmented Residual Networks. This result suggests that an extensive study on different gating mechanisms for Highway Neural Networks can be extremely fruitful, once the original design is equivalent to a Highway Net with scalar gates. This also goes against the suggestions in the literature not to add gates to shortcut connections in order to keep an uncorrupted gradient flow through the network.\n\n- Added Gated Plain Networks to the table with mean k values, along with an explanation for the significant difference when compared to mean k's of Gated ResNets.\n\n- Added 'Understanding deep learning requires rethinking generalization' to bibliography.\n\n- Rephrased a few parts when augmented models are compared to Highway Nets, showing more clearly the differences between the two designs. Also added a brief discussion regarding the impact for Highway Nets of the new results (Gated Plain Nets).\n\n\nWe are currently gathering results to introduce the following changes to the next revision:\n\n- Add results for more aggressive depths (200+ layers), in order to better compare different models.\n\n- Add results for Gated Plain Net on CIFAR.\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"TITLE": "claims not convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThis paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.\n\nIt seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.\n\nThe proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "A simple gating mechanism", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.\n\nThe basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.\n\nDid the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?\n\nFor the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.\n\nFor CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.\n\nSome questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "16 Dec 2016", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the recent revisions of our work, we have performed the following main changes:\n\n- Run complex models on CIFAR-10 and on CIFAR-100, achieving 3.65% and 18.27% test error, respectively. We have then removed our belief that the technique could achieve SOTA results, since now we have empirical indications of it.\n- Added evaluation of the final k parameters for fully-connected and convolutional networks, showing interesting patterns for both architectures.\n- Added layer pruning experiment to Wide GResNet.\n- Made the intuition behind the distance to identity clearer.\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"TITLE": "Interesting approach for optimizing network architecture ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). \n\nUsing an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. \n\nAlthough good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "metric and experiments", "IS_META_REVIEW": false, "comments": "\n- \"assume that the squared parameter-wise distance between .. surrogate to the paths length\". Can you elaborate more on this? Why should this be the case? \n\n- Why do you substitute mean and variance with initialization mean variance in derivation in Eq 15 and 16?\n\n-  Regarding \"A comparison of the Total Squared Distance to Identity\u2026\": where is the comparison?\n\n- Why there is no Highway Neural Networks result in Table 5? \n\n- In Table 5, why does  He et al. (2015b)  give 6.61 which does not match with 6.43 in that paper? \n\n- Regarding \"Our results don\u2019t surpass the state-of-the-art, which was expected considering the hardware limitations. However, taking into account the improvement observed when augmenting a smaller Wide ResNet, we believe that the technique proposed can be used to surpass the state-of-the-art\", you may use that result for sanity check during the empirical work but how can you claim that you will get any improvement over baselines in final comparison? \n\n- Baselines have Imagenet results. In order to obtain satisfactory comparison, you would need that too. Are there any results on Imagenet comparison?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Questions about Experiment design", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "questions regarding SDI and layer pruning ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). \n\nUsing an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. \n\nAlthough good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Feb 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "We have also added a section (3.3) that connects our technique to the Unrolled Iterative Estimation interpretation of Highway and ResNets. We further elaborate on how the learned k values are indicators of the abstraction jumps in representations defined in Greff et al, and analyze the k values for both experiments (MNIST and CIFAR) under this perspective.", "OTHER_KEYS": "Pedro Savarese"}, {"DATE": "03 Feb 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the last revision of our work, we have performed the following main changes:\n\n- Added 4 extra experiments comparing Wide ResNet and their augmented counterparts (Table 5) on CIFAR-100. With this we hope to provide stronger indications of the generality and advantages of our technique.\n\nWe are currently running Gated PlainNets on CIFAR-100.\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"DATE": "30 Jan 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the last revision of our work, we have performed the following main changes:\n\n- Added 6 extra experiments comparing Wide ResNet and their augmented counterparts (Table 4). With this we hope to provide stronger indications of the generality and advantages of our technique.\n\n\nWe are currently running the same 6 models on CIFAR-100 to add them to the table. Are have also run Gated PlainNets (u = g(k)f(x) + (1 - g(k))x) on CIFAR-10, as suggested by reviewer 3, and will add results after running on CIFAR-100 as well.\n\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"DATE": "24 Jan 2017", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the last revision of our work, we have performed the following main changes:\n\n- Added results with u = g(k)f(x) + (1 - g(k))x (Gated Plain Networks), along with some intuitions of why it outperformed non-augmented Residual Networks. This result suggests that an extensive study on different gating mechanisms for Highway Neural Networks can be extremely fruitful, once the original design is equivalent to a Highway Net with scalar gates. This also goes against the suggestions in the literature not to add gates to shortcut connections in order to keep an uncorrupted gradient flow through the network.\n\n- Added Gated Plain Networks to the table with mean k values, along with an explanation for the significant difference when compared to mean k's of Gated ResNets.\n\n- Added 'Understanding deep learning requires rethinking generalization' to bibliography.\n\n- Rephrased a few parts when augmented models are compared to Highway Nets, showing more clearly the differences between the two designs. Also added a brief discussion regarding the impact for Highway Nets of the new results (Gated Plain Nets).\n\n\nWe are currently gathering results to introduce the following changes to the next revision:\n\n- Add results for more aggressive depths (200+ layers), in order to better compare different models.\n\n- Add results for Gated Plain Net on CIFAR.\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"TITLE": "claims not convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThis paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.\n\nIt seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.\n\nThe proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "A simple gating mechanism", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.\n\nThe basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.\n\nDid the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?\n\nFor the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.\n\nFor CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.\n\nSome questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "16 Dec 2016", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "In the recent revisions of our work, we have performed the following main changes:\n\n- Run complex models on CIFAR-10 and on CIFAR-100, achieving 3.65% and 18.27% test error, respectively. We have then removed our belief that the technique could achieve SOTA results, since now we have empirical indications of it.\n- Added evaluation of the final k parameters for fully-connected and convolutional networks, showing interesting patterns for both architectures.\n- Added layer pruning experiment to Wide GResNet.\n- Made the intuition behind the distance to identity clearer.\n\nWe thank all the received feedback.", "OTHER_KEYS": "Pedro Savarese"}, {"TITLE": "Interesting approach for optimizing network architecture ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). \n\nUsing an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. \n\nAlthough good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "metric and experiments", "IS_META_REVIEW": false, "comments": "\n- \"assume that the squared parameter-wise distance between .. surrogate to the paths length\". Can you elaborate more on this? Why should this be the case? \n\n- Why do you substitute mean and variance with initialization mean variance in derivation in Eq 15 and 16?\n\n-  Regarding \"A comparison of the Total Squared Distance to Identity\u2026\": where is the comparison?\n\n- Why there is no Highway Neural Networks result in Table 5? \n\n- In Table 5, why does  He et al. (2015b)  give 6.61 which does not match with 6.43 in that paper? \n\n- Regarding \"Our results don\u2019t surpass the state-of-the-art, which was expected considering the hardware limitations. However, taking into account the improvement observed when augmenting a smaller Wide ResNet, we believe that the technique proposed can be used to surpass the state-of-the-art\", you may use that result for sanity check during the empirical work but how can you claim that you will get any improvement over baselines in final comparison? \n\n- Baselines have Imagenet results. In order to obtain satisfactory comparison, you would need that too. Are there any results on Imagenet comparison?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Questions about Experiment design", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "questions regarding SDI and layer pruning ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Pedro H. P. Savarese, Leonardo O. Mazza, Daniel R. Figueiredo", "accepted": false, "id": "744"}