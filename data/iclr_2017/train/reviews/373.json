{"conference": "ICLR 2017 conference submission", "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks", "abstract": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering.  However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained.  These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.\n\nIt'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "One weak and one positive review without much concrete substance. The third review is positive, but the experiments are not that convincing: the gains from transfer are small in table 3 and in table 2 it is unclear how strong the baselines are. Given how competitive ICLR is, the area chair has no alternative than to unfortunately reject this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Dec 2016", "TITLE": "Table 2 updated to include comparison of different architectures for one task pair.", "IS_META_REVIEW": false, "comments": "We study the effects of using T-A, T-B, and T-C when transferring from PTB to Genia. The results are included in the lower part of Table 2. It is clear that the performance gain decreases when less parameters are shared.", "OTHER_KEYS": "Zhilin Yang"}, {"TITLE": "Interesting research direction, but the scientific advances are limited and the experiments are not very convincing.", "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks.\n\nThe field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared \"layers\" being dependent of the task of interest.\n\nThe novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks.\n\nThe experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale.\n\nFigure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results.\n\nOverall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Accept", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Authors' response well answered my questions. Thanks!\nEvaluation not changed.\n\n###\n\nThis paper proposes a hierarchical framework of transfer learning for sequence tagging, which is expected to help the target task with the source task, by sharing as many levels of representation as possible. It is a general framework for various neural models. The paper has extensive and solid experiments, and the performance is competitive with the state of the art on multiple benchmark datasets. The framework is clear by itself, except that more details about training procedure, i.e. sec-3.3, need to be added. \n\nThe experimental results show that for some task pairs {s,t}, this framework can help low-resource target task t, and the improvement increases with more levels of representations can be shared. Firstly, I suggest that the terms *source* and *target* should be more precisely defined in the current framework, because, due to Sec-3.3, the s and t in each pair are sort of interchangeable. That is, either of them can be the *source* or *target* task, especially when p(X=s)=p(X=t)=0.5 is used in the task sampling. The difference is: one is low-resourced and the other is not. Thus it could be thought of as multi-tasking between tasks with imbalanced resource. So one question is: does this framework simultaneously help both tasks in the pair, by learning more generalizable representations for different domains/applications/languages? Or is it mostly likely to only help the low-resourced one? Does it come with sacrifice on the high-resourced side? \n\nSecondly, as the paper shows that the low-resourced tasks are improved for the selected task pairs, it would also be interesting and helpful to know how often this could happen. That is, when the tasks are randomly paired (one chosen from a low-resource pool and the other from a high resource pool), how often could this framework help the low-resourced one?\n\nMoreover, the choice of T-A/T-B/T-C lies intuitively in how many levels of representation *could* be shared as possible. This implicitly assumes share more, help more. Although I tend to believe so, it would be interesting to have some empirical comparison. For example, one could perhaps select some cross-domain pair, and see if T-A > T-B > T-C on such pairs, as mentioned in the author\u2019s answer to the pre-review question. \n\nIn general, I think this is a solid paper, and more exploration could be done in this direction. So I tend to accept this paper. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Accept", "comments": "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.\n\nIt'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "variance in experiments", "SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 4, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "14 Dec 2016"}, {"DATE": "01 Dec 2016", "TITLE": "Details about training framework and experiments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "Are the different transfer models ever compared on the same task?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.\n\nIt'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "One weak and one positive review without much concrete substance. The third review is positive, but the experiments are not that convincing: the gains from transfer are small in table 3 and in table 2 it is unclear how strong the baselines are. Given how competitive ICLR is, the area chair has no alternative than to unfortunately reject this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Dec 2016", "TITLE": "Table 2 updated to include comparison of different architectures for one task pair.", "IS_META_REVIEW": false, "comments": "We study the effects of using T-A, T-B, and T-C when transferring from PTB to Genia. The results are included in the lower part of Table 2. It is clear that the performance gain decreases when less parameters are shared.", "OTHER_KEYS": "Zhilin Yang"}, {"TITLE": "Interesting research direction, but the scientific advances are limited and the experiments are not very convincing.", "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks.\n\nThe field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared \"layers\" being dependent of the task of interest.\n\nThe novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks.\n\nThe experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale.\n\nFigure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results.\n\nOverall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Accept", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Authors' response well answered my questions. Thanks!\nEvaluation not changed.\n\n###\n\nThis paper proposes a hierarchical framework of transfer learning for sequence tagging, which is expected to help the target task with the source task, by sharing as many levels of representation as possible. It is a general framework for various neural models. The paper has extensive and solid experiments, and the performance is competitive with the state of the art on multiple benchmark datasets. The framework is clear by itself, except that more details about training procedure, i.e. sec-3.3, need to be added. \n\nThe experimental results show that for some task pairs {s,t}, this framework can help low-resource target task t, and the improvement increases with more levels of representations can be shared. Firstly, I suggest that the terms *source* and *target* should be more precisely defined in the current framework, because, due to Sec-3.3, the s and t in each pair are sort of interchangeable. That is, either of them can be the *source* or *target* task, especially when p(X=s)=p(X=t)=0.5 is used in the task sampling. The difference is: one is low-resourced and the other is not. Thus it could be thought of as multi-tasking between tasks with imbalanced resource. So one question is: does this framework simultaneously help both tasks in the pair, by learning more generalizable representations for different domains/applications/languages? Or is it mostly likely to only help the low-resourced one? Does it come with sacrifice on the high-resourced side? \n\nSecondly, as the paper shows that the low-resourced tasks are improved for the selected task pairs, it would also be interesting and helpful to know how often this could happen. That is, when the tasks are randomly paired (one chosen from a low-resource pool and the other from a high resource pool), how often could this framework help the low-resourced one?\n\nMoreover, the choice of T-A/T-B/T-C lies intuitively in how many levels of representation *could* be shared as possible. This implicitly assumes share more, help more. Although I tend to believe so, it would be interesting to have some empirical comparison. For example, one could perhaps select some cross-domain pair, and see if T-A > T-B > T-C on such pairs, as mentioned in the author\u2019s answer to the pre-review question. \n\nIn general, I think this is a solid paper, and more exploration could be done in this direction. So I tend to accept this paper. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Accept", "comments": "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.\n\nIt'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "variance in experiments", "SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 4, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "14 Dec 2016"}, {"DATE": "01 Dec 2016", "TITLE": "Details about training framework and experiments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "Are the different transfer models ever compared on the same task?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016"}], "authors": "Zhilin Yang, Ruslan Salakhutdinov, William W. Cohen", "accepted": true, "id": "373"}