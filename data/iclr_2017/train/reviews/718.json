{"conference": "ICLR 2017 conference submission", "title": "Multiagent System for Layer Free Network", "abstract": "We propose a multiagent system that have feedforward networks as its subset  while free from layer structure with matrix-vector scheme. Deep networks are often compared to the brain neocortex or visual perception system. One of the largest difference from human brain is the use of matrix-vector multiplication based on layer architecture. It would help understanding the way human brain works if we manage to develop good deep network model without the layer architecture while preserving their performance. The brain neocortex works as an aggregation of the local level interactions between neurons,  which is rather similar to multiagent system consists of autonomous partially observing agents than units aligned in column vectors and manipulated by global level algorithm. Therefore we suppose that it is an effective approach for developing more biologically plausible model while preserving compatibility with deep networks to alternate units with multiple agents. Our method also has advantage in scalability and memory efficiency. We reimplemented Stacked Denoising Autoencoder(SDAE) as a concrete instance with our multiagent system and verified its equivalence with the standard SDAE from both theoritical and empirical perspectives. Additionary, we also proposed a variant of our multiagent SDAE named \"Sparse Connect SDAE\", and showed its computational advantage with the MNIST dataset.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers were consistent in their review that they thought this was a strong rejection.\n Two of the reviewers expressed strong confidence in their reviews.\n The main arguments made by the reviewers against acceptance were:\n Lack of novelty (R2, R3)\n Lack of knowledge of literature and development history; particularly with respect to biological inspiration of ANNs (R3)\n Inappropriate baseline comparison (R2)\n Not clear (R1)\n \n The authors did not provide a response to the official reviews. Therefore I have decided to follow the consensus towards rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The multiagent system is proposed as a generalization of neural network. The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph. Unfortunately, I don't find the proposed system different from the framework of artificial neural network. Although for today's neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture. In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer/connectivity structures while considering each neuron as layer. The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "25 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Lack of novelty and of knowledge of the relevant literature and development history", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper reframes feed forward neural networks as a multi-agent system.\n\nIt seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications. This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel. The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.\n\nBesides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.\n\nPulling in useful insights from a different communitiy such as multi-agent systems would be most welcome. But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency. This is not achieved in the present paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 1, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "04 Dec 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "What is a multiagent network ?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers were consistent in their review that they thought this was a strong rejection.\n Two of the reviewers expressed strong confidence in their reviews.\n The main arguments made by the reviewers against acceptance were:\n Lack of novelty (R2, R3)\n Lack of knowledge of literature and development history; particularly with respect to biological inspiration of ANNs (R3)\n Inappropriate baseline comparison (R2)\n Not clear (R1)\n \n The authors did not provide a response to the official reviews. Therefore I have decided to follow the consensus towards rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The multiagent system is proposed as a generalization of neural network. The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph. Unfortunately, I don't find the proposed system different from the framework of artificial neural network. Although for today's neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture. In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer/connectivity structures while considering each neuron as layer. The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "25 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Lack of novelty and of knowledge of the relevant literature and development history", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper reframes feed forward neural networks as a multi-agent system.\n\nIt seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications. This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel. The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.\n\nBesides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.\n\nPulling in useful insights from a different communitiy such as multi-agent systems would be most welcome. But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency. This is not achieved in the present paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 1, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "04 Dec 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "What is a multiagent network ?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Hiroki Kurotaki, Kotaro Nakayama, Yutaka Matsuo", "accepted": false, "id": "718"}