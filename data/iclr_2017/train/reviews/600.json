{"conference": "ICLR 2017 conference submission", "title": "Group Sparse CNNs for Question Sentence Classification with Answer Sets", "abstract": "Classifying question sentences into their corresponding categories is an important task with wide applications, for example in many websites' FAQ sections.  However, traditional question classification techniques do not fully utilize the well-prepared answer data which has great potential for improving question sentence representations which could lead to better classification performance. In order to encode answer information into question representation, we first introduce novel group sparse autoencoders which could utilize the group information in the answer set to refine question representation. We then propose a new group sparse convolutional neural network which could naturally learn the question representation with respect to their corresponding answers by implanting the group sparse autoencoders into the traditional convolutional neural network. The proposed model show significant improvements over strong baselines on four datasets.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. \n\nPros: \n- group-sparse auto-encoder seems new to me.\n- extensive experiments on QA tasks. \n\nCons:\n- The idea is somewhat incremental.\n- Writing need to be improved. \n- Lack of ablation studies to show the effectiveness of the proposed approach. \n\nMoreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes to use a group sparsity penalty to train an autoencoder on question answering. The idea to leverage hierarchies of categories in labels is an appealing one. However the paper has problems:\n - it is not clear. In particular, some of the equations do not make sense. This has been pointed by reviewers and not corrected.\n - the choice of the particular group sparsity penalty is not well justified or empirically validated with ablation experiments: experiments lack key comparisons and simply compare to unrelated baselines.\n In its current form, the paper cannot be recommended for acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Clarity is low, experiments are not convincing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision). The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset. The paper also propose to combine both group-level and individual level sparsity as in Equation (9). \n\nClarity of the paper is a bit low. \n- Do you use only p-th group's activation for reconstruction? If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only? \n- In Equation (7), RHS misses the summation over p, and wondering it is a simple typo.\n- Is the algorithm end-to-end trainable? It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs).\n\nOther comments are as follows:\n- Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised. That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn't. I think the fair comparison should be to autoencoders that combines classification for their objective function.\n- Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim. For example, the first row contains many filters that doesn't look like 1 (e.g., very last column looks like 3).\n- Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments?\n- The comparison to the baseline model is missing. I believe the baseline model shouldn't be the sequential CNN, but the sequential CNN + sparse autoencoder. In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of \\alpha and \\beta.\n\nMissing reference:\nShang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. \n\nPros: \n- group-sparse auto-encoder seems new to me.\n- extensive experiments on QA tasks. \n\nCons:\n- The idea is somewhat incremental.\n- Writing need to be improved. \n- Lack of ablation studies to show the effectiveness of the proposed approach. \n\nMoreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "17 Dec 2016", "TITLE": "group Lasso regularization on DNNs", "IS_META_REVIEW": false, "comments": "A recent paper using group Lasso regularization on DNNs was proposed (", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper propose to classify questions by leveraging corresponding answers. The proposed method uses group sparse autoencoders to model question groups.\n\nThe proposed method offers improved accuracy over baselines. But the baseline used is a little stale. Would be interesting to see how it compares to more recent CNN and RNN based methods. It would also be interesting to see the contribution of each components. For example, how much GSA contributed to the improvement.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Why not compare against a standard sparse auto-encoder?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. \n\nPros: \n- group-sparse auto-encoder seems new to me.\n- extensive experiments on QA tasks. \n\nCons:\n- The idea is somewhat incremental.\n- Writing need to be improved. \n- Lack of ablation studies to show the effectiveness of the proposed approach. \n\nMoreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes to use a group sparsity penalty to train an autoencoder on question answering. The idea to leverage hierarchies of categories in labels is an appealing one. However the paper has problems:\n - it is not clear. In particular, some of the equations do not make sense. This has been pointed by reviewers and not corrected.\n - the choice of the particular group sparsity penalty is not well justified or empirically validated with ablation experiments: experiments lack key comparisons and simply compare to unrelated baselines.\n In its current form, the paper cannot be recommended for acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Clarity is low, experiments are not convincing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision). The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset. The paper also propose to combine both group-level and individual level sparsity as in Equation (9). \n\nClarity of the paper is a bit low. \n- Do you use only p-th group's activation for reconstruction? If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only? \n- In Equation (7), RHS misses the summation over p, and wondering it is a simple typo.\n- Is the algorithm end-to-end trainable? It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs).\n\nOther comments are as follows:\n- Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised. That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn't. I think the fair comparison should be to autoencoders that combines classification for their objective function.\n- Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim. For example, the first row contains many filters that doesn't look like 1 (e.g., very last column looks like 3).\n- Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments?\n- The comparison to the baseline model is missing. I believe the baseline model shouldn't be the sequential CNN, but the sequential CNN + sparse autoencoder. In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of \\alpha and \\beta.\n\nMissing reference:\nShang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. \n\nPros: \n- group-sparse auto-encoder seems new to me.\n- extensive experiments on QA tasks. \n\nCons:\n- The idea is somewhat incremental.\n- Writing need to be improved. \n- Lack of ablation studies to show the effectiveness of the proposed approach. \n\nMoreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "17 Dec 2016", "TITLE": "group Lasso regularization on DNNs", "IS_META_REVIEW": false, "comments": "A recent paper using group Lasso regularization on DNNs was proposed (", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper propose to classify questions by leveraging corresponding answers. The proposed method uses group sparse autoencoders to model question groups.\n\nThe proposed method offers improved accuracy over baselines. But the baseline used is a little stale. Would be interesting to see how it compares to more recent CNN and RNN based methods. It would also be interesting to see the contribution of each components. For example, how much GSA contributed to the improvement.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Why not compare against a standard sparse auto-encoder?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Mingbo Ma, Liang Huang, Bing Xiang, Bowen Zhou", "accepted": false, "id": "600"}