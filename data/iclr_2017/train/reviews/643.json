{"conference": "ICLR 2017 conference submission", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model\u2019s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the \u201cneuronal birth\u201d is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. \u201cNeuronal death\u201d is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "I'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).\n The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:\n - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity\n - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the \"state-of-the-art\" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.\n In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.\n Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Authors' response to all the three reviews.", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their insightful comments that helped us to improve the paper; to address those comments, we modified the paper as follows:\n\n1.\tAdded a subsection 3.1 in section 3, describing various details of the proposed algorithm, as suggested by Reviewer1.\n\n2.\tAdded a section (Appendix, B9, Figures 21-26) demonstrating stability of our results (our method consistently outperforming the non-adaptive dictionary learning)  while varying all of the algorithm\u2019s parameters, one at a time, over a wide range of values.  \n\n3.\tAdded a section (Appendix, B7, Fig. 19) with new empirical results to address the concern of Reviewer3 regarding comparison with a version of the baseline method which involved re-initialization of ``dead\u2019\u2019 elements; we did not observe any significance difference in the performance of such augmented method versus the original baseline (the number of dead elements appearing in the baseline method was quite small); on the contrary, our method employing group-sparsity was removing larger number of ``weak\u2019\u2019 (low l2-norm) elements and yielded better performance due to such explicit regularization.\n\n4.\tAdded a section (Appendix, B8, Fig. 20) with new results evaluating our method under different ordering of the input data, to address the question of the Reviewer1 regarding the potential sensitivity of our results to the ordering of the training domains  (e.g., from easier to harder). No significant changes  were observed due to permutation of the input datasets (our method was still outperforming the baseline), i.e. buildings images were not necessarily \u2018simpler\u2019 than natural ones.\n\n5.\tA summary of our contributions is added at the end of the intro section. Our main point is that the contribution of this paper is truly novel and nontrivial. Indeed,\n\n    a.\tWe are the first to propose an online model-selection approach to dictionary learning (DL) based on dynamic addition and deletion of the elements (hidden units), which leads to significant performance improvements over the state-of-art online DL, especially in non-stationary settings. None of the reviewers have provided a reference that would contradict the above statement.\n\n    b.\tWhile some prior work (e.g., Bengio et al, NIPS 2009) involved deletion of dictionary elements, no addition was involved in that approach and, more importantly, their approach only concerned an offline dictionary learning, not online. Similarly, approaches to neural nets involving cascade correlations and other methods adding hidden units are lacking regularization (deletion) that we achieve via group sparsity constraint.    Finally, Reviewer3 mentions prior work on off-line MDL-based model selection in dictionary learning, but the major difference is, again, that our setting is online learning, while their approach is off-line.  \n\n    c.\tFurthermore, none of the prior approaches explored the interplay between addition and deletion, neither theoretically nor empirically, while we performed such evaluation and identified regimes when our online model-selection would outperform the non-adaptive baseline   (Section 5).\n\n\nThus, to the best of our knowledge, our proposed method is completely novel, and provides a considerable improvement over the state-of-art, especially when the input is nonstationary.\n  \n\nOur detailed replies to each review are put as separate comments below.\n", "OTHER_KEYS": "Sahil Garg"}, {"TITLE": "Simple interesting modified online dictionary learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. \n\nThe online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments.\n\nThe overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. \n\nStill: this is a nice work!   ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting idea related to biology, good experimental validation, but more work is probably needed", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is coming, from a nonstationary distribution.\n\nI have a few points to make:\n\n1. the algorithm could be discussed more, to give a more solid view of the contribution. The technique is not novel in spirit. Codes are added when they are needed, and removed when they dont do much. \n\n2. Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc)\n\nIn a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain.\n\nThe paper is interesting, the idea useful with some interesting insights. I am not sure it is ready for publication yet.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nI'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Question regarding baseline", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "I'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).\n The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:\n - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity\n - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the \"state-of-the-art\" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.\n In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.\n Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Authors' response to all the three reviews.", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their insightful comments that helped us to improve the paper; to address those comments, we modified the paper as follows:\n\n1.\tAdded a subsection 3.1 in section 3, describing various details of the proposed algorithm, as suggested by Reviewer1.\n\n2.\tAdded a section (Appendix, B9, Figures 21-26) demonstrating stability of our results (our method consistently outperforming the non-adaptive dictionary learning)  while varying all of the algorithm\u2019s parameters, one at a time, over a wide range of values.  \n\n3.\tAdded a section (Appendix, B7, Fig. 19) with new empirical results to address the concern of Reviewer3 regarding comparison with a version of the baseline method which involved re-initialization of ``dead\u2019\u2019 elements; we did not observe any significance difference in the performance of such augmented method versus the original baseline (the number of dead elements appearing in the baseline method was quite small); on the contrary, our method employing group-sparsity was removing larger number of ``weak\u2019\u2019 (low l2-norm) elements and yielded better performance due to such explicit regularization.\n\n4.\tAdded a section (Appendix, B8, Fig. 20) with new results evaluating our method under different ordering of the input data, to address the question of the Reviewer1 regarding the potential sensitivity of our results to the ordering of the training domains  (e.g., from easier to harder). No significant changes  were observed due to permutation of the input datasets (our method was still outperforming the baseline), i.e. buildings images were not necessarily \u2018simpler\u2019 than natural ones.\n\n5.\tA summary of our contributions is added at the end of the intro section. Our main point is that the contribution of this paper is truly novel and nontrivial. Indeed,\n\n    a.\tWe are the first to propose an online model-selection approach to dictionary learning (DL) based on dynamic addition and deletion of the elements (hidden units), which leads to significant performance improvements over the state-of-art online DL, especially in non-stationary settings. None of the reviewers have provided a reference that would contradict the above statement.\n\n    b.\tWhile some prior work (e.g., Bengio et al, NIPS 2009) involved deletion of dictionary elements, no addition was involved in that approach and, more importantly, their approach only concerned an offline dictionary learning, not online. Similarly, approaches to neural nets involving cascade correlations and other methods adding hidden units are lacking regularization (deletion) that we achieve via group sparsity constraint.    Finally, Reviewer3 mentions prior work on off-line MDL-based model selection in dictionary learning, but the major difference is, again, that our setting is online learning, while their approach is off-line.  \n\n    c.\tFurthermore, none of the prior approaches explored the interplay between addition and deletion, neither theoretically nor empirically, while we performed such evaluation and identified regimes when our online model-selection would outperform the non-adaptive baseline   (Section 5).\n\n\nThus, to the best of our knowledge, our proposed method is completely novel, and provides a considerable improvement over the state-of-art, especially when the input is nonstationary.\n  \n\nOur detailed replies to each review are put as separate comments below.\n", "OTHER_KEYS": "Sahil Garg"}, {"TITLE": "Simple interesting modified online dictionary learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. \n\nThe online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments.\n\nThe overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. \n\nStill: this is a nice work!   ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting idea related to biology, good experimental validation, but more work is probably needed", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is coming, from a nonstationary distribution.\n\nI have a few points to make:\n\n1. the algorithm could be discussed more, to give a more solid view of the contribution. The technique is not novel in spirit. Codes are added when they are needed, and removed when they dont do much. \n\n2. Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc)\n\nIn a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain.\n\nThe paper is interesting, the idea useful with some interesting insights. I am not sure it is ready for publication yet.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nI'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Question regarding baseline", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano", "accepted": false, "id": "643"}