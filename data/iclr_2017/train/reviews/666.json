{"conference": "ICLR 2017 conference submission", "title": "OMG: Orthogonal Method of Grouping With Application of K-Shot Learning", "abstract": "Training a classifier with only a few examples remains a significant barrier when using neural networks with large number of parameters. Though various specialized network architectures have been proposed for these k-shot learning tasks to avoid overfitting, a question remains: is there a generalizable framework for the k-shot learning problem that can leverage existing deep models as well as avoid model overfitting? In this paper, we proposed a generalizable k-shot learning framework that can be used on any pre-trained network, by grouping network parameters to produce a low-dimensional representation of the parameter space. The grouping of the parameters is based on an orthogonal decomposition of the parameter space. To avoid overfitting, groups of parameters will be updated together during the k-shot training process. Furthermore, this framework can be integrated with any existing popular deep neural networks such as VGG, GoogleNet, ResNet, without any changes in the original network structure or any sacrifices in performance. We evaluate our framework on a wide range of intra/inter-dataset k-shot learning tasks and show state-of-the-art performance.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.\n\n1. There are many typos in this manuscript. \n2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. \n3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.\n4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. \n\nThe idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication:\n\nPerformance:\nThe method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It's not clear how this method compares against them.\n\nMissing explanation:\nExperimental setting for k-shot learning should be more detailed.\n\nMeasure:\nAccuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. \n\nMany grammatical errors and inappropriate formatting of citations, such as:\nM. et al. (2011)\nImageNet (Alex et al. (2012))\nJudy et al. (2013): this reference appears three times in the reference section.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a regularization technique for k-shot learning based on orthogonal grouping of units in a neural network. The units within a group are forced to be maximally similar, at the same time the units from different groups are encouraged to be orthogonal. While I like the motivation of the approach, the empirical analysis provided in the paper doesn\u2019t look particularly convincing.\n\nMy main concerns are the following:\n\n1. The method is sensitive to the values of alpha and beta and a poor choice of those hyperparameters can lead to a quite drastic drop in performance comparing the minor gains one gets when alpha and beta are set properly.\n\n2. It seems strange that the best performance is obtained when the group's size ratio is 0.5. From the figures in the paper, it follows that usually, one has more \u201corthogonal\u201d groups in a filter bank. I have an impression that the empirical evidence doesn\u2019t align well with the motivation of the proposed approach.\n\n3. The paper contains a significant amount of typos and incorrectly formatted references. There are also several places in the manuscript that I found hard to understand due to unusual phrasing.\n\nI would like to thank the authors for answering/addressing my pre-review questions. I would be grateful if the authors could provide more clarifications of the following:\n\n1. Question 2: I\u2019m not sure if modifying \\theta_{map} alone would result in any learning at all. Do I understand correctly that \\theta_{map} is only used to define groups? If so, then I don\u2019t see how the proposed method can be used in the purely unsupervised regime.\n\n2. Question 3: I was not referring to the fixed clustering based on the filter of the pre-trained network. One can perform that clustering at every step of the k-shot learning process. I\u2019m not sure I understand why the authors visualize grouping of _filters_ while in the actual algorithm they group _activations_. \n\nOverall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Dimensionality Reduction Approach.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.\n\n1. There are many typos in this manuscript. \n2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. \n3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.\n4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "05 Dec 2016", "TITLE": "quesitons", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.\n\n1. There are many typos in this manuscript. \n2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. \n3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.\n4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. \n\nThe idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication:\n\nPerformance:\nThe method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It's not clear how this method compares against them.\n\nMissing explanation:\nExperimental setting for k-shot learning should be more detailed.\n\nMeasure:\nAccuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. \n\nMany grammatical errors and inappropriate formatting of citations, such as:\nM. et al. (2011)\nImageNet (Alex et al. (2012))\nJudy et al. (2013): this reference appears three times in the reference section.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a regularization technique for k-shot learning based on orthogonal grouping of units in a neural network. The units within a group are forced to be maximally similar, at the same time the units from different groups are encouraged to be orthogonal. While I like the motivation of the approach, the empirical analysis provided in the paper doesn\u2019t look particularly convincing.\n\nMy main concerns are the following:\n\n1. The method is sensitive to the values of alpha and beta and a poor choice of those hyperparameters can lead to a quite drastic drop in performance comparing the minor gains one gets when alpha and beta are set properly.\n\n2. It seems strange that the best performance is obtained when the group's size ratio is 0.5. From the figures in the paper, it follows that usually, one has more \u201corthogonal\u201d groups in a filter bank. I have an impression that the empirical evidence doesn\u2019t align well with the motivation of the proposed approach.\n\n3. The paper contains a significant amount of typos and incorrectly formatted references. There are also several places in the manuscript that I found hard to understand due to unusual phrasing.\n\nI would like to thank the authors for answering/addressing my pre-review questions. I would be grateful if the authors could provide more clarifications of the following:\n\n1. Question 2: I\u2019m not sure if modifying \\theta_{map} alone would result in any learning at all. Do I understand correctly that \\theta_{map} is only used to define groups? If so, then I don\u2019t see how the proposed method can be used in the purely unsupervised regime.\n\n2. Question 3: I was not referring to the fixed clustering based on the filter of the pre-trained network. One can perform that clustering at every step of the k-shot learning process. I\u2019m not sure I understand why the authors visualize grouping of _filters_ while in the actual algorithm they group _activations_. \n\nOverall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Dimensionality Reduction Approach.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.\n\n1. There are many typos in this manuscript. \n2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. \n3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.\n4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "05 Dec 2016", "TITLE": "quesitons", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Haoqi Fan, Yu Zhang, Kris M. Kitani", "accepted": false, "id": "666"}