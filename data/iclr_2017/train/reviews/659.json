{"conference": "ICLR 2017 conference submission", "title": "Sequence to Sequence Transduction with Hard Monotonic Attention", "abstract": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "While this area chair disagrees with some reviewers about (1) the narrowness of the approach's applicability and hence lack of relevance to ICLR, and also (2) the fairness of the methodology, it is nonetheless clear that a stronger case needs to be made for novelty and applicability.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Nice idea, but limited applicability (need an auxiliary solver for alignments)", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. \n\nThe model is moderately novel - similar architecture was proposed for speech recognition (", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. \n\nI found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors \"suggest\" that their approach is sufficient for shorter sequences, but don't compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).\n\nIn summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small. That said, I feel this work would make a very good short paper. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Not novel enough", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "05 Dec 2016", "TITLE": "Attention pretraining", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Clarifications for choice to ensemble + not do beamsearch", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "30 Nov 2016", "TITLE": "Missing prior work", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "typos in section 2.3 \"Encoder\"", "IS_META_REVIEW": false, "comments": "In the \"Encoder\" section, the authors appear to use plain math italic x_i for input elements (characters) and boldface x_i for biLSTM encodings.\n\nI believe the subscript in e_{x_i} should be the plain version since here x_i represents an input element.\nAlso, the definition of boldface x_i should take e_{x_i} and not boldface x_i as input; otherwise the definition is circular.\n\n(The numeric subscripts i should not be boldfaced either since they are also not vectors; but that typo is less confusing.)\n\nCorrect?\n", "OTHER_KEYS": "Jason Eisner"}, {"IS_META_REVIEW": true, "comments": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "While this area chair disagrees with some reviewers about (1) the narrowness of the approach's applicability and hence lack of relevance to ICLR, and also (2) the fairness of the methodology, it is nonetheless clear that a stronger case needs to be made for novelty and applicability.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Nice idea, but limited applicability (need an auxiliary solver for alignments)", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. \n\nThe model is moderately novel - similar architecture was proposed for speech recognition (", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. \n\nI found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors \"suggest\" that their approach is sufficient for shorter sequences, but don't compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).\n\nIn summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small. That said, I feel this work would make a very good short paper. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Not novel enough", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "05 Dec 2016", "TITLE": "Attention pretraining", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Clarifications for choice to ensemble + not do beamsearch", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "30 Nov 2016", "TITLE": "Missing prior work", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "typos in section 2.3 \"Encoder\"", "IS_META_REVIEW": false, "comments": "In the \"Encoder\" section, the authors appear to use plain math italic x_i for input elements (characters) and boldface x_i for biLSTM encodings.\n\nI believe the subscript in e_{x_i} should be the plain version since here x_i represents an input element.\nAlso, the definition of boldface x_i should take e_{x_i} and not boldface x_i as input; otherwise the definition is circular.\n\n(The numeric subscripts i should not be boldfaced either since they are also not vectors; but that typo is less confusing.)\n\nCorrect?\n", "OTHER_KEYS": "Jason Eisner"}], "authors": "Roee Aharoni, Yoav Goldberg", "accepted": false, "id": "659"}