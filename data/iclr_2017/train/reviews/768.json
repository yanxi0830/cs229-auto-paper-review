{"conference": "ICLR 2017 conference submission", "title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "21 Jan 2017", "TITLE": "Update", "IS_META_REVIEW": false, "comments": "We've already updated the paper. \n- The abstract and introduction have been rewritten with more explanation (on the motivation) and comparison. \n- The difference from ensemble models was highlighted in the related works.\n- We found that the Fig 1. is a bit confusing and have already updated it in the revised revision.\n- Eqn 3. has been corrected.\n- New results on ImageNet dataset.", "OTHER_KEYS": "Yunpeng Chen"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.\n\n\nPros:\n\nProposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.\n\nThe use of foreground segmentation masks to improve image classification is also novel.\n\nThe method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.\n\n\nCons:\n\nThe evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.\n\nIt would be nice to see the results with \u201cIncomplete Privileged Information\u201d on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it\u2019s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.\n\nThe presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled \u201cA Unified Architecture: GoCNN\u201d, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).\n\nMinor: calling eq 3 a \u201cregression loss\u201d and writing \u201c||0 - x||\u201d rather than just \u201c||x||\u201d is not necessary and makes understanding more difficult -- I\u2019ve never seen a norm regularization term written this way or described as a \u201cregression to 0\u201d.\n\nMinor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the \u201csuppress foreground\u201d mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).\n\n\nAn additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?\n\nThe ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.\n\n===============\n\nEdit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.\n\n(I'll reiterate a very minor point about Figure 1 though: I still think the \"0\" and \"1\" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text \"suppress foreground\", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 30 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Unclear focus", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.\n\nPros:\n- The paper is clear and easy to follow\n- The experimental results seem to show some benefit from the proposed approach\n\nCons:\n(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation\n(2) No comparison with an ensemble\n(3) Full experiments on ImageNet under the \"partial privileged information\" setting would be more impactful\n\nThis paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.\n\nFirst, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.\n\nSecond, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.\n\nMinor suggestions / comments:\n- The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)\n- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Suppressing background features", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "28 Nov 2016", "TITLE": "Relationship to \"DeCov?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "21 Jan 2017", "TITLE": "Update", "IS_META_REVIEW": false, "comments": "We've already updated the paper. \n- The abstract and introduction have been rewritten with more explanation (on the motivation) and comparison. \n- The difference from ensemble models was highlighted in the related works.\n- We found that the Fig 1. is a bit confusing and have already updated it in the revised revision.\n- Eqn 3. has been corrected.\n- New results on ImageNet dataset.", "OTHER_KEYS": "Yunpeng Chen"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.\n\n\nPros:\n\nProposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.\n\nThe use of foreground segmentation masks to improve image classification is also novel.\n\nThe method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.\n\n\nCons:\n\nThe evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.\n\nIt would be nice to see the results with \u201cIncomplete Privileged Information\u201d on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it\u2019s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.\n\nThe presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled \u201cA Unified Architecture: GoCNN\u201d, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).\n\nMinor: calling eq 3 a \u201cregression loss\u201d and writing \u201c||0 - x||\u201d rather than just \u201c||x||\u201d is not necessary and makes understanding more difficult -- I\u2019ve never seen a norm regularization term written this way or described as a \u201cregression to 0\u201d.\n\nMinor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the \u201csuppress foreground\u201d mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).\n\n\nAn additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?\n\nThe ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.\n\n===============\n\nEdit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.\n\n(I'll reiterate a very minor point about Figure 1 though: I still think the \"0\" and \"1\" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text \"suppress foreground\", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 30 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Unclear focus", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.\n\nPros:\n- The paper is clear and easy to follow\n- The experimental results seem to show some benefit from the proposed approach\n\nCons:\n(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation\n(2) No comparison with an ensemble\n(3) Full experiments on ImageNet under the \"partial privileged information\" setting would be more impactful\n\nThis paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.\n\nFirst, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.\n\nSecond, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.\n\nMinor suggestions / comments:\n- The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)\n- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Suppressing background features", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "28 Nov 2016", "TITLE": "Relationship to \"DeCov?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Yunpeng Chen, Xiaojie Jin, Jiashi Feng, Shuicheng Yan", "accepted": false, "id": "768"}