{"conference": "ICLR 2017 conference submission", "title": "Generating Interpretable Images with Controllable Structure", "abstract": "We demonstrate improved text-to-image synthesis with controllable object locations using an extension of Pixel Convolutional Neural Networks (PixelCNN). In addition to conditioning on text, we show how the model can generate images conditioned on part keypoints and segmentation masks. The character-level text encoder and image generation network are jointly trained end-to-end via maximum likelihood. We establish quantitative baselines in terms of text and structure-conditional pixel log-likelihood for three data sets: Caltech-UCSD Birds (CUB), MPII Human Pose (MHP), and Common Objects in Context (MS-COCO).", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper extends PixelCNN to do text and location conditional image generation. The reviewers praise the diversity of the generated samples, which seems like the strongest result of the paper. On the other hand, they are concerned with their low resolution. The authors made an effort of showing a few high-resolution samples in the rebuttal, which indeed look better. Two reviewers mention that the work with respect to PixelCNN is very incremental, and the AC agrees. Overall, this paper is very borderline. While all reviewers became slightly more positive, none was particularly swayed. The paper will make a nice workshop contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Rebuttal, addition of 64x64 and 128x128 samples", "IS_META_REVIEW": false, "comments": "We thank all reviewers for their detailed feedback, and note that all reviewers recommend the paper for acceptance. Based on reviewer feedback about image resolution, we trained a 64x64 and 128x128 version of the model on the CUB dataset, results of which can be seen at sites.google.com/view/iclr2017figures. These and additional higher resolution results will be added to the revised paper. Since low-resolution was one of the main drawbacks to the paper according to the reviews, we hope that this can be reflected in an improved score.\n\nWe posted an updated version of the paper to OpenReview with an important correction to the caption of table 1: likelihoods are in *nats* per dim, not bits.\n\nBelow we respond to each review individually.\n\nAR1:\n\nThe time required for sampling is the main constraint on generating higher-resolution samples. However, we have been able to train some higher-resolution models in time for the rebuttal (see sites.google.com/view/iclr2017figures for some results). We agree that adding many more examples for comparison would help; these will be added in the upcoming version.  Please see the reply to AnonReviewer 3 for more precise details on timing and the experimental setup.\n\nPlease see other replies regarding comparison of GANs to pixelCNNs. In short, there are trade-offs between these two. We accentuate the trade-offs in the paper, in the hope that researchers will then know what are the key problems of each approach, and focus on developing solutions to those problems. A quantitative comparison is problematic because GANs don\u2019t provide us with likelihoods. We can however include more samples. To this extent we will add more high-resolution samples, including the ones already provided via the link above.\n\nAR2:\n\nWe appreciate your suggestion of adding more results to the appendix for the final version, or even better create a website where users can explore the generated images by both approaches. The figures in the paper provide a reasonable depiction of the trade-offs between existing GANs and pixelCNNs, but we agree adding more comparisons will help.\n\nAR3:\n\nMatching GANs:\nIn the paper we demonstrate that autoregressive models can do text- and location-conditional image generation; although as the reviewer points out, the resolution is much lower so \u201cmatch\u201d is not the right word; GANs and auto-regressive models have complementary strengths and weaknesses. We are happy to add a more thorough discussion of these issues earlier in the paper, rather than at the end. Figure 9 queries were from a figure of positive results in the paper to which we compare - so presumably favourable to GAN - but we agree that many more comparisons are needed to study the different types of mistakes each method makes.\n\nReplaying training data:\nOne way to check for this is to compare likelihoods for the training set and held-out sets of data. In our case we did not observe significant overfitting, so copying seems unlikely. We also observe significant diversity of samples even with fixed text and structure constraints. However, as noted in the GAN paper to which we compare, even if the model had largely memorized the (text,location,image) training data, it could still produce novel images by conditioning on novel combinations of (text,location), or in general the combinatorial space of all its conditioning variables.\n\nMore implementation details:\nCurrently the paper says \u201cWe used RMSprop with a learning rate schedule starting at 1e-4 and decaying to 1e-5, trained for 200k steps with batch size of 128\u201d. Additional details: The number of epochs varies by dataset - more for CUB because it is smaller, fewer for MS-COCO. Training took about 4 days and sampling at 32x32 resolution took about 2 minutes per image with batch size of 30. Sampling 64x64 took about 16 minutes per image, and 128x128 took about 2 hours. (However, note that sampling time is highly implementation dependent, and we used only the most naive approach in this paper).\n\nHigh-resolution comparisons:\nWe will add further hi-res comparisons in the revised paper.\n\nAutoregressive approach:\nI (first author) also have a bias toward GANs, having written several papers using them. However, I also think autoregressive approaches have complementary benefits compared to GANs - stable, easy to train, do not overfit to a few modes, best available image density estimators, etc - and are worth further developing. Also, autoregressive and adversarial approaches could be naturally combined; e.g. as likelihood and posterior models in PPGNs (", "OTHER_KEYS": "Scott Reed"}, {"TITLE": "Conditional PixelCNN ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes an extension of PixelCNN method that can be conditioned on text and spatially-structured constraints (segmentation / keypoints). It is similar to Reed 2016a except the extension is built on top of PixelCNN instead of GAN. After reading the author's comment, I realized the argument is not that conditional PixelCNN is much better than conditional GAN. I think the authors can add more discussions about pros and cons of each model in the paper. I agree with the other reviewer that some analysis of training and generation time would be helpful. I understand it takes O(N) instead of O(1) for PixelCNN method to do sampling, but is that the main reason that the experiments can only be conducted in low resolution (32 x 32)? I also think since there are not quantitative comparisons, it makes more sense to show more visualizations than 3 examples. Overall, the generated results look reasonably good and have enough diversity. The color mistake is an issue where the author has provided some explanations in the comment. I would say the technical novelty is incremental since the extension is straightforward and similar to previous work. I lean toward accepting because it is very relevant to ICLR community and it provides a good opportunity for future investigation and comparison between different deep image synthesis methods. \n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review: worthwhile extension of PixelCNN capabilities", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This work focuses on conditional image synthesis in the autoregressive framework.  Based on PixelCNN, it trains models that condition on text as well as segmentation masks or keypoints.  Experiments show results for keypoint conditional synthesis on the CUB (birds) and MHP (human pose) dataset, and segmentation conditional synthesis on MS-COCO (objects).  This extension to keypoint/segment conditioning is the primary contribution over existing PixelCNN work.  Qualitative comparison is made to GAN approaches for synthesis.\n\nPros:\n(1) The paper demonstrates additional capabilities for image generation in the autoregressive framework, suggesting that it can keep pace with the latest capabilities of GANs.\n(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.\n(3) Some effort is put forth to establish quantitative evaluation in terms of log-likelihoods (Table 1).\n\nCons:\n(1) Comparison to other work is difficult and limited to qualitative results.  The qualitative results can still be somewhat difficult to interpret.  I believe supplementary material or an appendix with many additional examples could partially alleviate this problem.\n(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward.  This is a solid engineering contribution, but not a surprising new concept.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting results, but comparisons seem lacking", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\"First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).\" Does it, though? Because the resolution is so bad. And resolution limitations aren't addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.\n\nThe segmentation masks and keypoints are pretty strong input constraints. It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.\n\nThe amount of variation is impressive, though.\n\nHow can we assess how much the model is \"replaying\" training data? Figure 8 tries to get at this, but I wonder how much each of the \"red birds\", for instance, is mostly copied from a particular training example.\n\nI'm unsatisfied with the answers to the pre-review questions. You didn't answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn't say why you can't make high resolution comparisons. Yes, it's slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive. \n\nTo be clear about my biases: I don't think PixelCNN is \"the right way\" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn't feel very honest about the disadvantages.\n\nOverall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). \n\nThe deep image synthesis literature is moving very quickly. The field needs to move on from \"proof of concept\" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard. There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "11 Dec 2016", "TITLE": "Resolution", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "comparison with GANs", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper extends PixelCNN to do text and location conditional image generation. The reviewers praise the diversity of the generated samples, which seems like the strongest result of the paper. On the other hand, they are concerned with their low resolution. The authors made an effort of showing a few high-resolution samples in the rebuttal, which indeed look better. Two reviewers mention that the work with respect to PixelCNN is very incremental, and the AC agrees. Overall, this paper is very borderline. While all reviewers became slightly more positive, none was particularly swayed. The paper will make a nice workshop contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Rebuttal, addition of 64x64 and 128x128 samples", "IS_META_REVIEW": false, "comments": "We thank all reviewers for their detailed feedback, and note that all reviewers recommend the paper for acceptance. Based on reviewer feedback about image resolution, we trained a 64x64 and 128x128 version of the model on the CUB dataset, results of which can be seen at sites.google.com/view/iclr2017figures. These and additional higher resolution results will be added to the revised paper. Since low-resolution was one of the main drawbacks to the paper according to the reviews, we hope that this can be reflected in an improved score.\n\nWe posted an updated version of the paper to OpenReview with an important correction to the caption of table 1: likelihoods are in *nats* per dim, not bits.\n\nBelow we respond to each review individually.\n\nAR1:\n\nThe time required for sampling is the main constraint on generating higher-resolution samples. However, we have been able to train some higher-resolution models in time for the rebuttal (see sites.google.com/view/iclr2017figures for some results). We agree that adding many more examples for comparison would help; these will be added in the upcoming version.  Please see the reply to AnonReviewer 3 for more precise details on timing and the experimental setup.\n\nPlease see other replies regarding comparison of GANs to pixelCNNs. In short, there are trade-offs between these two. We accentuate the trade-offs in the paper, in the hope that researchers will then know what are the key problems of each approach, and focus on developing solutions to those problems. A quantitative comparison is problematic because GANs don\u2019t provide us with likelihoods. We can however include more samples. To this extent we will add more high-resolution samples, including the ones already provided via the link above.\n\nAR2:\n\nWe appreciate your suggestion of adding more results to the appendix for the final version, or even better create a website where users can explore the generated images by both approaches. The figures in the paper provide a reasonable depiction of the trade-offs between existing GANs and pixelCNNs, but we agree adding more comparisons will help.\n\nAR3:\n\nMatching GANs:\nIn the paper we demonstrate that autoregressive models can do text- and location-conditional image generation; although as the reviewer points out, the resolution is much lower so \u201cmatch\u201d is not the right word; GANs and auto-regressive models have complementary strengths and weaknesses. We are happy to add a more thorough discussion of these issues earlier in the paper, rather than at the end. Figure 9 queries were from a figure of positive results in the paper to which we compare - so presumably favourable to GAN - but we agree that many more comparisons are needed to study the different types of mistakes each method makes.\n\nReplaying training data:\nOne way to check for this is to compare likelihoods for the training set and held-out sets of data. In our case we did not observe significant overfitting, so copying seems unlikely. We also observe significant diversity of samples even with fixed text and structure constraints. However, as noted in the GAN paper to which we compare, even if the model had largely memorized the (text,location,image) training data, it could still produce novel images by conditioning on novel combinations of (text,location), or in general the combinatorial space of all its conditioning variables.\n\nMore implementation details:\nCurrently the paper says \u201cWe used RMSprop with a learning rate schedule starting at 1e-4 and decaying to 1e-5, trained for 200k steps with batch size of 128\u201d. Additional details: The number of epochs varies by dataset - more for CUB because it is smaller, fewer for MS-COCO. Training took about 4 days and sampling at 32x32 resolution took about 2 minutes per image with batch size of 30. Sampling 64x64 took about 16 minutes per image, and 128x128 took about 2 hours. (However, note that sampling time is highly implementation dependent, and we used only the most naive approach in this paper).\n\nHigh-resolution comparisons:\nWe will add further hi-res comparisons in the revised paper.\n\nAutoregressive approach:\nI (first author) also have a bias toward GANs, having written several papers using them. However, I also think autoregressive approaches have complementary benefits compared to GANs - stable, easy to train, do not overfit to a few modes, best available image density estimators, etc - and are worth further developing. Also, autoregressive and adversarial approaches could be naturally combined; e.g. as likelihood and posterior models in PPGNs (", "OTHER_KEYS": "Scott Reed"}, {"TITLE": "Conditional PixelCNN ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes an extension of PixelCNN method that can be conditioned on text and spatially-structured constraints (segmentation / keypoints). It is similar to Reed 2016a except the extension is built on top of PixelCNN instead of GAN. After reading the author's comment, I realized the argument is not that conditional PixelCNN is much better than conditional GAN. I think the authors can add more discussions about pros and cons of each model in the paper. I agree with the other reviewer that some analysis of training and generation time would be helpful. I understand it takes O(N) instead of O(1) for PixelCNN method to do sampling, but is that the main reason that the experiments can only be conducted in low resolution (32 x 32)? I also think since there are not quantitative comparisons, it makes more sense to show more visualizations than 3 examples. Overall, the generated results look reasonably good and have enough diversity. The color mistake is an issue where the author has provided some explanations in the comment. I would say the technical novelty is incremental since the extension is straightforward and similar to previous work. I lean toward accepting because it is very relevant to ICLR community and it provides a good opportunity for future investigation and comparison between different deep image synthesis methods. \n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review: worthwhile extension of PixelCNN capabilities", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This work focuses on conditional image synthesis in the autoregressive framework.  Based on PixelCNN, it trains models that condition on text as well as segmentation masks or keypoints.  Experiments show results for keypoint conditional synthesis on the CUB (birds) and MHP (human pose) dataset, and segmentation conditional synthesis on MS-COCO (objects).  This extension to keypoint/segment conditioning is the primary contribution over existing PixelCNN work.  Qualitative comparison is made to GAN approaches for synthesis.\n\nPros:\n(1) The paper demonstrates additional capabilities for image generation in the autoregressive framework, suggesting that it can keep pace with the latest capabilities of GANs.\n(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.\n(3) Some effort is put forth to establish quantitative evaluation in terms of log-likelihoods (Table 1).\n\nCons:\n(1) Comparison to other work is difficult and limited to qualitative results.  The qualitative results can still be somewhat difficult to interpret.  I believe supplementary material or an appendix with many additional examples could partially alleviate this problem.\n(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward.  This is a solid engineering contribution, but not a surprising new concept.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting results, but comparisons seem lacking", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\"First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).\" Does it, though? Because the resolution is so bad. And resolution limitations aren't addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.\n\nThe segmentation masks and keypoints are pretty strong input constraints. It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.\n\nThe amount of variation is impressive, though.\n\nHow can we assess how much the model is \"replaying\" training data? Figure 8 tries to get at this, but I wonder how much each of the \"red birds\", for instance, is mostly copied from a particular training example.\n\nI'm unsatisfied with the answers to the pre-review questions. You didn't answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn't say why you can't make high resolution comparisons. Yes, it's slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive. \n\nTo be clear about my biases: I don't think PixelCNN is \"the right way\" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn't feel very honest about the disadvantages.\n\nOverall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). \n\nThe deep image synthesis literature is moving very quickly. The field needs to move on from \"proof of concept\" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard. There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "11 Dec 2016", "TITLE": "Resolution", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "comparison with GANs", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Scott Reed, A\u00e4ron van den Oord, Nal Kalchbrenner, Victor Bapst, Matt Botvinick, Nando de Freitas", "accepted": false, "id": "520"}