{"conference": "ICLR 2017 conference submission", "title": "Words or Characters? Fine-grained Gating for Reading Comprehension", "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test and Who Did What datasets. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.", "histories": [], "reviews": [{"DATE": "07 Feb 2017", "TITLE": "Prior work on this area", "IS_META_REVIEW": false, "comments": "Sorry to pop up late, but I've been looking over ICLR accepted papers, and I noticed this one. Something very similar, by combining word and character information at the feature level using sigmoid gating, has been done before, see ", "OTHER_KEYS": "Kris Cao"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The consensus amongst reviewers is that this paper's proposal for combining character level information with word-level information is sound, well presented, and well evaluated. The main negative sentiment was that the approach was perhaps a little increment, although the conceptual size of the increment was sufficient to warrant publication, and will be relevant to the interests of ICLR attendees working on the intersection of deep learning and NLP. After a cursory reading of the paper, I see no reason to disagree, and recommend acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "review", "comments": "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. \n\nIntuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! \n\nIn sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.\n\nI am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "SUMMARY.\n\nThe paper proposes a gating mechanism to combine word embeddings with character-level word representations.\nThe gating mechanism uses features associated to a word to decided which word representation is the most useful.\nThe fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.\nFor the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.\nIn both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.\n\n\n----------\n\nOVERALL JUDGMENT\n\nThis paper proposes a clever fine-grained extension of a scalar gate for combining word representation.\nIt is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.\n\nI liked the ablation study that shows quite clearly the impact of individual contributions.\nAnd I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. \nIt would be interesting to see if syntactic features can be helpful.\n\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Deep baselines", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 4}, {"TITLE": "character-level encoding", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}, {"DATE": "07 Feb 2017", "TITLE": "Prior work on this area", "IS_META_REVIEW": false, "comments": "Sorry to pop up late, but I've been looking over ICLR accepted papers, and I noticed this one. Something very similar, by combining word and character information at the feature level using sigmoid gating, has been done before, see ", "OTHER_KEYS": "Kris Cao"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The consensus amongst reviewers is that this paper's proposal for combining character level information with word-level information is sound, well presented, and well evaluated. The main negative sentiment was that the approach was perhaps a little increment, although the conceptual size of the increment was sufficient to warrant publication, and will be relevant to the interests of ICLR attendees working on the intersection of deep learning and NLP. After a cursory reading of the paper, I see no reason to disagree, and recommend acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "review", "comments": "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. \n\nIntuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! \n\nIn sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.\n\nI am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "comments": "SUMMARY.\n\nThe paper proposes a gating mechanism to combine word embeddings with character-level word representations.\nThe gating mechanism uses features associated to a word to decided which word representation is the most useful.\nThe fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.\nFor the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.\nIn both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.\n\n\n----------\n\nOVERALL JUDGMENT\n\nThis paper proposes a clever fine-grained extension of a scalar gate for combining word representation.\nIt is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.\n\nI liked the ablation study that shows quite clearly the impact of individual contributions.\nAnd I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. \nIt would be interesting to see if syntactic features can be helpful.\n\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Deep baselines", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 4}, {"TITLE": "character-level encoding", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}], "authors": "Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, Ruslan Salakhutdinov", "accepted": true, "id": "374"}