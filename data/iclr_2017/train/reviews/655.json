{"conference": "ICLR 2017 conference submission", "title": "Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment", "abstract": "Transfer learning is a popular practice in deep neural networks, but fine-tuning of a large number of parameters is a hard challenge due to the complex wiring of neurons between splitting layers and imbalance class distributions of original and transferred domains. Recent advances in evidence theory show that in an imbalance multiclass learning problem, optimizing of proper objective functions based on contingency tables prevents biases towards high-prior classes. Transfer learning usually deals with highly non-convex objectives and local minima in deep neural architectures. We propose a novel distributed transfer learning to tackle both optimization complexity and class-imbalance problem jointly. Our solution imposes separated greedy regularization to each individual convolutional filter to make single-filter neural networks such that the minority classes perform as the majority ones. Then, basic probability assignment from evidence theory boosts these distributed networks to improve the recognition performance on the target domains. Our experiments on several standard datasets confirm the consistent improvement as a result of our distributed transfer learning strategy.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposed to use the BPA criterion for classifier ensembles.\n\nMy major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:\n\n(1) \"Distributed\" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.\n\n(2) The paper uses \"Transfer learning\" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.\n\nAside from the concept mixture of the paper, other comments I have about the paper are:\n\n(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.\n\n(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of \"train/validation\" and \"test\" quite confusing: why \"train/validation\" is always presented together? How to properly distinguish between them?\n\n(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.\n\nIn general, I find the paper not presented in its clearest form and a number of key definitions ambiguous."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All three reviewers appeared to have substantial difficulties understanding the proposed approach due to unclear presentation. This makes it hard for the reviewers to evaluate the originality and potential merits of the proposed approach, and to assess the quality of the empirical evaluation. I encourage the authors to improve the presentation of the study.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. \n\nThe main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?\n\nOverall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. \n\nI suggest a reject. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Update: I thank the author for his comments! At this point, the paper is still not suitable for publication, so I'm leaving the rating untouched.\n\nThis paper proposes a transfer learning method addressing optimization complexity and class imbalance.\n\nMy main concerns are the following:\n\n1. The paper is quite hard to read due to typos, unusual phrasing and loose use of terminology like \u201cdistributed\u201d, \u201ctransfer learning\u201d (meaning \u201cfine-tuning\u201d), \u201csoftmax\u201d (meaning \u201cfully-connected\u201d), \u201cdeep learning\u201d (meaning \u201cbase neural network\u201d),  etc. I\u2019m still not sure I got all the details of the actual algorithm right.\n\n2. The captions to the figures and tables are not very informative \u2013 one has to jump back and forth through the paper to understand what the numbers/images mean.\n\n3. From what I understand, the authors use \u201cconventional transfer learning\u201d to refer to fine-tuning of the fully-connected layers only (I\u2019m judging by Figure 1). In this case, it\u2019s essential to compare the proposed method with regimes when some of the convolutional layers are also updated. This comparison is not present in the paper.\n\nComments on the pre-review questions:\n\n1. Question 1: If the paper only considers the case |C|==|L|, it would be better to reduce the notation clutter.\n\n2. Question 2: It is still not clear what the authors mean by distributed transfer learning. Figure 1 is supposed to highlight the difference from the conventional approach (fine-tuning of the fully-connected layers; by the way, I don\u2019t think, Softmax is a conventional term for fully-connected layers). From the diagram, it follows that the base CNN has the same number of convolutional filters at every layer and, in order to obtain a distributed ensemble, we need to connect (for some reason) filters with the same indices. This does not make a lot of sense to me but I\u2019m probably misinterpreting the figure. Could the authors revise the diagram to make it clearer?\n\nOverall, I think the paper needs significant refinement in order improve the clarity of presentation and thus cannot be accepted as it is now.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposed to use the BPA criterion for classifier ensembles.\n\nMy major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:\n\n(1) \"Distributed\" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.\n\n(2) The paper uses \"Transfer learning\" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.\n\nAside from the concept mixture of the paper, other comments I have about the paper are:\n\n(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.\n\n(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of \"train/validation\" and \"test\" quite confusing: why \"train/validation\" is always presented together? How to properly distinguish between them?\n\n(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.\n\nIn general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "BPA and weak classifiers", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper proposed to use the BPA criterion for classifier ensembles.\n\nMy major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:\n\n(1) \"Distributed\" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.\n\n(2) The paper uses \"Transfer learning\" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.\n\nAside from the concept mixture of the paper, other comments I have about the paper are:\n\n(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.\n\n(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of \"train/validation\" and \"test\" quite confusing: why \"train/validation\" is always presented together? How to properly distinguish between them?\n\n(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.\n\nIn general, I find the paper not presented in its clearest form and a number of key definitions ambiguous."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All three reviewers appeared to have substantial difficulties understanding the proposed approach due to unclear presentation. This makes it hard for the reviewers to evaluate the originality and potential merits of the proposed approach, and to assess the quality of the empirical evaluation. I encourage the authors to improve the presentation of the study.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. \n\nThe main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?\n\nOverall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. \n\nI suggest a reject. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Update: I thank the author for his comments! At this point, the paper is still not suitable for publication, so I'm leaving the rating untouched.\n\nThis paper proposes a transfer learning method addressing optimization complexity and class imbalance.\n\nMy main concerns are the following:\n\n1. The paper is quite hard to read due to typos, unusual phrasing and loose use of terminology like \u201cdistributed\u201d, \u201ctransfer learning\u201d (meaning \u201cfine-tuning\u201d), \u201csoftmax\u201d (meaning \u201cfully-connected\u201d), \u201cdeep learning\u201d (meaning \u201cbase neural network\u201d),  etc. I\u2019m still not sure I got all the details of the actual algorithm right.\n\n2. The captions to the figures and tables are not very informative \u2013 one has to jump back and forth through the paper to understand what the numbers/images mean.\n\n3. From what I understand, the authors use \u201cconventional transfer learning\u201d to refer to fine-tuning of the fully-connected layers only (I\u2019m judging by Figure 1). In this case, it\u2019s essential to compare the proposed method with regimes when some of the convolutional layers are also updated. This comparison is not present in the paper.\n\nComments on the pre-review questions:\n\n1. Question 1: If the paper only considers the case |C|==|L|, it would be better to reduce the notation clutter.\n\n2. Question 2: It is still not clear what the authors mean by distributed transfer learning. Figure 1 is supposed to highlight the difference from the conventional approach (fine-tuning of the fully-connected layers; by the way, I don\u2019t think, Softmax is a conventional term for fully-connected layers). From the diagram, it follows that the base CNN has the same number of convolutional filters at every layer and, in order to obtain a distributed ensemble, we need to connect (for some reason) filters with the same indices. This does not make a lot of sense to me but I\u2019m probably misinterpreting the figure. Could the authors revise the diagram to make it clearer?\n\nOverall, I think the paper needs significant refinement in order improve the clarity of presentation and thus cannot be accepted as it is now.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposed to use the BPA criterion for classifier ensembles.\n\nMy major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:\n\n(1) \"Distributed\" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.\n\n(2) The paper uses \"Transfer learning\" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.\n\nAside from the concept mixture of the paper, other comments I have about the paper are:\n\n(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.\n\n(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of \"train/validation\" and \"test\" quite confusing: why \"train/validation\" is always presented together? How to properly distinguish between them?\n\n(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.\n\nIn general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "BPA and weak classifiers", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Arash Shahriari", "accepted": false, "id": "655"}