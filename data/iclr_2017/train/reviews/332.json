{"conference": "ICLR 2017 conference submission", "title": "Transfer of View-manifold Learning to Similarity Perception of Novel Objects", "abstract": "We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017 (modified: 17 Jan 2017)", "TITLE": "General Responses to the Reviewers:", "IS_META_REVIEW": false, "comments": "We thank all the official and unofficial reviewers for their extremely useful suggestions and are encouraged by the positive feedbacks. \n\nFollowing these suggestions, we have made a number of revisions and uploaded a new version of our manuscript, including adding important references, baselines and an experiment showing performance of features from different layers in Appendix B. We have colored the modifications red in order to provide an easier way to track the changes from the original submission.", "OTHER_KEYS": "Xingyu Lin"}, {"TITLE": "Potentially interesting idea, but important references and baseline comparisons are missing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the \"Tenenbaum objects\u201d is shown.\n\nPositives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.\n\nNegatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.\n\nMore details:\n\nThe \u201cimage purification\u201d paper is very related to this work:\n\n[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.\n\nThere they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online (", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice form of supervision to explore", "comments": "I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. \n\nI think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that.\n\nI think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet\"\n\nI'm not an expert on human vision, but from browsing online and from what I've learned before it seems that \"object persistence\" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming.\n\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting connections to human perception", "MEANINGFUL_COMPARISON": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Similarity to \"unsupervised\" feature learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "CLARITY": 5}, {"TITLE": "applicability of t-STE", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 4, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017 (modified: 17 Jan 2017)", "TITLE": "General Responses to the Reviewers:", "IS_META_REVIEW": false, "comments": "We thank all the official and unofficial reviewers for their extremely useful suggestions and are encouraged by the positive feedbacks. \n\nFollowing these suggestions, we have made a number of revisions and uploaded a new version of our manuscript, including adding important references, baselines and an experiment showing performance of features from different layers in Appendix B. We have colored the modifications red in order to provide an easier way to track the changes from the original submission.", "OTHER_KEYS": "Xingyu Lin"}, {"TITLE": "Potentially interesting idea, but important references and baseline comparisons are missing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the \"Tenenbaum objects\u201d is shown.\n\nPositives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.\n\nNegatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.\n\nMore details:\n\nThe \u201cimage purification\u201d paper is very related to this work:\n\n[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.\n\nThere they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online (", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice form of supervision to explore", "comments": "I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. \n\nI think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that.\n\nI think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet\"\n\nI'm not an expert on human vision, but from browsing online and from what I've learned before it seems that \"object persistence\" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming.\n\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting connections to human perception", "MEANINGFUL_COMPARISON": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Similarity to \"unsupervised\" feature learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "CLARITY": 5}, {"TITLE": "applicability of t-STE", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 4, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "CLARITY": 5}], "authors": "Xingyu Lin, Hao Wang, Zhihao Li, Yimeng Zhang, Alan Yuille, Tai Sing Lee", "accepted": true, "id": "332"}