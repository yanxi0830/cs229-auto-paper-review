{"conference": "ICLR 2017 conference submission", "title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product networks (SPNs) are expressive deep architectures for representing probability distributions, yet allowing exact and efficient inference. SPNs have been successfully applied in several domains, however always as black-box distribution estimators. In this paper, we argue that due to their recursive definition, SPNs can also be naturally employed as hierarchical feature extractors and thus for unsupervised representation learning. Moreover, when converted into Max-Product Networks (MPNs), it is possible to decode such representations back into the original input space. In this way, MPNs can be interpreted as a kind of generative autoencoder, even if they were never trained to reconstruct the input data. We show how these learned representations, if visualized, indeed correspond to \"meaningful parts\" of the training data. They also yield a large improvement when used in structured prediction tasks. As shown in extensive experiments, SPN and MPN encoding and decoding schemes prove very competitive  against the ones employing RBMs and other stacked autoencoder architectures.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.\n\nThis paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:\n\n1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.\n2. They propose how to decode MPN's with partial data.\n3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.\n4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.\n\nMy main concerns with this paper are as follows:\n\n- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. \n\n- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?\n\n- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? \n\n- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.\n\nI *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:\n\nInput | Predicted Output | Decoder | Hamming | Exact Match\n----\nX | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)\nSPN E_X | P(Y) | n/a | xx.xx | xx.xx \nX | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)\n\nDoes a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Dear authors, in general the reviewers found that the paper was interesting and has potential but needs additional work in the presentation and experiments. Unfortunately, even if all reviews had been a weak accept (i.e. all 6s) it would not have met the very competitive standard for this year.\n \n A general concern among the reviewers was the presentation of the research, the paper and the experiments. Too much of the text was dedicated to the explanation of concepts which should be considered to be general knowledge to the ICLR audience (for example the justification for and description of generative models). That text could be replaced with further analysis and justification.\n \n The choice of baseline comparisons and benchmarks did not seem appropriate given the presented model and text. Specifically, it is difficult to determine how good of a generative model it is if the authors don't compare it to other generative models in terms of data likelihood under the model. Similarly, it's difficult to place it in the literature as a model for representation learning if it isn't compared to the state-of-the-art for RL on standard benchmarks.\n \n The clarifications of the authors and revisions to the manuscript are greatly appreciated. Hopefully this will help the authors to improve the manuscript and submit to another conference in the near future.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "11 Jan 2017", "TITLE": "Updated revision", "IS_META_REVIEW": false, "comments": "Dear reviewers, we have updated the paper following your suggestions.\nThe new revision contains the following updates:\n\n    - Overall refactoring of the experiment section, improving the presentation (AnonReviewer1, AnonReviewer2)\n    - Removed references to Q1, Q2, etc (AnonReviewer1)\n    - Added CRFs as a fully supervised and discriminative method to the experiments (AnonReviewer1)\n    - Extended the discussion on structured output prediction into Section 5.2.1 discussing 'why' the performance improvements (AnonReviewer1)\n    - Restructured Table 1 to hold aggregated relative improvements w.r.t LR for all datasets (AnonReviewer1)\n    - Refactored the experimental setting explanation, better motivating the choice of the competitors (AnonReviewer2)\n", "OTHER_KEYS": "antonio vergari"}, {"DATE": "11 Jan 2017", "TITLE": "Choice of benchmark comparisons", "IS_META_REVIEW": false, "comments": "Dear authors, the reviewers brought up an interesting point in their reviews.  They would like to understand the choice of benchmarks and baselines.  Specifically, the comparisons in this paper focus on MADE, NADE and MANIAC.  MANIAC seems like a curious choice, since as far as I know, it is not well known in the community.  NADE and MADE seem like good state-of-the-art comparisons.  However, these models are designed to estimate the probability of the data given the model, and as such report the log-likelihood of the data in their experiments.  The experiments in this paper seem to focus on multi-label classification accuracy.  This difference would suggest that other baselines, which are designed for that purpose, would be more appropriate.  Could you clarify why multi-label classification accuracy was chosen as the metric under which to compare this model to these specific baselines?  Since a major claim is that the model is probabilistic, wouldn't it at least be much more natural to report log-likelihood of the correct classification rather than zero-one error?\n\nAnother point which makes understanding the experiments difficult is the introduction of new datasets.  As far as I can tell, neither NADE or MADE run experiments on the datasets used in these papers.  However, those models' results are reported on other datasets.  Reporting results on the same problems as the state-of-the-art models would make the experiments much more compelling.  Why was this particular data set chosen?  Why was e.g. the \"Cal\" data set included (as far as I can tell every method gets 0 errors).", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Interesting work with limited contribution.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks.  The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings.\n\nPros:\n-The idea is interesting and interpretable models/representations is an important topic.\n-Generating embeddings to interpret SPNs is a novel idea.\n-The experiments are interesting but could be extended.\n\nCons:\n-The author's contribution isn't fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? \n\n-A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade.\n\nI would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "unconvinced on multiple fronts", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThe paper's aim is - as argued in the paper and the responses to other reviewers comments - that SPN and MPN can be interpreted as encoders and decoders of RL. Well - this is an interesting perspective and could be (potentially) worth a paper. \n\nHowever\n\n- the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now. The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far. \n\n- the experiments are (as also pointed out by other reviewers) rather unstructured and difficult to see much of an insight. I should probably also list (as the other other reviewers) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so\n\nSo in essence the paper simply does not deliver at this point on its promise (as far as I am concerned) and in that sense I suggest a very clear reject for this conference. \n\nAs for the dataset employed: MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me (and many other people that you might want to convince) to safe this paper. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.\n\nThis paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:\n\n1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.\n2. They propose how to decode MPN's with partial data.\n3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.\n4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.\n\nMy main concerns with this paper are as follows:\n\n- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. \n\n- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?\n\n- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? \n\n- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.\n\nI *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:\n\nInput | Predicted Output | Decoder | Hamming | Exact Match\n----\nX | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)\nSPN E_X | P(Y) | n/a | xx.xx | xx.xx \nX | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)\n\nDoes a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "Updated version", "IS_META_REVIEW": false, "comments": "We updated the paper to take into account the reviewers' comments.\nIn particular, the following additions/modifications were made:\n   Section 1 Introduction:\n      - clarified the previous work cited from the arxiv paper, as suggested by AnonReviewer3\n   Section 3:\n      - added a citation to the arxiv paper\n   Section 4:\n      - better explained the decoding scheme (AnonReviewer1)\n      - explained the label embedding use (AnonReviewer1)\n   Section 5:\n      - divided the experiment tasks (AnonReviewer1)\n      - better highlighted the rank information in Table 1 (AnonReviewer1)\n      - added experiments to evaluate the resilience of the decoding scheme to missing at random embedding components\n   Appendix:\n      - added references to the datasets (AnonReviewer3)\n      - added figures for the new experiments\n\nPlease let us know if further modifications are needed.", "OTHER_KEYS": "antonio vergari"}, {"DATE": "02 Dec 2016", "TITLE": "Please clarify the story of the paper", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "What are the key contributions of this paper?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.\n\nThis paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:\n\n1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.\n2. They propose how to decode MPN's with partial data.\n3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.\n4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.\n\nMy main concerns with this paper are as follows:\n\n- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. \n\n- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?\n\n- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? \n\n- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.\n\nI *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:\n\nInput | Predicted Output | Decoder | Hamming | Exact Match\n----\nX | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)\nSPN E_X | P(Y) | n/a | xx.xx | xx.xx \nX | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)\n\nDoes a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Dear authors, in general the reviewers found that the paper was interesting and has potential but needs additional work in the presentation and experiments. Unfortunately, even if all reviews had been a weak accept (i.e. all 6s) it would not have met the very competitive standard for this year.\n \n A general concern among the reviewers was the presentation of the research, the paper and the experiments. Too much of the text was dedicated to the explanation of concepts which should be considered to be general knowledge to the ICLR audience (for example the justification for and description of generative models). That text could be replaced with further analysis and justification.\n \n The choice of baseline comparisons and benchmarks did not seem appropriate given the presented model and text. Specifically, it is difficult to determine how good of a generative model it is if the authors don't compare it to other generative models in terms of data likelihood under the model. Similarly, it's difficult to place it in the literature as a model for representation learning if it isn't compared to the state-of-the-art for RL on standard benchmarks.\n \n The clarifications of the authors and revisions to the manuscript are greatly appreciated. Hopefully this will help the authors to improve the manuscript and submit to another conference in the near future.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "11 Jan 2017", "TITLE": "Updated revision", "IS_META_REVIEW": false, "comments": "Dear reviewers, we have updated the paper following your suggestions.\nThe new revision contains the following updates:\n\n    - Overall refactoring of the experiment section, improving the presentation (AnonReviewer1, AnonReviewer2)\n    - Removed references to Q1, Q2, etc (AnonReviewer1)\n    - Added CRFs as a fully supervised and discriminative method to the experiments (AnonReviewer1)\n    - Extended the discussion on structured output prediction into Section 5.2.1 discussing 'why' the performance improvements (AnonReviewer1)\n    - Restructured Table 1 to hold aggregated relative improvements w.r.t LR for all datasets (AnonReviewer1)\n    - Refactored the experimental setting explanation, better motivating the choice of the competitors (AnonReviewer2)\n", "OTHER_KEYS": "antonio vergari"}, {"DATE": "11 Jan 2017", "TITLE": "Choice of benchmark comparisons", "IS_META_REVIEW": false, "comments": "Dear authors, the reviewers brought up an interesting point in their reviews.  They would like to understand the choice of benchmarks and baselines.  Specifically, the comparisons in this paper focus on MADE, NADE and MANIAC.  MANIAC seems like a curious choice, since as far as I know, it is not well known in the community.  NADE and MADE seem like good state-of-the-art comparisons.  However, these models are designed to estimate the probability of the data given the model, and as such report the log-likelihood of the data in their experiments.  The experiments in this paper seem to focus on multi-label classification accuracy.  This difference would suggest that other baselines, which are designed for that purpose, would be more appropriate.  Could you clarify why multi-label classification accuracy was chosen as the metric under which to compare this model to these specific baselines?  Since a major claim is that the model is probabilistic, wouldn't it at least be much more natural to report log-likelihood of the correct classification rather than zero-one error?\n\nAnother point which makes understanding the experiments difficult is the introduction of new datasets.  As far as I can tell, neither NADE or MADE run experiments on the datasets used in these papers.  However, those models' results are reported on other datasets.  Reporting results on the same problems as the state-of-the-art models would make the experiments much more compelling.  Why was this particular data set chosen?  Why was e.g. the \"Cal\" data set included (as far as I can tell every method gets 0 errors).", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Interesting work with limited contribution.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks.  The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings.\n\nPros:\n-The idea is interesting and interpretable models/representations is an important topic.\n-Generating embeddings to interpret SPNs is a novel idea.\n-The experiments are interesting but could be extended.\n\nCons:\n-The author's contribution isn't fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? \n\n-A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade.\n\nI would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "unconvinced on multiple fronts", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThe paper's aim is - as argued in the paper and the responses to other reviewers comments - that SPN and MPN can be interpreted as encoders and decoders of RL. Well - this is an interesting perspective and could be (potentially) worth a paper. \n\nHowever\n\n- the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now. The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far. \n\n- the experiments are (as also pointed out by other reviewers) rather unstructured and difficult to see much of an insight. I should probably also list (as the other other reviewers) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so\n\nSo in essence the paper simply does not deliver at this point on its promise (as far as I am concerned) and in that sense I suggest a very clear reject for this conference. \n\nAs for the dataset employed: MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me (and many other people that you might want to convince) to safe this paper. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.\n\nThis paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:\n\n1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.\n2. They propose how to decode MPN's with partial data.\n3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.\n4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.\n\nMy main concerns with this paper are as follows:\n\n- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. \n\n- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?\n\n- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? \n\n- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.\n\nI *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:\n\nInput | Predicted Output | Decoder | Hamming | Exact Match\n----\nX | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)\nSPN E_X | P(Y) | n/a | xx.xx | xx.xx \nX | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)\n\nDoes a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "Updated version", "IS_META_REVIEW": false, "comments": "We updated the paper to take into account the reviewers' comments.\nIn particular, the following additions/modifications were made:\n   Section 1 Introduction:\n      - clarified the previous work cited from the arxiv paper, as suggested by AnonReviewer3\n   Section 3:\n      - added a citation to the arxiv paper\n   Section 4:\n      - better explained the decoding scheme (AnonReviewer1)\n      - explained the label embedding use (AnonReviewer1)\n   Section 5:\n      - divided the experiment tasks (AnonReviewer1)\n      - better highlighted the rank information in Table 1 (AnonReviewer1)\n      - added experiments to evaluate the resilience of the decoding scheme to missing at random embedding components\n   Appendix:\n      - added references to the datasets (AnonReviewer3)\n      - added figures for the new experiments\n\nPlease let us know if further modifications are needed.", "OTHER_KEYS": "antonio vergari"}, {"DATE": "02 Dec 2016", "TITLE": "Please clarify the story of the paper", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "What are the key contributions of this paper?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Antonio Vergari, Robert Peharz, Nicola Di Mauro, Floriana Esposito", "accepted": false, "id": "658"}