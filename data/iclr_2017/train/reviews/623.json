{"conference": "ICLR 2017 conference submission", "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond", "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.\n\nQuality:\nIt seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:\n1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.\n2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.\n3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.\n4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.\n\nClarity:\n1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.\n2- The texts in the figures are very small and hard to read."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is quite an important topic to understand, and I think the spectrum of the Hessian in deep learning deserves more attention. However, all 3 official reviewers (and the public reviewer) comment that the paper needs more work. In particular, there are some concerns that the experiments are too preliminary/controlled and about whether the algorithm has actually converged. One reviewer also comments that the work is lacking a key insight/conclusion. I like the topic of the paper and would encourage the authors to pursue it more deeply, but at this time all reviewers have recommended rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Revisions", "IS_META_REVIEW": false, "comments": "In light of the constructive feedback, we revised the paper with multiple edits and figure revisions. We also included a new experiment showing that the phenomena that we observe is not specific to the log-loss, that it holds in the case of MSE loss, as well. \n\nWe also noticed that the title can be misleading in that it may suggest that our work's focus is on the singularity of Hessian only. Indeed, the good part of the work is related to the singularity, however, we have a second main message regarding the eigenvalue spectrum: the discrete part depends on data. We revised the title and parts of the body of the text to emphasize this point.", "OTHER_KEYS": "Levent Sagun"}, {"TITLE": "Important problem, but should be better situated in related work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.\n\nThe overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. \u201cAlmost All Learning Machines are Singular\u201d, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further\u2014a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. \u201cIdentifying and attacking the saddle point problem in high dimensional non-convex optimization\u201d NIPS 2014 or the work of Amari and others.\n\nExperimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out.\n\nThis paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting insights, but lack of control experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper analyzes the properties of the Hessian of the training objective for various neural networks and data distributions. The authors study in particular, the eigenspectrum of the Hessian, which relates to the difficulty and the local convexity of the optimization problem.\n\nWhile there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and Hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.\n\nFor example, regarding the concentration of eigenvalues to zero in Figure 6, it is unclear whether the concentration effect is really caused by training (e.g. increasing insensitivity to local perturbations), or the consequence of a specific choice of scale for the initial parameters.\n\nIn Figure 8, the complexity of the data is not defined. It is not clear whether two fully overlapping distributions (the Hessian would then become zero?) is considered as complex or simple data.\n\nSome of the plots legends (Fig. 1 and 2) and labels are unreadable in printed format. Plots of Figure 3 don't have the same range for the x-axis. The image of Hessian matrix of Figure 1 does not render properly in printed format.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting empirical observations, but unfortunately no theory", "OTHER_KEYS": "(anonymous)", "comments": "The work presents some empirical observations to support the statement that \u201cthe Hessian of the loss functions in deep learning is degenerate\u201d. But what does this statement refer to? To my understanding, there are at least three interpretations:\n\n(i) The Hessian of the loss functions in deep learning is degenerate at any point in the parameter space, i.e., any network weight matrices.\n\n(ii) The Hessian of the loss functions in deep learning is degenerate at any critical point.\n\n(iii) The Hessian of the loss functions in deep learning is degenerate at any local minimum, or any global minimum.\n\nNone of these interpretations is solidly supported by the observations provided in the paper.\n\nMore comments are as follows:\n\n1) The authors state that \u201cwe don\u2019t have much information on what the actual Hessian looks like.\u201d Then I just wonder what Hessian is investigated. Is it the actual one or approximate one? Please clarify and provide the references for computing the actual Hessian.\n\n2) It is not clear whether the optimization was done by a batch gradient descent algorithm, i.e., batch back propagation (BP) algorithm, or a stochastic BP algorithm. If the training was done via a stochastic BP algorithm, it is hard to conclude that the the Neural Network has been trained to its local minimum. When it was done by a full-batch BP algorithm, what was the accumulating point? Was it local minimum or global minimum?\n\n3) Since the negative log likelihood function was used as at the end of training, it is essentially a joint learning approach in both the Newton weight matrices and the negative log likelihood vector. Certainly, the whole loss function is not convex in these two parameters. But if least squares error function is used at the end, would it make any difference in claiming the degeneracy of the Hessian?\n\n4) Finally, the statement \u201cThere are still negative eigenvalues even when they are small in magnitude\u201d is very puzzling. Potential reasons are:\n(a) If the training algorithm did converge, the accumulating points were not local minima, i.e., they were saddle points.\n(b) Training algorithms did not converge, or have not converged yet.\n(c) The calculation of the actual Hessian might be inaccurate.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "This paper presents empirical evidence for the singularity of the Hessian. This works has interesting experiments and observations but the paper needs more work and it is not a complete work.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.\n\nQuality:\nIt seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:\n1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.\n2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.\n3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.\n4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.\n\nClarity:\n1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.\n2- The texts in the figures are very small and hard to read.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "08 Dec 2016", "TITLE": "hidden units, histogram range", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "08 Dec 2016", "TITLE": "Parameters and plot clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Eigenvalues of the Hessian", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.\n\nQuality:\nIt seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:\n1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.\n2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.\n3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.\n4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.\n\nClarity:\n1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.\n2- The texts in the figures are very small and hard to read."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is quite an important topic to understand, and I think the spectrum of the Hessian in deep learning deserves more attention. However, all 3 official reviewers (and the public reviewer) comment that the paper needs more work. In particular, there are some concerns that the experiments are too preliminary/controlled and about whether the algorithm has actually converged. One reviewer also comments that the work is lacking a key insight/conclusion. I like the topic of the paper and would encourage the authors to pursue it more deeply, but at this time all reviewers have recommended rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Revisions", "IS_META_REVIEW": false, "comments": "In light of the constructive feedback, we revised the paper with multiple edits and figure revisions. We also included a new experiment showing that the phenomena that we observe is not specific to the log-loss, that it holds in the case of MSE loss, as well. \n\nWe also noticed that the title can be misleading in that it may suggest that our work's focus is on the singularity of Hessian only. Indeed, the good part of the work is related to the singularity, however, we have a second main message regarding the eigenvalue spectrum: the discrete part depends on data. We revised the title and parts of the body of the text to emphasize this point.", "OTHER_KEYS": "Levent Sagun"}, {"TITLE": "Important problem, but should be better situated in related work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.\n\nThe overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. \u201cAlmost All Learning Machines are Singular\u201d, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further\u2014a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. \u201cIdentifying and attacking the saddle point problem in high dimensional non-convex optimization\u201d NIPS 2014 or the work of Amari and others.\n\nExperimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out.\n\nThis paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting insights, but lack of control experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper analyzes the properties of the Hessian of the training objective for various neural networks and data distributions. The authors study in particular, the eigenspectrum of the Hessian, which relates to the difficulty and the local convexity of the optimization problem.\n\nWhile there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and Hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.\n\nFor example, regarding the concentration of eigenvalues to zero in Figure 6, it is unclear whether the concentration effect is really caused by training (e.g. increasing insensitivity to local perturbations), or the consequence of a specific choice of scale for the initial parameters.\n\nIn Figure 8, the complexity of the data is not defined. It is not clear whether two fully overlapping distributions (the Hessian would then become zero?) is considered as complex or simple data.\n\nSome of the plots legends (Fig. 1 and 2) and labels are unreadable in printed format. Plots of Figure 3 don't have the same range for the x-axis. The image of Hessian matrix of Figure 1 does not render properly in printed format.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting empirical observations, but unfortunately no theory", "OTHER_KEYS": "(anonymous)", "comments": "The work presents some empirical observations to support the statement that \u201cthe Hessian of the loss functions in deep learning is degenerate\u201d. But what does this statement refer to? To my understanding, there are at least three interpretations:\n\n(i) The Hessian of the loss functions in deep learning is degenerate at any point in the parameter space, i.e., any network weight matrices.\n\n(ii) The Hessian of the loss functions in deep learning is degenerate at any critical point.\n\n(iii) The Hessian of the loss functions in deep learning is degenerate at any local minimum, or any global minimum.\n\nNone of these interpretations is solidly supported by the observations provided in the paper.\n\nMore comments are as follows:\n\n1) The authors state that \u201cwe don\u2019t have much information on what the actual Hessian looks like.\u201d Then I just wonder what Hessian is investigated. Is it the actual one or approximate one? Please clarify and provide the references for computing the actual Hessian.\n\n2) It is not clear whether the optimization was done by a batch gradient descent algorithm, i.e., batch back propagation (BP) algorithm, or a stochastic BP algorithm. If the training was done via a stochastic BP algorithm, it is hard to conclude that the the Neural Network has been trained to its local minimum. When it was done by a full-batch BP algorithm, what was the accumulating point? Was it local minimum or global minimum?\n\n3) Since the negative log likelihood function was used as at the end of training, it is essentially a joint learning approach in both the Newton weight matrices and the negative log likelihood vector. Certainly, the whole loss function is not convex in these two parameters. But if least squares error function is used at the end, would it make any difference in claiming the degeneracy of the Hessian?\n\n4) Finally, the statement \u201cThere are still negative eigenvalues even when they are small in magnitude\u201d is very puzzling. Potential reasons are:\n(a) If the training algorithm did converge, the accumulating points were not local minima, i.e., they were saddle points.\n(b) Training algorithms did not converge, or have not converged yet.\n(c) The calculation of the actual Hessian might be inaccurate.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "This paper presents empirical evidence for the singularity of the Hessian. This works has interesting experiments and observations but the paper needs more work and it is not a complete work.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.\n\nQuality:\nIt seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:\n1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.\n2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.\n3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.\n4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.\n\nClarity:\n1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.\n2- The texts in the figures are very small and hard to read.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "08 Dec 2016", "TITLE": "hidden units, histogram range", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "08 Dec 2016", "TITLE": "Parameters and plot clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Eigenvalues of the Hessian", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Levent Sagun, Leon Bottou, Yann LeCun", "accepted": false, "id": "623"}