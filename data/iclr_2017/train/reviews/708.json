{"conference": "ICLR 2017 conference submission", "title": "Simple Black-Box Adversarial Perturbations for Deep Networks", "abstract": "Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world.  In this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our first attack is based on a simple idea of adding perturbation to a randomly selected single pixel or a small set of them. We then improve the effectiveness of this attack by carefully constructing a small set of pixels to perturb by using the idea of greedy local-search. Our proposed attacks also naturally extend to a stronger notion of misclassification. Our extensive experimental results illustrate that even these elementary attacks can reveal a deep neural network's vulnerabilities. The simplicity and effectiveness of our proposed schemes mean that they could serve as a litmus test while designing robust networks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.\n\nThe idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.\n\nMy main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.\n\nThe greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.\nIt is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large \"p\" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.\n\nIn conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "While this is an interesting topic, both the method description and experimental setup could be improved.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Too verbose for little insight", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\n\nPaper summary:\nThis work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight.\n\n\nReview summary:\nThe topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear.\nThis looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR.\n\n\nPros:\n* Interesting topic\n* Black-box setup is most relevant\n* Multiple experiments\n* Shows that with flipping only 1~5% of pixels, adversarial images can be created\n\n\nCons:\n* Too long, yet key details are not well addressed\n* Some of the experiments are of little interest\n* Main experiments lack key measures or additional baselines\n* Limited technical novelty\n\n\n\n\nQuality: the method description and experimental setup leave to be desired. \n\n\nClarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise.\n\n\nOriginality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising.\n\n\nSignificance: the work is incremental, the issues in the experiments limit potential impact of this paper.\n\n\nSpecific comments:\n* I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments.\n* Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB].\n* [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB]\n* The modification is done after normalization, is this realistic ? \n* Alg 2, why not clamping to [LB, UB] ?\n* Section 6, \u201cimplementing algorithm LocSearchAdv\u201d, the text is unclear on how p is adjusted; new variables are added. This is confusion.\n* Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ?\n* Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. \u201cin many cases far less average perturbation\u201d claim).\n* Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the \u201ceffectiveness\u201d of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. \n* How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ?\n* Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims.\n* Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting.\n* If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the \u201ctop N\u201d largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest.\n\n\nMinor comments:\n* The is an abuse of footnotes, most of them should be inserted in the main text.\n* I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB)\n* Table 1,2,3 should be figures\n* Last line of first paragraph of section 6 is uninformative.\n* Very tiny -> small", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review: incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.\n\nA similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.\n\nA major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Blackbox adversarial examples", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.\n\nThe idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.\n\nMy main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.\n\nThe greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.\nIt is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large \"p\" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.\n\nIn conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "threat model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Motivate paper better.", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.\n\nThe idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.\n\nMy main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.\n\nThe greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.\nIt is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large \"p\" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.\n\nIn conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "While this is an interesting topic, both the method description and experimental setup could be improved.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Too verbose for little insight", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\n\nPaper summary:\nThis work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight.\n\n\nReview summary:\nThe topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear.\nThis looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR.\n\n\nPros:\n* Interesting topic\n* Black-box setup is most relevant\n* Multiple experiments\n* Shows that with flipping only 1~5% of pixels, adversarial images can be created\n\n\nCons:\n* Too long, yet key details are not well addressed\n* Some of the experiments are of little interest\n* Main experiments lack key measures or additional baselines\n* Limited technical novelty\n\n\n\n\nQuality: the method description and experimental setup leave to be desired. \n\n\nClarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise.\n\n\nOriginality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising.\n\n\nSignificance: the work is incremental, the issues in the experiments limit potential impact of this paper.\n\n\nSpecific comments:\n* I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments.\n* Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB].\n* [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB]\n* The modification is done after normalization, is this realistic ? \n* Alg 2, why not clamping to [LB, UB] ?\n* Section 6, \u201cimplementing algorithm LocSearchAdv\u201d, the text is unclear on how p is adjusted; new variables are added. This is confusion.\n* Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ?\n* Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. \u201cin many cases far less average perturbation\u201d claim).\n* Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the \u201ceffectiveness\u201d of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. \n* How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ?\n* Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims.\n* Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting.\n* If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the \u201ctop N\u201d largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest.\n\n\nMinor comments:\n* The is an abuse of footnotes, most of them should be inserted in the main text.\n* I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB)\n* Table 1,2,3 should be figures\n* Last line of first paragraph of section 6 is uninformative.\n* Very tiny -> small", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review: incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.\n\nA similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.\n\nA major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Blackbox adversarial examples", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.\n\nThe idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.\n\nMy main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.\n\nThe greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.\nIt is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large \"p\" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.\n\nIn conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "threat model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Motivate paper better.", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Nina Narodytska, Shiva Kasiviswanathan", "accepted": false, "id": "708"}