{"conference": "ICLR 2017 conference submission", "title": "The Neural Noisy Channel", "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. \n\nThe experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see \n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\n\nfrom Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are\n\n1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?\n\n2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. \n\nThe paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper adapts NMT to a noisy channel formulation utilizing the recently developed SSNT framework. The paper is well-written and has solid experimental results. Howver, the paper can be improved with a bit more originality and impact. In, short the pros and cons of the paper are:\n \n Pro: \n - Clarity: All agree paper was very \"well written\" \n - Quality: Reviewers note the \"strong experimental section\". Comprehensive results. \n \n Cons: \n - Originality: There were concerns about technical novelty: (a) \"this paper does not present anything that is particular novel on top of the SSNT\" (b) not that \"conceptually different from the work of Tillmann et al\". \n - Impact: Reviewers were not completely convinced that this method could not work with simpler means. For instance by using clever reranking or utilizing the deep speech style unprincipled combination. However, this paper does produce a better approach for this problem.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Comments on related work and MT experiments", "IS_META_REVIEW": false, "comments": "This is an interesting paper that investigates alternative neural machine translation approaches. Falling back to traditional concepts machine translation while using powerful neural models is essential to understanding the leap achieved by neural machine translation over conventional methods.\n\nIn terms of related work, there is a close relation to the paper Alignment-Based Neural Machine Translation (Alkhouli et al. 2016)*, whether in the decomposition of the model into neural alignment and  lexical/word probabilities, or using a weighted log-linear combination to combine the models which are eventually used in a standalone decoder. A notable difference is that the paper uses direct translation models (as opposed to the inverted ones proposed by the authors here), and the fact that the models are trained on word-aligned data which speeds up training. As for the alignment model, the former choses to model non-monotone source jumps whereas the current paper choses two classes (emit and shift) to model monotone alignments. In (Alkhouli et al. 2016), decoding is done using a beam search decoder that hypothesizes alignments and word translations\n\nAs for the MT experiments, did you try the model combination using larger data (i.e. >=100M tokens). How long did the current models take to train on the 184K sentence pairs you are using? It would also be interesting to see how much you gain/lose if the models were to be trained using given word alignments (e.g. obtained using GIZA++). Training then should be faster.\n\n* ", "OTHER_KEYS": "Tamer Alkhouli"}, {"DATE": "07 Jan 2017", "TITLE": "Authors: Reviews now in, chance for rebuttal", "IS_META_REVIEW": false, "comments": "Hi authors,\n\nWe apologize for the delay. There are now 3 full reviews on this work. When you have an opportunity please enter in a rebuttal so that reviewers can discuss. ", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.\n\nPROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results.\n\nCONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It's worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.\n\nOther Comments: \n - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. \n\n - It initially seems strange to suggest a noisy-channel model as a way of addressing the \"explaining away\" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "04 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "hideously late review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.\n\nStrong points:\n- well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models\nHandling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community\n- strong experimental section, with some strong results (though not complete: see weak points)\n\nWeak points:\n- Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)\n- What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?\n\nSome other (minor) comments:\n\n- Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). \n- How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever?\n- Some more comments on details of the auxiliary direct model would be definitely of interest.\n- How crucial is the correct choice of the pruning variables (K1 and K2)? \n- Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?\n\nTypos:\nTable 1: chanel -> channel (one before last row)\n\nApologies for late review.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. \n\nThe experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see \n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\n\nfrom Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are\n\n1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?\n\n2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. \n\nThe paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "05 Dec 2016", "TITLE": "comments on readability", "IS_META_REVIEW": false, "comments": "After presenting everything as noisy channel (i.e. p(x|y)), it would be beneficial to also mention p(x|y) in the model description in section 2 and in the inference section 2.2.\n\nIn section 3, the presentation changes to using p(x|y), before more exactly introducing, how p(x|y) is modeled and trained.\nIt is not immediately clear, what is the auxiliary direct model q(y,z|x) - i.e. it is the model inferred in section 2.2. (== proposal model == decoding model).\n\nThe explaining paragraph in section 3: \"the search problem remains nontrivial [...] softmax over the input variables\",\ncould be made more clear by saying, that in the model p(x|y) we need to avoid conditioning on the full (output word) sequence y,\nwhich is possible by using only the forward LSTM on the outputs. \n\nThe sentence in section 3 is not complete: \"The top K1 partial output sequences.\"\n\nIn Tables 1/2, you should introduce the meaning of RG-1/RG-2/RG-L.\nWhat is the effect of leaving out/adding the length bias?\nbias(uni)/bias(bi) is misleading, it should be channel(bi)/direct(bi)\n\nThe alignment transition model (EMIT/SHIFT) is calculated frame-by-frame (end of section 2.1). Is also the decoding algorithm in appendix A running frame-by-frame? \n\nIn Appendix A, the function getCandidateOutputs is not clear. Does it mean a partial backtrace to obtain the full sequence y_1..j?", "OTHER_KEYS": "Mirko Hannemann"}, {"DATE": "30 Nov 2016", "TITLE": "Difference from previous work", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. \n\nThe experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see \n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\n\nfrom Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are\n\n1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?\n\n2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. \n\nThe paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper adapts NMT to a noisy channel formulation utilizing the recently developed SSNT framework. The paper is well-written and has solid experimental results. Howver, the paper can be improved with a bit more originality and impact. In, short the pros and cons of the paper are:\n \n Pro: \n - Clarity: All agree paper was very \"well written\" \n - Quality: Reviewers note the \"strong experimental section\". Comprehensive results. \n \n Cons: \n - Originality: There were concerns about technical novelty: (a) \"this paper does not present anything that is particular novel on top of the SSNT\" (b) not that \"conceptually different from the work of Tillmann et al\". \n - Impact: Reviewers were not completely convinced that this method could not work with simpler means. For instance by using clever reranking or utilizing the deep speech style unprincipled combination. However, this paper does produce a better approach for this problem.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Comments on related work and MT experiments", "IS_META_REVIEW": false, "comments": "This is an interesting paper that investigates alternative neural machine translation approaches. Falling back to traditional concepts machine translation while using powerful neural models is essential to understanding the leap achieved by neural machine translation over conventional methods.\n\nIn terms of related work, there is a close relation to the paper Alignment-Based Neural Machine Translation (Alkhouli et al. 2016)*, whether in the decomposition of the model into neural alignment and  lexical/word probabilities, or using a weighted log-linear combination to combine the models which are eventually used in a standalone decoder. A notable difference is that the paper uses direct translation models (as opposed to the inverted ones proposed by the authors here), and the fact that the models are trained on word-aligned data which speeds up training. As for the alignment model, the former choses to model non-monotone source jumps whereas the current paper choses two classes (emit and shift) to model monotone alignments. In (Alkhouli et al. 2016), decoding is done using a beam search decoder that hypothesizes alignments and word translations\n\nAs for the MT experiments, did you try the model combination using larger data (i.e. >=100M tokens). How long did the current models take to train on the 184K sentence pairs you are using? It would also be interesting to see how much you gain/lose if the models were to be trained using given word alignments (e.g. obtained using GIZA++). Training then should be faster.\n\n* ", "OTHER_KEYS": "Tamer Alkhouli"}, {"DATE": "07 Jan 2017", "TITLE": "Authors: Reviews now in, chance for rebuttal", "IS_META_REVIEW": false, "comments": "Hi authors,\n\nWe apologize for the delay. There are now 3 full reviews on this work. When you have an opportunity please enter in a rebuttal so that reviewers can discuss. ", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.\n\nPROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results.\n\nCONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It's worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.\n\nOther Comments: \n - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. \n\n - It initially seems strange to suggest a noisy-channel model as a way of addressing the \"explaining away\" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "04 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "hideously late review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.\n\nStrong points:\n- well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models\nHandling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community\n- strong experimental section, with some strong results (though not complete: see weak points)\n\nWeak points:\n- Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)\n- What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?\n\nSome other (minor) comments:\n\n- Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). \n- How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever?\n- Some more comments on details of the auxiliary direct model would be definitely of interest.\n- How crucial is the correct choice of the pruning variables (K1 and K2)? \n- Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?\n\nTypos:\nTable 1: chanel -> channel (one before last row)\n\nApologies for late review.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. \n\nThe experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see \n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\n\nfrom Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are\n\n1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?\n\n2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. \n\nThe paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "05 Dec 2016", "TITLE": "comments on readability", "IS_META_REVIEW": false, "comments": "After presenting everything as noisy channel (i.e. p(x|y)), it would be beneficial to also mention p(x|y) in the model description in section 2 and in the inference section 2.2.\n\nIn section 3, the presentation changes to using p(x|y), before more exactly introducing, how p(x|y) is modeled and trained.\nIt is not immediately clear, what is the auxiliary direct model q(y,z|x) - i.e. it is the model inferred in section 2.2. (== proposal model == decoding model).\n\nThe explaining paragraph in section 3: \"the search problem remains nontrivial [...] softmax over the input variables\",\ncould be made more clear by saying, that in the model p(x|y) we need to avoid conditioning on the full (output word) sequence y,\nwhich is possible by using only the forward LSTM on the outputs. \n\nThe sentence in section 3 is not complete: \"The top K1 partial output sequences.\"\n\nIn Tables 1/2, you should introduce the meaning of RG-1/RG-2/RG-L.\nWhat is the effect of leaving out/adding the length bias?\nbias(uni)/bias(bi) is misleading, it should be channel(bi)/direct(bi)\n\nThe alignment transition model (EMIT/SHIFT) is calculated frame-by-frame (end of section 2.1). Is also the decoding algorithm in appendix A running frame-by-frame? \n\nIn Appendix A, the function getCandidateOutputs is not clear. Does it mean a partial backtrace to obtain the full sequence y_1..j?", "OTHER_KEYS": "Mirko Hannemann"}, {"DATE": "30 Nov 2016", "TITLE": "Difference from previous work", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefenstette, Tomas Kocisky", "accepted": true, "id": "443"}