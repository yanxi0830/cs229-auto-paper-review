{"conference": "ICLR 2017 conference submission", "title": "Neural Code Completion", "abstract": "Code completion, an essential part of modern software development, yet can bechallenging for dynamically typed programming languages.  In this paper we ex-plore the use of neural network techniques to automatically learn code completionfrom  a  large  corpus  of  dynamically  typed  JavaScript  code.   We  show  differentneural networks that leverage not only token level information but also structuralinformation,  and  evaluate  their  performance  on  different  prediction  tasks.   Wedemonstrate that our models can outperform the state-of-the-art approach, whichis based on decision tree techniques, on both next non-terminal and next terminalprediction tasks by 3.8 points and 0.5 points respectively.  We believe that neuralnetwork techniques can play a transformative role in helping software developersmanage the growing complexity of software systems, and we see this work as afirst step in that direction.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:\n\n1. Incorrect claims about related work affecting novelty:\n\n  - This work is not the first to explore a deep learning approach to automatic code completion: \u201cToward Deep Learning Software Repositories\u201d, MSR\u201915 also uses deep learning for code completion, and is not cited.\n\n  - \u201cCode Completion with Statistical Language Models\u201d, PLDI\u201914 is cited incorrectly -- it also does code completion with recurrent neural networks.\n\n  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA\u201916 below). \n\n  - This submission is not the only one that \u201ccan automatically extract features\u201d. Some high-precision (cited) baselines do it.\n\n  - \u201cStructured generative models of natural source code\u201d is an incorrect citation. It is from ICML\u201914 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.\n\n\n2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA\u201916 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA\u201916 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).\n\n\n3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but \u201cProbabilistic Model for Code with Decision Trees\u201d, (OOPSLA 2016) which appeared before the submission deadline for ICLR\u201917:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper extends existing code completion methods over discrete symbols with an LSTM-based neural network. This constitutes a novel application of neural networks to this domain, but is rather incremental. Alone, I don't think this would be a bad thing as good work can be incremental and make a useful contribution, but the scores do not show an amazingly significant improvement over the baseline. There are many design choices that could be better justified empirically with the introduction of neural benchmarks or an ablation study. We encourage the authors to further refine this work and re-submit.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "22 Jan 2017", "TITLE": "Technical Details of the Approach", "IS_META_REVIEW": false, "comments": "Given that the #\u2019s changed by few % between the two versions, the authors should also include further technical details describing how the models were trained. In particular, several details should be easy to include such that results are reproducible, such as:\n\n1) Is the size of the embedding the same as the hidden size (i.e., 1500)?\n2) Are the embeddings pre-trained or trained simultaneously with the model? How are they pre-trained?\n3) What is the motivation behind using only the first segment of a program for training after 8 epochs?\n4) After 8 epochs only the first segment of every program is used for training. What is the criterion for stopping training once this process of only using the first segments has started?\n5) \u201cThe last segment of a program, which may not be full, is padded with EOF tokens.\u201d Do these EOF predictions count as correct prediction? \n6) How is the training and validation dataset selected and how many samples it contains?\n7) Which dataset is used to build the vocabulary of frequent terminals used for the UNK token replacement? That is, which of the training, validation and testing datasets were used to built it?\n8) What loss function is used?\n9) Can you provide details on how the network parameters are randomly initialized?\n10) Is regularization used? E.g. dropout? If so, what is the dropout rate?\n11) Is sampled softmax used during training? If so, what is the sample size?\n12) What is a typical training duration for one of the experiments from the evaluation section? Including the results for smaller models in the appendix e.g. with reduced hidden size would also be useful.\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "19 Jan 2017", "TITLE": "A new revision to address some of reviewers' comments", "IS_META_REVIEW": false, "comments": "The latest revision adds the following content beyond Jan-16's version:\n\n1) We add the results for one single models' performance (rather than an ensemble's performance) for the next non-terminal and terminal predictions in Section 5.3.\n2) A new paragraph in Section 5.3 to report the effectiveness of setting the UNK threshold.\n3) We add a new subsection, Section 5.5, to report the deny prediction experiments. We also add a paragraph in Sec 5.5 to examine how different choices of alpha affect a model's accuracy.\n", "OTHER_KEYS": "Chang Liu"}, {"DATE": "17 Jan 2017", "TITLE": "Methods have been improved, and paper is updated", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have improved our approaches and revised the paper based on the comments. We thank all reviewers for the valuable comments, and hope our work has the chance to be discussed further!\n\nWe are still working to update the paper, and some new results will be available by tomorrow.\n\nHere is the list of all changes as of Jan-15-2017:\n\n1) We have fine-tuned the model to achieve better performance. Now, our approaches can achieve a better performance than prior art. We have updated the following related section:\n   a) Abstract & Introduction\n   b) Section 5.2. Training details\n   c) All results in Section 5.3 & 5.4. In 5.3, the baseline is updated to Raychev et al. (2016a).\n   d) Some results in Section 5.5 are still pending. So this section is removed right now.\n\n2) The related work section and items in the reference have been revised.\n\n3) We have added a paragraph in Section 3.3 to explain the difference between next node prediction and next token prediction.\n\n", "OTHER_KEYS": "Chang Liu"}, {"TITLE": "Great problem, too many decisions taken for granted and not explored", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting paper but only initial work about neural network based code completion.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Pros:\n  using neural network on a new domain.\nCons:\n  It is not clear how it is guaranteed that the network generates syntactically correct code.\n\nQuestions, comments:\n  How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications?\n  Were all combinations of NT2N decision with all possible NTN2T considered?\n\n  Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the\n  lexicon, thus limited the possible predictions.\n  The question should then more likely be what is the optimal value of alpha for UNK.\n  See also my previous comment on estimating and using UNK.\n\n  Section 5.5, second paragraph, compares numbers which are not comparable.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Ok paper, but not a big enough contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Dec 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"TITLE": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results.", "OTHER_KEYS": "(anonymous)", "comments": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:\n\n1. Incorrect claims about related work affecting novelty:\n\n  - This work is not the first to explore a deep learning approach to automatic code completion: \u201cToward Deep Learning Software Repositories\u201d, MSR\u201915 also uses deep learning for code completion, and is not cited.\n\n  - \u201cCode Completion with Statistical Language Models\u201d, PLDI\u201914 is cited incorrectly -- it also does code completion with recurrent neural networks.\n\n  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA\u201916 below). \n\n  - This submission is not the only one that \u201ccan automatically extract features\u201d. Some high-precision (cited) baselines do it.\n\n  - \u201cStructured generative models of natural source code\u201d is an incorrect citation. It is from ICML\u201914 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.\n\n\n2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA\u201916 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA\u201916 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).\n\n\n3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but \u201cProbabilistic Model for Code with Decision Trees\u201d, (OOPSLA 2016) which appeared before the submission deadline for ICLR\u201917:\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Nov 2016", "REVIEWER_CONFIDENCE": 5}, {"IS_META_REVIEW": true, "comments": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:\n\n1. Incorrect claims about related work affecting novelty:\n\n  - This work is not the first to explore a deep learning approach to automatic code completion: \u201cToward Deep Learning Software Repositories\u201d, MSR\u201915 also uses deep learning for code completion, and is not cited.\n\n  - \u201cCode Completion with Statistical Language Models\u201d, PLDI\u201914 is cited incorrectly -- it also does code completion with recurrent neural networks.\n\n  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA\u201916 below). \n\n  - This submission is not the only one that \u201ccan automatically extract features\u201d. Some high-precision (cited) baselines do it.\n\n  - \u201cStructured generative models of natural source code\u201d is an incorrect citation. It is from ICML\u201914 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.\n\n\n2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA\u201916 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA\u201916 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).\n\n\n3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but \u201cProbabilistic Model for Code with Decision Trees\u201d, (OOPSLA 2016) which appeared before the submission deadline for ICLR\u201917:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper extends existing code completion methods over discrete symbols with an LSTM-based neural network. This constitutes a novel application of neural networks to this domain, but is rather incremental. Alone, I don't think this would be a bad thing as good work can be incremental and make a useful contribution, but the scores do not show an amazingly significant improvement over the baseline. There are many design choices that could be better justified empirically with the introduction of neural benchmarks or an ablation study. We encourage the authors to further refine this work and re-submit.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "22 Jan 2017", "TITLE": "Technical Details of the Approach", "IS_META_REVIEW": false, "comments": "Given that the #\u2019s changed by few % between the two versions, the authors should also include further technical details describing how the models were trained. In particular, several details should be easy to include such that results are reproducible, such as:\n\n1) Is the size of the embedding the same as the hidden size (i.e., 1500)?\n2) Are the embeddings pre-trained or trained simultaneously with the model? How are they pre-trained?\n3) What is the motivation behind using only the first segment of a program for training after 8 epochs?\n4) After 8 epochs only the first segment of every program is used for training. What is the criterion for stopping training once this process of only using the first segments has started?\n5) \u201cThe last segment of a program, which may not be full, is padded with EOF tokens.\u201d Do these EOF predictions count as correct prediction? \n6) How is the training and validation dataset selected and how many samples it contains?\n7) Which dataset is used to build the vocabulary of frequent terminals used for the UNK token replacement? That is, which of the training, validation and testing datasets were used to built it?\n8) What loss function is used?\n9) Can you provide details on how the network parameters are randomly initialized?\n10) Is regularization used? E.g. dropout? If so, what is the dropout rate?\n11) Is sampled softmax used during training? If so, what is the sample size?\n12) What is a typical training duration for one of the experiments from the evaluation section? Including the results for smaller models in the appendix e.g. with reduced hidden size would also be useful.\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "19 Jan 2017", "TITLE": "A new revision to address some of reviewers' comments", "IS_META_REVIEW": false, "comments": "The latest revision adds the following content beyond Jan-16's version:\n\n1) We add the results for one single models' performance (rather than an ensemble's performance) for the next non-terminal and terminal predictions in Section 5.3.\n2) A new paragraph in Section 5.3 to report the effectiveness of setting the UNK threshold.\n3) We add a new subsection, Section 5.5, to report the deny prediction experiments. We also add a paragraph in Sec 5.5 to examine how different choices of alpha affect a model's accuracy.\n", "OTHER_KEYS": "Chang Liu"}, {"DATE": "17 Jan 2017", "TITLE": "Methods have been improved, and paper is updated", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have improved our approaches and revised the paper based on the comments. We thank all reviewers for the valuable comments, and hope our work has the chance to be discussed further!\n\nWe are still working to update the paper, and some new results will be available by tomorrow.\n\nHere is the list of all changes as of Jan-15-2017:\n\n1) We have fine-tuned the model to achieve better performance. Now, our approaches can achieve a better performance than prior art. We have updated the following related section:\n   a) Abstract & Introduction\n   b) Section 5.2. Training details\n   c) All results in Section 5.3 & 5.4. In 5.3, the baseline is updated to Raychev et al. (2016a).\n   d) Some results in Section 5.5 are still pending. So this section is removed right now.\n\n2) The related work section and items in the reference have been revised.\n\n3) We have added a paragraph in Section 3.3 to explain the difference between next node prediction and next token prediction.\n\n", "OTHER_KEYS": "Chang Liu"}, {"TITLE": "Great problem, too many decisions taken for granted and not explored", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting paper but only initial work about neural network based code completion.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Pros:\n  using neural network on a new domain.\nCons:\n  It is not clear how it is guaranteed that the network generates syntactically correct code.\n\nQuestions, comments:\n  How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications?\n  Were all combinations of NT2N decision with all possible NTN2T considered?\n\n  Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the\n  lexicon, thus limited the possible predictions.\n  The question should then more likely be what is the optimal value of alpha for UNK.\n  See also my previous comment on estimating and using UNK.\n\n  Section 5.5, second paragraph, compares numbers which are not comparable.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Ok paper, but not a big enough contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Dec 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"TITLE": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results.", "OTHER_KEYS": "(anonymous)", "comments": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:\n\n1. Incorrect claims about related work affecting novelty:\n\n  - This work is not the first to explore a deep learning approach to automatic code completion: \u201cToward Deep Learning Software Repositories\u201d, MSR\u201915 also uses deep learning for code completion, and is not cited.\n\n  - \u201cCode Completion with Statistical Language Models\u201d, PLDI\u201914 is cited incorrectly -- it also does code completion with recurrent neural networks.\n\n  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA\u201916 below). \n\n  - This submission is not the only one that \u201ccan automatically extract features\u201d. Some high-precision (cited) baselines do it.\n\n  - \u201cStructured generative models of natural source code\u201d is an incorrect citation. It is from ICML\u201914 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.\n\n\n2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA\u201916 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA\u201916 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).\n\n\n3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but \u201cProbabilistic Model for Code with Decision Trees\u201d, (OOPSLA 2016) which appeared before the submission deadline for ICLR\u201917:\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Nov 2016", "REVIEWER_CONFIDENCE": 5}], "authors": "Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, Dawn Song", "accepted": false, "id": "603"}