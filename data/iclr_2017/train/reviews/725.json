{"conference": "ICLR 2017 conference submission", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN -> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "The connection between different models is interesting, except for Bayesian net which is superficial and need to discuss more; MNIST results are interesting but more tasks need to be explored.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "interesting connection between DNN and simplified SFNN but its practical significance is unknown", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.\n\nThe connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nMy main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Promising MNIST classification results, but stronger baseline on CIFAR-10, CIFAR-100, or SVHN would have been nice", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Update: Because no revision of the paper has been provided by the authors, I am reducing my rating to \"marginally below acceptance\".\n\n----------\n\nThis paper addresses the problem of training stochastic feedforward neural networks.  It proposes to transfer weights from a deterministic deep neural network trained using standard procedures (including techniques such as dropout and batch normalization) to a stochastic network having the same topology.  The initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights, and appropriate specification of the stochastic latent units if the DNN used for pretraining employs ReLU nonlinearities.  Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the DNN used for pretraining uses sigmoid nonlinearities, but not if the pretraining DNN uses ReLUs.  To tackle this problem, the paper introduces the \"simplified stochastic feedforward neural network,\" in which every stochastic layer is followed by a layer that takes an expectation over samples from its input, thus limiting the propagation of stochasticity in the network.  A modified process for transferring weights from a pretraining DNN to the simplified SFNN is described and justified.  The training process then occurs in three steps:  (1) pretrain a DNN, (2) transfer weights from the DNN to a simplified SFNN and continue training, and (3) optionally transfer the weights to a full SFNN and continue training or transfer them to a deterministic model (called DNN*) and continue training.  The third step can be skipped and the simplified SFNN may also be used directly as an inference model.  Experimental results on MNIST classification show that the use of simplified SFNN training can improve a deterministic DNN* model over a DNN baseline trained with batch normalization and dropout.  Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods.  Finally, experiments on CIFAR-10, CIFAR-100, and SVHN with the LeNet-5, network-in-network, and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic (DNN*) model.\n\nIt is a bit confusing to refer to \"multi-modal\" tasks, when what is meant is \"generative tasks with a multimodal target distribution\" because \"multi-modal\" task can also refer to a learning task that crosses sensory modalities such as audio-visual speech recognition, text-based image retrieval, or image captioning.  I recommend that you use the more precise term (\"generative tasks with a multimodal target distribution\") early in the introduction and then say that you will refer to such tasks as \"multi-modal tasks\" in the rest of the paper for the sake of brevity.\n\nThe paper would be easier to read if \"SFNN\" were not used to refer to both the singular (\"stochastic feedforward neural network\") and plural (\"stochastic feedforward neural networks\") cases.  When the plural is meant, write \"SFNNs\".\n\nIn Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nThe notation that uses superscripts to indicate layer indexes is confusing.  The reader naturally parses N\u00b2 as \"N squared\" and not as \"the number of units in the second layer.\"\n\nWhen you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nWhat does NCSFNN stand for in the supplementary material?\n\nPros\n+ The proposed model is easy to implement and apply to other tasks.\n+ The MNIST results showing that the stochastic model training can produce a deterministic model (called DNN* in the paper) that generalizes better than a DNN trained with batch normalization and dropout is quite exciting.\n\nCons\n- For the reasons outlined above, the paper is at times a bit hard to follow.\n- The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  While this is shown on MINST, demonstration of a similar result on a more challenging task would strengthen the paper.\n\nMinor issues\n\nIt has been believed that stochastic \u2192 It is believed that stochastic\n\nunderlying these successes is on the efficient training methods \u2192 underlying these successes is efficient training methods\n\nnecessary in order to model complex stochastic natures in many real-world tasks \u2192 necessary in to model the complex stochastic nature of many real-world tasks\n\nstructured prediction, image generation and memory networks : memory networks are models, not tasks.\n\nFurthermore, it has been believed that SFNN \u2192 Furthermore, it is believed that SFNN\n\nusing backpropagation under the variational techniques and the reparameterization tricks  \u2192 using backpropagation with variational techniques and reparameterization tricks\n\nThere have been several efforts developing efficient training methods \u2192 There have been several efforts toward developing efficient training methods\n\nHowever, training SFNN is still significantly slower than doing DNN \u2192 However, training a SFNN is still significantly slower than training a DNN\n\ne.g., most prior works on this line have considered a \u2192 consequently most prior works in this area have considered a\n\nInstead of training SFNN directly \u2192 Instead of training a SFNN directly\n\nwhether pre-trained parameters of DNN \u2192 whether pre-trained parameters from a DNN\n\nwith further fine-tuning of light cost \u2192 with further low-cost fine-tuning\n\nrecent advances in DNN on its design and training \u2192 recent advances in DNN design and training\n\nit is rather believed that transferring parameters \u2192  it is believed that transferring parameters\n\nbut the opposite direction is unlikely possible \u2192 but the opposite is unlikely\n\nTo address the issues, we propose \u2192 To address these issues, we propose\n\nwhich intermediates between SFNN and DNN, \u2192 which is intermediate between SFNN and DNN,\n\nin forward pass and computing gradients in backward pass \u2192 in the forward pass and computing gradients in the backward pass\n\nin order to handle the issue in forward pass \u2192  in order to handle the issue in the forward pass\n\nNeal (1990) proposed a Gibbs sampling \u2192 Neal (1990) proposed Gibbs sampling\n\nfor making DNN and SFNN are equivalent \u2192 for making the DNN and SFNN equivalent\n\nin the case when DNN uses the unbounded ReLU \u2192 in the case when the DNN uses the unbounded ReLU\n\nare of ReLU-DNN type due to the gradient vanishing problem \u2192 are of the ReLU-DNN type because they mitigate the gradient vanishing problem\n\nmultiple modes in outupt space y \u2192 multiple modes in output space y\n\nThe only first hidden layer of DNN \u2192 Only the first hidden layer of the DNN\n\nis replaced by stochastic one, \u2192 is replaced by a stochastic layer,\n\nthe former significantly outperforms for the latter for the \u2192 the former significantly outperforms the latter for the\n\nsimple parameter transformations from DNN to SFNN are not clear to work in general, \u2192 simple parameter transformations from DNN to SFNN do not clearly work in general,\n\nis a special form of stochastic neural networks \u2192 is a special form of stochastic neural network\n\nAs like (3), the first layer is \u2192 As in (3), the first layer is\n\nThis connection naturally leads an efficient training procedure \u2192 This connection naturally leads to an efficient training procedure\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "14 Dec 2016 (modified: 25 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "How is simplified SFNN to DNN* transfer performed?", "IS_META_REVIEW": false, "comments": "When you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "12 Dec 2016", "TITLE": "Question about Table 1", "IS_META_REVIEW": false, "comments": "In Table 1, do the 4-layer SFNNs have one or two layers of stochastic units?  What about the 3-layer networks?  I suppose you could take the expectation in the output layer.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "12 Dec 2016", "TITLE": "Citation format does not match the ICLR template", "IS_META_REVIEW": false, "comments": "In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Generative nature of Stochastic DNN vs. Generative Deep Bayesian Network", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "29 Nov 2016", "TITLE": "How do MNIST half experiments differ from those in Raiko et al., 2014?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the margins to the correct spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "The connection between different models is interesting, except for Bayesian net which is superficial and need to discuss more; MNIST results are interesting but more tasks need to be explored.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "interesting connection between DNN and simplified SFNN but its practical significance is unknown", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.\n\nThe connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nMy main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Promising MNIST classification results, but stronger baseline on CIFAR-10, CIFAR-100, or SVHN would have been nice", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Update: Because no revision of the paper has been provided by the authors, I am reducing my rating to \"marginally below acceptance\".\n\n----------\n\nThis paper addresses the problem of training stochastic feedforward neural networks.  It proposes to transfer weights from a deterministic deep neural network trained using standard procedures (including techniques such as dropout and batch normalization) to a stochastic network having the same topology.  The initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights, and appropriate specification of the stochastic latent units if the DNN used for pretraining employs ReLU nonlinearities.  Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the DNN used for pretraining uses sigmoid nonlinearities, but not if the pretraining DNN uses ReLUs.  To tackle this problem, the paper introduces the \"simplified stochastic feedforward neural network,\" in which every stochastic layer is followed by a layer that takes an expectation over samples from its input, thus limiting the propagation of stochasticity in the network.  A modified process for transferring weights from a pretraining DNN to the simplified SFNN is described and justified.  The training process then occurs in three steps:  (1) pretrain a DNN, (2) transfer weights from the DNN to a simplified SFNN and continue training, and (3) optionally transfer the weights to a full SFNN and continue training or transfer them to a deterministic model (called DNN*) and continue training.  The third step can be skipped and the simplified SFNN may also be used directly as an inference model.  Experimental results on MNIST classification show that the use of simplified SFNN training can improve a deterministic DNN* model over a DNN baseline trained with batch normalization and dropout.  Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods.  Finally, experiments on CIFAR-10, CIFAR-100, and SVHN with the LeNet-5, network-in-network, and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic (DNN*) model.\n\nIt is a bit confusing to refer to \"multi-modal\" tasks, when what is meant is \"generative tasks with a multimodal target distribution\" because \"multi-modal\" task can also refer to a learning task that crosses sensory modalities such as audio-visual speech recognition, text-based image retrieval, or image captioning.  I recommend that you use the more precise term (\"generative tasks with a multimodal target distribution\") early in the introduction and then say that you will refer to such tasks as \"multi-modal tasks\" in the rest of the paper for the sake of brevity.\n\nThe paper would be easier to read if \"SFNN\" were not used to refer to both the singular (\"stochastic feedforward neural network\") and plural (\"stochastic feedforward neural networks\") cases.  When the plural is meant, write \"SFNNs\".\n\nIn Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nThe notation that uses superscripts to indicate layer indexes is confusing.  The reader naturally parses N\u00b2 as \"N squared\" and not as \"the number of units in the second layer.\"\n\nWhen you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nWhat does NCSFNN stand for in the supplementary material?\n\nPros\n+ The proposed model is easy to implement and apply to other tasks.\n+ The MNIST results showing that the stochastic model training can produce a deterministic model (called DNN* in the paper) that generalizes better than a DNN trained with batch normalization and dropout is quite exciting.\n\nCons\n- For the reasons outlined above, the paper is at times a bit hard to follow.\n- The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  While this is shown on MINST, demonstration of a similar result on a more challenging task would strengthen the paper.\n\nMinor issues\n\nIt has been believed that stochastic \u2192 It is believed that stochastic\n\nunderlying these successes is on the efficient training methods \u2192 underlying these successes is efficient training methods\n\nnecessary in order to model complex stochastic natures in many real-world tasks \u2192 necessary in to model the complex stochastic nature of many real-world tasks\n\nstructured prediction, image generation and memory networks : memory networks are models, not tasks.\n\nFurthermore, it has been believed that SFNN \u2192 Furthermore, it is believed that SFNN\n\nusing backpropagation under the variational techniques and the reparameterization tricks  \u2192 using backpropagation with variational techniques and reparameterization tricks\n\nThere have been several efforts developing efficient training methods \u2192 There have been several efforts toward developing efficient training methods\n\nHowever, training SFNN is still significantly slower than doing DNN \u2192 However, training a SFNN is still significantly slower than training a DNN\n\ne.g., most prior works on this line have considered a \u2192 consequently most prior works in this area have considered a\n\nInstead of training SFNN directly \u2192 Instead of training a SFNN directly\n\nwhether pre-trained parameters of DNN \u2192 whether pre-trained parameters from a DNN\n\nwith further fine-tuning of light cost \u2192 with further low-cost fine-tuning\n\nrecent advances in DNN on its design and training \u2192 recent advances in DNN design and training\n\nit is rather believed that transferring parameters \u2192  it is believed that transferring parameters\n\nbut the opposite direction is unlikely possible \u2192 but the opposite is unlikely\n\nTo address the issues, we propose \u2192 To address these issues, we propose\n\nwhich intermediates between SFNN and DNN, \u2192 which is intermediate between SFNN and DNN,\n\nin forward pass and computing gradients in backward pass \u2192 in the forward pass and computing gradients in the backward pass\n\nin order to handle the issue in forward pass \u2192  in order to handle the issue in the forward pass\n\nNeal (1990) proposed a Gibbs sampling \u2192 Neal (1990) proposed Gibbs sampling\n\nfor making DNN and SFNN are equivalent \u2192 for making the DNN and SFNN equivalent\n\nin the case when DNN uses the unbounded ReLU \u2192 in the case when the DNN uses the unbounded ReLU\n\nare of ReLU-DNN type due to the gradient vanishing problem \u2192 are of the ReLU-DNN type because they mitigate the gradient vanishing problem\n\nmultiple modes in outupt space y \u2192 multiple modes in output space y\n\nThe only first hidden layer of DNN \u2192 Only the first hidden layer of the DNN\n\nis replaced by stochastic one, \u2192 is replaced by a stochastic layer,\n\nthe former significantly outperforms for the latter for the \u2192 the former significantly outperforms the latter for the\n\nsimple parameter transformations from DNN to SFNN are not clear to work in general, \u2192 simple parameter transformations from DNN to SFNN do not clearly work in general,\n\nis a special form of stochastic neural networks \u2192 is a special form of stochastic neural network\n\nAs like (3), the first layer is \u2192 As in (3), the first layer is\n\nThis connection naturally leads an efficient training procedure \u2192 This connection naturally leads to an efficient training procedure\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "14 Dec 2016 (modified: 25 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "How is simplified SFNN to DNN* transfer performed?", "IS_META_REVIEW": false, "comments": "When you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "12 Dec 2016", "TITLE": "Question about Table 1", "IS_META_REVIEW": false, "comments": "In Table 1, do the 4-layer SFNNs have one or two layers of stochastic units?  What about the 3-layer networks?  I suppose you could take the expectation in the output layer.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "12 Dec 2016", "TITLE": "Citation format does not match the ICLR template", "IS_META_REVIEW": false, "comments": "In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Generative nature of Stochastic DNN vs. Generative Deep Bayesian Network", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "29 Nov 2016", "TITLE": "How do MNIST half experiments differ from those in Raiko et al., 2014?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the margins to the correct spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}], "authors": "Kimin Lee, Jaehyung Kim, Song Chong, Jinwoo Shin", "accepted": false, "id": "725"}