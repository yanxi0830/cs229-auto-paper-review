{"conference": "ICLR 2017 conference submission", "title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Review response", "IS_META_REVIEW": false, "comments": "We thank all three reviewers for the valuable comments and suggestions. \n\nWe agree with reviewer 1 that the lack of test results is not ideal and sadly we do not yet have a manner in which we can run on the hidden test set, as not all of our code is open-sourced. However, we hope that the significant dev set size of 10k items, along with the cross-validation results add some reassurance that our hyperparameter tuning scheme has not overfit the data.\n\nReviewer 3 points out that the results in this paper are no longer state of the art. It is true that there are other papers on the leaderboard that have now surpassed our results, largely through ensembling. However, we believe that our paper is the only work to specifically study the impact of different span representations and we agree with Reviewer 3 that our findings should be complementary to other recent work on this dataset. We have added some extra quantitative and qualitative analysis of the differences between the span classifier and the endpoints predictor to illustrate the manner in which the quality of endpoint predictions degrade for longer sentences, in particular showing the tendency of endpoint models to pick out endpoints from separate answer candidates.\n\nReviewer 2 points out that the difference in performance between our model and the Match-LSTM cannot be accounted for by the difference in label type alone, and asks for the other most salient differences between the two approaches. While there are many small differences between the two implementations, the ablations in Table 2.a. suggest that most of this gap is accounted for by the passage independent question representation that is missing in the Match-LSTM. We have added an analysis of this representation in a new Table 3 and we have updated our discussion of the Match LSTM to clarify the basis of our comparison.", "OTHER_KEYS": "Kenton Lee"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.\n\nIt is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems/submissions on the SQUAD leader board that were available for comparison.\n\nGiven that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Review response", "IS_META_REVIEW": false, "comments": "We thank all three reviewers for the valuable comments and suggestions. \n\nWe agree with reviewer 1 that the lack of test results is not ideal and sadly we do not yet have a manner in which we can run on the hidden test set, as not all of our code is open-sourced. However, we hope that the significant dev set size of 10k items, along with the cross-validation results add some reassurance that our hyperparameter tuning scheme has not overfit the data.\n\nReviewer 3 points out that the results in this paper are no longer state of the art. It is true that there are other papers on the leaderboard that have now surpassed our results, largely through ensembling. However, we believe that our paper is the only work to specifically study the impact of different span representations and we agree with Reviewer 3 that our findings should be complementary to other recent work on this dataset. We have added some extra quantitative and qualitative analysis of the differences between the span classifier and the endpoints predictor to illustrate the manner in which the quality of endpoint predictions degrade for longer sentences, in particular showing the tendency of endpoint models to pick out endpoints from separate answer candidates.\n\nReviewer 2 points out that the difference in performance between our model and the Match-LSTM cannot be accounted for by the difference in label type alone, and asks for the other most salient differences between the two approaches. While there are many small differences between the two implementations, the ablations in Table 2.a. suggest that most of this gap is accounted for by the passage independent question representation that is missing in the Match-LSTM. We have added an analysis of this representation in a new Table 3 and we have updated our discussion of the Match LSTM to clarify the basis of our comparison.", "OTHER_KEYS": "Kenton Lee"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.\n\nIt is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems/submissions on the SQUAD leader board that were available for comparison.\n\nGiven that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Kenton Lee, Tom Kwiatkowksi, Ankur Parikh, Dipanjan Das", "accepted": false, "id": "711"}