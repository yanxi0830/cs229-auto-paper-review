{"conference": "ICLR 2017 conference submission", "title": "Multiplicative LSTM for sequence modelling", "abstract": "We introduce multiplicative LSTM (mLSTM), a novel recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. mLSTM is characterised by its ability to have different recurrent transition functions for each possible input, which we argue makes it more expressive for autoregressive density estimation. We demonstrate empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level modelling tasks, and that this improvement increases with the complexity of the task. This model achieves a test error of 1.19 bits/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Pros:\n* Clearly written.\n* New model mLSTM which seems to be useful according to the results.\n* Some interesting experiments on big data.\n\nCons:\n* Number of parameters in comparisons of different models is missing.\n* mLSTM is behind some other models in most tasks."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a new way of doing multiplicative / tensored recurrent weights in RNNs. The multiplicative weights are input dependent. Results are presented on language modeling (PTB and Hutter). We found the paper to be clearly written, and the idea well motivated. However, as pointed out by the reviewers, the results were not state of the art. We feel that is that this is because the authors did not make a strong attempt at regularizing the training. Better results on a larger set of tasks would have probably made this paper easier to accept. \n \n Pros:\n - interesting idea, and reasonable results\n Cons:\n - only shown on language modeling tasks\n - results were not very strong, when compared to other methods (which typically used strong regularization and training like batch normalization etc).\n - reviewers did not find the experiments convincing enough, and felt that a fair comparison would be to compare with dynamic weights on the competing RNNs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "An Interesting paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "* Brief Summary: \n\nThis paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear.\n\n* Criticisms:\n\n- In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper.\n- The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. \n- There are some non-standard choices on modifications on the standard algorithms, such as \"l\" parameter of RMSProp and multiplying output gate before the nonlinearity.\n- The experimental results are only limited to character-level language modeling only. \n\n* An Overview of the Review:\n\nPros:\n- A simple modification that seems to reasonably well in practice.\n- Well-written.\n\nCons:\n- Lack of good enough experimental results.\n- Not enough contributions (almost trivial extension over existing algorithms).\n- Non-standard modifications over the existing algorithms.\n\n[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).\n[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Not bad work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes an extension of the multiplicative RNN [1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM. \n\nThe paper proposes some interesting tricks, but none of them seems to be very crucial. For instance, in Eq. (16), the authors propose to multiply the output gate inside the activation function in order to alleviate the saturation problem in logistic sigmoid or hyperbolic tangent. Also, the authors share m_t across the inference of different gating units and cell-state candidates, at the end this brings only 1.25 times increase on the number of model parameters. Lastly, the authors use a variant of RMSProp where they add an additional hyper-parameter $\\ell$ and schedule it across the training time. It would be nicer to apply the same tricks to other baseline models and show the improvement with regard to each trick.\n\nWith the new architectural modification to the LSTM and all the tricks combined, the performance is not as great as we would expect. Why didn\u2019t the authors apply batch normalization, layer normalization or zoneout to their models? Was there any issue with applying one of those regularization or optimization techniques? \n\nAt the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading. I find it a bit hard to connect dynamic evaluation as a variant of fast weights. Fast weights do not use test error signal. In the paper, the authors claim that \u201cdynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted\u201d, and I am afraid to tell that this assumption is very misleading. We should never assume that test label information is given at the inference time. The test label information is there to evaluate the generalization performance of the model. In some applications, we may get the label information at test time, e.g., stock prediction, weather forecasting, however, in many other applications, we don\u2019t. For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting. Also, it would be fair to apply dynamic evaluation to all the other baseline models as well to compare with the BPC score 1.19 achieved by the proposed mLSTM.\n\nThe quality of the work is not that bad, but the novelty of the paper is not that good either. The performance of the proposed model is oftentime worse than other methods, and it is only better when dynamic evaluation is coupled together. However, dynamic evaluation can improve the other methods as well.\n\n[1] Ilya et al., \u201cGenerating Text with Recurrent Neural Networks\u201d, ICML\u201911", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "useful new LSTM variant", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Pros:\n* Clearly written.\n* New model mLSTM which seems to be useful according to the results.\n* Some interesting experiments on big data.\n\nCons:\n* Number of parameters in comparisons of different models is missing.\n* mLSTM is behind some other models in most tasks.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "Some more details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Some small details  ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "other models, more details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "Pros:\n* Clearly written.\n* New model mLSTM which seems to be useful according to the results.\n* Some interesting experiments on big data.\n\nCons:\n* Number of parameters in comparisons of different models is missing.\n* mLSTM is behind some other models in most tasks."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a new way of doing multiplicative / tensored recurrent weights in RNNs. The multiplicative weights are input dependent. Results are presented on language modeling (PTB and Hutter). We found the paper to be clearly written, and the idea well motivated. However, as pointed out by the reviewers, the results were not state of the art. We feel that is that this is because the authors did not make a strong attempt at regularizing the training. Better results on a larger set of tasks would have probably made this paper easier to accept. \n \n Pros:\n - interesting idea, and reasonable results\n Cons:\n - only shown on language modeling tasks\n - results were not very strong, when compared to other methods (which typically used strong regularization and training like batch normalization etc).\n - reviewers did not find the experiments convincing enough, and felt that a fair comparison would be to compare with dynamic weights on the competing RNNs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "An Interesting paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "* Brief Summary: \n\nThis paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear.\n\n* Criticisms:\n\n- In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper.\n- The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. \n- There are some non-standard choices on modifications on the standard algorithms, such as \"l\" parameter of RMSProp and multiplying output gate before the nonlinearity.\n- The experimental results are only limited to character-level language modeling only. \n\n* An Overview of the Review:\n\nPros:\n- A simple modification that seems to reasonably well in practice.\n- Well-written.\n\nCons:\n- Lack of good enough experimental results.\n- Not enough contributions (almost trivial extension over existing algorithms).\n- Non-standard modifications over the existing algorithms.\n\n[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).\n[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Not bad work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes an extension of the multiplicative RNN [1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM. \n\nThe paper proposes some interesting tricks, but none of them seems to be very crucial. For instance, in Eq. (16), the authors propose to multiply the output gate inside the activation function in order to alleviate the saturation problem in logistic sigmoid or hyperbolic tangent. Also, the authors share m_t across the inference of different gating units and cell-state candidates, at the end this brings only 1.25 times increase on the number of model parameters. Lastly, the authors use a variant of RMSProp where they add an additional hyper-parameter $\\ell$ and schedule it across the training time. It would be nicer to apply the same tricks to other baseline models and show the improvement with regard to each trick.\n\nWith the new architectural modification to the LSTM and all the tricks combined, the performance is not as great as we would expect. Why didn\u2019t the authors apply batch normalization, layer normalization or zoneout to their models? Was there any issue with applying one of those regularization or optimization techniques? \n\nAt the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading. I find it a bit hard to connect dynamic evaluation as a variant of fast weights. Fast weights do not use test error signal. In the paper, the authors claim that \u201cdynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted\u201d, and I am afraid to tell that this assumption is very misleading. We should never assume that test label information is given at the inference time. The test label information is there to evaluate the generalization performance of the model. In some applications, we may get the label information at test time, e.g., stock prediction, weather forecasting, however, in many other applications, we don\u2019t. For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting. Also, it would be fair to apply dynamic evaluation to all the other baseline models as well to compare with the BPC score 1.19 achieved by the proposed mLSTM.\n\nThe quality of the work is not that bad, but the novelty of the paper is not that good either. The performance of the proposed model is oftentime worse than other methods, and it is only better when dynamic evaluation is coupled together. However, dynamic evaluation can improve the other methods as well.\n\n[1] Ilya et al., \u201cGenerating Text with Recurrent Neural Networks\u201d, ICML\u201911", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "useful new LSTM variant", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Pros:\n* Clearly written.\n* New model mLSTM which seems to be useful according to the results.\n* Some interesting experiments on big data.\n\nCons:\n* Number of parameters in comparisons of different models is missing.\n* mLSTM is behind some other models in most tasks.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "Some more details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Some small details  ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "other models, more details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Ben Krause, Iain Murray, Steve Renals, Liang Lu", "accepted": false, "id": "527"}