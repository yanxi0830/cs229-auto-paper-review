{"conference": "ICLR 2017 conference submission", "title": "Sequence generation with a physiologically plausible model of handwriting and Recurrent Mixture Density Networks", "abstract": "The purpose of this study is to explore the feasibility and potential benefits of using a physiological plausible model of handwriting as a feature representation for sequence generation with recurrent mixture density networks. We build on recent results in handwriting prediction developed by Graves (2013), and we focus on generating sequences that possess the statistical and dynamic qualities of handwriting and calligraphic art forms. Rather than model raw sequence data, we first preprocess and reconstruct the input training data with a concise representation given by a motor plan (in the form of a coarse sequence of `ballistic' targets) and corresponding dynamic parameters (which define the velocity and curvature of the pen-tip trajectory). This representation provides a number of advantages, such as enabling the system to learn from very few examples by introducing artificial variability in the training data, and mixing of visual and dynamic qualities learned from different datasets.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a system, namely a recurrent model for handwriting generation. However it doesn't make a clear case for what contribution is being made, or convincing experimental comparisons. The reviews, while short, provide consistent suggestions and directions for how this work could be improved and reworked.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Nice paper, but no machine learning contribution or evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM.\nThere are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art.\n\nSo I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting, but low novelty, and sub-standard evaluation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method. The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation. In principle, rich literature on motion prediction for various applications could be relevant here. Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. \n\nEvaluation is a big concern. There is no quantitative evaluation. There is no comparision with other methods.\n\nI still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself. This should be evaluated.\n\nDetails:\n\nThere are several typos and word omissions, which can be found by carefully rereading the paper.\n\nAt the beginning of section 3, it is still unclear what the application is. Prediction of dynamic parameters? What for? Section 3 should give a better motivation of the work.\n\nConcerning the following paragraph\n\n\"While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace. \n\"\nThis method has not been explained. A paper should be self-contained.\n\nThe authors mentioned that the \"V2V-model is conditioned on (...)\"; but not enough details are given. \n\nGenerally speaking, more efforts could be made to make the paper more self-contained.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "New representation space for a cut down version of an old model, with no quantitative results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.) ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "07 Dec 2016", "TITLE": "Offline/online", "IS_META_REVIEW": false, "comments": "The IAM dataset is an offline dataset (images). How did you generate the stroke sequences?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Dec 2016", "TITLE": "Sequence prediction", "IS_META_REVIEW": false, "comments": "Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. Did you look at the state of the art on this? Shouldn't this be applicable here?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Dec 2016", "TITLE": "Intermediate representation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a system, namely a recurrent model for handwriting generation. However it doesn't make a clear case for what contribution is being made, or convincing experimental comparisons. The reviews, while short, provide consistent suggestions and directions for how this work could be improved and reworked.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Nice paper, but no machine learning contribution or evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM.\nThere are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art.\n\nSo I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting, but low novelty, and sub-standard evaluation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method. The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation. In principle, rich literature on motion prediction for various applications could be relevant here. Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. \n\nEvaluation is a big concern. There is no quantitative evaluation. There is no comparision with other methods.\n\nI still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself. This should be evaluated.\n\nDetails:\n\nThere are several typos and word omissions, which can be found by carefully rereading the paper.\n\nAt the beginning of section 3, it is still unclear what the application is. Prediction of dynamic parameters? What for? Section 3 should give a better motivation of the work.\n\nConcerning the following paragraph\n\n\"While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace. \n\"\nThis method has not been explained. A paper should be self-contained.\n\nThe authors mentioned that the \"V2V-model is conditioned on (...)\"; but not enough details are given. \n\nGenerally speaking, more efforts could be made to make the paper more self-contained.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "New representation space for a cut down version of an old model, with no quantitative results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.) ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "07 Dec 2016", "TITLE": "Offline/online", "IS_META_REVIEW": false, "comments": "The IAM dataset is an offline dataset (images). How did you generate the stroke sequences?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Dec 2016", "TITLE": "Sequence prediction", "IS_META_REVIEW": false, "comments": "Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. Did you look at the state of the art on this? Shouldn't this be applicable here?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Dec 2016", "TITLE": "Intermediate representation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Daniel Berio, Memo Akten, Frederic Fol Leymarie, Mick Grierson, R\u00e9jean Plamondon", "accepted": false, "id": "581"}