{"conference": "ICLR 2017 conference submission", "title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "abstract": "The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.\n\nThe pros of the paper are:\n1. A novel perspective to understand the recent progress of neural network is proposed.\n2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.\n\nThe main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the \u201cunrolled iterative estimation\" and the stage 4 of Figure 3 seems not follow the assumption of \"unrolled iterative estimation\" and the authors says: \"We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture.\". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. \n\nMoreover, the paper should provide more experiments to show the evidence of \"unrolled iterative estimation\", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper provides interesting new interpretations of highway and residual networks, which should be of great interest to the community.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Updated Version", "IS_META_REVIEW": false, "comments": "We've uploaded an updated version of the paper which addresses reviewers' concerns and makes several improvements. Apologies for the late revision, mainly due to the time-consuming ImageNet experiments. \n\n- To further confirm the iterative estimation, and contrast our view with the findings of Zeiler and Fergus (2014), we did a literature survey and planned on adding some visualizations of ResNet features.  We found a technical report from Brian Chu at Berkeley who applied known visualization techniques to Resnets independently and report exactly the behaviour we were expecting: The features within a stage get refined and sharpened. So now we refer to their paper and (with their kind permission) reproduce one of their visualizations. We have added a discussion of these findings which support our proposed view. \n\n- We've added mean+-std results for the experiments in Section 5.1.\nThese experiments take a long time, so we performed only three runs each, but the results are stable enough for comparison.\n\n- We've elaborated a bit further on the role of batch normalization in Section 5.1. \n\n- improved language and organization\n\nApart from these changes we've also started investigating the unexpected behaviour of stage 4, by creating modified architectures. We've added three blocks to that stage and we've tried adding a fifth stage. We found that both variants improve performance to ca. 6.8% top5 error.\nBut the average estimation error stays high for the first few blocks and only later starts to decrease: [-0.32, -0.31, -0.30, -0.27, -0.17] (compare Figure 3)\nWe are further investigating this and we'll add our findings as soon as they paint a coherent picture.  But as of now, we didn't consider them to be interesting enough to be included in the paper. \n", "OTHER_KEYS": "Klaus Greff"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "An interesting angle on resNets and Highway nets", "comments": "Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                 \n                                                                                                                                                                                                          \nPros                                                                                                                                                                                                      \n- the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                 \n- Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                    \n                                                                                                                                                                                                          \nCons                                                                                                                                                                                                      \n- Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                         \n- Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   ", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "An interesting perspective on representations in DNNs", "comments": "The paper describes an alternative view on hierarchical feature representations in deep neural networks. The viewpoint of refining representations is well motivated and is in agreement with the success of recent model structures like ResNets.\n\nPros:\n\n- Good motivation for the effectiveness of ResNets and Highway networks\n- Convincing analysis and evaluation\n\nCons:\n\n- The effect of this finding of the interpretation of batch-normalization is only captured briefly but seems to be significant\n- Explanation of findings in (Zeiler & Fergus (2014)) using UIE viewpoint missing\n\nRemarks:\n\n- Missing word in line 223: \"that it *is* valid\"", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "A New Perspective of ResNet and Highway Network", "comments": "This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.\n\nThe pros of the paper are:\n1. A novel perspective to understand the recent progress of neural network is proposed.\n2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.\n\nThe main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the \u201cunrolled iterative estimation\" and the stage 4 of Figure 3 seems not follow the assumption of \"unrolled iterative estimation\" and the authors says: \"We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture.\". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. \n\nMoreover, the paper should provide more experiments to show the evidence of \"unrolled iterative estimation\", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.\n\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 5, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "several questions", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "13 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Batch Normalization", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.\n\nThe pros of the paper are:\n1. A novel perspective to understand the recent progress of neural network is proposed.\n2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.\n\nThe main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the \u201cunrolled iterative estimation\" and the stage 4 of Figure 3 seems not follow the assumption of \"unrolled iterative estimation\" and the authors says: \"We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture.\". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. \n\nMoreover, the paper should provide more experiments to show the evidence of \"unrolled iterative estimation\", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper provides interesting new interpretations of highway and residual networks, which should be of great interest to the community.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Updated Version", "IS_META_REVIEW": false, "comments": "We've uploaded an updated version of the paper which addresses reviewers' concerns and makes several improvements. Apologies for the late revision, mainly due to the time-consuming ImageNet experiments. \n\n- To further confirm the iterative estimation, and contrast our view with the findings of Zeiler and Fergus (2014), we did a literature survey and planned on adding some visualizations of ResNet features.  We found a technical report from Brian Chu at Berkeley who applied known visualization techniques to Resnets independently and report exactly the behaviour we were expecting: The features within a stage get refined and sharpened. So now we refer to their paper and (with their kind permission) reproduce one of their visualizations. We have added a discussion of these findings which support our proposed view. \n\n- We've added mean+-std results for the experiments in Section 5.1.\nThese experiments take a long time, so we performed only three runs each, but the results are stable enough for comparison.\n\n- We've elaborated a bit further on the role of batch normalization in Section 5.1. \n\n- improved language and organization\n\nApart from these changes we've also started investigating the unexpected behaviour of stage 4, by creating modified architectures. We've added three blocks to that stage and we've tried adding a fifth stage. We found that both variants improve performance to ca. 6.8% top5 error.\nBut the average estimation error stays high for the first few blocks and only later starts to decrease: [-0.32, -0.31, -0.30, -0.27, -0.17] (compare Figure 3)\nWe are further investigating this and we'll add our findings as soon as they paint a coherent picture.  But as of now, we didn't consider them to be interesting enough to be included in the paper. \n", "OTHER_KEYS": "Klaus Greff"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "An interesting angle on resNets and Highway nets", "comments": "Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                 \n                                                                                                                                                                                                          \nPros                                                                                                                                                                                                      \n- the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                 \n- Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                    \n                                                                                                                                                                                                          \nCons                                                                                                                                                                                                      \n- Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                         \n- Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   ", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "An interesting perspective on representations in DNNs", "comments": "The paper describes an alternative view on hierarchical feature representations in deep neural networks. The viewpoint of refining representations is well motivated and is in agreement with the success of recent model structures like ResNets.\n\nPros:\n\n- Good motivation for the effectiveness of ResNets and Highway networks\n- Convincing analysis and evaluation\n\nCons:\n\n- The effect of this finding of the interpretation of batch-normalization is only captured briefly but seems to be significant\n- Explanation of findings in (Zeiler & Fergus (2014)) using UIE viewpoint missing\n\nRemarks:\n\n- Missing word in line 223: \"that it *is* valid\"", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "A New Perspective of ResNet and Highway Network", "comments": "This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.\n\nThe pros of the paper are:\n1. A novel perspective to understand the recent progress of neural network is proposed.\n2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.\n\nThe main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the \u201cunrolled iterative estimation\" and the stage 4 of Figure 3 seems not follow the assumption of \"unrolled iterative estimation\" and the authors says: \"We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture.\". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. \n\nMoreover, the paper should provide more experiments to show the evidence of \"unrolled iterative estimation\", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.\n\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 5, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "several questions", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "13 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Batch Normalization", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}], "authors": "Klaus Greff, Rupesh K. Srivastava, J\u00fcrgen Schmidhuber", "accepted": true, "id": "338"}