{"conference": "ICLR 2017 conference submission", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Summary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "While the reviewers found some interest in this work, I'm afraid I have to agree with the critique that the model studied is too simple that its relevance for deep learning is questionable.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Official Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.\n\nIn my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.\n\nA deep neural network is fundamentally different from a single layer---it is the \"deep\" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.\n\nFor any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThe authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing. They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT). They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.\n\nThis is a well-written paper that presents a new angle on why the current CNN architectures work so well. The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section. The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers. The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.\n\nA few aspects should be improved. First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well. A discussion of how this is inverted (e.g., with pooling switches) is needed.\n\nThe relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.\n\nAlthough it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).\n\nFinally, the filter coherence measure must be defined either mathematically or with a proper reference.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Inconsistent notations with DL, and room for improvements in the presentation and the theory  ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\n\n\nSummary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "single vs multiple layers", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "notations ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 8)", "IS_META_REVIEW": false, "comments": "Section 4 Continue:\nThe experiments in 4.5 are misleading and together with 4.4 they are contradictory to their theoretical results. \nForemost, the readers need to be aware that the decoders with switch units are very powerful (if you ever trained one you would know it too). Therefore what figure 4 presents is extremely misleading because it *only* takes the very last conv layer and use their proposed linear reconstruction algorithm to recover that layer\u2019s input only.  The rest was propagated with the very powerful decoder plus switch units information (see [6] for how powerful the decoder is).  Visually, we can also see how much info switch units carry: for their conv 5 reconstruction with random activation, there is no meaningful info carried by the activation values but all through switch units and we can still obtain the silhouette of the objects in the original images; in fact, pool-1\u2019s switch units alone carries at least 64x224x224 bit information. Their results back in the appendix (Figure5) also shows their reconstruction algorithm\u2019s weakness. The lower layer has extremely bad reconstruction quality from the proposed algorithms. The higher the layer, the better the reconstruction from their proposed algorithms because decoder together switch units has more chance to correct the reconstruction.What the authors should do, is to use their IHT to reconstruct all the way, following what has been done in [1]\u2019s linear reconstruction algorithm, which the authors here failed to compare against, despite its high relevance. In fact, the author of [1] used to be in charge of this submitted project and proposed this experimental approach, with a slightly different algorithm that is presented in [1]; but current authors of this submission failed in achieving similar reconstruction results with their algorithm and hence can only perform their reconstruction algorithm over a single layer. The biggest difference between the reconstruction algorithm from [1] and this submission is that the former used pseudo-inverse of the weight matrix and the latter used the transpose because in their theory, the weight matrix is approximate orthogonal hence its transpose is approximately its inverse.\n There is another serious issue with using high-up conv layer to verify their theoretical claim: recall that the excuse this submission used to discard ReLU non-linearity is that [1] introduced the pairing phenomenon, however, [1] stressed that it only appeared in the first few layers. Hence the authors should have at least chosen the first few layers for their experiments; yet, they knowingly only presented their analysis of the highest layer. \nTable 3 again presents very misleading results. First their \u201crandom activation\u201d has a relative error 1.414 for activation space. But if we simply pick zero vectors instead of \u201crandom\u201d, it gives 1 as a relative error, which is much smaller than 1.414. Therefore, this \u201crelative error\u201d in the activation space is a very inappropriate measurement for the reconstruction quality from their algorithm. But measuring the relative error in the input space is also a bad metric because of the powerful effects from the decoders. In fact, the term \u201crelative error\u201d is a paraphrase of the evaluation method used in [1], where it is called \u201creconstruction ratio\u201d. However, it is a much more appropriate measurement coupling the reconstruction algorithm used in [1] because the reconstruction from [1] will always be a subset of the original input by using the pseudo-inverse.\n Recall that in 4.4, the authors approximate the conv weight matrix for conv(5,2) has approximately 0.05-0.1 distortion constant, which means that the \u201crelative error\u201d based on their Theorem 3.3 is 0.6-0.2, but in reality, the relative error is 1.051, which is not only worse than their theoretical guarantee but even worse than simply choosing zero vector as reconstruction. Keep in mind that these results are built on top of the assumption that ReLU does not exist, if we add it on top, these results in Table3 and Figure5 can get further downgraded. One question why there is such a huge gap. In fact, such bad reconstruction results based on their algorithm is not surprising because after all, unlike random weights, the learned conv weights are not approximately orthogonal so Model-RIP does not fit into realistic CNN models. In fact, the coherence is a very direct measurement of how \u201corthogonal\u201d the learned weights are, which both this submission and [5] (again this submission took over [5]\u2019s idea) measure this quantity: the higher the coherence the less orthogonal.  \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 7)", "IS_META_REVIEW": false, "comments": "Section 4, experiments: This section contains so much misleading information to misguide the audience.\n The issues with 4.1 has been discussed in Section2(1).\nThe experiments in 4.4 are very deceiving.The minor misleading part is the title of Figure 3(b), ( c)  saying \u201cbefore/after ReLU\u201d, note the ReLU they refer to is at the input level not the *activation* level. In other words, they did not consider ReLU after the linear responses,  and only compare whether the input to this conv layer goes through ReLU or not, which does not add any difficulty to their theoretical analysis, because they assumed the input comes from the span of the conv weights, which is also [1] and [5]\u2019s assumption. However, the most serious issue is how they collect the results for Figure 3. I tried to replicate their result for conv(1,2) because the authors claim that \u201cwe choose conv(5, 2), while other layers show similar results\u201d. Precisely, I pass randomly sampled ImageNet Validation images through VGG-16, take the activation of conv(1,2) after max-pooling to obtain c, and then reconstruct conv(1,1) activation by multiplying c with the transpose of the conv weights W^T after subtracting the bias to obtain the reconstruction (W^T*c), and calculate the term, ||W^T c||/||c||.  However, the norm ratio I obtained is 3.16 +- 0.10, very far from their 1 +- 0.1.  My experiments give a distortion constant \\delta strictly bigger than 2, which is out of the range of 0 and 1 hence a meaningless value. This gap between our result and their result can potentially originate from the following causes: the authors using fancier sparse approximation method for c instead of following the procedures of feedforward CNNs, the authors discarding ReLU on the linear response therefore their c has negative components, and the authors have bugs in their code.  \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 6)", "IS_META_REVIEW": false, "comments": "Section 3 Continue:\nThere are again several issues with their proof to the reconstruction bounds. Lemma B.3 and B.4 carry out a very crude estimation on \\|c-h\\|, since the author discard any potential contribution of filters activated from max-pooling but belong to the set of original filters that form the input signal x. That is, \\|c_{\\Omega^c}\\| can potentially be covered by \\|h_{\\Pi - \\Omega}\\|. Intuitively, if max-pooling chooses to keep a shifted filter positive and the rest zero and if the shifts of filters are highly correlated, then even if the activated filter is not one of the original filters that constitute x, this activated filter still potentially cover x along the direction of the correct filter and therefore too much information can be lost without taking into such contribution into consideration. Moreover, the logic (i.e. proof technique) behind the reconstruction bounds (Theorem 3.3) is the same as that in Theorem A.5 from [1]. The idea is to decompose the l_2 norm of the reconstruction (or reconstruction signal) into \u201coriginal/ground truth\u201d +  \u201cdifference\u201d  and then bound the reconstruction ratio [1] (this term is renamed as \u201crelative error\u201d in this work) with the aid of the singular values of the weight matrix W. But again, the authors failed to address the similarity but rather attempt to present the proof as different as possible to create artificial novelty.  Finally, their final bound is very suboptimal and only consider 2 max-pooling regions. \n The entire section 3 is conducted under the assumption of linear responses. Because the compressed sensing model, Model-RIP, is fundamentally a linear model. The formulation (see equation1) only bounds the norm but not the metric. In other words, (1-\\delta)|x|<|Wx|<(1+\\delta)|x| is what the authors attempt to prove (but with flaws), i.e. a norm preserving property between the input and its linear responses\u2014this has been discussed in [5] but the authors did not cite or relate to this work\u2014despite [5] informs the authors to explain the similarity during their previous submission attempt. However, preserving linear norms do not give any reconstruction guarantee because of ReLU can theoretically ruin the upper bound, by, for example, zeroing all activations if they are all negative.\nthe authors cannot draw any conclusion from their theoretical results to any implication in training or network design, all they concluded is the last paragraph on page 14, which is rather meaningless.  \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 5)", "IS_META_REVIEW": false, "comments": "Section 3, Analysis: This section contains many contents that are either a copy from [1]&[5] or invalid.\nProposition 1 is a trivial special case of Proposition A.1 from [1], because the pseudo-inverse of an orthogonal matrix is simply its transpose, which is the set up in Prop 1. Also since Prop 1 assumes that \\Phi spans the entire input space, then range(W) easily becomes R^(M*D), however this assumption is generally invalid, which was empirically verified by Table S.1 from [1]\u2019s appendix for the case of deep CNNs for CIFAR-10/100. The authors clearly copied [1]\u2019s idea and decide to present Prop 1 since they know [1] very well, but they did not address this copying action. They should have cited [1]\u2019s Prop A.1, then present Porp 1 as a corollary and explain their set-up is a simplified version with no novel technicality. \n Following Proposition 1, the author cited [1] (the only time they mentioned it throughout this submission) and claims that based on [1]\u2019s result they can assume linear responses (\u201cassuming that all of the entries in the vectors are real numbers, rather than only non-negative.\u201d) However, [1] never claims that linear responses are acceptable (the authors of [1] in fact even did experiments with no non-linearity and the results significantly dropped) and [1] finds that only the first few convolution convolutional layers appear such pairing phenomenon. But in this submission, the experimental section focuses on the last convolutional layer of VGGNet, which by no means has such negative and positive pairing property. Therefore, their arguments of leaving out ReLU and assuming linear responses are groundless.\nThe authors fail to address convolution in their proof of Theorem 3.1 which resulting the proof to the convolution layer is no different from fully connected layer. More specifically, when proving theorem 3.1, the authors need Lemma A.1, however there is a severe issue with the proof of Lemma A.1\u2014it does not properly take convolution into consideration. Note that they assume the \u201cshift\u201d of the same convolution filter has expected dot product 0 (E((w_{I,m1}^j_1)T(w_{I,m1}^{j2})) = 0 if j_1 \\neq j_2, this assumption only holds if the convolution filters are of high frequency but it is a common knowledge that real convolution filters are more often smooth than highly oscillatory, and the dot products between shifts by a small number of pixels can be large, opposite from being 0 as the authors claim here. Their assumption contradicts the important fact that convoluted smooth filters stay highly correlated. Also, there is a constant C that is not tracked in Theorem 3.1 \u2014only a \u201cthere exists\u201d in a mathematical sense. Not to mention the whole proof is based on random filters which, we stress again, is far away from being realistic. \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 4)", "IS_META_REVIEW": false, "comments": "Section 2, Preliminaries:\nThere are serious issues with the random filter assumption (Section 2.1 and Section 4.1 are addressed together). First of all, we all know very well that random filters have extremely limited ability, especially with very deep networks. No one has in any way successfully applied random filters to deep networks. Second of all, their one-layer results in Table 1 is very off. For instance, [4] used one-layer unsupervised learning to train 1600 conv filters (this submission uses 2048 and 1024 conv filters) that achieved 77.9% accuracy on CIFAR-10, versus their 66% (random)/68% (learned), despite their architecture is far more complicated than [4], which ironically is far beyond the scope of their theoretical setup. But the most inappropriate component is the number of convolutional filters they used. None of the commonly used deep convolutional networks carry these many filters--for their first layer, there are about 30 times more filters than the dimension of its convolutional kernel! Recall that the most valuable part of convolutional layer is that it can use a small number of conv filters to extract abstract features by stacking many of them together. Unsurprisingly, as a consequence of using excessive amount of filters in order to span as much input space as possible, their models carry a huge number of parameters, an architecture design that is totally against the principles of CNNs. Finally, Table 4 shows, random filters can form a weight matrix that is a lot more \u201corthogonal\u201d than learned filters. And such low coherence, i.e. \u201corthogonal\u201d property of the weight matrix W, is the backbone of their Model-RIP. Thus some simple, unconvincing shallow networks do not manage to fill in the gap between random filters and learned filters, especially when this gap breaks their primary theoretical promise. \n Notation in 2.2: The notations introduced in 2.2 are identical to the notations and problem set up used in [1]&[5] except that in [1]&[5], M = 1, which does not change the theoretical fundamentals. In addition, the terminologies such as \u201cshift\u201d, \u201cblock\u201d are also shared between [1]&[5] and this submission.\nThere are serious issues with the Model-RIP setup: the model-RIP mathematical model is in fact nothing fancy. The only difference between model-RIP and a generic compressed sensing RIP model is that model-RIP requires model-sparsity. To be more straightforward, in this case, the model-sparsity corresponds with max-pooling\u2014meaning sparsity is structured so that each pooling region only has at most 1 non-zero element and correspondingly, the assumption on the data space is that it is a linear combination of filters such that at most one filter from each pooling region is used. Note that this is the same assumption as appeared in [1] (see Appendix, equation S1), in fact, this assumption was originally proposed by the first author of [1]. However, the fundamental flaw of this model-RIP setup are first the lack of non-linearity, notice that if ReLU is added after the linear response then the activation can be very close to zero which makes the reconstruction has norm arbitrarily small/close to zero; second real weight matrix W cannot meet the conditions that random matrix can meet, in other words, real W is not even close to being \u201corthogonal\u201d, even the authors give support along this direction (see Table 4). Hence the actual values of the reconstruction bounds in reality are far from being significant. \n(4)  IHT: This algorithm, again, does not take into any non-linearity on the activation into account.\n ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 3)", "IS_META_REVIEW": false, "comments": "Section 1, Introduction:\nA neural network that can be shown theoretically of high invertibility does not necessarily perform well. For example, ResNets over 34 layers, which are impossible to analyze using the proposed theoretical framework, perform significantly better than VGGNets with much fewer parameters but a lot more depth; by comparison, a standard variational autoencoder would have a lot worse generative power if we set the penalty for the prior KL-divergence term to be very small and encourage good reconstruction.\n \u201cDespite these interesting results, there is no clear theoretical explanations as to why CNNs are invertible yet\u201d. There are multiple issues with this statement. First of all, this invertibility is a rather trivial common knowledge among ML researchers; people don\u2019t like to analytically explore because its theoretical bounds are too bad\u2014as proved once again in this submission. Second of all, their theoretical proof does not consider non-linear layer and it does not have the correct convolutional setup (details to come at Section 3 (3)) and their theoretical bounds have no meaningful implications for training or network design. Third of all, there have been works exploring such signal recovery property with random filters on fully connected layers ([2] and [3]) as well as exploring such property with learned filters on convolutional layers ([1] and [5]). They all have very similar underlying mathematical models and highly resembling reconstruction algorithms ([1] and [2]) with this submission. The authors ignore addressing these important related works. Lastly, their analysis only tackles a single layer, which does not improve upon existing works, especially considering that [2] takes into account of multiple layers as well as dropout stability.\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 2)", "IS_META_REVIEW": false, "comments": "The invertibility of CNNs has been explored fairly extensively in recent years both empirically [6,7,8,9] and theoretically [1,2,3]. The most critical problem of this submission is \u201cthe amount of overlap\u201d to [1,5] in terms of theoretical results as well as approaches for experimental validation. Before providing a list of overlapping contents, we point out the most fundamental differences between these two works:\nThis submission assumes random filters for their theoretical results whereas [1]&[5] impose assumptions on the learned filters.\n[1,5] base their theoretical results (as well as experimental validation) on concatenated ReLU (CReLU) whereas this submission assumes linear responses for theoretical analysis yet their experimental setup uses ReLU networks.\n\n\nHere is a list of overlaps between these two works:\nThe notation defined in Section 2.2 is identical as in [1] and [5].\nProposition 1 has identical proof outline as Proposition A.1 in appendix of [1]. In fact, Prop.1 is a special case of Prop.A.1 [1] with more stringent constraints on the weight matrix.\n[5] introduced the equivalent theoretical results and called it \u201cnorm preserving\u201d (see Theorem 2.2 in [5]) instead of the RIP used in this submission.\nThe idea behind the proof of reconstruction bounds and the reconstruction algorithm is very similar to that of [1] with a minor difference that they use weight matrix transpose instead of inverse. The \u201crelative error\u201d used for evaluation in this submission is termed as \u201creconstruction ratio\u201d in [1].\nThe coherence of the convolution filters (Table 4) is measured to estimate the level of orthogonality of the weight matrix as is done in [5] (see Table 1 and 2 in [5]) and came to the same conclusion that weight matrix is in fact incoherent.\n\n\nIn conclusion this submission adds minor tweaks under rather unrealistic assumptions and misleading empirical analysis -- which the author of [1,5] is morally against -- without notifying her or explaining that the ideas and the proofs are originated from her. ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (part 1)", "IS_META_REVIEW": false, "comments": "Summary:\nThe paper proposes to use Model-RIP, a simple concept from compressed sensing community, to theoretically describe a invertibility of a single convolution + max-pool operation (i.e., without non-linear activation function) assuming that the convolution filters are drawn from random Gaussian distribution. In the high level, they attempted to show that convolution weights consisting of random filters form a roughly orthogonal matrix, hence its transpose times the activation roughly reconstruct the input. Empirically, this submission tries to verify the model-RIP property of learned CNN filters by computing the norm ratio between the activation and the weight transpose multiplying the activation, and verify the reconstruction bounds by computing the relative errors between the input and the reconstructed input through the proposed reconstruction algorithm. Qualitatively, they also try to invert the activation to the previous layer output through the same algorithm then followed up by a learned decoder to reconstruct the raw input.\n\n\nHowever, there are many flaws in their assumptions but little useful consequence from enforcing CNNs to their mathematical model. The most evident flaws in their theorems are (1) the weight matrix of learned CNNs does not meet their model-RIP assumption (i.e., low coherence) and (2) even without the first flaw, their reconstruction bound doesn\u2019t hold analytically with ReLU non-linearity: RIP can only bound the norm of the transposed weights times the activation (see Eq. 1) instead of providing enough conditions for a metric-preserving property between input space and activation space [3] since ReLU zeroes out the negative activations, i.e., the activation can be arbitrarily close to zero. Furthermore, the presentations of their experimental results are very misleading and self-contradicting: a pre-trained decoder with switch units is responsible for most of the reconstruction and they only applied their proposed recon algorithm on one layer; the \u201crelative error\u201d does not give reasonable measurement of the reconstruction property, especially in the pixel space; the norm ratio ||W^Tc||/||c|| is also an inappropriate measurement for model-RIP condition, not to mention the discrepancy between the presented norm ratio and the learned filter\u2019s coherence level (if ||W^Tc||/||c|| is close to 1, coherence should be close to 0, but it is actually much larger than 0).  \n\n\n[1] W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In ICML, 2016.\u2028\n[2] S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training.\n[3] R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing, 2016.\u2028\n[4] A. Coates, and A. Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, 2015. \n[5] W. Shang. A Preliminary Study of the Norm Preservation Properties of CNN\n. In WiML Workshop, 2015. (", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "Summary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "While the reviewers found some interest in this work, I'm afraid I have to agree with the critique that the model studied is too simple that its relevance for deep learning is questionable.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Official Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.\n\nIn my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.\n\nA deep neural network is fundamentally different from a single layer---it is the \"deep\" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.\n\nFor any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThe authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing. They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT). They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.\n\nThis is a well-written paper that presents a new angle on why the current CNN architectures work so well. The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section. The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers. The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.\n\nA few aspects should be improved. First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well. A discussion of how this is inverted (e.g., with pooling switches) is needed.\n\nThe relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.\n\nAlthough it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).\n\nFinally, the filter coherence measure must be defined either mathematically or with a proper reference.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Inconsistent notations with DL, and room for improvements in the presentation and the theory  ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\n\n\nSummary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "single vs multiple layers", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "notations ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 8)", "IS_META_REVIEW": false, "comments": "Section 4 Continue:\nThe experiments in 4.5 are misleading and together with 4.4 they are contradictory to their theoretical results. \nForemost, the readers need to be aware that the decoders with switch units are very powerful (if you ever trained one you would know it too). Therefore what figure 4 presents is extremely misleading because it *only* takes the very last conv layer and use their proposed linear reconstruction algorithm to recover that layer\u2019s input only.  The rest was propagated with the very powerful decoder plus switch units information (see [6] for how powerful the decoder is).  Visually, we can also see how much info switch units carry: for their conv 5 reconstruction with random activation, there is no meaningful info carried by the activation values but all through switch units and we can still obtain the silhouette of the objects in the original images; in fact, pool-1\u2019s switch units alone carries at least 64x224x224 bit information. Their results back in the appendix (Figure5) also shows their reconstruction algorithm\u2019s weakness. The lower layer has extremely bad reconstruction quality from the proposed algorithms. The higher the layer, the better the reconstruction from their proposed algorithms because decoder together switch units has more chance to correct the reconstruction.What the authors should do, is to use their IHT to reconstruct all the way, following what has been done in [1]\u2019s linear reconstruction algorithm, which the authors here failed to compare against, despite its high relevance. In fact, the author of [1] used to be in charge of this submitted project and proposed this experimental approach, with a slightly different algorithm that is presented in [1]; but current authors of this submission failed in achieving similar reconstruction results with their algorithm and hence can only perform their reconstruction algorithm over a single layer. The biggest difference between the reconstruction algorithm from [1] and this submission is that the former used pseudo-inverse of the weight matrix and the latter used the transpose because in their theory, the weight matrix is approximate orthogonal hence its transpose is approximately its inverse.\n There is another serious issue with using high-up conv layer to verify their theoretical claim: recall that the excuse this submission used to discard ReLU non-linearity is that [1] introduced the pairing phenomenon, however, [1] stressed that it only appeared in the first few layers. Hence the authors should have at least chosen the first few layers for their experiments; yet, they knowingly only presented their analysis of the highest layer. \nTable 3 again presents very misleading results. First their \u201crandom activation\u201d has a relative error 1.414 for activation space. But if we simply pick zero vectors instead of \u201crandom\u201d, it gives 1 as a relative error, which is much smaller than 1.414. Therefore, this \u201crelative error\u201d in the activation space is a very inappropriate measurement for the reconstruction quality from their algorithm. But measuring the relative error in the input space is also a bad metric because of the powerful effects from the decoders. In fact, the term \u201crelative error\u201d is a paraphrase of the evaluation method used in [1], where it is called \u201creconstruction ratio\u201d. However, it is a much more appropriate measurement coupling the reconstruction algorithm used in [1] because the reconstruction from [1] will always be a subset of the original input by using the pseudo-inverse.\n Recall that in 4.4, the authors approximate the conv weight matrix for conv(5,2) has approximately 0.05-0.1 distortion constant, which means that the \u201crelative error\u201d based on their Theorem 3.3 is 0.6-0.2, but in reality, the relative error is 1.051, which is not only worse than their theoretical guarantee but even worse than simply choosing zero vector as reconstruction. Keep in mind that these results are built on top of the assumption that ReLU does not exist, if we add it on top, these results in Table3 and Figure5 can get further downgraded. One question why there is such a huge gap. In fact, such bad reconstruction results based on their algorithm is not surprising because after all, unlike random weights, the learned conv weights are not approximately orthogonal so Model-RIP does not fit into realistic CNN models. In fact, the coherence is a very direct measurement of how \u201corthogonal\u201d the learned weights are, which both this submission and [5] (again this submission took over [5]\u2019s idea) measure this quantity: the higher the coherence the less orthogonal.  \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 7)", "IS_META_REVIEW": false, "comments": "Section 4, experiments: This section contains so much misleading information to misguide the audience.\n The issues with 4.1 has been discussed in Section2(1).\nThe experiments in 4.4 are very deceiving.The minor misleading part is the title of Figure 3(b), ( c)  saying \u201cbefore/after ReLU\u201d, note the ReLU they refer to is at the input level not the *activation* level. In other words, they did not consider ReLU after the linear responses,  and only compare whether the input to this conv layer goes through ReLU or not, which does not add any difficulty to their theoretical analysis, because they assumed the input comes from the span of the conv weights, which is also [1] and [5]\u2019s assumption. However, the most serious issue is how they collect the results for Figure 3. I tried to replicate their result for conv(1,2) because the authors claim that \u201cwe choose conv(5, 2), while other layers show similar results\u201d. Precisely, I pass randomly sampled ImageNet Validation images through VGG-16, take the activation of conv(1,2) after max-pooling to obtain c, and then reconstruct conv(1,1) activation by multiplying c with the transpose of the conv weights W^T after subtracting the bias to obtain the reconstruction (W^T*c), and calculate the term, ||W^T c||/||c||.  However, the norm ratio I obtained is 3.16 +- 0.10, very far from their 1 +- 0.1.  My experiments give a distortion constant \\delta strictly bigger than 2, which is out of the range of 0 and 1 hence a meaningless value. This gap between our result and their result can potentially originate from the following causes: the authors using fancier sparse approximation method for c instead of following the procedures of feedforward CNNs, the authors discarding ReLU on the linear response therefore their c has negative components, and the authors have bugs in their code.  \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 6)", "IS_META_REVIEW": false, "comments": "Section 3 Continue:\nThere are again several issues with their proof to the reconstruction bounds. Lemma B.3 and B.4 carry out a very crude estimation on \\|c-h\\|, since the author discard any potential contribution of filters activated from max-pooling but belong to the set of original filters that form the input signal x. That is, \\|c_{\\Omega^c}\\| can potentially be covered by \\|h_{\\Pi - \\Omega}\\|. Intuitively, if max-pooling chooses to keep a shifted filter positive and the rest zero and if the shifts of filters are highly correlated, then even if the activated filter is not one of the original filters that constitute x, this activated filter still potentially cover x along the direction of the correct filter and therefore too much information can be lost without taking into such contribution into consideration. Moreover, the logic (i.e. proof technique) behind the reconstruction bounds (Theorem 3.3) is the same as that in Theorem A.5 from [1]. The idea is to decompose the l_2 norm of the reconstruction (or reconstruction signal) into \u201coriginal/ground truth\u201d +  \u201cdifference\u201d  and then bound the reconstruction ratio [1] (this term is renamed as \u201crelative error\u201d in this work) with the aid of the singular values of the weight matrix W. But again, the authors failed to address the similarity but rather attempt to present the proof as different as possible to create artificial novelty.  Finally, their final bound is very suboptimal and only consider 2 max-pooling regions. \n The entire section 3 is conducted under the assumption of linear responses. Because the compressed sensing model, Model-RIP, is fundamentally a linear model. The formulation (see equation1) only bounds the norm but not the metric. In other words, (1-\\delta)|x|<|Wx|<(1+\\delta)|x| is what the authors attempt to prove (but with flaws), i.e. a norm preserving property between the input and its linear responses\u2014this has been discussed in [5] but the authors did not cite or relate to this work\u2014despite [5] informs the authors to explain the similarity during their previous submission attempt. However, preserving linear norms do not give any reconstruction guarantee because of ReLU can theoretically ruin the upper bound, by, for example, zeroing all activations if they are all negative.\nthe authors cannot draw any conclusion from their theoretical results to any implication in training or network design, all they concluded is the last paragraph on page 14, which is rather meaningless.  \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 5)", "IS_META_REVIEW": false, "comments": "Section 3, Analysis: This section contains many contents that are either a copy from [1]&[5] or invalid.\nProposition 1 is a trivial special case of Proposition A.1 from [1], because the pseudo-inverse of an orthogonal matrix is simply its transpose, which is the set up in Prop 1. Also since Prop 1 assumes that \\Phi spans the entire input space, then range(W) easily becomes R^(M*D), however this assumption is generally invalid, which was empirically verified by Table S.1 from [1]\u2019s appendix for the case of deep CNNs for CIFAR-10/100. The authors clearly copied [1]\u2019s idea and decide to present Prop 1 since they know [1] very well, but they did not address this copying action. They should have cited [1]\u2019s Prop A.1, then present Porp 1 as a corollary and explain their set-up is a simplified version with no novel technicality. \n Following Proposition 1, the author cited [1] (the only time they mentioned it throughout this submission) and claims that based on [1]\u2019s result they can assume linear responses (\u201cassuming that all of the entries in the vectors are real numbers, rather than only non-negative.\u201d) However, [1] never claims that linear responses are acceptable (the authors of [1] in fact even did experiments with no non-linearity and the results significantly dropped) and [1] finds that only the first few convolution convolutional layers appear such pairing phenomenon. But in this submission, the experimental section focuses on the last convolutional layer of VGGNet, which by no means has such negative and positive pairing property. Therefore, their arguments of leaving out ReLU and assuming linear responses are groundless.\nThe authors fail to address convolution in their proof of Theorem 3.1 which resulting the proof to the convolution layer is no different from fully connected layer. More specifically, when proving theorem 3.1, the authors need Lemma A.1, however there is a severe issue with the proof of Lemma A.1\u2014it does not properly take convolution into consideration. Note that they assume the \u201cshift\u201d of the same convolution filter has expected dot product 0 (E((w_{I,m1}^j_1)T(w_{I,m1}^{j2})) = 0 if j_1 \\neq j_2, this assumption only holds if the convolution filters are of high frequency but it is a common knowledge that real convolution filters are more often smooth than highly oscillatory, and the dot products between shifts by a small number of pixels can be large, opposite from being 0 as the authors claim here. Their assumption contradicts the important fact that convoluted smooth filters stay highly correlated. Also, there is a constant C that is not tracked in Theorem 3.1 \u2014only a \u201cthere exists\u201d in a mathematical sense. Not to mention the whole proof is based on random filters which, we stress again, is far away from being realistic. \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 4)", "IS_META_REVIEW": false, "comments": "Section 2, Preliminaries:\nThere are serious issues with the random filter assumption (Section 2.1 and Section 4.1 are addressed together). First of all, we all know very well that random filters have extremely limited ability, especially with very deep networks. No one has in any way successfully applied random filters to deep networks. Second of all, their one-layer results in Table 1 is very off. For instance, [4] used one-layer unsupervised learning to train 1600 conv filters (this submission uses 2048 and 1024 conv filters) that achieved 77.9% accuracy on CIFAR-10, versus their 66% (random)/68% (learned), despite their architecture is far more complicated than [4], which ironically is far beyond the scope of their theoretical setup. But the most inappropriate component is the number of convolutional filters they used. None of the commonly used deep convolutional networks carry these many filters--for their first layer, there are about 30 times more filters than the dimension of its convolutional kernel! Recall that the most valuable part of convolutional layer is that it can use a small number of conv filters to extract abstract features by stacking many of them together. Unsurprisingly, as a consequence of using excessive amount of filters in order to span as much input space as possible, their models carry a huge number of parameters, an architecture design that is totally against the principles of CNNs. Finally, Table 4 shows, random filters can form a weight matrix that is a lot more \u201corthogonal\u201d than learned filters. And such low coherence, i.e. \u201corthogonal\u201d property of the weight matrix W, is the backbone of their Model-RIP. Thus some simple, unconvincing shallow networks do not manage to fill in the gap between random filters and learned filters, especially when this gap breaks their primary theoretical promise. \n Notation in 2.2: The notations introduced in 2.2 are identical to the notations and problem set up used in [1]&[5] except that in [1]&[5], M = 1, which does not change the theoretical fundamentals. In addition, the terminologies such as \u201cshift\u201d, \u201cblock\u201d are also shared between [1]&[5] and this submission.\nThere are serious issues with the Model-RIP setup: the model-RIP mathematical model is in fact nothing fancy. The only difference between model-RIP and a generic compressed sensing RIP model is that model-RIP requires model-sparsity. To be more straightforward, in this case, the model-sparsity corresponds with max-pooling\u2014meaning sparsity is structured so that each pooling region only has at most 1 non-zero element and correspondingly, the assumption on the data space is that it is a linear combination of filters such that at most one filter from each pooling region is used. Note that this is the same assumption as appeared in [1] (see Appendix, equation S1), in fact, this assumption was originally proposed by the first author of [1]. However, the fundamental flaw of this model-RIP setup are first the lack of non-linearity, notice that if ReLU is added after the linear response then the activation can be very close to zero which makes the reconstruction has norm arbitrarily small/close to zero; second real weight matrix W cannot meet the conditions that random matrix can meet, in other words, real W is not even close to being \u201corthogonal\u201d, even the authors give support along this direction (see Table 4). Hence the actual values of the reconstruction bounds in reality are far from being significant. \n(4)  IHT: This algorithm, again, does not take into any non-linearity on the activation into account.\n ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 3)", "IS_META_REVIEW": false, "comments": "Section 1, Introduction:\nA neural network that can be shown theoretically of high invertibility does not necessarily perform well. For example, ResNets over 34 layers, which are impossible to analyze using the proposed theoretical framework, perform significantly better than VGGNets with much fewer parameters but a lot more depth; by comparison, a standard variational autoencoder would have a lot worse generative power if we set the penalty for the prior KL-divergence term to be very small and encourage good reconstruction.\n \u201cDespite these interesting results, there is no clear theoretical explanations as to why CNNs are invertible yet\u201d. There are multiple issues with this statement. First of all, this invertibility is a rather trivial common knowledge among ML researchers; people don\u2019t like to analytically explore because its theoretical bounds are too bad\u2014as proved once again in this submission. Second of all, their theoretical proof does not consider non-linear layer and it does not have the correct convolutional setup (details to come at Section 3 (3)) and their theoretical bounds have no meaningful implications for training or network design. Third of all, there have been works exploring such signal recovery property with random filters on fully connected layers ([2] and [3]) as well as exploring such property with learned filters on convolutional layers ([1] and [5]). They all have very similar underlying mathematical models and highly resembling reconstruction algorithms ([1] and [2]) with this submission. The authors ignore addressing these important related works. Lastly, their analysis only tackles a single layer, which does not improve upon existing works, especially considering that [2] takes into account of multiple layers as well as dropout stability.\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (Part 2)", "IS_META_REVIEW": false, "comments": "The invertibility of CNNs has been explored fairly extensively in recent years both empirically [6,7,8,9] and theoretically [1,2,3]. The most critical problem of this submission is \u201cthe amount of overlap\u201d to [1,5] in terms of theoretical results as well as approaches for experimental validation. Before providing a list of overlapping contents, we point out the most fundamental differences between these two works:\nThis submission assumes random filters for their theoretical results whereas [1]&[5] impose assumptions on the learned filters.\n[1,5] base their theoretical results (as well as experimental validation) on concatenated ReLU (CReLU) whereas this submission assumes linear responses for theoretical analysis yet their experimental setup uses ReLU networks.\n\n\nHere is a list of overlaps between these two works:\nThe notation defined in Section 2.2 is identical as in [1] and [5].\nProposition 1 has identical proof outline as Proposition A.1 in appendix of [1]. In fact, Prop.1 is a special case of Prop.A.1 [1] with more stringent constraints on the weight matrix.\n[5] introduced the equivalent theoretical results and called it \u201cnorm preserving\u201d (see Theorem 2.2 in [5]) instead of the RIP used in this submission.\nThe idea behind the proof of reconstruction bounds and the reconstruction algorithm is very similar to that of [1] with a minor difference that they use weight matrix transpose instead of inverse. The \u201crelative error\u201d used for evaluation in this submission is termed as \u201creconstruction ratio\u201d in [1].\nThe coherence of the convolution filters (Table 4) is measured to estimate the level of orthogonality of the weight matrix as is done in [5] (see Table 1 and 2 in [5]) and came to the same conclusion that weight matrix is in fact incoherent.\n\n\nIn conclusion this submission adds minor tweaks under rather unrealistic assumptions and misleading empirical analysis -- which the author of [1,5] is morally against -- without notifying her or explaining that the ideas and the proofs are originated from her. ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "20 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "Overlapping with Existing Work (part 1)", "IS_META_REVIEW": false, "comments": "Summary:\nThe paper proposes to use Model-RIP, a simple concept from compressed sensing community, to theoretically describe a invertibility of a single convolution + max-pool operation (i.e., without non-linear activation function) assuming that the convolution filters are drawn from random Gaussian distribution. In the high level, they attempted to show that convolution weights consisting of random filters form a roughly orthogonal matrix, hence its transpose times the activation roughly reconstruct the input. Empirically, this submission tries to verify the model-RIP property of learned CNN filters by computing the norm ratio between the activation and the weight transpose multiplying the activation, and verify the reconstruction bounds by computing the relative errors between the input and the reconstructed input through the proposed reconstruction algorithm. Qualitatively, they also try to invert the activation to the previous layer output through the same algorithm then followed up by a learned decoder to reconstruct the raw input.\n\n\nHowever, there are many flaws in their assumptions but little useful consequence from enforcing CNNs to their mathematical model. The most evident flaws in their theorems are (1) the weight matrix of learned CNNs does not meet their model-RIP assumption (i.e., low coherence) and (2) even without the first flaw, their reconstruction bound doesn\u2019t hold analytically with ReLU non-linearity: RIP can only bound the norm of the transposed weights times the activation (see Eq. 1) instead of providing enough conditions for a metric-preserving property between input space and activation space [3] since ReLU zeroes out the negative activations, i.e., the activation can be arbitrarily close to zero. Furthermore, the presentations of their experimental results are very misleading and self-contradicting: a pre-trained decoder with switch units is responsible for most of the reconstruction and they only applied their proposed recon algorithm on one layer; the \u201crelative error\u201d does not give reasonable measurement of the reconstruction property, especially in the pixel space; the norm ratio ||W^Tc||/||c|| is also an inappropriate measurement for model-RIP condition, not to mention the discrepancy between the presented norm ratio and the learned filter\u2019s coherence level (if ||W^Tc||/||c|| is close to 1, coherence should be close to 0, but it is actually much larger than 0).  \n\n\n[1] W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In ICML, 2016.\u2028\n[2] S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training.\n[3] R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing, 2016.\u2028\n[4] A. Coates, and A. Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, 2015. \n[5] W. Shang. A Preliminary Study of the Norm Preservation Properties of CNN\n. In WiML Workshop, 2015. (", "OTHER_KEYS": "(anonymous)"}], "authors": "Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee", "accepted": false, "id": "674"}