{"conference": "ICLR 2017 conference submission", "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain", "abstract": "Transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a mixture of experts framework to combine learnt policies to improve multi-task learning while avoiding negative transfer. The key to their approach is a soft attention mechanism, learnt with RL, which enables positive transfer. They give limited empirical validation of their approach. This is a general attention method which can be applied for policy gradient or value iteration learning methods. The authors were very responsive and added additional experiments based on the reviews.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017 (modified: 18 Jan 2017)", "TITLE": "Summary of Revisions ", "IS_META_REVIEW": false, "comments": "1) Appendix H - Visualization of Attention Weight Evolution - (Asked by AnonReviewer4)\n2) Appendix I - Partial Expert Positive Source Task A2T experiment - (Asked by AnonReviewer3 WRT base network learning complementary skills and AnonReviewer5 pointing out a con that experiments relied on source tasks containing the optimal policy for the target task.)\n3) Appendix J - Sparse Pong Target Task Added - (Asked by AnonReviewer5 for illustrating transfer learning in a case where the target task performance is limited by data availability)\n4) Appendix A - Revised to give precise details of network architecture (Asked by AnonReviewer5 specifically WRT comparing # of parameters in A2T vs Learning from scratch when source tasks contain an optimal policy for the target task, where it is better if there are fewer parameters for A2T). \n5) Mentioned in the Conclusion Section about extensions of A2T to HRL and Lifelong Learning. (Pointed out by AnonReviewer5)\n", "OTHER_KEYS": "Aravind Lakshminarayanan"}, {"TITLE": "Final Review: Learned convex combination of many fixed and a jointly learned expert is used to represent action policies in proof-of-concept, transfer/hierarchical RL, settings.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "02 Jan 2017 (modified: 25 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer. The proposed approach is based on a convex combination of existing solutions and the being-learned solution to the novel task. The non-negative weight of each solution implies that the solution of negative effect is ignored and more weights are allocated to more relevant solution in each state. This paper derives this so-called \"A2T\" learning algorithm for policy transfer and value transfer for REINFORCE and ACTOR-CRITIC algorithms and experiments with synthetic Chain World and Puddle World simulation and Atari 2600 game Pong. \n+This paper presents a novel approach for transfer reinforcement learning.\n+The experiments are cleverly designed to demonstrate the ability of the proposed method.\n-An important aspect of transfer learning is that the algorithm can automatically figure out if the existing solutions to known tasks are sufficient to solve the novel task so that it can save the time and energy of learning-from-scratch. This issue is not studied in this paper as most of experiments have a learning-from-scratch solution as base network. It will be interesting to see how well the algorithm performs without base network. In addition, from Figure 3, 5 and 6, the proposed algorithm seems to accelerate the learning speed, but the overall network seems not better than the solo base network. It will be more convincing to show some example that existing solutions are complementary to the base network.\n-If ignoring the base network, the proposed network can be considered as ensemble reinforcement learning that take advantages of learned agents with different expertise to solve the novel task. ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper tackles important problems in multi-task reinforcement learning: avoid negative transfer and allow finer selective transfer. The method is based on soft attention mechanism, very general, and demonstrated to be applicable in both policy gradient and value iteration methods. The introduction of base network allows learning new policy if the prior policies aren't directly applicable. State-dependent sub policy selection allows finer control and can be thought of assigning state space to different sub policies/experts. The tasks are relatively simplistic but sufficient to demonstrate the benefits. One limitation is that the method is simple and the results/claims are mostly empirical. It would be interesting to see extensions to option-based framework, stochastic hard attention mechanism, sub-policy pruning, progressive networks. \n\nIn figure 6, the read curve seems to perform worse than the rest in terms of final performance. Perhaps alternative information to put with figures is the attention mask activation statistics during learning, so that we may observe that it learns to turn off adversarial sub-policies and rely on newly learned base policy mostly. This is also generally good to check to see if any weird co-adaptation is happening. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "details on re-training during transfer", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IS_META_REVIEW": true, "comments": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a mixture of experts framework to combine learnt policies to improve multi-task learning while avoiding negative transfer. The key to their approach is a soft attention mechanism, learnt with RL, which enables positive transfer. They give limited empirical validation of their approach. This is a general attention method which can be applied for policy gradient or value iteration learning methods. The authors were very responsive and added additional experiments based on the reviews.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017 (modified: 18 Jan 2017)", "TITLE": "Summary of Revisions ", "IS_META_REVIEW": false, "comments": "1) Appendix H - Visualization of Attention Weight Evolution - (Asked by AnonReviewer4)\n2) Appendix I - Partial Expert Positive Source Task A2T experiment - (Asked by AnonReviewer3 WRT base network learning complementary skills and AnonReviewer5 pointing out a con that experiments relied on source tasks containing the optimal policy for the target task.)\n3) Appendix J - Sparse Pong Target Task Added - (Asked by AnonReviewer5 for illustrating transfer learning in a case where the target task performance is limited by data availability)\n4) Appendix A - Revised to give precise details of network architecture (Asked by AnonReviewer5 specifically WRT comparing # of parameters in A2T vs Learning from scratch when source tasks contain an optimal policy for the target task, where it is better if there are fewer parameters for A2T). \n5) Mentioned in the Conclusion Section about extensions of A2T to HRL and Lifelong Learning. (Pointed out by AnonReviewer5)\n", "OTHER_KEYS": "Aravind Lakshminarayanan"}, {"TITLE": "Final Review: Learned convex combination of many fixed and a jointly learned expert is used to represent action policies in proof-of-concept, transfer/hierarchical RL, settings.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "02 Jan 2017 (modified: 25 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer. The proposed approach is based on a convex combination of existing solutions and the being-learned solution to the novel task. The non-negative weight of each solution implies that the solution of negative effect is ignored and more weights are allocated to more relevant solution in each state. This paper derives this so-called \"A2T\" learning algorithm for policy transfer and value transfer for REINFORCE and ACTOR-CRITIC algorithms and experiments with synthetic Chain World and Puddle World simulation and Atari 2600 game Pong. \n+This paper presents a novel approach for transfer reinforcement learning.\n+The experiments are cleverly designed to demonstrate the ability of the proposed method.\n-An important aspect of transfer learning is that the algorithm can automatically figure out if the existing solutions to known tasks are sufficient to solve the novel task so that it can save the time and energy of learning-from-scratch. This issue is not studied in this paper as most of experiments have a learning-from-scratch solution as base network. It will be interesting to see how well the algorithm performs without base network. In addition, from Figure 3, 5 and 6, the proposed algorithm seems to accelerate the learning speed, but the overall network seems not better than the solo base network. It will be more convincing to show some example that existing solutions are complementary to the base network.\n-If ignoring the base network, the proposed network can be considered as ensemble reinforcement learning that take advantages of learned agents with different expertise to solve the novel task. ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper tackles important problems in multi-task reinforcement learning: avoid negative transfer and allow finer selective transfer. The method is based on soft attention mechanism, very general, and demonstrated to be applicable in both policy gradient and value iteration methods. The introduction of base network allows learning new policy if the prior policies aren't directly applicable. State-dependent sub policy selection allows finer control and can be thought of assigning state space to different sub policies/experts. The tasks are relatively simplistic but sufficient to demonstrate the benefits. One limitation is that the method is simple and the results/claims are mostly empirical. It would be interesting to see extensions to option-based framework, stochastic hard attention mechanism, sub-policy pruning, progressive networks. \n\nIn figure 6, the read curve seems to perform worse than the rest in terms of final performance. Perhaps alternative information to put with figures is the attention mask activation statistics during learning, so that we may observe that it learns to turn off adversarial sub-policies and rely on newly learned base policy mostly. This is also generally good to check to see if any weird co-adaptation is happening. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "details on re-training during transfer", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}], "authors": "Janarthanan Rajendran, Aravind Lakshminarayanan, Mitesh M. Khapra, Prasanna P, Balaraman Ravindran", "accepted": true, "id": "407"}