{"conference": "ICLR 2017 conference submission", "title": "Deep Learning with Sets and Point Clouds", "abstract": "We introduce a simple permutation equivariant layer for deep learning with set structure. This type of layer, obtained by parameter-sharing, has a simple implementation and linear-time complexity in the size of each set. We use deep permutation-invariant networks to perform point-could classification and MNIST digit summation, where in both cases the output is invariant to permutations of the input. In a semi-supervised setting, where the goal is make predictions for each instance within a set, we demonstrate the usefulness of this type of layer in set-outlier detection as well as semi-supervised learning with clustering side-information.", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.\n \n The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant. \n \n In light of these reviews, the AC invites their work to the workshop track. \n Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.\n \n As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 (", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Major Revision", "IS_META_REVIEW": false, "comments": "We thank all reviewers for their comments! A commonl issue in reviews was regarding the disconnect between our general treatment of invariance and experimental results. The other major issue was regarding the clarity of the first part. \n\nTo resolve both issues we have removed our general treatment and focused on \"deep learning with sets and point-clouds\", incorporating all reviewer feedback on this part. We welcome any further comments on the revised version.", "OTHER_KEYS": "Siamak Ravanbakhsh"}, {"TITLE": "Excellent choice of topic, but more work is needed for the results to be actionable.", "OTHER_KEYS": "Andrew William Wagner", "comments": "Pros:\n* Part of the paper addresses an industrially important topic, namely how to make deep networks work properly on point clouds, i.e. in many (most?) potential applications they should be invariant to permutations of the points within the cloud, as well as rigid transformations of the cloud (depends on the application).\n* The authors propose a formalism for dealing with compositions of different kinds of invariance.\n\nCons:\n* For me the explanation of the generalization is really hard to follow. For me, the paper would be stronger if were less broad, but went into more depth for the permutation-invariance case.\n* It is very easy to sit down and come up with network structures that are permutation invariant. It seems the author tried a few networks in the family (a few different point cloud sizes, a couple options for the number of parameters, averaging vs. max in the set, dropout vs. no dropout), but unless the space is more completely and systematically explored, there's not much reason for a practitioner to use the proposed structure vs. some other random structure they cook up that is also permutation invariant. i.e. what about just using a FC layer that is shared between the points instead of your three \"set invariant\" layers? Seems simpler, more general, and also permutation invariant...\n* It is not clear to me how valuable the author's definition of \"minimally invariant\" is. Is a sufficiently large composition of \"set invariant\" layers a universal approximator for permutation invariant functions?\n* I'm concerned that proposed \"set invariant layer\" might be strongly variant to spatial transformations, as well as vulnerable to large outliers. In particular there is a term that subtracts a corner of the clouds bounding box (i.e. the max over set operator inside the first layer), before the cloud goes through a learned affine transform and pixelwise nonlinearity. Seems like that could saturate the whole network...\n\nI'm reviewing with low confidence, because there's a chance the formalism in the first part of the paper is more valuable than I realize; I haven't fully understood it. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "05 Jan 2017", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "A promising work!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Pros : \n- New and clear formalism for invariance on signals with known structure\n- Good numerical results\n\nCons :\n- The structure must be specified.\n- The set structure dataset is too simple\n- There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing\n\nBesides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.  Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that's an issue I had to deal with at some points)", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting formalization of invariance in neural networks, but too abstract and weak experimental results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThis paper discusses ways to enforce invariance in neural networks using weight sharing.  The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a \u201cset-invariant\u201d function, which is used in an anomaly detection setting and a point cloud classification problem.  \n\n\u201cInvariance\u201d is, at a high level, an important issue of course, since we don\u2019t want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper.  However, there are a few weaknesses that I feel prevent this from being a strong submission.  First, the exposition is too abstract and this paper could really use a running and *concrete* example starting from the very beginning.\n\nSecond, \u201cset invariance\u201d, which is the main type of invariance studied in the paper is defined via the author\u2019s formalization of invariance, but is never explicitly related to what I might think of as \u201cset invariance\u201d \u2014 e.g. to permutations of input or output dimensions.  Explicitly defining set invariance in some other way, then relating it to the  \u201cstructural invariance\u201d formulation may be a better way to explain things.  It is never made clear, for example, why Figure 1(b) is *the* set data-structure.\n\nI like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?).  But the authors have ignored the other kind of compositionality that is important to neural networks \u2014 specifically that relating the proposed notion of invariance to function composition seems important \u2014 i.e. under what conditions do compositions of invariant functions remain invariant?  And  It is clear to me that just by having one layer of invariance in a network doesn\u2019t make the entire network invariant, for example.  So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is \u201cset invariant\u201d in some sense?  \n\nRegarding experiments, there are no baselines presented for anomaly detection.  Baselines *are* presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed.  (I should say that I don\u2019t know enough about the dataset to say whether these are exactly fair comparisons or not).  It is also never really made clear why set invariance is a desirable property for a point cloud classification setting.  As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.  Also, what about classical set kernels?\n\nOther random things:\n* Example 2.2: Shouldn\u2019t |S|=5 in the case of left-right and up-down symmetry?\n* \u201cParameters shared within a relation\u201d is vague and undefined.\n* Why is \u201cset convolution\u201d called \u201cset convolution\u201d in the appendix?  What is convolutional about it?\n* Is there a relationship to symmetric function theory?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting topic, but hard to follow for a non-expert ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. \nI have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.\n\nI would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. \n\nI therefore would like to provide feedback only at the level of presentation. \n\nMy main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by \"revelation\" rather than by explaining how they connect to the previous concepts. \n\nThe one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. \nIt is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. \nGoing from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.\n\nWhy is the 3x3 convolution associated to 9 relations? \nAre these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? \nWhat is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). \nAre we talking about separable filters? \nWhat are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?\n\nI realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. \n\nClearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. \n\nI would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 1}, {"DATE": "02 Dec 2016", "TITLE": "Set structure questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.\n \n The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant. \n \n In light of these reviews, the AC invites their work to the workshop track. \n Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.\n \n As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 (", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Major Revision", "IS_META_REVIEW": false, "comments": "We thank all reviewers for their comments! A commonl issue in reviews was regarding the disconnect between our general treatment of invariance and experimental results. The other major issue was regarding the clarity of the first part. \n\nTo resolve both issues we have removed our general treatment and focused on \"deep learning with sets and point-clouds\", incorporating all reviewer feedback on this part. We welcome any further comments on the revised version.", "OTHER_KEYS": "Siamak Ravanbakhsh"}, {"TITLE": "Excellent choice of topic, but more work is needed for the results to be actionable.", "OTHER_KEYS": "Andrew William Wagner", "comments": "Pros:\n* Part of the paper addresses an industrially important topic, namely how to make deep networks work properly on point clouds, i.e. in many (most?) potential applications they should be invariant to permutations of the points within the cloud, as well as rigid transformations of the cloud (depends on the application).\n* The authors propose a formalism for dealing with compositions of different kinds of invariance.\n\nCons:\n* For me the explanation of the generalization is really hard to follow. For me, the paper would be stronger if were less broad, but went into more depth for the permutation-invariance case.\n* It is very easy to sit down and come up with network structures that are permutation invariant. It seems the author tried a few networks in the family (a few different point cloud sizes, a couple options for the number of parameters, averaging vs. max in the set, dropout vs. no dropout), but unless the space is more completely and systematically explored, there's not much reason for a practitioner to use the proposed structure vs. some other random structure they cook up that is also permutation invariant. i.e. what about just using a FC layer that is shared between the points instead of your three \"set invariant\" layers? Seems simpler, more general, and also permutation invariant...\n* It is not clear to me how valuable the author's definition of \"minimally invariant\" is. Is a sufficiently large composition of \"set invariant\" layers a universal approximator for permutation invariant functions?\n* I'm concerned that proposed \"set invariant layer\" might be strongly variant to spatial transformations, as well as vulnerable to large outliers. In particular there is a term that subtracts a corner of the clouds bounding box (i.e. the max over set operator inside the first layer), before the cloud goes through a learned affine transform and pixelwise nonlinearity. Seems like that could saturate the whole network...\n\nI'm reviewing with low confidence, because there's a chance the formalism in the first part of the paper is more valuable than I realize; I haven't fully understood it. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "05 Jan 2017", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "A promising work!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Pros : \n- New and clear formalism for invariance on signals with known structure\n- Good numerical results\n\nCons :\n- The structure must be specified.\n- The set structure dataset is too simple\n- There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing\n\nBesides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.  Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that's an issue I had to deal with at some points)", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting formalization of invariance in neural networks, but too abstract and weak experimental results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThis paper discusses ways to enforce invariance in neural networks using weight sharing.  The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a \u201cset-invariant\u201d function, which is used in an anomaly detection setting and a point cloud classification problem.  \n\n\u201cInvariance\u201d is, at a high level, an important issue of course, since we don\u2019t want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper.  However, there are a few weaknesses that I feel prevent this from being a strong submission.  First, the exposition is too abstract and this paper could really use a running and *concrete* example starting from the very beginning.\n\nSecond, \u201cset invariance\u201d, which is the main type of invariance studied in the paper is defined via the author\u2019s formalization of invariance, but is never explicitly related to what I might think of as \u201cset invariance\u201d \u2014 e.g. to permutations of input or output dimensions.  Explicitly defining set invariance in some other way, then relating it to the  \u201cstructural invariance\u201d formulation may be a better way to explain things.  It is never made clear, for example, why Figure 1(b) is *the* set data-structure.\n\nI like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?).  But the authors have ignored the other kind of compositionality that is important to neural networks \u2014 specifically that relating the proposed notion of invariance to function composition seems important \u2014 i.e. under what conditions do compositions of invariant functions remain invariant?  And  It is clear to me that just by having one layer of invariance in a network doesn\u2019t make the entire network invariant, for example.  So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is \u201cset invariant\u201d in some sense?  \n\nRegarding experiments, there are no baselines presented for anomaly detection.  Baselines *are* presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed.  (I should say that I don\u2019t know enough about the dataset to say whether these are exactly fair comparisons or not).  It is also never really made clear why set invariance is a desirable property for a point cloud classification setting.  As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.  Also, what about classical set kernels?\n\nOther random things:\n* Example 2.2: Shouldn\u2019t |S|=5 in the case of left-right and up-down symmetry?\n* \u201cParameters shared within a relation\u201d is vague and undefined.\n* Why is \u201cset convolution\u201d called \u201cset convolution\u201d in the appendix?  What is convolutional about it?\n* Is there a relationship to symmetric function theory?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting topic, but hard to follow for a non-expert ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. \nI have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.\n\nI would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. \n\nI therefore would like to provide feedback only at the level of presentation. \n\nMy main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by \"revelation\" rather than by explaining how they connect to the previous concepts. \n\nThe one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. \nIt is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. \nGoing from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.\n\nWhy is the 3x3 convolution associated to 9 relations? \nAre these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? \nWhat is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). \nAre we talking about separable filters? \nWhat are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?\n\nI realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. \n\nClearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. \n\nI would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 1}, {"DATE": "02 Dec 2016", "TITLE": "Set structure questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Siamak Ravanbakhsh, Jeff Schneider, Barnabas Poczos", "accepted": false, "id": "514"}