{"conference": "ICLR 2017 conference submission", "title": "Demystifying ResNet", "abstract": "We provide a theoretical explanation for the superb performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1-shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "not ready yet", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "this paper has nice observation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. \n\nThis result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:\n\n1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?\n\n2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.\n\n3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?\n\nMinor: \n- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error\n- sec 4.1 what do you mean by \"zero initialization with small random perturbations\"? why not exactly zero initialization, how large is the random perturbation?\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "This paper tries to explain the reason behind the success of ResNet architecture through theoretical arguments and experimental results. Unfortunately, theoretical arguments and experimental results are not enough to support the main claims of the paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "06 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Linear vs non-linear, are they really similar?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "preliminary questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "22 Nov 2016", "TITLE": "Definition of condition number", "IS_META_REVIEW": false, "comments": "Hi Authors,\n\nI went through your paper in detail and it was very good linking the Hessian at the zero initial point to the success of the ResNet. However, I am a little unsure of the definition of the condition number in the paper. Across several standard refernces, the condition number is defined as the ratio the maximum and minimum singular values where as you defined it as $ |\\lambda|_max / |\\lambda|_min  $.  Unless the matrix, which is the Hessian H in your case, is positive semi-definite, these two are not the same and the spectra of the singular values and the eigen values can be quite different in general. Especially, for n=2, the initial point zero is a saddle point as stated in your theorem and as such the Hessian is not positive-definite. Please let me know if I am missing out on something and whether or not this modification affects the analysis in the paper. Thanks!", "OTHER_KEYS": "Ashok Vardhan Makkuva"}, {"IS_META_REVIEW": true, "comments": "ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "not ready yet", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "this paper has nice observation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. \n\nThis result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:\n\n1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?\n\n2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.\n\n3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?\n\nMinor: \n- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error\n- sec 4.1 what do you mean by \"zero initialization with small random perturbations\"? why not exactly zero initialization, how large is the random perturbation?\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "This paper tries to explain the reason behind the success of ResNet architecture through theoretical arguments and experimental results. Unfortunately, theoretical arguments and experimental results are not enough to support the main claims of the paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "06 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Linear vs non-linear, are they really similar?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "preliminary questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "22 Nov 2016", "TITLE": "Definition of condition number", "IS_META_REVIEW": false, "comments": "Hi Authors,\n\nI went through your paper in detail and it was very good linking the Hessian at the zero initial point to the success of the ResNet. However, I am a little unsure of the definition of the condition number in the paper. Across several standard refernces, the condition number is defined as the ratio the maximum and minimum singular values where as you defined it as $ |\\lambda|_max / |\\lambda|_min  $.  Unless the matrix, which is the Hessian H in your case, is positive semi-definite, these two are not the same and the spectra of the singular values and the eigen values can be quite different in general. Especially, for n=2, the initial point zero is a saddle point as stated in your theorem and as such the Hessian is not positive-definite. Please let me know if I am missing out on something and whether or not this modification affects the analysis in the paper. Thanks!", "OTHER_KEYS": "Ashok Vardhan Makkuva"}], "authors": "Sihan Li, Jiantao Jiao, Yanjun Han, Tsachy Weissman", "accepted": false, "id": "755"}