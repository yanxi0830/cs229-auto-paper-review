{"conference": "ICLR 2017 conference submission", "title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally.  In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors. Our approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper's clarity and experimental evaluation can be improved.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Good idea but not enough to convince", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.\n\nOn balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.\n\nDetailed/minor points below:\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n2) The tables need better and more descriptive labels.\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Difficult to understand", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting idea, unconvincing experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "Semantic vs syntactic analogy performance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper's clarity and experimental evaluation can be improved.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Good idea but not enough to convince", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.\n\nOn balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.\n\nDetailed/minor points below:\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n2) The tables need better and more descriptive labels.\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Difficult to understand", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting idea, unconvincing experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "Semantic vs syntactic analogy performance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Yuanzhi Ke, Masafumi Hagiwara", "accepted": false, "id": "771"}