{"conference": "ICLR 2017 conference submission", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Paper Strengths: \n-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner\n\n-- The effective batch size for training the MoE drastically increased also\n\n-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.\n\n\nPaper Weaknesses:\n\n--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper uses mixtures of experts to increase the capacity of deep networks, and describes the implementation of such a model on a cluster of GPUs. The proposed mixture model achieves strong performances in language modeling and machine translation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their instructive feedback. We made a significant effort to improve our work both in terms of presentation and content to reflect the\nreviewers' suggestions. We believe our latest draft clarifies in detail our contributions with respect to the previous state of the field (e.g., conditional computation).\n\nThe main idea of our paper can be summarized as this:  Massively increasing the capacity of deep networks by employing efficient, general-purpose conditional computation.  This idea seems hugely promising and hugely obvious.  At first glance, it is utterly shocking that no one had successfully implemented it prior to us.  In practice, however, there are major challenges in achieving high performance and high quality.  We enumerate these challenges in the introduction section of our new draft.  Our paper discusses how other authors have attacked these challenges, as well as our particular solutions.   \n\nWhile some of our particular solutions (e.g., noisy-top-k gating, the particular batching schemes, the load-balancing loss, even the mixture-of-experts formalism) may not withstand the test of time, our main contribution, which is larger than these particulars, is to prove by example that efficient, general-purpose conditional computation in deep networks is possible and very beneficial. As such, this is likely a seminal paper in the field.\n\nOur apologies for not making this clearer in our first drafts. Please re-read our paper with this in mind, and consider updating your reviews accordingly, if appropriate.\n\nIn addition to the major changes described above, we have made several other improvements:\n - We added experimental tests of our balancing losses (see Appendix A).\n - We added computational efficiency metrics (TFLOPS/GPU) to our language modeling experiments (see Tables 1, 7 and 8).\n - We added a set of language modeling experiments on a 100 billion word corpus, using MoE models with up to 137 billion parameters. These demonstrate major quality improvements and good computational efficiency up to 68 billion parameters (see Section 5.2).\n - We added a set of experiments on learning a multilingual machine translation model, showing very large improvements over recently published results (see Table 5).\n - We moved some of the less important content to appendices, bringing the paper length down to 9 pages.\n ", "OTHER_KEYS": "Azalia Mirhoseini"}, {"DATE": "05 Jan 2017", "TITLE": "From an interested reader: I would give this at least a 9 rating.", "IS_META_REVIEW": false, "comments": "I read the paper and I feel the ratings are too low. The authors introduce a general-purpose mechanism to scale up neural networks significantly beyond their current size using sparsity of activation, i.e. by forcing the activation of most neurons in the net to be zero for any given training example.\n\nFirstly, I believe the sheer size of the models successfully trained in this paper warrant an 8 rating all by themselves.\n\nSecondly, we know historically that sparsity of parameters is among the most important modelling principles in machine learning, being used with great success in e.g. Lasso with the l1 penalty, in SVM with the hinge loss and in ConvNets by setting connections outside the receptive field to zero. This paper, in addition to sparsity of parameters (neurons in different experts are not connected) employs sparsity of activation, where the computation path is customized for each training example. It is, as far as I can tell, the first paper to implement this in a practical, scalable and general way for neural networks. If sparsity of activation turns out to be even a small fraction as important as sparsity of parameters, this paper will have a major impact.\n\nThirdly, I love the computational efficiency of the model presented. The authors achieve extreme sparsity yet fully utilize their GPUs. In particular, the authors design the network in such a way that there are very few connections between active and non-active units. If we have, say, a sparsely activated fully-connected network, most computation would be wasted on network connections that start on active units and end on non-active units.\n\nFourthly, the authors discuss and provide a practical and elegant strategy for large-scale cluster implementation, showcasing their technical sophistication.\n\nIt is perhaps unfortunate that current baseline datasets may not even be able to fully utilize the power of MoE or other to-be-designed networks following similar principles, but models like the one presented here are bound to only become more prominent in the future.\n\nI would rate this paper at least 9.\n\n\n", "OTHER_KEYS": "George Philipp"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). \nAdditionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs.\nExperiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.\n\nI have the several comments on the paper:\n- I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. \n- The paper is missing some important references in conditional computation (e.g. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "26 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Elegant use of MoE for expanding model capacity, but it would be very nice to discuss MoE alternatives in terms of computational efficiency and other factors.", "comments": "Paper Strengths: \n-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner\n\n-- The effective batch size for training the MoE drastically increased also\n\n-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.\n\n\nPaper Weaknesses:\n\n--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice use of MoE with good results", "comments": "This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.\n\nExperiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.\n\nAn area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.\n\nOverall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.\n\n\n\nSmall comment:\nI like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "Difference between Importance and Load", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Multilayer mapping net baseline?", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"TITLE": "Use of huge Mixture-of-Experts ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "Paper Strengths: \n-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner\n\n-- The effective batch size for training the MoE drastically increased also\n\n-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.\n\n\nPaper Weaknesses:\n\n--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper uses mixtures of experts to increase the capacity of deep networks, and describes the implementation of such a model on a cluster of GPUs. The proposed mixture model achieves strong performances in language modeling and machine translation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their instructive feedback. We made a significant effort to improve our work both in terms of presentation and content to reflect the\nreviewers' suggestions. We believe our latest draft clarifies in detail our contributions with respect to the previous state of the field (e.g., conditional computation).\n\nThe main idea of our paper can be summarized as this:  Massively increasing the capacity of deep networks by employing efficient, general-purpose conditional computation.  This idea seems hugely promising and hugely obvious.  At first glance, it is utterly shocking that no one had successfully implemented it prior to us.  In practice, however, there are major challenges in achieving high performance and high quality.  We enumerate these challenges in the introduction section of our new draft.  Our paper discusses how other authors have attacked these challenges, as well as our particular solutions.   \n\nWhile some of our particular solutions (e.g., noisy-top-k gating, the particular batching schemes, the load-balancing loss, even the mixture-of-experts formalism) may not withstand the test of time, our main contribution, which is larger than these particulars, is to prove by example that efficient, general-purpose conditional computation in deep networks is possible and very beneficial. As such, this is likely a seminal paper in the field.\n\nOur apologies for not making this clearer in our first drafts. Please re-read our paper with this in mind, and consider updating your reviews accordingly, if appropriate.\n\nIn addition to the major changes described above, we have made several other improvements:\n - We added experimental tests of our balancing losses (see Appendix A).\n - We added computational efficiency metrics (TFLOPS/GPU) to our language modeling experiments (see Tables 1, 7 and 8).\n - We added a set of language modeling experiments on a 100 billion word corpus, using MoE models with up to 137 billion parameters. These demonstrate major quality improvements and good computational efficiency up to 68 billion parameters (see Section 5.2).\n - We added a set of experiments on learning a multilingual machine translation model, showing very large improvements over recently published results (see Table 5).\n - We moved some of the less important content to appendices, bringing the paper length down to 9 pages.\n ", "OTHER_KEYS": "Azalia Mirhoseini"}, {"DATE": "05 Jan 2017", "TITLE": "From an interested reader: I would give this at least a 9 rating.", "IS_META_REVIEW": false, "comments": "I read the paper and I feel the ratings are too low. The authors introduce a general-purpose mechanism to scale up neural networks significantly beyond their current size using sparsity of activation, i.e. by forcing the activation of most neurons in the net to be zero for any given training example.\n\nFirstly, I believe the sheer size of the models successfully trained in this paper warrant an 8 rating all by themselves.\n\nSecondly, we know historically that sparsity of parameters is among the most important modelling principles in machine learning, being used with great success in e.g. Lasso with the l1 penalty, in SVM with the hinge loss and in ConvNets by setting connections outside the receptive field to zero. This paper, in addition to sparsity of parameters (neurons in different experts are not connected) employs sparsity of activation, where the computation path is customized for each training example. It is, as far as I can tell, the first paper to implement this in a practical, scalable and general way for neural networks. If sparsity of activation turns out to be even a small fraction as important as sparsity of parameters, this paper will have a major impact.\n\nThirdly, I love the computational efficiency of the model presented. The authors achieve extreme sparsity yet fully utilize their GPUs. In particular, the authors design the network in such a way that there are very few connections between active and non-active units. If we have, say, a sparsely activated fully-connected network, most computation would be wasted on network connections that start on active units and end on non-active units.\n\nFourthly, the authors discuss and provide a practical and elegant strategy for large-scale cluster implementation, showcasing their technical sophistication.\n\nIt is perhaps unfortunate that current baseline datasets may not even be able to fully utilize the power of MoE or other to-be-designed networks following similar principles, but models like the one presented here are bound to only become more prominent in the future.\n\nI would rate this paper at least 9.\n\n\n", "OTHER_KEYS": "George Philipp"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). \nAdditionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs.\nExperiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.\n\nI have the several comments on the paper:\n- I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. \n- The paper is missing some important references in conditional computation (e.g. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "26 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Elegant use of MoE for expanding model capacity, but it would be very nice to discuss MoE alternatives in terms of computational efficiency and other factors.", "comments": "Paper Strengths: \n-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner\n\n-- The effective batch size for training the MoE drastically increased also\n\n-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.\n\n\nPaper Weaknesses:\n\n--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice use of MoE with good results", "comments": "This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.\n\nExperiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.\n\nAn area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.\n\nOverall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.\n\n\n\nSmall comment:\nI like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "Difference between Importance and Load", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IMPACT": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Multilayer mapping net baseline?", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"TITLE": "Use of huge Mixture-of-Experts ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean", "accepted": true, "id": "399"}