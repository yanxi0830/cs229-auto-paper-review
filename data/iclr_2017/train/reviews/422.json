{"conference": "ICLR 2017 conference submission", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper provides a clear application of variational methods for learning non-linear state-space models, which is of increasing interest, and of general relevance to the community.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Comment to all reviewers", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nThank you for your kind and constructive feedback. According to your reviews, our paper is \u201cwell-written\u201d and \u201cwell-motivated\u201d. We agree that applying SGVB to identify and filter latent state-space models and \u201c[forcing] the latent state to maintain all the information relevant for predictions\u201d, is a \u201cnovel\u201d model and \u201can interesting application\u201d. Our experimental results are \u201cillustrative\u201d and \u201cshow promise\u201d, as \u201c[DVBF] is better able to learn meaningful representations of sequence data\u201d---an \u201cinteresting and potentially useful\u201d contribution.\n\nWe have replied to each of your reviews individually to address your specific remarks and to clarify open questions and misunderstandings. These have found their way into a revised draft, which we will upload to OpenReview tomorrow (Feb 14). Notable improvements include:\n- We have greatly reworked introduction and background. It is crisper and more to the point.\n- At the same time, we have added more emphasis on specifying the two tasks we are solving (system identification and inference in the learned system), and their significance for controlled systems.\n- Section 3 has seen minor changes to clarify misconceptions raised in your reviews.\n\nWe hope to have understood and addressed your reviews in sufficient detail. Thank you for your time and consideration.\n", "OTHER_KEYS": "Maximilian Soelch"}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Rather incremental, but interesting experiments", "comments": "This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.\n\nMy compliments for the amount of detail put in the paper and appendix.\n\nThe experiments are on toy examples, but show promise.\n\n- Section 2.1: \u201cIn our notation, one would typically set beta_t = w_t, though other variants are possible\u201d -> It\u2019s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).\n\n- Section 2.2 last paragraph: \u201cA key contribution is [\u2026] forcing the latent space to fit the transition\u201d. This seems rather trivial to achieve.\n\n- Eq 9: \u201cThis interpretation implies the factorization of the recognition model:..\u201d\nThe factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting way of learning nonlinear state space models", "comments": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 2, "SUBSTANCE": 1, "RECOMMENDATION_UNOFFICIAL": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "No Title", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "16 Dec 2016", "TITLE": "Missing relevant prior work", "IS_META_REVIEW": false, "comments": "Thanks for the interesting paper!\n\nIt seems you may have missed some very relevant early prior work on variational inference for nonlinear state-space models in Valpola and Karhunen (Neural Computation 2002, doi:10.1162/089976602760408017) further developed in Honkela et al. (JMLR 2010).", "OTHER_KEYS": "Antti Honkela"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "why state space assumption helps with prediction?", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "27 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper provides a clear application of variational methods for learning non-linear state-space models, which is of increasing interest, and of general relevance to the community.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Comment to all reviewers", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nThank you for your kind and constructive feedback. According to your reviews, our paper is \u201cwell-written\u201d and \u201cwell-motivated\u201d. We agree that applying SGVB to identify and filter latent state-space models and \u201c[forcing] the latent state to maintain all the information relevant for predictions\u201d, is a \u201cnovel\u201d model and \u201can interesting application\u201d. Our experimental results are \u201cillustrative\u201d and \u201cshow promise\u201d, as \u201c[DVBF] is better able to learn meaningful representations of sequence data\u201d---an \u201cinteresting and potentially useful\u201d contribution.\n\nWe have replied to each of your reviews individually to address your specific remarks and to clarify open questions and misunderstandings. These have found their way into a revised draft, which we will upload to OpenReview tomorrow (Feb 14). Notable improvements include:\n- We have greatly reworked introduction and background. It is crisper and more to the point.\n- At the same time, we have added more emphasis on specifying the two tasks we are solving (system identification and inference in the learned system), and their significance for controlled systems.\n- Section 3 has seen minor changes to clarify misconceptions raised in your reviews.\n\nWe hope to have understood and addressed your reviews in sufficient detail. Thank you for your time and consideration.\n", "OTHER_KEYS": "Maximilian Soelch"}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Rather incremental, but interesting experiments", "comments": "This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.\n\nMy compliments for the amount of detail put in the paper and appendix.\n\nThe experiments are on toy examples, but show promise.\n\n- Section 2.1: \u201cIn our notation, one would typically set beta_t = w_t, though other variants are possible\u201d -> It\u2019s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).\n\n- Section 2.2 last paragraph: \u201cA key contribution is [\u2026] forcing the latent space to fit the transition\u201d. This seems rather trivial to achieve.\n\n- Eq 9: \u201cThis interpretation implies the factorization of the recognition model:..\u201d\nThe factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting way of learning nonlinear state space models", "comments": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 2, "SUBSTANCE": 1, "RECOMMENDATION_UNOFFICIAL": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "No Title", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "16 Dec 2016", "TITLE": "Missing relevant prior work", "IS_META_REVIEW": false, "comments": "Thanks for the interesting paper!\n\nIt seems you may have missed some very relevant early prior work on variational inference for nonlinear state-space models in Valpola and Karhunen (Neural Computation 2002, doi:10.1162/089976602760408017) further developed in Honkela et al. (JMLR 2010).", "OTHER_KEYS": "Antti Honkela"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "why state space assumption helps with prediction?", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "27 Nov 2016"}], "authors": "Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der Smagt", "accepted": true, "id": "422"}