{"conference": "ICLR 2017 conference submission", "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language", "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are in- terested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communi- cation. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message to the receiver, while the receiver must rely on it to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore whether the \u201cword meanings\u201d induced in the game reflect intuitive semantic properties of the objects depicted in the image, and we present a simple strategy for grounding the agents\u2019 code into natural language, a necessary step in developing machines that should eventually be able to communicate with humans.", "histories": [], "reviews": [{"DATE": "08 Feb 2017", "TITLE": "interesting paper, should enhance section about cooperation", "IS_META_REVIEW": false, "comments": "Very interesting paper. Maybe you could read paper by Gleizes about cooperative agent, self-organization and resolution through emergence \u2026 This paper could interest you : Self-adaptive complex systems, Marie-Pierre Gleizes, 2011, EUMAS.", "OTHER_KEYS": "Jonathan Bonnet"}, {"DATE": "07 Feb 2017", "TITLE": "Interesting paper, some important references seem to be missing", "IS_META_REVIEW": false, "comments": "The paper is interesting and well-written. The topic of language evolution has fascinated researchers from so many different fields (all kinds of linguistics, anthropology, psychology, sociology, but even mathematics and computer science), and now it looks like there is a new surge of interest -- even a long-time sceptic Noam Chomsky recently published a book. \n\nIt is a pity the paper is rather weak on references and related work. Not on language evolution in general, but on modeling language evolution as a multi-agent cooperation. For example, the submission does not even mention the Lewis Signaling Game which is basically what the paper proposes (", "OTHER_KEYS": "Katja Filippova"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present some initial findings on language emergence using multi-agent, referential games. The learning alternates between REINFORCE and supervised classification, which grounds the language. Pro- this is a relevant, novel paper. Con - experiments are somewhat simple/limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting idea, but could have been solved using a transfer learning approach", "comments": "Thank you for an interesting read.\n\nPros\n- This paper tackles a very crucial problem of understanding communications between 2 agents. As more and more applications of reinforcement learning are being explored, this approach brings us back to a basic question. Is the problem solving approach of machines similar to that of humans.\n\n- The task is simple enough to make the post learning analysis intuitive.\n\n- It was interesting to see how informed agents made use of multiple symbols to transmit the message, where as agnostic agents relied only on 2 symbols. \n\nCons\n- The task effectively boils down to image classification, if the 2 images sent are from different categories. The symbols used are effectively the image class which the second agent learns to assign to either of the images. By all means, this approach boils down to a transfer learning problem which could probably be trained much faster than a reinforcement learning algorithm.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "23 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Final Review", "comments": "To train natural language systems by putting multiple agents within an interactive referential communication game is very nice. As the authors mention, there has been some (although seemingly not much) previous work on using multi-agent games to teach communication, and it certainly seems like a direction worth pursuing. Moreover, the approach of switching between these games and some supervised learning, as in the experiment described in Section 5 and suggested in Section 6, seems particularly fruitful. \n\nNote: For \u201cclarity\u201d, I believe some of the network connections in Fig 1 have been omitted. However, given the rather highly-customized architecture and the slightly hard-to-follow description in Section 3, the shorthand diagram only adds to the confusion. The diagram probably needs to be fine-tuned, but at the very least (especially if I am misunderstanding it!), a caption must [still] be added to help the reader interpret the figure. \n\nOverall, the framework (Section 2) is great and seems quite effective/useful in various ways, the results are reasonable, and I expect there will be some interesting future variations on this work as well.\n\nCaveat: While I am quite confident I understood the paper (as per confidence score below), I do not feel I am sufficiently familiar with the most closely related literature to accurately assess the place of this work within that context. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016 (modified: 25 Jan 2017)", "APPROPRIATENESS": 2, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "In this paper, a referential game is proposed between two agents. Both agents observe two images. The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target. The agents both get a reward, if the receiver agent can predict the target. The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE. In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts. In addition to this, the paper presents experiments on a variant of the game grounded on different image classes. In this setting, the agents appear to learn even more meaningful concepts. Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images). Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.\n\nLearning shared representations for communication in a multi-agent setup is an interesting research direction to explore. This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task. To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel. As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects. I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work. For this reason, I think the paper should be accepted.\n\n\n\nOther comments:\n- How is the target (t) incorporated into the sender networks? Please clarify this.\n- Table 1 and Table 2 use percentage (%) values differently. In the first, percentages seem to be written in the interval [0, 100], and in the second in the interval [0, 1]. Please correct this. Perhaps related to this, in Table 1, the column \"obs-chance purity\" seems to have extremely small values. I assume this was mistake?\n- \"assest\" -> \"assess\"\n- \"usufal\" -> \"usual\"", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Feb 2017", "TITLE": "interesting paper, should enhance section about cooperation", "IS_META_REVIEW": false, "comments": "Very interesting paper. Maybe you could read paper by Gleizes about cooperative agent, self-organization and resolution through emergence \u2026 This paper could interest you : Self-adaptive complex systems, Marie-Pierre Gleizes, 2011, EUMAS.", "OTHER_KEYS": "Jonathan Bonnet"}, {"DATE": "07 Feb 2017", "TITLE": "Interesting paper, some important references seem to be missing", "IS_META_REVIEW": false, "comments": "The paper is interesting and well-written. The topic of language evolution has fascinated researchers from so many different fields (all kinds of linguistics, anthropology, psychology, sociology, but even mathematics and computer science), and now it looks like there is a new surge of interest -- even a long-time sceptic Noam Chomsky recently published a book. \n\nIt is a pity the paper is rather weak on references and related work. Not on language evolution in general, but on modeling language evolution as a multi-agent cooperation. For example, the submission does not even mention the Lewis Signaling Game which is basically what the paper proposes (", "OTHER_KEYS": "Katja Filippova"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present some initial findings on language emergence using multi-agent, referential games. The learning alternates between REINFORCE and supervised classification, which grounds the language. Pro- this is a relevant, novel paper. Con - experiments are somewhat simple/limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting idea, but could have been solved using a transfer learning approach", "comments": "Thank you for an interesting read.\n\nPros\n- This paper tackles a very crucial problem of understanding communications between 2 agents. As more and more applications of reinforcement learning are being explored, this approach brings us back to a basic question. Is the problem solving approach of machines similar to that of humans.\n\n- The task is simple enough to make the post learning analysis intuitive.\n\n- It was interesting to see how informed agents made use of multiple symbols to transmit the message, where as agnostic agents relied only on 2 symbols. \n\nCons\n- The task effectively boils down to image classification, if the 2 images sent are from different categories. The symbols used are effectively the image class which the second agent learns to assign to either of the images. By all means, this approach boils down to a transfer learning problem which could probably be trained much faster than a reinforcement learning algorithm.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "23 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Final Review", "comments": "To train natural language systems by putting multiple agents within an interactive referential communication game is very nice. As the authors mention, there has been some (although seemingly not much) previous work on using multi-agent games to teach communication, and it certainly seems like a direction worth pursuing. Moreover, the approach of switching between these games and some supervised learning, as in the experiment described in Section 5 and suggested in Section 6, seems particularly fruitful. \n\nNote: For \u201cclarity\u201d, I believe some of the network connections in Fig 1 have been omitted. However, given the rather highly-customized architecture and the slightly hard-to-follow description in Section 3, the shorthand diagram only adds to the confusion. The diagram probably needs to be fine-tuned, but at the very least (especially if I am misunderstanding it!), a caption must [still] be added to help the reader interpret the figure. \n\nOverall, the framework (Section 2) is great and seems quite effective/useful in various ways, the results are reasonable, and I expect there will be some interesting future variations on this work as well.\n\nCaveat: While I am quite confident I understood the paper (as per confidence score below), I do not feel I am sufficiently familiar with the most closely related literature to accurately assess the place of this work within that context. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016 (modified: 25 Jan 2017)", "APPROPRIATENESS": 2, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "In this paper, a referential game is proposed between two agents. Both agents observe two images. The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target. The agents both get a reward, if the receiver agent can predict the target. The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE. In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts. In addition to this, the paper presents experiments on a variant of the game grounded on different image classes. In this setting, the agents appear to learn even more meaningful concepts. Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images). Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.\n\nLearning shared representations for communication in a multi-agent setup is an interesting research direction to explore. This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task. To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel. As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects. I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work. For this reason, I think the paper should be accepted.\n\n\n\nOther comments:\n- How is the target (t) incorporated into the sender networks? Please clarify this.\n- Table 1 and Table 2 use percentage (%) values differently. In the first, percentages seem to be written in the interval [0, 100], and in the second in the interval [0, 1]. Please correct this. Perhaps related to this, in Table 1, the column \"obs-chance purity\" seems to have extremely small values. I assume this was mistake?\n- \"assest\" -> \"assess\"\n- \"usufal\" -> \"usual\"", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 3}], "authors": "Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni", "accepted": true, "id": "310"}