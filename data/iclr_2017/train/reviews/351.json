{"conference": "ICLR 2017 conference submission", "title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The problem addressed here is practically important (supervised learning with n<"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agreed that the paper proposed a solid and original contribution that was evaluated well. The authors would be encouraged to improve the presentation and provide a more precise mathematical presentation, as discussed by reviewer 1.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Jan 2017 (modified: 09 Jan 2017)", "TITLE": "Revisions", "IS_META_REVIEW": false, "comments": "We hope the revisions made to the paper properly address the concerns raised by the reviewers and other researchers, and improve the quality of the manuscript. \n\nSummary of revisions:\n\n* Embedding learnt end-to-end from raw data: Added paragraph (Section 2.2, page 5) to explain what the embedding learnt end-to-end is and how it is used within the Diet Networks framework.\n\n* Figure 3:  Scaled both confusion matrices so that maximum for each class is 1.\n\n* Table 1:  Added extended PCA results. \n\n* Table 1: Updated number of free parameters column to only account for the number of free parameters in the fat layers of the model, with the aim to make the presentation clearer.\n\n* Made code publicly available and added footnote with the corresponding url (page 6).\n\n* Section 4.2. (Results), paragraph 6: introduced extended PCA results (more PCs and MLP + classifier).\n\n* Appendix B: Added the instructions to download the dataset.\n\n* Appendix C: Added an analysis on PCA (number of components used) and performance.\n\n* Fixed typos.\n\n* Table 1: Added comment about feature embeddings dimensionality.\n\n", "OTHER_KEYS": "Adriana Romero"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting but somewhat early work on deep learning with very high dimensional genomic data", "comments": "The paper presents an application of deep learning to genomic SNP data\nwith a comparison of possible approaches for dealing with the very\nhigh data dimensionality. The approach looks very interesting but the\nexperiments are too limited to draw firm conclusions about the\nstrengths of different approaches. The presentation would benefit from\nmore precise math.\n\n\nQuality:\n\nThe basic idea of the paper is interesting and the applied deep\nlearning methodology appears reasonable. The experimental evaluation\nis rather weak as it only covers a single data set and a very limited\nnumber of cross validation folds. Given the significant variation in\nthe performances of all the methods, it seems the differences between\nthe better-performing methods are probably not statistically\nsignificant. More comprehensive empirical validation could clearly\nstrengthen the paper.\n\n\nClarity:\n\nThe writing is generally good both in terms of the biology and ML, but\nmore mathematical rigour would make it easier to understand precisely\nwhat was done. The different architectures are explained on an\nintuitive level and might benefit from a clear mathematical\ndefinition. I was ultimately left unsure of what the \"raw end2end\"\nmodel is - given so few parameters it cannot work on raw 300k\ndimensional input but I could not figure out what kind of embedding\nwas used.\n\nThe results in Fig. 3 might be clearer if scaled so that maximum for\neach class is 1 to avoid confounding from different numbers of\nsubjects in different classes. In the text, please use the standard\nitalics math font for all symbols such as N, N_d, ...\n\n\nOriginality:\n\nThe application and the approach appear quite novel.\n\n\nSignificance:\n\nThere is clearly strong interest for deep learning in the genomics\narea and the paper seeks to address some of the major bottlenecks\nhere. It is too early to tell whether the specific techniques proposed\nin the paper will be the ultimate solution, but at the very least the\npaper provides interesting new ideas for others to work on.\n\n\nOther comments:\n\nI think releasing the code as promised would be a must.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper addresses the important problem (d>>n) in deep learning. \nThe proposed approach, based on lower-dimensional feature embeddings, is reasonable and makes applying deep learning methods to data with large d possible.\nThe paper is well written and the results show improvements over reasonable baselines.\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "13 Dec 2016 (modified: 14 Dec 2016)", "TITLE": "Few questions", "IS_META_REVIEW": false, "comments": "Thanks for this very nice paper. I have a few technical questions:\n\n1- How do you handle missing genotypes? When I download the same 1000G Affymetrix file and preprocess it using plink2 options \"--maf 0.05 --geno 0 --not-chr X Y MT --indep-pairwise 500 5 0.5\", I end up with 120,036 SNPs, rather than 315,345. I appreciate it if you could share more details on preprocessing.\n\n2- What are the final dropout and weight decay rates that have been used?\n\n3- Are you planning to apply the method on full 1000G Phase3 dataset which consists of 84 million variants to make n<", "OTHER_KEYS": "G\u00f6kcen Eraslan"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Nice paper; very clear presentation.", "comments": "The problem addressed here is practically important (supervised learning with n<", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "12 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Comparison to PCA + MLP", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "05 Dec 2016"}, {"TITLE": "Clarification on the experimental procedure", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "The problem addressed here is practically important (supervised learning with n<"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agreed that the paper proposed a solid and original contribution that was evaluated well. The authors would be encouraged to improve the presentation and provide a more precise mathematical presentation, as discussed by reviewer 1.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Jan 2017 (modified: 09 Jan 2017)", "TITLE": "Revisions", "IS_META_REVIEW": false, "comments": "We hope the revisions made to the paper properly address the concerns raised by the reviewers and other researchers, and improve the quality of the manuscript. \n\nSummary of revisions:\n\n* Embedding learnt end-to-end from raw data: Added paragraph (Section 2.2, page 5) to explain what the embedding learnt end-to-end is and how it is used within the Diet Networks framework.\n\n* Figure 3:  Scaled both confusion matrices so that maximum for each class is 1.\n\n* Table 1:  Added extended PCA results. \n\n* Table 1: Updated number of free parameters column to only account for the number of free parameters in the fat layers of the model, with the aim to make the presentation clearer.\n\n* Made code publicly available and added footnote with the corresponding url (page 6).\n\n* Section 4.2. (Results), paragraph 6: introduced extended PCA results (more PCs and MLP + classifier).\n\n* Appendix B: Added the instructions to download the dataset.\n\n* Appendix C: Added an analysis on PCA (number of components used) and performance.\n\n* Fixed typos.\n\n* Table 1: Added comment about feature embeddings dimensionality.\n\n", "OTHER_KEYS": "Adriana Romero"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting but somewhat early work on deep learning with very high dimensional genomic data", "comments": "The paper presents an application of deep learning to genomic SNP data\nwith a comparison of possible approaches for dealing with the very\nhigh data dimensionality. The approach looks very interesting but the\nexperiments are too limited to draw firm conclusions about the\nstrengths of different approaches. The presentation would benefit from\nmore precise math.\n\n\nQuality:\n\nThe basic idea of the paper is interesting and the applied deep\nlearning methodology appears reasonable. The experimental evaluation\nis rather weak as it only covers a single data set and a very limited\nnumber of cross validation folds. Given the significant variation in\nthe performances of all the methods, it seems the differences between\nthe better-performing methods are probably not statistically\nsignificant. More comprehensive empirical validation could clearly\nstrengthen the paper.\n\n\nClarity:\n\nThe writing is generally good both in terms of the biology and ML, but\nmore mathematical rigour would make it easier to understand precisely\nwhat was done. The different architectures are explained on an\nintuitive level and might benefit from a clear mathematical\ndefinition. I was ultimately left unsure of what the \"raw end2end\"\nmodel is - given so few parameters it cannot work on raw 300k\ndimensional input but I could not figure out what kind of embedding\nwas used.\n\nThe results in Fig. 3 might be clearer if scaled so that maximum for\neach class is 1 to avoid confounding from different numbers of\nsubjects in different classes. In the text, please use the standard\nitalics math font for all symbols such as N, N_d, ...\n\n\nOriginality:\n\nThe application and the approach appear quite novel.\n\n\nSignificance:\n\nThere is clearly strong interest for deep learning in the genomics\narea and the paper seeks to address some of the major bottlenecks\nhere. It is too early to tell whether the specific techniques proposed\nin the paper will be the ultimate solution, but at the very least the\npaper provides interesting new ideas for others to work on.\n\n\nOther comments:\n\nI think releasing the code as promised would be a must.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper addresses the important problem (d>>n) in deep learning. \nThe proposed approach, based on lower-dimensional feature embeddings, is reasonable and makes applying deep learning methods to data with large d possible.\nThe paper is well written and the results show improvements over reasonable baselines.\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "13 Dec 2016 (modified: 14 Dec 2016)", "TITLE": "Few questions", "IS_META_REVIEW": false, "comments": "Thanks for this very nice paper. I have a few technical questions:\n\n1- How do you handle missing genotypes? When I download the same 1000G Affymetrix file and preprocess it using plink2 options \"--maf 0.05 --geno 0 --not-chr X Y MT --indep-pairwise 500 5 0.5\", I end up with 120,036 SNPs, rather than 315,345. I appreciate it if you could share more details on preprocessing.\n\n2- What are the final dropout and weight decay rates that have been used?\n\n3- Are you planning to apply the method on full 1000G Phase3 dataset which consists of 84 million variants to make n<", "OTHER_KEYS": "G\u00f6kcen Eraslan"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Nice paper; very clear presentation.", "comments": "The problem addressed here is practically important (supervised learning with n<", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "12 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Comparison to PCA + MLP", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "05 Dec 2016"}, {"TITLE": "Clarification on the experimental procedure", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}], "authors": "Adriana Romero, Pierre Luc Carrier, Akram Erraqabi, Tristan Sylvain, Alex Auvolat, Etienne Dejoie, Marc-Andr\u00e9 Legault, Marie-Pierre Dub\u00e9, Julie G. Hussin, Yoshua Bengio", "accepted": true, "id": "351"}