{"conference": "ICLR 2017 conference submission", "title": "Regularizing CNNs with Locally Constrained Decorrelations", "abstract": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively.  In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a new regularization approach for deep learning that penalizes positive correlations between features in a network. The experimental evaluation is solid, and suggests the proposed regularized may help in learning better convolutional networks (but the gains are relatively small).", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Paper v2.0", "IS_META_REVIEW": false, "comments": "We thank again the reviewers for the thorough revision and the valuable feedback. We have updated the paper accordingly:\n\n1. We have applied OrthoReg on the new version of Wide ResNets, consistently reducing the error rates to 3.69%, and 18.56% on Cifar10, and Cifar100 respectively.\n\n2.  Figure 6 has been updated so as to compare the base regularization to the regularization of positively correlated feature weights. All the curves are now obtained from Wide ResNet v2.\n\nMulti-bias neural networks are now discussed in the third paragraph of section 2.\n3. The need for investigating the effects of activation functions like ReLU on the different existing regularizations is now reflected on the discussion section.\n\n4. When appearing alone, the word \u201cfeatures\u201d has been replaced for \u201cfeature weights\u201d or \u201cfeature activations\u201d accordingly.\nThe dichotomy between regularization types has been changed according to reviewer 3 comments.\n", "OTHER_KEYS": "Pau Rodriguez"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Good results on deep nets", "comments": "The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Solid experimental validation", "comments": "The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n\nThe experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nOne of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nThe paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nIn response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nThe dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nOverall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Orthogonality of weight features might not always lead to improved performance", "comments": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Very good results", "comments": "This is a good paper with the appropriate experimentation. Regularization must be tested on deep and complex topologies, near to state of the art. Other papers test reg. with simple models where regularization helps but are not in the edge...\n\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Regularization helps", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"TITLE": "activations vs. weights", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"TITLE": "Orthogonality of weight features might not always lead to improved performance", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a new regularization approach for deep learning that penalizes positive correlations between features in a network. The experimental evaluation is solid, and suggests the proposed regularized may help in learning better convolutional networks (but the gains are relatively small).", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Paper v2.0", "IS_META_REVIEW": false, "comments": "We thank again the reviewers for the thorough revision and the valuable feedback. We have updated the paper accordingly:\n\n1. We have applied OrthoReg on the new version of Wide ResNets, consistently reducing the error rates to 3.69%, and 18.56% on Cifar10, and Cifar100 respectively.\n\n2.  Figure 6 has been updated so as to compare the base regularization to the regularization of positively correlated feature weights. All the curves are now obtained from Wide ResNet v2.\n\nMulti-bias neural networks are now discussed in the third paragraph of section 2.\n3. The need for investigating the effects of activation functions like ReLU on the different existing regularizations is now reflected on the discussion section.\n\n4. When appearing alone, the word \u201cfeatures\u201d has been replaced for \u201cfeature weights\u201d or \u201cfeature activations\u201d accordingly.\nThe dichotomy between regularization types has been changed according to reviewer 3 comments.\n", "OTHER_KEYS": "Pau Rodriguez"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Good results on deep nets", "comments": "The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Solid experimental validation", "comments": "The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n\nThe experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nOne of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nThe paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nIn response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nThe dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nOverall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Orthogonality of weight features might not always lead to improved performance", "comments": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Very good results", "comments": "This is a good paper with the appropriate experimentation. Regularization must be tested on deep and complex topologies, near to state of the art. Other papers test reg. with simple models where regularization helps but are not in the edge...\n\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Regularization helps", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"TITLE": "activations vs. weights", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"TITLE": "Orthogonality of weight features might not always lead to improved performance", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}], "authors": "Pau Rodr\u00edguez, Jordi Gonz\u00e0lez, Guillem Cucurull, Josep M. Gonfaus, Xavier Roca", "accepted": true, "id": "415"}