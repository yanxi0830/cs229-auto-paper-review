{"conference": "ICLR 2017 conference submission", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Responses", "IS_META_REVIEW": false, "comments": "Thank you for your helpful comments and suggestions.  We have uploaded a revised version of the paper addressing some of the comments as well as fixing some typos etc.  Below are our replies to specific comments.\n\n*Regarding the data set and task (Reviewers 1, 4):  \nThe data set is indeed on the small size, and the assumption of known word boundaries is a strong one.  This paper focuses on improving the current state of research on learning acoustic embeddings, so we are comparing to the most relevant prior work, which largely uses this data set and the word discrimination task.  Now that we have achieved very high average precisions on this task, we believe that future work should indeed focus on (and standardize) larger data sets and tasks.\n\nWe would like to point out, however, some prior work suggesting that improvements on this data set/task can transfer to other data/tasks.  Specifically, Levin et al. took embeddings optimized on this data set/task (Levin et al., ASRU 2013) to improve a query-by-example system without known word boundaries (Levin et al., ICASSP 2015), by simply applying their embedding approach to non-word segments as well.  This is encouraging.  On the other hand, it would also be straightforward, and more principled, to extend our approach to directly train on both word and non-word segments.  We mention this in the revised paper.\n\n*Regarding experimenting with phone sequences rather than character sequences (Reviewer 1):  \nAlthough working with phone sequences requires a bit more supervision, we agree that this is an interesting and straightforward experiment and we are currently working on it (though it is not complete).  In the meantime, in our revised paper we have included the rank correlation between our embedding distances (trained with character supervision) and phonetic edit distances, which are not too different from the correlations with orthographic distance (Table 3).   This is nice since it suggests we might not be losing too much by using orthographic supervision vs. phonetic supervision.\n\n*Regarding homophones (Reviewer 1):  \nWe expect our approach to be unable to distinguish homophones, since they can only be distinguished in context.  Our data set does not include a sufficient number of homophones to confirm this, but future work should look at this problem in the context of more data and longer contexts.\n\n*Regarding ASR baselines (Reviewer 4):  \nThe revised paper includes an ASR-based baseline from prior work in Table 2, using DTW on phone posteriors, which is worse than ours despite training on vastly more data.  While it is an older result, it shows that it is not trivial to get our numbers.  We believe that this is because there is a benefit to embedding the entire sequence and training with a loss that explicitly optimizes a measure related to the discrimination task at hand.\n\n*Regarding additional analysis (Reviewer 4):  \nWe have added (in the appendix) a precision-recall curve and scatterplot of embedding vs. orthographic distances.\n\n*Regarding open-sourcing the code (Reviewer 3):\nWe agree.   We are in the process of updating our code and releasing it online.\n\nThank you also for pointing out the issue with Fig. 1 (it has been fixed in the revised paper).\n\n", "OTHER_KEYS": "Weiran Wang"}, {"TITLE": "The paper investigates jointly trained acoustic and character level word embeddings, but only on a very small task.", "SUBSTANCE": 5, "RECOMMENDATION_UNOFFICIAL": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Pros:\n  Interesting training criterion.\nCons:\n  Missing proper ASR technique based baselines.\n\nComments:\n  The dataset is quite small.\n  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.\n  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection\n  performance of out-of-vocabulary words.\n  It would be interesting to show scatter plots for embedding vs. orthographic distances.\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption. The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.\n\nThere's a lot of discussion of character edit distance relative to acoustic span similarity. It seems very natural to also include phoneme string edit distance in this discussion and experiments. This is especially true of the word similarity test. Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances. Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings. I believe the last function could remain identical it's just swapping out characters for phones as the symbol set.  finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.\n\n the vocabulary size and training data amount make this really a toy problem. although there are many pairs constructed most of those pairs will be easy distinctions. the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.\n\n it seems this approach is unable to address the task of keyword spotting in longer spoken utterances. If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations. The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "well-done domain adaptation", "comments": "this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "questions", "SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "RECOMMENDATION_UNOFFICIAL": 4, "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "combination of losses & pretraining", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Responses", "IS_META_REVIEW": false, "comments": "Thank you for your helpful comments and suggestions.  We have uploaded a revised version of the paper addressing some of the comments as well as fixing some typos etc.  Below are our replies to specific comments.\n\n*Regarding the data set and task (Reviewers 1, 4):  \nThe data set is indeed on the small size, and the assumption of known word boundaries is a strong one.  This paper focuses on improving the current state of research on learning acoustic embeddings, so we are comparing to the most relevant prior work, which largely uses this data set and the word discrimination task.  Now that we have achieved very high average precisions on this task, we believe that future work should indeed focus on (and standardize) larger data sets and tasks.\n\nWe would like to point out, however, some prior work suggesting that improvements on this data set/task can transfer to other data/tasks.  Specifically, Levin et al. took embeddings optimized on this data set/task (Levin et al., ASRU 2013) to improve a query-by-example system without known word boundaries (Levin et al., ICASSP 2015), by simply applying their embedding approach to non-word segments as well.  This is encouraging.  On the other hand, it would also be straightforward, and more principled, to extend our approach to directly train on both word and non-word segments.  We mention this in the revised paper.\n\n*Regarding experimenting with phone sequences rather than character sequences (Reviewer 1):  \nAlthough working with phone sequences requires a bit more supervision, we agree that this is an interesting and straightforward experiment and we are currently working on it (though it is not complete).  In the meantime, in our revised paper we have included the rank correlation between our embedding distances (trained with character supervision) and phonetic edit distances, which are not too different from the correlations with orthographic distance (Table 3).   This is nice since it suggests we might not be losing too much by using orthographic supervision vs. phonetic supervision.\n\n*Regarding homophones (Reviewer 1):  \nWe expect our approach to be unable to distinguish homophones, since they can only be distinguished in context.  Our data set does not include a sufficient number of homophones to confirm this, but future work should look at this problem in the context of more data and longer contexts.\n\n*Regarding ASR baselines (Reviewer 4):  \nThe revised paper includes an ASR-based baseline from prior work in Table 2, using DTW on phone posteriors, which is worse than ours despite training on vastly more data.  While it is an older result, it shows that it is not trivial to get our numbers.  We believe that this is because there is a benefit to embedding the entire sequence and training with a loss that explicitly optimizes a measure related to the discrimination task at hand.\n\n*Regarding additional analysis (Reviewer 4):  \nWe have added (in the appendix) a precision-recall curve and scatterplot of embedding vs. orthographic distances.\n\n*Regarding open-sourcing the code (Reviewer 3):\nWe agree.   We are in the process of updating our code and releasing it online.\n\nThank you also for pointing out the issue with Fig. 1 (it has been fixed in the revised paper).\n\n", "OTHER_KEYS": "Weiran Wang"}, {"TITLE": "The paper investigates jointly trained acoustic and character level word embeddings, but only on a very small task.", "SUBSTANCE": 5, "RECOMMENDATION_UNOFFICIAL": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Pros:\n  Interesting training criterion.\nCons:\n  Missing proper ASR technique based baselines.\n\nComments:\n  The dataset is quite small.\n  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.\n  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection\n  performance of out-of-vocabulary words.\n  It would be interesting to show scatter plots for embedding vs. orthographic distances.\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption. The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.\n\nThere's a lot of discussion of character edit distance relative to acoustic span similarity. It seems very natural to also include phoneme string edit distance in this discussion and experiments. This is especially true of the word similarity test. Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances. Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings. I believe the last function could remain identical it's just swapping out characters for phones as the symbol set.  finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.\n\n the vocabulary size and training data amount make this really a toy problem. although there are many pairs constructed most of those pairs will be easy distinctions. the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.\n\n it seems this approach is unable to address the task of keyword spotting in longer spoken utterances. If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations. The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "well-done domain adaptation", "comments": "this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "questions", "SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "RECOMMENDATION_UNOFFICIAL": 4, "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "combination of losses & pretraining", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}], "authors": "Wanjia He, Weiran Wang, Karen Livescu", "accepted": true, "id": "408"}