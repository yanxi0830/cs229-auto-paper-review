{"conference": "ICLR 2017 conference submission", "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.\n\nOverall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don\u2019t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The approach here looks at learning policies that are robust over a parameterized class of MDPs (in the sense the probability that the policy doesn't perform well is small over this class. The idea is fairly straightforward, but the algorithm seems novel and the results show that the approach does seem to provide substantial benefit. The reviewers were all in agreement that the paper is worth accepting.\n \n Pros:\n + Nice application of robust (really stochastic, since these are chance constraints) optimization to policy search\n + Compelling demonstration of the improved range of good performance over methods like vanilla TRPO\n \n Cons:\n - The question of how to parameterize a class of MDPs for real-world scenarios is still somewhat unclear\n - The description of the method as optimizing CVaR seems incorrect, since they appear to be using an actual chance constraint, whereas CVaR is essentially a convex relaxation ... this may be related to the work in (Tamar, 2015), but needs to be better explained if so.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 2, "SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Ensemble training and transfer, a good submission", "comments": "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.\n\nThat said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.\n\n", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016 (modified: 20 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 1, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain. \n\n> Significance\n\nPaper addresses and important and significant problem. The approach taken in addressing it is also interesting \n\n> Clarity\n\nPaper is well written, but does require domain knowledge to understand. \n\nMy main concerns were well addressed by the rebuttal and corresponding revisions to the paper. ", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "ICLR 2017 conference review", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "comments": "The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.\n\nOverall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don\u2019t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.\n", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016"}, {"IMPACT": 1, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 2}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Questions", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"IS_META_REVIEW": true, "comments": "The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.\n\nOverall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don\u2019t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The approach here looks at learning policies that are robust over a parameterized class of MDPs (in the sense the probability that the policy doesn't perform well is small over this class. The idea is fairly straightforward, but the algorithm seems novel and the results show that the approach does seem to provide substantial benefit. The reviewers were all in agreement that the paper is worth accepting.\n \n Pros:\n + Nice application of robust (really stochastic, since these are chance constraints) optimization to policy search\n + Compelling demonstration of the improved range of good performance over methods like vanilla TRPO\n \n Cons:\n - The question of how to parameterize a class of MDPs for real-world scenarios is still somewhat unclear\n - The description of the method as optimizing CVaR seems incorrect, since they appear to be using an actual chance constraint, whereas CVaR is essentially a convex relaxation ... this may be related to the work in (Tamar, 2015), but needs to be better explained if so.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 2, "SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Ensemble training and transfer, a good submission", "comments": "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.\n\nThat said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.\n\n", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016 (modified: 20 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 1, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain. \n\n> Significance\n\nPaper addresses and important and significant problem. The approach taken in addressing it is also interesting \n\n> Clarity\n\nPaper is well written, but does require domain knowledge to understand. \n\nMy main concerns were well addressed by the rebuttal and corresponding revisions to the paper. ", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "ICLR 2017 conference review", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "comments": "The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.\n\nOverall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don\u2019t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.\n", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016"}, {"IMPACT": 1, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 2}, {"SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Questions", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}], "authors": "Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, Sergey Levine", "accepted": true, "id": "406"}