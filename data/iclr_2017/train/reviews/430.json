{"conference": "ICLR 2017 conference submission", "title": "Latent Sequence Decompositions", "abstract": "Sequence-to-sequence models rely on a fixed decomposition of the target sequences into a sequence of tokens that may be words, word-pieces or characters. The choice of these tokens and the decomposition of the target sequences into a sequence of tokens is often static, and independent of the input, output data domains. This can potentially lead to a sub-optimal choice of token dictionaries, as the decomposition is not informed by the particular problem being solved. In this paper we present Latent Sequence Decompositions (LSD), a framework in which the decomposition of sequences into constituent tokens is learnt during the training of the model. The decomposition depends both on the input sequence and on the output sequence. In LSD, during training, the model samples decompositions incrementally, from left to right by locally sampling between valid extensions. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve a WER of 9.6%.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work proposes a method for segmenting target generation sequence that is learned as part of the model. Generally all reviewers found this paper novel and interesting.\n \n Pros:\n - Quality: The paper is both containing \"with solid theoretically justified\" and \"present(s) nice improvements over character based result\". One reviewer asked for the \"the experimental study could be more solid.\"\n - Impact: methods like \"BPE\" are now somewhat standard hacks in seq2seq modeling. This type of model could be potentially impactful at disrupting. \n - Clarity: Reviewers found the work to be a \"clearly written paper\" \n \n Cons:\n - Some of the reviewers were not as enthuthiastic about this work compared to other papers. There were several comments asking for further experimental results. However I found that the author's responses clearly explained these away and provided clear justificition for why they were not necessary, already included, or explained by previous results. I feel that this takes care of the major issues, and warrants a small raise in score.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6", "comments": "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes. \n\nMinor points\n- Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it's hard to tell if the model is learning phonemes or just most frequent character n-grams.\n- Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "03 Jan 2017 (modified: 26 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting paper, but raising a few questions.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.\n\nAlthough the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.\n\nThe authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.\n\nObviously, only text based segmentation would result in two (but simpler) independent tasks. In order to extract such segmentation several publicly open tools are available and should be cited. Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.\n\nIt looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models. In another way, less tokens are more probable due to less multiplication of probabilities. As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all? (WSJ is a closed vocabulary task). It would be good to show that the sub word model could outperform even a full-word model (no segmentation).\nYour model estimates p(z_t|x,z", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 27 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "29 Nov 2016", "TITLE": "The collapse function and language model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work proposes a method for segmenting target generation sequence that is learned as part of the model. Generally all reviewers found this paper novel and interesting.\n \n Pros:\n - Quality: The paper is both containing \"with solid theoretically justified\" and \"present(s) nice improvements over character based result\". One reviewer asked for the \"the experimental study could be more solid.\"\n - Impact: methods like \"BPE\" are now somewhat standard hacks in seq2seq modeling. This type of model could be potentially impactful at disrupting. \n - Clarity: Reviewers found the work to be a \"clearly written paper\" \n \n Cons:\n - Some of the reviewers were not as enthuthiastic about this work compared to other papers. There were several comments asking for further experimental results. However I found that the author's responses clearly explained these away and provided clear justificition for why they were not necessary, already included, or explained by previous results. I feel that this takes care of the major issues, and warrants a small raise in score.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6", "comments": "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes. \n\nMinor points\n- Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it's hard to tell if the model is learning phonemes or just most frequent character n-grams.\n- Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "03 Jan 2017 (modified: 26 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting paper, but raising a few questions.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.\n\nAlthough the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.\n\nThe authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.\n\nObviously, only text based segmentation would result in two (but simpler) independent tasks. In order to extract such segmentation several publicly open tools are available and should be cited. Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.\n\nIt looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models. In another way, less tokens are more probable due to less multiplication of probabilities. As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all? (WSJ is a closed vocabulary task). It would be good to show that the sub word model could outperform even a full-word model (no segmentation).\nYour model estimates p(z_t|x,z", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 27 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "29 Nov 2016", "TITLE": "The collapse function and language model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly", "accepted": true, "id": "430"}