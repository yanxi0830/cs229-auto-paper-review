{"conference": "ICLR 2017 conference submission", "title": "Development of JavaScript-based deep learning platform and application to distributed training", "abstract": "Deep learning is increasingly attracting attention for processing big data. Existing frameworks for deep learning must be set up to specialized computer systems. Gaining sufficient computing resources therefore entails high costs of deployment and maintenance. In this work, we implement a matrix library and deep learning framework that uses JavaScript. It can run on web browsers operating on ordinary personal computers and smartphones. Using JavaScript, deep learning can be accomplished in widely diverse environments without the necessity for software installation. Using GPGPU from WebCL framework, our framework can train large scale convolutional neural networks such as VGGNet and ResNet. In the experiments, we demonstrate their practicality by training VGGNet in a distributed manner using web browsers as the client.", "histories": [], "reviews": [{"DATE": "14 Feb 2017", "TITLE": "A small detail in the related work.", "IS_META_REVIEW": false, "comments": "In the \u00a7 Related work:\n\"deeplearning4j 2 provides distributed computing of deep learning framework\nthat runs on the distributed computing Hadoop. However, Hadoop must be installed in all\ncomputing nodes, thereby imposing high deployment and maintenance costs.\"\n\nThis is inexact, Deeplearning4j's most basic mode of operation is on a single machine, with Java installed. A GPU is used if available but is not a requirement (Deeplearning4j documentation: ", "OTHER_KEYS": "Fran\u00e7ois Garillot"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "A summary of the reviews and discussion is as follows:\n \n Strengths\n Code for matrix library sushi2 and DL library sukiyaki2 are on Github, including live demos -- work is reproduceable (R2)\n Work/vision is exciting (R2)\n \n Weaknesses\n Projects preliminary (documentation, engineering of convolutions, speed, etc.) (R2)\n Perhaps not the right fit for ICLR? (R3) AC comment: ICLR specifically lists *implementation issues, parallelization, software platforms, hardware* as one of the topics of interest\n Doesn\u2019t advance the state-of-the-art in performance (e.g. no new algorithm or UI/UX improvement) (R3)\n \n The authors responded to the pre-review questions and also the official reviews; they updated their demo and paper accordingly.\n \n Looking at the overall sentiment of the reviews, the extensive feedback from the authors, and the openness of the project I feel that it is a valuable contribution to the community. \n \n However, given that the paper doesn't clearly advance the state of the art, the PCs believe it would be more appropriate to present it as part of the Workshop Track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Validity:\nThe presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Javascript wrapper for DNN code allows training within a web browser, even using GPU. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. \nThere is no new algorithm here, nor is there any UI/meta-design improvement to make it easier for non-experts to design and train neural network systems. \n\nI think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. \nI have no significant objection to the presentation or methodology of the paper. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.\n\nMy main points of criticism are:\n1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).\n\n2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many \"low performance\" clients, the setting of having a few dedicated high performance servers is quite likely. Even if not, these are good values to compare with. For the sake of consistency, please include in both subfigures Firefox, Chrome, node.js.\n\nApart from these points, well-written, understandable and conclusive.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "12 Dec 2016", "TITLE": "use cases", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Evaluation question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "14 Feb 2017", "TITLE": "A small detail in the related work.", "IS_META_REVIEW": false, "comments": "In the \u00a7 Related work:\n\"deeplearning4j 2 provides distributed computing of deep learning framework\nthat runs on the distributed computing Hadoop. However, Hadoop must be installed in all\ncomputing nodes, thereby imposing high deployment and maintenance costs.\"\n\nThis is inexact, Deeplearning4j's most basic mode of operation is on a single machine, with Java installed. A GPU is used if available but is not a requirement (Deeplearning4j documentation: ", "OTHER_KEYS": "Fran\u00e7ois Garillot"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "A summary of the reviews and discussion is as follows:\n \n Strengths\n Code for matrix library sushi2 and DL library sukiyaki2 are on Github, including live demos -- work is reproduceable (R2)\n Work/vision is exciting (R2)\n \n Weaknesses\n Projects preliminary (documentation, engineering of convolutions, speed, etc.) (R2)\n Perhaps not the right fit for ICLR? (R3) AC comment: ICLR specifically lists *implementation issues, parallelization, software platforms, hardware* as one of the topics of interest\n Doesn\u2019t advance the state-of-the-art in performance (e.g. no new algorithm or UI/UX improvement) (R3)\n \n The authors responded to the pre-review questions and also the official reviews; they updated their demo and paper accordingly.\n \n Looking at the overall sentiment of the reviews, the extensive feedback from the authors, and the openness of the project I feel that it is a valuable contribution to the community. \n \n However, given that the paper doesn't clearly advance the state of the art, the PCs believe it would be more appropriate to present it as part of the Workshop Track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Validity:\nThe presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Javascript wrapper for DNN code allows training within a web browser, even using GPU. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. \nThere is no new algorithm here, nor is there any UI/meta-design improvement to make it easier for non-experts to design and train neural network systems. \n\nI think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. \nI have no significant objection to the presentation or methodology of the paper. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.\n\nMy main points of criticism are:\n1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).\n\n2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many \"low performance\" clients, the setting of having a few dedicated high performance servers is quite likely. Even if not, these are good values to compare with. For the sake of consistency, please include in both subfigures Firefox, Chrome, node.js.\n\nApart from these points, well-written, understandable and conclusive.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "12 Dec 2016", "TITLE": "use cases", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Evaluation question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Masatoshi Hidaka, Ken Miura, Tatsuya Harada", "accepted": false, "id": "543"}