{"conference": "ICLR 2017 conference submission", "title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "09 Jan 2017 (modified: 27 Jan 2017)", "TITLE": "Reply to reviewers", "IS_META_REVIEW": false, "comments": "We strongly request that our paper be reviewed for its contents, contributions, significance, originality, and impact. We also would like to highlight the following three points to reviewers and readers. \n\n(i) Claim of \"sentence-level\" lipreading: This was the main concern of reviewers 1 and 2, and likely why they gave us the surprisingly low scores of 4 (7 being borderline for acceptance). Our claim is that we proposed the first \"end-to-end sentence-level\" lipreading approach, and we have edited the paper to make this precise. The phrase \u201cend-to-end\u201d is very important. The previous few attempts at sentence-level lipreading used heuristic pipelines and obtained poor results. LipNet, on the other hand, is fully end-to-end and achieves state-of-the-art results by a significant margin. on the largest available public dataset  (with over 24 hours of video, making it far from trivial). Given the removal of the phrase sentence-level, the reviews of reviewers 1 and 2 regarding prior work are not longer applicable.\n\n(ii) Is the dataset too simple? GRID despite all its shortcomings, is the largest available public dataset with over 24 hours of video (more than 2,000,000 frames and 64,000 possible sentences), making it more comparable to ImageNet than to say MNIST or CIFAR-10. May deep learning papers have used simplified versions of this dataset (eg restrictions to speaker-dependent recognition, or classification only) and even then obtained worse results. Moreover, we have embarked on more commercial datasets with success as reported in this keynote by NVIDIA's CEO at CES (177,393 attendees). See this video (", "OTHER_KEYS": "Nando de Freitas"}, {"TITLE": "Impressive result on lip reading, interesting analysis, some flawed comparison", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.\n\nOverall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Nice application of end to end training on the visual pipeline of traditional AV-ASR systems. Sentence level sequence objectives (MPE/MMI/fMPE) have been applied to this problem in the past", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "- Proven again that end to end training with deep networks gives large\ngains over traditional hybrid systems with hand crafted features. The results \nare very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.\n\n- Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.\n\n- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A nice result, but with limited novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Comments / Questions", "IS_META_REVIEW": false, "comments": "\n\nThis is an interesting paper. Here are some comments:\n\n\n1) Testing on 4 subjects only is rather limited given that there are 33 subjects. For example Ngiam et al. used 18 subjects for training and 18 for testing on CUAVE which contains a similar number of subjects. So a similar setup would be 17 subjects for training and 16 for testing. This would make the results much more convincing.\n\n2) The authors state that the performance of human lipreaders is around 20% and cite Hilder et al. In that paper, the human performance is around 70%, please correct this claim. In addition, in that paper machine lip-reading outperforms humans as well so please make this point clear. \n\n3) The state of the art performance on GRID is not 79.6% as mentioned in the text. It is the one mentioned in the other comments. Actually, as of last week there is a new state-of-the-art ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "02 Dec 2016", "TITLE": "prior work on lipreading", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "16 Nov 2016", "TITLE": "state-of-the-art system", "IS_META_REVIEW": false, "comments": "Dear all,\n\nmany thanks for sharing the paper, which I did find hugely interesting - while the GRiD corpus is small, I do agree that it is currently a good task for taking first steps towards video-based and audio-visual speech recognition, and I believe the work you did here will ultimately be very useful for lipreading (and for lipreading-based speech enhancement) in more general scenarios.\n\nI do have one comment regarding the prior-state-of-the-art system that you cite: As far as I know, our group's Interspeech 2016 holds the record for best lipreading performance on the GRiD corpus, with 86.4% word accuracy (this does include a grammar, but it might still be interesting).\nPlease cite: S. Gergen, S. Zeiler, A. Hussen Abdelaziz, R. Nickel and D. Kolossa: \" Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR,\" in Proc. Interspeech 2016, San Francisco, Sept. 2016.\n", "OTHER_KEYS": "Dorothea Kolossa"}, {"DATE": "08 Nov 2016", "TITLE": "Some comments and suggestions", "IS_META_REVIEW": false, "comments": "- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting.", "OTHER_KEYS": "Mohamed Yousef Bassyouni"}, {"DATE": "06 Nov 2016", "TITLE": "Not so much reacting to the paper but to the 'twitter-storm' it generated. ", "IS_META_REVIEW": false, "comments": "This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n", "OTHER_KEYS": "Neil D Lawrence"}, {"IS_META_REVIEW": true, "comments": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "09 Jan 2017 (modified: 27 Jan 2017)", "TITLE": "Reply to reviewers", "IS_META_REVIEW": false, "comments": "We strongly request that our paper be reviewed for its contents, contributions, significance, originality, and impact. We also would like to highlight the following three points to reviewers and readers. \n\n(i) Claim of \"sentence-level\" lipreading: This was the main concern of reviewers 1 and 2, and likely why they gave us the surprisingly low scores of 4 (7 being borderline for acceptance). Our claim is that we proposed the first \"end-to-end sentence-level\" lipreading approach, and we have edited the paper to make this precise. The phrase \u201cend-to-end\u201d is very important. The previous few attempts at sentence-level lipreading used heuristic pipelines and obtained poor results. LipNet, on the other hand, is fully end-to-end and achieves state-of-the-art results by a significant margin. on the largest available public dataset  (with over 24 hours of video, making it far from trivial). Given the removal of the phrase sentence-level, the reviews of reviewers 1 and 2 regarding prior work are not longer applicable.\n\n(ii) Is the dataset too simple? GRID despite all its shortcomings, is the largest available public dataset with over 24 hours of video (more than 2,000,000 frames and 64,000 possible sentences), making it more comparable to ImageNet than to say MNIST or CIFAR-10. May deep learning papers have used simplified versions of this dataset (eg restrictions to speaker-dependent recognition, or classification only) and even then obtained worse results. Moreover, we have embarked on more commercial datasets with success as reported in this keynote by NVIDIA's CEO at CES (177,393 attendees). See this video (", "OTHER_KEYS": "Nando de Freitas"}, {"TITLE": "Impressive result on lip reading, interesting analysis, some flawed comparison", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.\n\nOverall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Nice application of end to end training on the visual pipeline of traditional AV-ASR systems. Sentence level sequence objectives (MPE/MMI/fMPE) have been applied to this problem in the past", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "- Proven again that end to end training with deep networks gives large\ngains over traditional hybrid systems with hand crafted features. The results \nare very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.\n\n- Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.\n\n- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A nice result, but with limited novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Comments / Questions", "IS_META_REVIEW": false, "comments": "\n\nThis is an interesting paper. Here are some comments:\n\n\n1) Testing on 4 subjects only is rather limited given that there are 33 subjects. For example Ngiam et al. used 18 subjects for training and 18 for testing on CUAVE which contains a similar number of subjects. So a similar setup would be 17 subjects for training and 16 for testing. This would make the results much more convincing.\n\n2) The authors state that the performance of human lipreaders is around 20% and cite Hilder et al. In that paper, the human performance is around 70%, please correct this claim. In addition, in that paper machine lip-reading outperforms humans as well so please make this point clear. \n\n3) The state of the art performance on GRID is not 79.6% as mentioned in the text. It is the one mentioned in the other comments. Actually, as of last week there is a new state-of-the-art ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "02 Dec 2016", "TITLE": "prior work on lipreading", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "16 Nov 2016", "TITLE": "state-of-the-art system", "IS_META_REVIEW": false, "comments": "Dear all,\n\nmany thanks for sharing the paper, which I did find hugely interesting - while the GRiD corpus is small, I do agree that it is currently a good task for taking first steps towards video-based and audio-visual speech recognition, and I believe the work you did here will ultimately be very useful for lipreading (and for lipreading-based speech enhancement) in more general scenarios.\n\nI do have one comment regarding the prior-state-of-the-art system that you cite: As far as I know, our group's Interspeech 2016 holds the record for best lipreading performance on the GRiD corpus, with 86.4% word accuracy (this does include a grammar, but it might still be interesting).\nPlease cite: S. Gergen, S. Zeiler, A. Hussen Abdelaziz, R. Nickel and D. Kolossa: \" Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR,\" in Proc. Interspeech 2016, San Francisco, Sept. 2016.\n", "OTHER_KEYS": "Dorothea Kolossa"}, {"DATE": "08 Nov 2016", "TITLE": "Some comments and suggestions", "IS_META_REVIEW": false, "comments": "- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting.", "OTHER_KEYS": "Mohamed Yousef Bassyouni"}, {"DATE": "06 Nov 2016", "TITLE": "Not so much reacting to the paper but to the 'twitter-storm' it generated. ", "IS_META_REVIEW": false, "comments": "This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n", "OTHER_KEYS": "Neil D Lawrence"}], "authors": "Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas", "accepted": false, "id": "694"}