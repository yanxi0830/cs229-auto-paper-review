{"conference": "ICLR 2017 conference submission", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting model, overreaching conclusions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes.\n\nPositives\n------------\n- Clear definition of the task (handling missing values for classification of time series)\n- Many interesting baselines to test the new model against.\n- The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters).\n- The extensive tests done on the datasets is probably the greatest asset of this paper.\n\nNegatives\n-------------\n- The paper could use some double checking for typos.\n- The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space.\n- No mention of any methods from the statistics litterature.\n\nHere are the two main points of this review that informs my decision:\n\n1. The results, while promising, are below expectations. The paper hasn\u2019t been able to convince me that GRU-simple (without intervals) isn\u2019t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it\u2019s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn\u2019t significantly outperforms it.\n\n2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient\u2019s condition. That would mean that an expert system based on this data would always seem to be one step behind. \nThe second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that \u201csimply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values\u201d is unsubstantiated and actually disproven later in the paper. \nYet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. \nLastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. \n\nGiven that, while I find that the work that has been put into the paper is above average, I wouldn\u2019t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. \n\npros: \n1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting;\n2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. \n\ncons:\n1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. \n2) the datasets used in this paper are small. \n3) the decaying effect might not be able to generalize to other domains. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "05 Dec 2016", "TITLE": "Comparison to multi-layer RNNs", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "11 Nov 2016", "TITLE": "A couple of questions", "IS_META_REVIEW": false, "comments": "Thank you for a very interesting work! \n\n\nI have a couple of questions:\n\n1) Lipton at al. (2016) achieve the best result with zero filling and indicators. As I understood from the equation 3 and the following description, you did not experiment with zero filling and used either forward or mean imputation. Is it correct?\n\n2) It will be interesting to see not only AUC but also Sensitivity and Positive Predictivity, as well as min(Se, +P) since that was the official scoring metric in the PhysioNet Challenge 2012 (although we cannot compare these score directly, obviously)\n\n3) How did you combat such high class imbalance in case of mortality prediction?\n", "OTHER_KEYS": "Alexey Romanov"}, {"IS_META_REVIEW": true, "comments": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting model, overreaching conclusions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes.\n\nPositives\n------------\n- Clear definition of the task (handling missing values for classification of time series)\n- Many interesting baselines to test the new model against.\n- The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters).\n- The extensive tests done on the datasets is probably the greatest asset of this paper.\n\nNegatives\n-------------\n- The paper could use some double checking for typos.\n- The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space.\n- No mention of any methods from the statistics litterature.\n\nHere are the two main points of this review that informs my decision:\n\n1. The results, while promising, are below expectations. The paper hasn\u2019t been able to convince me that GRU-simple (without intervals) isn\u2019t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it\u2019s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn\u2019t significantly outperforms it.\n\n2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient\u2019s condition. That would mean that an expert system based on this data would always seem to be one step behind. \nThe second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that \u201csimply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values\u201d is unsubstantiated and actually disproven later in the paper. \nYet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. \nLastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. \n\nGiven that, while I find that the work that has been put into the paper is above average, I wouldn\u2019t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. \n\npros: \n1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting;\n2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. \n\ncons:\n1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. \n2) the datasets used in this paper are small. \n3) the decaying effect might not be able to generalize to other domains. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "05 Dec 2016", "TITLE": "Comparison to multi-layer RNNs", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "11 Nov 2016", "TITLE": "A couple of questions", "IS_META_REVIEW": false, "comments": "Thank you for a very interesting work! \n\n\nI have a couple of questions:\n\n1) Lipton at al. (2016) achieve the best result with zero filling and indicators. As I understood from the equation 3 and the following description, you did not experiment with zero filling and used either forward or mean imputation. Is it correct?\n\n2) It will be interesting to see not only AUC but also Sensitivity and Positive Predictivity, as well as min(Se, +P) since that was the official scoring metric in the PhysioNet Challenge 2012 (although we cannot compare these score directly, obviously)\n\n3) How did you combat such high class imbalance in case of mortality prediction?\n", "OTHER_KEYS": "Alexey Romanov"}], "authors": "Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu", "accepted": false, "id": "599"}