{"conference": "ICLR 2017 conference submission", "title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "19 Dec 2016", "TITLE": "About the codes for our models", "IS_META_REVIEW": false, "comments": "Dear Reviewers and Readers,\n\nFor the codes of the models and the tasks which we have explored/experimented in our paper, please see our repo:\n", "OTHER_KEYS": "Caglar Gulcehre"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting extension to NTM", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016 (modified: 15 Dec 2016)", "TITLE": "Code available? + other questions", "IS_META_REVIEW": false, "comments": "The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?\n\nDifferent cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? \n\nSome variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?\n\n\u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\n\n\"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  \"Training discrete attention with feed-forward controller and REINFORCE is challenging\". It seems that these two statements contradict each other, could you comment on it? \n\nIn the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them\". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. \n\nIn the introduction, the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Question about the experiments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "formulas, notations, address matrix usage, etc", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "footnote page 2", "IS_META_REVIEW": false, "comments": "I think the footnote is missing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "19 Dec 2016", "TITLE": "About the codes for our models", "IS_META_REVIEW": false, "comments": "Dear Reviewers and Readers,\n\nFor the codes of the models and the tasks which we have explored/experimented in our paper, please see our repo:\n", "OTHER_KEYS": "Caglar Gulcehre"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting extension to NTM", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016 (modified: 15 Dec 2016)", "TITLE": "Code available? + other questions", "IS_META_REVIEW": false, "comments": "The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?\n\nDifferent cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? \n\nSome variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?\n\n\u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\n\n\"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  \"Training discrete attention with feed-forward controller and REINFORCE is challenging\". It seems that these two statements contradict each other, could you comment on it? \n\nIn the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them\". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. \n\nIn the introduction, the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Question about the experiments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "formulas, notations, address matrix usage, etc", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "footnote page 2", "IS_META_REVIEW": false, "comments": "I think the footnote is missing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio", "accepted": false, "id": "662"}