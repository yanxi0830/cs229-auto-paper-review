{"conference": "ICLR 2017 conference submission", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our tasks. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features. Besides, our boundary model also achieves the best performance on the MSMARCO dataset (Nguyen et al. 2016).", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "SUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review: Interesting combination of existing approaches with encouraging results", "comments": "The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.\n\nStrength:\n-\tThe suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)\n-\tThe paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.\n\n\nWeaknesses:\n1.\tIt is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.\n2.\tExperimental evaluation\n2.1.\tIt is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\n2.2.\tIt would be interested if this approach generalizes to other datasets.\n\n\nOther (minor/discussion points)\n-\tThe task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\n-\tI am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process.\n-\tPage 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d\n\n\n\nSummary:\nWhile the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "More analyses / ablation studies / insights needed regarding the functioning of the proposed model", "comments": "Summary:\nThe paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.\n\nStrengths:\n1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.\n2. Significant performance boost over the baseline presented in the SQuAD paper.\n3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.\n\nWeaknesses/Questions/Suggestions:\n1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\n2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\n3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? \n5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\n6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\n\nReview Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "SUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "13 Dec 2016", "TITLE": "An updated pdf version", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nThank you for your valuable comments again! We have made the corresponding revisions and updated a new pdf version. We briefly list the changes here:\n\nAnonReviewer1:\n1.We clarify the dimension of row vector $\\alpha_i$ to be $1\\times Q$.\n2.We add the visualization of the $\\alpha$ values for the question requiring world knowledge in Figure 2 and add the corresponding analysis at the end of the section \"Experiments\".\n\nAnonReviewer2:\nWe revise the description of the state-of-the-art results in the last paragraph in \"Introduction\".\nWe clarify the dimension of $G$ to be $l\\time Q$, the row vector $\\alpha_i$ to be $1\\times Q$, the column vector $w$ to be $l\\times 1$ for equation (2). So is the equation (8).\nWe clarify the statement of footnote 3 about the output gates in the pre-processing layer.\nWe clarify the description of global search on the spans in both the boundary model description part and the Table 2.\n\nAnonReviewer3:\nWe clarify the integration of match-LSTM and pointer network in the last two paragraphs of the \"Introduction\".\nWe directly cite the works of the baselines in Table 2.\n\nThanks,\nShuohang", "OTHER_KEYS": "Shuohang Wang"}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Clarify contribution", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Clarification Questions", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"DATE": "01 Dec 2016", "TITLE": "few doubts", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "SUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review: Interesting combination of existing approaches with encouraging results", "comments": "The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.\n\nStrength:\n-\tThe suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)\n-\tThe paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.\n\n\nWeaknesses:\n1.\tIt is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.\n2.\tExperimental evaluation\n2.1.\tIt is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\n2.2.\tIt would be interested if this approach generalizes to other datasets.\n\n\nOther (minor/discussion points)\n-\tThe task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\n-\tI am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process.\n-\tPage 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d\n\n\n\nSummary:\nWhile the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "More analyses / ablation studies / insights needed regarding the functioning of the proposed model", "comments": "Summary:\nThe paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.\n\nStrengths:\n1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.\n2. Significant performance boost over the baseline presented in the SQuAD paper.\n3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.\n\nWeaknesses/Questions/Suggestions:\n1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\n2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\n3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? \n5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\n6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\n\nReview Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 20 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "SUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "13 Dec 2016", "TITLE": "An updated pdf version", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nThank you for your valuable comments again! We have made the corresponding revisions and updated a new pdf version. We briefly list the changes here:\n\nAnonReviewer1:\n1.We clarify the dimension of row vector $\\alpha_i$ to be $1\\times Q$.\n2.We add the visualization of the $\\alpha$ values for the question requiring world knowledge in Figure 2 and add the corresponding analysis at the end of the section \"Experiments\".\n\nAnonReviewer2:\nWe revise the description of the state-of-the-art results in the last paragraph in \"Introduction\".\nWe clarify the dimension of $G$ to be $l\\time Q$, the row vector $\\alpha_i$ to be $1\\times Q$, the column vector $w$ to be $l\\times 1$ for equation (2). So is the equation (8).\nWe clarify the statement of footnote 3 about the output gates in the pre-processing layer.\nWe clarify the description of global search on the spans in both the boundary model description part and the Table 2.\n\nAnonReviewer3:\nWe clarify the integration of match-LSTM and pointer network in the last two paragraphs of the \"Introduction\".\nWe directly cite the works of the baselines in Table 2.\n\nThanks,\nShuohang", "OTHER_KEYS": "Shuohang Wang"}, {"SUBSTANCE": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Clarify contribution", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Clarification Questions", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"DATE": "01 Dec 2016", "TITLE": "few doubts", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Shuohang Wang, Jing Jiang", "accepted": true, "id": "384"}