{"conference": "ICLR 2017 conference submission", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.\n\nalthough the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:\n\nfirst, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.\n\nsecond, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. \n\nthird, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. \n\nminor issue:\n- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].\n\n[1]"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "A joint model that actually works, limited novelty, a lot of experiments but possibly missing few important points.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).\n\nNovelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP \"complexity\") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "experimental setup should be improved", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.\n\nalthough the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:\n\nfirst, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.\n\nsecond, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. \n\nthird, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. \n\nminor issue:\n- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].\n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.\n\nOn the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair.\n\nI admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc).", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "alternative to label embeddings", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "questions & suggestions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.\n\nalthough the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:\n\nfirst, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.\n\nsecond, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. \n\nthird, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. \n\nminor issue:\n- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].\n\n[1]"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "A joint model that actually works, limited novelty, a lot of experiments but possibly missing few important points.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).\n\nNovelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP \"complexity\") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "experimental setup should be improved", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.\n\nalthough the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:\n\nfirst, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.\n\nsecond, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. \n\nthird, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. \n\nminor issue:\n- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].\n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.\n\nOn the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair.\n\nI admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc).", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "alternative to label embeddings", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "questions & suggestions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher", "accepted": false, "id": "584"}