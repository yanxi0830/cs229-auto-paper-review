{"conference": "ICLR 2017 conference submission", "title": "Investigating Different Context Types and Representations for Learning Word Embeddings", "abstract": "The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their context. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and representations for learning word embeddings. We conduct comprehensive experiments to evaluate their effectiveness under 4 tasks (21 datasets), which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.", "reviews": [{"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Reviewers agree that the findings are not clear enough to be of interest, though the effort to do a controlled study is appreciated.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "GloVe model", "OTHER_KEYS": "li bofang", "comments": "\nDear reviewers,\n\nWe have analyzed GloVe model and updated our paper:\n   -The overall tendency of GloVe with different contexts is similar to Skip-Gram. \n   -GloVe is more sensitive to different contexts than Skip-Gram and CBOW, which is probably due to its explicitly defined/optimized objective function.\nPlease see our revised paper for more details.\n\nThe source code at Github is also updated to support GloVe. (", "IS_META_REVIEW": false, "DATE": "14 Jan 2017", "is_meta_review": false}, {"TITLE": "revised version and response to all reviews", "OTHER_KEYS": "li bofang", "comments": "Dear reviewers,\n\n\tWe have added three sequence labeling tasks (POS, Chunking, and NER) and a word analogy dataset. \n\tThe models are currently evaluated on 4 tasks with 21 datasets. It is indeed hard to find any universal insight. However, after revisiting our experimental results and re-organizing the experiment section, we draw the following conclusions according to specific tasks and are excited to share with you:\n\t\t1.Dependency-based context type does not get all the credit for capturing functional similarity. Bound context representation also plays an important role, especially for GBOW.\n\t\t2.Syntactic word analogy benefits less from bound context representation. Bound context representation already contains syntactic information, which makes it difficult to capture this information based on the input word-context pairs.\n\t\t3.Bound context representation is suitable for sequence labeling task, especially when it is used along with dependency-based context.\n\t\t4.On text classification task, different contexts do not affect the final performance much. Nonetheless, the use of pre-trained word embeddings is crucial and linear context type with unbound representation is still the best choice.\n\tFor more details, please see our revised experiment section.\n\tAs for the GloVe model, we just need a few more days to re-run it for the best configuration. We promise that the results will be added before January 14th (Saturday). So sorry for the delay.\n\tWe sincerely thank you for the detailed and constructive reviews. Please do kindly leave comments and suggests to help us further improve this work. \n\nCheers,\nAuthors of this paper.\n", "IS_META_REVIEW": false, "DATE": "08 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Belowline", "is_meta_review": false, "comments": "This paper analyzes dependency trees vs standard window contexts for word vector learning.\nWhile that's a good goal I believe the paper falls short of a thorough analysis of the subject matter.\nIt does not analyze Glove like objective functions which often work better than the algorithms used here.\nIt doesn't compare in absolute terms to other published vectors or models.\n\nIt fails to gain any particularly interesting insights that will modify other people's work.\nIt fails to push the state of the art or make available new resources for people.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper evaluates how different context types affect the quality of word embeddings on a plethora of benchmarks.\n\nI am ambivalent about this paper. On one hand, it continues an important line of work in decoupling various parameters from the embedding algorithms (this time focusing on context); on the other hand, I am not sure I understand what the conclusion from these experiments is. There does not appear to be a significant and consistent advantage to any one context type. Why is this? Are the benchmarks sensitive enough to detect these differences, if they exist?\n\nWhile I am OK with this paper being accepted, I would rather see a more elaborate version of it, which tries to answer these more fundamental questions.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Solid work, but inconclusive and of narrow interest", "is_meta_review": false, "comments": "This paper investigates the issue of whether and how to use syntactic dependencies in unsupervised word representation learning models like CBOW or Skip-Gram, with a focus one the issue of bound (word+dependency type, 'She-nsubj') vs. unbound (word alone, 'She') representations for context at training time. The empirical results are extremely mixed, and no specific novel method consistently outperforms existing methods.\n\nThe paper is systematic and I have no major concerns about its soundness. However, I don't think that this paper is of broad interest to the ICLR community. The paper is focused on a fairly narrow detail of representation learning that is entirely specific to NLP, and its results are primarily negative. A short paper at an ACL conference would be a more reasonable target.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "11 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Additional Context Types", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "There are many other types of contexts which should be discussed; see \"Open IE as an Intermediate Structure for Semantic Tasks\" (Stanovsky et al., ACL 2015).", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "What is the main claim?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "glove", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Extrinsic evaluation?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "19 Nov 2016", "is_meta_review": false}], "authors": "Bofang Li, Tao Liu, Zhe Zhao, Buzhou Tang, Xiaoyong Du", "KEYWORDS": "This paper investigate different context types and representations for learning word embeddings.", "accepted": false, "id": ""}
