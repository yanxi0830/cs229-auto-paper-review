{"conference": "ICLR 2017 conference submission", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "reviews": [{"is_meta_review": true, "comments": "The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.\n\nI see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.\n\nStill, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.\n\nComing back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.\n\nThe paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.\n\nOverall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).\n\nMinor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.", "IS_META_REVIEW": true}, {"TITLE": "updated results", "OTHER_KEYS": "Jeff Donahue", "comments": "Note: the latest version includes significantly improved ImageNet classification results in Table 2 (up ~2% across the board compared to the previous version).  This is due to the fact that in previous versions, we evaluated using the predictions for a single (center) crop at test time, rather than averaging over 10 crops as we learned by correspondence was how the previous results from Noroozi & Favaro (2016) were obtained.  (Thanks to Mehdi for bringing this up!) There are also very slightly improved VOC classification results (Table 3) due to an unrelated minor bug.", "IS_META_REVIEW": false, "DATE": "01 Apr 2017", "is_meta_review": false}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "All reviewers unanimously praised the novelty and quality of the paper. Minor revisions, following the reviewers' suggestions, will make the paper even better. ", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Update in response to reviewer feedback", "OTHER_KEYS": "Jeff Donahue", "comments": "We thank all reviewers for their thoughtful comments and suggestions. We\u2019ve uploaded a revision with an expanded introduction which clarifies the motivation for BiGAN and the theory in Section 3 in the context of feature learning.\n\nNote that our theoretical arguments go beyond simply showing that BiGAN maintains the properties of the original GAN framework.  In particular, we show that the BiGAN objective is equivalent to that of an \u201cL0 autoencoder\u201d, encouraging the encoder and generator to invert one another (Theorem 3). If fully optimized, the encoder and generator are inverses of one another almost everywhere (Theorem 2).  We believe that these insights are quite important to understanding BiGAN\u2019s behavior and its use as a model for feature learning.  We hope that the revised introduction explains this better.\n\nWith respect to the ImageNet visual feature learning evaluation, our results show BiGAN is state-of-the-art among purely unsupervised learning methods.  The methods that outperform BiGAN (e.g., Doersch et al.) are all based on \u201cself-supervision,\u201d requiring the design of a domain-specific supervised prediction task, and some (Agrawal et al. and Wang & Gupta) are \u201cweakly supervised,\u201d requiring auxiliary information (video, egomotion, or tracking) unavailable in images alone.  We\u2019ve reorganized Table 3 to emphasize this point, and added discussion to Section 4.4 on the benefits of purely unsupervised approaches vs. self-supervised approaches.  (Following your suggestion, we\u2019ve also bolded the best results of each group in Tables 2 and 3.)  We do agree, however, that our results are somewhat preliminary and likely to improve significantly with further model architecture search and improvements to the optimization.\n\nMost optimization details (optimization algorithm, learning rate / step size, etc.) were included in Appendix C.  In the updated version, network initialization details are now included in Appendix C as well.  We\u2019ll also be releasing the training code.", "IS_META_REVIEW": false, "DATE": "06 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "An interesting idea", "is_meta_review": false, "comments": "This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "29 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This is a parallel work with ALI.  The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result. For feature learning part of BiGAN, there still is a lot of space to improve, compare to standard supervised convnet. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Nice work, some structural issues", "is_meta_review": false, "comments": "The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.\n\nI see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.\n\nStill, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.\n\nComing back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.\n\nThe paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.\n\nOverall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).\n\nMinor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Prereview Question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Jeff Donahue, Philipp Kr\u00e4henb\u00fchl, Trevor Darrell", "accepted": true, "id": ""}
