{"conference": "ICLR 2017 conference submission", "title": "Generative Multi-Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game.  In this paper, we propose the \\emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher.  Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.", "reviews": [{"is_meta_review": true, "comments": "In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator.\n\nMy main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a \u2019soft-discriminator\u2019. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier.\n\nOverall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Using an ensemble in the discriminator portion of a GAN is a sensible idea, and it is well explored and described in this paper. Further clarification and exploration of how the multiple discriminators are combined (max versus averaging versus weighted averaging) would be good. The results are fairly strong, across a variety of datasets.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. \n\nThe main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect.\n\nI think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting ideas, needs more empirical results.", "is_meta_review": false, "comments": "The paper extends the GAN framework to accommodate multiple discriminators. The authors motivate this from two points of view:\n\n(1) Having multiple discriminators tackle the task is equivalent to optimizing the value function using random restarts, which can potentially help optimization given the nonconvexity of the value function.\n\n(2) Having multiple discriminators can help overcome the optimization problems arising when a discriminator is too harsh a critic. A generator receiving signal from multiple discriminators is less likely to be receiving poor gradient signal from all discriminators.\n\nThe paper's main idea looks straightforward to implement in practice and makes for a good addition to the GAN training toolbelt.\n\nI am not very convinced by the GAM (and by extension the GMAM) evaluation metric. Without evidence that the GAN game is converging (even approximately), it is hard to make the case that the discriminators tell something meaningful about the generators with respect to the data distribution. In particular, it does not inform on mode coverage or probability mass misallocation.\n\nThe learning curves (Figure 3) look more convincing to me: they provide good evidence that increasing the number of discriminators has a stabilizing effect on the learning dynamics. However, it seems like this figure along with Figure 4 also show that the unmodified generator objective is more stable even with only one discriminator. In that case, is it even necessary to have more than one discriminator to train the generator using an unmodified objective?\n\nOverall, I think the ideas presented in this paper show good potential, but I would like to see an extended analysis in the line of Figures 3 and 4 for more datasets before I think it is ready for publication.\n\nUPDATE: The rating has been revised to a 7 following discussion with the authors.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator.\n\nMy main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a \u2019soft-discriminator\u2019. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier.\n\nOverall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Pre-review Question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Training against the best discriminator", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Section 3.1 questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "is_meta_review": false}, {"TITLE": "setup of boosting", "OTHER_KEYS": "Yaodong Yang", "comments": "Thanks for presenting the great paper.\nIn your experiments, you mentioned using boosting ad D didn't work very well. Could you please add more details of what parameters settings you had tried for the boost OL?", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "is_meta_review": false}, {"TITLE": "More results on CIFAR", "OTHER_KEYS": "Xun Huang", "comments": "This is a great paper and I really enjoy reading it. One thing I would like to see is more quantitative results on CIFAR 10 dataset. For example, is there a table of pairwise GMAM metrics on CIFAR, similar to the one on MNIST? Is it possible to show the Inception Scores on CIFAR? In the paper it's said that: \"Salimans et al. (2016) recommend an Inception score, however, it assumes labels exist for the dataset.\". To my knowledge it's not true, in Salimans et al. (2016) they also compute Inception Scores for models without using label information (See Table 3).", "IS_META_REVIEW": false, "DATE": "08 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Ishan Durugkar, Ian Gemp, Sridhar Mahadevan", "KEYWORDS": "GANs with multiple discriminators accelerate training to more robust performance.", "accepted": true, "id": ""}
