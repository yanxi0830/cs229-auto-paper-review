{"conference": "ICLR 2017 conference submission", "title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "reviews": [{"is_meta_review": true, "comments": "This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner. The idea of the paper is interesting, which mimic the human's behavior. However, there are several cons of the paper:\n\n1. The paper is now well written. There are too many 'TODO', 'CITE' in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version.\n\n2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th)\n\n3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper shows some strong performance numbers, but I agree with the reviewers that it requires more analysis of where those gains come from. The model is very simple, which is a positive, but more studies such as ablation studies and other examples would help a lot.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Added Analysis", "OTHER_KEYS": "Pranav Shyam", "comments": "We have added a 2+ page detailed analysis section with ablation studies and attention maps. ", "IS_META_REVIEW": false, "DATE": "08 Jan 2017 (modified: 26 Feb 2017)", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "experimental section improved but still very weak on analysis and insight", "is_meta_review": false, "comments": "This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input.\n\nThe paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character?  Although it is zooming in, shouldn\u2019t it attend to relevant parts of the character?  Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from.\n\nWhile the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Strong experimental results, but somewhat unclear where the improvements are coming from", "is_meta_review": false, "comments": "This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.\n\nAfter reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Updated Paper", "OTHER_KEYS": "Pranav Shyam", "comments": " An updated version of the paper taking into consideration the reviewers comments has been uploaded", "IS_META_REVIEW": false, "DATE": "15 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "The paper need more improvements to be accepted", "is_meta_review": false, "comments": "This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner. The idea of the paper is interesting, which mimic the human's behavior. However, there are several cons of the paper:\n\n1. The paper is now well written. There are too many 'TODO', 'CITE' in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version.\n\n2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th)\n\n3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "what is the results on LFW dataset?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "It is weird that the paper describe LFW dataset but do not provides the results on it.", "IS_META_REVIEW": false, "DATE": "14 Dec 2016", "is_meta_review": false}, {"TITLE": "discussion of results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "missing results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 3, "authors": "Pranav Shyam, Ambedkar Dukkipati", "KEYWORDS": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "accepted": false, "id": ""}
