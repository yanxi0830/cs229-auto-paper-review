{
  "name" : "532.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "GRADIENT DESCENT", "Yang Fan", "Fei Tian", "Tie-Yan Liu" ],
    "emails" : [ "v-yanfa@microsoft.com", "tie-yan.liu}@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "With large amount of training data as its fuel, deep neural networks (DNN) have achieved stateof-art performances in multiple tasks. Examples include deep convolutional neural network (CNN) for image understanding (Krizhevsky et al., 2012; Ioffe & Szegedy, 2015; He et al., 2015; Ren et al., 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al., 2014; Kiros et al., 2015; Dai & Le, 2015; Shang et al., 2015). To effectively train DNN with large scale of data, typically mini-batch based Stochastic Gradient Descent (SGD) (and its variants such as Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014)) is used. The mini-batch based SGD training is a sequential process, in which mini-batches of data D = {D1, · · ·Dt, . . . , DT } arrive sequentially in a random order. Here Dt = (d1, · · · , dM ) is the mini-batch of data arriving at the t-th time step and consisting of M training instances. After receivingDt at t-th step, the loss and gradient w.r.t. current model parametersWt are Lt = 1M l(dm) and gt = ∂Lt∂Wt , based on which the neural network model gets updated:\nWt+1 =Wt − ηtgt. (1) Here l(·) is the loss function specified by the neural network and ηt is the learning rate at t-th step. With the sequential execution of SGD training, the neural network evolves constantly from a raw state to a fairly mature state, rendering different views even for the same training data. For example, as imposed by the spirit of Curriculum Learning (CL) (Bengio et al., 2009) and Self-Paced Learning (SPL) (Kumar et al., 2010), at the baby stage of the neural network, easy examples play important roles whereas hard examples are comparatively negligible. In contrast, at the adult age, the neural ∗Works done when Yang Fan is an intern at Microsoft Research Asia.\nnetwork tends to favor harder training examples, since easy ones bring minor changes. It remains an important question that, how to optimally and dynamically allocate training data at different stages of SGD training?\nA possible approach is to solve this problem in an active manner: at each time step t, the minibatch data Dt is chosen from all the left untrained data (Tsvetkov et al., 2016; Sachan & Xing, 2016). However, this typically requires a feed-forward pass over the whole remaining dataset at each training step, making it computationally expensive. We therefore consider a passive way in this paper, in which the random ordering of all the mini-batches is pre-given and maintained during the training process. What actually do is, after receiving the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered, based on the features extracted from the feedforward pass only on Dt. Acting in this way avoids unnecessary computational steps on those filtered data and thus speeds-up the training process.\nPrevious works such as curriculum learning (CL) and self-paced learning (SPL) can be leveraged to fulfill such a data filtration task. However, they are typically based on simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010; Jiang et al., 2014a).\nIn this work, we propose a Neural Data Filter (NDF) framework from a more principled and selfadaptive view. In this framework, as illustrated in Figure 1, the SGD training for DNN is naturally casted into a Markov Decision Process (MDP) (Sutton & Barto, 1998) and data filtration strategy is fully controlled through deep reinforcement learning (Mnih et al., 2013; Lillicrap et al., 2015b; Mnih et al., 2016). In such an MDP, a state (namely s1, · · · , st, · · ·) is composed of two parts: the minibatch of data arrived and the parameters of the current neural network model, i.e, st = {Dt,Wt}. In each time step t, NDF receives a representation f(st) for current state from SGD, outputs the action at specifying which instances in Dt will be filtered according to its policy At. Afterwards, the remaining data determined by at will be used by SGD to update the neural network state and generate a reward rt (such as validation accuracy), which will be leveraged by NDF as the feedback for updating its own policy.\nFrom another view, while SGD acts as the trainer for base model, i.e., DNN, it meanwhile is the trainee of reinforcement learning module. In other words, reinforcement learning acts at the teacher module while SGD for DNN is the student. Speaking more ambitiously, such a teacher-student framework based on reinforcement learning goes far beyond data filtration for neural network training: On one hand, the base model the can be benefitted is not limited to neural networks; on the other, the action space in reinforcement learning teacher module covers any strategies in machine learning process, such as hyper-parameter tuning and distributed scheduling. Through carefully designed interaction between the two modules, the training process of general machine learning models can be more elaborately controlled.\nThe rest of the paper is organized as follows: in the next section 2, we will introduce the details of Neural Data Filter (NDF), including the MDP language to model Stochastic Gradient Descent training, and the policy gradient algorithms to learn NDF. Then in section 3, the empirical results\nof training LSTM RNN will be shown to verify the effectiveness of NDF. We discuss related work in subsequent section 4 and conclude the paper in the last section 5."
    }, {
      "heading" : "2 NEURAL DATA FILTER",
      "text" : "We introduce the mathematical details of Neural Data Filter (NDF) for SGD training in this section. As a summary, NDF aims to filter certain amount of training data within a mini-batch, in order to achieve better convergence speed for SGD training. To achieve that, as introduced in last section and Figure 1, we cast Stochastic Gradient Descent training for DNN as a Markov Decision Process (MDP), termed as SGD-MDP.\nSGD-MDP: As traditional MDP, SGD-MDP is composed of the tuple < s, a,P, r, γ >, illustrated as:\n• s is the state, corresponding to the mini-batch data arrived and current neural network state: st = (Dt,Wt).\n• a represents the actions space and for data filtration task, we have a = {am}Mm=1 ∈ {0, 1}M , where M is the batch size and am ∈ {0, 1} denotes whether to filter the mth data instance in Dt or not1. Those filtered instances will have no effects to neural network training.\n• Pass′ = P (s′|s, a) is the state transition probability, determined by two factors: 1) The uniform distribution of sequentially arrived training batch data; 2) The optimization process specified by Gradient Descent principle (c.f. equation 1). The randomness comes from stochastic factors in training, such as dropout (Srivastava et al., 2014).\n• r = r(s, a) is the reward, set to be any signal indicating how well the training goes, such as validation accuracy, or the lost gap for current mini-batch data before/after model update.\n• Furthermore future reward r is discounted by a discounting factor γ ∈ [0, 1] into the cumulative reward.\nNDF samples the action a by its policy function A = PΘ(a|s) with parameters Θ to be learnt. For example, NDF policy A can be set as logistic regression:\nA(s, a; Θ) = PΘ(a|s) = aσ(θf(s) + b) + (1− a)(1− σ(θf(s) + b)), (2) where σ(x) = 1/(1 + exp(−x)) is sigmoid function, Θ = {θ, b}. f(s) is the feature vector to effectively represent state s, discussed as below.\nState Features: The aim of designing state feature vector f(s) is to effectively and efficiently represent SGD-MDP state. Since state s includes both arrived training data and current neural network state, we adopt three categories features to compose f(s):\n• Data features, contains information for data instance, such as its label category (we use 1 of |Y | representations), the length of sentence, or linguistic features for text segments (Tsvetkov et al., 2016). Data features are commonly used in Curriculum Learning (Bengio et al., 2009; Tsvetkov et al., 2016).\n• Neural network features, include the signals reflecting how well current neural network is trained. We collect several simple features, such as passed mini-batch number (i.e., iteration), the average historical training loss and current validation accuracy. They are proven to be effective enough to represent current neural network status.\n• Features to represent the combination of both data and model. By using these features, we target to represent how important the arrived training data is for current neural network. We mainly use three parts of such signals in our classification tasks: 1) the predicted probabilities of each class; 2)the cross-entropy loss, which appears frequently in Self-Paced\n1We consider data instances within the same mini-batch are independent with each other, therefore for statement simplicity, when the context is clear, a will be used to denote the remain/filter decision for single data instance, i.e., a ∈ {0, 1}. Similarly, the notation s will sometimes represent the state for only one training instance.\nLearning algorithms (Kumar et al., 2010; Jiang et al., 2014a; Sachan & Xing, 2016); 3) the margin value 2.\nThe state features f(s) are computed once each mini-batch training data arrives.\nThe whole process for training neural networks is listed in Algorithm 1. In particular, we take the similar generalization framework proposed in (Andrychowicz et al., 2016), in which we use part of training data to train the policy of NDF (Step 1 and 2), and apply the data filtration model to the training process on the whole dataset (Step 3). The detailed algorithm to train NDF policy will be introduced in the next subsection.\nAlgorithm 1 Training Neural Networks with Neural Data Filter. Input: Training Data D. 1. Sample part of NDF training data D′ from D. 2. Optimize NDF policy network A(s; Θ) (c.f. equation 2) based on D′ by policy gradient. 3. Apply A(s; Θ) to full dataset D to train neural network model by SGD. Output: The Neural Network Model."
    }, {
      "heading" : "2.1 TRAINING ALGORITHM FOR NDF POLICY",
      "text" : "Policy gradient methods are adopted to learn NDF policy A. In particular, according to different policy gradient methods, we designed two algorithms: NDF-REINFORCE and NDF-ActorCritic.\nNDF-REINFORCE. NDF-REINFORCE is based on REINFORCE algorithm (Williams, 1992), an elegant Monto-Carlo based policy gradient method which favors action with high sampled reward. The algorithm details are listed in Algorithm 2. Particularly, as indicated in equation 3, NDFREINFORCE will support data filtration policy leading to higher cumulative reward vt.\nAlgorithm 2 NDF-REINFORCE algorithm to train NDF policy. Input: Training data D′. Episode number L. Mini-batch size M . Discount factor γ ∈ [0, 1]. for each episode l = 1, 2, · · · , L do\nInitialize the base neural network model. Shuffle D′ to get the mini-batches sequence D′ = {D1, D2, · · · , DT }. for t = 1, · · · , T do\nSample data filtration action for each data instance in Dt = {d1, · · · , dM}: a = {am}Mm=1, am ∝ A(sm, a; Θ), sm is the state corresponding to the dm\nUpdate neural network model by Gradient Descent based on the selected data in Dt. Receive reward rt.\nend for for t = 1, · · · , T do\nCompute cumulative reward vt = rt + γrt+1 + · · ·+ γT−trT . Update policy parameter Θ:\nΘ← Θ + αvt ∑ m ∂ logA(s, am; Θ) ∂Θ (3)\nend for end for Output: The NDF policy network A(s, a; Θ).\nNDF-ActorCritic.\nThe gradient estimator in REINFORCE poses high variance given its Monto-Carlo nature. Furthermore, it is quite inefficient to update policy network only once in each episode. We therefore design NDF-ActorCritic algorithm based on value function estimation. In NDF-ActorCritic, a parametric value function estimator Q(s, a;W ) (i.e., a critic) with parameters W for estimating state-action\n2The margin for a training instance (x, y) is defined as P (y|x)−maxy′ 6=y P (y′|x) (Cortes et al., 2013)\nvalue function is leveraged to avoid the high variance of vt from Monto-Carlo sampling in NDFREINFORCE. It remains an open and challenging question that how to define optimal value function estimator Q(s, a;W ) for SGD-MDP. Particularly in this work, as a preliminary attempt, the following function is used as the critic:\nQ(s, a;W ) = σ(wT0 relu(f(s)W1a) + b), (4)\nwhere f(s) = (f(s1); f(s2); · · · , f(sM )) is a matrix with M rows and each row f(sm) represents state features for the corresponding training instance dm. W = {w0,W1, b} is the parameter set to be learnt by Temporal-Difference algorithm. Base on such a formulation, the details of NDFActorCritic is listed in Algorithm 3.\nAlgorithm 3 NDF-ActorCritic algorithm to train NDF policy. Input: Training data D′. Episode number L. Mini-batch size M . Discount factor γ ∈ [0, 1]. for each episode l = 1, 2, · · · , L do\nInitialize the base neural network model. Shuffle D′ to get the mini-batches sequence D′ = {D1, D2, · · · , DT }. for t = 1, · · · , T do\nSample data filtration action for each data instance in Dt = {d1, · · · , dM}: a = {am}Mm=1, am ∝ A(sm, a; Θ), sm is the state corresponding to the dm . s = {sm}Mm=1.\nUpdate neural network model by Gradient Descent based on the selected data. Receive reward rt. Update policy(actor) parameter Θ: Θ← Θ + αQ(s, a;W ) ∑ m ∂ logA(s,am;Θ)\n∂Θ . Update critic parameter W :\nq = rt−1 + γQ(s, a;W )−Q(s′, a′;W ), W = W − βq ∂Q(s′, a′;W )\n∂W (5)\na′ ← a, s′ ← s end for\nend for Output: The NDF policy network A(s, a; Θ)."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 EXPERIMENTS SETUP",
      "text" : "We conduct experiments on two different tasks/models: IMDB movie review sentiment classification (with Recurrent Neural Network) and MNIST digital image classification (with Multilayer Perceptron Network). Different data filtration strategies we applied to SGD training include:\n• Unfiltered SGD. The SGD training algorithm without any data filtration. Here rather than vanilla sgd (c.f. equation 1), we use its advanced variants such as Adadelta (Zeiler, 2012) or Adam (Kingma & Ba, 2014) to each of the task.\n• Self-Paced Learning (SPL) (Kumar et al., 2010). It refers to filtering training data by its ‘hardness’, as reflected by loss value. Mathematically speaking, those training data d satisfying l(d) > η will be filtered out, where the threshold η grows from smaller to larger during training process. In our implementation, to improve the robustness of SPL, following the widely used trick (Jiang et al., 2014b), we filter data using its loss rank in one mini-batch, rather than the absolute loss value. That is to say, we filter data instances with topK largest training losses within a M -sized mini-batch, where K linearly drops from M − 1 to 0 during training.\n• NDF-REINFORCE. The policy trained with NDF-REINFORCE, as shown in Algorithm 2. We use a signal to indicate training speed as reward. To be concrete, we set an accuracy threshold τ ∈ [0, 1] and record the first mini-batch index iτ in which validation accuracy\nexceeds τ , then the reward is set as rT = − log(τ/T ). Note here only terminal reward exists (i.e., rt = 0,∀t < T ). • NDF-ActorCritic. The policy trained with NDF-ActorCritic, as shown in Algorithm 3.\nDiscount factor is set as γ = 0.95. Since actor-critic algorithm makes it possible to update policy per time step, rather than per episode, different with the terminal reward set in NDF-REINFORCE, validation accuracy is used as the immediate reward for each time step. To save time cost, only part of validation set is extracted to compute validation accuracy.\n• Randomly Drop. To conduct more comprehensive comparison, for NDF-REINFORCE and NDF-ActorCritic, we record the ratio of filtered data instances per epoch, and then randomly filter data in each mini-batch according to the logged ratio. In this way we form two more baselines, referred to as RandDropREINFORCE and RandDropActorCritic respectively.\nFor all strategies other than Plain SGD, we make sure that the base neural network model will not be updated until M un-trained, yet selected data instances are accumulated. In that way we make sure that the batch size are the same for every strategies (i.e., M ), thus convergence speed is only determined by the effectiveness of data filtration strategies, not by different batch size led by different number of filtered data. For NDF strategies, we initialize b = 2 (c.f. equation 2), with the goal of maintaining training data at the early age, and use Adam (Kingma & Ba, 2014) to optimize the policy. The model is implemented with Theano (Theano Development Team, 2016) and run on one Telsa K40 GPU."
    }, {
      "heading" : "3.2 IMDB SENTIMENT CLASSIFICATION",
      "text" : "IMDB movie review dataset3 is a binary sentiment classification dataset consisting of 50k movie review comments with positive/negative sentiment labels (Maas et al., 2011). We apply LSTM (Hochreiter & Schmidhuber, 1997) RNN to each sentence, and the last hidden state of LSTM is fed into a logistic regression classifier to predict the sentiment label (Dai & Le, 2015). The model size (i.e., word embedding size × hidden state size) is 256× 512 and mini-batch size is set as M = 16. Adadelta (Zeiler, 2012) is used to perform LSTM model training.\nThe IMDB dataset contains 25k training sentences and 25k test sentences. For NDF-REINFORCE and NDF-ActorCritic, from all the training data we randomly sample 10k and 5k as the training/validation set to learn data filtration policy. For NDF-REINFORCE, the validation accuracy threshold is set as τ = 0.8. For NDF-ActorCritic, the size of sub validation set to compute immediate reward is 1k. The episode number is set as L = 30. Early stop on validation set is used to control training process in each episode.\nThe detailed results are shown in Figure 2, whose x-axis represents the number of effective training instances and y-axis denotes the accuracy on test dataset. All the curves are results of 5 repeated runs. From the figure we have the following observations:\n• NDF (shown by the two solid lines) significantly boosts the convergence of SGD training for LSTM. With much less data, NDF achieves satisfactory classification accuracy. For example, NDF-REINFORCE achieves 80% test accuracy with only roughly half training data (about 40k) of Plain SGD consumes (about 80k). Furthermore, NDF significantly outperforms the two Randomly Drop baselines, demonstrating the effectiveness of learnt policies.\n• Self-Paced Learning (shown by the red dashed line) helps for the initialization of LSTM, however, it delays training after the middle phrase.\n• For the two variants of NDF, NDF-REINFORCE performs better than NDF-ActorCritic. Our conjecture for the reason is: 1) For NDF-REINFORCE, we use a terminal reward fully devoted to indicate training convergence; 2) The critic function (c.f., equation 4) may not be expressive enough to approximate true state-action value functions. Deep critic function should be the next step.\n3http://ai.stanford.edu/˜amaas/data/sentiment/\nTo better understand the learnt policies of NDF, in Figure 3 we plot the ratio of filtered data instances per every certain number of iterations. It can be observed that more and more training data are kept during the training process, which are consistent with the intuition of Curriculum Learning and SelfPaced Learning. Furthermore, the learnt feature weights for NDF policies (i.e. θ in equation 2) are listed in Table 1. From the table, we can observe:\n• Longer movie reviews, with positive sentiments are likely to be kept. • Margin plays critical value in determining the importance of data. As reflected by its fairly\nlarge positive weights, training data with large margin is likely to be kept.\n• Note that the feature − log py is the training loss, its negative weights mean that training instances with larger loss values tend to be filtered, thus more and more data will be kept since loss values get smaller and smaller during training, which is consistent with the curves\nin Figure 3. However, such a trend is diminished by the negative weight values for neural network features, i.e., historical training accuracy and normalized iteration."
    }, {
      "heading" : "3.3 IMAGE CLASSIFICATION ON CORRUPTED-MNIST",
      "text" : "We further test different data filtration strategies for multilayer perceptron network training on image recognition task. The dataset we used is MNIST, which consists of 60k training and 10k testing images of handwritten digits from 10 categories (i.e., 0, · · ·, 9). To further demonstrate the effectiveness of the proposed neural data filter in automatically choosing important instances for training, we manually corrupt the original MNIST dataset by injecting some noises to the original pictures as follows: We randomly split 60k training images into ten folds, and flip (i−1)×10% randomly chosen pixels of each image in the i-th fold, i = 1, 2, · · · , 10. The 10k test set are remained unchanged. Flipping a pixel means setting its value r as r = 1.0 − r. Such a corrupted dataset is named as C-MNIST. Some sampled images from C-MNIST are shown in Figure 4.\nA three-layer feedforward neural network with size 784×300×10 is used to classify the C-MNIST dataset. For data filtration policy, different from the single-layer logistic regression in equation 2, in this task, NDF-REINFORCE and NDF-ActorCritic leverage a three-layer neural network with model size 24 × 12 × 1 as policy network, where the first layer node number 24 is the dimension of state features fs 4, and sigmoid function is used as the activation function for the middle layer. 10k randomly selected images out of 60k training set acts as validation set to provide reward signals to NDF-REINFORCE and NDF-ActorCritic. For NDF-REINFORCE, the validation accuracy threshold is set as τ = 0.90. For NDF-ActorCritic, the immediate reward is computed on the whole validation set. The episode number for policy training is set as L = 50 and we control training in each episode by early stopping based on validation set accuracy. We use Adam (Kingma & Ba, 2014) to optimize policy network.\nThe test set accuracy curves (averaged over five repeated runs) of different data filtration strategies are demonstrated in Figure 5. From Figure 5 we can observe:\n• Similar to the result in IMDB sentiment classification, NDF-REINFORCE achieves the best convergence speed;\n• The performance of NDF-ActorCritic is inferior to NDF-REINFORCE. In fact, NDFActorCritic acts similar to sgd training without any data filtration. This further shows although Actor-Critic reduces variance compared with REINFORCE, the difficulty in designing/training better critic functions hurts its performance."
    }, {
      "heading" : "4 RELATED WORK",
      "text" : "Plenty of previous works talk about data scheduling (e.g., filtration and ordering) strategies for machine learning. A remarkable example is Curriculum Learning (CL) (Bengio et al., 2009) showing that a data order from easy instances to hard ones, a.k.a., a curriculum, benefits learning process.\n4fs is similar to the features in Table 1, except that (y0, y1) and (log p0, log p1) are switched into (y0, · · · , y9) and (log p0, · · · , log p9) respectively, given there are ten target classes in mnist classification.\nThe measure of hardness in CL is typically determined by heuristic understandings of data (Bengio et al., 2009; Spitkovsky et al., 2010; Tsvetkov et al., 2016). As a comparison, Self-Paced Learning (SPL) (Kumar et al., 2010; Jiang et al., 2014a;b; Supancic & Ramanan, 2013) quantifies the hardness by the loss on data. In SPL, those training instances with loss values larger than a threshold η will be neglected and η gradually increases in the training process such that finally all training instances will play effects. Apparently SPL can be viewed as a data filtration strategy considered in this paper.\nRecently researchers have noticed the importance of data scheduling for training Deep Neural Network models. For example, in (Loshchilov & Hutter, 2015), a simple batch selection strategy based on the loss values of training data is proposed for speed up neural networks training. (Tsvetkov et al., 2016) leverages Bayesian Optimization to optimize a curriculum function for training distributed word representations. The authors of (Sachan & Xing, 2016) investigated several handcrafted criteria for data ordering in solving Question Answering tasks based on DNN. Our works differs significantly with these works in that 1) We aim to filter data in randomly arrived mini-batches in training process to save computational efforts, rather than actively select mini-batch; 2) We leverage reinforcement learning to automatically derive the optimal policy according to the feedback of training process, rather than use naive and heuristic rules.\nThe proposed Neural Data Filter (NDL) for data filtration is based on deep reinforcement learning (DRL) (Mnih et al., 2013; 2016; Lillicrap et al., 2015a; Silver et al., 2016), which applies deep neural networks to reinforcement learning (Sutton & Barto, 1998). In particular, NDL belongs to policy based reinforcement learning, seeking to search directly for optimal control policy. REINFORCE (Williams, 1992) and actor-critic (Konda & Tsitsiklis, 1999) are two representative policy gradient algorithms, with the difference that actor-critic adopts value function approximation to reduce the high variance of policy gradient estimator in REINFORCE."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper we introduce Neural Data Filter (NDF), a reinforcement learning framework to select/filter data for training deep neural network. Experiments on several deep neural networks training demonstrate that NDF boosts the convergence of Stochastic Gradient Descent. Going beyond data filtration, the proposed framework is able to supervise any sequential training process, thus opens a new view for self-adaptively tuning/controlling machine learning process.\nAs to future work, on one aspect, we aim to test NDF to more tasks and models, such as Convolutional Neural Network (CNN) for image classification. We would also plan to give clearer explanation on the behavior of NDF, such as what data is dropped at different phrases of training, and whether the proposed critic function is good enough. On the other aspect, we aim to apply such a reinforcement learning based teacher-student framework to other strategy design problems for machine learning, such as hyper-parameter tuning, structure learning and distributed scheduling, with the hope of providing better guidance for controlled training process."
    } ],
    "references" : [ {
      "title" : "Learning to learn by gradient descent by gradient descent",
      "author" : [ "Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas" ],
      "venue" : "arXiv preprint arXiv:1606.04474,",
      "citeRegEx" : "Andrychowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andrychowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dai and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Easy samples first: Selfpaced reranking for zero-example multimedia search",
      "author" : [ "Lu Jiang", "Deyu Meng", "Teruko Mitamura", "Alexander G Hauptmann" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Multimedia,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2014
    }, {
      "title" : "Self-paced learning with diversity",
      "author" : [ "Lu Jiang", "Deyu Meng", "Shoou-I Yu", "Zhenzhong Lan", "Shiguang Shan", "Alexander Hauptmann" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors. In Advances in neural information processing",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler" ],
      "venue" : null,
      "citeRegEx" : "Kiros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Actor-critic algorithms",
      "author" : [ "Vijay R Konda", "John N Tsitsiklis" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Konda and Tsitsiklis.,? \\Q1999\\E",
      "shortCiteRegEx" : "Konda and Tsitsiklis.",
      "year" : 1999
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Self-paced learning for latent variable models",
      "author" : [ "M Pawan Kumar", "Benjamin Packer", "Daphne Koller" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2010
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1509.02971,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1509.02971,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "Online batch selection for faster training of neural networks",
      "author" : [ "Ilya Loshchilov", "Frank Hutter" ],
      "venue" : "arXiv preprint arXiv:1511.06343,",
      "citeRegEx" : "Loshchilov and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2015
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Maas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1602.01783,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Easy questions first? a case study on curriculum learning for question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "author" : [ "Mrinmaya Sachan", "Eric Xing" ],
      "venue" : null,
      "citeRegEx" : "Sachan and Xing.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sachan and Xing.",
      "year" : 2016
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : "arXiv preprint arXiv:1503.02364,",
      "citeRegEx" : "Shang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "From baby steps to leapfrog: How less is more in unsupervised dependency parsing",
      "author" : [ "Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky" ],
      "venue" : "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Spitkovsky et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2010
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Self-paced learning for long-term tracking",
      "author" : [ "James S Supancic", "Deva Ramanan" ],
      "venue" : "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "citeRegEx" : "Supancic and Ramanan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Supancic and Ramanan.",
      "year" : 2013
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Learning the curriculum with bayesian optimization for task-specific word representation learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "author" : [ "Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Brian MacWhinney", "Chris Dyer" ],
      "venue" : null,
      "citeRegEx" : "Tsvetkov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Examples include deep convolutional neural network (CNN) for image understanding (Krizhevsky et al., 2012; Ioffe & Szegedy, 2015; He et al., 2015; Ren et al., 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al.",
      "startOffset" : 81,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Examples include deep convolutional neural network (CNN) for image understanding (Krizhevsky et al., 2012; Ioffe & Szegedy, 2015; He et al., 2015; Ren et al., 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al.",
      "startOffset" : 81,
      "endOffset" : 164
    }, {
      "referenceID" : 22,
      "context" : "Examples include deep convolutional neural network (CNN) for image understanding (Krizhevsky et al., 2012; Ioffe & Szegedy, 2015; He et al., 2015; Ren et al., 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al.",
      "startOffset" : 81,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : ", 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al., 2014; Kiros et al., 2015; Dai & Le, 2015; Shang et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : ", 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al., 2014; Kiros et al., 2015; Dai & Le, 2015; Shang et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 150
    }, {
      "referenceID" : 24,
      "context" : ", 2015) and recurrent neural networks (RNN) for natural language processing (Cho et al., 2014; Kiros et al., 2015; Dai & Le, 2015; Shang et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "To effectively train DNN with large scale of data, typically mini-batch based Stochastic Gradient Descent (SGD) (and its variants such as Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014)) is used.",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 33,
      "context" : ", 2011), Adadelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014)) is used.",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "For example, as imposed by the spirit of Curriculum Learning (CL) (Bengio et al., 2009) and Self-Paced Learning (SPL) (Kumar et al.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : ", 2009) and Self-Paced Learning (SPL) (Kumar et al., 2010), at the baby stage of the neural network, easy examples play important roles whereas hard examples are comparatively negligible.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 31,
      "context" : "It remains an important question that, how to optimally and dynamically allocate training data at different stages of SGD training? A possible approach is to solve this problem in an active manner: at each time step t, the minibatch data Dt is chosen from all the left untrained data (Tsvetkov et al., 2016; Sachan & Xing, 2016).",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 1,
      "context" : "However, they are typically based on simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al.",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : ", 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010; Jiang et al., 2014a).",
      "startOffset" : 102,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : "In this framework, as illustrated in Figure 1, the SGD training for DNN is naturally casted into a Markov Decision Process (MDP) (Sutton & Barto, 1998) and data filtration strategy is fully controlled through deep reinforcement learning (Mnih et al., 2013; Lillicrap et al., 2015b; Mnih et al., 2016).",
      "startOffset" : 237,
      "endOffset" : 300
    }, {
      "referenceID" : 21,
      "context" : "In this framework, as illustrated in Figure 1, the SGD training for DNN is naturally casted into a Markov Decision Process (MDP) (Sutton & Barto, 1998) and data filtration strategy is fully controlled through deep reinforcement learning (Mnih et al., 2013; Lillicrap et al., 2015b; Mnih et al., 2016).",
      "startOffset" : 237,
      "endOffset" : 300
    }, {
      "referenceID" : 31,
      "context" : "• Data features, contains information for data instance, such as its label category (we use 1 of |Y | representations), the length of sentence, or linguistic features for text segments (Tsvetkov et al., 2016).",
      "startOffset" : 185,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "Data features are commonly used in Curriculum Learning (Bengio et al., 2009; Tsvetkov et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "Data features are commonly used in Curriculum Learning (Bengio et al., 2009; Tsvetkov et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Learning algorithms (Kumar et al., 2010; Jiang et al., 2014a; Sachan & Xing, 2016); 3) the margin value 2.",
      "startOffset" : 20,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "In particular, we take the similar generalization framework proposed in (Andrychowicz et al., 2016), in which we use part of training data to train the policy of NDF (Step 1 and 2), and apply the data filtration model to the training process on the whole dataset (Step 3).",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 32,
      "context" : "NDF-REINFORCE is based on REINFORCE algorithm (Williams, 1992), an elegant Monto-Carlo based policy gradient method which favors action with high sampled reward.",
      "startOffset" : 46,
      "endOffset" : 62
    }, {
      "referenceID" : 33,
      "context" : "equation 1), we use its advanced variants such as Adadelta (Zeiler, 2012) or Adam (Kingma & Ba, 2014) to each of the task.",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "• Self-Paced Learning (SPL) (Kumar et al., 2010).",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "IMDB movie review dataset3 is a binary sentiment classification dataset consisting of 50k movie review comments with positive/negative sentiment labels (Maas et al., 2011).",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 33,
      "context" : "Adadelta (Zeiler, 2012) is used to perform LSTM model training.",
      "startOffset" : 9,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "A remarkable example is Curriculum Learning (CL) (Bengio et al., 2009) showing that a data order from easy instances to hard ones, a.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "The measure of hardness in CL is typically determined by heuristic understandings of data (Bengio et al., 2009; Spitkovsky et al., 2010; Tsvetkov et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 159
    }, {
      "referenceID" : 26,
      "context" : "The measure of hardness in CL is typically determined by heuristic understandings of data (Bengio et al., 2009; Spitkovsky et al., 2010; Tsvetkov et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 159
    }, {
      "referenceID" : 31,
      "context" : "The measure of hardness in CL is typically determined by heuristic understandings of data (Bengio et al., 2009; Spitkovsky et al., 2010; Tsvetkov et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "As a comparison, Self-Paced Learning (SPL) (Kumar et al., 2010; Jiang et al., 2014a;b; Supancic & Ramanan, 2013) quantifies the hardness by the loss on data.",
      "startOffset" : 43,
      "endOffset" : 112
    }, {
      "referenceID" : 31,
      "context" : "(Tsvetkov et al., 2016) leverages Bayesian Optimization to optimize a curriculum function for training distributed word representations.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 20,
      "context" : "The proposed Neural Data Filter (NDL) for data filtration is based on deep reinforcement learning (DRL) (Mnih et al., 2013; 2016; Lillicrap et al., 2015a; Silver et al., 2016), which applies deep neural networks to reinforcement learning (Sutton & Barto, 1998).",
      "startOffset" : 104,
      "endOffset" : 175
    }, {
      "referenceID" : 25,
      "context" : "The proposed Neural Data Filter (NDL) for data filtration is based on deep reinforcement learning (DRL) (Mnih et al., 2013; 2016; Lillicrap et al., 2015a; Silver et al., 2016), which applies deep neural networks to reinforcement learning (Sutton & Barto, 1998).",
      "startOffset" : 104,
      "endOffset" : 175
    }, {
      "referenceID" : 32,
      "context" : "REINFORCE (Williams, 1992) and actor-critic (Konda & Tsitsiklis, 1999) are two representative policy gradient algorithms, with the difference that actor-critic adopts value function approximation to reduce the high variance of policy gradient estimator in REINFORCE.",
      "startOffset" : 10,
      "endOffset" : 26
    } ],
    "year" : 2017,
    "abstractText" : "Mini-batch based Stochastic Gradient Descent(SGD) has been widely used to train deep neural networks efficiently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it Neural Data Filter (NDF). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to filter some data in sequentially arrived mini-batches so as to maximize future accumulative reward (e.g., validation accuracy). The SGD process accompanied with NDF is able to use less data and converge faster while achieving comparable accuracy as the standard SGD trained on the full dataset. Our experiments show that NDF bootstraps SGD training for different neural network models including Multi Layer Perceptron Network and Recurrent Neural Network trained on various types of tasks including image classification and text understanding.",
    "creator" : "LaTeX with hyperref package"
  }
}