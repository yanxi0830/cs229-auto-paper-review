{
  "name" : "315.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "SHARP MINIMA", "Nitish Shirish Keskar", "Dheevatsa Mudigere", "Mikhail Smelyanskiy" ],
    "emails" : [ "keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32–512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep Learning has emerged as one of the cornerstones of large-scale machine learning. Deep Learning models are used for achieving state-of-the-art results on a wide variety of tasks including computer vision, natural language processing and reinforcement learning; see (Bengio et al., 2016) and the references therein. The problem of training these networks is one of non-convex optimization. Mathematically, this can be represented as:\nmin x∈Rn\nf(x) := 1\nM M∑ i=1 fi(x), (1)\nwhere fi is a loss function for data point i ∈ {1, 2, · · · ,M} which captures the deviation of the model prediction from the data, and x is the vector of weights being optimized. The process of optimizing this function is also called training of the network. Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) and its variants are often used for training deep networks.\n∗Work was performed when author was an intern at Intel Corporation\nThese methods minimize the objective function f by iteratively taking steps of the form:\nxk+1 = xk − αk\n( 1\n|Bk| ∑ i∈Bk ∇fi(xk)\n) , (2)\nwhereBk ⊂ {1, 2, · · · ,M} is the batch sampled from the data set and αk is the step size at iteration k. These methods can be interpreted as gradient descent using noisy gradients, which and are often referred to as mini-batch gradients with batch size |Bk|. SGD and its variants are employed in a small-batch regime, where |Bk| M and typically |Bk| ∈ {32, 64, · · · , 512}. These configurations have been successfully used in practice for a large number of applications; see e.g. (Simonyan & Zisserman, 2014; Graves et al., 2013; Mnih et al., 2013). Many theoretical properties of these methods are known. These include guarantees of: (a) convergence to minimizers of strongly-convex functions and to stationary points for non-convex functions (Bottou et al., 2016), (b) saddle-point avoidance (Ge et al., 2015; Lee et al., 2016), and (c) robustness to input data (Hardt et al., 2015).\nStochastic gradient methods have, however, a major drawback: owing to the sequential nature of the iteration and small batch sizes, there is limited avenue for parallelization. While some efforts have been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al., 2015), the speed-ups and scalability obtained are often limited by the small batch sizes. One natural avenue for improving parallelism is to increase the batch size |Bk|. This increases the amount of computation per iteration, which can be effectively distributed. However, practitioners have observed that this leads to a loss in generalization performance; see e.g. (LeCun et al., 2012). In other words, the performance of the model on testing data sets is often worse when trained with largebatch methods as compared to small-batch methods. In our experiments, we have found the drop in generalization (also called generalization gap) to be as high as 5% even for smaller networks.\nIn this paper, we present numerical results that shed light into this drawback of large-batch methods. We observe that the generalization gap is correlated with a marked sharpness of the minimizers obtained by large-batch methods. This motivates efforts at remedying the generalization problem, as a training algorithm that employs large batches without sacrificing generalization performance would have the ability to scale to a much larger number of nodes than is possible today. This could potentially reduce the training time by orders-of-magnitude; we present an idealized performance model in the Appendix C to support this claim.\nThe paper is organized as follows. In the remainder of this section, we define the notation used in this paper, and in Section 2 we present our main findings and their supporting numerical evidence. In Section 3 we explore the performance of small-batch methods, and in Section 4 we briefly discuss the relationship between our results and recent theoretical work. We conclude with open questions concerning the generalization gap, sharp minima, and possible modifications to make large-batch training viable. In Appendix E, we present some attempts to overcome the problems of large-batch training."
    }, {
      "heading" : "1.1 NOTATION",
      "text" : "We use the notation fi to denote the composition of loss function and a prediction function corresponding to the ith data point. The vector of weights is denoted by x and is subscripted by k to denote an iteration. We use the term small-batch (SB) method to denote SGD, or one of its variants like ADAM (Kingma & Ba, 2015) and ADAGRAD (Duchi et al., 2011), with the proviso that the gradient approximation is based on a small mini-batch. In our setup, the batch Bk is randomly sampled and its size is kept fixed for every iteration. We use the term large-batch (LB) method to denote any training algorithm that uses a large mini-batch. In our experiments, ADAM is used to explore the behavior of both a small or a large batch method."
    }, {
      "heading" : "2 DRAWBACKS OF LARGE-BATCH METHODS",
      "text" : ""
    }, {
      "heading" : "2.1 OUR MAIN OBSERVATION",
      "text" : "As mentioned in Section 1, practitioners have observed a generalization gap when using large-batch methods for training deep learning models. Interestingly, this is despite the fact that large-batch methods usually yield a similar value of the training function as small-batch methods. One may put\nforth the following as possible causes for this phenomenon: (i) LB methods over-fit the model; (ii) LB methods are attracted to saddle points; (iii) LB methods lack the explorative properties of SB methods and tend to zoom-in on the minimizer closest to the initial point; (iv) SB and LB methods converge to qualitatively different minimizers with differing generalization properties. The data presented in this paper supports the last two conjectures.\nThe main observation of this paper is as follows:\nThe lack of generalization ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function. These minimizers are characterized by a significant number of large positive eigenvalues in ∇2f(x), and tend to generalize less well. In contrast, small-batch methods converge to flat minimizers characterized by having numerous small eigenvalues of∇2f(x). We have observed that the loss function landscape of deep neural networks is such that large-batch methods are attracted to regions with sharp minimizers and that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.\nThe concept of sharp and flat minimizers have been discussed in the statistics and machine learning literature. (Hochreiter & Schmidhuber, 1997) (informally) define a flat minimizer x̄ as one for which the function varies slowly in a relatively large neighborhood of x̄. In contrast, a sharp minimizer x̂ is such that the function increases rapidly in a small neighborhood of x̂. A flat minimum can be described with low precision, whereas a sharp minimum requires high precision. The large sensitivity of the training function at a sharp minimizer negatively impacts the ability of the trained model to generalize on new data; see Figure 1 for a hypothetical illustration. This can be explained through the lens of the minimum description length (MDL) theory, which states that statistical models that require fewer bits to describe (i.e., are of low complexity) generalize better (Rissanen, 1983). Since flat minimizers can be specified with lower precision than to sharp minimizers, they tend to have better generalization performance. Alternative explanations are proffered through the Bayesian view of learning (MacKay, 1992), and through the lens of free Gibbs energy; see e.g. Chaudhari et al. (2016)."
    }, {
      "heading" : "2.2 NUMERICAL EXPERIMENTS",
      "text" : "In this section, we present numerical results to support the observations made above. To this end, we make use of the visualization technique employed by (Goodfellow et al., 2014b) and a proposed heuristic metric of sharpness (Equation (4)). We consider 6 multi-class classification network configurations for our experiments; they are described in Table 1. The details about the data sets and network configurations are presented in Appendices A and B respectively. As is common for such problems, we use the mean cross entropy loss as the objective function f .\nThe networks were chosen to exemplify popular configurations used in practice like AlexNet (Krizhevsky et al., 2012) and VGGNet (Simonyan & Zisserman, 2014). Results on other networks\nand using other initialization strategies, activation functions, and data sets showed similar behavior. Since the goal of our work is not to achieve state-of-the-art accuracy or time-to-solution on these tasks but rather to characterize the nature of the minima for LB and SB methods, we only describe the final testing accuracy in the main paper and ignore convergence trends.\nFor all experiments, we used 10% of the training data as batch size for the large-batch experiments and 256 data points for small-batch experiments. We used the ADAM optimizer for both regimes. Experiments with other optimizers for the large-batch experiments, including ADAGRAD (Duchi et al., 2011), SGD (Sutskever et al., 2013) and adaQN (Keskar & Berahas, 2016), led to similar results. All experiments were conducted 5 times from different (uniformly distributed random) starting points and we report both mean and standard-deviation of measured quantities. The baseline performance for our setup is presented Table 2. From this, we can observe that on all networks, both approaches led to high training accuracy but there is a significant difference in the generalization performance. The networks were trained, without any budget or limits, until the loss function ceased to improve.\nWe emphasize that the generalization gap is not due to over-fitting or over-training as commonly observed in statistics. This phenomenon manifest themselves in the form of a testing accuracy curve that, at a certain iterate peaks, and then decays due to the model learning idiosyncrasies of the training data. This is not what we observe in our experiments; see Figure 2 for the training–testing curve of the F2 and C1 networks, which are representative of the rest. As such, early-stopping heuristics aimed at preventing models from over-fitting would not help reduce the generalization gap. The difference between the training and testing accuracies for the networks is due to the specific choice of the network (e.g. AlexNet, VGGNet etc.) and is not the focus of this study. Rather, our goal is to study the source of the testing performance disparity of the two regimes, SB and LB, on a given network model."
    }, {
      "heading" : "2.2.1 PARAMETRIC PLOTS",
      "text" : "We first present parametric 1-D plots of the function as described in (Goodfellow et al., 2014b). Let x?s and x ? ` indicate the solutions obtained by running ADAM using small and large batch sizes respectively. We plot the loss function, on both training and testing data sets, along a line-segment containing the two points. Specifically, for α ∈ [−1, 2], we plot the function f(αx?` + (1 − α)x?s) and also superimpose the classification accuracy at the intermediate points; see Figure 31. For this\n1The code to reproduce the parametric plot on exemplary networks can be found in our GitHub repository: https://github.com/keskarnitish/large-batch-training.\nexperiment, we randomly chose a pair of SB and LB minimizers from the 5 trials used to generate the data in Table 2. The plots show that the LB minima are strikingly sharper than the SB minima in this one-dimensional manifold. The plots in Figure 3 only explore a linear slice of the function, but in Figure 7 in Appendix D, we plot f(sin(απ2 )x ? ` + cos( απ 2 )x ? s) to monitor the function along a curved path between the two minimizers . There too, the relative sharpness of the minima is evident."
    }, {
      "heading" : "2.2.2 SHARPNESS OF MINIMA",
      "text" : "So far, we have used the term sharp minimizer loosely, but we noted that this concept has received attention in the literature (Hochreiter & Schmidhuber, 1997). Sharpness of a minimizer can be characterized by the magnitude of the eigenvalues of ∇2f(x), but given the prohibitive cost of this computation in deep learning applications, we employ a sensitivity measure that, although imperfect, is computationally feasible, even for large networks. It is based on exploring a small neighborhood of a solution and computing the largest value that the function f can attain in that neighborhood. We use that value to measure the sensitivity of the training function at the given local minimizer. Now, since the maximization process is not accurate, and to avoid being mislead by the case when a large value of f is attained only in a tiny subspace of Rn, we perform the maximization both in the entire space Rn as well as in random manifolds. For that purpose, we introduce an n× p matrix A, whose columns are randomly generated. Here p determines the dimension of the manifold, which in our experiments is chosen as p = 100.\nSpecifically, let C denote a box around the solution over which the maximization of f is performed, and letA ∈ Rn×p be the matrix defined above. In order to ensure invariance of sharpness to problem dimension and sparsity, we define the constraint set C as:\nC = {z ∈ Rp : − (|(A+x)i|+ 1) ≤ zi ≤ (|(A+x)i|+ 1) ∀i ∈ {1, 2, · · · , p}}, (3)\nwhere A+ denotes the pseudo-inverse of A. Thus controls the size of the box. We can now define our measure of sharpness (or sensitivity). Metric 2.1. Given x ∈ Rn, > 0 and A ∈ Rn×p, we define the (C , A)-sharpness of f at x as:\nφx,f ( , A) := (maxy∈C f(x+Ay))− f(x)\n1 + f(x) × 100. (4)\nUnless specified otherwise, we use this metric for sharpness for the rest of the paper; ifA is not specified, it is assumed to be the identity matrix, In. (We note in passing that, in the convex optimization literature, the term sharp minimum has a different definition (Ferris, 1988), but that concept is not useful for our purposes.)\nIn Tables 3 and 4, we present the values of the sharpness metric (4) for the minimizers of the various problems. Table 3 explores the full-space (i.e., A = In) whereas Table 4 uses a randomly sampled n × 100 dimensional matrix A. We report results with two values of , (10−3, 5 · 10−4). In all experiments, we solve the maximization problem in Equation (4) inexactly by applying 10 iterations of L-BFGS-B (Byrd et al., 1995). This limit on the number of iterations was necessitated by the\nlarge cost of evaluating the true objective f . Both tables show a 1–2 order-of-magnitude difference between the values of our metric for the SB and LB regimes. These results reinforce the view that the solutions obtained by a large-batch method defines points of larger sensitivity of the training function. In Appedix E, we describe approaches to attempt to remedy this generalization problem of LB methods. These approaches include data augmentation, conservative training and adversarial training. Our preliminary findings show that these approaches help reduce the generalization gap but still lead to relatively sharp minimizers and as such, do not completely remedy the problem.\nNote that Metric 2.1 is closely related to the spectrum of ∇2f(x). Assuming to be small enough, when A = In, the value (4) relates to the largest eigenvalue of ∇2f(x) and when A is randomly sampled it approximates the Ritz value of∇2f(x) projected onto the column-space of A.\nWe conclude this section by noting that the sharp minimizers identified in our experiments do not resemble a cone, i.e., the function does not increase rapidly along all (or even most) directions. By sampling the loss function in a neighborhood of LB solutions, we observe that it rises steeply only along a small dimensional subspace (e.g. 5% of the whole space); on most other directions, the function is relatively flat."
    }, {
      "heading" : "3 SUCCESS OF SMALL-BATCH METHODS",
      "text" : "It is often reported that when increasing the batch size for a problem, there exists a threshold after which there is a deterioration in the quality of the model. This behavior can be observed for the F2 and C1 networks in Figure 4. In both of these experiments, there is a batch size (≈ 15000 for F2 and ≈ 500 for C1) after which there is a large drop in testing accuracy. Notice also that the upward drift in value of the sharpness is considerably reduced around this threshold. Similar thresholds exist for the other networks in Table 1.\nLet us now consider the behavior of SB methods, which use noisy gradients in the step computation. From the results reported in the previous section, it appears that noise in the gradient pushes the iterates out of the basin of attraction of sharp minimizers and encourages movement towards a flatter minimizer where noise will not cause exit from that basin. When the batch size is greater than the threshold mentioned above, the noise in the stochastic gradient is not sufficient to cause ejection from the initial basin leading to convergence to sharper a minimizer.\nTo explore that in more detail, consider the following experiment. We train the network for 100 epochs using ADAM with a batch size of 256, and retain the iterate after each epoch in memory. Using these 100 iterates as starting points we train the network using a LB method for 100 epochs and receive a 100 piggybacked (or warm-started) large-batch solutions. We plot in Figure 5 the testing accuracy and sharpness of these large-batch solutions, along with the testing accuracy of the small-batch iterates. Note that when warm-started with only a few initial epochs, the LB method does not yield a generalization improvement. The concomitant sharpness of the iterates also stays high. On the other hand, after certain number of epochs of warm-starting, the accuracy improves and sharpness of the large-batch iterates drop. This happens, apparently, when the SB method has ended its exploration phase and discovered a flat minimizer; the LB method is then able to converge towards it, leading to good testing accuracy.\nIt has been speculated that LB methods tend to be attracted to minimizers close to the starting point x0, whereas SB methods move away and locate minimizers that are farther away. Our numerical\nexperiments support this view: we observed that the ratio of ‖x?s − x0‖2 and ‖x?` − x0‖2 was in the range of 3–10.\nIn order to further illustrate the qualitative difference between the solutions obtained by SB and LB methods, we plot in Figure 6 our sharpness measure (4) against the loss function (cross entropy) for one random trial of the F2 and C1 networks. For larger values of the loss function, i.e., near the initial point, SB and LB method yield similar values of sharpness. As the loss function reduces, the sharpness of the iterates corresponding to the LB method rapidly increases, whereas for the SB method the sharpness stays relatively constant initially and then reduces, suggesting an exploration phase followed by convergence to a flat minimizer."
    }, {
      "heading" : "4 DISCUSSION AND CONCLUSION",
      "text" : "In this paper, we present numerical experiments that support the view that convergence to sharp minimizers gives rise to the poor generalization of large-batch methods for deep learning. To this end, we provide one-dimensional parametric plots and perturbation (sharpness) measures for a variety of deep learning architectures. In Appendix E, we describe our attempts to remedy the problem, including data augmentation, conservative training and robust optimization. Our preliminary investigation suggests that these strategies do not correct the problem; they improve the generalization of large-batch methods but still lead to relatively sharp minima. Another prospective remedy includes the use of dynamic sampling where the batch size is increased gradually as the iteration progresses (Byrd et al., 2012; Friedlander & Schmidt, 2012). The potential viability of this approach is suggested by our warm-starting experiments (see Figure 5) wherein high testing accuracy is achieved using a large-batch method that is warm-start with a small-batch method.\nRecently, a number of researchers have described interesting theoretical properties of the loss surface of deep neural networks; see e.g. (Choromanska et al., 2015; Soudry & Carmon, 2016; Lee et al., 2016). Their work shows that, under certain regularity assumptions, the loss function of deep learning models is fraught with many local minimizers and that many of these minimizers correspond to a similar loss function value. Our results are in alignment these observations since, in our experiments, both sharp and flat minimizers have very similar loss function values. We do not know, however, if the theoretical models mentioned above provide information about the existence and density of sharp minimizers of the loss surface.\nOur results suggest some questions: (a) can one prove that large-batch (LB) methods typically converge to sharp minimizers of deep learning training functions? (In this paper, we only provided some numerical evidence.); (b) what is the relative density of the two kinds of minima?; (c) can one design neural network architectures for various tasks that are suitable to the properties of LB methods?; (d) can the networks be initialized in a way that enables LB methods to succeed?; (e) is it possible, through algorithmic or regulatory means to steer LB methods away from sharp minimizers?"
    }, {
      "heading" : "A DETAILS ABOUT DATA SETS",
      "text" : "We summarize the data sets used in our experiments in Table 5. TIMIT is a speech recognition data set which is pre-processed using Kaldi (Povey et al., 2011) and trained using a fully-connected network. The rest of the data sets are used without any pre-processing."
    }, {
      "heading" : "B ARCHITECTURE OF NETWORKS",
      "text" : ""
    }, {
      "heading" : "B.1 NETWORK F1",
      "text" : "For this network, we use a 784-dimensional input layer followed by 5 batch-normalized (Ioffe & Szegedy, 2015) layers of 512 neurons each with ReLU activations. The output layer consists of 10 neurons with the softmax activation."
    }, {
      "heading" : "B.2 NETWORK F2",
      "text" : "The network architecture for F2 is similar to F1. We use a 360-dimensional input layer followed by 7 batch-normalized layers of 512 neurons with ReLU activation. The output layer consists of 1973 neurons with the softmax activation."
    }, {
      "heading" : "B.3 NETWORKS C1 AND C3",
      "text" : "The C1 network is a modified version of the popular AlexNet configuration (Krizhevsky et al., 2012). For simplicity, denote a stack of n convolution layers of a filters and a Kernel size of b × c with stride length of d as n×[a, b, c, d]. TheC1 configuration uses 2 sets of [64, 5, 5, 2]–MaxPool(3) followed by 2 dense layers of sizes (384, 192) and finally, an output layer of size 10. We use batchnormalization for all layers and ReLU activations. We also use Dropout (Srivastava et al., 2014) of 0.5 retention probability for the two dense layers. The configuration C3 is identical to C1 except it uses 100 softmax outputs instead of 10."
    }, {
      "heading" : "B.4 NETWORKS C2 AND C4",
      "text" : "The C2 network is a modified version of the popular VGG configuration (Simonyan & Zisserman, 2014). TheC3 network uses the configuration: 2×[64, 3, 3, 1], 2×[128, 3, 3, 1], 3×[256, 3, 3, 1], 3× [512, 3, 3, 1], 3× [512, 3, 3, 1] which a MaxPool(2) after each stack. This stack is followed by a 512- dimensional dense layer and finally, a 10-dimensional output layer. The activation and properties of each layer is as in B.3. As is the case with C3 and C1, the configuration C4 is identical to C2 except that it uses 100 softmax outputs instead of 10."
    }, {
      "heading" : "C PERFORMANCE MODEL",
      "text" : "As mentioned in Section 1, a training algorithm that operates in the large-batch regime without suffering from a generalization gap would have the ability to scale to much larger number of nodes than is currently possible. Such and algorithm might also improve training time through faster convergence. We present an idealized performance model that demonstrates our goal.\nFor LB method to be competitive with SB method, the LB method must (i) converge to minimizers that generalize well, and (ii) do it in a reasonably number of iterations, which we analyze here. Let Is and I` be number of iterations required by SB and LB methods to reach the point of comparable test accuracy, respectively. Let Bs and B` be corresponding batch sizes and P be number of processors being used for training. Assume that P < B`, and let fs(P ) be the parallel efficiency of the SB method. For simplicity, we assume that f`(P ), the parallel efficiency of the LB method, is 1.0. In other words, we assume that the LB method is perfectly scalable due to use of a large batch size.\nFor LB to be faster than SB, we must have\nI` B` P < Is Bs Pfs(P ) .\nIn other words, the ratio of iterations of LB to the iterations of SB should be I` Is < Bs fs(P )B` .\nFor example, if fs(P ) = 0.2 and Bs/B` = 0.1, the LB method must converge in at most half as many iterations as the SB method to see performance benefits. We refer the reader to (Das et al., 2016) for a more detailed model and a commentary on the effect of batch-size on the performance."
    }, {
      "heading" : "D CURVILINEAR PARAMETRIC PLOTS",
      "text" : "The parametric plots for the curvilinear path from x?s to x ? ` , i.e., f(sin( απ 2 )x ? ` + cos( απ 2 )x ? s) can be found in Figure 7."
    }, {
      "heading" : "E ATTEMPTS TO IMPROVE LB METHODS",
      "text" : "In this section, we discuss a few strategies that aim to remedy the problem of poor generalization for large-batch methods. As in Section 2, we use 10% as the percentage batch-size for large-batch experiments and 256 for small-batch methods. For all experiments, we use ADAM as the optimizer irrespective of batch-size."
    }, {
      "heading" : "E.1 DATA AUGMENTATION",
      "text" : "Given that large-batch methods appear to be attracted to sharp minimizers, one can ask whether it is possible to modify the geometry of the loss function so that it is more benign to large-batch methods. The loss function depends both on the geometry of the objective function and to the size and properties of the training set. One approach we consider is data augmentation; see e.g. (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014). The application of this technique is domain specific but generally involves augmenting the data set through controlled modifications on the training data. For instance, in the case of image recognition, the training set can be augmented through translations, rotations, shearing and flipping of the training data. This technique leads to regularization of the network and has been employed for improving testing accuracy on several data sets.\nIn our experiments, we train the 4 image-based (convolutional) networks using aggressive data augmentation and present the results in Table 6. For the augmentation, we use horizontal reflections, random rotations up to 10◦ and random translation of up to 0.2 times the size of the image. It is evident from the table that, while the LB method achieves accuracy comparable to the SB method (also with training data augmented), the sharpness of the minima still exists, suggesting sensitivity to images contained in neither training or testing set. In this section, we exclude parametric plots and sharpness values for the SB method owing to space constraints and the similarity to those presented in Section 2.2."
    }, {
      "heading" : "E.2 CONSERVATIVE TRAINING",
      "text" : "In (Li et al., 2014), the authors argue that the convergence rate of SGD for the large-batch setting can be improved by obtaining iterates through the following proximal sub-problem.\nxk+1 = arg min x\n1 |Bk| ∑ i∈Bk fi(x) + λ 2 ‖x− xk‖22 (5)\nThe motivation for this strategy is, in the context of large-batch methods, to better utilize a batch before moving onto the next one. The minimization problem is solved inexactly using 3–5 iterations of gradient descent, co-ordinate descent or L-BFGS. (Li et al., 2014) report that this not only improves the convergence rate of SGD but also leads to improved empirical performance on convex machine learning problems. The underlying idea of utilizing a batch is not specific to convex problems and we can apply the same framework for deep learning, however, without theoretical guarantees. Indeed, similar algorithms were proposed in (Zhang et al., 2015) and (Mobahi, 2016) for Deep Learning. The former placed emphasis on parallelization of small-batch SGD and asynchrony while the latter on a diffusion-continuation mechanism for training. The results using the conservative training approach are presented in Figure 7. In all experiments, we solve the problem (5) using 3 iterations of ADAM and set the regularization parameter λ to be 10−3. Again, there is a statistically significant improvement in the testing accuracy of the large-batch method but it does not solve the problem of sensitivity."
    }, {
      "heading" : "E.3 ROBUST TRAINING",
      "text" : "A natural way of avoiding sharp minima is through robust optimization techniques. These methods attempt to optimize a worst-case cost as opposed to the nominal (or true) cost. Mathematically, given an > 0, these techniques solve the problem\nmin x φ(x) := max ‖∆x‖≤ f(x+ ∆x) (6)\nGeometrically, classical (nominal) optimization attempts to locate the lowest point of a valley, while robust optimization attempts to lower an –disc down the loss surface. We refer an interested reader to (Bertsimas et al., 2010), and the references therein, for a review of non-convex robust optimization. A direct application of this technique is, however, not feasible in our context since each iteration is prohibitively expensive because it involves solving a large-scale second-order conic program (SOCP).\nIn the context of Deep Learning, there are two inter-dependent forms of robustness: robustness to the data and robustness to the solution. The former exploits the fact that the function f is inherently a statistical model, while the latter treats f as a black-box function. In (Shaham et al., 2015), the authors prove the equivalence between robustness of the solution (with respect to the data) and adversarial training (Goodfellow et al., 2014a).\nGiven the partial success of the data augmentation strategy, it is natural to question the efficacy of adversarial training. As described in (Goodfellow et al., 2014a), adversarial training also aims to artificially increase the training set but, unlike randomized data augmentation, uses the model’s sensitivity to construct new examples. Despite its intuitive appeal, in our experiments, we found that this strategy did not improve generalization. Similarly, we observed no generalization benefit from the stability training proposed by (Zheng et al., 2016). In both cases, the testing accuracy, sharpness values and the parametric plots were similar to the unmodified (baseline) case discussed in Section 2. It remains to be seen whether adversarial training (or any other form of robust training) can increase the viability of large-batch training."
    } ],
    "references" : [ {
      "title" : "Deep learning. Book in preparation for MIT Press, 2016",
      "author" : [ "Yoshua Bengio", "Ian Goodfellow", "Aaron Courville" ],
      "venue" : "URL http://www.deeplearningbook.org",
      "citeRegEx" : "Bengio et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust optimization for unconstrained simulation-based problems",
      "author" : [ "Dimitris Bertsimas", "Omid Nohadani", "Kwong Meng Teo" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Bertsimas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bertsimas et al\\.",
      "year" : 2010
    }, {
      "title" : "Online learning and stochastic approximations",
      "author" : [ "Léon Bottou" ],
      "venue" : "On-line learning in neural networks,",
      "citeRegEx" : "Bottou.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bottou.",
      "year" : 1998
    }, {
      "title" : "Optimization methods for large-scale machine learning",
      "author" : [ "Léon Bottou", "Frank E Curtis", "Jorge Nocedal" ],
      "venue" : "arXiv preprint arXiv:1606.04838,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2016
    }, {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "Richard H Byrd", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Byrd et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 1995
    }, {
      "title" : "Sample size selection in optimization methods for machine learning",
      "author" : [ "Richard H Byrd", "Gillian M Chin", "Jorge Nocedal", "Yuchen Wu" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Byrd et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 2012
    }, {
      "title" : "Entropy-sgd: Biasing gradient descent into wide valleys",
      "author" : [ "Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1611.01838,",
      "citeRegEx" : "Chaudhari et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chaudhari et al\\.",
      "year" : 2016
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed deep learning using synchronous stochastic gradient descent",
      "author" : [ "Dipankar Das", "Sasikanth Avancha", "Dheevatsa Mudigere", "Karthikeyan Vaidynathan", "Srinivas Sridharan", "Dhiraj Kalamkar", "Bharat Kaul", "Pradeep Dubey" ],
      "venue" : "arXiv preprint arXiv:1602.06709,",
      "citeRegEx" : "Das et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2016
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Dean et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2012
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Weak sharp minima and penalty functions in mathematical programming",
      "author" : [ "Michael Charles Ferris" ],
      "venue" : "PhD thesis, University of Cambridge,",
      "citeRegEx" : "Ferris.,? \\Q1988\\E",
      "shortCiteRegEx" : "Ferris.",
      "year" : 1988
    }, {
      "title" : "Hybrid deterministic-stochastic methods for data fitting",
      "author" : [ "Michael P Friedlander", "Mark Schmidt" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Friedlander and Schmidt.,? \\Q2012\\E",
      "shortCiteRegEx" : "Friedlander and Schmidt.",
      "year" : 2012
    }, {
      "title" : "Timit acoustic-phonetic continuous speech corpus",
      "author" : [ "John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathan G Fiscus", "David S Pallett", "Nancy L Dahlgren", "Victor Zue" ],
      "venue" : "Linguistic data consortium, Philadelphia,",
      "citeRegEx" : "Garofolo et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 1993
    }, {
      "title" : "Escaping from saddle pointsonline stochastic gradient for tensor decomposition",
      "author" : [ "Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, pp",
      "citeRegEx" : "Ge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2015
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Qualitatively characterizing neural network optimization problems",
      "author" : [ "Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe" ],
      "venue" : "arXiv preprint arXiv:1412.6544,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton" ],
      "venue" : "IEEE international conference on acoustics, speech and signal processing,",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "M. Hardt", "B. Recht", "Y. Singer" ],
      "venue" : "arXiv preprint arXiv:1509.01240,",
      "citeRegEx" : "Hardt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs, pp. 1–16",
      "author" : [ "Nitish Shirish Keskar", "Albert S. Berahas" ],
      "venue" : null,
      "citeRegEx" : "Keskar and Berahas.,? \\Q2016\\E",
      "shortCiteRegEx" : "Keskar and Berahas.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "In International Conference on Learning Representations (ICLR 2015),",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Yann LeCun", "Corinna Cortes", "Christopher JC Burges" ],
      "venue" : null,
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Yann A LeCun", "Léon Bottou", "Genevieve B Orr", "Klaus-Robert Müller" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient descent converges to minimizers",
      "author" : [ "Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht" ],
      "venue" : "University of California, Berkeley,",
      "citeRegEx" : "Lee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient mini-batch training for stochastic optimization",
      "author" : [ "Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "A practical bayesian framework for backpropagation networks",
      "author" : [ "David JC MacKay" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "MacKay.,? \\Q1992\\E",
      "shortCiteRegEx" : "MacKay.",
      "year" : 1992
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Training recurrent neural networks by diffusion",
      "author" : [ "Hossein Mobahi" ],
      "venue" : "arXiv preprint arXiv:1601.04114,",
      "citeRegEx" : "Mobahi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mobahi.",
      "year" : 2016
    }, {
      "title" : "The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, number EPFL-CONF-192584",
      "author" : [ "Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz" ],
      "venue" : "IEEE Signal Processing Society,",
      "citeRegEx" : "Povey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2011
    }, {
      "title" : "A universal prior for integers and estimation by minimum description length",
      "author" : [ "Jorma Rissanen" ],
      "venue" : "The Annals of statistics,",
      "citeRegEx" : "Rissanen.,? \\Q1983\\E",
      "shortCiteRegEx" : "Rissanen.",
      "year" : 1983
    }, {
      "title" : "Understanding adversarial training: Increasing local stability of neural nets through robust optimization",
      "author" : [ "Uri Shaham", "Yutaro Yamada", "Sahand Negahban" ],
      "venue" : "arXiv preprint arXiv:1511.05432,",
      "citeRegEx" : "Shaham et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shaham et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "No bad local minima: Data independent training error guarantees for multilayer neural networks",
      "author" : [ "Daniel Soudry", "Yair Carmon" ],
      "venue" : "arXiv preprint arXiv:1605.08361,",
      "citeRegEx" : "Soudry and Carmon.,? \\Q2016\\E",
      "shortCiteRegEx" : "Soudry and Carmon.",
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep learning with elastic averaging sgd",
      "author" : [ "2017 Sixin Zhang", "Anna E Choromanska", "Yann LeCun" ],
      "venue" : "In Advances in Neural Information Processing Systems, pp. 685–693,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep Learning models are used for achieving state-of-the-art results on a wide variety of tasks including computer vision, natural language processing and reinforcement learning; see (Bengio et al., 2016) and the references therein.",
      "startOffset" : 183,
      "endOffset" : 204
    }, {
      "referenceID" : 2,
      "context" : "Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) and its variants are often used for training deep networks.",
      "startOffset" : 34,
      "endOffset" : 72
    }, {
      "referenceID" : 38,
      "context" : "Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) and its variants are often used for training deep networks.",
      "startOffset" : 34,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "(Simonyan & Zisserman, 2014; Graves et al., 2013; Mnih et al., 2013).",
      "startOffset" : 0,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "(Simonyan & Zisserman, 2014; Graves et al., 2013; Mnih et al., 2013).",
      "startOffset" : 0,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "These include guarantees of: (a) convergence to minimizers of strongly-convex functions and to stationary points for non-convex functions (Bottou et al., 2016), (b) saddle-point avoidance (Ge et al.",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 14,
      "context" : ", 2016), (b) saddle-point avoidance (Ge et al., 2015; Lee et al., 2016), and (c) robustness to input data (Hardt et al.",
      "startOffset" : 36,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : ", 2016), (b) saddle-point avoidance (Ge et al., 2015; Lee et al., 2016), and (c) robustness to input data (Hardt et al.",
      "startOffset" : 36,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : ", 2016), and (c) robustness to input data (Hardt et al., 2015).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "While some efforts have been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al., 2015), the speed-ups and scalability obtained are often limited by the small batch sizes.",
      "startOffset" : 71,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "While some efforts have been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al., 2015), the speed-ups and scalability obtained are often limited by the small batch sizes.",
      "startOffset" : 71,
      "endOffset" : 128
    }, {
      "referenceID" : 39,
      "context" : "While some efforts have been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al., 2015), the speed-ups and scalability obtained are often limited by the small batch sizes.",
      "startOffset" : 71,
      "endOffset" : 128
    }, {
      "referenceID" : 26,
      "context" : "(LeCun et al., 2012).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "We use the term small-batch (SB) method to denote SGD, or one of its variants like ADAM (Kingma & Ba, 2015) and ADAGRAD (Duchi et al., 2011), with the proviso that the gradient approximation is based on a small mini-batch.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 33,
      "context" : ", are of low complexity) generalize better (Rissanen, 1983).",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 29,
      "context" : "Alternative explanations are proffered through the Bayesian view of learning (MacKay, 1992), and through the lens of free Gibbs energy; see e.",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "Chaudhari et al. (2016).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "The networks were chosen to exemplify popular configurations used in practice like AlexNet (Krizhevsky et al., 2012) and VGGNet (Simonyan & Zisserman, 2014).",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "2 TIMIT (Garofolo et al., 1993) C1 (Shallow) Convolutional Section B.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "Experiments with other optimizers for the large-batch experiments, including ADAGRAD (Duchi et al., 2011), SGD (Sutskever et al.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 38,
      "context" : ", 2011), SGD (Sutskever et al., 2013) and adaQN (Keskar & Berahas, 2016), led to similar results.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "(We note in passing that, in the convex optimization literature, the term sharp minimum has a different definition (Ferris, 1988), but that concept is not useful for our purposes.",
      "startOffset" : 115,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "In all experiments, we solve the maximization problem in Equation (4) inexactly by applying 10 iterations of L-BFGS-B (Byrd et al., 1995).",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "Another prospective remedy includes the use of dynamic sampling where the batch size is increased gradually as the iteration progresses (Byrd et al., 2012; Friedlander & Schmidt, 2012).",
      "startOffset" : 136,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "(Choromanska et al., 2015; Soudry & Carmon, 2016; Lee et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "(Choromanska et al., 2015; Soudry & Carmon, 2016; Lee et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 67
    } ],
    "year" : 2017,
    "abstractText" : "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32–512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",
    "creator" : "LaTeX with hyperref package"
  }
}