{
  "name" : "507.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Philip Blair", "Yuval Merhav" ],
    "emails" : [ "pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "High quality datasets for evaluating word and phrase representations are essential for building better models that can advance natural language understanding. Various researchers have developed and shared datasets for syntactic and semantic intrinsic evaluation. The majority of these datasets are based on word similarity (e.g., Finkelstein et al. (2001); Bruni et al. (2012); Hill et al. (2016)) and analogy tasks (e.g., Mikolov et al. (2013a;b)). While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016). A significant problem with word similarity tasks is that human bias and subjectivity result in low inter-annotator agreement and, consequently, human performance that is lower than automatic methods (Hill et al., 2016). Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015).\nRecently, Camacho-Collados & Navigli (2016) proposed the outlier detection task as an intrinsic evaluation method that improved upon some of the shortcomings of word similarity tasks. The task builds upon the “word intrusion” task initially described in Chang et al. (2009): given a set of words, the goal is to identify the word that does not belong in the set. However, like the vast majority of existing datasets, this dataset requires manual annotations that suffer from human subjectivity and bias, and it is not multilingual.\nInspired by Camacho-Collados & Navigli (2016), we have created a new outlier detection dataset that can be used for intrinsic evaluation of semantic models. The main advantage of our approach is that it is fully automated using Wikidata and Wikipedia, and it is also diverse in the number of included topics, words and phrases, and languages. At a high-level, our approach is simple: we view Wikidata as a graph, where nodes are entities (e.g., 〈Chicago Bulls, Q128109〉, 〈basketball team, Q13393265〉), edges represent “instance of” and “subclass of” relations (e.g., 〈Chicago Bulls, Q128109〉 is an instance of 〈basketball team, Q13393265〉, 〈basketball team, Q13393265〉 is a subclass of 〈sports team, Q12973014〉), and the semantic similarity between two entities is inversely proportional to their graph distance (e.g., 〈Chicago Bulls, Q128109〉 and 〈Los Angeles Lakers, Q121783〉 are semantically similar since they are both instance of 〈basketball team, Q13393265〉). This way we can form semantic clusters\nby picking entities that are members of the same class, and picking outliers with different notions of dissimilarity based on their distance from the cluster entities.\nWe release the first version of our dataset, which we call WikiSem500, to the research community. It contains around 500 per-language cluster groups for English, Spanish, German, Chinese, and Japanese (a total of 13,314 test cases). While we have not studied yet the correlation between performance on this dataset and various downstream tasks, our results show correlation with sentiment analysis. We hope that this diverse and multilingual dataset will help researchers to advance the state-of-the-art of word and phrase representations."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Word similarity tasks have been popular for evaluating distributional similarity models. The basic idea is having annotators assigning similarity scores for word pairs. Models that can automatically assign similarity scores to the same word pairs are evaluated by computing the correlation between their and the human assigned scores. Schnabel et al. (2015) and Hill et al. (2016) review many of these datasets. Hill et al. (2016) also argue that the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. Their main argument is that many such benchmarks measure association and relatedness and not necessarily similarity, which limits their suitability for a wide range of applications. One of their motivating examples is the word pair “coffee” and “cup,” which have high similarity ratings in some benchmarks despite not being very similar. Consequently, they developed guidelines that distinguish between association and similarity and used five hundred Amazon Mechanical Turk annotators to create a new dataset called SimLex-999, which has higher inter annotator agreement than previous datasets. Avraham & Goldberg (2016) improved this line of work further by redesigning the annotation task from rating scales to ranking, in order to alleviate bias, and also redefined the evaluation measure to penalize models more for making wrong predictions on reliable rankings than on unreliable ones.\nAnother popular task based on is word analogies. The analogy dataset proposed by Mikolov et al. (2013a) has become a standard evaluation set. The dataset contains fourteen categories, but only about half of them are for semantic evaluation (e.g. “US Cities”, “Common Capitals”, “All Capitals”). In contrast, WikiSem500 contains hundreds of categories, making it a far more diverse and challenging dataset for the general-purpose evaluation of word representations. The Mikolov dataset has the advantage of additionally including syntactic categories, which we have left for future work.\nCamacho-Collados & Navigli (2016) addressed some of the issues mentioned previously by proposing the outlier detection task. Given a set of words, the goal is to identify the word that does not belong in the set. Their pilot dataset consists of eight different topics each made up of a cluster of eight words and eight possible outliers. Four annotators were used for the creation of the dataset. The main advantage of this dataset is its near perfect human performance. However, we believe a major reason for that is the specific choice of clusters and the small size of the dataset."
    }, {
      "heading" : "3 GENERATING THE DATASET",
      "text" : "In a similar format to the one used in the dataset furnished by Camacho-Collados & Navigli (2016), we generated sets of entities which were semantically similar to one another, known as a “cluster”, followed by up to three pairs (as available) of dissimilar entities, or “outliers”, each with different levels of semantic similarity to the cluster. The core thesis behind our design is that our knowledge base, Wikidata (2016), can be treated like a graph, where the semantic similarity between two elements is inversely proportional to their graph distance.\nInformally, we treat Wikidata entities which are instances of a common entity as a cluster (see Figure 1). Then, starting from that common entity (which we call a ‘class’), we follow “subclass of” relationships to find a sibling class (see “American Football Team” in Figure 1). Two items which are instances of the sibling class (but not instances of the original class) are chosen as outliers. The process is then repeated with a ‘cousin’ class with a common grandparent to the original class (see “Ice Hockey Team” in Figure 1). Finally, we choose two additional outliers by randomly selecting items which are a distance of at least 7 steps away from the original class. These three “outlier classes” are referred to as O1, O2, and O3 outlier classes, respectively.\nA full formalization of our approach is described in Appendix A."
    }, {
      "heading" : "3.1 REFINING THE DATASET QUALITY",
      "text" : "Prior to developing a framework to improve the quality of the generated dataset, we performed a small amount of manual pruning of our Wikidata graph. Disambiguation pages led to bizarre clusters of entities, for their associated relationships are not true semantic connections, but are instead artifacts of the structure of our knowledge base. As such, they were removed. Additionally, classes within a distance of three from the entity for “Entity” itself1 (Q35120) had instances which had quite weak semantic similarity (one example being “human”). We decided that entities at this depth range ought to be removed from the Wikidata graph as well.\nOnce our Wikidata dump was pruned, we employed a few extra steps at generation time to further improve the quality of the dataset; first and foremost were how we chose representative instances and outliers for each class (see σi and σo in Appendix A). While “San Antonio Spurs” and “Chicago Bulls” may both be instances of “basketball team”, so are “BC Andorra” and “Olimpia Milano.” We wanted the cluster entities to be as strongly related as possible, so we sought a class-agnostic heuristic to accomplish this. Ultimately, we found that favoring entities whose associated Wikipedia pages had higher sitelink counts gave us the desired effect.\nAs such, we created clusters by choosing the top eight instances of a given class, ranked by sitelink count. Additionally, we only chose items as outliers when they had at least ten sitelinks so as to remove those which were ‘overly obscure,’ for the ability of word embeddings to identify rare words (Schnabel et al., 2015) would artificially decrease the difficulty of such outliers.\nWe then noticed that many cluster entities had similarities in their labels that could be removed if a different label was chosen. For example, 80% of the entities chosen for “association football club” ended with the phrase “F.C.” This essentially invalidates the cluster, for the high degree of syntactic overlap artificially increases the cosine similarity of all cluster items in word-level embeddings. In order to increase the quality of the surface forms chosen for each entity, we modified our resolution of entity QIDs to surface forms (see τ in Appendix A) to incorporate a variant2 of the work from\n1Q35120 is effectively the “root” node of the Wikidata graph; 95.5% of nodes have “subclass of” chains which terminate at this node.\n2By ‘variant,’ we are referring to the fact that the dictionaries in which we perform the probability lookups are constructed for each language, as opposed to the cross-lingual dictionaries originally described by Spitkovsky & Chang (2012).\nSpitkovsky & Chang (2012):\nτ(QID) = argmax s {P (s | wikipedia page(QID))} (1)\nThat is, the string for an entity is the string which is most likely to link to the Wikipedia page associated with that entity. For example, half of the inlinks to the page for Manchester United FC are the string “Manchester United,” which is the colloquial way of referring to the team.\nNext, we filter out remaining clusters using a small set of heuristics. The following clusters are rejected:\n• Clusters with more than two items are identical after having all digits removed. This handles cases such as entities only differing by years (e.g. “January 2010,” “January 2012,” etc.).\n• Clusters with more than three elements have identical first or last six characters3. Characters are compared instead of words in order to better support inflected languages. This was inspired by clusters for classes such as “counties of Texas” (Q11774097), where even the dictionary-resolved aliases have high degrees of syntactic overlap (namely, over half of the cluster items ended with the word “County”).\n• Clusters in which any item has an occurrence of a ‘stop affix,’ such as the prefix “Category:” or the suffix “一覧” (a Japanese Wikipedia equivalent of “List of”). In truth, this could be done during preprocessing, but doing it at cluster generation time instead has no bearing on the final results. These were originally all included under an additional stop class (“Wikimedia page outside the main knowledge tree”) at prune time, but miscategorizations in the Wikidata hierarchy prevented us from doing so; for example, a now-removed link resulted in every country being pruned from the dataset. As such, we opted to take a more conservative approach and perform this on at cluster-generation time and fine tune our stoplist as needed.\n• Clusters with more than one entity with a string length of one. This prevents clusters such as “letters of the alphabet” being created. Note that this heuristic was disabled for the creation of Chinese and Japanese clusters.\n• Clusters with too few entities, after duplicates introduced by resolving entities to surface forms (τ ) are removed."
    }, {
      "heading" : "3.2 THE WIKISEM500 DATASET",
      "text" : "Using the above heuristics and preprocessing, we have generated a dataset, which we call WikiSem5004. Our dataset is formatted as a series of files containing test groups, comprised of a cluster and a series of outliers. Test cases can be constructed by taking each outlier in a given group with that group’s cluster. Table 1 shows the number of included test groups and test cases for each language. Each group contains a cluster of 7-8 entities and up to two entities from each of the three outlier classes. Table 2 shows example clusters taken from the dataset."
    }, {
      "heading" : "4 EVALUATION",
      "text" : "For clarity, we first restate the definitions of the scoring metrics defined by Camacho-Collados & Navigli (2016) in terms of test groups (in contrast to the original definition, which is defined in terms of test cases). The way in which out-of-vocabulary entities are handled and scores are reported makes this distinction important, as will be seen in Section 4.3.\n3 For Chinese and Japanese, this is modified such that the at least six entities must have identical (non-kana) first or last characters, or more than three must have identical the same first or last two characters. Because English is not inflected, we simply use spaces as approximate word boundaries and check that the first or last of those does not occur too often.\n4The dataset is available for download at https://github.com/belph/wiki-sem-500\nThe core measure during evaluation is known as the compactness score; given a set W of words, it is defined as follows:\n∀w ∈W, c(w) = 1 (|W | − 1)(|W | − 2) ∑ wi∈W\\{w} ∑ wj∈W\\{w}\nwj 6=wi\nsim(wi, wj) (2)\nwhere sim is a vector similarity measure (typically cosine similarity). Note that Camacho-Collados & Navigli (2016) reduces the asymptotic complexity of c(w) from O(n3) to O(n2). We denote P (W,w) to be the (zero-indexed) position of w in the list of elements of W , sorted by compactness score in descending order. From this, we can describe the following definition for Outlier Position (OP), where 〈C,O〉 is a test group and o ∈ O:\nOP (C ∪ {o}) = P (C ∪ {o}, o) (3)\nThis gives rise to the boolean-valued Outlier Detection (OD) function:\nOD(C ∪ {o}) = { 1 OP (C ∪ {o}) = |C| 0 otherwise\n(4)\nFinally, we can now describe the Outlier Position Percentage (OPP) and Accuracy scores:\nOPP (D) =\n∑ 〈C,O〉∈D ∑ o∈O\nOP (C∪{o}) |C|∑\n〈C,O〉∈D|O| (5)\nAccuracy(D) =\n∑ 〈C,O〉∈D ∑ o∈O OD(C ∪ {o})∑\n〈C,O〉∈D|O| (6)"
    }, {
      "heading" : "4.1 HANDLING OUT-OF-VOCABULARY WORDS",
      "text" : "One thing Camacho-Collados & Navigli (2016) does not address is how out-of-vocabulary (OOV) items should be handled. Because our dataset is much larger and contains a wider variety of words, we have extended their work to include additional scoring provisions which better encapsulate the performance of vector sets trained on different corpora.\nThere are two approaches to handling out-of-vocabulary entities: use a sentinel vector to represent all such entities or discard such entities entirely. The first approach is simpler, but it has a number of drawbacks; for one, a poor choice of sentinel can have a drastic impact on results. For example, an implementation which uses the zero vector as a sentinel and defines sim(~x,~0) = 0∀~x places many non-out-of-vocabulary outliers at a large disadvantage in a number of vector spaces, for we have found that negative compactness scores are rare. The second approach avoids deliberately introducing invalid data into the testing evaluation, but comparing scores across vector embeddings with different vocabularies is difficult due to them having different in-vocabulary subsets of the test set.\nWe have opted for the latter approach, computing the results on both the entire dataset and on only the intersection of in-vocabulary entities between all evaluated vector embeddings. This allows us to compare embedding performance both when faced with the same unknown data and when evaluated on the same, in-vocabulary data."
    }, {
      "heading" : "4.2 HUMAN BASELINE",
      "text" : "In order to gauge how well embeddings should perform on our dataset, we conducted a human evaluation. We asked participants to select the outlier from a given test case, providing us with a human baseline for the accuracy score on the dataset. We computed the non-out-of-vocabulary intersection of the embeddings shown in Table 4, from which 60 test groups were sampled. Due to the wide array of domain knowledge needed to perform well on the dataset, participants were allowed to refer to Wikipedia (but explicitly told not to use Wikidata). We collected 447 responses, with an overall precision of 68.9%.\nThe performance found is not as high as on the baseline described in Camacho-Collados & Navigli (2016), so we conducted a second human evaluation on a smaller hand-picked set of clusters in order to determine whether a lack of domain knowledge or a systemic issue with our method was to blame. We had 6 annotators fully annotate 15 clusters generated with our system. Each cluster had one outlier, with a third of the clusters having each of the three outlier classes. Human performance was at 93%, with each annotator missing exactly one cluster. Five out of the six annotators missed the same cluster, which was based on books and contained an O1 outlier (the most difficult class). We interviewed the annotators, and three of them cited a lack of clarity on Wikipedia over whether or not the presented outlier was a book (leading them to guess), while the other two cited a conflation with one of the book titles and a recently popular Broadway production.\nWith the exception of this cluster, the performance was near-perfect, with one annotator missing one cluster. Consequently, we believe that the lower human performance on our dataset is primarily a result of the dataset’s broad domain."
    }, {
      "heading" : "4.3 EMBEDDING RESULTS",
      "text" : "We evaluated our dataset on a number of publicly available vector embeddings: the Google Newstrained CBOW model released by Mikolov et al. (2013a), the 840-billion token Common Crawl corpus-trained GloVe model released by Pennington et al. (2014), and the English, Spanish, German, Japanese, and Chinese MultiCCA vectors5 from Ammar et al. (2016), which are trained on a combination of the Europarl (Koehn, 2005) and Leipzig (Quasthoff et al., 2006) corpora. In ad-\n5The vectors are word2vec CBOW vectors, and the non-English vectors are aligned to the English vector space. Reproducing the original (unaligned) non-English vectors yields near-identical results to the aligned vectors.\ndition, we trained GloVe, CBOW, and Skip-Gram (Mikolov et al., 2013a) models on an identical corpus comprised of an English Wikipedia dump and Gigaword corpus6.\nThe bulk of the embeddings we evaluated were word embeddings (as opposed to phrase embeddings), so we needed to combine each embeddings’ vectors in order to represent multi-word entities. If the embedding does handle phrases (only Google News), we perform a greedy lookup for the longest matching subphrase in the embedding, averaging the subphrase vectors; otherwise, we take a simple average of the vectors for each token in the phrase. If a token is out-of-vocabulary, it is ignored. If all tokens are out-of-vocabulary, the entity is discarded. This check happens as a preprocessing step in order to guarantee that a test case does not have its outlier thrown away. As such, we report the percentage of cluster entities filtered out for being out-of-vocabulary separately from the outliers which are filtered out, for the latter results in an entire test case being discarded.\nIn order to compare how well each vector embedding would do when run on unknown input data, we first collected the scores of each embedding on the entire dataset. Table 3 shows the Outlier Position Percentage (OPP) and accuracy scores of each embedding, along with the number of test groups which were skipped entirely7 and the mean percentage of out-of-vocabulary cluster entities and outliers among all test groups8. As in Camacho-Collados & Navigli (2016), we used cosine similarity for the sim measure in Equation 2.\nThe MultiCCA (Leipzig+Europarl) CBOW vectors have the highest rate of out-of-vocabulary entities, likely due in large part to the fact that its vocabulary is an order of magnitude smaller than the other embeddings (176,691, while the other embeddings had vocabulary sizes of over 1,000,000). Perhaps most surprising is the below-average performance of the Google News vectors. While attempting to understand this phenomenon, we noticed that disabling the phrase vectors boosted performance; as such, we have reported the performance of the vectors with and without phrase vectors enabled.\nInspecting the vocabulary of the Google News vectors, we have inferred that the vocabulary has undergone some form of normalization; performing the normalizations which we can be reasonably certain were done before evaluating has a negligible impact (≈ +0.01%) on the overall score. The Google News scores shown in Table 3 are with the normalization enabled. Ultimately, we hypothesize that the discrepancy in Google News scores comes down to the training corpus. We observe a bias in performance on our training set towards Wikipedia-trained vectors (discussed below; see Table 5), and, additionally, we expect that the Google News corpus did not have the wide regional\n6We used the July 2016 Wikipedia dump (Wikimedia, 2016) and the 2011 Gigaword corpus (Parker et al., 2011).\n7This happens when either all outliers are out-of-vocabulary or fewer than two cluster items are invocabulary. No meaningful evaluation can be performed on the remaining data, so the group is skipped.\n8This includes the out-of-vocabulary rates of the skipped groups.\ncoverage that Wikidata has, limiting the training exposure to many of the more niche classes in the training set.\nIn order to get a better comparison between the embeddings under identical conditions, we then took the intersection of in-vocabulary entities across all embeddings and reevaluated on this subset. 23.88% of cluster entities and 22.37% of outliers were out-of-vocabulary across all vectors, with 23 test groups removed from evaluation. Table 4 shows the results of this evaluation.\nThe scores appear to scale roughly linearly when compared to Table 3, but these results serve as a more reliable ‘apples to apples’ comparison of the algorithms and training corpora.\nBecause Wikidata was the source of the dataset, we analyzed how using Wikipedia as a training corpus influenced the evaluation results. We trained three GloVe models with smaller vocabularies: one trained on only Gigaword, one trained on only Wikipedia, and one trained on both. The results of evaluating on the embeddings’ common intersection are shown in Table 5. We observe a slight (≈ 3.15% relative change) bias in OPP scores with Wikipedia over Gigaword, while finding a significantly larger (≈ 19.12% relative change) bias in accuracy scores. We believe that this bias is acceptable, for OPP scores (which we believe to be more informative) are not as sensitive to the bias and the numerous other factors involved in embedding generation (model, window size, etc.) can still be compared by controlling for the training corpora.\nAdditionally, we wanted to verify that the O1 outlier class (most similar) was the most difficult to distinguish from the cluster entities, followed by the O2 and O3 classes. We generated three separate datasets, each with only one class of outliers, and evaluated each embedding on each dataset. Figure 2 illustrates a strong positive correlation between outlier class and both OPP scores and accuracy.\nFinally, we used the non-English MultiCCA vectors (Ammar et al., 2016) to evaluate the multilingual aspect of our dataset. We expect to see Spanish and German perform similarly to the English Europarl+Leipzig vectors, for the monolingual training corpora used to generate them consisted of\nSpanish and German equivalents of the English training corpus. Table 6 shows the results of the non-English evaluations.\nWe observe a high degree of consistency with the results of the English vectors. The Japanese and Chinese scores are somewhat lower, but this is likely due to their having smaller training corpora and more limited vocabularies than their counterparts in other languages."
    }, {
      "heading" : "4.4 CORRELATION WITH DOWNSTREAM PERFORMANCE",
      "text" : "In light of recent concerns raised about the correlation between intrinsic word embedding evaluations and performance in downstream tasks, we sought to investigate the correlation between WikiSem500 performance and extrinsic evaluations. We used the embeddings from Schnabel et al. (2015) and ran the outlier detection task on them with our dataset.\nAs a baseline measurement of how well our dataset correlates with performance on alternative intrinsic tasks, we our evaluation with the scores reported in Schnabel et al. (2015) on the well-known analogy task (Mikolov et al., 2013a). Figure 3a illustrates strong correlations between analogy task performance and our evaluation’s OPP scores and accuracy.\nFigure 3b displays the Pearson’s correlation between the performance of each embedding on the WikiSem500 dataset and the extrinsic scores of each embedding on noun-phrase chunking and sentiment analysis reported in Schnabel et al. (2015).\nSimilar to the results seen in the paper, performance on our dataset correlates strongly with performance on a semantic-based task (sentiment analysis), with Pearson’s correlation coefficients higher than 0.97 for both accuracy and OPP scores. On the other hand, we observe a weak-to-nonexistent correlation with chunking. This is expected, however, for the dataset we have constructed consists of items which differ in semantic meaning; syntactic meaning is not captured by the dataset. It is worth noting the inconsistency between this and the intrinsic results in Figure 3a, which indicate a stronger correlation with the syntactic subset of the analogy task than its semantic subset. This is\nexpected, for it agrees with the poor correlation between chunking and intrinsic performance shown in Schnabel et al. (2015)."
    }, {
      "heading" : "5 FUTURE WORK",
      "text" : "Due to the favorable results we have seen from the WikiSem500 dataset, we intend to release test groups in additional languages using the method described in this paper. Additionally, we plan to study further the downstream correlation of performance on our dataset with additional downstream tasks.\nMoreover, while we find a substantial correlation between performance on our dataset and on a semantically-based extrinsic task, the relationship between performance and syntactically-based tasks leaves much to be desired. We believe that the approach taken in this paper to construct our dataset could be retrofitted to a system such as WordNet (2010) or Wiktionary (2016) (for multilingual data) in order to construct syntactically similar clusters of items in a similar manner. We hypothesize that performance on such a dataset would correlate much more strongly with syntactically-based extrinsic evaluations such as chunking and part of speech tagging."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We have described a language-agnostic technique for generating a dataset consisting of semantically related items by treating a knowledge base as a graph. In addition, we have used this approach to construct the WikiSem500 dataset, which we have released. We show that performance on this dataset correlates strongly with downstream performance on sentiment analysis. This method allows for creation of much larger scale datasets in a larger variety of languages without the time-intensive task of human creation. Moreover, the parallel between Wikidata’s graph structure and the annotation guidelines from Camacho-Collados & Navigli (2016) preserve the simple-to-understand structure of the original dataset."
    }, {
      "heading" : "A FORMALIZATION",
      "text" : "We now provide a formal description of the approach taken to generate our dataset.\nLet V be the set of entities in Wikidata. For all v1, v2 ∈ V , we denote the relations v1≺Iv2 when v1 is an instance of v2, and v1≺Sv2 when v1 is a subclass of v2. We then define I : V → V ∗ as the following ‘instances’ mapping:\nI(v) = {v′ ∈ V | v′≺Iv} (7)\nFor convenience, we then denote C = {v ∈ V | |I(v)| ≥ 2}; the interpretation being that C is the set of entities which have enough instances to possibly be viable clusters. We now formally state the following definition: Definition 1. A set A ⊆ V is a cluster if A = I(v) for some v ∈ C. We additionally say that v is the class associated with the cluster A.\nLet P : V → V ∗ be the following ‘parent of’ mapping:\nP (v) = {v′ ∈ V | v≺Sv′} (8)\nFurthermore, let P−1 : V → V ∗ be the dual of P :\nP−1(v) = {v′ ∈ V | v′≺Sv} (9)\nFor additional convenience, we denote the following:\nP k(v) = { P (v) k = 1⋃\nv′∈P (v) P k−1(v′) k > 1\n(10)\nAs an abuse of notation, we define the following:\nI∗(v) = I(v) ∪  ⋃ v′∈P−1(v) I∗(v)  (11) That is, I∗(v) is the set of all instances of v and all instances of anything that is a subclass of v (recursively).\nWe then define the measure d : V × V → N to be the graph distance between any entities in V , using the following set of edges:\nESU = {(v1, v2) | v1≺Sv2 ∨ v2≺Sv1} (12)\nFinally, we define9 three additional mappings for outliers parametrized10 by µ ∈ N+:\nO1(v) =  ⋃ p∈P (v)  ⋃ c∈P−1(p)\\{v} I∗(c)  \\ I(v) (13) 9For the definition of O2, note that we do not say that it must be true that p ∈ P 2(v) \\ P (v). In practice, however, avoiding (if not excluding) certain values of p in this manner can help improve the quality of resulting clusters, at the cost of reducing the number of clusters which can be produced.\n10The WikiSem500 dataset was generated with a value of µ = 7.\nO2(v) =  ⋃ p∈P 2(v)  ⋃ c∈P−1(p)\\{v} I∗(c)  \\ I(v) (14) O3(v) =\n ⋃ p∈P (v) {e ∈ I(v′) | µ ≤ d(p, v′)}  \\ I(v) (15) To simplify the model, we assume that all three of the above sets are mutually exclusive. Given these, we can formally state the following definition: Definition 2. Let A = I(v) be a cluster based on a class v. An outlier for A is any o ∈ O1(v) ∪ O2(v)∪O3(v). If o is in O1(v), O2(v), or O3(v), we denote the outlier class of o as O1, O2, or O3 (respectively).\nIntuitively, the three outlier classes denote different degrees of ‘dissimilarity’ from the original cluster; O1 outliers are the most challenging to distinguish, for they are semantically quite similar to the cluster. O2 outliers are slightly easier to distinguish, and O3 outliers should be quite simple to pick out.\nThe final dataset (a set of 〈cluster, outliers〉 pairs) is then created by serializing the following:\nD = τ ( fD (⋃ c∈C 〈fi(σi[I(c)]), fo (σo[O1(c)] ∪ σo[O2(c)] ∪ σo[O3(c)])〉 )) (16)\nWhere σi and σo are functions which select up to a given number of elements from the given set of instances and outliers (respectively), and fD, fi, and fo are functions which filter out dataset elements, instances, and outliers (respectively) based on any number of heuristics (see Section 3.1). Finally, τ takes the resulting tuples and resolves their QIDs to the appropriate surface strings.\nThe benefit of stating the dataset in the above terms is that it is highly configurable. In particular, different languages can be targeted by simply changing τ to resolve Wikidata entities to their labels in that language."
    } ],
    "references" : [ {
      "title" : "Improving reliability of word similarity evaluation by redesigning annotation task and performance measure",
      "author" : [ "Oded Avraham", "Yoav Goldberg" ],
      "venue" : "ACL 2016,",
      "citeRegEx" : "Avraham and Goldberg.,? \\Q2016\\E",
      "shortCiteRegEx" : "Avraham and Goldberg.",
      "year" : 2016
    }, {
      "title" : "A critique of word similarity as a method for evaluating distributional semantic models",
      "author" : [ "Miroslav Batchkarov", "Thomas Kober", "Jeremy Reffin", "Julie Weeds", "David Weir" ],
      "venue" : null,
      "citeRegEx" : "Batchkarov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Batchkarov et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributional semantics in technicolor",
      "author" : [ "Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume",
      "citeRegEx" : "Bruni et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2012
    }, {
      "title" : "Find the word that does not belong: A framework for an intrinsic evaluation of word vector representations",
      "author" : [ "José Camacho-Collados", "Roberto Navigli" ],
      "venue" : "In ACL Workshop on Evaluating Vector Space Representations for NLP,",
      "citeRegEx" : "Camacho.Collados and Navigli.,? \\Q2016\\E",
      "shortCiteRegEx" : "Camacho.Collados and Navigli.",
      "year" : 2016
    }, {
      "title" : "Reading tea leaves: How humans interpret topic models",
      "author" : [ "Jonathan Chang", "Jordan Boyd-Graber", "Chong Wang", "Sean Gerrish", "David M. Blei" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Chang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2009
    }, {
      "title" : "Intrinsic evaluation of word vectors fails to predict extrinsic performance",
      "author" : [ "Billy Chiu", "Anna Korhonen", "Sampo Pyysalo" ],
      "venue" : "ACL 2016,",
      "citeRegEx" : "Chiu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chiu et al\\.",
      "year" : 2016
    }, {
      "title" : "Problems with evaluation of word embeddings using word similarity tasks",
      "author" : [ "Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer" ],
      "venue" : "arXiv preprint arXiv:1605.02276,",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin" ],
      "venue" : "In Proceedings of the 10th international conference on World Wide Web,",
      "citeRegEx" : "Finkelstein et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2001
    }, {
      "title" : "Intrinsic evaluations of word embeddings: What can we do better",
      "author" : [ "Anna Gladkova", "Aleksandr Drozd", "Computing Center" ],
      "venue" : "ACL 2016,",
      "citeRegEx" : "Gladkova et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gladkova et al\\.",
      "year" : 2016
    }, {
      "title" : "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Hill et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "In MT summit,",
      "citeRegEx" : "Koehn.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Issues in evaluating semantic spaces using word analogies",
      "author" : [ "Tal Linzen" ],
      "venue" : "arXiv preprint arXiv:1606.07736,",
      "citeRegEx" : "Linzen.,? \\Q2016\\E",
      "shortCiteRegEx" : "Linzen.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig" ],
      "venue" : "In HLT-NAACL,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Corpus portal for search in monolingual corpora",
      "author" : [ "Uwe Quasthoff", "Matthias Richter", "Christian Biemann" ],
      "venue" : "In Proceedings of the fifth international conference on language resources and evaluation,",
      "citeRegEx" : "Quasthoff et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Quasthoff et al\\.",
      "year" : 2006
    }, {
      "title" : "Evaluation methods for unsupervised word embeddings",
      "author" : [ "Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "Schnabel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schnabel et al\\.",
      "year" : 2015
    }, {
      "title" : "A cross-lingual dictionary for english wikipedia",
      "author" : [ "Valentin I Spitkovsky", "Angel X Chang" ],
      "venue" : "concepts. Conference on Language Resources Evaluation,",
      "citeRegEx" : "Spitkovsky and Chang.,? \\Q2012\\E",
      "shortCiteRegEx" : "Spitkovsky and Chang.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016).",
      "startOffset" : 267,
      "endOffset" : 396
    }, {
      "referenceID" : 8,
      "context" : "While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016).",
      "startOffset" : 267,
      "endOffset" : 396
    }, {
      "referenceID" : 9,
      "context" : "While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016).",
      "startOffset" : 267,
      "endOffset" : 396
    }, {
      "referenceID" : 11,
      "context" : "While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016).",
      "startOffset" : 267,
      "endOffset" : 396
    }, {
      "referenceID" : 1,
      "context" : "While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016).",
      "startOffset" : 267,
      "endOffset" : 396
    }, {
      "referenceID" : 9,
      "context" : "A significant problem with word similarity tasks is that human bias and subjectivity result in low inter-annotator agreement and, consequently, human performance that is lower than automatic methods (Hill et al., 2016).",
      "startOffset" : 199,
      "endOffset" : 218
    }, {
      "referenceID" : 5,
      "context" : "Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015).",
      "startOffset" : 90,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015).",
      "startOffset" : 90,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : ", Finkelstein et al. (2001); Bruni et al.",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "(2001); Bruni et al. (2012); Hill et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "(2001); Bruni et al. (2012); Hill et al. (2016)) and analogy tasks (e.",
      "startOffset" : 8,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016). A significant problem with word similarity tasks is that human bias and subjectivity result in low inter-annotator agreement and, consequently, human performance that is lower than automatic methods (Hill et al., 2016). Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015). Recently, Camacho-Collados & Navigli (2016) proposed the outlier detection task as an intrinsic evaluation method that improved upon some of the shortcomings of word similarity tasks.",
      "startOffset" : 48,
      "endOffset" : 472
    }, {
      "referenceID" : 1,
      "context" : ", 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016). A significant problem with word similarity tasks is that human bias and subjectivity result in low inter-annotator agreement and, consequently, human performance that is lower than automatic methods (Hill et al., 2016). Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015). Recently, Camacho-Collados & Navigli (2016) proposed the outlier detection task as an intrinsic evaluation method that improved upon some of the shortcomings of word similarity tasks. The task builds upon the “word intrusion” task initially described in Chang et al. (2009): given a set of words, the goal is to identify the word that does not belong in the set.",
      "startOffset" : 48,
      "endOffset" : 702
    }, {
      "referenceID" : 1,
      "context" : ", 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016). A significant problem with word similarity tasks is that human bias and subjectivity result in low inter-annotator agreement and, consequently, human performance that is lower than automatic methods (Hill et al., 2016). Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015). Recently, Camacho-Collados & Navigli (2016) proposed the outlier detection task as an intrinsic evaluation method that improved upon some of the shortcomings of word similarity tasks. The task builds upon the “word intrusion” task initially described in Chang et al. (2009): given a set of words, the goal is to identify the word that does not belong in the set. However, like the vast majority of existing datasets, this dataset requires manual annotations that suffer from human subjectivity and bias, and it is not multilingual. Inspired by Camacho-Collados & Navigli (2016), we have created a new outlier detection dataset that can be used for intrinsic evaluation of semantic models.",
      "startOffset" : 48,
      "endOffset" : 1006
    }, {
      "referenceID" : 13,
      "context" : "Schnabel et al. (2015) and Hill et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "(2015) and Hill et al. (2016) review many of these datasets.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "(2015) and Hill et al. (2016) review many of these datasets. Hill et al. (2016) also argue that the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity.",
      "startOffset" : 11,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "(2015) and Hill et al. (2016) review many of these datasets. Hill et al. (2016) also argue that the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. Their main argument is that many such benchmarks measure association and relatedness and not necessarily similarity, which limits their suitability for a wide range of applications. One of their motivating examples is the word pair “coffee” and “cup,” which have high similarity ratings in some benchmarks despite not being very similar. Consequently, they developed guidelines that distinguish between association and similarity and used five hundred Amazon Mechanical Turk annotators to create a new dataset called SimLex-999, which has higher inter annotator agreement than previous datasets. Avraham & Goldberg (2016) improved this line of work further by redesigning the annotation task from rating scales to ranking, in order to alleviate bias, and also redefined the evaluation measure to penalize models more for making wrong predictions on reliable rankings than on unreliable ones.",
      "startOffset" : 11,
      "endOffset" : 840
    }, {
      "referenceID" : 9,
      "context" : "(2015) and Hill et al. (2016) review many of these datasets. Hill et al. (2016) also argue that the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. Their main argument is that many such benchmarks measure association and relatedness and not necessarily similarity, which limits their suitability for a wide range of applications. One of their motivating examples is the word pair “coffee” and “cup,” which have high similarity ratings in some benchmarks despite not being very similar. Consequently, they developed guidelines that distinguish between association and similarity and used five hundred Amazon Mechanical Turk annotators to create a new dataset called SimLex-999, which has higher inter annotator agreement than previous datasets. Avraham & Goldberg (2016) improved this line of work further by redesigning the annotation task from rating scales to ranking, in order to alleviate bias, and also redefined the evaluation measure to penalize models more for making wrong predictions on reliable rankings than on unreliable ones. Another popular task based on is word analogies. The analogy dataset proposed by Mikolov et al. (2013a) has become a standard evaluation set.",
      "startOffset" : 11,
      "endOffset" : 1214
    }, {
      "referenceID" : 9,
      "context" : "(2015) and Hill et al. (2016) review many of these datasets. Hill et al. (2016) also argue that the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. Their main argument is that many such benchmarks measure association and relatedness and not necessarily similarity, which limits their suitability for a wide range of applications. One of their motivating examples is the word pair “coffee” and “cup,” which have high similarity ratings in some benchmarks despite not being very similar. Consequently, they developed guidelines that distinguish between association and similarity and used five hundred Amazon Mechanical Turk annotators to create a new dataset called SimLex-999, which has higher inter annotator agreement than previous datasets. Avraham & Goldberg (2016) improved this line of work further by redesigning the annotation task from rating scales to ranking, in order to alleviate bias, and also redefined the evaluation measure to penalize models more for making wrong predictions on reliable rankings than on unreliable ones. Another popular task based on is word analogies. The analogy dataset proposed by Mikolov et al. (2013a) has become a standard evaluation set. The dataset contains fourteen categories, but only about half of them are for semantic evaluation (e.g. “US Cities”, “Common Capitals”, “All Capitals”). In contrast, WikiSem500 contains hundreds of categories, making it a far more diverse and challenging dataset for the general-purpose evaluation of word representations. The Mikolov dataset has the advantage of additionally including syntactic categories, which we have left for future work. Camacho-Collados & Navigli (2016) addressed some of the issues mentioned previously by proposing the outlier detection task.",
      "startOffset" : 11,
      "endOffset" : 1731
    }, {
      "referenceID" : 16,
      "context" : "Additionally, we only chose items as outliers when they had at least ten sitelinks so as to remove those which were ‘overly obscure,’ for the ability of word embeddings to identify rare words (Schnabel et al., 2015) would artificially decrease the difficulty of such outliers.",
      "startOffset" : 192,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "Additionally, we only chose items as outliers when they had at least ten sitelinks so as to remove those which were ‘overly obscure,’ for the ability of word embeddings to identify rare words (Schnabel et al., 2015) would artificially decrease the difficulty of such outliers. We then noticed that many cluster entities had similarities in their labels that could be removed if a different label was chosen. For example, 80% of the entities chosen for “association football club” ended with the phrase “F.C.” This essentially invalidates the cluster, for the high degree of syntactic overlap artificially increases the cosine similarity of all cluster items in word-level embeddings. In order to increase the quality of the surface forms chosen for each entity, we modified our resolution of entity QIDs to surface forms (see τ in Appendix A) to incorporate a variant2 of the work from Q35120 is effectively the “root” node of the Wikidata graph; 95.5% of nodes have “subclass of” chains which terminate at this node. By ‘variant,’ we are referring to the fact that the dictionaries in which we perform the probability lookups are constructed for each language, as opposed to the cross-lingual dictionaries originally described by Spitkovsky & Chang (2012).",
      "startOffset" : 193,
      "endOffset" : 1257
    }, {
      "referenceID" : 10,
      "context" : "(2016), which are trained on a combination of the Europarl (Koehn, 2005) and Leipzig (Quasthoff et al.",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "(2016), which are trained on a combination of the Europarl (Koehn, 2005) and Leipzig (Quasthoff et al., 2006) corpora.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "We evaluated our dataset on a number of publicly available vector embeddings: the Google Newstrained CBOW model released by Mikolov et al. (2013a), the 840-billion token Common Crawl corpus-trained GloVe model released by Pennington et al.",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "We evaluated our dataset on a number of publicly available vector embeddings: the Google Newstrained CBOW model released by Mikolov et al. (2013a), the 840-billion token Common Crawl corpus-trained GloVe model released by Pennington et al. (2014), and the English, Spanish, German, Japanese, and Chinese MultiCCA vectors5 from Ammar et al.",
      "startOffset" : 124,
      "endOffset" : 247
    }, {
      "referenceID" : 11,
      "context" : "We evaluated our dataset on a number of publicly available vector embeddings: the Google Newstrained CBOW model released by Mikolov et al. (2013a), the 840-billion token Common Crawl corpus-trained GloVe model released by Pennington et al. (2014), and the English, Spanish, German, Japanese, and Chinese MultiCCA vectors5 from Ammar et al. (2016), which are trained on a combination of the Europarl (Koehn, 2005) and Leipzig (Quasthoff et al.",
      "startOffset" : 124,
      "endOffset" : 347
    }, {
      "referenceID" : 12,
      "context" : "dition, we trained GloVe, CBOW, and Skip-Gram (Mikolov et al., 2013a) models on an identical corpus comprised of an English Wikipedia dump and Gigaword corpus6. The bulk of the embeddings we evaluated were word embeddings (as opposed to phrase embeddings), so we needed to combine each embeddings’ vectors in order to represent multi-word entities. If the embedding does handle phrases (only Google News), we perform a greedy lookup for the longest matching subphrase in the embedding, averaging the subphrase vectors; otherwise, we take a simple average of the vectors for each token in the phrase. If a token is out-of-vocabulary, it is ignored. If all tokens are out-of-vocabulary, the entity is discarded. This check happens as a preprocessing step in order to guarantee that a test case does not have its outlier thrown away. As such, we report the percentage of cluster entities filtered out for being out-of-vocabulary separately from the outliers which are filtered out, for the latter results in an entire test case being discarded. In order to compare how well each vector embedding would do when run on unknown input data, we first collected the scores of each embedding on the entire dataset. Table 3 shows the Outlier Position Percentage (OPP) and accuracy scores of each embedding, along with the number of test groups which were skipped entirely7 and the mean percentage of out-of-vocabulary cluster entities and outliers among all test groups8. As in Camacho-Collados & Navigli (2016), we used cosine similarity for the sim measure in Equation 2.",
      "startOffset" : 47,
      "endOffset" : 1501
    }, {
      "referenceID" : 14,
      "context" : "We used the embeddings from Schnabel et al. (2015) and ran the outlier detection task on them with our dataset.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "We used the embeddings from Schnabel et al. (2015) and ran the outlier detection task on them with our dataset. As a baseline measurement of how well our dataset correlates with performance on alternative intrinsic tasks, we our evaluation with the scores reported in Schnabel et al. (2015) on the well-known analogy task (Mikolov et al.",
      "startOffset" : 28,
      "endOffset" : 291
    }, {
      "referenceID" : 12,
      "context" : "(2015) on the well-known analogy task (Mikolov et al., 2013a). Figure 3a illustrates strong correlations between analogy task performance and our evaluation’s OPP scores and accuracy. Figure 3b displays the Pearson’s correlation between the performance of each embedding on the WikiSem500 dataset and the extrinsic scores of each embedding on noun-phrase chunking and sentiment analysis reported in Schnabel et al. (2015). Similar to the results seen in the paper, performance on our dataset correlates strongly with performance on a semantic-based task (sentiment analysis), with Pearson’s correlation coefficients higher than 0.",
      "startOffset" : 39,
      "endOffset" : 422
    }, {
      "referenceID" : 16,
      "context" : "expected, for it agrees with the poor correlation between chunking and intrinsic performance shown in Schnabel et al. (2015).",
      "startOffset" : 102,
      "endOffset" : 125
    } ],
    "year" : 2016,
    "abstractText" : "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of “outlier” elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}