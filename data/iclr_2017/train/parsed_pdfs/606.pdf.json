{
  "name" : "606.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LEARNING APPROXIMATE DISTRIBUTION-SENSITIVE DATA STRUCTURES",
    "authors" : [ "Zenna Tavares", "Armando Solar Lezama" ],
    "emails" : [ "zenna@mit.edu", "asolar@csail.mit.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recent progress in artificial intelligence is driven by the ability to learn representations from data. Yet not all kinds of representations are equal, and many of the fundamental properties of representations (both as theoretical constructs and as observed experimentally in humans) are missing. Perhaps the most critical property of a system of representations is compositionality, which as described succinctly in (Fodor & Lepore, 2002), is when (i) it contains both primitive symbols and symbols that are complex; and (ii) the latter inherit their syntactic/semantic properties from the former. Compositionality is powerful because it enables a system of representation to support an infinite number of semantically distinct representations by means of combination. This argument has been supported experimentally; a growing body of evidence (Spelke & Kinzler, 2007) has shown that humans possess a small number of primitive systems of mental representation - of objects, agents, number and geometry - and new representations are built upon these core foundations.\nRepresentations learned with modern machine learning methods possess few or none of these properties, which is a severe impediment. For illustration consider that navigation depends upon some representation of geometry, and yet recent advances such as end-to-end autonomous driving (Bojarski et al., 2016) side-step building explicit geometric representations of the world by learning to map directly from image inputs to motor commands. Any representation of geometry is implicit, and has the advantage that it is economical in only possessing information necessary for the task. However, this form of representation lacks (i) the ability to reuse these representations for other related tasks such as predicting object stability or performing mental rotation, (ii) the ability to compose these representations with others, for instance to represent a set or count of geometric objects, and (iii) the ability to perform explicit inference using representations, for instance to infer why a particular route would be faster or slower.\nThis contribution provides a computational model of mental representation which inherits the compositional and productivity advantages of symbolic representations, and the data-driven and economical advantages of representations learned using deep learning methods. To this end, we model mental representations as a form of data-structure, which by design possess various forms of compositionality. In addition, in step with deep learning methods we refrain from imposing a particular representations on a system and allow it instead be learned. That is, rather than specify a concrete data type (for example polygons or voxels for geometry), we instead define a class of representations as abstract data types, and impose invariants, or axioms, that any representation must adhere to.\nMathematicians have sought an axiomatic account of our mental representations since the end of the nineteenth century, but both as an account of human mental representations, and as a means of specifying representations for intelligent systems, the axiomatic specifications suffer from a number\nof problems. Axioms are universally quantified - for all numbers, sets, points, etc - while humans, in contrast, are not uniformly good at manipulating numbers of different magnitude (Hyde, 2011; Nuerk & Willmes, 2005; Dehaene, 1997), rotating geometry of different shapes (Izard et al., 2011), or sets of different cardinality. Second, axioms have no algorithmic content; they are declarative rules which do not suggest how to construct concrete representations that satisfy them. Third, only simple systems have reasonable axioms, whereas many representations are complex and cannot in practice be fully axiomitized; conventional axiomatic specifications do not readily accommodate partial specification. A fourth, potentially fatal threat is offered by Dehaene (1997), where he shows that there are infinitely many systems, most easily dismissed by even a child as clearly not number-like, which satisfy Peano’s axioms of arithmetic. Moreover these ”nonstandard models of arithmetic” can never be eliminated by adding more axioms, leading Dehaene to conclude ”Hence, our brain does not rely on axioms.”.\nWe extend, rather than abandon, the axiomatic approach to specifying mental representations, and employ it purely as mechanism to embed domain specific knowledge. We model a mental representation as an implementation of an abstract data type which adheres approximately to a probabilistic axiomatic specification. We refer to this implementation as a distribution-sensitive data-structure.\nIn summary, in this paper:\n• We introduce probabilistic axiomatic specifications as a quantifier-free relaxation of a conventional specification, which replaces universally quantified variables with random variables. • Synthesis of a representation is formulated as synthesis of functions which collectively\nsatisfy the axioms. When the axioms are probabilistic, this is amounts of maximizing the probability that the axiom is true. • We present a number of methods to approximate a probabilistic specification, reducing it\nto a continuous loss function. • We employ neural networks as function approximators, and through gradient based opti-\nmization learn representations for a number of fundamental data structures."
    }, {
      "heading" : "2 BACKGROUND: ABSTRACT DATA TYPES",
      "text" : "Abstract data types model representations as a set of types and functions which act on values of those types. They can also be regarded as a generalized approach to algebraic structures, such as lattices, groups, and rings. The prototypical example of an abstract data type is the Stack, which models an ordered, first-in, last-out container of items. We can abstractly define a Stack of Items, in part, by defining the interface:\nempty : Stack\npush : Stack × Item→ Stack pop : Stack → Stack × Item\nisempty : Stack → {0, 1}\nThe interface lists the function names and types (domains and range). Note that this is a functional (rather than imperative) abstract data type, and each function in the interface has no internal state. For example, push is a function that takes an instance of a Stack and an Item and returns a Stack. empty : Stack denotes a constant of type Stack, the empty stack of no items.\nThe meaning of the constants and functions is not specified in the interface. To give meaning to these names, we supplement the abstract data type with a specification as a set of axioms. The specification as a whole is the logical conjunction of this set of axioms. Continuing our example, for all s ∈ Stack, i ∈ Item:\npop(push(s, i)) = (s, i) (1) isempty(empty) = 1 (2)\nisempty(push(s, i)) = 0 (3) pop(empty) = ⊥ (4)\nA concrete representation of a stack is a data structure which assigns constants and functions to the names empty, push, pop and isempty. The data structure is a stack if and only if it satisfies the specification."
    }, {
      "heading" : "2.1 COMPOSITION OF DATA STRUCTURES",
      "text" : "There are a number of distinct forms of compositionality with respect to data structures. One example is algorithmic compositionality, by which we can compose algorithms which use as primitive operations the interfaces to these representations. These algorithms can in turn form the interfaces to other representations, and so on.\nAn important property of an abstract data types which supports algorithmic compositionality is encapsulation. Encapsulation means that the particular details of how the functions are implemented should not matter to the user of the data type, only that it behaves as specified. Many languages enforce that the internals are unobservable, and that the data type can only be interacted with through its interface. Encapsulation means that data-structures can be composed without reasoning about their internal behavior.\nIn this paper however, we focus on parametric compositionality. Some data structures, in particular containers such as a stack, or set, or tree are parametric with respect to some other type, e.g. the type of item. Parametric compositionality means for example that if we have a representation of a set, and a representation of a number, we get a set of numbers for free. Or, given a representations for a tree and representations for Boolean logic, we acquire the ability to form logical expressions for free."
    }, {
      "heading" : "2.2 DISTRIBUTION SENSITIVE DATA STRUCTURES",
      "text" : "Axiomatic specifications almost always contain universal quantifiers. The stack axioms are quantified over all possible stacks and all possible items. Real world use of a data structure is however never exhaustive, and rarely uniform. Continuing our stack example, we will never store an infinite number of items, and the distribution over how many items are stored, and in which order relative to each other, will highly non-uniform in typical use cases. Conventional data structures are agnostic to these distributional properties.\nData structures that exploit non-uniform query distributions are typically termed distributionsensitive (Bose et al., 2013), and are often motivated by practical concerns since queries observed in real-world applications are not uniformly random. An example is the optimum binary search tree on n keys, introduced by Knuth (Bose et al., 2013), which given a probability for each key has an average search cost no larger than any other key. More generally, distribution-sensitive data structures exploit underlying patterns in a sequence of operations in order to reduce time and space complexity."
    }, {
      "heading" : "3 PROBABILISTIC AXIOMATIC SPECIFICATION",
      "text" : "To make the concept of a distribution-sensitive data-structure precise, we first develop the concept of an probabilistically axiomatized abstract data type (T,O, F ), which replaces universally quantified variables in its specification with random variables. T and O are respectively sets of type and interface names. F is a set of type specifications, each taking the form m : τ for a constant of type τ , or o : τ1 → τ2 denoting a function from τ1 to τ2. Here τ ∈ T or a Cartesian product T1×· · ·×Tn. A concrete data type σ implements an abstract data type by assigning a value (function or constant) to each name in O. A concrete data type is deemed a valid implementation only with respect to an algebraic specification A. A is a set of equational axioms of the form p = q, p and q are constants, random variables, or transformations of random variables by functions in O.\nSince a transformation of a random variable yields a random variable, and an axiom is simply a predicate of its left and right hand side arguments, random variables present in an axiom implies that the axiom itself is a Boolean valued random variable. For example if we have a distribution over items i of the stack, axiom (1) itself is a random variable which is true or false depending on i, push, pop, and can only be satisfied with some probability. We let P [A(σ)] denote the probability\nthat a concrete data type σ satisfies the axioms:\nP [A(σ)] := P [∧ipi = qi] (5)\nProbabilistic axioms do not imply that the concrete data-structure itself is probabilistic. On the contrary, we are concerned with specifying and synthesizing deterministic concrete data structures which exploit uncertainty stemming only from the patterns in which the data-structure is used.\nWhen P [A(σ)] = 1, σ can be said to fully satisfy the axioms. More generally, with respect to a space Σ of concrete data types, we denote the maximum likelihood σ∗ as one which maximizes the probability that the axioms hold:\nσ∗ = arg max σ∈Σ P [A(σ)] (6)"
    }, {
      "heading" : "4 APPROXIMATE ABSTRACT DATA TYPES",
      "text" : "A probabilistic specification is not easier to satisfy than a universally quantified one, but it can lend itself more naturally to a number of approximations. In this section we outline a number of relaxations we apply to a probabilistic abstract data type to make synthesis tractable.\nRESTRICT TYPES TO REAL VALUED ARRAYS\nEach type τ ∈ T will correspond to a finite dimensional real valued multidimensional array Rn. Interface functions are continuous mappings between these arrays.\nUNROLL AXIOMS\nAxiom (1) of the stack is intensional in the sense that it refers to the underlying stack s. This provides an inductive property allowing us to fully describe the behavior of an unbounded number of push and pop operations with a single equational axiom. However, from an extensional perspective, we do not care about the internal properties of the stack; only that it behaves in the desired way. Put plainly, we only care that if we push an item i to the stack, then pop, that we get back i. We do not care that the stack is returned to its initial state, only that it is returned to some state that will continue to obey this desired behavior.\nAn extensional view leads more readily to approximation; since we cannot expect to implement a stack which satisfies the inductive property of axiom 1 if it is internally a finite dimensional vector. Instead we can unroll the axiom to be able to stack some finite number of n items:\nAPPROXIMATE DISTRIBUTIONS WITH DATA\nWe approximate random variables by a finite data distribution assumed to be a representative set of samples from that distribution. Given an axiom p = q, we denote p̂ and q̂ as values (arrays) computed by evaluating p and q respectively with concrete data from the data distributions of random variables and the interface functions.\nRELAX EQUALITY TO DISTANCE\nWe relax equality constraints in axioms to a distance function, in particular the L2 norm. This transforms the equational axioms into a loss function. Given i axioms, the approximate maximum likelihood concrete data type σ̂∗ is then:\nσ̂∗ = arg min i\n∑ ‖p̂i − q̂i‖ (7)\nConstants and parameterized functions (e.g. neural networks) which minimizes this loss function then compose a distribution-sensitive concrete data type."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We successfully synthesized approximate distribution-sensitive data-structures from a number of abstract data types:\n• Natural number (from Peano’s axioms) • Stack • Queue • Set • Binary tree\nWith the exception of natural number (for which we used Peano’s axioms), we use axiomitizations from (Dale & Walker, 1996). As described in section 4, since we use finite dimensional representations we unroll the axioms some finite number of times (e.g., to learn a stack of three items rather than it be unbounded) and ”extensionalize” them.\nIn each example we used we used single layer convolutional neural networks with 24, 3 by 3 filters and rectifier non-linearities. In container examples such as Stack and Queue, the Item type was sampled from MNIST dataset, and the internal stack representation was chosen (for visualization) to also be a 28 by 28 matrix. We minimized the equational distance loss function described in section 3 using the adam optimization algorithm, with a learning rate of 0.0001 In figures 1 and 2 we visualize the properties of the learned stack.\nTo explore compositionality, we also learned a Stack,Queue and Set ofNumber, whereNumber was itself a data type learned from Peano’s axioms."
    }, {
      "heading" : "6 ANALYSIS",
      "text" : "The learned internal representations depend on three things (i) the axioms themselves, (ii) the architecture of the networks for each function in the interface, and (iii) the optimization procedure. In the stack example, we observed that if we decreased the size of the internal representation of a stack, we would need to increase the size and complexity of the neural network to compensate. This implies that statistical information about images must be stored somewhere, but there is some flexibility over where.\nGiven the same architecture, the system learned different representations depending on the axioms and optimization. The stack representation learned in figure 1 differs from that in figure 3, indicating that there is not a unique solution to the problem, and different initialization strategies will yield different results. The queue internal representation is also different to them both, and the encoding is less clear. The queue and stack representations could have been the same (with only the interface functions push, pop, queue and dequeue taking different form).\nAs shown in figure 2, data-structures exhibit some generalization beyond the data distributions on which they are trained. In this case, a stack trained to store three items, is able to store four with some error, but degrades rapidly beyond that. Of course we cannot expect a finite capacity representation to store an unbounded number of items; lack of generalization is the cost of having optimized performance on the distribution of interest."
    }, {
      "heading" : "7 RELATED WORK",
      "text" : "Our contribution builds upon the foundations of distribution-sensitive data structures (Bose et al., 2013), but departs from conventional work on distribution-sensitive data structures in that: (i) we\nsynthesize data structures automatically from specification, and (ii) the distributions of interest are complex data distributions, which prevents closed form solutions as in the optimum binary tree.\nVarious forms of machine learning and inference learn representations of data. Our approach bears resemblance to the auto-encoder (Bengio, 2009), which exploits statistics of a data distribution to learn a compressed representation as a hidden layer of a neural network. As in our approach, an auto-encoder is distribution sensitive by the constraints of the architecture and the training procedure (the hidden layer is of smaller capacity than the data and which forces the exploitation of regularities). However, an auto-encoder permits just two operations: encode and decode, and has no notion explicit notion of compositionality.\nA step closer to our approach than the auto-encoder are distributed representations of words as developed in (Mikolov et al., 2000). These representations have a form of compositionality such that vector arithmetic on the representation results in plausible combinations (Air + Canada = AirCanada).\nOur approach to learning representation can be viewed as a form of data-type synthesis from specification. From the very introduction of abstract data types, verification that a given implementation satisfies its specification was a motivating concern (Guttag et al., 1978; Guttag, 1978; Spitzen & Wegbreit, 1975). Modern forms of function synthesis (Solar-Lezama, 2009; Polikarpova & SolarLezama, 2016) use verification as an oracle to assist with synthesis. Our approach in a broad sense is similar, in that derivatives from loss function which is derived from relaxing the specification, guide the optimization through the paramterized function spaces.\nProbabilistic assertions appear in first-order lifting (Poole, 2003), and Sampson (Sampson et al., 2014) introduce probabilistic assertions. Implementation of data type is a program. Main difference is that we synthesize data type from probabilistic assertion. Sumit’s work (Sankaranarayanan, 2014) seeks upper and lower bounds for the probability of the assertion for the programs which operate on uncertain data.\nRecent work in deep learning has sought to embed discrete data structures into continuous form. Examples are the push down automata (Sun et al., 1993), networks containing stacks (Grefenstette et al., 2015), and memory networks (Sukhbaatar et al., 2015). Our approach can be used to synthesize arbitrary data-structure, purely from its specification, but is parameterized by the neural network structure. This permits it more generality, with a loss of efficiency."
    }, {
      "heading" : "8 DISCUSSION",
      "text" : "In this contribution we presented a model of mental representations as distribution sensitive data structures, and a method which employs neural networks (or any parameterized function) to synthesize concrete data types from a relaxed specification. We demonstrated this on a number of examples, and visualized the results from the stack and queue.\nOne of the important properties of conventional data structures is that they compose; they can be combined to form more complex data structures. In this paper we explored a simple form of parametric composition by synthesizing containers of numbers. This extends naturally to containers of containers, .e.g sets of sets, or sets of sets of numbers. Future work is to extend this to richer forms of composition. In conventional programming languages, trees and sets are often made by composing arrays, which are indexed with numbers. This kind of composition ls fundamental to building complex software from simple parts.\nIn this work we learned representations from axioms. Humans, in contrast, learn representations mostly from experience in the world. One rich area of future work is to extend data-structure learning to the unsupervised setting, such that for example an agent operating in the real world would learn a geometric data-structures purely from observation."
    } ],
    "references" : [ {
      "title" : "Learning Deep Architectures for AI, volume",
      "author" : [ "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bengio.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "End to End Learning for Self-Driving Cars",
      "author" : [ "Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D. Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang", "Xin Zhang", "Jake Zhao", "Karol Zieba" ],
      "venue" : null,
      "citeRegEx" : "Bojarski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bojarski et al\\.",
      "year" : 2016
    }, {
      "title" : "Space-Efficient Data Structures, Streams, and Algorithms",
      "author" : [ "Prosenjit Bose", "John Howat", "Pat Morin" ],
      "venue" : "Papers in Honor of J. Ian Munro on the Occasion of His 66th Birthday. chapter A History,",
      "citeRegEx" : "Bose et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bose et al\\.",
      "year" : 2013
    }, {
      "title" : "Abstract Data Types: Specifications, Implementations, and Applications",
      "author" : [ "N Dale", "H M Walker" ],
      "venue" : "D.C. Heath,",
      "citeRegEx" : "Dale and Walker.,? \\Q1996\\E",
      "shortCiteRegEx" : "Dale and Walker.",
      "year" : 1996
    }, {
      "title" : "The Number sense, volume",
      "author" : [ "Stanislas Dehaene" ],
      "venue" : "ISBN 9780199753871",
      "citeRegEx" : "Dehaene.,? \\Q1997\\E",
      "shortCiteRegEx" : "Dehaene.",
      "year" : 1997
    }, {
      "title" : "The Compositionality Papers",
      "author" : [ "Jerry A Fodor", "Ernest Lepore" ],
      "venue" : null,
      "citeRegEx" : "Fodor and Lepore.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fodor and Lepore.",
      "year" : 2002
    }, {
      "title" : "Learning to Transduce with Unbounded Memory",
      "author" : [ "Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "arXiv preprint, pp",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2015
    }, {
      "title" : "Algebraic Specification of Abstract Data Types",
      "author" : [ "John Guttag" ],
      "venue" : "Software Pioneers,",
      "citeRegEx" : "Guttag.,? \\Q1978\\E",
      "shortCiteRegEx" : "Guttag.",
      "year" : 1978
    }, {
      "title" : "Abstract data types and software validation",
      "author" : [ "John V. Guttag", "Ellis Horowitz", "David R. Musser" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Guttag et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Guttag et al\\.",
      "year" : 1978
    }, {
      "title" : "Two Systems of Non-Symbolic Numerical Cognition",
      "author" : [ "Daniel C. Hyde" ],
      "venue" : "Frontiers in Human Neuroscience,",
      "citeRegEx" : "Hyde.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hyde.",
      "year" : 2011
    }, {
      "title" : "Geometry as a Universal Mental Construction, volume 1",
      "author" : [ "Vronique Izard", "Pierre Pica", "Stanislas Dehaene", "Danielle Hinchey", "Elizabeth Spelke" ],
      "venue" : "Elsevier Inc.,",
      "citeRegEx" : "Izard et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Izard et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed Representations ofWords and Phrases and their Compositionality",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "Arvix, 1:1–9,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2000
    }, {
      "title" : "On the magnitude representations of two-digit numbers",
      "author" : [ "Hc Nuerk", "K Willmes" ],
      "venue" : "Psychology Science,",
      "citeRegEx" : "Nuerk and Willmes.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nuerk and Willmes.",
      "year" : 2005
    }, {
      "title" : "Program Synthesis from Polymorphic Refinement Types. PLDI: Programming Languages Design and Implementation, 2016",
      "author" : [ "Nadia Polikarpova", "Armando Solar-Lezama" ],
      "venue" : "URL http:// arxiv.org/abs/1510.08419",
      "citeRegEx" : "Polikarpova and Solar.Lezama.,? \\Q2016\\E",
      "shortCiteRegEx" : "Polikarpova and Solar.Lezama.",
      "year" : 2016
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "David Poole" ],
      "venue" : "In IJCAI International Joint Conference on Artificial Intelligence, pp",
      "citeRegEx" : "Poole.,? \\Q2003\\E",
      "shortCiteRegEx" : "Poole.",
      "year" : 2003
    }, {
      "title" : "Expressing and Verifying Probabilistic Assertions",
      "author" : [ "Adrian Sampson", "Pavel Panchekha", "Todd Mytkowicz", "Kathryn S McKinley", "Dan Grossman", "Luis Ceze" ],
      "venue" : "In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation,",
      "citeRegEx" : "Sampson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sampson et al\\.",
      "year" : 2014
    }, {
      "title" : "Static Analysis for Probabilistic Programs : Inferring Whole Program Properties from Finitely Many Paths",
      "author" : [ "Sriram Sankaranarayanan" ],
      "venue" : "pp. 447–458,",
      "citeRegEx" : "Sankaranarayanan.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sankaranarayanan.",
      "year" : 2014
    }, {
      "title" : "The sketching approach to program synthesis. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "author" : [ "Armando Solar-Lezama" ],
      "venue" : null,
      "citeRegEx" : "Solar.Lezama.,? \\Q2009\\E",
      "shortCiteRegEx" : "Solar.Lezama.",
      "year" : 2009
    }, {
      "title" : "The verification and synthesis of data structures",
      "author" : [ "Jay Spitzen", "Ben Wegbreit" ],
      "venue" : "Acta Informatica,",
      "citeRegEx" : "Spitzen and Wegbreit.,? \\Q1975\\E",
      "shortCiteRegEx" : "Spitzen and Wegbreit.",
      "year" : 1975
    }, {
      "title" : "End-To-End Memory Networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "pp. 1–11,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "The Neural Network Pushdown Automaton: Model, Stack and Learning Simulations",
      "author" : [ "G Z Sun", "Lee C Giles", "H H Chen", "Y C Lee" ],
      "venue" : null,
      "citeRegEx" : "Sun et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "For illustration consider that navigation depends upon some representation of geometry, and yet recent advances such as end-to-end autonomous driving (Bojarski et al., 2016) side-step building explicit geometric representations of the world by learning to map directly from image inputs to motor commands.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "Axioms are universally quantified - for all numbers, sets, points, etc - while humans, in contrast, are not uniformly good at manipulating numbers of different magnitude (Hyde, 2011; Nuerk & Willmes, 2005; Dehaene, 1997), rotating geometry of different shapes (Izard et al.",
      "startOffset" : 170,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "Axioms are universally quantified - for all numbers, sets, points, etc - while humans, in contrast, are not uniformly good at manipulating numbers of different magnitude (Hyde, 2011; Nuerk & Willmes, 2005; Dehaene, 1997), rotating geometry of different shapes (Izard et al.",
      "startOffset" : 170,
      "endOffset" : 220
    }, {
      "referenceID" : 10,
      "context" : "Axioms are universally quantified - for all numbers, sets, points, etc - while humans, in contrast, are not uniformly good at manipulating numbers of different magnitude (Hyde, 2011; Nuerk & Willmes, 2005; Dehaene, 1997), rotating geometry of different shapes (Izard et al., 2011), or sets of different cardinality.",
      "startOffset" : 260,
      "endOffset" : 280
    }, {
      "referenceID" : 4,
      "context" : "Axioms are universally quantified - for all numbers, sets, points, etc - while humans, in contrast, are not uniformly good at manipulating numbers of different magnitude (Hyde, 2011; Nuerk & Willmes, 2005; Dehaene, 1997), rotating geometry of different shapes (Izard et al., 2011), or sets of different cardinality. Second, axioms have no algorithmic content; they are declarative rules which do not suggest how to construct concrete representations that satisfy them. Third, only simple systems have reasonable axioms, whereas many representations are complex and cannot in practice be fully axiomitized; conventional axiomatic specifications do not readily accommodate partial specification. A fourth, potentially fatal threat is offered by Dehaene (1997), where he shows that there are infinitely many systems, most easily dismissed by even a child as clearly not number-like, which satisfy Peano’s axioms of arithmetic.",
      "startOffset" : 206,
      "endOffset" : 758
    }, {
      "referenceID" : 2,
      "context" : "Data structures that exploit non-uniform query distributions are typically termed distributionsensitive (Bose et al., 2013), and are often motivated by practical concerns since queries observed in real-world applications are not uniformly random.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "An example is the optimum binary search tree on n keys, introduced by Knuth (Bose et al., 2013), which given a probability for each key has an average search cost no larger than any other key.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "Our contribution builds upon the foundations of distribution-sensitive data structures (Bose et al., 2013), but departs from conventional work on distribution-sensitive data structures in that: (i) we",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Our approach bears resemblance to the auto-encoder (Bengio, 2009), which exploits statistics of a data distribution to learn a compressed representation as a hidden layer of a neural network.",
      "startOffset" : 51,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "A step closer to our approach than the auto-encoder are distributed representations of words as developed in (Mikolov et al., 2000).",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "From the very introduction of abstract data types, verification that a given implementation satisfies its specification was a motivating concern (Guttag et al., 1978; Guttag, 1978; Spitzen & Wegbreit, 1975).",
      "startOffset" : 145,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "From the very introduction of abstract data types, verification that a given implementation satisfies its specification was a motivating concern (Guttag et al., 1978; Guttag, 1978; Spitzen & Wegbreit, 1975).",
      "startOffset" : 145,
      "endOffset" : 206
    }, {
      "referenceID" : 17,
      "context" : "Modern forms of function synthesis (Solar-Lezama, 2009; Polikarpova & SolarLezama, 2016) use verification as an oracle to assist with synthesis.",
      "startOffset" : 35,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "Probabilistic assertions appear in first-order lifting (Poole, 2003), and Sampson (Sampson et al.",
      "startOffset" : 55,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "Probabilistic assertions appear in first-order lifting (Poole, 2003), and Sampson (Sampson et al., 2014) introduce probabilistic assertions.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "Sumit’s work (Sankaranarayanan, 2014) seeks upper and lower bounds for the probability of the assertion for the programs which operate on uncertain data.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "Examples are the push down automata (Sun et al., 1993), networks containing stacks (Grefenstette et al.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : ", 1993), networks containing stacks (Grefenstette et al., 2015), and memory networks (Sukhbaatar et al.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : ", 2015), and memory networks (Sukhbaatar et al., 2015).",
      "startOffset" : 29,
      "endOffset" : 54
    } ],
    "year" : 2016,
    "abstractText" : "We model representations as data-structures which are distribution sensitive, i.e., which exploit regularities in their usage patterns to reduce time or space complexity. We introduce probabilistic axiomatic specifications to extend abstract data structures which specify a class of representations with equivalent logical behavior to a distribution-sensitive data structures. We reformulate synthesis of distribution-sensitive data structures as a continuous function approximation problem, such that the functions of a data-structure deep neural networks, such as a stack, queue, natural number, set, and binary tree.",
    "creator" : "LaTeX with hyperref package"
  }
}