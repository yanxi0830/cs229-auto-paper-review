{
  "name" : "664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GENERATIVE ADVERSARIAL NETWORKS FOR IMAGE STEGANOGRAPHY",
    "authors" : [ "Denis Volkhonskiy", "Boris Borisenko", "Evgeny Burnaev" ],
    "emails" : [ "dvolkhonskiy@gmail.com,", "bborisenko@hse.ru", "e.burnaev@skoltech.ru" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recently developed Generative Adversarial Networks (GAN, see Goodfellow et al. (2014)) are powerful generative models, the main idea of which is to train a generator and a discriminator network through playing a minimax game. In the image domain, for a dataset generated by some density pdata(x) a generator G attempts to approximate the image generating distribution and to synthesize as realistic image as possible, while a discriminator D strives to distinguish real images from fake ones.\nThere are several modifications of GAN that can generate realistic images:\n• Deep Convolutional Generative Adversarial Networks (DCGAN, see Radford et al. (2015)) — this model is a modification of a GAN, specialized for generation of images; • Conditional GAN — it allows generating objects from a specified class, see Mirza &\nOsindero (2014); • Generation of images from textual description, see Reed et al. (2016).\nIn the present study we apply the DCGAN model to the problem of secure steganography. We construct a special container-image generator, synthetic output of which is less susceptible to successful steganalysis compared to containers, directly derived from original images. In particular, we investigate whether this methodology allows to deceive a given steganography analyzer, represented by a binary classifier detecting presence of hidden messages in an image."
    }, {
      "heading" : "2 STEGANOGRAPHY",
      "text" : "Steganography is the practice of concealing a secret message, e.g. a document, an image, or a video, within another non-secret message in the most inconspicuous manner possible. In this paper we consider a text-to-image embedding, with the text given by bit string. More formally, for a message T and an image I , a steganography algorithm is a map S : T × I → Î , where Î is an image, containing the message T , such that Î can not be visually distinguished from I .\nThe most popular and easy-to-implement algorithm of embedding is the Least Significant Bit (LSB) algorithm. The main idea of LSB is to store the secret message in the least significant bits (last bits) of some color channel of each pixel in the given image container. Since pixels are adjusted independently of each other, the LSB algorithm alters the distribution of the least significant bits, thereby simplifying detection of the payload. A modification of this method, which does not substantially alter the distribution of the least significant bits, is a so-called ±1-embedding (Ker, 2005). This approach randomly adds or subtracts 1 from some color channel pixel so that the last bits would match the ones needed. In this paper we basically consider the ±1-embedding algorithm. There are more sophisticated algorithms for information embedding to raster images: WOW (Holub & Fridrich, 2012), HUGO (Pevny et al., 2010), S-UNIWARD (Holub et al., 2014), and others. They are derived from key ideas of the LSB algorithm, but utilize a more strategic pixel manipulation technique: for the raw image X and its final version with a secret message X̂ the pixels are picked in such a way as to minimize the distortion function\nD(X, X̂) = n1∑ i=1 n2∑ j=1 ρ(Xij , X̂ij)|Xij − X̂ij | ,\nwhere ρ(Xij , X̂ij) is the cost of changing pixel of X , specific for each particular steganography algorithm.\nFor detecting presence of hidden information in the container Steganalysis is usually used. The stage which distinguishes images with some hidden message from empty is usually performed by binary classification. The basic approach to steganalysis is based on feature extractors (such as SPAM (Pevnỳ et al., 2010), SRM (Fridrich & Kodovskỳ, 2012), etc.) combined with traditional machine learning classifiers, such as SVM, decision trees, ensembles etc. With the recent overwhelming success of deep neural networks, newer neural network based approaches to steganalysis are gaining popularity, Qian et al. (2015b). For example, in Pibre et al. (2015) authors propose to use deep convolution neural networks (CNN) for steganalysis and show that classification accuracy can be significantly increased while using CNN instead of usual classifiers."
    }, {
      "heading" : "3 ADVERSARIAL NETWORKS",
      "text" : "Generative Adversarial Networks (GAN) is a recent approach to deep unsupervised learning, proposed in 2014 in Goodfellow et al. (2014), which is capable of dynamically representing a sampler from input data distribution and generate new data samples.\nThe main idea of such approach to learning is that two neural networks are trained simultaneously:\n• a generative model (G) that receives noise from the prior distribution pnoise(z) on input and transforms it into a data sample from the distribution pg(x) that approximates pdata(x); • a discriminative model (D) which tries to detect if an object is real or generated by G.\nThe learning process can be described as a minimax game: the discriminator D maximizes the expected log-likelihood of correctly distinguishing real samples from fake ones, while the generator G maximizes the expected error of the discriminator by trying to synthesize better images. Therefore during the training GAN solve the following optimization problem:\nL(D,G) = Ex∼pdata(x) [ logD(x) ] + Ez∼pnoise(z) [ log(1−D(G(z))) ]→ min G max D , (1)\nwhere D(x) represents the probability that x is a real image rather then synthetic, and G(z) is a synthetic image for input noise z.\nCoupled optimization problem (1) is solved by alternating the maximization and minimization steps: on each iteration of the mini-batch stochastic gradient optimization we first make a gradient ascent step on D and then a gradient descent step on G. If by θM we denote the parameters of the neural network M , then the update rules are:\n• Keeping the G fixed, update the model D by θD ← θD + γD∇DL with\n∇DL = ∂\n∂θD\n{ Ex∼pdata(x) [ logD(x, θD) ]+Ez∼pnoise(z) [ log(1−D(G(z, θG), θD)) ] } ,\n(2)\n• Keeping D fixed, update G by θG ← θG − γG∇GL where\n∇GL = ∂\n∂θG Ez∼pnoise(z) [ log(1−D(G(z, θG), θD)) ] . (3)\nIn Radford et al. (2015) the GAN idea was extended to deep convolutional networks (DCGAN), which are specialized for image generation. The paper discusses the advantages of adversarial training in image recognition and generation, and give recommendations on constructing and training DCGANs. In fig. 1 we depict a sample of synthetic images of a freshly trained DCGAN on the Celebrities dataset (Ziwei Liu & Tang, 2015). The images indeed look realistic, albeit with occasional artifacts."
    }, {
      "heading" : "4 STEGANOGRAPHIC GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "In order to apply GAN methodology to steganographic applications, we introduce Steganographic Generative Adversarial Networks model (SGAN), which consists of\n• a generator network G, which produces realistic looking images from noise; • a discriminator network D, which classifies whether an image is synthetic or real; • a discriminator network S, the steganalyser, which determines if an image contains a\nconcealed secret message.\nBy Stego(x) we denote the result of embedding some hidden message in the container x.\nSince we want the generator to produce realistic images that could serve as containers for secure message embedding, we force G to compete against the models D and S simultaneously. If we denote by S(x) the probability that x has some hidden information, then we arrive at the following game:\nL = α ( Ex∼pdata(x) [ logD(x) ] + Ez∼pnoise(z) [ log(1−D(G(z))) ] ) +\n+(1− α)Ez∼pnoise(z) [ logS(Stego(G(z))) + log(1− S(G(z))) ]→ min G max D max S\n. (4)\nWe use a convex combination of errors of D and S with parameter α ∈ [0, 1], which controls the trade-off between the importance of realism of generated images and their quality as containers\nagainst the steganalysis. Analysis of preliminary experimental results showed that for α ≤ 0.7 the generated images are unrealistic and resemble noise.\nThe full scheme of SGAN is presented in fig. 2. Each arrows represent output- input data flows.\nStochastic mini-batch Gradient descent update rules for components of SGAN are listed below:\n• for D the rule is θD ← θD + γD∇GL with\n∇GL = ∂\n∂θD\n{ Ex∼pdata(x) [ logD(x, θD) ]+Ez∼pnoise(z) [ log(1−D(G(z, θG), θD)) ] } ;\n• for S (it is updated similarly to D): θS ← θS + γS∇SL where\n∇SL = ∂\n∂θS Ez∼pnoise(z) [ logS(Stego(G(z, θG)), θS) + log(1− S(G(z, θG), θS)) ] ;\n• for the generator G: θG ← θG − γG∇GL with∇GL given by\n∇GL = ∂\n∂θG αEz∼pnoise(z) [ log(1−D(G(z, θG), θD)) ]\n+ ∂\n∂θG (1− α)Ez∼pnoise(z) [ log(S(Stego(G(z, θG), θS))) ]\n+ ∂\n∂θG (1− α)Ez∼pnoise(z) [ log(1− S(G(z, θG), θS)) ] .\nThe main distinction from the GAN model is that we update G in order to maximize not only the error of D, but to maximize the error of the linear combination of the classifiers D and S."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1 DATA DESCRIPTION",
      "text" : "In our experiments 1 we use the Celebrities dataset (Ziwei Liu & Tang, 2015) that contains 200 000 images. All images were cropped to 64× 64 pixels. For steganalysis purposes we consider 10% of data as a test set. We denote the train set by A, the test set by B and steganography algorithms used for hiding information by Stego(x). After embedding some secret information we get the train set A+ Stego(A) and the test set B + Stego(B), We end up with 380 000 images for steganalysis training and 20 000 for testing. For training the SGAN model we used all 200 000 cropped images. After 8 epochs of training our SGAN produces images displayed in fig. 3.\nFor information embedding we use the ±1-embedding algorithm with a payload size equal to 0.4 bits per pixel for only one channel out of three. As a text for embedding we use randomly selected excerpts from some article from The New York Times.\n1Code is available at https://github.com/dvolkhonskiy/adversarial-steganography"
    }, {
      "heading" : "5.2 EXPERIMENTAL SETUP",
      "text" : "In this section we describe the SGAN model structure. By C2D-BN-LR we denote the following structural block of a convolutional neural network: Conv2d→ Batch Normalization→ Leaky ReLU. The Steganalyser network S and the Image Discriminator network have similar structure: four C2D-BN-LR layers, then a fully connected layer (1 neuron)→ Sigmoid function is used to compute an output. The Image generator network G is (in order) a fully-connected layer (8192 neurons), four C2D-BN-LR with Fractional-Strided convolution, then the Hyperbolic tangent function layer is used to compute normalised output.\nThe SGAN model is trained to solve (4) using the Adam optimization algorithm (Kingma & Ba, 2014) with the learning rate 2−4 and update parameters β1 = 0.5 and β2 = 0.999. For each mini-batch of images we update weights of D and S once, then we update weights of G twice.\nIn the following experiments, in addition to the steganalyser S we use an independent steganalyser S∗. We define a filter F (0) that is special for steganalysis applications (see Pibre et al. (2015), Qian et al. (2015a), Tan & Li (2014)), as follows\nF (0) = 1\n12  −1 2 −2 2 −1 2 −6 8 −6 2 −2 8 −12 8 −2 2 −6 8 −6 2 −1 2 −2 2 −1  .\nThe structure of the individual steganalyser S∗ has the form: 2D convolution with F (0) filter → Conv2D→ Conv2D→Max Pooling→ Conv2D→ Conv2D→Max Pooling→ Fully connected layer (1024 neurons) → Fully connected layer (1 neuron) → Sigmoid function for output. This structure provides state-of-the-art steganalysis accuracy, Pibre et al. (2015), and the filter F (0) allows to increase convergence speed of the steganalyser S∗ training.\nFor training of this steganalyser we use the Adam optimization algorithm on the loss (4) with the learning rate equal to 5−6, β1 = 0.9, β2 = 0.999. As a loss function we use a binary cross-entropy.\nThe setup of experiments can be described as follows:\n• We train and use the SGAN and/or DCGAN model to generate images to be used as containers;\n• We train the independent steganalyser S∗ using either real images (sec. 5.3) or generated images (sec. 5.4);\n• We measure the accuracy of the steganalyser S∗."
    }, {
      "heading" : "5.3 TRAINING/TESTING ON REAL IMAGES",
      "text" : "In this set of experiments we train the independent steganalyser S∗ on real images. Results are provided in tab. 1. From the results we conclude that even the usual DCGAN generate synthetic\ncontainer images, that can easily deceive a steganalyser. Although containers generated by on the SGAN model, which is specially designed to deceive the steganalyser, improve on the result of DCGAN, the improvement is not significant. These results also suggest that S∗ may have been overfit to the data distribution of the real images, and failed to generalize to the GAN approximation of the data distribution in the synthetic image domain.\nAnother interesting result follows from an experiment with HUGO steganography algorithm and SGAN generated containers:\n• We use the ±1-embedding algorithm for training the SGAN model; • We test the HUGO steganography analyzer on real images and on images, generated by the\nSGAN model.\nContainers generated by the SGAN model decrease HUGO steganalysis accuracy from 0.624 to 0.499, which demonstrates that the Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating Steganography containers tuned to deceive any specific steganalysis algorithm."
    }, {
      "heading" : "5.4 TRAINING/TESTING ON GENERATED IMAGES",
      "text" : "In this set of experiments we train the steganalyser S∗ on images, generated using the DCGAN model from the prior noise distribution pnoise(z) for some fixed seed value. In all experiments the size of the train set is fixed at ≈ 160 000. After training we test the analyser S∗ on images generated with the DCGAN model according to the following experimental setups:\nC1. We use the same seed value;\nC2. We use some randomly selected seed value;\nC3. We use the same seed value, as in C2, and we additionally tune the DCGAN model for several epochs.\nThe experiment results in tab. 2 indicate that using different seed values when generating containers most likely affects the distribution of pixel value bits in such a way as make it easier to deceive\nthe steganalyser, fitted to another distribution in the train sample. Additional tuning of the image generator G make this effect even more pronounced.\nIn the next set of experiments we train and test the steganalyser S∗ on images, generated according to the following experimental conditions:\nC4. We generate a train set for the steganalyser S∗ using several different randomly selected seed values, and when generating the test set we use another fixed seed value;\nC5. We generate the train set and the test set using a number of different randomly selected seed values;\nC6. We use the same train and test sets, as in C5, and we additionally train the DCGAN model for several epochs.\nAccording to tab. 3 the accuracy in case C5 is lower than in the C4 case, which can be explained by the test set of C5 having more variability, being generated with different randomly selected seed values. Similarly, the accuracy in the C4 case is higher than in C2, since in C4 the train set was generated with several different randomly selected seed values, and thus is more representative. These observations confirm out initial conclusions, drawn from tab. 2.\nWe also conduct an experiment with classification of generated images without steganographic embeddings. For this purposes we train a DCGAN conditional model on the MNIST dataset, and train a separate classifier for the MNIST classification task. The trained classifier achieved almost perfect accuracy both on the held-out real MNIST dataset, and on synthetic images produced by the DCGAN. This provides evidence that it is possible to train an image classifier that shows acceptable accuracy both on real and synthetic images. However it is the artificial generation of image containers that breaks the usual approaches to steganalysis."
    }, {
      "heading" : "6 CONCLUSIONS AND FUTURE WORK",
      "text" : "In this work\n1. We open a new field for applications of Generative Adversarial Networks, namely, container generation for steganography applications;\n2. We consider the ±1-embedding algorithm and test novel approaches to more steganalysissecure information embedding:\na) we demonstrate that both SGAN and DCGAN models are capable of decreasing the detection accuracy of a steganalysis method almost to that of a random classifier;\nb) if we initialize a generator of containers with different random seed values, we can even further decrease the steganography detection accuracy.\nIn future, We plan to test our approach on more advanced steganographic algorithms, e.g. WOW (Holub & Fridrich, 2012), HUGO (Pevny et al., 2010) and S-UNIWARD (Holub et al., 2014)."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The research was supported solely by the Russian Science Foundation grant (project 14-50-00150). The authors would like to thank I. Nazarov for his assistance in preparation of this paper."
    } ],
    "references" : [ {
      "title" : "Rich models for steganalysis of digital images",
      "author" : [ "Jessica Fridrich", "Jan Kodovskỳ" ],
      "venue" : "Information Forensics and Security, IEEE Transactions on,",
      "citeRegEx" : "Fridrich and Kodovskỳ.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fridrich and Kodovskỳ.",
      "year" : 2012
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Designing steganographic distortion using directional filters",
      "author" : [ "Vojtech Holub", "Jessica J. Fridrich" ],
      "venue" : "WIFS,",
      "citeRegEx" : "Holub and Fridrich.,? \\Q2012\\E",
      "shortCiteRegEx" : "Holub and Fridrich.",
      "year" : 2012
    }, {
      "title" : "Universal distortion function for steganography in an arbitrary domain",
      "author" : [ "Vojtěch Holub", "Jessica Fridrich", "Tomáš Denemark" ],
      "venue" : "EURASIP Journal on Information Security,",
      "citeRegEx" : "Holub et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Holub et al\\.",
      "year" : 2014
    }, {
      "title" : "Resampling and the detection of lsb matching in color bitmaps",
      "author" : [ "Andrew D Ker" ],
      "venue" : "In Electronic Imaging",
      "citeRegEx" : "Ker.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ker.",
      "year" : 2005
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Conditional generative adversarial nets",
      "author" : [ "Mehdi Mirza", "Simon Osindero" ],
      "venue" : "arXiv preprint arXiv:1411.1784,",
      "citeRegEx" : "Mirza and Osindero.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza and Osindero.",
      "year" : 2014
    }, {
      "title" : "Steganalysis by subtractive pixel adjacency matrix. information Forensics and Security",
      "author" : [ "Tomáš Pevnỳ", "Patrick Bas", "Jessica Fridrich" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Pevnỳ et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pevnỳ et al\\.",
      "year" : 2010
    }, {
      "title" : "Using High-Dimensional Image Models to Perform Highly Undetectable Steganography",
      "author" : [ "Tomas Pevny", "Tomas Filler", "Patrick Bas" ],
      "venue" : "In Information Hiding,",
      "citeRegEx" : "Pevny et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pevny et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep learning for steganalysis is better than a rich model with an ensemble classifier, and is natively robust to the cover sourcemismatch",
      "author" : [ "Lionel Pibre", "Pasquet Jérôme", "Dino Ienco", "Marc Chaumont" ],
      "venue" : "arXiv preprint arXiv:1511.04855,",
      "citeRegEx" : "Pibre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pibre et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning for steganalysis via convolutional neural networks",
      "author" : [ "Yinlong Qian", "Jing Dong", "Wei Wang", "Tieniu Tan" ],
      "venue" : "In IS&T/SPIE Electronic Imaging, pp. 94090J–94090J. International Society for Optics and Photonics,",
      "citeRegEx" : "Qian et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning for steganalysis via convolutional neural networks",
      "author" : [ "Yinlong Qian", "Jing Dong", "Wei Wang", "Tieniu Tan" ],
      "venue" : "In SPIE/IS&T Electronic Imaging, pp. 94090J–94090J. International Society for Optics and Photonics,",
      "citeRegEx" : "Qian et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial text to image synthesis",
      "author" : [ "Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "arXiv preprint arXiv:1605.05396,",
      "citeRegEx" : "Reed et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2016
    }, {
      "title" : "Stacked convolutional auto-encoders for steganalysis of digital images",
      "author" : [ "Shunquan Tan", "Bin Li" ],
      "venue" : "In Asia-Pacific Signal and Information Processing Association,",
      "citeRegEx" : "Tan and Li.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tan and Li.",
      "year" : 2014
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Xiaogang Wang Ziwei Liu", "Ping Luo", "Xiaoou Tang" ],
      "venue" : "In Proceedings of International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recently developed Generative Adversarial Networks (GAN, see Goodfellow et al. (2014)) are powerful generative models, the main idea of which is to train a generator and a discriminator network through playing a minimax game.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "• Deep Convolutional Generative Adversarial Networks (DCGAN, see Radford et al. (2015)) — this model is a modification of a GAN, specialized for generation of images; • Conditional GAN — it allows generating objects from a specified class, see Mirza & Osindero (2014); • Generation of images from textual description, see Reed et al.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "• Deep Convolutional Generative Adversarial Networks (DCGAN, see Radford et al. (2015)) — this model is a modification of a GAN, specialized for generation of images; • Conditional GAN — it allows generating objects from a specified class, see Mirza & Osindero (2014); • Generation of images from textual description, see Reed et al.",
      "startOffset" : 65,
      "endOffset" : 268
    }, {
      "referenceID" : 12,
      "context" : "• Deep Convolutional Generative Adversarial Networks (DCGAN, see Radford et al. (2015)) — this model is a modification of a GAN, specialized for generation of images; • Conditional GAN — it allows generating objects from a specified class, see Mirza & Osindero (2014); • Generation of images from textual description, see Reed et al. (2016).",
      "startOffset" : 65,
      "endOffset" : 341
    }, {
      "referenceID" : 4,
      "context" : "A modification of this method, which does not substantially alter the distribution of the least significant bits, is a so-called ±1-embedding (Ker, 2005).",
      "startOffset" : 142,
      "endOffset" : 153
    }, {
      "referenceID" : 8,
      "context" : "There are more sophisticated algorithms for information embedding to raster images: WOW (Holub & Fridrich, 2012), HUGO (Pevny et al., 2010), S-UNIWARD (Holub et al.",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : ", 2010), S-UNIWARD (Holub et al., 2014), and others.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "The basic approach to steganalysis is based on feature extractors (such as SPAM (Pevnỳ et al., 2010), SRM (Fridrich & Kodovskỳ, 2012), etc.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "The basic approach to steganalysis is based on feature extractors (such as SPAM (Pevnỳ et al., 2010), SRM (Fridrich & Kodovskỳ, 2012), etc.) combined with traditional machine learning classifiers, such as SVM, decision trees, ensembles etc. With the recent overwhelming success of deep neural networks, newer neural network based approaches to steganalysis are gaining popularity, Qian et al. (2015b). For example, in Pibre et al.",
      "startOffset" : 81,
      "endOffset" : 401
    }, {
      "referenceID" : 7,
      "context" : "The basic approach to steganalysis is based on feature extractors (such as SPAM (Pevnỳ et al., 2010), SRM (Fridrich & Kodovskỳ, 2012), etc.) combined with traditional machine learning classifiers, such as SVM, decision trees, ensembles etc. With the recent overwhelming success of deep neural networks, newer neural network based approaches to steganalysis are gaining popularity, Qian et al. (2015b). For example, in Pibre et al. (2015) authors propose to use deep convolution neural networks (CNN) for steganalysis and show that classification accuracy can be significantly increased while using CNN instead of usual classifiers.",
      "startOffset" : 81,
      "endOffset" : 438
    }, {
      "referenceID" : 1,
      "context" : "Generative Adversarial Networks (GAN) is a recent approach to deep unsupervised learning, proposed in 2014 in Goodfellow et al. (2014), which is capable of dynamically representing a sampler from input data distribution and generate new data samples.",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "In Radford et al. (2015) the GAN idea was extended to deep convolutional networks (DCGAN), which are specialized for image generation.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "We define a filter F (0) that is special for steganalysis applications (see Pibre et al. (2015), Qian et al.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "We define a filter F (0) that is special for steganalysis applications (see Pibre et al. (2015), Qian et al. (2015a), Tan & Li (2014)), as follows",
      "startOffset" : 76,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "We define a filter F (0) that is special for steganalysis applications (see Pibre et al. (2015), Qian et al. (2015a), Tan & Li (2014)), as follows",
      "startOffset" : 76,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "This structure provides state-of-the-art steganalysis accuracy, Pibre et al. (2015), and the filter F (0) allows to increase convergence speed of the steganalyser S∗ training.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "WOW (Holub & Fridrich, 2012), HUGO (Pevny et al., 2010) and S-UNIWARD (Holub et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : ", 2010) and S-UNIWARD (Holub et al., 2014).",
      "startOffset" : 22,
      "endOffset" : 42
    } ],
    "year" : 2016,
    "abstractText" : "Steganography is collection of methods to hide secret information (“payload”) within non-secret information (“container”). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications.",
    "creator" : "LaTeX with hyperref package"
  }
}