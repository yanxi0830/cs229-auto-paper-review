{
  "name" : "415.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "REGULARIZING CNNS WITH LOCALLY CONSTRAINED DECORRELATIONS",
    "authors" : [ "Pau Rodrı́guez", "Jordi Gonzàlez", "Guillem Cucurull", "Josep M. Gonfaus", "Xavier Roca" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al. (2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures.\nFrom the literature, two different regularization strategies can be defined. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation. Due to their nature, although this set of strategies have proved to be very effective, they do not leverage all the capacity of the models they regularize.\nThe second group of regularizations is those which improve the effectiveness and generality of the trained model without reducing its capacity. In this second group, the most relevant approaches decorrelate the weights or feature maps, e.g. Bengio & Bergstra (2009) introduced a new criterion so as to learn slow decorrelated features while pre-training models. In the same line Bao et al. (2013) presented ”incoherent training”, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition. Although regularizations in the second group are promising and have already been used to reduce the overfitting in different tasks, even with the presence of Dropout (as shown by Cogswell et al. (2016)), they are seldom used in the large scale image recognition domain because of the small improvement margins they provide together with the computational overhead they introduce.\nAlthough they are not directly presented as regularizers, there are other strategies to reduce the overfitting such as Batch Normalization (Ioffe & Szegedy (2015)), which decreases the overfitting by reducing the internal covariance shift. In the same line, initialization strategies such as ”Xavier” (Glorot & Bengio (2010)) or ”He” (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well.\nIn this work we hypothesize that regularizing negatively correlated features is an obstacle for achieving better results and we introduce OrhoReg, a novel regularization technique that addresses the performance margin issue by only regularizing positively correlated feature weights. Moreover, OrthoReg is computationally efficient since it only regularizes the feature weights, which makes it very suitable for the latest CNN models. We verify our hypothesis through a series of experiments: first using MNIST as a proof of concept, secondly we regularize wide residual networks on CIFAR-10, CIFAR-100, and SVHN (Netzer et al. (2011)) achieving the lowest error rates in the dataset to the best of our knowledge."
    }, {
      "heading" : "2 DEALING WITH WEIGHT REDUNDANCIES",
      "text" : "Deep Neural Networks (DNN) are very expressive models which can usually have millions of parameters. However, with limited data, they tend to overfit. There is an abundant number of techniques in order to deal with this problem, from L1 and L2 regularizations (Nowlan & Hinton (1992)), early-stopping, Dropout or DropConnect. Models presenting high levels of overfitting usually have a lot of redundancy in their feature weights, capturing similar patterns with slight differences which usually correspond to noise in the training data. A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even ”dead” ones, as it was remarked by Zeiler & Fergus (2014).\nIn fact, given a set of parameters θI,j connecting a set of inputs I = {i1, i2, . . . , in} to a neuron hj , two neurons {hj , hk}, j 6= k will be positively correlated, and thus fire always together if θI,j = θI,k and negatively correlated if θI,j = −θI,k. In other words, two neurons with the same or slightly different weights will produce very similar outputs. In order to reduce the redundancy present in the network parameters, one should maximize the amount of information encoded by each neuron. From an information theory point of view, this means one should not be able to predict the output of a neuron given the output of the rest of the neurons of the layer. However, this measure requires batch statistics, huge joint probability tables, and it would have a high computational cost.\nIn this paper, we will focus on the weights correlation rather than activation independence since it still is an open problem in many neural network models and it can be addressed without introducing too much overhead, see Table 1. Then, we show that models generalize better when different feature detectors are enforced to be dissimilar. Although it might seem contradictory, CNNs can benefit from having repeated filter weights with different biases, as shown by Li et al. (2016). However, those repeated filters must be shared copies and adding too many unshared filter weights to CNNs increases overfitting and the need for stronger regularization (Zagoruyko & Komodakis (May 2016)). Thus, our proposed method and multi-bias neural networks are complementary since they jointly increase the representation power of the network with fewer parameters.\nIn order to find a good target to optimize so as to reduce the correlation between weights, it is first required to find a metric to measure it. In this paper, we propose to use the cosine similarity between feature detectors to express how strong is their relationship. Note that the cosine similarity is equivalent to the Pearson correlation for mean-centered normalized vectors, but we will use the term correlation for the sake of clarity."
    }, {
      "heading" : "2.1 ORTHOGONAL WEIGHT REGULARIZATION",
      "text" : "This section introduces the orthogonal weight regularization, a regularization technique that aims to reduce feature detector correlation enforcing local orthogonality between all pairs of weight vectors. In order to keep the magnitudes of the detectors unaffected, we have chosen the cosine similarity between the vector pairs in order to solely focus on the vectors angle β ∈ [−π, π]. Then, given any pair of feature vectors of the same size θ1, θ2 the cosine of their relative angle is:\ncos(θ1, θ2) = 〈θ1, θ2〉 ||θ1||||θ2||\n(1)\nWhere 〈θ1, θ2〉 denotes the inner product between θ1 and θ2. We then square the cosine similarity in order to define a regularization cost function for steepest descent that has its local minima when vectors are orthogonal:\nC(θ) = 1\n2 n∑ i=1 n∑ j=1,j 6=i cos2(θi, θj) = 1 2 n∑ i=1 n∑ j=1,j 6=i ( 〈θi, θj〉 ||θi||||θj || )2 (2)\nWhere θi are the weights connecting the output of the layer l − 1 to the neuron i of the layer l, which has n hidden units. Interestingly, minimizing this cost function relates to the minimization of the Frobenius norm of the cross-covariance matrix without the diagonal. This cost will be added to the global cost of the model J(θ;X, y), where X are the inputs and y are the labels or targets, obtaining J̃(θ;X, y) = J(θ;X, y) + γC(θ). Note that γ is an hyperparameter that weights the relative contribution of the regularization term. We can now define the gradient with respect to the parameters:\nδ\nδθ(i,j) C(θ) = n∑ k=1,k 6=i θ(k,j)〈θi, θk〉 〈θi, θi〉〈θk, θk〉 − θ(i,j)〈θi, θk〉2 〈θi, θi〉2〈θk, θk〉 (3)\nThe second term is introduced by the magnitude normalization. As magnitudes are not relevant for the vector angle problem, this equation can be simplified just by assuming normalized feature detectors:\nδ\nδθ(i,j) C(θ) = n∑ k=1,k 6=i θ(k,j)〈θi, θk〉 (4)\nWe then add eq. 4 to the backpropagation gradient:\n∆θ(i,j) = −α ( ∇Jθ(i,j) + γ n∑ k=1,k 6=i θ(k,j)〈θi, θk〉 )\n(5)\nWhere α is the global learning rate coefficient, J any target loss function for the backpropagation algorithm.\nAlthough this update can be done sequentially for each feature-detector pair, it can be vectorized to speedup computations. Let Θ be a matrix where each row is a feature detector θ(I,j) corresponding to the normalized weights connecting the whole input I of the layer to the neuron j. Then, ΘΘt contains the inner product of each pair of vectors i and j in each position i, j. Subsequently, we\nAlgorithm 1 Orthogonal Regularization Step.\nRequire: Layer parameter matrices Θl, regularization coefficient γ, global learning rate α. 1: for each layer l to regularize do 2: η1 = norm rows(Θl) . Keep norm of the rows of Θl. 3: Θl1 = div rows(Θ\nl, η1) . Keep a Θl1 with normalized rows. 4: innerProdMat = Θl1transpose(Θ l 1) 5: ∇Θl1 = γ(innerProdMat− diag(innerProdMat))Θl1 . Second term in eq. 6 6: ∆Θl = −α(∇JΘl + γ∇Θl1) . Complete eq. 6. 7: end for\n(a) Global loss plot (eq. 2) (b) Local loss plot (eq. 7)\n(c) Direction of gradients for the loss in (a). (d) Direction of gradients for loss in (b).\nFigure 1: Comparison between the two loss functions represented by eq.2 and 7. (a) is the original loss, (b) is the new loss that discards negative correlations given for different λ values. It can be seen λ = 10 reaches a plateau when approximating to π2 . (c) and (d) shows the directions of the gradients for the two loss functions above. For instance, a red arrow coming from a green ball represents the gradient of the loss between the red and green balls with respect to the green one. In (d) most of the arrows disappear since the loss in (b) only applies to angles smaller than π2 .\nsubtract the diagonal so as to ignore the angle from each feature with respect to itself and multiply by Θ to compute the final value corresponding to the sum in eq. 5:\n∆Θ = −α ( ∇JΘ + γ(ΘΘt − diag(ΘΘt))Θ ) (6)\nWhere the second term is∇CΘ. Algorithm 1 summarizes the steps in order to apply OrthoReg."
    }, {
      "heading" : "2.2 NEGATIVE CORRELATIONS",
      "text" : "Note that the presented algorithm, based on the cosine similarity, penalizes any kind of correlation between all pairs of feature detectors, i.e. the positive and the negative correlations, see Figure 1a. However, negative correlations are related to inhibitory connections, competitive learning, and self-organization. In fact, there is evidence that negative correlations can help a neural population to increase the signal-to-noise ratio (Chelaru & Dragoi (2016)) in the V1. In order to find out\nthe advantages of keeping negative correlations, we propose to use an exponential to squash the gradients for angles greater than π2 (orthogonal):\nC(θ) = n∑ i=1 n∑ j=1,j 6=i log(1 + eλ(cos(θi,θj)−1)) = log(1 + eλ(〈θi,θj〉−1)), ||θi|| = ||θj || = 1 (7)\nWhere λ is a coefficient that controls the minimum angle-of-influence of the regularizer, i.e. the minimum angle between two feature weights so that there exists a gradient pushing them apart, see Figure 1b. We empirically found that the regularizer worked well for λ = 10, see Figure 2b. Note that when λ ' 10 the loss and the gradients approximate to zero when vectors are at more than π2 (orthogonal). As a result of incorporating the squashing function on the cosine similarity, negatively correlated feature weights will not be regularized. This is different from all previous approaches and the loss presented in eq. 2, where all pairs of weight vectors influence each other. Thus, from now on, the loss in eq. 2 is named as global loss and the loss in eq. 7 is named as local loss.\nThe derivative of eq. 7 is:\nδ\nδθ(i,j) C(θ) = n∑ k=1,k 6=i λ eλ〈θi,θk〉θ(k,j) eλ〈θi,θk〉 + eλ (8)\nThen, given the element-wise exponential operator exp, we define the following expression in order to simplify the formulas:\nΘ̂ = exp(λ(ΘΘt)) (9)\nand thus, the ∆ in vectorial form can be formulated as:\n∇CΘ = λ (Θ̂− diag(Θ̂))Θ\nΘ̂− diag(Θ̂) + eλ (10)\nIn order to provide a visual example, we have created a 2D toy dataset and used the previous equations for positive and negative γ values, see Figure 2. As expected, it can be seen that the angle between all pairs of adjacent feature weights becomes more uniform after regularization. Note that Figure 2b shows that regularization with the global loss (eq. 2) results in less uniform angles than using the local loss as shown in 2c (which corresponds to the local loss presented in eq. 7) because vectors in opposite quadrants still influence each other. This is why in Figure 2d, it can be seen that the mean nearest neighbor angle using the global loss (b) is more unstable than the local loss (c). As a proof of concept, we also performed gradient ascent, which minimizes the angle between the vectors. Thus, in Figures 2e and 2f, it can be seen that the locality introduced by the local loss reaches a stable configuration where feature weights with angle π2 are too far to attract each other.\nThe effects of global and local regularizations on Alexnet, VGG-16 and a 50-layer ResNet are shown on Figure 3. As it can be seen, OrthoReg reaches higher decorrelation bounds. Lower decorrelation peaks are still observed when the input dimensionality of the layers is smaller than the output since all vectors cannot be orthogonal at the same time. In this case, local regularization largely outperforms global regularization since it removes interferences caused by negatively correlated feature weights. This suggests why increasing fully connected layers’ size has not improved networks performance."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "In this section we provide a set of experiments that verify that (i) training with the proposed regularization increases the performance of naive unregularized models, (ii) negatively correlated feature weights are useful, and (iii) the proposed regularization improves the performance of state-of-the-art models."
    }, {
      "heading" : "3.1 VERIFICATION EXPERIMENTS",
      "text" : "As a sanity check, we first train a three-hidden-layer Multi-Layer Perceptron (MLP) with ReLU non-liniarities on the MNIST dataset (LeCun et al. (1998)). Our code is based in the train-a-digit-classifier example included in torch/demos1, which uses an upsampled version of the dataset (32×32). The only pre-processing applied to the data is a global standardization. The model is trained with SGD and a batch size of 200 during 200 epochs. No momentum neither weight decay was applied. By default, the magnitude of the weights of this experiments is recovered after each regularization step in order to prove the regularization only affects their angle.\nSensitivity to hyperparameters. We train a three-hidden-layer MLP with 1024 hidden units, and different γ and λ values so as to verify how they affect the performance of the model. Figure 4a\n1https://github.com/torch/demos\nshows that the model effectively achieves the best error rate for the highest gamma value (γ = 1), thus proving the advantages of the regularization. On Figure 4b, we verify that higher regularization rates produce more general models. Figure 5a depicts the sensitivity of the model to λ. As expected, the best value is found when lambda corresponds to Orthogonality (λ ' 10). Negative Correlations. Figure 5b highlights the difference between regularizing with the global or the local regularizer. Although both regularizations reach better error rates than the unregularized counterpart, the local regularization is better than the global. This confirms the hypothesis that negative correlations are useful and thus, performance decreases when we reduce them.\nCompatibility with initialization and dropout. To demonstrate the proposed regularization can help even when other regularizations are present, we trained a CNN with (i) dropout (c32-c64-l512-d0.5-l10)2 or (ii) LSUV initialization (Mishkin & Matas (2016)). In Table 2, we show that best results are obtained when orthogonal regularization is present. The results are consistent with the hypothesis that OrthoReg, as well as Dropout and LSUV, focuses on reducing the model redundancy. Thus, when one of them is present, the margin of improvement for the others is reduced.\n2cxx = convolution with xx filters. lxx = fully-connected with xx units. dxx = dropout with prob xx."
    }, {
      "heading" : "3.2 REGULARIZATION ON CIFAR-10 AND CIFAR-100",
      "text" : "We show that the proposed OrthoReg can help to improve the performance of state-of-the-art models such as deep residual networks (He et al. (2015a)). In order to show the regularization is suitable for deep CNNs, we successfuly regularize a 110-layer ResNet3 on CIFAR-10, decreasing its error from 6.55% to 6.29% without data augmentation.\nIn order to compare with the most recent state-of-the-art, we train a wide residual network (Zagoruyko & Komodakis (November 2016)) on CIFAR-10 and CIFAR-100. The experiment is based on a torch implementation of the 28-layer and 10th width factor wide deep residual model, for which the median error rate on CIFAR-10 is 3.89% and 18.85% on CIFAR-1004. As it can be seen in Figure 6, regularizing with OrthoReg yields the best test error rates compared to the baselines.\nThe regularization coefficient γ was chosen using grid search although similar values were found for all the experiments, specially if regularization gradients are normalized before adding them to the weights. The regularization was equally applied to all the convolution layers of the (wide) ResNet. We found that, although the regularized models were already using weight decay, dropout, and batch normalization, best error rates were always achieved with OrthoReg.\nTable 3 compares the performance of the regularized models with other state-of-the-art results. As it can be seen the regularized model surpasses the state of the art, with a 5.1% relative error improvement on CIFAR-10, and a 1.5% relative error improvement on CIFAR-100."
    }, {
      "heading" : "3.3 REGULARIZATION ON SVHN",
      "text" : "For SVHN we follow the procedure depicted in Zagoruyko & Komodakis (May 2016), training a wide residual network of depth=28, width=4, and dropout. Results are shown in Table 4. As it\n3https://github.com/gcr/torch-residual-networks 4https://github.com/szagoruyko/wide-residual-networks\ncan be seen, we reduce the error rate from 1.64% to 1.54%, which is the lowest value reported on this dataset to the best of our knowledge."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "Regularization by feature decorrelation can reduce Neural Networks overfitting even in the presence of other kinds of regularizations. However, especially when the number of feature detectors is higher than the input dimensionality, its decorrelation capacity is limited due to the effects of negatively correlated features. We showed that imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing regularizers to reach higher decorrelation bounds, and reducing the overfitting more effectively.\nIn particular, we show that the models regularized with the constrained regularization present lower overfitting even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of 110-layer ResNets and wide ResNets on CIFAR-10, CIFAR-100, and SVHN improving their performance. Note that despite OrthoReg consistently improves state of the art ReLU networks, the choice of the activation function could affect regularizers like the one presented in this work. In this sense, the effect of asymmetrical activations on feature correlations and regularizers should be further investigated in the future."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "Authors acknowledge the support of the Spanish project TIN2015-65464-R (MINECO/FEDER), the 2016FI B 01163 grant of Generalitat de Catalunya, and the COST Action IC1307 iV&L Net (European Network on Integrating Vision and Language) supported by COST (European Cooperation in Science and Technology). We also gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU and a GTX TITAN GPU, used for this research."
    } ],
    "references" : [ {
      "title" : "Incoherent training of deep neural networks to de-correlate bottleneck features for speech recognition",
      "author" : [ "Yebo Bao", "Hui Jiang", "Lirong Dai", "Cong Liu" ],
      "venue" : "IEEE ICASSP,",
      "citeRegEx" : "Bao et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2013
    }, {
      "title" : "Slow, decorrelated features for pretraining complex cell-like networks",
      "author" : [ "Yoshua Bengio", "James S Bergstra" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Bengio and Bergstra.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio and Bergstra.",
      "year" : 2009
    }, {
      "title" : "Negative correlations in visual cortical networks",
      "author" : [ "Mircea I Chelaru", "Valentin Dragoi" ],
      "venue" : "Cerebral Cortex,",
      "citeRegEx" : "Chelaru and Dragoi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chelaru and Dragoi.",
      "year" : 2016
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (ELUs)",
      "author" : [ "Djork-Arn Clevert", "Thomas Unterthiner", "Sepp Hochreiter" ],
      "venue" : null,
      "citeRegEx" : "Clevert et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2016
    }, {
      "title" : "Reducing overfitting in deep networks by decorrelating representations",
      "author" : [ "Michael Cogswell", "Faruk Ahmed", "Ross Girshick", "Larry Zitnick", "Dhruv Batra" ],
      "venue" : null,
      "citeRegEx" : "Cogswell et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cogswell et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Fractional max-pooling",
      "author" : [ "Benjamin Graham" ],
      "venue" : "arXiv preprint arXiv:1412.6071,",
      "citeRegEx" : "Graham.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graham.",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Multi-bias non-linear activation in deep neural networks",
      "author" : [ "Hongyang Li", "Wanli Ouyang", "Xiaogang Wang" ],
      "venue" : "arXiv preprint arXiv:1604.00676,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "All you need is a good init",
      "author" : [ "Dmytro Mishkin", "Jiri Matas" ],
      "venue" : null,
      "citeRegEx" : "Mishkin and Matas.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mishkin and Matas.",
      "year" : 2016
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Simplifying neural networks by soft weight-sharing",
      "author" : [ "Steven J. Nowlan", "Geoffrey E. Hinton" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Nowlan and Hinton.,? \\Q1992\\E",
      "shortCiteRegEx" : "Nowlan and Hinton.",
      "year" : 1992
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : null,
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M. Saxe", "James L. McClelland", "Surya Ganguli" ],
      "venue" : null,
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost T. Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "In ICLR (workshop track),",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2015
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh K. Srivastava", "Klaus Greff", "Jürgen Schmidhuber" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In CVPR, pp",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Li Wan", "Matthew D Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Wan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2013
    }, {
      "title" : "Wide residual networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2016
    }, {
      "title" : "Wide residual networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "arXiv preprint arXiv:1605.07146,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2016
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D. Zeiler", "Rob Fergus" ],
      "venue" : "In ECCV, pp",
      "citeRegEx" : "Zeiler and Fergus.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zeiler and Fergus.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)).",
      "startOffset" : 141,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al.",
      "startOffset" : 141,
      "endOffset" : 221
    }, {
      "referenceID" : 7,
      "context" : "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al. (2015); He et al.",
      "startOffset" : 141,
      "endOffset" : 272
    }, {
      "referenceID" : 5,
      "context" : "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)).",
      "startOffset" : 8,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. From the literature, two different regularization strategies can be defined. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al.",
      "startOffset" : 8,
      "endOffset" : 655
    }, {
      "referenceID" : 5,
      "context" : "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. From the literature, two different regularization strategies can be defined. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al.",
      "startOffset" : 8,
      "endOffset" : 735
    }, {
      "referenceID" : 5,
      "context" : "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. From the literature, two different regularization strategies can be defined. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation.",
      "startOffset" : 8,
      "endOffset" : 792
    }, {
      "referenceID" : 5,
      "context" : "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. From the literature, two different regularization strategies can be defined. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation. Due to their nature, although this set of strategies have proved to be very effective, they do not leverage all the capacity of the models they regularize. The second group of regularizations is those which improve the effectiveness and generality of the trained model without reducing its capacity. In this second group, the most relevant approaches decorrelate the weights or feature maps, e.g. Bengio & Bergstra (2009) introduced a new criterion so as to learn slow decorrelated features while pre-training models.",
      "startOffset" : 8,
      "endOffset" : 1255
    }, {
      "referenceID" : 0,
      "context" : "In the same line Bao et al. (2013) presented ”incoherent training”, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "In the same line Bao et al. (2013) presented ”incoherent training”, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition. Although regularizations in the second group are promising and have already been used to reduce the overfitting in different tasks, even with the presence of Dropout (as shown by Cogswell et al. (2016)), they are seldom used in the large scale image recognition domain because of the small improvement margins they provide together with the computational overhead they introduce.",
      "startOffset" : 17,
      "endOffset" : 396
    }, {
      "referenceID" : 4,
      "context" : "Table 1: Count of the Flops for the models used in this paper: the 3-hidden-layer MLP and the 110layer ResNet we use later in the experiments section when not regularized, using DeCov (Cogswell et al. (2016)) and using OrthoReg.",
      "startOffset" : 185,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "In the same line, initialization strategies such as ”Xavier” (Glorot & Bengio (2010)) or ”He” (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks.",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "In the same line, initialization strategies such as ”Xavier” (Glorot & Bengio (2010)) or ”He” (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al.",
      "startOffset" : 95,
      "endOffset" : 475
    }, {
      "referenceID" : 7,
      "context" : "In the same line, initialization strategies such as ”Xavier” (Glorot & Bengio (2010)) or ”He” (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well.",
      "startOffset" : 95,
      "endOffset" : 589
    }, {
      "referenceID" : 7,
      "context" : "In the same line, initialization strategies such as ”Xavier” (Glorot & Bengio (2010)) or ”He” (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well. In this work we hypothesize that regularizing negatively correlated features is an obstacle for achieving better results and we introduce OrhoReg, a novel regularization technique that addresses the performance margin issue by only regularizing positively correlated feature weights. Moreover, OrthoReg is computationally efficient since it only regularizes the feature weights, which makes it very suitable for the latest CNN models. We verify our hypothesis through a series of experiments: first using MNIST as a proof of concept, secondly we regularize wide residual networks on CIFAR-10, CIFAR-100, and SVHN (Netzer et al. (2011)) achieving the lowest error rates in the dataset to the best of our knowledge.",
      "startOffset" : 95,
      "endOffset" : 1280
    }, {
      "referenceID" : 11,
      "context" : "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even ”dead” ones, as it was remarked by Zeiler & Fergus (2014).",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even ”dead” ones, as it was remarked by Zeiler & Fergus (2014). In fact, given a set of parameters θI,j connecting a set of inputs I = {i1, i2, .",
      "startOffset" : 55,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even ”dead” ones, as it was remarked by Zeiler & Fergus (2014). In fact, given a set of parameters θI,j connecting a set of inputs I = {i1, i2, . . . , in} to a neuron hj , two neurons {hj , hk}, j 6= k will be positively correlated, and thus fire always together if θI,j = θI,k and negatively correlated if θI,j = −θI,k. In other words, two neurons with the same or slightly different weights will produce very similar outputs. In order to reduce the redundancy present in the network parameters, one should maximize the amount of information encoded by each neuron. From an information theory point of view, this means one should not be able to predict the output of a neuron given the output of the rest of the neurons of the layer. However, this measure requires batch statistics, huge joint probability tables, and it would have a high computational cost. In this paper, we will focus on the weights correlation rather than activation independence since it still is an open problem in many neural network models and it can be addressed without introducing too much overhead, see Table 1. Then, we show that models generalize better when different feature detectors are enforced to be dissimilar. Although it might seem contradictory, CNNs can benefit from having repeated filter weights with different biases, as shown by Li et al. (2016). However, those repeated filters must be shared copies and adding too many unshared filter weights to CNNs increases overfitting and the need for stronger regularization (Zagoruyko & Komodakis (May 2016)).",
      "startOffset" : 55,
      "endOffset" : 1478
    }, {
      "referenceID" : 12,
      "context" : "As a sanity check, we first train a three-hidden-layer Multi-Layer Perceptron (MLP) with ReLU non-liniarities on the MNIST dataset (LeCun et al. (1998)).",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "We show that the proposed OrthoReg can help to improve the performance of state-of-the-art models such as deep residual networks (He et al. (2015a)).",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "57 YES Highway Network (Srivastava et al. (2015)) 7.",
      "startOffset" : 24,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "24 YES All-CNN (Springenberg et al. (2015)) 7.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "71 NO 110-Layer ResNet (He et al. (2015a)) 6.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "4 NO ELU-Network (Clevert et al. (2016)) 6.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "4 NO ELU-Network (Clevert et al. (2016)) 6.55 24.28 NO OrthoReg on 110-Layer ResNet* 6.29± 0.19 28.33± 0.5 NO LSUV (Mishkin & Matas (2016)) 5.",
      "startOffset" : 18,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "4 NO ELU-Network (Clevert et al. (2016)) 6.55 24.28 NO OrthoReg on 110-Layer ResNet* 6.29± 0.19 28.33± 0.5 NO LSUV (Mishkin & Matas (2016)) 5.84 YES Fract. Max-Pooling (Graham (2014)) 4.",
      "startOffset" : 18,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "92 Stochastic Depth ResNet (Huang et al. (2016)) 1.",
      "startOffset" : 28,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.",
    "creator" : "LaTeX with hyperref package"
  }
}