{
  "name" : "418.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UNROLLED GENERATIVE ADVERSARIAL NETWORKS",
    "authors" : [ "Luke Metz", "Ben Poole", "David Pfau" ],
    "emails" : [ "lmetz@google.com", "poole@cs.stanford.edu", "pfau@google.com", "jaschasd@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The use of deep neural networks as generative models for complex data has made great advances in recent years. This success has been achieved through a surprising diversity of training losses and model architectures, including denoising autoencoders (Vincent et al., 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Kulkarni et al., 2015; Burda et al., 2015; Kingma et al., 2016), generative stochastic networks (Alain et al., 2015), diffusion probabilistic models (Sohl-Dickstein et al., 2015), autoregressive models (Theis & Bethge, 2015; van den Oord et al., 2016a;b), real non-volume preserving transformations (Dinh et al., 2014; 2016), Helmholtz machines (Dayan et al., 1995; Bornschein et al., 2015), and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014)."
    }, {
      "heading" : "1.1 GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "While most deep generative models are trained by maximizing log likelihood or a lower bound on log likelihood, GANs take a radically different approach that does not require inference or explicit calculation of the data likelihood. Instead, two models are used to solve a minimax game: a generator which samples data, and a discriminator which classifies the data as real or generated. In theory these models are capable of modeling an arbitrarily complex probability distribution. When using the optimal discriminator for a given class of generators, the original GAN proposed by Goodfellow et al. minimizes the Jensen-Shannon divergence between the data distribution and the generator, and extensions generalize this to a wider class of divergences (Nowozin et al., 2016; Sonderby et al., 2016; Poole et al., 2016).\nThe ability to train extremely flexible generating functions, without explicitly computing likelihoods or performing inference, and while targeting more mode-seeking divergences as made GANs extremely successful in image generation (Odena et al., 2016; Salimans et al., 2016; Radford et al., 2015), and image super resolution (Ledig et al., 2016). The flexibility of the GAN framework has also enabled a number of successful extensions of the technique, for instance for structured prediction (Reed et al., 2016a;b; Odena et al., 2016), training energy based models (Zhao et al., 2016), and combining the GAN loss with a mutual information loss (Chen et al., 2016). ∗Work done as a member of the Google Brain Residency program (g.co/brainresidency) †Work completed as part of a Google Brain internship\nIn practice, however, GANs suffer from many issues, particularly during training. One common failure mode involves the generator collapsing to produce only a single sample or a small family of very similar samples. Another involves the generator and discriminator oscillating during training, rather than converging to a fixed point. In addition, if one agent becomes much more powerful than the other, the learning signal to the other agent becomes useless, and the system does not learn. To train GANs many tricks must be employed, such as careful selection of architectures (Radford et al., 2015), minibatch discrimination (Salimans et al., 2016), and noise injection (Salimans et al., 2016; Sonderby et al., 2016). Even with these tricks the set of hyperparameters for which training is successful is generally very small in practice.\nOnce converged, the generative models produced by the GAN training procedure normally do not cover the whole distribution (Dumoulin et al., 2016; Che et al., 2016), even when targeting a modecovering divergence such as KL. Additionally, because it is intractable to compute the GAN training loss, and because approximate measures of performance such as Parzen window estimates suffer from major flaws (Theis et al., 2016), evaluation of GAN performance is challenging. Currently, human judgement of sample quality is one of the leading metrics for evaluating GANs. In practice this metric does not take into account mode dropping if the number of modes is greater than the number of samples one is visualizing. In fact, the mode dropping problem generally helps visual sample quality as the model can choose to focus on only the most common modes. These common modes correspond, by definition, to more typical samples. Additionally, the generative model is able to allocate more expressive power to the modes it does cover than it would if it attempted to cover all modes."
    }, {
      "heading" : "1.2 DIFFERENTIATING THROUGH OPTIMIZATION",
      "text" : "Many optimization schemes, including SGD, RMSProp (Tieleman & Hinton, 2012), and Adam (Kingma & Ba, 2014), consist of a sequence of differentiable updates to parameters. Gradients can be backpropagated through unrolled optimization updates in a similar fashion to backpropagation through a recurrent neural network. The parameters output by the optimizer can thus be included, in a differentiable way, in another objective (Maclaurin et al., 2015). This idea was first suggested for minimax problems in (Pearlmutter & Siskind, 2008), while (Zhang & Lesser, 2010) provided a theoretical analysis and experimental results on differentiating through a single step of gradient ascent for simple matrix games. Differentiating through unrolled optimization was first scaled to deep networks in (Maclaurin et al., 2015), where it was used for hyperparameter optimization. More recently, (Belanger & McCallum, 2015; Han et al., 2016; Andrychowicz et al., 2016) backpropagate through optimization procedures in contexts unrelated to GANs or minimax games.\nIn this work we address the challenges of unstable optimization and mode collapse in GANs by unrolling optimization of the discriminator objective during training."
    }, {
      "heading" : "2 METHOD",
      "text" : ""
    }, {
      "heading" : "2.1 GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "The GAN learning problem is to find the optimal parameters θ∗G for a generator function G (z; θG) in a minimax objective,\nθ∗G = argmin θG max θD f (θG, θD) (1)\n= argmin θG\nf (θG, θ ∗ D (θG)) (2)\nθ∗D (θG) = argmax θD f (θG, θD) , (3)\nwhere f is commonly chosen to be\nf (θG, θD) = Ex∼pdata [log (D (x; θD))] + Ez∼N (0,I) [log (1−D (G (z; θG) ; θD))] . (4) Here x ∈ X is the data variable, z ∈ Z is the latent variable, pdata is the data distribution, the discriminator D (·; θD) : X → [0, 1] outputs the estimated probability that a sample x comes from the data distribution, θD and θG are the discriminator and generator parameters, and the generator function G (·; θG) : Z → X transforms a sample in the latent space into a sample in the data space.\nFor the minimax loss in Eq. 4, the optimal discriminator D∗ (x) is a known smooth function of the generator probability pG (x) (Goodfellow et al., 2014),\nD∗ (x) = pdata (x)\npdata (x) + pG (x) . (5)\nWhen the generator loss in Eq. 2 is rewritten directly in terms of pG (x) and Eq. 5 rather than θG and θ∗D (θG), then it is similarly a smooth function of pG (x). These smoothness guarantees are typically lost when D (x; θD) and G (z; θG) are drawn from parametric families. They nonetheless suggest that the true generator objective in Eq. 2 will often be well behaved, and is a desirable target for direct optimization.\nExplicitly solving for the optimal discriminator parameters θ∗D (θG) for every update step of the generator G is computationally infeasible for discriminators based on neural networks. Therefore this minimax optimization problem is typically solved by alternating gradient descent on θG and ascent on θD.\nThe optimal solution θ∗ = {θ∗G, θ∗D} is a fixed point of these iterative learning dynamics. Additionally, if f (θG, θD) is convex in θG and concave in θD, then alternating gradient descent (ascent) trust region updates are guaranteed to converge to the fixed point, under certain additional weak assumptions (Juditsky et al., 2011). However in practice f (θG, θD) is typically very far from convex in θG and concave in θD, and updates are not constrained in an appropriate way. As a result GAN training suffers from mode collapse, undamped oscillations, and other problems detailed in Section 1.1. In order to address these difficulties, we will introduce a surrogate objective function fK (θG, θD) for training the generator which more closely resembles the true generator objective f (θG, θ∗D (θG))."
    }, {
      "heading" : "2.2 UNROLLING GANS",
      "text" : "A local optimum of the discriminator parameters θ∗D can be expressed as the fixed point of an iterative optimization procedure,\nθ0D = θD (6)\nθk+1D = θ k D + η\nk df ( θG, θ k D ) dθkD\n(7)\nθ∗D (θG) = lim k→∞ θkD, (8)\nwhere ηk is the learning rate schedule. For clarity, we have expressed Eq. 7 as a full batch steepest gradient ascent equation. More sophisticated optimizers can be similarly unrolled. In our experiments we unroll Adam (Kingma & Ba, 2014).\nBy unrolling for K steps, we create a surrogate objective for the update of the generator, fK (θG, θD) = f ( θG, θ K D (θG, θD) ) . (9)\nWhen K = 0 this objective corresponds exactly to the standard GAN objective, while as K → ∞ it corresponds to the true generator objective function f (θG, θ∗D (G)). By adjusting the number of unrolling steps K, we are thus able to interpolate between standard GAN training dynamics with their associated pathologies, and more costly gradient descent on the true generator loss."
    }, {
      "heading" : "2.3 PARAMETER UPDATES",
      "text" : "The generator and discriminator parameter updates using this surrogate loss are\nθG ← θG − η dfK (θG, θD)\ndθG (10)\nθD ← θD + η df (θG, θD)\ndθD . (11)\nFor clarity we use full batch steepest gradient descent (ascent) with stepsize η above, while in experiments we instead use minibatch Adam for both updates. The gradient in Eq. 10 requires backpropagating through the optimization process in Eq. 7. A clear description of differentiation through\ngradient descent is given as Algorithm 2 in (Maclaurin et al., 2015), though in practice the use of an automatic differentiation package means this step does not need to be programmed explicitly. A pictorial representation of these updates is provided in Figure 1.\nIt is important to distinguish this from an approach suggested in (Goodfellow et al., 2014), that several update steps of the discriminator parameters should be run before each single update step for the generator. In that approach, the update steps for both models are still gradient descent (ascent) with respect to fixed values of the other model parameters, rather than the surrogate loss we describe in Eq. 9. Performing K steps of discriminator update between each single step of generator update corresponds to updating the generator parameters θG using only the first term in Eq. 12 below."
    }, {
      "heading" : "2.4 THE MISSING GRADIENT TERM",
      "text" : "To better understand the behavior of the surrogate loss fK (θG, θD), we examine its gradient with respect to the generator parameters θG,\ndfK (θG, θD) dθG = ∂f\n( θG, θ K D (θG, θD) ) ∂θG + ∂f ( θG, θ K D (θG, θD) ) ∂θKD (θG, θD) dθKD (θG, θD) dθG . (12)\nStandard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient, with θKD (θG, θD) being the parameters resulting from the discriminator update step. An optimal generator for any fixed discriminator is a delta function at the x to which the discriminator assigns highest data probability. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function.\nThe second term captures how the discriminator would react to a change in the generator. It reduces the tendency of the generator to engage in mode collapse. For instance, the second term reflects that as the generator collapses towards a delta function, the discriminator reacts and assigns lower probability to that state, increasing the generator loss. It therefore discourages the generator from collapsing, and may improve stability.\nAs K →∞, θKD goes to a local optimum of f , where ∂f ∂θKD = 0, and therefore the second term in Eq. 12 goes to 0 (Danskin, 1967). The gradient of the unrolled surrogate loss fK (θG, θD) with respect to θG is thus identical to the gradient of the standard GAN loss f (θG, θD) both whenK = 0 and when K → ∞, where we take K → ∞ to imply that in the standard GAN the discriminator is also fully optimized between each generator update. Between these two extremes, fK (θG, θD) captures additional information about the response of the discriminator to changes in the generator."
    }, {
      "heading" : "2.5 CONSEQUENCES OF THE SURROGATE LOSS",
      "text" : "GANs can be thought of as a game between the discriminator (D) and the generator (G). The agents take turns taking actions and updating their parameters until a Nash equilibrium is reached. The optimal action for D is to evaluate the probability ratio pdata(x)pG(x)+pdata(x) for the generator’s move x (Eq. 5). The optimal generator action is to move its mass to maximize this ratio.\nThe initial move forGwill be to move as much mass as its parametric family and update step permits to the single point that maximizes the ratio of probability densities. The action D will then take is quite simple. It will track that point, and to the extent allowed by its own parametric family and update step assign low data probability to it, and uniform probability everywhere else. This cycle of G moving and D following will repeat forever or converge depending on the rate of change of the two agents. This is similar to the situation in simple matrix games like rock-paper-scissors and matching pennies, where alternating gradient descent (ascent) with a fixed learning rate is known not to converge (Singh et al., 2000; Bowling & Veloso, 2002).\nIn the unrolled case, however, this undesirable behavior no longer occurs. NowG’s actions take into account how D will respond. In particular, G will try to make steps that D will have a hard time responding to. This extra information helps the generator spread its mass to make the next D step less effective instead of collapsing to a point.\nIn principle, a surrogate loss function could be used for both D and G. In the case of 1-step unrolled optimization this is known to lead to convergence for games in which gradient descent (ascent) fails (Zhang & Lesser, 2010). However, the motivation for using the surrogate generator loss in Section 2.2, of unrolling the inner of two nested min and max functions, does not apply to using a surrogate discriminator loss. Additionally, it is more common for the discriminator to overpower the generator than vice-versa when training a GAN. Giving more information to G by allowing it to ‘see into the future’ may thus help the two models be more balanced."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "In this section we demonstrate improved mode coverage and stability by applying this technique to five datasets of increasing complexity. Evaluation of generative models is a notoriously hard problem (Theis et al., 2016). As such the de facto standard in GAN literature has become sample quality as evaluated by a human and/or evaluated by a heuristic (Inception score for example, (Salimans et al., 2016)). While these evaluation metrics do a reasonable job capturing sample quality, they fail to capture sample diversity. In our first 2 experiments diversity is easily evaluated via visual inspection. In our later experiments this is not the case, and we will use a variety of methods to quantify coverage of samples. Our measures are individually strongly suggestive of unrolling reducing mode-collapse and improving stability, but none of them alone are conclusive. We believe that taken together however, they provide extremely compelling evidence for the advantages of unrolling.\nWhen doing stochastic optimization, we must choose which minibatches to use in the unrolling updates in Eq. 7. We experimented with both a fixed minibatch and re-sampled minibatches for each unrolling step, and found it did not significantly impact the result. We use fixed minibatches for all experiments in this section.\nWe provide a reference implementation of this technique at github.com/poolio/unrolled gan."
    }, {
      "heading" : "3.1 MIXTURE OF GAUSSIANS DATASET",
      "text" : "To illustrate the impact of discriminator unrolling, we train a simple GAN architecture on a 2D mixture of 8 Gaussians arranged in a circle. For a detailed list of architecture and hyperparameters see Appendix A. Figure 2 shows the dynamics of this model through time. Without unrolling the generator rotates around the valid modes of the data distribution but is never able to spread out mass. When adding in unrolling steps G quickly learns to spread probability mass and the system converges to the data distribution.\nIn Appendix B we perform further experiments on this toy dataset. We explore how unrolling compares to historical averaging, and compares to using the unrolled discriminator to update the\ngenerator, but without backpropagating through the generator. In both cases we find that the unrolled objective performs better."
    }, {
      "heading" : "3.2 PATHOLOGICAL MODEL WITH MISMATCHED GENERATOR AND DISCRIMINATOR",
      "text" : "To evaluate the ability of this approach to improve trainability, we look to a traditionally challenging family of models to train – recurrent neural networks (RNNs). In this experiment we try to generate MNIST samples using an LSTM (Hochreiter & Schmidhuber, 1997). MNIST digits are 28x28 pixel images. At each timestep of the generator LSTM, it outputs one column of this image, so that after 28 timesteps it has output the entire sample. We use a convolutional neural network as the discriminator. See Appendix C for the full model and training details. Unlike in all previously successful GAN models, there is no symmetry between the generator and the discriminator in this task, resulting in a more complex power balance. Results can be seen in Figure 3. Once again, without unrolling the model quickly collapses, and rotates through a sequence of single modes. Instead of rotating spatially, it cycles through proto-digit like blobs. When running with unrolling steps the generator disperses and appears to cover the whole data distribution, as in the 2D example."
    }, {
      "heading" : "3.3 MODE AND MANIFOLD COLLAPSE USING AUGMENTED MNIST",
      "text" : "GANs suffer from two different types of model collapse – collapse to a subset of data modes, and collapse to a sub-manifold within the data distribution. In these experiments we isolate both effects using artificially constructed datasets, and demonstrate that unrolling can largely rescue both types of collapse."
    }, {
      "heading" : "3.3.1 DISCRETE MODE COLLAPSE",
      "text" : "To explore the degree to which GANs drop discrete modes in a dataset, we use a technique similar to one from (Che et al., 2016). We construct a dataset by stacking three randomly chosen MNIST digits, so as to construct an RGB image with a different MNIST digit in each color channel. This new dataset has 1,000 distinct modes, corresponding to each combination of the ten MNIST classes in the three channels.\nWe train a GAN on this dataset, and generate samples from the trained model (25,600 samples for all experiments). We then compute the predicted class label of each color channel using a pre-trained MNIST classifier. To evaluate performance, we use two metrics: the number of modes for which the generator produced at least one sample, and the KL divergence between the model and the expected data distribution. Within this discrete label space, a KL divergence can be estimated tractably between the generated samples and the data distribution over classes, where the data distribution is a uniform distribution over all 1,000 classes.\nAs presented in Table 1, as the number of unrolling steps is increased, both mode coverage and reverse KL divergence improve. Contrary to (Che et al., 2016), we found that reasonably sized models (such as the one used in Section 3.4) covered all 1,000 modes even without unrolling. As such we use smaller convolutional GAN models. Details on the models used are provided in Appendix E.\nWe observe an additional interesting effect in this experiment. The benefits of unrolling increase as the discriminator size is reduced. We believe unrolling effectively increases the capacity of the discriminator. The unrolled discriminator can better react to any specific way in which the generator is producing non-data-like samples. When the discriminator is weak, the positive impact of unrolling is thus larger."
    }, {
      "heading" : "3.3.2 MANIFOLD COLLAPSE",
      "text" : "In addition to discrete modes, we examine the effect of unrolling when modeling continuous manifolds. To get at this quantity, we constructed a dataset consisting of colored MNIST digits. Unlike in the previous experiment, a single MNIST digit was chosen, and then assigned a single monochromatic color. With a perfect generator, one should be able to recover the distribution of colors used to generate the digits. We use colored MNIST digits so that the generator also has to model the digits, which makes the task sufficiently complex that the generator is unable to perfectly solve it. The color of each digit is sampled from a 3D normal distribution. Details of this dataset are provided in Appendix F. We will examine the distribution of colors in the samples generated by the trained GAN. As will also be true in the CIFAR10 example in Section 3.4, the lack of diversity in generated colors is almost invisible using only visual inspection of the samples. Samples can be found in Appendix F.\nIn order to recover the color the GAN assigned to the digit, we used k-means with 2 clusters, to pick out the foreground color from the background. We then performed this transformation for both the training data and the generated images. Next we fit a Gaussian kernel density estimator to both distributions over digit colors. Finally, we computed the JS divergence between the model and data distributions over colors. Results can be found in Table 2 for several model sizes. Details of the models are provided in Appendix F.\nIn general, the best performing models are unrolled for 5-10 steps, and larger models perform better than smaller models. Counter-intuitively, taking 1 unrolling step seems to hurt this measure of diversity. We suspect that this is due to it introducing oscillatory dynamics into training. Taking more unrolling steps however leads to improved performance with unrolling."
    }, {
      "heading" : "3.4 IMAGE MODELING OF CIFAR10",
      "text" : "Here we test our technique on a more traditional convolutional GAN architecture and task, similar to those used in (Radford et al., 2015; Salimans et al., 2016). In the previous experiments we tested models where the standard GAN training algorithm would not converge. In this section we improve a standard model by reducing its tendency to engage in mode collapse. We ran 4 configurations of this model, varying the number of unrolling steps to be 0, 1, 5, or 10. Each configuration was run 5 times with different random seeds. For full training details see Appendix D. Samples from each of the 4 configurations can be found in Figure 4. There is no obvious difference in visual quality across these model configurations. Visual inspection however provides only a poor measure of sample diversity.\nBy training with an unrolled discriminator, we expect to generate more diverse samples which more closely resemble the underlying data distribution. We introduce two techniques to examine sample diversity: inference via optimization, and pairwise distance distributions."
    }, {
      "heading" : "3.4.1 INFERENCE VIA OPTIMIZATION",
      "text" : "Since likelihood cannot be tractably computed, over-fitting of GANs is typically tested by taking samples and computing the nearest-neighbor images in pixel space from the training data (Goodfellow et al., 2014). We will do the reverse, and measure the ability of the generative model to generate images that look like specific samples from the training data. If we did this by generating random samples from the model, we would need an exponentially large number of samples. We instead treat finding the nearest neighbor xnearest to a target image xtarget as an optimization task,\nznearest = argmin z ||G (z; θG)− xtarget||22 (13)\nxnearest = G (znearest; θG) . (14)\nThis concept of backpropagating to generate images has been widely used in visualizing features from discriminative networks (Simonyan et al., 2013; Yosinski et al., 2015; Nguyen et al., 2016) and has been applied to explore the visual manifold of GANs in (Zhu et al., 2016).\nWe apply this technique to each of the models trained. We optimize with 3 random starts using LBFGS, which is the optimizer typically used in similar settings such as style transfer (Johnson et al., 2016; Champandard, 2016). Results comparing average mean squared errors between xnearest and xtarget in pixel space can be found in Table 3. In addition we compute the percent of images for which a certain configuration achieves the lowest loss when compared to the other configurations.\nIn the zero step case, there is poor reconstruction and less than 1% of the time does it obtain the lowest error of the 4 configurations. Taking 1 unrolling step results in a significant improvement in MSE. Taking 10 unrolling steps results in more modest improvement, but continues to reduce the reconstruction MSE.\nTo visually see this, we compare the result of the optimization process for 0, 1, 5, and 10 step configurations in Figure 5. To select for images where differences in behavior is most apparent, we sort the data by the absolute value of a fractional difference in MSE between the 0 and 10 step models, ∣∣∣ l0step−l10step1 2 (l0step+l10step)\n∣∣∣. This highlights examples where either the 0 or 10 step model cannot accurately fit the data example but the other can. In Appendix G we show the same comparison for models initialized using different random seeds. Many of the zero step images are fuzzy and illdefined suggesting that these images cannot be generated by the standard GAN generative model, and come from a dropped mode. As more unrolling steps are added, the outlines become more clear and well defined – the model covers more of the distribution and thus can recreate these samples."
    }, {
      "heading" : "3.4.2 PAIRWISE DISTANCES",
      "text" : "A second complementary approach is to compare statistics of data samples to the corresponding statistics for samples generated by the various models. One particularly simple and relevant statistic is the distribution over pairwise distances between random pairs of samples. In the case of mode collapse, greater probability mass will be concentrated in smaller volumes, and the distribution over inter-sample distances should be skewed towards smaller distances. We sample random pairs of images from each model, as well as from the training data, and compute histograms of the `2 distances between those sample pairs. As illustrated in Figure 6, the standard GAN, with zero unrolling steps, has its probability mass skewed towards smaller `2 intersample distances, compared\nto real data. As the number of unrolling steps is increased, the histograms over intersample distances increasingly come to resemble that for the data distribution. This is further evidence in support of unrolling decreasing the mode collapse behavior of GANs."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "In this work we developed a method to stabilize GAN training and reduce mode collapse by defining the generator objective with respect to unrolled optimization of the discriminator. We then demonstrated the application of this method to several tasks, where it either rescued unstable training, or reduced the tendency of the model to drop regions of the data distribution.\nThe main drawback to this method is computational cost of each training step, which increases linearly with the number of unrolling steps. There is a tradeoff between better approximating the true generator loss and the computation required to make this estimate. Depending on the architecture, one unrolling step can be enough. In other more unstable models, such as the RNN case, more are needed to stabilize training. We have some initial positive results suggesting it may be sufficient to further perturb the training gradient in the same direction that a single unrolling step perturbs it. While this is more computationally efficient, further investigation is required.\nThe method presented here bridges some of the gap between theoretical and practical results for training of GANs. We believe developing better update rules for the generator and discriminator is an important line of work for GAN training. In this work we have only considered a small fraction of the design space. For instance, the approach could be extended to unroll G when updating D as well – letting the discriminator react to how the generator would move. It is also possible to unroll sequences of G and D updates. This would make updates that are recursive: G could react to maximize performance as if G and D had already updated."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Laurent Dinh, David Dohan, Vincent Dumoulin, Liam Fedus, Ishaan Gulrajani, Julian Ibarz, Eric Jang, Matthew Johnson, Marc Lanctot, Augustus Odena, Gabriel Pereyra,\nColin Raffel, Sam Schoenholz, Ayush Sekhari, Jon Shlens, and Dale Schuurmans for insightful conversation, as well as the rest of the Google Brain Team."
    }, {
      "heading" : "B MORE MIXTURE OF GAUSSIAN EXPERIMENTS",
      "text" : "B.1 EFFECTS OF TIME DELAY / HISTORICAL AVERAGING\nAnother comparison we looked at was with regard to historical averaging based approaches. Recently similarly inspired approaches have been used in (Salimans et al., 2016) to stabilize training. For our study, we looked at taking an ensemble of discriminators over time.\nFirst, we looked at taking an ensemble of the last N steps, as shown in Figure App.1.\n1\n5\n20\n0\n50\n5000 10000 15000 20000 25000 30000 35000 40000 45000 50000\nUpdate Steps\nN u m\nb e r\nE n se\nm b le\ns\nFigure App.1: Historical averaging does not visibly increase stability on the mixture of Gaussians task. Each row corresponds to an ensemble of discriminators which consists of the indicated number of immediately preceding discriminators. The columns correspond to different numbers of training steps.\nTo further explore this idea, we ran experiments with an ensemble of 5 discriminators, but with different periods between replacing discriminators in the ensemble. For example, if I sample at a rate of 100, it would take 500 steps to replace all 5 discriminators. Results can be seen in Figure App.2.\nWe observe that given longer and longer time delays, the model becomes less and less stable. We hypothesize that this is due to the initial shape of the discriminator loss surface. When training, the discriminator’s estimates of probability densities are only accurate on regions where it was trained. When fixing this discriminator, we are removing the feedback between the generator exploitation\n1\n10\n100\n0\n1000\n5000 10000 15000 20000 25000 30000 35000 40000 45000 50000\nUpdate Steps\nS te\np s\nB e tw\ne e n E\nn se\nm b le\ns\nFigure App.2: Introducing longer time delays between the discriminator ensemble results in instability and probability distributions that are not in the window being visualized. The x axis is the number of weight updates and the y axis is how many steps to skip between discriminator updates when selecting the ensemble of 5 discriminators.\nand the discriminators ability to move. As a result, the generator is able to exploit these fixed areas of poor performance for older discriminators in the ensemble. New discriminators (over)compensate for this, leading the system to diverge.\nB.2 EFFECTS OF THE SECOND GRADIENT\nA second factor we analyzed is the effect of backpropagating the learning signal through the unrolling in Equation 12. We can turn on or off this backpropagation through the unrolling by introducing stop gradient calls into our computation graph between each unrolling step. With the stop gradient in place, the update signal corresponds only to the first term in Equation 12. We looked at 3 configurations: without stop gradients; vanilla unrolled GAN, with stop gradients; and with stop gradients but taking the average over the k unrolling steps instead of taking the final value. Results can be see in Figure App.3.\nWe initially observed no difference between unrolling with and without the second gradient, as both required 3 unrolling steps to become stable. When the discriminator is unrolled to convergence, the second gradient term becomes zero. Due to the simplicity of the problem, we suspect that the discriminator nearly converged for every generator step, and the second gradient term was thus irrelevant.\nTo test this, we modified the dynamics to perform five generator steps for each discriminator update. Results are shown in Figure App.4. With the discriminator now kept out of equilibrium, successful training can be achieved with half as many unrolling steps when using both terms in the gradient than when only including the first term."
    }, {
      "heading" : "C RNN MNIST TRAINING DETAILS",
      "text" : "The network architecture for the experiment in Section 3.2 is as follows:\nThe MNIST dataset is scaled to [-1, 1).\nThe generator first scales the 256D noise vector through a 256 unit fully connected layer with relu activation. This is then fed into the initial state of a 256D LSTM(Hochreiter & Schmidhuber, 1997) that runs 28 steps corresponding to the number of columns in MNIST. The resulting sequence of activations is projected through a fully connected layer with 28 outputs with a tanh activation function. All weights are initialized via the ”Xavier” initialization (Glorot & Bengio, 2010). The forget bias on the LSTM is initialized to 1.\nThe discriminator network feeds the input into a Convolution(16, stride=2) followed by a Convolution(32, stride=2) followed by Convolution(32, stride=2). All convolutions have stride 2. As in (Radford et al., 2015) leaky rectifiers are used with a 0.3 leak. Batch normalization is applied after each layer (Ioffe & Szegedy, 2015). The resulting 4D tensor is then flattened and a linear projection is performed to a single scalar.\nUnrolled GAN\n0\n1\n3\n5\n10\n0\n20\n5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 Update Steps\nU n ro\nlli n g S\nte p s\nUnrolled GAN without second gradient\n0\n1\n3\n5\n10\n0\n20\n5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 Update Steps\nU n ro\nlli n g S\nte p s\nFigure App.3: If the discriminator remains nearly at its optimum during learning, then performance is nearly identical with and without the second gradient term in Equation 12. As shown in Figure App.4, when the discriminator lags behind the generator, backpropagating through unrolling aids convergence.\nThe generator network minimises LG = log(D(G(z))) and the discriminator minimizes LD = log(D(x)) + log(1 −D(G(z))). Both networks are trained with Adam(Kingma & Ba, 2014) with learning rates of 1e-4 and β1=0.5. The network is trained alternating updating the generator and the discriminator for 150k steps. One step consists of just 1 network update."
    }, {
      "heading" : "D CIFAR10/MNIST TRAINING DETAILS",
      "text" : "The network architectures for the discriminator, generator, and encoder as as follows. All convolutions have a kernel size of 3x3 with batch normalization and leaky ReLU’s with a 0.3 leak.\nThe generator network is defined as:\nnumber outputs stride Input: z ∼ N (0, I256) Fully connected 4 * 4 * 512 Reshape to image 4,4,512 Convolution 256 2 Convolution 128 2 Convolution 64 2 Convolution 1 or 3 1\nUnrolled GAN with 5 G Steps per D\n0\n1\n3\n5\n10\n0\n20\n5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 Update Steps\nU n ro\nlli n g S\nte p s\nUnrolled GAN with 5 G Steps per D without second gradient\n0\n1\n3\n5\n10\n0\n20\n5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 Update Steps\nU n ro\nlli n g S\nte p s\nFigure App.4: Backpropagating through the unrolling process aids convergence when the discriminator does not fully converge between generator updates. When taking 5 generator steps per discriminator step unrolling greatly increases stability, requiring only 5 unrolling steps to converge. Without the second gradient it requires 10 unrolling steps. Also see Figure App.3.\nThe discriminator network is defined as:\nnumber outputs stride Input: x ∼ pdata or G Transposed Convolution 64 2 Transposed Convolution 128 2 Transposed Convolution 256 2 Flatten Fully Connected 1\nThe generator network minimises LG = log(D(G(z))) and the discriminator minimizes LD = log(D(x))+ log(1−D(G(z))). The networks are trained with Adam with a generator learning rate of 1e-4, and a discriminator learning rate of 2e-4. The network is trained alternating updating the generator and the discriminator for 100k steps. One step consists of just 1 network update.\nE 1000 CLASS MNIST\nnumber outputs stride Input: z ∼ N (0, I256) Fully connected 4 * 4 * 64 Reshape to image 4,4,64 Convolution 32 2 Convolution 16 2 Convolution 8 2 Convolution 3 1\nThe discriminator network is parametrized by a size X and is defined as follows. In our tests, we used X of 1/4 and 1/2.\nnumber outputs stride Input: x ∼ pdata or G Transposed Convolution 8*X 2 Transposed Convolution 16*X 2 Transposed Convolution 32*X 2 Flatten Fully Connected 1"
    }, {
      "heading" : "F COLORED MNIST DATASET",
      "text" : "F.1 DATASET\nTo generate this dataset we first took the mnist digit, I , scaled between 0 and 1. For each image we sample a color, C, normally distributed with mean=0 and std=0.5. To generate a colored digit between (-1, 1) we do I ∗ C + (I − 1). Finally, we add a small amount of pixel independent noise sampled from a normal distribution with std=0.2, and the resulting values are cliped between (-1, 1). When visualized, this generates images and samples that can be seen in figure App.5. Once again it is very hard to visually see differences in sample diversity when comparing the 128 and the 512 sized models.\nFigure App.5: Right: samples from the data distribution. Middle: Samples from 1/4 size model with 0 look ahead steps (worst diversity). Left: Samples from 1/1 size model with 10 look ahead steps (most diversity).\nF.2 MODELS\nThe models used in this section are parametrized by a variable X to control capacity. A value of X=1 is same architecture used in the cifar10 experiments. We used 1/4, 1/2 and 1 as these values.\nThe generator network is defined as:\nnumber outputs stride Input: z ∼ N (0, I256) Fully connected 4 * 4 * 512*X Reshape to image 4,4,512*X Convolution 256*X 2 Convolution 128*X 2 Convolution 64*X 2 Convolution 3 1\nThe discriminator network is defined as:\nnumber outputs stride Input: x ∼ pdata or G Transposed Convolution 64*X 2 Transposed Convolution 128*X 2 Transposed Convolution 256*X 2 Flatten Fully Connected 1"
    }, {
      "heading" : "G OPTIMIZATION BASED VISUALIZATIONS",
      "text" : "More examples of model based optimization. We performed 5 runs with different seeds of each of of the unrolling steps configuration. Bellow are comparisons for each run index. Ideally this would be a many to many comparison, but for space efficiency we grouped the runs by the index in which they were run.\nData\n0.026\n0 step\n0.0154\n1 step\n0.016\n5 step\n0.0084\n10 step\n0.0326 0.0146 0.0158 0.0107\n0.0323 0.0103 0.0158 0.0108\n0.0327 0.0193 0.0184 0.0113\n0.0212 0.0136 0.0133 0.0078\n0.0158 0.0108 0.0103 0.006\n0.0612 0.027 0.0317 0.0239\n0.0254 0.0083 0.0128 0.0102\n0.0321 0.0182 0.0177 0.0128\n0.0382 0.0139 0.0133 0.0154\n0.0142 0.0059 0.0067 0.0059\n0.0332 0.0119 0.0186 0.0139\n0.0061 0.0049 0.0034 0.0026\nData\n0.0351\n0 step\n0.0341\n1 step\n0.0216\n5 step\n0.0149\n10 step\n0.0183 0.0087 0.0073 0.0078\n0.0449 0.0251 0.0238 0.0194\n0.0185 0.009 0.0094 0.008\n0.0672 0.0366 0.0428 0.0295\n0.0146 0.0112 0.0146 0.0064\n0.0465 0.0302 0.0272 0.0206\n0.05 0.0274 0.0238 0.0223\n0.012 0.007 0.0085 0.0054\n0.0252 0.0167 0.0172 0.0113\n0.0268 0.0157 0.0154 0.0121\n0.0405 0.0189 0.0235 0.0185\n0.048 0.0325 0.0295 0.0222\nFigure App.6: Samples from 1/5 with different random seeds.\nData\n0.0183\n0 step\n0.0151\n1 step\n0.0068\n5 step\n0.0051\n10 step\n0.0216 0.0119 0.0058 0.0066\n0.0555 0.0217 0.0182 0.017\n0.0317 0.0141 0.0144 0.014\n0.0058 0.0035 0.0041 0.0026\n0.0488 0.033 0.0316 0.0223\n0.0165 0.0085 0.0106 0.0082\n0.0221 0.0134 0.0217 0.011\n0.0231 0.013 0.0168 0.0117\n0.0627 0.0393 0.0536 0.032\n0.0151 0.0129 0.0286 0.0079\n0.0168 0.0101 0.0104 0.0087\n0.0276 0.0217 0.0193 0.0146\nData\n0.0338\n0 step\n0.024\n1 step\n0.0232\n5 step\n0.0178\n10 step\n0.0273 0.0168 0.0259 0.0145\n0.0355 0.0232 0.0262 0.0189\n0.0151 0.0127 0.0137 0.008\n0.0213 0.0159 0.0221 0.0114\n0.0368 0.0305 0.0255 0.0199\n0.005 0.0046 0.0039 0.0027\n0.0292 0.0239 0.0211 0.016\n0.0263 0.0201 0.0217 0.0144\n0.0334 0.0213 0.0207 0.0183\n0.0355 0.0339 0.0215 0.0196\n0.0438 0.0211 0.0291 0.0242\n0.0226 0.015 0.0156 0.0125\nFigure App.7: Samples from 2/5 with different random seeds.\nData\n0.03\n0 step\n0.0075\n1 step\n0.0066\n5 step\n0.0053\n10 step\n0.0213 0.0081 0.0096 0.006\n0.0453 0.0157 0.0127 0.0143\n0.0161 0.0114 0.0075 0.0063\n0.0168 0.0125 0.0137 0.0066\n0.0304 0.0186 0.0229 0.013\n0.0421 0.0284 0.0272 0.0185\n0.0234 0.0147 0.0173 0.0103\n0.0242 0.0156 0.0144 0.011\n0.0223 0.0152 0.0174 0.0103\n0.0341 0.0274 0.02 0.016\n0.0079 0.0049 0.0052 0.0037\n0.0207 0.0133 0.011 0.0099\nData\n0.0403\n0 step\n0.0246\n1 step\n0.0352\n5 step\n0.0197\n10 step\n0.0227 0.0164 0.0109 0.0111\n0.0202 0.0148 0.0146 0.01\n0.0269 0.02 0.0221 0.0133\n0.0172 0.0144 0.0144 0.0086\n0.0223 0.0157 0.0196 0.0111\n0.0509 0.0357 0.0351 0.0258\n0.021 0.016 0.015 0.0107\n0.0219 0.0105 0.013 0.0112\n0.0206 0.0177 0.0175 0.0105\n0.024 0.0177 0.0256 0.0123\n0.0286 0.019 0.0188 0.0146\n0.0169 0.015 0.0135 0.0087\nFigure App.8: Samples from 3/5 with different random seeds.\nData\n0.0259\n0 step\n0.016\n1 step\n0.0288\n5 step\n0.0096\n10 step\n0.0374 0.0388 0.0301 0.0161\n0.0429 0.021 0.0228 0.0191\n0.0233 0.0278 0.0222 0.011\n0.0528 0.0303 0.0274 0.0249\n0.0329 0.0202 0.0274 0.0155\n0.0286 0.0204 0.0164 0.0135\n0.022 0.0146 0.0155 0.0105\n0.0366 0.0248 0.0205 0.0178\n0.0336 0.0228 0.0257 0.0164\n0.0191 0.0115 0.011 0.0096\n0.0557 0.0322 0.0304 0.0282\n0.0464 0.0261 0.0269 0.0239\nData\n0.0216\n0 step\n0.0155\n1 step\n0.0218\n5 step\n0.0112\n10 step\n0.0403 0.024 0.0244 0.0212\n0.055 0.0363 0.0361 0.0289\n0.0341 0.0236 0.0333 0.018\n0.0414 0.0215 0.0274 0.0219\n0.0167 0.0104 0.0122 0.0089\n0.0241 0.0105 0.015 0.0129\n0.0063 0.0028 0.0046 0.0034\n0.0402 0.0278 0.0314 0.0217\n0.0157 0.0133 0.0142 0.0084\n0.0301 0.0281 0.0294 0.0163\n0.0507 0.0362 0.0494 0.0277\n0.0375 0.0323 0.0247 0.0206\nFigure App.9: Samples from 4/5 with different random seeds.\nData\n0.0452\n0 step\n0.0064\n1 step\n0.0064\n5 step\n0.0081\n10 step\n0.0333 0.0078 0.0076 0.0065\n0.043 0.0124 0.0235 0.0134\n0.03 0.0064 0.008 0.0104\n0.0128 0.0058 0.0065 0.0058\n0.0392 0.0195 0.0218 0.0177\n0.0402 0.0308 0.0286 0.0184\n0.0168 0.0096 0.0119 0.0077\n0.0402 0.0299 0.0233 0.0188\n0.026 0.0144 0.0165 0.0122\n0.0097 0.0061 0.005 0.0046\n0.0105 0.0051 0.0042 0.005\n0.0331 0.0236 0.0256 0.0158\nData\n0.0557\n0 step\n0.0373\n1 step\n0.0344\n5 step\n0.0271\n10 step\n0.0565 0.031 0.0364 0.0276\n0.0315 0.0115 0.0137 0.0154\n0.0285 0.0123 0.0183 0.014\n0.0552 0.0314 0.0307 0.0271\n0.0327 0.015 0.0172 0.0161\n0.0735 0.0577 0.0386 0.0365\n0.0204 0.0121 0.0111 0.0102\n0.0291 0.0163 0.0195 0.0145\n0.0261 0.0135 0.015 0.013\n0.0286 0.0189 0.02 0.0143\n0.027 0.019 0.019 0.0135\n0.0156 0.0091 0.012 0.0078\nFigure App.10: Samples from 5/5 with different random seeds."
    } ],
    "references" : [ {
      "title" : "Learning to learn by gradient descent by gradient descent",
      "author" : [ "Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas" ],
      "venue" : "arXiv preprint arXiv:1606.04474,",
      "citeRegEx" : "Andrychowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andrychowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Structured prediction energy networks",
      "author" : [ "David Belanger", "Andrew McCallum" ],
      "venue" : "arXiv preprint arXiv:1511.06350,",
      "citeRegEx" : "Belanger and McCallum.,? \\Q2015\\E",
      "shortCiteRegEx" : "Belanger and McCallum.",
      "year" : 2015
    }, {
      "title" : "Bidirectional helmholtz machines",
      "author" : [ "Jorg Bornschein", "Samira Shabanian", "Asja Fischer", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1506.03877,",
      "citeRegEx" : "Bornschein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bornschein et al\\.",
      "year" : 2015
    }, {
      "title" : "Multiagent learning using a variable learning rate",
      "author" : [ "Michael Bowling", "Manuela Veloso" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Bowling and Veloso.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bowling and Veloso.",
      "year" : 2002
    }, {
      "title" : "Importance weighted autoencoders",
      "author" : [ "Yuri Burda", "Roger B. Grosse", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1509.00519,",
      "citeRegEx" : "Burda et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic style transfer and turning two-bit doodles into fine artworks",
      "author" : [ "Alex J. Champandard" ],
      "venue" : "arXiv preprint arXiv:1603.01768,",
      "citeRegEx" : "Champandard.,? \\Q2016\\E",
      "shortCiteRegEx" : "Champandard.",
      "year" : 2016
    }, {
      "title" : "Mode regularized generative adversarial networks",
      "author" : [ "Tong Che", "Yanran Li", "Athul Paul Jacob", "Yoshua Bengio", "Wenjie Li" ],
      "venue" : "arXiv preprint arXiv:",
      "citeRegEx" : "Che et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2016
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1606.03657,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "The theory of max-min and its application to weapons allocation problems, volume",
      "author" : [ "John M Danskin" ],
      "venue" : "Science & Business Media,",
      "citeRegEx" : "Danskin.,? \\Q1967\\E",
      "shortCiteRegEx" : "Danskin.",
      "year" : 1967
    }, {
      "title" : "The helmholtz machine",
      "author" : [ "Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Dayan et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Dayan et al\\.",
      "year" : 1995
    }, {
      "title" : "NICE: non-linear independent components estimation",
      "author" : [ "Laurent Dinh", "David Krueger", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1410.8516,",
      "citeRegEx" : "Dinh et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2014
    }, {
      "title" : "Density estimation using real NVP",
      "author" : [ "Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio" ],
      "venue" : "arXiv preprint arXiv:1605.08803,",
      "citeRegEx" : "Dinh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarially learned inference",
      "author" : [ "Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.00704,",
      "citeRegEx" : "Dumoulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In JMLR W&CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010),",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning, pp. 1462–1471,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Alternating back-propagation for generator network, 2016",
      "author" : [ "Tian Han", "Yang Lu", "Song-Chun Zhu", "Ying Nian Wu" ],
      "venue" : "URL https://arxiv.org/abs/1606.08571",
      "citeRegEx" : "Han et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Perceptual losses for real-time style transfer and super-resolution",
      "author" : [ "Justin Johnson", "Alexandre Alahi", "Fei-Fei Li" ],
      "venue" : "arXiv preprint arXiv:1603.08155,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "First order methods for nonsmooth convex large-scale optimization, i: general purpose methods",
      "author" : [ "Anatoli Juditsky", "Arkadi Nemirovski" ],
      "venue" : "Optimization for Machine Learning,",
      "citeRegEx" : "Juditsky and Nemirovski,? \\Q2011\\E",
      "shortCiteRegEx" : "Juditsky and Nemirovski",
      "year" : 2011
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "URL https: //arxiv.org/abs/1312.6114",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Diederik P. Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep convolutional inverse graphics network",
      "author" : [ "Tejas D. Kulkarni", "Will Whitney", "Pushmeet Kohli", "Joshua B. Tenenbaum" ],
      "venue" : "arXiv preprint arXiv:1503.03167,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Photo-realistic single image super-resolution using a generative adversarial network, 2016",
      "author" : [ "Christian Ledig", "Lucas Theis", "Ferenc Huszar", "Jose Caballero", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi" ],
      "venue" : "URL https://arxiv.org/abs/1609.04802",
      "citeRegEx" : "Ledig et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ledig et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradient-based hyperparameter optimization through reversible learning",
      "author" : [ "Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams" ],
      "venue" : null,
      "citeRegEx" : "Maclaurin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maclaurin et al\\.",
      "year" : 2015
    }, {
      "title" : "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
      "author" : [ "Anh Nguyen", "Alexey Dosovitskiy", "Jason Yosinski", "Thomas Brox", "Jeff Clune" ],
      "venue" : "arXiv preprint arXiv:1605.09304,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "f-gan: Training generative neural samplers using variational divergence minimization",
      "author" : [ "Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka" ],
      "venue" : "arXiv preprint arXiv:1606.00709,",
      "citeRegEx" : "Nowozin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nowozin et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional image synthesis with auxiliary classifier gans",
      "author" : [ "Augustus Odena", "Christopher Olah", "Jonathon Shlens" ],
      "venue" : "arXiv preprint arXiv:1610.09585,",
      "citeRegEx" : "Odena et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Odena et al\\.",
      "year" : 2016
    }, {
      "title" : "Reverse-mode ad in a functional framework: Lambda the ultimate backpropagator",
      "author" : [ "Barak A. Pearlmutter", "Jeffrey Mark Siskind" ],
      "venue" : "ACM Trans. Program. Lang. Syst.,",
      "citeRegEx" : "Pearlmutter and Siskind.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pearlmutter and Siskind.",
      "year" : 2008
    }, {
      "title" : "Improved generator objectives for gans",
      "author" : [ "Ben Poole", "Alexander A Alemi", "Jascha Sohl-Dickstein", "Anelia Angelova" ],
      "venue" : "arXiv preprint arXiv:1612.02780,",
      "citeRegEx" : "Poole et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Poole et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning what and where to draw",
      "author" : [ "Scott Reed", "Zeynep Akata", "Santosh Mohan", "Samuel Tenka", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Reed et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial text-to-image synthesis",
      "author" : [ "Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Reed et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2016
    }, {
      "title" : "Stochastic backpropagation and variational inference in deep latent gaussian models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning. Citeseer,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "arXiv preprint arXiv:1606.03498,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1312.6034,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2013
    }, {
      "title" : "Nash convergence of gradient dynamics in general-sum games",
      "author" : [ "Satinder Singh", "Michael Kearns", "Yishay Mansour" ],
      "venue" : "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Singh et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2000
    }, {
      "title" : "Deep unsupervised learning using nonequilibrium thermodynamics",
      "author" : [ "Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning, pp. 2256–2265,",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Amortised map inference for image super-resolution, 2016",
      "author" : [ "Casper Kaae Sonderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Huszar" ],
      "venue" : "URL https://arxiv.org/abs/1610",
      "citeRegEx" : "Sonderby et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sonderby et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative image modeling using spatial lstms",
      "author" : [ "L. Theis", "M. Bethge" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Theis and Bethge.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis and Bethge.",
      "year" : 2015
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "L. Theis", "A. van den Oord", "M. Bethge" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Theis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2016
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "Aäron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1601.06759,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional image generation with pixelcnn decoders",
      "author" : [ "Aäron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1606.05328,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    }, {
      "title" : "Understanding neural networks through deep visualization",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson" ],
      "venue" : "arXiv preprint arXiv:1506.06579,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-agent learning with policy prediction",
      "author" : [ "Chongjie Zhang", "Victor R Lesser" ],
      "venue" : "In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Zhang and Lesser.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and Lesser.",
      "year" : 2010
    }, {
      "title" : "Energy-based generative adversarial network",
      "author" : [ "Junbo Zhao", "Michael Mathieu", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1609.03126,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative visual manipulation on the natural image manifold",
      "author" : [ "Jun-Yan Zhu", "Philipp Krähenbühl", "Eli Shechtman", "Alexei A. Efros" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Zhu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 46,
      "context" : "This success has been achieved through a surprising diversity of training losses and model architectures, including denoising autoencoders (Vincent et al., 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al.",
      "startOffset" : 139,
      "endOffset" : 161
    }, {
      "referenceID" : 35,
      "context" : ", 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Kulkarni et al., 2015; Burda et al., 2015; Kingma et al., 2016), generative stochastic networks (Alain et al.",
      "startOffset" : 34,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : ", 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Kulkarni et al., 2015; Burda et al., 2015; Kingma et al., 2016), generative stochastic networks (Alain et al.",
      "startOffset" : 34,
      "endOffset" : 165
    }, {
      "referenceID" : 24,
      "context" : ", 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Kulkarni et al., 2015; Burda et al., 2015; Kingma et al., 2016), generative stochastic networks (Alain et al.",
      "startOffset" : 34,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : ", 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Kulkarni et al., 2015; Burda et al., 2015; Kingma et al., 2016), generative stochastic networks (Alain et al.",
      "startOffset" : 34,
      "endOffset" : 165
    }, {
      "referenceID" : 23,
      "context" : ", 2010), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Kulkarni et al., 2015; Burda et al., 2015; Kingma et al., 2016), generative stochastic networks (Alain et al.",
      "startOffset" : 34,
      "endOffset" : 165
    }, {
      "referenceID" : 39,
      "context" : ", 2015), diffusion probabilistic models (Sohl-Dickstein et al., 2015), autoregressive models (Theis & Bethge, 2015; van den Oord et al.",
      "startOffset" : 40,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : ", 2016a;b), real non-volume preserving transformations (Dinh et al., 2014; 2016), Helmholtz machines (Dayan et al.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : ", 2014; 2016), Helmholtz machines (Dayan et al., 1995; Bornschein et al., 2015), and Generative Adversarial Networks (GANs) (Goodfellow et al.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : ", 2014; 2016), Helmholtz machines (Dayan et al., 1995; Bornschein et al., 2015), and Generative Adversarial Networks (GANs) (Goodfellow et al.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : ", 2015), and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 28,
      "context" : "minimizes the Jensen-Shannon divergence between the data distribution and the generator, and extensions generalize this to a wider class of divergences (Nowozin et al., 2016; Sonderby et al., 2016; Poole et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 217
    }, {
      "referenceID" : 40,
      "context" : "minimizes the Jensen-Shannon divergence between the data distribution and the generator, and extensions generalize this to a wider class of divergences (Nowozin et al., 2016; Sonderby et al., 2016; Poole et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 217
    }, {
      "referenceID" : 31,
      "context" : "minimizes the Jensen-Shannon divergence between the data distribution and the generator, and extensions generalize this to a wider class of divergences (Nowozin et al., 2016; Sonderby et al., 2016; Poole et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "The ability to train extremely flexible generating functions, without explicitly computing likelihoods or performing inference, and while targeting more mode-seeking divergences as made GANs extremely successful in image generation (Odena et al., 2016; Salimans et al., 2016; Radford et al., 2015), and image super resolution (Ledig et al.",
      "startOffset" : 232,
      "endOffset" : 297
    }, {
      "referenceID" : 36,
      "context" : "The ability to train extremely flexible generating functions, without explicitly computing likelihoods or performing inference, and while targeting more mode-seeking divergences as made GANs extremely successful in image generation (Odena et al., 2016; Salimans et al., 2016; Radford et al., 2015), and image super resolution (Ledig et al.",
      "startOffset" : 232,
      "endOffset" : 297
    }, {
      "referenceID" : 32,
      "context" : "The ability to train extremely flexible generating functions, without explicitly computing likelihoods or performing inference, and while targeting more mode-seeking divergences as made GANs extremely successful in image generation (Odena et al., 2016; Salimans et al., 2016; Radford et al., 2015), and image super resolution (Ledig et al.",
      "startOffset" : 232,
      "endOffset" : 297
    }, {
      "referenceID" : 25,
      "context" : ", 2015), and image super resolution (Ledig et al., 2016).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "The flexibility of the GAN framework has also enabled a number of successful extensions of the technique, for instance for structured prediction (Reed et al., 2016a;b; Odena et al., 2016), training energy based models (Zhao et al.",
      "startOffset" : 145,
      "endOffset" : 187
    }, {
      "referenceID" : 49,
      "context" : ", 2016), training energy based models (Zhao et al., 2016), and combining the GAN loss with a mutual information loss (Chen et al.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : ", 2016), and combining the GAN loss with a mutual information loss (Chen et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 32,
      "context" : "To train GANs many tricks must be employed, such as careful selection of architectures (Radford et al., 2015), minibatch discrimination (Salimans et al.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 36,
      "context" : ", 2015), minibatch discrimination (Salimans et al., 2016), and noise injection (Salimans et al.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 36,
      "context" : ", 2016), and noise injection (Salimans et al., 2016; Sonderby et al., 2016).",
      "startOffset" : 29,
      "endOffset" : 75
    }, {
      "referenceID" : 40,
      "context" : ", 2016), and noise injection (Salimans et al., 2016; Sonderby et al., 2016).",
      "startOffset" : 29,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "Once converged, the generative models produced by the GAN training procedure normally do not cover the whole distribution (Dumoulin et al., 2016; Che et al., 2016), even when targeting a modecovering divergence such as KL.",
      "startOffset" : 122,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "Once converged, the generative models produced by the GAN training procedure normally do not cover the whole distribution (Dumoulin et al., 2016; Che et al., 2016), even when targeting a modecovering divergence such as KL.",
      "startOffset" : 122,
      "endOffset" : 163
    }, {
      "referenceID" : 42,
      "context" : "Additionally, because it is intractable to compute the GAN training loss, and because approximate measures of performance such as Parzen window estimates suffer from major flaws (Theis et al., 2016), evaluation of GAN performance is challenging.",
      "startOffset" : 178,
      "endOffset" : 198
    }, {
      "referenceID" : 26,
      "context" : "The parameters output by the optimizer can thus be included, in a differentiable way, in another objective (Maclaurin et al., 2015).",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 26,
      "context" : "Differentiating through unrolled optimization was first scaled to deep networks in (Maclaurin et al., 2015), where it was used for hyperparameter optimization.",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "More recently, (Belanger & McCallum, 2015; Han et al., 2016; Andrychowicz et al., 2016) backpropagate through optimization procedures in contexts unrelated to GANs or minimax games.",
      "startOffset" : 15,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "More recently, (Belanger & McCallum, 2015; Han et al., 2016; Andrychowicz et al., 2016) backpropagate through optimization procedures in contexts unrelated to GANs or minimax games.",
      "startOffset" : 15,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "4, the optimal discriminator D∗ (x) is a known smooth function of the generator probability pG (x) (Goodfellow et al., 2014), D∗ (x) = pdata (x) pdata (x) + pG (x) .",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "gradient descent is given as Algorithm 2 in (Maclaurin et al., 2015), though in practice the use of an automatic differentiation package means this step does not need to be programmed explicitly.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "It is important to distinguish this from an approach suggested in (Goodfellow et al., 2014), that several update steps of the discriminator parameters should be run before each single update step for the generator.",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "12 goes to 0 (Danskin, 1967).",
      "startOffset" : 13,
      "endOffset" : 28
    }, {
      "referenceID" : 38,
      "context" : "This is similar to the situation in simple matrix games like rock-paper-scissors and matching pennies, where alternating gradient descent (ascent) with a fixed learning rate is known not to converge (Singh et al., 2000; Bowling & Veloso, 2002).",
      "startOffset" : 199,
      "endOffset" : 243
    }, {
      "referenceID" : 42,
      "context" : "Evaluation of generative models is a notoriously hard problem (Theis et al., 2016).",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 36,
      "context" : "As such the de facto standard in GAN literature has become sample quality as evaluated by a human and/or evaluated by a heuristic (Inception score for example, (Salimans et al., 2016)).",
      "startOffset" : 160,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "1 DISCRETE MODE COLLAPSE To explore the degree to which GANs drop discrete modes in a dataset, we use a technique similar to one from (Che et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "Contrary to (Che et al., 2016), we found that reasonably sized models (such as the one used in Section 3.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "4 IMAGE MODELING OF CIFAR10 Here we test our technique on a more traditional convolutional GAN architecture and task, similar to those used in (Radford et al., 2015; Salimans et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 188
    }, {
      "referenceID" : 36,
      "context" : "4 IMAGE MODELING OF CIFAR10 Here we test our technique on a more traditional convolutional GAN architecture and task, similar to those used in (Radford et al., 2015; Salimans et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "1 INFERENCE VIA OPTIMIZATION Since likelihood cannot be tractably computed, over-fitting of GANs is typically tested by taking samples and computing the nearest-neighbor images in pixel space from the training data (Goodfellow et al., 2014).",
      "startOffset" : 215,
      "endOffset" : 240
    }, {
      "referenceID" : 37,
      "context" : "(14) This concept of backpropagating to generate images has been widely used in visualizing features from discriminative networks (Simonyan et al., 2013; Yosinski et al., 2015; Nguyen et al., 2016) and has been applied to explore the visual manifold of GANs in (Zhu et al.",
      "startOffset" : 130,
      "endOffset" : 197
    }, {
      "referenceID" : 47,
      "context" : "(14) This concept of backpropagating to generate images has been widely used in visualizing features from discriminative networks (Simonyan et al., 2013; Yosinski et al., 2015; Nguyen et al., 2016) and has been applied to explore the visual manifold of GANs in (Zhu et al.",
      "startOffset" : 130,
      "endOffset" : 197
    }, {
      "referenceID" : 27,
      "context" : "(14) This concept of backpropagating to generate images has been widely used in visualizing features from discriminative networks (Simonyan et al., 2013; Yosinski et al., 2015; Nguyen et al., 2016) and has been applied to explore the visual manifold of GANs in (Zhu et al.",
      "startOffset" : 130,
      "endOffset" : 197
    }, {
      "referenceID" : 50,
      "context" : ", 2016) and has been applied to explore the visual manifold of GANs in (Zhu et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "We optimize with 3 random starts using LBFGS, which is the optimizer typically used in similar settings such as style transfer (Johnson et al., 2016; Champandard, 2016).",
      "startOffset" : 127,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "We optimize with 3 random starts using LBFGS, which is the optimizer typically used in similar settings such as style transfer (Johnson et al., 2016; Champandard, 2016).",
      "startOffset" : 127,
      "endOffset" : 168
    }, {
      "referenceID" : 36,
      "context" : "Recently similarly inspired approaches have been used in (Salimans et al., 2016) to stabilize training.",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : "As in (Radford et al., 2015) leaky rectifiers are used with a 0.",
      "startOffset" : 6,
      "endOffset" : 28
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator’s objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",
    "creator" : "LaTeX with hyperref package"
  }
}