{
  "name" : "392.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "COMPRESSIVE AUTOENCODERS", "Lucas Theis", "Wenzhe Shi", "Andrew Cunningham", "Ferenc Huszár" ],
    "emails" : [ "ltheis@twitter.com", "wshi@twitter.com", "acunningham@twitter.com", "fhuszar@twitter.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Advances in training of neural networks have helped to improve performance in a number of domains, but neural networks have yet to surpass existing codecs in lossy image compression. Promising first results have recently been achieved using autoencoders (Ballé et al., 2016; Toderici et al., 2016b) – in particular on small images (Toderici et al., 2016a; Gregor et al., 2016; van den Oord et al., 2016b) – and neural networks are already achieving state-of-the-art results in lossless image compression (Theis & Bethge, 2015; van den Oord et al., 2016a).\nAutoencoders have the potential to address an increasing need for flexible lossy compression algorithms. Depending on the situation, encoders and decoders of different computational complexity are required. When sending data from a server to a mobile device, it may be desirable to pair a powerful encoder with a less complex decoder, but the requirements are reversed when sending data in the other direction. The amount of computational power and bandwidth available also changes over time as new technologies become available. For the purpose of archiving, encoding and decoding times matter less than for streaming applications. Finally, existing compression algorithms may be far from optimal for new media formats such as lightfield images, 360 video or VR content. While the development of a new codec can take years, a more general compression framework based on neural networks may be able to adapt much quicker to these changing tasks and environments.\nUnfortunately, lossy compression is an inherently non-differentiable problem. In particular, quantization is an integral part of the compression pipeline but is not differentiable. This makes it difficult to train neural networks for this task. Existing transformations have typically been manually chosen (e.g., the DCT transformation used in JPEG) or have been optimized for a task different from lossy compression (e.g. Testa & Rossi, 2016, used denoising autoencoders for compression). In contrast to most previous work, but in line with Ballé et al. (2016), we here aim at directly optimizing the rate-distortion tradeoff produced by an autoencoder. We propose a simple but effective approach for dealing with the non-differentiability of rounding-based quantization, and for approximating the non-differentiable cost of coding the generated coefficients.\nUsing this approach, we achieve performance similar to or better than JPEG 2000 when evaluated for perceptual quality. Unlike JPEG 2000, however, our framework can be optimized for specific content (e.g., thumbnails or non-natural images), arbitrary metrics, and is readily generalizable to other\nforms of media. Notably, we achieve this performance using efficient neural network architectures which would allow near real-time decoding of large images even on low-powered consumer devices."
    }, {
      "heading" : "2 COMPRESSIVE AUTOENCODERS",
      "text" : "We define a compressive autoencoder (CAE) to have three components: an encoder f , a decoder g, and a probabilistic model Q,\nf : RN → RM , g : RM → RN , Q : ZM → [0, 1]. (1)\nThe discrete probability distribution defined by Q is used to assign a number of bits to representations based on their frequencies, that is, for entropy coding. All three components may have parameters and our goal is to optimize the tradeoff between using a small number of bits and having small distortion,\n− log2Q ([f(x)])︸ ︷︷ ︸ Number of bits +β · d (x, g([f(x)]))︸ ︷︷ ︸ Distortion . (2)\nHere, β controls the tradeoff, square brackets indicate quantization through rounding to the nearest integer, and d measures the distortion introduced by coding and decoding. The quantized output of the encoder is the code used to represent an image and is stored losslessly. The main source of information loss is the quantization (Appendix A.3). Additional information may be discarded by the encoder, and the decoder may not perfectly decode the available information, increasing distortion.\nUnfortunately we cannot optimize Equation 2 directly using gradient-based techniques, as Q and [·] are non-differentiable. The following two sections propose a solution to deal with this problem."
    }, {
      "heading" : "2.1 QUANTIZATION AND DIFFERENTIABLE ALTERNATIVES",
      "text" : "The derivative of the rounding function is zero everywhere except at integers, where it is undefined. We propose to replace its derivative in the backward pass of backpropagation (Rumelhart et al., 1986) with the derivative of a smooth approximation, r, that is, effectively defining the derivative to be\nd\ndy [y] :=\nd\ndy r(y). (3)\nImportantly, we do not fully replace the rounding function with a smooth approximation but only its derivative, which means that quantization is still performed as usual in the forward pass. If we replaced rounding with a smooth approximation completely, the decoder might learn to invert the\nsmooth approximation, thereby removing the information bottle neck that forces the network to compress information.\nEmpirically, we found the identity, r(y) = y, to work as well as more sophisticated choices. This makes this operation easy to implement, as we simply have to pass gradients without modification from the decoder to the encoder.\nNote that the gradient with respect to the decoder’s parameters can be computed without resorting to approximations, assuming d is differentiable. In contrast to related approaches, our approach has the advantage that it does not change the gradients of the decoder, since the forward pass is kept the same.\nIn the following, we discuss alternative approaches proposed by other authors. Motivated by theoretical links to dithering, Ballé et al. (2016) proposed to replace quantization by additive uniform noise,\n[f(x)] ≈ f(x) + u. (4)\nToderici et al. (2016a), on the other hand, used a stochastic form of binarization (Williams, 1992). Generalizing this idea to integers, we define the following stochastic rounding operation:\n{y} ≈ byc+ ε, ε ∈ {0, 1}, P (ε = 1) = y − byc, (5)\nwhere b·c is the floor operator. In the backward pass, the derivative is replaced with the derivative of the expectation,\nd dy {y} := d dy E [{y}] = d dy y = 1. (6)\nFigure 1 shows the effect of using these two alternatives as part of JPEG, whose encoder and decoder are based on a block-wise DCT transformation (Pennebaker & Mitchell, 1993). Note that the output is visibly different from the output produced with regular quantization by rounding and that the error signal sent to the autoencoder depends on these images. Whereas in Fig. 1B the error signal received by the decoder would be to remove blocking artefacts, the signal in Fig. 1D will be to remove high-frequency noise. We expect this difference to be less of a problem with simple metrics such as mean-squared error and to have a bigger impact when using more perceptually meaningful measures of distortion.\nAn alternative would be to use the latter approximations only for the gradient of the encoder but not for the gradients of the decoder. While this is possible, it comes at the cost of increased computational and implementational complexity, since we would have to perform the forward and backward pass through the decoder twice: once using rounding, once using the approximation. With our approach the gradient of the decoder is correct even for a single forward and backward pass."
    }, {
      "heading" : "2.2 ENTROPY RATE ESTIMATION",
      "text" : "SinceQ is a discrete function, we cannot differentiate it with respect to its argument, which prevents us from computing a gradient for the encoder. To solve this problem, we use a continuous, differentiable approximation. We upper-bound the non-differentiable number of bits by first expressing the model’s distribution Q in terms of a probability density q,\nQ(z) = ∫ [−.5,.5[M q(z+ u) du. (7)\nAn upper bound is given by: − log2Q (z) = − log2 ∫ [−.5,.5[M q(z+ u) du ≤ ∫ [−.5,.5[M − log2 q(z+ u) du, (8)\nwhere the second step follows from Jensen’s inequality (see also Theis et al., 2016). An unbiased estimate of the upper bound is obtained by sampling u from the unit cube [−.5, .5[M . If we use a differentiable density, this estimate will be differentiable in z and therefore can be used to train the encoder."
    }, {
      "heading" : "2.3 VARIABLE BIT RATES",
      "text" : "In practice we often want fine-gained control over the number of bits used. One way to achieve this is to train an autoencoder for different rate-distortion tradeoffs. But this would require us to train and store a potentially large number of models. To reduce these costs, we finetune a pre-trained autoencoder for different rates by introducing scale parameters1 λ ∈ RM ,\n− log2 q ([f(x) ◦ λ] + u) + β · d (x, g([f(x) ◦ λ] /λ)) . (9)\nHere, ◦ indicates point-wise multiplication and division is also performed point-wise. To reduce the number of trainable scales, they may furthermore be shared across dimensions. Where f and g are convolutional, for example, we share scale parameters across spatial dimensions but not across channels.\nAn example of learned scale parameters is shown in Figure 3A. For more fine-grained control over bit rates, the optimized scales can be interpolated."
    }, {
      "heading" : "2.4 RELATED WORK",
      "text" : "Perhaps most closely related to our work is the work of Ballé et al. (2016). The main differences lie in the way we deal with quantization (see Section 2.1) and entropy rate estimation. The transformations used by Ballé et al. (2016) consist of a single linear layer combined with a form of contrast gain control, while our framework relies on more standard deep convolutional neural networks.\nToderici et al. (2016a) proposed to use recurrent neural networks (RNNs) for compression. Instead of entropy coding as in our work, the network tries to minimize the distortion for a given number of bits. The image is encoded in an iterative manner, and decoding is performed in each step to be able to take into account residuals at the next iteration. An advantage of this design is that it allows for progressive coding of images. A disadvantage is that compression is much more time consuming than in our approach, as we use efficient convolutional neural networks and do not necessarily require decoding at the encoding stage.\nGregor et al. (2016) explored using variational autoencoders with recurrent encoders and decoders for compression of small images. This type of autoencoder is trained to maximize the lower bound of a log-likelihood, or equivalently to minimize\n−Ep(y|x) [ log\nq(y)q(x | y) p(y | x)\n] , (10)\nwhere p(y | x) plays the role of the encoder, and q(x | y) plays the role of the decoder. While Gregor et al. (2016) used a Gaussian distribution for the encoder, we can link their approach to the work of Ballé et al. (2016) by assuming it to be uniform, p(y | x) = f(x) + u. If we also assume a Gaussian likelihood with fixed variance, q(x | y) = N (x | g(y), σ2I), the objective function can be written\nEu [ − log q(f(x) + u) + 1\n2σ2 ||x− g(f(x) + u)||2\n] + C. (11)\nHere, C is a constant which encompasses the negative entropy of the encoder and the normalization constant of the Gaussian likelihood. Note that this equation is identical to a rate-distortion trade-off with β = σ−2/2 and quantization replaced by additive uniform noise. However, not all distortions have an equivalent formulation as a variational autoencoder (Kingma & Welling, 2014). This only works if e−d(x,y) is normalizable in x and the normalization constant does not depend on y, or otherwise C will not be constant. An direct empirical comparison of our approach with variational autoencoders is provided in Appendix A.5.\nOllivier (2015) discusses variational autoencoders for lossless compression as well as connections to denoising autoencoders."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 ENCODER, DECODER, AND ENTROPY MODEL",
      "text" : "We use common convolutional neural networks (LeCun et al., 1998) for the encoder and the decoder of the compressive autoencoder. Our architecture was inspired by the work of Shi et al. (2016), who demonstrated that super-resolution can be achieved much more efficiently by operating in the lowresolution space, that is, by convolving images and then upsampling instead of upsampling first and then convolving an image.\nThe first two layers of the encoder perform preprocessing, namely mirror padding and a fixed pixelwise normalization. The mirror-padding was chosen such that the output of the encoder has the same spatial extent as an 8 times downsampled image. The normalization centers the distribution of each channel’s values and ensures it has approximately unit variance. Afterwards, the image is convolved and spatially downsampled while at the same time increasing the number of channels to 128. This is followed by three residual blocks (He et al., 2015), where each block consists of an additional two convolutional layers with 128 filters each. A final convolutional layer is applied and the coefficients downsampled again before quantization through rounding to the nearest integer.\nThe decoder mirrors the architecture of the encoder (Figure 9). Instead of mirror-padding and valid convolutions, we use zero-padded convolutions. Upsampling is achieved through convolution followed by a reorganization of the coefficients. This reorganization turns a tensor with many channels into a tensor of the same dimensionality but with fewer channels and larger spatial extent (for details, see Shi et al., 2016). A convolution and reorganization of coefficients together form a sub-pixel convolution layer. Following three residual blocks, two sub-pixel convolution layers upsample the image to the resolution of the input. Finally, after denormalization, the pixel values are clipped to\n1To ensure positivity, we use a different parametrization and optimize log-scales rather than scales.\nA B\nthe range of 0 to 255. Similar to how we deal with gradients of the rounding function, we redefine the gradient of the clipping function to be 1 outside the clipped range. This ensures that the training signal is non-zero even when the decoded pixels are outside this range (Appendix A.1).\nTo model the distribution of coefficients and estimate the bit rate, we use independent Gaussian scale mixtures (GSMs),\nlog2 q(z+ u) = ∑ i,j,k log2 ∑ s πksN (zkij + ukij ; 0, σ2ks), (12)\nwhere i and j iterate over spatial positions, and k iterates over channels of the coefficients for a single image z. GSMs are well established as useful building blocks for modelling filter responses of natural images (e.g., Portilla et al., 2003). We used 6 scales in each GSM. Rather than using the more common parametrization above, we parametrized the GSM so that it can be easily used with gradient based methods, optimizing log-weights and log-precisions rather than weights and variances. We note that the leptokurtic nature of GSMs (Andrews & Mallows, 1974) means that the rate term encourages sparsity of coefficients.\nAll networks were implemented in Python using Theano (2016) and Lasagne (Dieleman et al., 2015). For entropy encoding of the quantized coefficients, we first created Laplace-smoothed histogram estimates of the coefficient distributions across a training set. The estimated probabilities were then used with a publicly available BSD licensed implementation of a range coder2."
    }, {
      "heading" : "3.2 INCREMENTAL TRAINING",
      "text" : "All models were trained using Adam (Kingma & Ba, 2015) applied to batches of 32 images 128×128 pixels in size. We found it beneficial to optimize coefficients in an incremental manner (Figure 3B). This is done by introducing an additional binary mask m,\n− log2 q ([f(x)] ◦m+ u) + β · d (x, g([f(x)] ◦m)) . (13) Initially, all but 2 entries of the mask are set to zero. Networks are trained until performance improvements reach below a threshold, and then another coefficient is enabled by setting an entry of the binary mask to 1. After all coefficients have been enabled, the learning rate is reduced from an initial value of 10−4 to 10−5. Training was performed for up to 106 updates but usually reached good performance much earlier.\nAfter a model has been trained for a fixed rate-distortion trade-off (β), we introduce and fine-tune scale parameters (Equation 9) for other values of β while keeping all other parameters fixed. Here\n2https://github.com/kazuho/rangecoder/\nwe used an initial learning rate of 10−3 and continuously decreased it by a factor of τκ/(τ + t)κ, where t is the current number of updates performed, κ = .8, and τ = 1000. Scales were optimized for 10,000 iterations. For even more fine-grained control over the bit rates, we interpolated between scales optimized for nearby rate-distortion tradeoffs."
    }, {
      "heading" : "3.3 NATURAL IMAGES",
      "text" : "We trained compressive autoencoders on 434 high quality images licensed under creative commons and obtained from flickr.com. The images were downsampled to below 1536 × 1536 pixels and stored as lossless PNGs to avoid compression artefacts. From these images, we extracted 128 × 128 crops to train the network. Mean squared error was used as a measure of distortion during training. Hyperparameters affecting network architecture and training were evaluated on a small set of held-out Flickr images. For testing, we use the commonly used Kodak PhotoCD dataset of 24 uncompressed 768× 512 pixel images3. We compared our method to JPEG (Wallace, 1991), JPEG 2000 (Skodras et al., 2001), and the RNNbased method of (Toderici et al., 2016b)4. Bits for header information were not counted towards the bit rate of JPEG and JPEG 2000. Among the different variants of JPEG, we found that optimized JPEG with 4:2:0 chroma sub-sampling generally worked best (Appendix A.2).\nWhile fine-tuning a single compressive autoencoder for a wide range of bit rates worked well, optimizing all parameters of a network for a particular rate distortion trade-off still worked better. We here chose the compromise of combining autoencoders trained for low, medium or high bit rates (see Appendix A.4 for details).\nFor each image and bit rate, we choose the autoencoder producing the smallest distortion. This increases the time needed to compress an image, since an image has to be encoded and decoded multiple times. However, decoding an image is still as fast, since it only requires choosing and running one decoder network. A more efficient but potentially less performant solution would be to always choose the same autoencoder for a given rate-distortion tradeoff. We added 1 byte to the coding cost to encode which autoencoder of an ensemble is used.\nRate-distortion curves averaged over all test images are shown in Figure 4. We evaluated the different methods in terms of PSNR, SSIM (Wang et al., 2004a), and multiscale SSIM (MS-SSIM; Wang et al., 2004b). We used the implementation of van der Walt et al. (2014) for SSIM and the implementation of Toderici et al. (2016b) for MS-SSIM. We find that in terms of PSNR, our method performs\n3http://r0k.us/graphics/kodak/ 4We used the code which was made available on https://github.com/tensorflow/models/ tree/2390974a/compression. We note that at the time of this writing, this implementation does not include entropy coding as in the paper of Toderici et al. (2016b).\nsimilar to JPEG 2000 although slightly worse at low and medium bit rates and slightly better at high bit rates. In terms of SSIM, our method outperforms all other tested methods. MS-SSIM produces very similar scores for all methods, except at very low bit rates. However, we also find these results to be highly image dependent. Results for individual images are provided as supplementary material5.\nIn Figure 5 we show crops of images compressed to low bit rates. In line with quantitative results, we find that JPEG 2000 reconstructions appear visually more similar to CAE reconstructions than those of other methods. However, artefacts produced by JPEG 2000 seem more noisy than CAE’s, which are smoother and sometimes appear Gábor-filter-like.\nTo quantify the subjective quality of compressed images, we ran a mean opinion score (MOS) test. While MOS tests have their limitations, they are a widely used standard for evaluating perceptual quality (Streijl et al., 2014). Our MOS test set included the 24 full-resolution uncompressed originals from the Kodak dataset, as well as the same images compressed using each of four algorithms at or near three different bit rates: 0.25, 0.372 and 0.5 bits per pixel. Only the low-bit-rate CAE was included in this test.\nFor each image, we chose the CAE setting which produced the highest bit rate but did not exceed the target bit rate. The average bit rates of CAE compressed images were 0.24479, 0.36446, and 0.48596, respectively. We then chose the smallest quality factor for JPEG and JPEG 2000 for which the bit rate exceeded that of the CAE. The average bit rates for JPEG were 0.25221, 0.37339 and 0.49534, for JPEG 2000 0.24631, 0.36748 and 0.49373. For some images the bit rate of the CAE at the lowest setting was still higher than the target bit rate. These images were excluded from the final results, leaving 15, 21, and 23 images, respectively.\nThe perceptual quality of the resulting 273 images was rated by n = 24 non-expert evaluators. One evaluator did not finish the experiment, so her data was discarded. The images were presented to each individual in a random order. The evaluators gave a discrete opinion score for each image from a scale between 1 (bad) to 5 (excellent). Before the rating began, subjects were presented an uncompressed calibration image of the same dimensions as the test images (but not from the Kodak dataset). They were then shown four versions of the calibration image using the worst quality setting of all four compression methods, and given the instruction “These are examples of compressed images. These are some of the worst quality examples.”\nFigure 6 shows average MOS results for each algorithm at each bit rate. 95% confidence intervals were computed via bootstrapping. We found that CAE and JPEG 2000 achieved higher MOS than JPEG or the method of Toderici et al. (2016b) at all bit rates we tested. We also found that CAE significantly outperformed JPEG 2000 at 0.375 bpp (p < 0.05) and 0.5 bpp (p < 0.001).\n5https://figshare.com/articles/supplementary_zip/4210152"
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "We have introduced a simple but effective way of dealing with non-differentiability in training autoencoders for lossy compression. Together with an incremental training strategy, this enabled us to achieve better performance than JPEG 2000 in terms of SSIM and MOS scores. Notably, this performance was achieved using an efficient convolutional architecture, combined with simple roundingbased quantization and a simple entropy coding scheme. Existing codecs often benefit from hardware support, allowing them to run at low energy costs. However, hardware chips optimized for convolutional neural networks are likely to be widely available soon, given that these networks are now key to good performance in so many applications.\nWhile other trained algorithms have been shown to provide similar results as JPEG 2000 (e.g. van den Oord & Schrauwen, 2014), to our knowledge this is the first time that an end-to-end trained architecture has been demonstrated to achieve this level of performance on high-resolution images. An end-to-end trained autoencoder has the advantage that it can be optimized for arbitrary metrics. Unfortunately, research on perceptually relevant metrics suitable for optimization is still in its infancy (e.g., Dosovitskiy & Brox, 2016; Ballé et al., 2016). While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs.\nIn future work, we would like to explore the optimization of compressive autoencoders for different metrics. A promising direction was presented by Bruna et al. (2016), who achieved interesting superresolution results using metrics based on neural networks trained for image classification. Gatys et al. (2016) used similar representations to achieve a breakthrough in perceptually meaningful style transfer. An alternative to perceptual metrics may be to use generative adversarial networks (GANs; Goodfellow et al., 2014). Building on the work of Bruna et al. (2016) and Dosovitskiy & Brox (2016), Ledig et al. (2016) recently demonstrated impressive super-resolution results by combining GANs with feature-based metrics."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Zehan Wang, Aly Tejani, Clément Farabet, and Luke Alonso for helpful feedback on the manuscript."
    }, {
      "heading" : "A APPENDIX",
      "text" : "A.1 GRADIENT OF CLIPPING\nWe redefine the gradient of the clipping operation to be constant, d\ndx̂ clip0,255(x̂) := 1. (14)\nConsider how this affects the gradients of a squared loss, d\ndx̂ (clip0,255(x̂)− x)2 = 2(clip0,255(x̂)− x)\nd\ndx̂ clip0,255(x̂). (15)\nAssume that x̂ is larger than 255. Without redefinition of the derivative, the error signal will be 0 and not helpful. Without any clipping, the error signal will depend on the value of x̂, even though any value above 255 will have the same effect on the loss at test time. On the other hand, using clipping but a different signal in the backward pass is intuitive, as it yields an error signal which is proportional to the error that would also be incurred at test time.\nA.2 DIFFERENT MODES OF JPEG\nWe compared optimized and non-optimized JPEG with (4:2:0) and without (4:4:4) chroma subsampling. Optimized JPEG computes a Huffman table specific to a given image, while unoptimized JPEG uses a predefined Huffman table. We did not count bits allocated to the header of the file format, but for optimized JPEG we counted the bits required to store the Huffman table. We found that on average, chroma-subsampled and optimized JPEG performed better on the Kodak dataset (Figure 7).\nA.3 COMPRESSION VS DIMENSIONALITY REDUCTION\nSince a single real number can carry as much information as a high-dimensional entity, dimensionality reduction alone does not amount to compression. However, if we constrain the architecture of the encoder, it may be forced to discard certain information. To better understand how much information is lost due to dimensionality reduction and how much information is lost due to quantization, Figure 8 shows reconstructions produced by a compressive autoencoder with and without quantization. The effect of dimensionality reduction is minimal compared to the effect of quantization.\nA.4 ENSEMBLE\nTo bring the parameter controlling the rate-distortion trade-off into a more intuitive range, we rescaled the distortion term and expressed the objective as follows:\n− α N ln q ([f(x) ◦ λ] + u) + 1− α 1000 ·M · ||x− g([f(x) ◦ λ] /λ)||2. (16)\nHere, N is the number of coefficients produced by the encoder and M is the dimensionality of x (i.e., 3 times the number of pixels).\nThe high-bit-rate CAE was trained with α = 0.01 and 96 output channels, the medium-bit-rate CAE was trained with α = 0.05 and 96 output channels, and the low-bit-rate CAE was trained with α = 0.2 and 64 output channels.\nA.5 COMPARISON WITH VAE\nA.6 COMPLETE IMAGES\nBelow we show complete images corresponding to the crops in Figure 5. For each image, we show the original image (top left), reconstructions using CAE (top right), reconstructions using JPEG 2000 (bottom left) and reconstructions using the method of (Toderici et al., 2016b) (bottom right). The images are best viewed on a monitor screen."
    } ],
    "references" : [ {
      "title" : "Scale mixtures of normal distributions",
      "author" : [ "D.F. Andrews", "C.L. Mallows" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Andrews and Mallows.,? \\Q1974\\E",
      "shortCiteRegEx" : "Andrews and Mallows.",
      "year" : 1974
    }, {
      "title" : "End-to-end optimization of nonlinear transform codes for perceptual quality",
      "author" : [ "J. Ballé", "V. Laparra", "E.P. Simoncelli" ],
      "venue" : "In Picture Coding Symposium,",
      "citeRegEx" : "Ballé et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ballé et al\\.",
      "year" : 2016
    }, {
      "title" : "Super-resolution with deep convolutional sufficient statistics",
      "author" : [ "J. Bruna", "P. Sprechmann", "Y. LeCun" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Bruna et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bruna et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating images with perceptual similarity metrics based on deep networks, 2016",
      "author" : [ "A. Dosovitskiy", "T. Brox" ],
      "venue" : null,
      "citeRegEx" : "Dosovitskiy and Brox.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy and Brox.",
      "year" : 2016
    }, {
      "title" : "Image style transfer using convolutional neural networks",
      "author" : [ "L.A. Gatys", "A.S. Ecker", "M. Bethge" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Gatys et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards conceptual compression, 2016",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra" ],
      "venue" : null,
      "citeRegEx" : "Gregor et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D. Kingma", "M. Welling" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Perceptual image quality assessment using a normalized Laplacian pyramid",
      "author" : [ "V. Laparra", "J. Ballé", "E.P. Simoncelli" ],
      "venue" : "In SPIE, Conf. on Human Vision and Electronic Imaging,",
      "citeRegEx" : "Laparra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Laparra et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : null,
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, 2016",
      "author" : [ "C. Ledig", "L. Theis", "F. Huszar", "J. Caballero", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang", "W. Shi" ],
      "venue" : null,
      "citeRegEx" : "Ledig et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ledig et al\\.",
      "year" : 2016
    }, {
      "title" : "Auto-encoders: reconstruction versus compression",
      "author" : [ "Y. Ollivier" ],
      "venue" : null,
      "citeRegEx" : "Ollivier.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ollivier.",
      "year" : 2015
    }, {
      "title" : "JPEG still image data compression",
      "author" : [ "W.B. Pennebaker", "J.L. Mitchell" ],
      "venue" : "standard. Springer,",
      "citeRegEx" : "Pennebaker and Mitchell.,? \\Q1993\\E",
      "shortCiteRegEx" : "Pennebaker and Mitchell.",
      "year" : 1993
    }, {
      "title" : "Image denoising using scale mixtures of gaussians in the wavelet domain",
      "author" : [ "J. Portilla", "V. Strela", "M.J. Wainwright", "E.P. Simoncelli" ],
      "venue" : "IEE Trans. Image Process.,",
      "citeRegEx" : "Portilla et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Portilla et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning representations by back-propagating",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network",
      "author" : [ "W. Shi", "J. Caballero", "F. Huszar", "J. Totz", "A. Aitken", "R. Bishop", "D. Rueckert", "Z. Wang" ],
      "venue" : "In IEEE Conf. on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Shi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "The JPEG 2000 still image compression standard",
      "author" : [ "A. Skodras", "C. Christopoulos", "T. Ebrahimi" ],
      "venue" : "Signal Processing Magazine,",
      "citeRegEx" : "Skodras et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Skodras et al\\.",
      "year" : 2001
    }, {
      "title" : "Mean opinion score (MOS) revisited: methods and applications, limitations and alternatives",
      "author" : [ "R.C. Streijl", "S. Winkler", "D.S. Hands" ],
      "venue" : "Multimedia Systems, pp",
      "citeRegEx" : "Streijl et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Streijl et al\\.",
      "year" : 2014
    }, {
      "title" : "Lightweight Lossy Compression of Biometric Patterns via Denoising Autoencoders",
      "author" : [ "D. Del Testa", "M. Rossi" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "Testa and Rossi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Testa and Rossi.",
      "year" : 2016
    }, {
      "title" : "Generative Image Modeling Using Spatial LSTMs",
      "author" : [ "L. Theis", "M. Bethge" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Theis and Bethge.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis and Bethge.",
      "year" : 2015
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "L. Theis", "A. van den Oord", "M. Bethge" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Theis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2016
    }, {
      "title" : "Variable rate image compression with recurrent neural networks",
      "author" : [ "G. Toderici", "S.M. O’Malley", "S.J. Hwang", "D. Vincent", "D. Minnen", "S. Baluja", "M. Covell", "R. Sukthankar" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Toderici et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Toderici et al\\.",
      "year" : 2016
    }, {
      "title" : "Full resolution image compression with recurrent neural networks, 2016b. arXiv:1608.05148v1",
      "author" : [ "G. Toderici", "D. Vincent", "N. Johnston", "S.J. Hwang", "D. Minnen", "J. Shor", "M. Covell" ],
      "venue" : null,
      "citeRegEx" : "Toderici et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Toderici et al\\.",
      "year" : 2016
    }, {
      "title" : "The student-t mixture as a natural image patch prior with application to image compression",
      "author" : [ "A. van den Oord", "B. Schrauwen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Oord and Schrauwen.,? \\Q2014\\E",
      "shortCiteRegEx" : "Oord and Schrauwen.",
      "year" : 2014
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional Image Generation with PixelCNN Decoders, 2016b. arXiv:1606.05328v2",
      "author" : [ "A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "scikit-image: image processing",
      "author" : [ "S. van der Walt", "J.L. Schönberger", "J. Nunez-Iglesias", "F. Boulogne", "J.D. Warner", "N. Yager", "E. Gouillart", "T. Yu" ],
      "venue" : "in Python. PeerJ,",
      "citeRegEx" : "Walt et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Walt et al\\.",
      "year" : 2014
    }, {
      "title" : "The JPEG still picture compression standard",
      "author" : [ "G.K. Wallace" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Wallace.,? \\Q1991\\E",
      "shortCiteRegEx" : "Wallace.",
      "year" : 1991
    }, {
      "title" : "Image quality assessment: from error visibility to structural similarity",
      "author" : [ "Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "Wang et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2004
    }, {
      "title" : "Multiscale structural similarity for image quality assessment",
      "author" : [ "Z. Wang", "E.P. Simoncelli", "A.C. Bovik" ],
      "venue" : "In Conference Record of the Thirty-Seventh Asilomar Conference on Signals, Systems and Computers,",
      "citeRegEx" : "Wang et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2004
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Promising first results have recently been achieved using autoencoders (Ballé et al., 2016; Toderici et al., 2016b) – in particular on small images (Toderici et al.",
      "startOffset" : 71,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : ", 2016b) – in particular on small images (Toderici et al., 2016a; Gregor et al., 2016; van den Oord et al., 2016b) – and neural networks are already achieving state-of-the-art results in lossless image compression (Theis & Bethge, 2015; van den Oord et al.",
      "startOffset" : 41,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "Promising first results have recently been achieved using autoencoders (Ballé et al., 2016; Toderici et al., 2016b) – in particular on small images (Toderici et al., 2016a; Gregor et al., 2016; van den Oord et al., 2016b) – and neural networks are already achieving state-of-the-art results in lossless image compression (Theis & Bethge, 2015; van den Oord et al., 2016a). Autoencoders have the potential to address an increasing need for flexible lossy compression algorithms. Depending on the situation, encoders and decoders of different computational complexity are required. When sending data from a server to a mobile device, it may be desirable to pair a powerful encoder with a less complex decoder, but the requirements are reversed when sending data in the other direction. The amount of computational power and bandwidth available also changes over time as new technologies become available. For the purpose of archiving, encoding and decoding times matter less than for streaming applications. Finally, existing compression algorithms may be far from optimal for new media formats such as lightfield images, 360 video or VR content. While the development of a new codec can take years, a more general compression framework based on neural networks may be able to adapt much quicker to these changing tasks and environments. Unfortunately, lossy compression is an inherently non-differentiable problem. In particular, quantization is an integral part of the compression pipeline but is not differentiable. This makes it difficult to train neural networks for this task. Existing transformations have typically been manually chosen (e.g., the DCT transformation used in JPEG) or have been optimized for a task different from lossy compression (e.g. Testa & Rossi, 2016, used denoising autoencoders for compression). In contrast to most previous work, but in line with Ballé et al. (2016), we here aim at directly optimizing the rate-distortion tradeoff produced by an autoencoder.",
      "startOffset" : 72,
      "endOffset" : 1898
    }, {
      "referenceID" : 1,
      "context" : "D: Uniform additive noise (Ballé et al., 2016).",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "C: Stochastic rounding to the nearest integer similar to the binarization of Toderici et al. (2016a). D: Uniform additive noise (Ballé et al.",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "We propose to replace its derivative in the backward pass of backpropagation (Rumelhart et al., 1986) with the derivative of a smooth approximation, r, that is, effectively defining the derivative to be d dy [y] := d dy r(y).",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 32,
      "context" : "(2016a), on the other hand, used a stochastic form of binarization (Williams, 1992).",
      "startOffset" : 67,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "Motivated by theoretical links to dithering, Ballé et al. (2016) proposed to replace quantization by additive uniform noise, [f(x)] ≈ f(x) + u.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "Motivated by theoretical links to dithering, Ballé et al. (2016) proposed to replace quantization by additive uniform noise, [f(x)] ≈ f(x) + u. (4) Toderici et al. (2016a), on the other hand, used a stochastic form of binarization (Williams, 1992).",
      "startOffset" : 45,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "Perhaps most closely related to our work is the work of Ballé et al. (2016). The main differences lie in the way we deal with quantization (see Section 2.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "Perhaps most closely related to our work is the work of Ballé et al. (2016). The main differences lie in the way we deal with quantization (see Section 2.1) and entropy rate estimation. The transformations used by Ballé et al. (2016) consist of a single linear layer combined with a form of contrast gain control, while our framework relies on more standard deep convolutional neural networks.",
      "startOffset" : 56,
      "endOffset" : 234
    }, {
      "referenceID" : 1,
      "context" : "Perhaps most closely related to our work is the work of Ballé et al. (2016). The main differences lie in the way we deal with quantization (see Section 2.1) and entropy rate estimation. The transformations used by Ballé et al. (2016) consist of a single linear layer combined with a form of contrast gain control, while our framework relies on more standard deep convolutional neural networks. Toderici et al. (2016a) proposed to use recurrent neural networks (RNNs) for compression.",
      "startOffset" : 56,
      "endOffset" : 418
    }, {
      "referenceID" : 1,
      "context" : "Perhaps most closely related to our work is the work of Ballé et al. (2016). The main differences lie in the way we deal with quantization (see Section 2.1) and entropy rate estimation. The transformations used by Ballé et al. (2016) consist of a single linear layer combined with a form of contrast gain control, while our framework relies on more standard deep convolutional neural networks. Toderici et al. (2016a) proposed to use recurrent neural networks (RNNs) for compression. Instead of entropy coding as in our work, the network tries to minimize the distortion for a given number of bits. The image is encoded in an iterative manner, and decoding is performed in each step to be able to take into account residuals at the next iteration. An advantage of this design is that it allows for progressive coding of images. A disadvantage is that compression is much more time consuming than in our approach, as we use efficient convolutional neural networks and do not necessarily require decoding at the encoding stage. Gregor et al. (2016) explored using variational autoencoders with recurrent encoders and decoders for compression of small images.",
      "startOffset" : 56,
      "endOffset" : 1047
    }, {
      "referenceID" : 5,
      "context" : "While Gregor et al. (2016) used a Gaussian distribution for the encoder, we can link their approach to the work of Ballé et al.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "(2016) used a Gaussian distribution for the encoder, we can link their approach to the work of Ballé et al. (2016) by assuming it to be uniform, p(y | x) = f(x) + u.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "Ollivier (2015) discusses variational autoencoders for lossless compression as well as connections to denoising autoencoders.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 17,
      "context" : "Inspired by the work of Shi et al. (2016), most convolutions are performed in a downsampled space to speed up computation, and upsampling is performed using sub-pixel convolutions (convolutions followed by reshaping/reshuffling of the coefficients).",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "1 ENCODER, DECODER, AND ENTROPY MODEL We use common convolutional neural networks (LeCun et al., 1998) for the encoder and the decoder of the compressive autoencoder.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "This is followed by three residual blocks (He et al., 2015), where each block consists of an additional two convolutional layers with 128 filters each.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "1 ENCODER, DECODER, AND ENTROPY MODEL We use common convolutional neural networks (LeCun et al., 1998) for the encoder and the decoder of the compressive autoencoder. Our architecture was inspired by the work of Shi et al. (2016), who demonstrated that super-resolution can be achieved much more efficiently by operating in the lowresolution space, that is, by convolving images and then upsampling instead of upsampling first and then convolving an image.",
      "startOffset" : 83,
      "endOffset" : 230
    }, {
      "referenceID" : 15,
      "context" : ", Portilla et al., 2003). We used 6 scales in each GSM. Rather than using the more common parametrization above, we parametrized the GSM so that it can be easily used with gradient based methods, optimizing log-weights and log-precisions rather than weights and variances. We note that the leptokurtic nature of GSMs (Andrews & Mallows, 1974) means that the rate term encourages sparsity of coefficients. All networks were implemented in Python using Theano (2016) and Lasagne (Dieleman et al.",
      "startOffset" : 2,
      "endOffset" : 465
    }, {
      "referenceID" : 23,
      "context" : "JPEG (4:2:0, optimized) JP2 Toderici et al. (2016b) CAE (ensemble)",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "We note that the blue line refers to the results of Toderici et al. (2016b) achieved without entropy encoding.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "We compared our method to JPEG (Wallace, 1991), JPEG 2000 (Skodras et al.",
      "startOffset" : 31,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "We compared our method to JPEG (Wallace, 1991), JPEG 2000 (Skodras et al., 2001), and the RNNbased method of (Toderici et al.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "We compared our method to JPEG (Wallace, 1991), JPEG 2000 (Skodras et al., 2001), and the RNNbased method of (Toderici et al., 2016b)4. Bits for header information were not counted towards the bit rate of JPEG and JPEG 2000. Among the different variants of JPEG, we found that optimized JPEG with 4:2:0 chroma sub-sampling generally worked best (Appendix A.2). While fine-tuning a single compressive autoencoder for a wide range of bit rates worked well, optimizing all parameters of a network for a particular rate distortion trade-off still worked better. We here chose the compromise of combining autoencoders trained for low, medium or high bit rates (see Appendix A.4 for details). For each image and bit rate, we choose the autoencoder producing the smallest distortion. This increases the time needed to compress an image, since an image has to be encoded and decoded multiple times. However, decoding an image is still as fast, since it only requires choosing and running one decoder network. A more efficient but potentially less performant solution would be to always choose the same autoencoder for a given rate-distortion tradeoff. We added 1 byte to the coding cost to encode which autoencoder of an ensemble is used. Rate-distortion curves averaged over all test images are shown in Figure 4. We evaluated the different methods in terms of PSNR, SSIM (Wang et al., 2004a), and multiscale SSIM (MS-SSIM; Wang et al., 2004b). We used the implementation of van der Walt et al. (2014) for SSIM and the implementation of Toderici et al.",
      "startOffset" : 59,
      "endOffset" : 1495
    }, {
      "referenceID" : 18,
      "context" : "We compared our method to JPEG (Wallace, 1991), JPEG 2000 (Skodras et al., 2001), and the RNNbased method of (Toderici et al., 2016b)4. Bits for header information were not counted towards the bit rate of JPEG and JPEG 2000. Among the different variants of JPEG, we found that optimized JPEG with 4:2:0 chroma sub-sampling generally worked best (Appendix A.2). While fine-tuning a single compressive autoencoder for a wide range of bit rates worked well, optimizing all parameters of a network for a particular rate distortion trade-off still worked better. We here chose the compromise of combining autoencoders trained for low, medium or high bit rates (see Appendix A.4 for details). For each image and bit rate, we choose the autoencoder producing the smallest distortion. This increases the time needed to compress an image, since an image has to be encoded and decoded multiple times. However, decoding an image is still as fast, since it only requires choosing and running one decoder network. A more efficient but potentially less performant solution would be to always choose the same autoencoder for a given rate-distortion tradeoff. We added 1 byte to the coding cost to encode which autoencoder of an ensemble is used. Rate-distortion curves averaged over all test images are shown in Figure 4. We evaluated the different methods in terms of PSNR, SSIM (Wang et al., 2004a), and multiscale SSIM (MS-SSIM; Wang et al., 2004b). We used the implementation of van der Walt et al. (2014) for SSIM and the implementation of Toderici et al. (2016b) for MS-SSIM.",
      "startOffset" : 59,
      "endOffset" : 1554
    }, {
      "referenceID" : 18,
      "context" : "We compared our method to JPEG (Wallace, 1991), JPEG 2000 (Skodras et al., 2001), and the RNNbased method of (Toderici et al., 2016b)4. Bits for header information were not counted towards the bit rate of JPEG and JPEG 2000. Among the different variants of JPEG, we found that optimized JPEG with 4:2:0 chroma sub-sampling generally worked best (Appendix A.2). While fine-tuning a single compressive autoencoder for a wide range of bit rates worked well, optimizing all parameters of a network for a particular rate distortion trade-off still worked better. We here chose the compromise of combining autoencoders trained for low, medium or high bit rates (see Appendix A.4 for details). For each image and bit rate, we choose the autoencoder producing the smallest distortion. This increases the time needed to compress an image, since an image has to be encoded and decoded multiple times. However, decoding an image is still as fast, since it only requires choosing and running one decoder network. A more efficient but potentially less performant solution would be to always choose the same autoencoder for a given rate-distortion tradeoff. We added 1 byte to the coding cost to encode which autoencoder of an ensemble is used. Rate-distortion curves averaged over all test images are shown in Figure 4. We evaluated the different methods in terms of PSNR, SSIM (Wang et al., 2004a), and multiscale SSIM (MS-SSIM; Wang et al., 2004b). We used the implementation of van der Walt et al. (2014) for SSIM and the implementation of Toderici et al. (2016b) for MS-SSIM. We find that in terms of PSNR, our method performs http://r0k.us/graphics/kodak/ We used the code which was made available on https://github.com/tensorflow/models/ tree/2390974a/compression. We note that at the time of this writing, this implementation does not include entropy coding as in the paper of Toderici et al. (2016b).",
      "startOffset" : 59,
      "endOffset" : 1895
    }, {
      "referenceID" : 23,
      "context" : "CAE JPEG 2000 JPEG Toderici et al. (2016b)",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "While MOS tests have their limitations, they are a widely used standard for evaluating perceptual quality (Streijl et al., 2014).",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "While MOS tests have their limitations, they are a widely used standard for evaluating perceptual quality (Streijl et al., 2014). Our MOS test set included the 24 full-resolution uncompressed originals from the Kodak dataset, as well as the same images compressed using each of four algorithms at or near three different bit rates: 0.25, 0.372 and 0.5 bits per pixel. Only the low-bit-rate CAE was included in this test. For each image, we chose the CAE setting which produced the highest bit rate but did not exceed the target bit rate. The average bit rates of CAE compressed images were 0.24479, 0.36446, and 0.48596, respectively. We then chose the smallest quality factor for JPEG and JPEG 2000 for which the bit rate exceeded that of the CAE. The average bit rates for JPEG were 0.25221, 0.37339 and 0.49534, for JPEG 2000 0.24631, 0.36748 and 0.49373. For some images the bit rate of the CAE at the lowest setting was still higher than the target bit rate. These images were excluded from the final results, leaving 15, 21, and 23 images, respectively. The perceptual quality of the resulting 273 images was rated by n = 24 non-expert evaluators. One evaluator did not finish the experiment, so her data was discarded. The images were presented to each individual in a random order. The evaluators gave a discrete opinion score for each image from a scale between 1 (bad) to 5 (excellent). Before the rating began, subjects were presented an uncompressed calibration image of the same dimensions as the test images (but not from the Kodak dataset). They were then shown four versions of the calibration image using the worst quality setting of all four compression methods, and given the instruction “These are examples of compressed images. These are some of the worst quality examples.” Figure 6 shows average MOS results for each algorithm at each bit rate. 95% confidence intervals were computed via bootstrapping. We found that CAE and JPEG 2000 achieved higher MOS than JPEG or the method of Toderici et al. (2016b) at all bit rates we tested.",
      "startOffset" : 107,
      "endOffset" : 2029
    }, {
      "referenceID" : 1,
      "context" : "Unfortunately, research on perceptually relevant metrics suitable for optimization is still in its infancy (e.g., Dosovitskiy & Brox, 2016; Ballé et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs.",
      "startOffset" : 107,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "An alternative to perceptual metrics may be to use generative adversarial networks (GANs; Goodfellow et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : ", Dosovitskiy & Brox, 2016; Ballé et al., 2016). While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs. In future work, we would like to explore the optimization of compressive autoencoders for different metrics. A promising direction was presented by Bruna et al. (2016), who achieved interesting superresolution results using metrics based on neural networks trained for image classification.",
      "startOffset" : 28,
      "endOffset" : 560
    }, {
      "referenceID" : 1,
      "context" : ", Dosovitskiy & Brox, 2016; Ballé et al., 2016). While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs. In future work, we would like to explore the optimization of compressive autoencoders for different metrics. A promising direction was presented by Bruna et al. (2016), who achieved interesting superresolution results using metrics based on neural networks trained for image classification. Gatys et al. (2016) used similar representations to achieve a breakthrough in perceptually meaningful style transfer.",
      "startOffset" : 28,
      "endOffset" : 703
    }, {
      "referenceID" : 1,
      "context" : ", Dosovitskiy & Brox, 2016; Ballé et al., 2016). While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs. In future work, we would like to explore the optimization of compressive autoencoders for different metrics. A promising direction was presented by Bruna et al. (2016), who achieved interesting superresolution results using metrics based on neural networks trained for image classification. Gatys et al. (2016) used similar representations to achieve a breakthrough in perceptually meaningful style transfer. An alternative to perceptual metrics may be to use generative adversarial networks (GANs; Goodfellow et al., 2014). Building on the work of Bruna et al. (2016) and Dosovitskiy & Brox (2016), Ledig et al.",
      "startOffset" : 28,
      "endOffset" : 961
    }, {
      "referenceID" : 1,
      "context" : ", Dosovitskiy & Brox, 2016; Ballé et al., 2016). While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs. In future work, we would like to explore the optimization of compressive autoencoders for different metrics. A promising direction was presented by Bruna et al. (2016), who achieved interesting superresolution results using metrics based on neural networks trained for image classification. Gatys et al. (2016) used similar representations to achieve a breakthrough in perceptually meaningful style transfer. An alternative to perceptual metrics may be to use generative adversarial networks (GANs; Goodfellow et al., 2014). Building on the work of Bruna et al. (2016) and Dosovitskiy & Brox (2016), Ledig et al.",
      "startOffset" : 28,
      "endOffset" : 991
    }, {
      "referenceID" : 1,
      "context" : ", Dosovitskiy & Brox, 2016; Ballé et al., 2016). While perceptual metrics exist which correlate well with human perception for certain types of distortions (e.g., Wang et al., 2004a; Laparra et al., 2016), developing a perceptual metric which can be optimized is a more challenging task, since this requires the metric to behave well for a much larger variety of distortions and image pairs. In future work, we would like to explore the optimization of compressive autoencoders for different metrics. A promising direction was presented by Bruna et al. (2016), who achieved interesting superresolution results using metrics based on neural networks trained for image classification. Gatys et al. (2016) used similar representations to achieve a breakthrough in perceptually meaningful style transfer. An alternative to perceptual metrics may be to use generative adversarial networks (GANs; Goodfellow et al., 2014). Building on the work of Bruna et al. (2016) and Dosovitskiy & Brox (2016), Ledig et al. (2016) recently demonstrated impressive super-resolution results by combining GANs with feature-based metrics.",
      "startOffset" : 28,
      "endOffset" : 1012
    } ],
    "year" : 2017,
    "abstractText" : "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.",
    "creator" : "LaTeX with hyperref package"
  }
}