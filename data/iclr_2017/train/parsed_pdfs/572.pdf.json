{
  "name" : "572.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ian Fischer", "Dawn Song" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Adversarial examples have been shown to exist for a variety of deep learning architectures.1 They are small perturbations of the original inputs, often barely visible to a human observer, but carefully crafted to misguide the network into producing incorrect outputs. Seminal work by Szegedy et al. (2013) and Goodfellow et al. (2014), as well as much recent work, has shown that adversarial examples are abundant and finding them is easy.\nMost previous work focuses on the application of adversarial examples to the task of classification, where the deep network assigns classes to input images. The attack adds small adversarial perturbations to the original input image. These perturbations cause the network to change its classification of the input, from the correct class to some other incorrect class (possibly chosen by the attacker). Critically, the perturbed input must still be recognizable to a human observer as belonging to the original input class.2\nDeep generative models, such as Kingma & Welling (2013), learn to generate a variety of outputs, ranging from handwritten digits to faces (Kulkarni et al., 2015), realistic scenes (Oord et al., 2016), videos (Kalchbrenner et al., 2016), 3D objects (Dosovitskiy et al., 2016), and audio (van den Oord et al., 2016). These models learn an approximation of the input data distribution in different ways, and then sample from this distribution to generate previously unseen but plausible outputs.\nTo the best of our knowledge, no prior work has explored using adversarial inputs to attack generative models. There are two main requirements for such work: describing a plausible scenario in which an attacker might want to attack a generative model; and designing and demonstrating an attack that succeeds against generative models. We address both of these requirements in this work.\nOne of the most basic applications of generative models is input reconstruction. Given an input image, the model first encodes it into a lower-dimensional latent representation, and then uses that representation to generate a reconstruction of the original input image. Since the latent representation\n1 Adversarial examples are even easier to produce against most other machine learning architectures, as shown in Papernot et al. (2016), but we are focused on deep networks.\n2 Random noise images and “fooling” images (Nguyen et al., 2014) do not belong to this strict definition of an adversarial input, although they do highlight other limitations of current classifiers.\nusually has much fewer dimensions than the original input, it can be used as a form of compression. The latent representation can also be used to remove some types of noise from inputs, even when the network has not been explicitly trained for denoising, due to the lower dimensionality of the latent representation restricting what information the trained network is able to represent. Many generative models also allow manipulation of the generated output by sampling different latent values or modifying individual dimensions of the latent vectors without needing to pass through the encoding step.\nThese properties of input reconstruction generative networks suggest a variety of different attacks that would be enabled by effective adversaries against generative networks. Any attack that targets the compression bottleneck of the latent representation can exploit natural security vulnerabilities in applications built to use that latent representation. Specifically, if the person doing the encoding step is separated from the person doing the decoding step, the attacker may be able to cause the encoding party to believe they have encoded a particular message for the decoding party, but in reality they have encoded a different message of the attacker’s choosing. We explore this idea in more detail as it applies to the application of compressing images using a VAE or VAE-GAN architecture."
    }, {
      "heading" : "2 RELATED WORK AND BACKGROUND",
      "text" : "This work focuses on adversaries for variational autoencoders (VAEs, proposed in Kingma & Welling (2013)) and VAE-GANs (VAEs composed with a generative adversarial network, proposed in Larsen et al. (2015))."
    }, {
      "heading" : "2.1 RELATED WORK ON ADVERSARIES",
      "text" : "Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack. Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).3\nGiven a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the following optimization: argminx∗ L(x,x\n∗) s.t. f(x∗) 6= f(x), where L(·) is a chosen distance measure between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x\n∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker.\nThese optimization problems can often be solved with optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as done in Szegedy et al. (2013) and Carlini & Wagner (2016). They can also be approximated with single-step gradient-based techniques like fast gradient sign (Goodfellow et al., 2014), fast gradient L2 (Huang et al., 2015), or fast least likely class (Kurakin et al., 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016).\nAn interesting variation of this type of attack can be found in Sabour et al. (2015). In that work, they attack the hidden state of the target network directly by taking an input image x and a target image xt and searching for a perturbed variant of x that generates similar hidden state at layer l of the target network to the hidden state at the same layer generated by xt. This approach can also be applied directly to attacking the latent vector of a generative model.\nA variant of this attack has also been applied to VAE models in the concurrent work of Tabacof et al. (2016)4, which uses the KL divergence between the latent representation of the source and target images to generate the adversarial example. However in their paper, the authors mention that they tried attacking the output directly and that this only managed to make the reconstructions more\n3 See Papernot et al. (2015) for an overview of different adversarial threat models. 4 This work was made public shortly after we published our early drafts.\nblurry. While they do not explain the exact experimental setting, the attack sounds similar to our LVAE attack, which we find very successful. Also, in their paper the authors do not consider the more advanced VAE-GAN models and more complex datasets like CelebA."
    }, {
      "heading" : "2.2 BACKGROUND ON VAES AND VAE-GANS",
      "text" : "The general architecture of a variational autoencoder consists of three components, as shown in Figure 8. The encoder fenc(x) is a neural network mapping a high-dimensional input representation x into a lower-dimensional (compressed) latent representation z. All possible values of z form a latent space. Similar values in the latent space should produce similar outputs from the decoder in a well-trained VAE. And finally, the decoder/generator fdec(z), which is a neural network mapping the compressed latent representation back to a high-dimensional output x̂. Composing these networks allows basic input reconstruction x̂ = fdec(fenc(x)). This composed architecture is used during training to backpropagate errors from the loss function.\nThe variational autoencoder’s loss functionLVAE enables the network to learn a latent representation that approximates the intractable posterior distribution p(z|x):\nLVAE = −DKL[q(z|x)||p(z)] + Eq[log p(x|z)]. (1)\nq(z|x) is the learned approximation of the posterior distribution p(z|x). p(z) is the prior distribution of the latent representation z. DKL denotes the Kullback–Leibler divergence. Eq[log p(x|z)] is the variational lower bound, which in the case of input reconstruction is the cross-entropy H[x, x̂] between the inputs x and their reconstructions x̂. In order to generate x̂ the VAE needs to sample q(z|x) and then compute fdec(z). For the VAE to be fully differentiable while sampling from q(z|x), the reparametrization trick (Kingma & Welling, 2013) extracts the random sampling step from the network and turns it into an input, ε. VAEs are often parameterized with Gaussian distributions. In this case, fenc(x) outputs the distribution parametersµ andσ2. That distribution is then sampled by computing z = µ+ε √ σ2 where ε ∼ N(0, 1) is the input random sample, which does not depend on any parameters of fenc, and thus does not impact differentiation of the network.\nThe VAE-GAN architecture of Larsen et al. (2015) has the same fenc and fdec pair as in the VAE. It also adds a discriminator fdisc that is used during training, as in standard generative adversarial networks (Goodfellow et al., 2014). The loss function of fdec uses the disciminator loss instead of cross-entropy for estimating the reconstruction error."
    }, {
      "heading" : "3 PROBLEM DEFINITION",
      "text" : "We provide a motivating attack scenario for adversaries against generative models, as well as a formal definition of an adversary in the generative setting."
    }, {
      "heading" : "3.1 MOTIVATING ATTACK SCENARIO",
      "text" : "To motivate the attacks presented below, we describe the attack scenario depicted in Figure 1. In this scenario, there are two parties, the sender and the receiver, who wish to share images with each other over a computer network. In order to conserve bandwidth, they share a VAE trained on the input distribution of interest, which will allow them to send only latent vectors z.\nThe attacker’s goal is to convince the sender to send an image of the attacker’s choosing to the receiver, but the attacker has no direct control over the bytes sent between the two parties. However, the attacker has a copy of the shared VAE. The attacker presents an image x∗ to the sender which resembles an image x that the sender wants to share with the receiver. For example, the sender wants to share pictures of kittens with the receiver, so the attacker presents a web page to the sender with a picture of a kitten, which is x∗. The sender chooses x∗ and sends its corresponding z to the receiver, who reconstructs it. However, because the attacker controlled the chosen image, when the receiver reconstructs it, instead of getting a faithful reproduction x̂ of x (e.g., a kitten), the receiver sees some other image of the attacker’s choosing, x̂adv, which has a different meaning from x (e.g., a request to send money to the attacker’s bank account).\nThere are other attacks of this general form, where the sender and the receiver may be separated by distance, as in this example, or by time, in the case of storing compressed images to disk for later retrieval. In the time-separated attack, the sender and the receiver may be the same person or multiple different people. In either case, if they are using the insecure channel of the VAE’s latent space, the messages they share may be under the control of an attacker. For example, an attacker may be able to fool an automatic surveillance system if the system uses this type of compression to store the video signal before it is processed by other systems. In this case, the subsequent analysis of the video signal could be on compromised data showing what the attacker wants to show.\nWhile we do not specifically attack their models, viable compression schemes based on deep neural networks have already been proposed in the literature, showing promising results Toderici et al. (2015; 2016)."
    }, {
      "heading" : "3.2 DEFINING ADVERSARIAL EXAMPLES AGAINST GENERATIVE MODELS",
      "text" : "We make the following assumptions about generating adversarial examples on a target generative model, Gtarg(x) = fdec(fenc(x)). Gtarg is trained on inputs X that can naturally be labeled with semantically meaningful classes Y , although there may be no such labels at training time, or the labels may not have been used during training. Gtarg normally succeeds at generating an output x̂ = Gtarg(x) in class y when presented with an input x from class y. In other words, whatever target output class the attacker is interested in, we assume that Gtarg successfully captures it in the latent representation such that it can generate examples of that class from the decoder. This target output class does not need to be from the most salient classes in the training dataset. For example, on models trained on MNIST, the attacker may not care about generating different target digits (which are the most salient classes). The attacker may prefer to generate the same input digits in a different style (perhaps to aid forgery). We also assume that the attacker has access to Gtarg. Finally, the attacker has access to a set of examples from the same distribution as X that have the target label\nyt the attacker wants to generate. This does not mean that the attacker needs access to the labeled training dataset (which may not exist), or to an appropriate labeled dataset with large numbers of examples labeled for each class y ∈ Y (which may be hard or expensive to collect). The attacks described here may be successful with only a small amount of data labeled for a single target class of interest.\nOne way to generate such adversaries is by solving the optimization problem argminx∗ L(x,x\n∗) s.t. ORACLE(Gtarg(x∗)) = yt, where ORACLE reliably discriminates between inputs of class yt and inputs of other classes. In practice, a classifier trained by the attacker may server as ORACLE. Other types of adversaries from Section 2.1 can also be used to approximate this optimization in natural ways, some of which we describe in Section 4.\nIf the attacker only needs to generate one successful attack, the problem of determining if an attack is successful can be solved by manually reviewing the x∗ and x̂adv pairs and choosing whichever the attacker considers best. However, if the attacker wants to generate many successful attacks, an automated method of evaluating the success of an attack is necessary. We show in Section 4.5 how to measure the effectiveness of an attack automatically using a classifier trained on z = fenc(x)."
    }, {
      "heading" : "4 ATTACK METHODOLOGY",
      "text" : "The attacker would like to construct an adversarially-perturbed input to influence the latent representation in a way that will cause the reconstruction process to reconstruct an output for a different class. We propose three approaches to attacking generative models: a classifier-based attack, where we train a new classifier on top of the latent space z and use that classifier to find adversarial examples in the latent space; an attack using LVAE to target the output directly; and an attack on the latent space, z. All three methods are technically applicable to any generative architecture that relies on a learned latent representation z. Without loss of generality, we focus on the VAE-GAN architecture."
    }, {
      "heading" : "4.1 CLASSIFIER ATTACK",
      "text" : "By adding a classifier fclass to the pre-trained generative model5, we can turn the problem of generating adversaries for generative models back into the previously solved problem of generating adversarial examples for classifiers. This approach allows us to apply all of the existing attacks on classifiers in the literature. However, as discussed below, using this classifier tends to produce lower-quality reconstructions from the adversarial examples than the other two attacks due to the inaccuracies of the classifier.\nStep 1. The weights of the target generative model are frozen, and a new classifier fclass(z)→ ŷ is trained on top of fenc using a standard classification loss Lclassifier such as cross-entropy, as shown in Figure 3. This process is independent of how the original model is trained, but it requires a\n5 This is similar to the process of semi-supervised learning in Kingma et al. (2014), although the goal is different.\ntraining corpus pulled from approximately the same input distribution as was used to train Gtarg, with ground truth labels for at least two classes: yt and yt̃, the negative class.\nStep 2. With the trained classifier, the attacker finds adversarial examples x∗ using the methods described in Section 4.4.\nUsing fclass to generate adversarial examples does not always result in high-quality reconstructions, as can be seen in the middle column of Figure 5 and in Figure 11. This appears to be due to the fact that fclass adds additional noise to the process. For example, fclass sometimes confidently misclassifies latent vectors z that represent inputs that are far from the training data distribution, resulting in fdec failing to reconstruct a plausible output from the adversarial example.\n4.2 LVAE ATTACK\nOur second approach generates adversarial perturbations using the VAE loss function. The attacker chooses two inputs, xs (the source) and xt (the target), and uses one of the standard adversarial methods to perturb xs into x∗ such that its reconstruction x̂∗ matches the reconstruction of xt, using the methods described in Section 4.4.\nThe adversary precomputes the reconstruction x̂t by evaluating fdec(fenc(xt)) once before performing optimization. In order to use LVAE in an attack, the second term (the reconstruction loss) of LVAE (see Equation 1) is changed so that instead of computing the reconstruction loss between x and x̂, the loss is computed between x̂∗ and x̂t. This means that during each optimization iteration, the adversary needs to compute x̂∗, which requires the full fdec(fenc(x∗)) to be evaluated."
    }, {
      "heading" : "4.3 LATENT ATTACK",
      "text" : "Our third approach attacks the latent space of the generative model.\nSingle latent vector target. This attack is similar to the work of Sabour et al. (2015), in which they use a pair of source image xs and target image xt to generate x∗ that induces the target network to produce similar activations at some hidden layer l as are produced by xt, while maintaining similarity between xs and x∗.\nFor this attack to work on latent generative models, it is sufficient to compute zt = fenc(xt) and then use the following loss function to generate adversarial examples from different source images xs, using the methods described in Section 4.4:\nLlatent = L(zt, fenc(x∗)). (2) L(·) is a distance measure between two vectors. We use the L2 norm, under the assumption that the latent space is approximately euclidean.\nWe also explored a variation on the single latent vector target attack, which we describe in Section A.1 in the Appendix."
    }, {
      "heading" : "4.4 METHODS FOR SOLVING THE ADVERSARIAL OPTIMIZATION PROBLEM",
      "text" : "We can use a number of different methods to generate the adversarial examples. We initially evaluated both the fast gradient sign Goodfellow et al. (2014) method and an L2 optimization method. As the latter produces much better results we focus on the L2 optimization method, while we include some FGS results in the Appendix. The attack can be used either in targeted mode (where we want a specific class, yt, to be reconstructed) or untargeted mode (where we just want an incorrect class to be reconstructed). In this paper, we focus on the targeted mode of the attacks.\nL2 optimization. The optimization-based approach, explored in Szegedy et al. (2013) and Carlini & Wagner (2016), poses the adversarial generation problem as the following optimization problem:\nargminx∗ λL(x,x ∗) + L(x∗, yt). (3)\nAs above, L(·) is a distance measure, and L is one of Lclassifier, LVAE, or Llatent. The constant λ is used to balance the two loss contributions. For the LVAE attack, the optimizer must do a full\nreconstruction at each step of the optimizer. The other two attacks do not need to do reconstructions while the optimizer is running, so they generate adversarial examples much more quickly, as shown in Table 1."
    }, {
      "heading" : "4.5 MEASURING ATTACK EFFECTIVENESS",
      "text" : "To generate a large number of adversarial examples automatically against a generative model, the attacker needs a way to judge the quality of the adversarial examples. We leverage fclass to estimate whether a particular attack was successful.6\nReconstruction feedback loop. The architecture is the same as shown in Figure 3. We use the generative model to reconstruct the attempted adversarial inputs x∗ by computing:\nx̂∗ = fdec(fenc(x ∗)). (4)\nThen, fclass is used to compute: ŷ = fclass(fenc(x̂\n∗)). (5) The input adversarial examples x∗ are not classified directly, but are first fed to the generative model for reconstruction. This reconstruction loop improves the accuracy of the classifier by 60% on average against the adversarial attacks we examined. The predicted label ŷ after the reconstruction feedback loop is compared with the attack target yt to determine if the adversarial example successfully reconstructed to the target class. If the precision and recall of fclass are sufficiently high on yt, fclass can be used to filter out most of the failed adversarial examples while keeping most of the good ones.\nWe derive two metrics from classifier predictions after one reconstruction feedback loop. The first metric is ASignore−target, the attack success rate ignoring targeting, i.e., without requiring the output class of the adversarial example to match the target class:\nASignore−target = 1\nN N∑ i=1 1ŷi 6=yi (6)\nN is the total number of reconstructed adversarial examples; 1ŷi 6=yi is 1 when ŷi, the classification of the reconstruction for image i, does not equal yi, the ground truth classification of the original image, and 0 otherwise. The second metric is AStarget, the attack success rate including targeting (i.e., requiring the output class of the adversarial example to match the target class), which we define similarly as:\nAStarget = 1\nN N∑ i=1 1ŷi=yit . (7)\nBoth metrics are expected to be higher for more successful attacks. Note that AStarget ≤ ASignore−target. When computing these metrics, we exclude input examples that have the same ground truth class as the target class."
    }, {
      "heading" : "5 EVALUATION",
      "text" : "We evaluate the three attacks on MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011) and CelebA (Liu et al., 2015), using the standard training and validation set splits. The VAE and VAEGAN architectures are implemented in TensorFlow (Abadi & et al., 2015). We optimized using Adam with learning rate 0.001 and other parameters set to default values for both the generative model and the classifier. For the VAE, we use two architectures: a simple architecture with a single fully-connected hidden layer with 512 units and ReLU activation function; and a convolutional architecture taken from the original VAE-GAN paper Larsen et al. (2015) (but trained with only the VAE loss). We use the same architecture trained with the additional GAN loss for the VAE-GAN model, as described in that work. For both VAE and VAE-GAN we use a 50-dimensional latent representation on MNIST, a 1024-dimensional latent representation on SVHN and 2048-dimensional latent representation on CelebA.\n6 Note that fclass here is being used in a different manner than when we use it to generate adversarial examples. However, the network itself is identical, so we don’t distinguish between the two uses in the notation.\nIn this section we only show results where no sampling from latent space has been performed. Instead we use the mean vector µ as the latent representation z. As sampling can have an effect on the resulting reconstructions, we evaluated it separately. We show the results with different number of samples in Figure 22 in the Appendix. On most examples, the visible change is small and in general the attack is still successful."
    }, {
      "heading" : "5.1 MNIST",
      "text" : "Both VAE and VAE-GAN by themselves reconstruct the original inputs well as show in Figure 9, although the quality from the VAE-GAN is noticeably better. As a control, we also generate random noise of the same magnitude as used for the adversarial examples (see Figure 13), to show that random noise does not cause the reconstructed noisy images to change in any significant way. Although we ran experiments on both VAEs and VAE-GANs, we only show results for the VAE-GAN as it generates much higher quality reconstructions than the corresponding VAE."
    }, {
      "heading" : "5.1.1 CLASSIFIER ATTACK",
      "text" : "We use a simple classifier architecture to help generate attacks on the VAE and VAE-GAN models. The classifier consists of two fully-connected hidden layers with 512 units each, using the ReLU activation function. The output layer is a 10 dimensional softmax. The input to the classifier is the 50 dimensional latent representation produced by the VAE/VAE-GAN encoder. The classifier achieves 98.05% accuracy on the validation set after training for 100 epochs.\nTo see if there are differences between classes, we generate targeted adversarial examples for each MNIST class and present the results per-class. For the targeted attacks we used the optimization method with lambda 0.001, where Adam-based optimization was performed for 1000 epochs with a learning rate of 0.1. The mean L2 norm of the difference between original images and generated adversarial examples using the classifier attack is 3.36, while the mean RMSD is 0.120.\nNumerical results in Table 2 show that the targeted classifier attack successfully fools the classifier. Classifier accuracy is reduced to 0%, while the matching rate (the ratio between the number of predictions matching the target class and the number of incorrectly classified images) is 100%, which means that all incorrect predictions match the target class. However, what we are interested in (as per the attack definition from Section 3.2) is how the generative model reconstructs the adversarial examples. If we look at the images generated by the VAE-GAN for class 0, shown in Figure 4, the targeted attack is successful on some reconstructed images (e.g. one, four, five, six and nine are reconstructed as zeroes). But even when the classifier accuracy is 0% and matching rate is 100%, an incorrect classification does not always result in a reconstruction to the target class, which shows that the classifier is fooled by an adversarial example more easily than the generative model.\nReconstruction feedback loop. The reconstruction feedback loop described in Section 4.5 can be used to measure how well a targeted attack succeeds in making the generative model change the\nreconstructed classes. Table 4 in the Appendix shows ASignore−target and AStarget for all source and target class pairs. A higher value signifies a more successful attack for that pair of classes. It is interesting to observe that attacking some source/target pairs is much easier than others (e.g. pair (4, 0) vs. (0, 8)) and that the results are not symmetric over source/target pairs. Also, some pairs do well in ASignore−target, but do poorly in AStarget (e.g., all source digits when targeting 4). As can be seen in Figure 11, the classifier adversarial examples targeting 4 consistently fail to reconstruct into something easily recognizable as a 4. Most of the reconstructions look like 5, but the adversarial example reconstructions of source 5s instead look like 0 or 3.\n5.1.2 LVAE ATTACK\nFor generating adversarial examples using the LVAE attack, we used the optimization method with λ = 1.0, where Adam-based optimization was performed for 1000 epochs with a learning rate of 0.1. The mean L2 norm of the difference between original images and generated adversarial examples with this approach is 3.68, while the mean RMSD is 0.131.\nWe show ASignore−target and AStarget of the LVAE attack in Table 5 in the Appendix. Comparing with the numerical evaluation results of the latent attack (below), we can see that both methods achieve similar results on MNIST."
    }, {
      "heading" : "5.1.3 LATENT ATTACK",
      "text" : "To generate adversarial examples using the latent attack, we used the optimization method with λ = 1.0, where Adam-based optimization was performed for 1000 epochs with a learning rate of 0.1. The mean L2 norm of the difference between original images and generated adversarial examples using this approach is 2.96, while the mean RMSD is 0.105.\nTable 3 shows ASignore−target and AStarget for all source and target class pairs. Comparing with the numerical evaluation results of the classifier attack we can see that the latent attack performs much better. This result remains true when visually comparing the reconstructed images, shown in Figure 5.\nWe also tried an untargeted version of the latent attack, where we change Equation 2 to maximize the distance in latent space between the encoding of the original image and the encoding of the adversarial example. In this case the loss we are trying to minimize is unbounded, since the L2 distance can always grow larger, so the attack normally fails to generate a reasonable adversarial example.\nAdditionally, we also experimented with targeting latent representations of specific images from the training set instead of taking the mean, as described in Section 4.3. We show the numerical results in Table 3 and the generated reconstructions in Figure 15 (in the Appendix). It is also interesting to compare the results with LVAE, by choosing the same image as the target. Results for LVAE for the same target images as in Table 3 are shown in Table 6 in the Appendix. The results are identical between the two attacks, which is expected as the target image is the same – only the loss function differs between the methods."
    }, {
      "heading" : "5.2 SVHN",
      "text" : "The SVHN dataset consists of cropped street number images and is much less clean than MNIST. Due to the way the images have been processed, each image may contain more than one digit; the target digit is roughly in the center. VAE-GAN produces high-quality reconstructions of the original images as shown in Figure 17 in the Appendix.\nFor the classifier attack, we set λ = 10−5 after testing a range of values, although we were unable to find an effective value for this attack against SVHN. For the latent and LVAE attacks we set λ = 10. In Table 10 we show ASignore−target and AStarget for the L2 optimization latent attack. The evaluation metrics are less strong on SVHN than on MNIST, but it is still straightforward for an attacker to find a successful attack for almost all source/target pairs. Figure 2 supports this evaluation. Visual inspection shows that 11 out of the 12 adversarial examples reconstructed as 0, the target digit. It is worth noting that 2 out of the 12 adversarial examples look like zeros (rows 1 and 11), and two others look like both the original digit and zero, depending on whether the viewer focuses on the light or dark areas of the image (rows 4 and 7). The L2 optimization latent attack achieves much better results than the LVAE attack (see Table 11 and Figure 6) on SVHN, while both attacks work equally well on MNIST."
    }, {
      "heading" : "5.3 CELEBA",
      "text" : "The CelebA dataset consists of more than 200,000 cropped faces of celebrities, each annotated with 40 different attributes. For our experiments, we further scale the images to 64x64 and ignore the attribute annotations. VAE-GAN reconstructions of original images after training are shown in Figure 19 in the Appendix.\nSince faces don’t have natural classes, we only evaluated the latent and LVAE attacks. We tried lambdas ranging from 0.1 to 0.75 for both attacks. Figure 20 shows adversarial examples generated\nusing the latent attack and a lambda value of 0.5 (L2 norm between original images and generated adversarial examples 9.78, RMSD 0.088) and the corresponding VAE-GAN reconstructions. Most of the reconstructions reflect the target image very well. We get even better results with the LVAE attack, using a lambda value of 0.75 (L2 norm between original images and generated adversarial examples 8.98, RMSD 0.081) as shown in Figure 21."
    }, {
      "heading" : "5.4 SUMMARY OF DIFFERENT ATTACK METHODS",
      "text" : "Table 1 shows a comparison of the mean distances between original images and generated adversarial examples for the three different attack methods. The larger the distance between the original image and the adversarial perturbation, the more noticeable the perturbation will tend to be, and the more likely a human observer will no longer recognize the original input, so effective attacks keep these distances small while still achieving their goal. The latent attack consistently gives the best results in our experiments, and the classifier attack performs the worst.\nWe also measure the time it takes to generate 1000 adversarial examples using the given attack method. The LVAE attack is by far the slowest of the three, due to the fact that it requires computing full reconstructions at each step of the optimizer when generating the adversarial examples. The other two attacks do not need to run the reconstruction step during optimization of the adversarial examples."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We explored generating adversarial examples against generative models such as VAEs and VAEGANs. These models are also vulnerable to adversaries that convince them to turn inputs into surprisingly different outputs. We have also motivated why an attacker might want to attack generative models. Our work adds further support to the hypothesis that adversarial examples are a general phenomenon for current neural network architectures, given our successful application of adversarial attacks to popular generative models. In this work, we are helping to lay the foundations for understanding how to build more robust networks. Future work will explore defense and robustification in greater depth as well as attacks on generative models trained using natural image datasets such as CIFAR-10 and ImageNet."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This material is in part based upon work supported by the National Science Foundation under Grant No. TWC-1409915. Any opinions, findings, and conclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
    }, {
      "heading" : "A APPENDIX",
      "text" : "A.1 MEAN LATENT VECTOR TARGETED ATTACK\nA variant of the single latent vector targeted attack described in Section 4.3, that was not explored in previous work to our knowledge is to take the mean latent vector of many target images and use that vector as xt. This variant is more flexible, in that the attacker can choose different latent properties to target without needing to find the ideal input. For example, in MNIST, the attacker may wish to have a particular line thickness or slant in the reconstructed digit, but may not have such an image available. In that case, by choosing some images of the target class with thinner lines or less slant, and some with thicker lines or more slant, the attacker can find a target latent vector that closely matches the desired properties.\nIn this case, the attack starts by using fenc to produce the target latent vector, zt, from the chosen target images, x(t).\nzt = 1\n|x(t)| |x(t)|∑ i=0 fenc(x i (t)). (8)\nIn this work, we choose to reconstruct “ideal” MNIST digits by taking the mean latent vector of all of the training digits of each class, and using those vectors as xt. Given a target class yt, a set of examples X and their corresponding ground truth labels y, we create a subset x(t) as follows:\nx(t) = {xi|xi ∈ X ∧ yi = yt} . (9)\nBoth variants of this attack appear to be similarly effective, as shown in Figure 15 and Figure 5. The trade-off between the two in these experiments is between the simplicity of the first attack and the flexibility of the second attack.\nA.2 EVALUATION RESULTS"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org",
      "author" : [ "Martı́n Abadi", "Ashish Agarwal" ],
      "venue" : null,
      "citeRegEx" : "Abadi and Agarwal,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi and Agarwal",
      "year" : 2015
    }, {
      "title" : "Towards evaluating the robustness of neural networks",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "arXiv preprint arXiv:1608.04644,",
      "citeRegEx" : "Carlini and Wagner.,? \\Q2016\\E",
      "shortCiteRegEx" : "Carlini and Wagner.",
      "year" : 2016
    }, {
      "title" : "Learning to generate chairs, tables and cars with convolutional networks",
      "author" : [ "Alexey Dosovitskiy", "Jost Springenberg", "Maxim Tatarchenko", "Thomas Brox" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative Adversarial Networks",
      "author" : [ "I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning with a strong adversary",
      "author" : [ "Ruitong Huang", "Bing Xu", "Dale Schuurmans", "Csaba Szepesvári" ],
      "venue" : "CoRR, abs/1511.03034,",
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Video pixel networks",
      "author" : [ "Nal Kalchbrenner", "Aaron van den Oord", "Karen Simonyan", "Ivo Danihelka", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1610.00527,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep convolutional inverse graphics network",
      "author" : [ "Tejas D Kulkarni", "William F Whitney", "Pushmeet Kohli", "Josh Tenenbaum" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial examples in the physical world",
      "author" : [ "Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio" ],
      "venue" : null,
      "citeRegEx" : "Kurakin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kurakin et al\\.",
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "Anders Boesen Lindbo Larsen", "Søren Kaae Sønderby", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1512.09300,",
      "citeRegEx" : "Larsen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In Proceedings of International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks. 2016",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard" ],
      "venue" : null,
      "citeRegEx" : "Moosavi.Dezfooli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Moosavi.Dezfooli et al\\.",
      "year" : 2016
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable",
      "author" : [ "Anh Mai Nguyen", "Jason Yosinski", "Jeff Clune" ],
      "venue" : "images. CoRR,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2014
    }, {
      "title" : "Conditional image generation with pixelcnn decoders",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1606.05328,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Proceedings of the 1st IEEE European Symposium on Security and Privacy,",
      "citeRegEx" : "Papernot et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2015
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697,",
      "citeRegEx" : "Papernot et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial manipulation of deep representations",
      "author" : [ "Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J. Fleet" ],
      "venue" : "CoRR, abs/1511.05122,",
      "citeRegEx" : "Sabour et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sabour et al\\.",
      "year" : 2015
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "Adversarial Images for Variational Autoencoders",
      "author" : [ "P. Tabacof", "J. Tavares", "E. Valle" ],
      "venue" : "ArXiv eprints,",
      "citeRegEx" : "Tabacof et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tabacof et al\\.",
      "year" : 2016
    }, {
      "title" : "Variable rate image compression with recurrent neural networks",
      "author" : [ "George Toderici", "Sean M O’Malley", "Sung Jin Hwang", "Damien Vincent", "David Minnen", "Shumeet Baluja", "Michele Covell", "Rahul Sukthankar" ],
      "venue" : "arXiv preprint arXiv:1511.06085,",
      "citeRegEx" : "Toderici et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Toderici et al\\.",
      "year" : 2015
    }, {
      "title" : "Full resolution image compression with recurrent neural networks",
      "author" : [ "George Toderici", "Damien Vincent", "Nick Johnston", "Sung Jin Hwang", "David Minnen", "Joel Shor", "Michele Covell" ],
      "venue" : "arXiv preprint arXiv:1608.05148,",
      "citeRegEx" : "Toderici et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Toderici et al\\.",
      "year" : 2016
    }, {
      "title" : "Wavenet: A generative model for raw audio",
      "author" : [ "Aäron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew W. Senior", "Koray Kavukcuoglu" ],
      "venue" : "CoRR, abs/1609.03499,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "2 Deep generative models, such as Kingma & Welling (2013), learn to generate a variety of outputs, ranging from handwritten digits to faces (Kulkarni et al., 2015), realistic scenes (Oord et al.",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : ", 2015), realistic scenes (Oord et al., 2016), videos (Kalchbrenner et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : ", 2016), videos (Kalchbrenner et al., 2016), 3D objects (Dosovitskiy et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : ", 2016), 3D objects (Dosovitskiy et al., 2016), and audio (van den Oord et al.",
      "startOffset" : 20,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "2 Random noise images and “fooling” images (Nguyen et al., 2014) do not belong to this strict definition of an adversarial input, although they do highlight other limitations of current classifiers.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "Seminal work by Szegedy et al. (2013) and Goodfellow et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "(2013) and Goodfellow et al. (2014), as well as much recent work, has shown that adversarial examples are abundant and finding them is easy.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "(2013) and Goodfellow et al. (2014), as well as much recent work, has shown that adversarial examples are abundant and finding them is easy. Most previous work focuses on the application of adversarial examples to the task of classification, where the deep network assigns classes to input images. The attack adds small adversarial perturbations to the original input image. These perturbations cause the network to change its classification of the input, from the correct class to some other incorrect class (possibly chosen by the attacker). Critically, the perturbed input must still be recognizable to a human observer as belonging to the original input class.2 Deep generative models, such as Kingma & Welling (2013), learn to generate a variety of outputs, ranging from handwritten digits to faces (Kulkarni et al.",
      "startOffset" : 11,
      "endOffset" : 722
    }, {
      "referenceID" : 2,
      "context" : ", 2016), 3D objects (Dosovitskiy et al., 2016), and audio (van den Oord et al., 2016). These models learn an approximation of the input data distribution in different ways, and then sample from this distribution to generate previously unseen but plausible outputs. To the best of our knowledge, no prior work has explored using adversarial inputs to attack generative models. There are two main requirements for such work: describing a plausible scenario in which an attacker might want to attack a generative model; and designing and demonstrating an attack that succeeds against generative models. We address both of these requirements in this work. One of the most basic applications of generative models is input reconstruction. Given an input image, the model first encodes it into a lower-dimensional latent representation, and then uses that representation to generate a reconstruction of the original input image. Since the latent representation 1 Adversarial examples are even easier to produce against most other machine learning architectures, as shown in Papernot et al. (2016), but we are focused on deep networks.",
      "startOffset" : 21,
      "endOffset" : 1090
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013).",
      "startOffset" : 123,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013).",
      "startOffset" : 123,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification.",
      "startOffset" : 133,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification.",
      "startOffset" : 133,
      "endOffset" : 180
    }, {
      "referenceID" : 22,
      "context" : "Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al.",
      "startOffset" : 111,
      "endOffset" : 156
    }, {
      "referenceID" : 20,
      "context" : "Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al.",
      "startOffset" : 111,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : ", 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : ", 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "They can also be approximated with single-step gradient-based techniques like fast gradient sign (Goodfellow et al., 2014), fast gradient L2 (Huang et al.",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : ", 2014), fast gradient L2 (Huang et al., 2015), or fast least likely class (Kurakin et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : ", 2015), or fast least likely class (Kurakin et al., 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : ", 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : ", 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "2 RELATED WORK AND BACKGROUND This work focuses on adversaries for variational autoencoders (VAEs, proposed in Kingma & Welling (2013)) and VAE-GANs (VAEs composed with a generative adversarial network, proposed in Larsen et al. (2015)).",
      "startOffset" : 215,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack.",
      "startOffset" : 124,
      "endOffset" : 467
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack. Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).3 Given a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the following optimization: argminx∗ L(x,x ∗) s.t. f(x∗) 6= f(x), where L(·) is a chosen distance measure between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x ∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker. These optimization problems can often be solved with optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as done in Szegedy et al. (2013) and Carlini & Wagner (2016).",
      "startOffset" : 124,
      "endOffset" : 1469
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack. Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).3 Given a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the following optimization: argminx∗ L(x,x ∗) s.t. f(x∗) 6= f(x), where L(·) is a chosen distance measure between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x ∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker. These optimization problems can often be solved with optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as done in Szegedy et al. (2013) and Carlini & Wagner (2016). They can also be approximated with single-step gradient-based techniques like fast gradient sign (Goodfellow et al.",
      "startOffset" : 124,
      "endOffset" : 1497
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack. Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).3 Given a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the following optimization: argminx∗ L(x,x ∗) s.t. f(x∗) 6= f(x), where L(·) is a chosen distance measure between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x ∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker. These optimization problems can often be solved with optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as done in Szegedy et al. (2013) and Carlini & Wagner (2016). They can also be approximated with single-step gradient-based techniques like fast gradient sign (Goodfellow et al., 2014), fast gradient L2 (Huang et al., 2015), or fast least likely class (Kurakin et al., 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016). An interesting variation of this type of attack can be found in Sabour et al. (2015). In that work, they attack the hidden state of the target network directly by taking an input image x and a target image xt and searching for a perturbed variant of x that generates similar hidden state at layer l of the target network to the hidden state at the same layer generated by xt.",
      "startOffset" : 124,
      "endOffset" : 1949
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack. Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).3 Given a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the following optimization: argminx∗ L(x,x ∗) s.t. f(x∗) 6= f(x), where L(·) is a chosen distance measure between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x ∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker. These optimization problems can often be solved with optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as done in Szegedy et al. (2013) and Carlini & Wagner (2016). They can also be approximated with single-step gradient-based techniques like fast gradient sign (Goodfellow et al., 2014), fast gradient L2 (Huang et al., 2015), or fast least likely class (Kurakin et al., 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016). An interesting variation of this type of attack can be found in Sabour et al. (2015). In that work, they attack the hidden state of the target network directly by taking an input image x and a target image xt and searching for a perturbed variant of x that generates similar hidden state at layer l of the target network to the hidden state at the same layer generated by xt. This approach can also be applied directly to attacking the latent vector of a generative model. A variant of this attack has also been applied to VAE models in the concurrent work of Tabacof et al. (2016)4, which uses the KL divergence between the latent representation of the source and target images to generate the adversarial example.",
      "startOffset" : 124,
      "endOffset" : 2446
    }, {
      "referenceID" : 3,
      "context" : "1 RELATED WORK ON ADVERSARIES Many adversarial attacks on classification models have been described in existing literature (Goodfellow et al., 2014; Szegedy et al., 2013). These attacks can be untargeted, where the adversary’s goal is to cause any misclassification, or the least likely misclassification (Goodfellow et al., 2014; Kurakin et al., 2016); or they can be targeted, where the attacker desires a specific misclassification. Moosavi-Dezfooli et al. (2016) gives a recent example of a strong targeted adversarial attack. Some adversarial attacks allow for a threat model where the adversary does not have access to the target model (Szegedy et al., 2013; Papernot et al., 2016), but commonly it is assumed that the attacker does have that access, in an online or offline setting (Goodfellow et al., 2014; Kurakin et al., 2016).3 Given a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the following optimization: argminx∗ L(x,x ∗) s.t. f(x∗) 6= f(x), where L(·) is a chosen distance measure between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x ∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker. These optimization problems can often be solved with optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as done in Szegedy et al. (2013) and Carlini & Wagner (2016). They can also be approximated with single-step gradient-based techniques like fast gradient sign (Goodfellow et al., 2014), fast gradient L2 (Huang et al., 2015), or fast least likely class (Kurakin et al., 2016); or they can be approximated with iterative variants of those and other gradient-based techniques (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016). An interesting variation of this type of attack can be found in Sabour et al. (2015). In that work, they attack the hidden state of the target network directly by taking an input image x and a target image xt and searching for a perturbed variant of x that generates similar hidden state at layer l of the target network to the hidden state at the same layer generated by xt. This approach can also be applied directly to attacking the latent vector of a generative model. A variant of this attack has also been applied to VAE models in the concurrent work of Tabacof et al. (2016)4, which uses the KL divergence between the latent representation of the source and target images to generate the adversarial example. However in their paper, the authors mention that they tried attacking the output directly and that this only managed to make the reconstructions more 3 See Papernot et al. (2015) for an overview of different adversarial threat models.",
      "startOffset" : 124,
      "endOffset" : 2759
    }, {
      "referenceID" : 3,
      "context" : "It also adds a discriminator fdisc that is used during training, as in standard generative adversarial networks (Goodfellow et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "The VAE-GAN architecture of Larsen et al. (2015) has the same fenc and fdec pair as in the VAE.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "This process is independent of how the original model is trained, but it requires a 5 This is similar to the process of semi-supervised learning in Kingma et al. (2014), although the goal is different.",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "This attack is similar to the work of Sabour et al. (2015), in which they use a pair of source image xs and target image xt to generate x∗ that induces the target network to produce similar activations at some hidden layer l as are produced by xt, while maintaining similarity between xs and x∗.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "We initially evaluated both the fast gradient sign Goodfellow et al. (2014) method and an L2 optimization method.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "We initially evaluated both the fast gradient sign Goodfellow et al. (2014) method and an L2 optimization method. As the latter produces much better results we focus on the L2 optimization method, while we include some FGS results in the Appendix. The attack can be used either in targeted mode (where we want a specific class, yt, to be reconstructed) or untargeted mode (where we just want an incorrect class to be reconstructed). In this paper, we focus on the targeted mode of the attacks. L2 optimization. The optimization-based approach, explored in Szegedy et al. (2013) and Carlini & Wagner (2016), poses the adversarial generation problem as the following optimization problem: argminx∗ λL(x,x ∗) + L(x∗, yt).",
      "startOffset" : 51,
      "endOffset" : 578
    }, {
      "referenceID" : 3,
      "context" : "We initially evaluated both the fast gradient sign Goodfellow et al. (2014) method and an L2 optimization method. As the latter produces much better results we focus on the L2 optimization method, while we include some FGS results in the Appendix. The attack can be used either in targeted mode (where we want a specific class, yt, to be reconstructed) or untargeted mode (where we just want an incorrect class to be reconstructed). In this paper, we focus on the targeted mode of the attacks. L2 optimization. The optimization-based approach, explored in Szegedy et al. (2013) and Carlini & Wagner (2016), poses the adversarial generation problem as the following optimization problem: argminx∗ λL(x,x ∗) + L(x∗, yt).",
      "startOffset" : 51,
      "endOffset" : 606
    }, {
      "referenceID" : 13,
      "context" : "5 EVALUATION We evaluate the three attacks on MNIST (LeCun et al., 1998), SVHN (Netzer et al.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : ", 1998), SVHN (Netzer et al., 2011) and CelebA (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : ", 2011) and CelebA (Liu et al., 2015), using the standard training and validation set splits.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "For the VAE, we use two architectures: a simple architecture with a single fully-connected hidden layer with 512 units and ReLU activation function; and a convolutional architecture taken from the original VAE-GAN paper Larsen et al. (2015) (but trained with only the VAE loss).",
      "startOffset" : 220,
      "endOffset" : 241
    } ],
    "year" : 2017,
    "abstractText" : "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.",
    "creator" : "LaTeX with hyperref package"
  }
}