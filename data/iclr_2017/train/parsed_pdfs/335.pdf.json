{
  "name" : "335.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NEURAL POPULATION INFOMAX",
    "authors" : [ "Wentao Huang", "Kechen Zhang" ],
    "emails" : [ "whuang21@jhmi.edu", "kzhang4@jhmi.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon’s mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "How to discover the unknown structures in data is a key task for machine learning. Learning good representations from observed data is important because a clearer description may help reveal the underlying structures. Representation learning has drawn considerable attention in recent years (Bengio et al., 2013). One category of algorithms for unsupervised learning of representations is based on probabilistic models (Lewicki & Sejnowski, 2000; Hinton & Salakhutdinov, 2006; Lee et al., 2008), such as maximum likelihood (ML) estimation, maximum a posteriori (MAP) probability estimation, and related methods. Another category of algorithms is based on reconstruction error or generative criterion (Olshausen & Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal et al., 2010; Goodfellow et al., 2014), and the objective functions usually involve squared errors with additional constraints. Sometimes the reconstruction error or generative criterion may also have a probabilistic interpretation (Olshausen & Field, 1997; Vincent et al., 2010).\nShannon’s information theory is a powerful tool for description of stochastic systems and could be utilized to provide a characterization for good representations (Vincent et al., 2010). However, computational difficulties associated with Shannon’s mutual information (MI) (Shannon, 1948) have hindered its wider applications. The Monte Carlo (MC) sampling (Yarrow et al., 2012) is a convergent method for estimating MI with arbitrary accuracy, but its computational inefficiency makes it unsuitable for difficult optimization problems especially in the cases of high-dimensional input stimuli and large population networks. Bell and Sejnowski (Bell & Sejnowski, 1995; 1997) have directly applied the infomax approach (Linsker, 1988) to independent component analysis (ICA) of data with independent non-Gaussian components assuming additive noise, but their method requires that the number of outputs be equal to the number of inputs. The extensions of ICA to overcomplete or undercomplete bases incur increased algorithm complexity and difficulty in learning of parameters (Lewicki & Sejnowski, 2000; Kreutz-Delgado et al., 2003; Karklin & Simoncelli, 2011).\nSince Shannon MI is closely related to ML and MAP (Huang & Zhang, 2017), the algorithms of representation learning based on probabilistic models should be amenable to information-theoretic treatment. Representation learning based on reconstruction error could be accommodated also by information theory, because the inverse of Fisher information (FI) is the Cramér-Rao lower bound on the mean square decoding error of any unbiased decoder (Rao, 1945). Hence minimizing the reconstruction error potentially maximizes a lower bound on the MI (Vincent et al., 2010).\nRelated problems arise also in neuroscience. It has long been suggested that the real nervous systems might approach an information-theoretic optimum for neural coding and computation (Barlow, 1961; Atick, 1992; Borst & Theunissen, 1999). However, in the cerebral cortex, the number of neurons is huge, with about 105 neurons under a square millimeter of cortical surface (Carlo & Stevens, 2013). It has often been computationally intractable to precisely characterize information coding and processing in large neural populations.\nTo address all these issues, we present a framework for unsupervised learning of representations in a large-scale nonlinear feedforward model based on infomax principle with realistic biological constraints such as neuron models with Poisson spikes. First we adopt an objective function based on an asymptotic formula in the large population limit for the MI between the stimuli and the neural population responses (Huang & Zhang, 2017). Since the objective function is usually nonconvex, choosing a good initial value is very important for its optimization. Starting from an initial value, we use a hierarchical infomax approach to quickly find a tentative global optimal solution for each layer by analytic methods. Finally, a fast convergence learning rule is used for optimizing the final objective function based on the tentative optimal solution. Our algorithm is robust and can learn complete, overcomplete or undercomplete basis vectors quickly from different datasets. Experimental results showed that the convergence rate of our method was significantly faster than other existing methods, often by an order of magnitude. More importantly, the number of output units processed by our method can be very large, much larger than the number of inputs. As far as we know, no existing model can easily deal with this situation."
    }, {
      "heading" : "2 METHODS",
      "text" : ""
    }, {
      "heading" : "2.1 APPROXIMATION OF MUTUAL INFORMATION FOR NEURAL POPULATIONS",
      "text" : "Suppose the input x is a K-dimensional vector, x = (x1, · · · , xK)T , the outputs of N neurons are denoted by a vector, r = (r1, · · · , rN )T , where we assume N is large, generally N K. We denote random variables by upper case letters, e.g., random variables X and R, in contrast to their vector values x and r. The MI between X and R is defined by I(X;R) = 〈 ln p(x|r)p(x) 〉 r,x , where 〈·〉r,x denotes the expectation with respect to the probability density function (PDF) p(r,x).\nOur goal is to maxmize MI I(X;R) by finding the optimal PDF p(r|x) under some constraint conditions, assuming that p(r|x) is characterized by a noise model and activation functions f(x;θn) with parameters θn for the n-th neuron (n = 1, · · · , N ). In other words, we optimize p(r|x) by solving for the optimal parameters θn. Unfortunately, it is intractable in most cases to solve for the optimal parameters that maximizes I(X;R). However, if p(x) and p(r|x) are twice continuously differentiable for almost every x ∈ RK , then for large N we can use an asymptotic formula to approximate the true value of I(X;R) with high accuracy (Huang & Zhang, 2017):\nI(X;R) ' IG = 1\n2\n〈 ln ( det ( G(x)\n2πe ))〉 x +H(X), (1)\nwhere det (·) denotes the matrix determinant and H(X) = −〈ln p(x)〉x is the stimulus entropy, G(x) = J(x) + P (x) , (2)\nJ(x) = − 〈 ∂2 ln p (r|x) ∂x∂xT 〉 r|x , (3)\nP(x) = −∂ 2 ln p (x)\n∂x∂xT . (4) Assuming independent noises in neuronal responses, we have p(r|x) = ∏N n=1 p(rn|x;θn),\nand the Fisher information matrix becomes J(x) ≈ N ∑K1 k=1 αkS(x;θk), where S(x;θk) =\n〈 ∂ ln p(r|x;θk)\n∂x ∂ ln p(r|x;θk) ∂xT 〉 r|x and αk > 0 (k = 1, · · · ,K1) is the population density of param-\neter θk, with ∑K1 k=1 αk = 1, and 1 ≤ K1 ≤ N (see Appendix A.1 for details). Since the cerebral cortex usually forms functional column structures and each column is composed of neurons with the same properties (Hubel & Wiesel, 1962), the positive integer K1 can be regarded as the number of distinct classes in the neural population.\nTherefore, given the activation function f(x;θk), our goal becomes to find the optimal population distribution density αk of parameter vector θk so that the MI between the stimulus x and the response r is maximized. By Eq. (1), our optimization problem can be stated as follows:\nminimize QG[{αk}] = − 1\n2 〈ln (det (G(x)))〉x , (5)\nsubject to K1∑ k=1 αk = 1, αk > 0, ∀k = 1, · · · ,K1. (6)\nSince QG[{αk}] is a convex function of {αk} (Huang & Zhang, 2017), we can readily find the optimal solution for small K by efficient numerical methods. For large K, however, finding an optimal solution by numerical methods becomes intractable. In the following we will propose an alternative approach to this problem. Instead of directly solving for the density distribution {αk}, we optimize the parameters {αk} and {θk} simultaneously under a hierarchical infomax framework."
    }, {
      "heading" : "2.2 HIERARCHICAL INFOMAX",
      "text" : "For clarity, we consider neuron model with Poisson spikes although our method is easily applicable to other noise models. The activation function f(x;θn) is generally a nonlinear function, such as sigmoid and rectified linear unit (ReLU) (Nair & Hinton, 2010). We assume that the nonlinear function for the n-th neuron has the following form: f(x;θn) = f̃(yn; θ̃n), where\nyn = w T nx. (7)\nwith wn being aK-dimensional weights vector, f̃(yn; θ̃n) is a nonlinear function, θn = (wTn , θ̃ T n ) T and θ̃n are the parameter vectors (n = 1, · · · , N ). In general, it is very difficult to find the optimal parameters, θn, n = 1, · · · , N , for the following reasons. First, the number of output neuronsN is very large, usuallyN K. Second, the activation function f(x;θn) is a nonlinear function, which usually leads to a nonconvex optimization problem. For nonconvex optimization problems, the selection of initial values often has a great influence on the final optimization results. Our approach meets these challenges by making better use of the large number of neurons and by finding good initial values by a hierarchical infomax method.\nWe divide the nonlinear transformation into two stages, mapping first from x to yn (n = 1, · · · , N ), and then from yn to f̃(yn; θ̃n), where yn can be regarded as the membrane potential of the n-th neuron, and f̃(yn; θ̃n) as its firing rate. As with the real neurons, we assume that the membrane potential is corrupted by noise:\nY̆n = Yn + Zn, (8) where Zn ∼ N ( 0,σ2 ) is a normal distribution with mean 0 and variance σ2. Then the mean membrane potential of the k-th class subpopulation with Nk = Nαk neurons is given by\nȲk = 1\nNk Nk∑ n=1 Y̆kn = Yk + Z̄k, k = 1, · · · ,K1, (9)\nZ̄k ∼ N (0, N−1k σ 2). (10)\nDefine vectors y̆ = (y̆1, · · · , y̆N )T , ȳ = (ȳ1, · · · , ȳK1)T and y = (y1, · · · , yK1)T , where yk = wTk x (k = 1, · · · ,K1). Notice that y̆n (n = 1, · · · , N ) is also divided into K1 classes, the same as for rn. If we assume f(x;θk) = f̃(ȳk; θ̃k), i.e. assuming an additive Gaussian noise for yn (see Eq. 9), then the random variables X , Y , Y̆ , Ȳ and R form a Markov chain, denoted by X → Y → Y̆ → Ȳ → R (see Figure 1), and we have the following proposition (see Appendix A.2).\nProposition 1. With the random variables X , Y , Y̆ , Ȳ , R and Markov chain X → Y → Y̆ → Ȳ → R, the following equations hold,\nI(X;R) = I(Y ;R) ≤ I(Y̆ ;R) ≤ I(Ȳ ;R), (11) I(X;R) ≤ I(X; Ȳ ) = I(X; Y̆ ) ≤ I(X;Y ), (12)\nand for large Nk (k = 1, · · · ,K1),\nI(Y̆ ;R) ' I(Ȳ ;R) ' I(Y ;R) = I(X;R), (13) I(X;Y ) ' I(X; Ȳ ) = I(X; Y̆ ). (14)\nA major advantage of incorporating membrane noise is that it facilitates finding the optimal solution by using the infomax principle. Moreover, the optimal solution obtained this way is more robust; that is, it discourages overfitting and has a strong ability to resist distortion. With vanishing noise σ2 → 0, we have Ȳk → Yk, f̃(ȳk; θ̃k) ' f̃(yk; θ̃k) = f(x;θk), so that Eqs. (13) and (14) hold as in the case of large Nk.\nTo optimize MI I(Y ;R), the probability distribution of random variable Y , p(y), needs to be determined, i.e. maximizing I(Y ;R) about p(y) under some constraints should yield an optimal distribution: p∗(y) = arg maxp(y) I(Y ;R). Let C = maxp(y) I (Y ;R) be the channel capacity of neural population coding, and we always have I(X;R) ≤ C (Huang & Zhang, 2017). To find a suitable linear transformation from X to Y that is compatible with this distribution p∗(y), a reasonable choice is to maximize I(X; Y̆ ) (≤ I(X;Y )), where Y̆ is a noise-corrupted version of Y . This implies minimum information loss in the first transformation step. However, there may exist many transformations from X to Y̆ that maximize I(X; Y̆ ) (see Appendix A.3.1). Ideally, if we can find a transformation that maximizes both I(X; Y̆ ) and I(Y ;R) simultaneously, then I(X;R) reaches its maximum value: I(X;R) = maxp(y) I (Y ;R) = C.\nFrom the discussion above we see that maximizing I(X;R) can be divided into two steps, namely, maximizing I(X; Y̆ ) and maximizing I(Y ;R). The optimal solutions of max I(X; Y̆ ) and max I(Y ;R) will provide a good initial approximation that tend to be very close to the optimal solution of max I(X;R).\nSimilarly, we can extend this method to multilayer neural population networks. For example, a twolayer network with outputs R(1) and R(2) form a Markov chain, X → R̃(1) → R(1) → R̄(1) →\nR(2), where random variable R̃(1) is similar to Y , random variable R(1) is similar to Y̆ , and R̄(1) is similar to Ȳ in the above. Then we can show that the optimal solution of max I(X;R(2)) can be approximated by the solutions of max I(X;R(1)) and max I(R̃(1);R(2)), with I(R̃(1);R(2)) ' I(R̄(1);R(2)).\nMore generally, consider a highly nonlinear feedforward neural network that maps the input x to output z, with z = F (x;θ) = hL ◦ · · · ◦ h1 (x), where hl (l = 1, · · · , L) is a linear or nonlinear function (Montufar et al., 2014). We aim to find the optimal parameter θ by maximizing I (X;Z). It is usually difficult to solve the optimization problem when there are many local extrema for F (x;θ). However, if each function hl is easy to optimize, then we can use the hierarchical infomax method described above to get a good initial approximation to its global optimization solution, and go from there to find the final optimal solution. This information-theoretic consideration from the neural population coding point of view may help explain why deep structure networks with unsupervised pre-training have a powerful ability for learning representations."
    }, {
      "heading" : "2.3 THE OBJECTIVE FUNCTION",
      "text" : "The optimization processes for maximizing I(X; Y̆ ) and maximizing I(Y ;R) are discussed in detail in Appendix A.3. First, by maximizing I(X; Y̆ ) (see Appendix A.3.1 for details), we can get the optimal weight parameter wk (k = 1, · · · ,K1, see Eq. 7) and its population density αk (see Eq. 6) which satisfy\nW = [w1, · · · ,wK1 ] = aU0Σ −1/2 0 C, (15) α1 = · · · = αK1 = K−11 , (16)\nwhere a = √ K1K −1 0 , C = [c1, · · · , cK1 ] ∈ RK0×K1 , CC\nT = IK0 , IK0 is a K0 × K0 identity matrix with integer K0 ∈ [1,K], the diagonal matrix Σ0 ∈ RK0×K0 and matrix U0 ∈ RK×K0 are given in (A.44) and (A.45), with K0 given by Eq. (A.52). Matrices Σ0 and U0 can be obtained by Σ and U with UT0 U0 = IK0 and U0Σ0U T 0 ≈ UΣU T ≈ 〈 xxT 〉 x\n(see Eq. A.23). The optimal weight parameter wk (15) means that the input variable x must first undergo a whiteninglike transformation x̂ = Σ−1/20 U T 0 x, and then goes through the transformation y = aC\nT x̂, with matrix C to be optimized below. Note that weight matrix W satisfies rank(W) = min(K0,K1), which is a low rank matrix, and its low dimensionality helps reduce overfitting during training (see Appendix A.3.1).\nBy maximizing I (Y ;R) (see Appendix A.3.2), we further solve the the optimal parameters θ̃k for the nonlinear functions f̃(yk; θ̃k), k = 1, · · · ,K1. Finally, the objective function for our optimization problem (Eqs. 5 and 6) turns into (see Appendix A.3.3 for details):\nminimize Q [C] =− 1 2\n〈 ln ( det ( CΦ̂C T ))〉\nx̂ , (17)\nsubject to CCT = IK0 , (18) where Φ̂ = diag ( φ(ŷ1) 2, · · · , φ(ŷK1)2 ) , φ(ŷk) = a−1 |∂gk(ŷk)/∂ŷk| (k = 1, · · · ,K1), gk(ŷk) =\n2 √ f̃(ŷk; θ̃k), ŷk = a−1yk = cTk x̂, and x̂ = Σ −1/2 0 U T 0 x. We apply the gradient descent method to optimize the objective function, with the gradient of Q[C] given by:\ndQ[C]\ndC = −\n〈( CΦ̂C T )−1 CΦ̂ + x̂ωT 〉\nx̂\n, (19)\nwhere ω = (ω1, · · · , ωK1) T , ωk = φ(ŷk)φ′(ŷk)cTk\n( CΦ̂C T )−1 ck, k = 1, · · · ,K1.\nWhen K0 = K1 (or K0 > K1), the objective function Q[C] can be reduced to a simpler form, and its gradient is also easy to compute (see Appendix A.4.1). However, when K0 < K1, it is computationally expensive to update C by applying the gradient of Q[C] directly, since it requires matrix inversion for every x̂. We use another objective function Q̂[C] (see Eq. A.118) which is an approximation to Q[C], but its gradient is easier to compute (see Appendix A.4.2). The function\nQ̂[C] is the approximation of Q[C], ideally they have the same optimal solution for the parameter C.\nUsually, for optimizing the objective in Eq. 17, the orthogonality constraint (Eq. 18) is unnecessary. However, this orthogonality constraint can accelerate the convergence rate if we employ it for the initial iteration to update C (see Appendix A.5)."
    }, {
      "heading" : "3 EXPERIMENTAL RESULTS",
      "text" : "We have applied our methods to the natural images from Olshausen’s image dataset (Olshausen & Field, 1996) and the images of handwritten digits from MNIST dataset (LeCun et al., 1998) using Matlab 2016a on a computer with 12 Intel CPU cores (2.4 GHz). The gray level of each raw image was normalized to the range of 0 to 1. M image patches with size w × w = K for training were randomly sampled from the images. We used the Poisson neuron model with a modified sigmoidal\ntuning function f̃(y; θ̃) = 1 4(1+exp(−βy−b))2 , with g(y) = 2 √ f̃(y; θ̃) = 11+exp(−βy−b) , where\nθ̃ = (β, b) T . We obtained the initial values (see Appendix A.3.2): b0 = 0 and β0 ≈ 1.81 √ K1K −1 0 . For our experiments, we set β = 0.5β0 for iteration epoch t = 1, · · · , t0 and β = β0 for t = t0 + 1, · · · , tmax, where t0 = 50. Firstly, we tested the case of K = K0 = K1 = 144 and randomly sampled M = 105 image patches with size 12×12 from the Olshausen’s natural images, assuming thatN = 106 neurons were divided into K1 = 144 classes and = 1 (see Eq. A.52 in Appendix). The input patches were preprocessed by the ZCA whitening filters (see Eq. A.68). To test our algorithms, we chose the batch size to be equal to the number of training samples M , although we could also choose a smaller batch size. We updated the matrix C from a random start, and set parameters tmax = 300, v1 = 0.4, and τ = 0.8 for all experiments.\nIn this case, the optimal solution C looked similar to the optimal solution of IICA (Bell & Sejnowski, 1997). We also compared with the fast ICA algorithm (FICA) (Hyvärinen, 1999), which is faster than IICA. We also tested the restricted Boltzmann machine (RBM) (Hinton et al., 2006) for a unsupervised learning of representations, and found that it could not easily learn Gabor-like filters from Olshausen’s image dataset as trained by contrastive divergence. However, an improved method by adding a sparsity constraint on the output units, e.g., sparse RBM (SRBM) (Lee et al., 2008) or sparse autoencoder (Hinton, 2010), could attain Gabor-like filters from this dataset. Similar results with Gabor-like filters were also reproduced by the denoising autoencoders (Vincent et al., 2010), which method requires a careful choice of parameters, such as noise level, learning rate, and batch size.\nIn order to compare our methods, i.e. Algorithm 1 (Alg.1, see Appendix A.4.1) and Algorithm 2 (Alg.2, see Appendix A.4.2), with other methods, i.e. IICA, FICA and SRBM, we implemented these algorithms using the same initial weights and the same training data set (i.e. 105 image patches preprocessed by the ZCA whitening filters). To get a good result by IICA, we must carefully select the parameters; we set the batch size as 50, the initial learning rate as 0.01, and final learning rate as 0.0001, with an exponential decay with the epoch of iterations. IICA tends to have a faster convergence rate for a bigger batch size but it may become harder to escape local minima. For FICA, we chose the nonlinearity function f(u) = log cosh(u) as contrast function, and for SRBM, we set the sparseness control constant p as 0.01 and 0.03. The number of epoches for iterations was set to 300 for all algorithms. Figure 2 shows the filters learned by our methods and other methods. Each filter in Figure 2(a) corresponds to a column vector of matrix Č (see Eq. A.69), where each vector for display is normalized by čk ← čk/max(|č1,k|, · · · , |čK,k|), k = 1, · · · ,K1. The results in Figures 2(a), 2(b) and 2(c) look very similar to one another, and slightly different from the results in Figure 2(d) and 2(e). There are no Gabor-like filters in Figure 2(f), which corresponds to SRBM with p = 0.03.\nFigure 3 shows how the coefficient entropy (CFE) (see Eq. A.122) and the conditional entropy (CDE) (see Eq. A.125) varied with training time. We calculated CFE and CDE by sampling once every 10 epoches from a total of 300 epoches. These results show that our algorithms had a fast convergence rate towards stable solutions while having CFE and CDE values similar to the algorithm of IICA, which converged much more slowly. Here the values of CFE and CDE should be as small\n100 101 102\n1.8\n1.85\n1.9\n1.95\n2\nco ef\nfic ie\nnt e\nnt ro\npy (\nbi ts ) Alg.1 Alg.2 IICA FICA SRBM (p = 0.01) SRBM (p = 0.03)\n100 101 102\n-400\n-350\n-300\n-250\n-200\n-150\nco nd\niti on\nal e\nnt ro\npy (\nbi ts ) Alg.1 Alg.2 IICA\n100 101 102\n-200\n-100\n0\n100\n200\n300\nco nd\niti on\nal e\nnt ro\npy (\nbi ts\n)\nSRBM (p = 0.01) SRBM (p = 0.03) SRBM (p = 0.05) SRBM (p = 0.10)\nas possible for a good representation learned from the same data set. Here we set epoch number t0 = 50 in our algorithms (see Alg.1 and Alg.2), and the start time was set to 1 second. This explains the step seen in Figure 3 (b) for Alg.1 and Alg.2 since the parameter β was updated when epoch number t = t0. FICA had a convergence rate close to our algorithms but had a big CFE, which is reflected by the quality of the filter results in Figure 2. The convergence rate and CFE for SRBM were close to IICA, but SRBM had a much bigger CDE than IICA, which implies that the information had a greater loss when passing through the system optimized by SRBM than by IICA or our methods.\nFrom Figure 3(c) we see that the CDE (or MI I(X;R), see Eq. A.124 and A.125) decreases (or increases) with the increase of the value of the sparseness control constant p. Note that a smaller p means sparser outputs. Hence, in this sense, increasing sparsity may result in sacrificing some information. On the other hand, a weak sparsity constraint may lead to failure of learning Gaborlike filters (see Figure 2(f)), and increasing sparsity has an advantage in reducing the impact of noise in many practical cases. Similar situation also occurs in sparse coding (Olshausen & Field, 1997), which provides a class of algorithms for learning overcomplete dictionary representations of the input signals. However, its training is time consuming due to its expensive computational cost, although many new training algorithms have emerged (e.g. Aharon et al., 2006; Elad & Aharon, 2006; Lee et al., 2006; Mairal et al., 2010). See Appendix A.5 for additional experimental results."
    }, {
      "heading" : "4 CONCLUSIONS",
      "text" : "In this paper, we have presented a framework for unsupervised learning of representations via information maximization for neural populations. Information theory is a powerful tool for machine learning and it also provides a benchmark of optimization principle for neural information processing in nervous systems. Our framework is based on an asymptotic approximation to MI for a large-scale neural population. To optimize the infomax objective, we first use hierarchical infomax to obtain a good approximation to the global optimal solution. Analytical solutions of the hierarchical infomax are further improved by a fast convergence algorithm based on gradient descent. This method allows us to optimize highly nonlinear neural networks via hierarchical optimization using infomax principle.\nFrom the viewpoint of information theory, the unsupervised pre-training for deep learning (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) may be reinterpreted as a process of hierarchical infomax, which might help explain why unsupervised pre-training helps deep learning (Erhan et al., 2010). In our framework, a pre-whitening step can emerge naturally by the hierarchical infomax, which might also explain why a pre-whitening step is useful for training in many learning algorithms (Coates et al., 2011; Bengio, 2012).\nOur model naturally incorporates a considerable degree of biological realism. It allows the optimization of a large-scale neural population with noisy spiking neurons while taking into account of multiple biological constraints, such as membrane noise, limited energy, and bounded connection weights. We employ a technique to attain a low-rank weight matrix for optimization, so as to reduce the influence of noise and discourage overfitting during training. In our model, many parameters are optimized, including the population density of parameters, filter weight vectors, and parameters for nonlinear tuning functions. Optimizing all these model parameters could not be easily done by many other methods.\nOur experimental results suggest that our method for unsupervised learning of representations has obvious advantages in its training speed and robustness over the main existing methods. Our model has a nonlinear feedforward structure and is convenient for fast learning and inference. This simple and flexible framework for unsupervised learning of presentations should be readily extended to training deep structure networks. In future work, it would interesting to use our method to train deep structure networks with either unsupervised or supervised learning."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Prof. Honglak Lee for sharing Matlab code for algorithm comparison, Prof. Shan Tan for discussions and comments and Kai Liu for helping draw Figure 1. Supported by grant NIH-NIDCD R01 DC013698."
    }, {
      "heading" : "APPENDIX",
      "text" : ""
    }, {
      "heading" : "A.1 FORMULAS FOR APPROXIMATION OF MUTUAL INFORMATION",
      "text" : "It follows from I(X;R) = 〈\nln p(x|r)p(x) 〉 r,x and Eq. (1) that the conditional entropy should read:\nH(X|R) = −〈ln p(x|r)〉r,x ' − 1\n2\n〈 ln ( det ( G(x)\n2πe ))〉 x . (A.1)\nThe Fisher information matrix J(x) (see Eq. 3), which is symmetric and positive semidefinite, can be written also as\nJ(x) =\n〈 ∂ ln p(r|x)\n∂x\n∂ ln p(r|x) ∂xT 〉 r|x . (A.2)\nIf we suppose p(r|x) is conditional independent, namely, p(r|x) = ∏N n=1 p(rn|x;θn), then we have (see Huang & Zhang, 2017)\nJ(x) = N ∫ Θ p(θ)S(x;θ)dθ, (A.3)\nS(x;θ) =\n〈 ∂ ln p(r|x;θ)\n∂x\n∂ ln p(r|x;θ) ∂xT 〉 r|x , (A.4)\nwhere p(θ) is the population density function of parameter θ,\np(θ) = 1\nN N∑ n=1 δ(θ − θn), (A.5)\nand δ(·) denotes the Dirac delta function. It can be proved that the approximation function of MI IG[p(θ)] (Eq. 1) is concave about p(θ) (Huang & Zhang, 2017). In Eq. (A.3), we can approximate the continuous integral by a discrete summation for numerical computation,\nJ(x) ≈ N K1∑ k=1 αkS(x;θk), (A.6)\nwhere ∑K1 k=1 αk = 1, αk > 0, k = 1, · · · ,K1, 1 ≤ K1 ≤ N .\nFor Poisson neuron model, by Eq. (A.4) we have (see Huang & Zhang, 2017)\np(r|x;θ) = f(x;θ) r\nr! exp (−f(x;θ)), (A.7)\nS(x;θ) = 1\nf(x;θ)\n∂f(x;θ)\n∂x\n∂f(x;θ)\n∂xT\n= ∂g(x;θ)\n∂x\n∂g(x;θ)\n∂xT , (A.8)\nwhere f(x;θ) ≥ 0 is the activation function (mean response) of neuron and g(x;θ) = 2 √ f(x;θ). (A.9)\nSimilarly, for Gaussian noise model, we have\np(r|x;θ) = 1 σ √ 2π exp\n( − (r − f(x;θ)) 2\n2σ2\n) , (A.10)\nS(x;θ) = 1 σ2 ∂f(x;θ) ∂x ∂f(x;θ) ∂xT , (A.11)\nwhere σ > 0 denotes the standard deviation of noise.\nSometimes we do not know the specific form of p(x) and only know M samples, x1, · · · , xM , which are independent and identically distributed (i.i.d.) samples drawn from the distribution p(x). Then we can use the empirical average to approximate the integral in Eq. (1):\nIG ≈ 1\n2 M∑ m=1 ln (det (G(xm))) +H (X) . (A.12)"
    }, {
      "heading" : "A.2 PROOF OF PROPOSITION 1",
      "text" : "Proof. It follows from the data-processing inequality (Cover & Thomas, 2006) that\nI(X;R) ≤ I(Y ;R) ≤ I(Y̆ ;R) ≤ I(Ȳ ;R), (A.13) I(X;R) ≤ I(X; Ȳ ) ≤ I(X; Y̆ ) ≤ I(X;Y ). (A.14)\nSince p(ȳk|x) = p(y̆k1 , · · · , y̆kNk |x) = N (w T k x, N −1 k σ 2), k = 1, · · · ,K1, (A.15)\nwe have\np(ȳ|x) = p(y̆|x), (A.16) p(ȳ) = p(y̆), (A.17)\nI(X; Ȳ ) = I(X; Y̆ ). (A.18)\nHence, by (A.14) and (A.18), expression (12) holds. On the other hand, when Nk is large, from Eq. (10) we know that the distribution of Z̄k, namely, N ( 0,N−1k σ 2 ) , approaches a Dirac delta function δ(z̄k). Then by (7) and (9) we have p (r|ȳ) ' p(r|y) = p (r|x) and\nI(X;R) = I (Y ;R)− 〈\nln p (r|y) p (r|x) 〉 r,x = I (Y ;R) , (A.19)\nI (Y ;R) = I(Ȳ ;R)− 〈\nln p (r|ȳ) p (r|y) 〉 r,y,ȳ ' I(Ȳ ;R), (A.20)\nI (Y ;R) = I(Y̆ ;R)− 〈\nln p (r|y̆) p (r|y) 〉 r,y,y̆ ' I(Y̆ ;R), (A.21)\nI(X;Y ) = I(X; Ȳ )− 〈\nln p (x|ȳ) p (x|y) 〉 x,y,ȳ ' I(X; Ȳ ). (A.22)\nIt follows from (A.13) and (A.19) that (11) holds. Combining (11), (12) and (A.20)–(A.22), we immediately get (13) and (14). This completes the proof of Proposition 1.\nA.3 HIERARCHICAL OPTIMIZATION FOR MAXIMIZING I(X;R)\nIn the following, we will discuss the optimization procedure for maximizing I(X;R) in two stages: maximizing I(X; Y̆ ) and maximizing I(Y ;R)."
    }, {
      "heading" : "A.3.1 THE 1ST STAGE",
      "text" : "In the first stage, our goal is to maximize the MI I(X; Y̆ ) and get the optimal parameters wk (k = 1, · · · ,K1). Assume that the stimulus x has zero mean (if not, let x ← x − 〈x〉x) and covariance matrix Σx. It follows from eigendecomposition that\nΣx = 〈 xxT 〉 x ≈ 1 M − 1 XXT = UΣUT , (A.23)\nwhere X = [x1, · · · , xM ], U = [u1, · · · ,uK ] ∈ RK×K is an unitary orthogonal matrix and Σ = diag ( σ21 , · · · , σ2K ) is a positive diagonal matrix with σ1 ≥ · · · ≥ σK > 0. Define\nx̃ = Σ−1/2UTx, (A.24)\nw̃k = Σ 1/2UTwk, (A.25)\nyk = w̃ T k x̃, (A.26)\nwhere k = 1, · · · ,K1. The covariance matrix of x̃ is given by Σx̃ = 〈 x̃x̃T 〉 x̃ ≈ IK , (A.27)\nand IK is a K ×K identity matrix. From (1) and (A.11) we have I(X; Y̆ ) = I(X̃; Y̆ ) and\nI(X̃; Y̆ ) ' I ′G = 1\n2 ln\n( det ( G̃\n2πe\n)) +H(X̃), (A.28)\nG̃ ≈ Nσ−2 K1∑ k=1 αkw̃kw̃ T k + IK . (A.29)\nThe following approximations are useful (see Huang & Zhang, 2017):\np(x̃) ≈ N (0, IK), (A.30)\nP(x̃) = −∂ 2 ln p (x̃)\n∂x̃∂x̃T ≈ IK . (A.31)\nBy the central limit theorem, the distribution of random variable X̃ is closer to a normal distribution than the distribution of the original random variable X . On the other hand, the PCA models assume multivariate gaussian data whereas the ICA models assume multivariate non-gaussian data. Hence by a PCA-like whitening transformation (A.24) we can use the approximation (A.31) with the Laplace’s method of asymptotic expansion, which only requires that the peak be close to its mean while random variable X̃ needs not be exactly Gaussian.\nWithout any constraints on the Gaussian channel of neural populations, especially the peak firing rates, the capacity of this channel may grow indefinitely: I(X̃; Y̆ ) → ∞. The most common constraint on the neural populations is an energy or power constraint which can also be regarded as a signal-to-noise ratio (SNR) constraint. The SNR for the output y̆n of the n-th neuron is given by\nSNRn = 1\nσ2\n〈( wTnx )2〉 x ≈ 1 σ2 w̃Tn w̃n, n = 1, · · · , N . (A.32)\nWe require that 1\nN N∑ n=1 SNRn ≈ 1 σ2 K1∑ k=1 αkw̃ T k w̃k ≤ ρ, (A.33)\nwhere ρ is a positive constant. Then by Eq. (A.28), (A.29) and (A.33), we have the following optimization problem:\nminimize Q′G[Ŵ] = − 1 2 ln ( det ( Nσ−2ŴŴ T + IK )) , (A.34)\nsubject to h = Tr ( ŴŴ T ) − E ≤ 0, (A.35)\nwhere Tr (·) denotes matrix trace and\nŴ = W̃A 1/2\n= Σ1/2UTWA1/2 = [ŵ1, · · · , ŵK1 ], (A.36) A = diag (α1, · · · , αK1), (A.37) W = [w1, · · · ,wK1 ], (A.38) W̃ = [w̃1, · · · , w̃K1 ], (A.39) E = ρσ2. (A.40)\nHere E is a constant that does not affect the final optimal solution so we set E = 1. Then we obtain an optimal solution as follows:\nW = aU0Σ −1/2 0 V T 0 , (A.41)\nA = K−11 IK1 , (A.42)\na = √ EK1K −1 0 = √ K1K −1 0 , (A.43)\nΣ0 = diag ( σ21 , · · · , σ2K0 ) , (A.44) U0 = U (:, 1:K0) ∈ RK×K0 , (A.45) V0 = V (:, 1:K0) ∈ RK1×K0 , (A.46)\nwhere V = [v1, · · · ,vK1 ] is an K1 ×K1 unitary orthogonal matrix, parameter K0 represents the size of the reduced dimension (1 ≤ K0 ≤ K), and its value will be determined below. Now the optimal parameters wn (n = 1, · · · , N ) are clustered into K1 classes (see Eq. A.6) and obey an uniform discrete distribution (see also Eq. A.60 in Appendix A.3.2).\nWhen K = K0 = K1, the optimal solution of W in Eq. (A.41) is a whitening-like filter. When V = IK , the optimal matrix W is the principal component analysis (PCA) whitening filters. In the symmetrical case with V = U, the optimal matrix W becomes a zero component analysis (ZCA) whitening filter. IfK < K1, this case leads to an overcomplete solution, whereas whenK0 ≤ K1 < K, the undercomplete solution arises. Since K0 ≤ K1 and K0 ≤ K, Q′G achieves its minimum when K0 = K. However, in practice other factors may prevent it from reaching this minimum. For example, consider the average of squared weights,\nς = K1∑ k=1 αk ‖wk‖2 = Tr ( WAWT ) = E K0 K0∑ k=1 σ−2k , (A.47)\nwhere ‖·‖ denotes the Frobenius norm. The value of ς is extremely large when any σk becomes vanishingly small. For real neurons these weights of connection are not allowed to be too large. Hence we impose a limitation on the weights: ς ≤ E1, where E1 is a positive constant. This yields another constraint on the objective function,\nh̃ = E\nK0 K0∑ k=1 σ−2k − E1 ≤ 0. (A.48)\nFrom (A.35) and (A.48) we get the optimal K0 = arg maxK̃0 ( EK̃−10 ∑K̃0 k=1 σ −2 k ) . By this constraint, small values of σ2k will often result in K0 < K and a low-rank matrix W (Eq. A.41).\nOn the other hand, the low-rank matrix W can filter out the noise of stimulus x. Consider the transformation Y = WTX with X = [x1, · · · , xM ] and Y = [y1, · · · , yM ] for M samples. It follows from the singular value decomposition (SVD) of X that\nX = USṼ T , (A.49)\nwhere U is given in (A.23), Ṽ is aM×M unitary orthogonal matrix, S is aK×M diagonal matrix with non-negative real numbers on the diagonal, Sk,k = √ M − 1σk (k = 1, · · · ,K, K ≤M ), and SST = (M − 1)Σ. Let X̆ = √ M − 1U0Σ1/20 ṼT0 ≈ X, (A.50)\nwhere Ṽ0 = Ṽ (:, 1:K0) ∈ RM×K0 , Σ0 and U0 are given in (A.44) and (A.45), respectively. Then\nY = WTX = aV0Σ −1/2 0 U T 0 USṼ T = WT X̆ = a √ M − 1V0ṼT0 , (A.51)\nwhere X̆ can be regarded as a denoised version of X. The determination of the effective rank K0 ≤ K of the matrix X̆ by using singular values is based on various criteria (Konstantinides & Yao, 1988). Here we choose K0 as follows:\nK0 = arg min K′0\n √√√√∑K′0k=1 σ2k∑K\nk=1 σ 2 k\n≥  , (A.52) where is a positive constant (0 < ≤ 1). Another advantage of a low-rank matrix W is that it can significantly reduce overfitting for learning neural population parameters. In practice, the constraint (A.47) is equivalent to a weight-decay regularization term used in many other optimization problems (Cortes & Vapnik, 1995; Hinton, 2010), which can reduce overfitting to the training data. To prevent the neural networks from overfitting, Srivastava et al. (2014) presented a technique to randomly drop units from the neural network during training, which may in fact be regarded as an attempt to reduce the rank of the weight matrix because the dropout can result in a sparser weights (lower rank matrix). This means that the update is only concerned with keeping the more important components, which is similar to first performing a denoising process by the SVD low rank approximation.\nIn this stage, we have obtained the optimal parameter W (see A.41). The optimal value of matrix V0 can also be determined, as shown in Appendix A.3.3."
    }, {
      "heading" : "A.3.2 THE 2ND STAGE",
      "text" : "For this stage, our goal is to maximize the MI I(Y ;R) and get the optimal parameters θ̃k, k = 1, · · · ,K1. Here the input is y = (y1, · · · , yK1)T and the output r = (r1, · · · , rN )T is also clustered into K1 classes. The responses of Nk neurons in the k-th subpopulation obey a Poisson distribution with mean f̃(eTk y; θ̃k), where ek is a unit vector with 1 in the k-th element and yk = e T k y. By (A.24) and (A.26), we have\n〈yk〉yk = 0, (A.53) σ2yk = 〈 y2k 〉 yk = ‖w̃k‖2 . (A.54)\nThen for large N , by (1)–(4) and (A.30) we can use the following approximation,\nI(Y ;R) ' ĬF = 1\n2\n〈 ln ( det ( J̆(y)\n2πe ))〉 y +H(Y ), (A.55)\nwhere\nJ̆(y) = diag ( Nα1 |g′1(y1)| 2 , · · · , NαK1 ∣∣g′K1(yK1)∣∣2) , (A.56) g′k(yk) = ∂gk(yk)\n∂yk , k = 1, · · · ,K1, (A.57)\ngk(yk) = 2 √ f̃(yk; θ̃k), k = 1, · · · ,K1. (A.58)\nIt is easy to get that\nĬF = 1\n2 K1∑ k=1\n〈 ln ( Nαk |g′k(yk)| 2\n2πe\n)〉 y +H(Y )\n≤ 1 2 K1∑ k=1\n〈 ln ( |g′k(yk)| 2\n2πe )〉 y − K1 2 ln ( K1 N ) +H(Y ), (A.59)\nwhere the equality holds if and only if\nαk = 1\nK1 , k = 1, · · · ,K1, (A.60)\nwhich is consistent with Eq. (A.42).\nOn the other hand, it follows from the Jensen’s inequality that\nĬF =\n〈 ln p (y)−1 det( J̆(y) 2πe )1/2〉 y\n≤ ln ∫ det ( J̆(y)\n2πe\n)1/2 dy, (A.61)\nwhere the equality holds if and only if p (y)−1 det ( J̆(y) )1/2 is a constant, which implies that\np (y) = det ( J̆(y) )1/2 ∫\ndet ( J̆(y) )1/2 dy = ∏K1 k=1 |g′k(yk)|∫ ∏K1 k=1 |g′k(yk)| dy . (A.62)\nFrom (A.61) and (A.62), maximizing ĨF yields\np (yk) = |g′k(yk)|∫ |g′k(yk)| dyk , k = 1, · · · ,K1. (A.63)\nWe assume that (A.63) holds, at least approximately. Hence we can let the peak of g′k(yk) be at yk = 〈yk〉yk = 0 and 〈 y2k 〉 yk = σ2yk = ‖w̃k‖ 2. Then combining (A.57), (A.61) and (A.63) we find the optimal parameters θ̃k for the nonlinear functions f̃(yk; θ̃k), k = 1, · · · ,K1."
    }, {
      "heading" : "A.3.3 THE FINAL OBJECTIVE FUNCTION",
      "text" : "In the preceding sections we have obtained the initial optimal solutions by maximizing I ( X; Y̆ ) and I(Y ;R). In this section, we will discuss how to find the final optimal V0 and other parameters by maximizing I(X;R) from the initial optimal solutions.\nFirst, we have y = W̃T x̃ = aŷ, (A.64)\nwhere a is given in (A.43) and\nŷ = (ŷ1, · · · , ŷK1)T = CT x̂ = ČT x̌, (A.65) x̂ = Σ −1/2 0 U T 0 x, (A.66) C = VT0 ∈ RK0×K1 , (A.67) x̌ = U0Σ −1/2 0 U T 0 x = U0x̂, (A.68)\nČ = U0C = [č1, · · · , čK1 ]. (A.69)\nIt follows that I(X;R) = I ( X̃;R ) ' ĨG = 1\n2\n〈 ln ( det ( G(x̂)\n2πe ))〉 x̂ +H(X̃), (A.70)\nG(x̂) = NŴΦ̂Ŵ T + IK , (A.71)\nŴ = Σ1/2UTWA1/2 = a √ K−11 I K K0C = √ K−10 I K K0C, (A.72)\nwhere IKK0 is a K ×K0 diagonal matrix with value 1 on the diagonal and\nΦ̂ = Φ2, (A.73) Φ = diag (φ(ŷ1), · · · , φ(ŷK1)) , (A.74)\nφ(ŷk) = a −1 ∣∣∣∣∂gk(ŷk)∂ŷk ∣∣∣∣ , (A.75) gk(ŷk) = 2 √ f̃(ŷk; θ̃k), (A.76)\nŷk = a −1yk = c T k x̂, k = 1, · · · ,K1. (A.77)\nThen we have det (G(x̂)) = det ( NK−10 CΦ̂C T + IK0 ) . (A.78)\nFor large N and K0/N → 0, we have det (G(x̂)) ≈ det (J(x̂)) = det ( NK−10 CΦ̂C T ) , (A.79)\nĨG ≈ ĨF = −Q− K 2 ln (2πe)− K0 2 ln (ε) +H(X̃), (A.80) Q = −1 2 〈 ln ( det ( CΦ̂C T ))〉 x̂ , (A.81)\nε = K0 N . (A.82)\nHence we can state the optimization problem as:\nminimize Q [C] =− 1 2\n〈 ln ( det ( CΦ̂C T ))〉\nx̂ , (A.83)\nsubject to CCT = IK0 . (A.84) The gradient from (A.83) is given by:\ndQ[C]\ndC = −\n〈( CΦ̂C T )−1 CΦ̂ + x̂ωT 〉\nx̂\n, (A.85)\nwhere C = [c1, · · · , cK1 ], ω = (ω1, · · · , ωK1) T , and\nωk = φ(ŷk)φ ′(ŷk)c T k\n( CΦ̂C T )−1 ck, k = 1, · · · ,K1. (A.86)\nIn the following we will discuss how to get the optimal solution of C for two specific cases."
    }, {
      "heading" : "A.4 ALGORITHMS FOR OPTIMIZATION OBJECTIVE FUNCTION",
      "text" : "A.4.1 ALGORITHM 1: K0 = K1\nNow CCT = CTC = IK1 , then by Eq. (A.83) we have\nQ1[C] = − 〈 K1∑ k=1 ln (φ(ŷk)) 〉 x̂ , (A.87)\ndQ1[C]\ndC = −\n〈 x̂ωT 〉 x̂ , (A.88)\nωk = φ′(ŷk)\nφ(ŷk) , k = 1, · · · ,K1. (A.89)\nUnder the orthogonality constraints (Eq. A.84), we can use the following update rule for learning C (Edelman et al., 1998; Amari, 1999):\nCt+1 = Ct + µt dCt\ndt , (A.90)\ndCt\ndt = −dQ1[C\nt]\ndCt + Ct\n( dQ1[C t]\ndCt\n)T Ct, (A.91)\nwhere the learning rate parameter µt changes with the iteration count t, t = 1, · · · , tmax. Here we can use the empirical average to approximate the integral in (A.88) (see Eq. A.12). We can also apply stochastic gradient descent (SGD) method for online updating of Ct+1 in (A.90).\nThe orthogonality constraint (Eq. A.84) can accelerate the convergence rate. In practice, the orthogonal constraint (A.84) for objective function (A.83) is not strictly necessary in this case. We can completely discard this constraint condition and consider\nminimize Q2 [C] =− 〈 K1∑ k=1 ln (φ (ŷk)) 〉 x̂ − 1 2 ln ( det ( CTC )) , (A.92)\nwhere we assume rank (C) = K1 = K0. If we let\ndC dt = −CCT dQ2 [C] dC , (A.93)\nthen\nTr\n( dQ2 [C]\ndC\ndCT\ndt\n) = −Tr ( CT dQ2 [C]\ndC\ndQ2 [C]\ndCT C\n) ≤ 0. (A.94)\nTherefore we can use an update rule similar to Eq. A.90 for learning C. In fact, the method can also be extended to the case K0 > K1 by using the same objective function (A.92).\nThe learning rate parameter µt (see A.90) is updated adaptively, as follows. First, calculate\nµt = vt κt , t = 1, · · · , tmax, (A.95)\nκt = 1\nK1 K1∑ k=1 ‖∇Ct(:, k)‖ ‖Ct(:, k)‖ , (A.96)\nand Ct+1 by (A.90) and (A.91), then calculate the value Q1 [ Ct+1 ] . If Q1 [ Ct+1 ] < Q1[C\nt], then let vt+1 ← vt, continue for the next iteration; otherwise, let vt ← τvt, µt ← vt/κt and recalculate Ct+1 and Q1 [ Ct+1 ] . Here 0 < v1 < 1 and 0 < τ < 1 are set as constants. After getting Ct+1 for each update, we employ a Gram–Schmidt orthonormalization process for matrix Ct+1, where the orthonormalization process can accelerate the convergence. However, we can discard the Gram– Schmidt orthonormalization process after iterative t0 (> 1) epochs for more accurate optimization solution C. In this case, the objective function is given by the Eq. (A.92). We can also further optimize parameter b by gradient descent.\nWhen K0 = K1, the objective function Q2 [C] in Eq. (A.92) without constraint is the same as the objective function of infomax ICA (IICA) (Bell & Sejnowski, 1995; 1997), and as a consequence we should get the same optimal solution C. Hence, in this sense, the IICA may be regarded as a special case of our method. Our method has a wider range of applications and can handle more generic situations. Our model is derived by neural populations with a huge number of neurons and it is not restricted to additive noise model. Moreover, our method has a faster convergence rate during training than IICA (see Section 3).\nA.4.2 ALGORITHM 2: K0 ≤ K1\nIn this case, it is computationally expensive to update C by using the gradient of Q (see Eq. A.85), since it needs to compute the inverse matrix for every x̂. Here we provide an alternative method for learning the optimal C. First, we consider the following inequalities.\nProposition 2. The following inequations hold, 1\n2\n〈 ln ( det ( CΦ̂CT ))〉 x̂ ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT ))\n, (A.97)〈 ln ( det ( CΦCT ))〉 x̂ ≤ ln ( det ( C 〈Φ〉x̂ C T )) (A.98)\n≤ 1 2\nln ( det ( C 〈Φ〉2x̂ C T ))\n(A.99)\n≤ 1 2\nln ( det ( C 〈 Φ̂ 〉 x̂ CT )) , (A.100)\nln ( det ( CΦCT )) ≤ 1 2 ln ( det ( CΦ̂CT )) , (A.101)\nwhere C ∈ RK0×K1 , K0 ≤ K1, and CCT = IK0 .\nProof. Functions ln ( det ( C 〈 Φ̂ 〉 x̂ CT )) and ln ( det ( C 〈Φ〉x̂ CT )) are concave functions about p (x̂) (see the proof of Proposition 5.2. in Huang & Zhang, 2017), which fact establishes inequalities (A.97) and (A.98).\nNext we will prove the inequality (A.101). By SVD, we have\nCΦ = ÜD̈V̈ T , (A.102)\nwhere Ü is a K0 ×K0 unitary orthogonal matrix, V̈ = [v̈1, v̈2, · · · , v̈K1 ] is an K1 ×K1 unitary orthogonal matrix, and D̈ is an K0×K1 rectangular diagonal matrix with K0 positive real numbers on the diagonal. By the matrix Hadamard’s inequality and Cauchy–Schwarz inequality we have\ndet ( CΦCTCΦCT ) det ( CΦ̂C T )−1\n= det ( D̈V̈ T CTCV̈D̈ T ( D̈D̈ T )−1)\n= det ( V̈T1 C TCV̈1 ) = det ( CV̈1\n)2 ≤\nK0∏ k=1 ( CV̈1 )2 k,k\n≤ K0∏ k=1 ( CCT )2 k,k ( V̈T1 V̈1 )2 k,k\n= 1, (A.103)\nwhere V̈1 = [v̈1, v̈2, · · · , v̈K0 ] ∈ RK1×K0 . The last equality holds because of CC T = IK0 and V̈T1 V̈1 = IK0 . This establishes inequality (A.101) and the equality holds if and only if K0 = K1 or CV̈1 = IK0 .\nSimilarly, we get inequality (A.99):\nln ( det ( C 〈Φ〉x̂ C T )) ≤ 1 2 ln ( det ( C 〈Φ〉2x̂ C T )) . (A.104)\nBy Jensen’s inequality, we have 〈φ (ŷk)〉2x̂ ≤ 〈 φ (ŷk) 2 〉\nx̂ , ∀k = 1, · · · ,K1. (A.105)\nThen it follows from (A.105) that inequality (A.100) holds:\n1 2 ln ( det ( C 〈Φ〉2x̂ C T )) ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT )) . (A.106)\nThis completes the proof of Proposition 2.\nBy Proposition 2, if K0 = K1 then we get 1\n2\n〈 ln ( det ( Φ̂ ))〉\nx̂ ≤ 1 2 ln ( det (〈 Φ̂ 〉 x̂ )) , (A.107)\n〈ln (det (Φ))〉x̂ ≤ ln (det (〈Φ〉x̂)) (A.108)\n= 1 2 ln ( det ( 〈Φ〉2x̂ )) (A.109) ≤ 1 2 ln ( det (〈 Φ̂ 〉 x̂ )) , (A.110)\nln (det (Φ)) = 1 2 ln ( det ( Φ̂ )) . (A.111)\nOn the other hand, it follows from (A.81) and Proposition 2 that〈 ln ( det ( CΦCT ))〉 x̂ ≤ −Q ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT ))\n, (A.112)〈 ln ( det ( CΦCT ))〉 x̂ ≤ −Q̂ ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT )) . (A.113)\nHence we can see that Q̂ is close to Q (see A.81). Moreover, it follows from the Cauchy–Schwarz inequality that 〈\n(Φ)k,k 〉 x̂ = 〈φ (ŷk)〉ŷk ≤ (∫ φ (ŷk) 2 dŷk ∫ p (ŷk) 2 dŷk )1/2 , (A.114)\nwhere k = 1, · · · ,K1, the equality holds if and only if the following holds:\np (ŷk) = φ (ŷk)∫ φ (ŷk) dŷk , k = 1, · · · ,K1, (A.115)\nwhich is the similar to Eq. (A.63).\nSince I(X;R) = I(Y ;R) (see Proposition 1), by maximizing I(X;R) we hope the equality in inequality (A.61) and equality (A.63) hold, at least approximatively. On the other hand, let\nCopt = arg min C Q[C] = arg max C\n(〈 ln ( det(CΦ̂CT ) )〉\nx̂\n) , (A.116)\nĈopt = arg min C Q̂[C] = arg max C\n( ln ( det ( C 〈Φ〉2x̂ C T ))) , (A.117)\nCopt and Ĉopt make (A.63) and (A.115) to hold true, which implies that they are the same optimal solution: Copt = Ĉopt.\nTherefore, we can use the following objective function Q̂[C] as a substitute for Q[C] and write the optimization problem as:\nminimize Q̂[C] =− 1 2\nln ( det ( C 〈Φ〉2x̂ C T )) , (A.118)\nsubject to CCT = IK0 . (A.119) The update rule (A.90) may also apply here and a modified algorithm similar to Algorithm 1 may be used for parameter learning."
    }, {
      "heading" : "A.5 SUPPLEMENTARY EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A.5.1 QUANTITATIVE METHODS FOR COMPARISON",
      "text" : "To quantify the efficiency of learning representations by the above algorithms, we calculate the coefficient entropy (CFE) for estimating coding cost as follows (Lewicki & Olshausen, 1999; Lewicki & Sejnowski, 2000):\ny̌k = ζw̌ T k x̌, k = 1, · · · ,K1, (A.120)\nζ = K1∑K1\nk=1 ‖w̌k‖ , (A.121)\nwhere x̌ is defined by Eq. (A.68), and w̌k is the corresponding optimal filter. To estimate the probability density of coefficients qk(y̌k) (k = 1, · · · ,K1) from the M training samples, we apply the kernel density estimation for qk(y̌k) and use a normal kernel with an adaptive optimal window width. Then we define the CFE h as\nh = 1\nK1 K1∑ k=1 Hk(Y̌k), (A.122)\nHk(Y̌k) = −∆ ∑ nqk(n∆) log2 qk(n∆), (A.123)\nwhere qk(y̌k) is quantized as discrete qk(n∆) and ∆ is the step size.\nMethods such as IICA and SRBM as well as our methods have feedforward structures in which information is transferred directly through a nonlinear function, e.g., the sigmoid function. We can use the amount of transmitted information to measure the results learned by these methods. Consider a neural population with N neurons, which is a stochastic system with nonlinear transfer functions. We chose a sigmoidal transfer function and Gaussian noise with standard deviation set to 1 as the system noise. In this case, from (1), (A.8) and (A.11), we see that the approximate MI IG is equivalent to the case of the Poisson neuron model. It follows from (A.70)–(A.82) that\nI(X;R) = I ( X̃;R ) = H(X̃)−H ( X̃|R ) ' ĨG = H(X̃)− h1, (A.124)\nH ( X̃|R ) ' h1 = − 1\n2\n〈 ln ( det ( 1\n2πe\n( NK−10 CΦ̂C T + IK0 )))〉 x̂ , (A.125)\nwhere we set N = 106. A good representation should make the MI I(X;R) as big as possible. Equivalently, for the same inputs, a good representation should make the conditional entropy (CDE) H ( X̃|R ) (or h1) as small as possible."
    }, {
      "heading" : "A.5.2 COMPARISON OF BASIS VECTORS",
      "text" : "We compared our algorithm with an up-to-date sparse coding algorithm, the mini-batch dictionary learning (MBDL) as given in (Mairal et al., 2009; 2010) and integrated in Python library, i.e. scikitlearn. The input data was the same as the above, i.e. 105 nature image patches preprocessed by the ZCA whitening filters.\nWe denotes the optimal dictionary learned by MBDL as B̌ ∈ RK×K1 for which each column represents a basis vector. Now we have\nx ≈ UΣ1/2UT B̌y = B̃y, (A.126) B̃ = UΣ1/2UT B̌, (A.127)\nwhere y = (y1, · · · , yK1) T is the coefficient vector.\nSimilarly, we can obtain a dictionary from the filter matrix C. Suppose rank (C) = K0 ≤ K1, then it follows from (A.64) that\nx̂ = ( aCCT )−1 Cy. (A.128)\nBy (A.66) and (A.128), we get\nx ≈ By = aBCTΣ−1/20 UT0 x, (A.129)\nB = a−1U0Σ 1/2 0\n( CCT )−1 C = [b1, · · · ,bK1 ] , (A.130)\nwhere y = WTx = aCTΣ−1/20 U T 0 x, the vectors b1, · · · ,bK1 can be regarded as the basis vectors and the strict equality holds when K0 = K1 = K. Recall that X = [x1, · · · , xM ] = USṼ T (see Eq. A.49) and Y = [y1, · · · , yM ] = WTX = a √ M − 1CT ṼT0 , then we get X̆ = BY =√\nM − 1U0Σ1/20 ṼT0 ≈ X. Hence, Eq. (A.129) holds. The basis vectors shown in Figure 4(a)–4(e) correspond to filters in Figure 2(a)–2(e). And Figure 4(f) illustrates the optimal dictionary B̃ learned by MBDL, where we set the regularization parameter as λ = 1.2/ √ K, the batch size as 50 and the total number of iterations to perform as 20000, which took about 3 hours for training. From Figure 4 we see that these basis vectors obtained by the above algorithms have local Gabor-like shapes except for those by SRBM. If rank(B̌) = K = K1, then the matrix B̌−T can be regarded as a filter matrix like matrix Č (see Eq. A.69). However, from the column vector of matrix B̌−T we cannot find any local Gabor-like filter that resembles the filters shown in Figure 2. Our algorithm has less computational cost and a much faster convergence rate than the sparse coding algorithm. Moreover, the sparse coding method involves a dynamic generative model that requires relaxation and is therefore unsuitable for fast inference, whereas the feedforward framework of our model is easy for inference because it only requires evaluating the nonlinear tuning functions."
    }, {
      "heading" : "A.5.3 LEARNING OVERCOMPLETE BASES",
      "text" : "We have trained our model on the Olshausen’s nature image patches with a highly overcomplete setup by optimizing the objective (A.118) by Alg.2 and got Gabor-like filters. The results of 400 typical filters chosen from 1024 output filters are displayed in Figure 5(a) and corresponding base (see Eq. A.130) are shown in Figure 5(b). Here the parameters are K1 = 1024, tmax = 100, v1 = 0.4, τ = 0.8, and = 0.98 (see A.52), from which we got rank (B) = K0 = 82. Compared to the ICA-like results in Figure 2(a)–2(c), the average size of Gabor-like filters in Figure 5(a) is bigger, indicating that the small noise-like local structures in the images have been filtered out.\nWe have also trained our model on 60,000 images of handwritten digits from MNIST dataset (LeCun et al., 1998) and the resultant 400 typical optimal filters and bases are shown in Figure 5(c) and Figure 5(d), respectively. All parameters were the same as Figure 5(a) and Figure 5(b): K1 = 1024, tmax = 100, v1 = 0.4, τ = 0.8 and = 0.98, from which we got rank (B) = K0 = 183. From these figures we can see that the salient features of the input images are reflected in these filters and bases. We could also get the similar overcomplete filters and bases by SRBM and MBDL. However, the results depended sensitively on the choice of parameters and the training took a long time.\nFigure 6 shows that CFE as a function of training time for Alg.2, where Figure 6(a) corresponds to Figure 5(a)-5(b) for learning nature image patches and Figure 6(b) corresponds to Figure 5(c)-5(d) for learning MNIST dataset. We set parameters tmax = 100 and τ = 0.8 for all experiments and varied parameter v1 for each experiment, with v1 = 0.2, 0.4, 0.6 or 0.8. These results indicate a fast convergence rate for training on different datasets. Generally, the convergence is insensitive to the change of parameter v1.\nWe have also performed additional tests on other image datasets and got similar results, confirming the speed and robustness of our learning method. Compared with other methods, e.g., IICA, FICA, MBDL, SRBM or sparse autoencoders etc., our method appeared to be more efficient and robust for unsupervised learning of representations. We also found that complete and overovercomplete filters and bases learned by our methods had local Gabor-like shapes while the results by SRBM or MBDL did not have this property.\nA.5.4 IMAGE DENOISING\nSimilar to the sparse coding method applied to image denoising (Elad & Aharon, 2006), our method (see Eq. A.130) can also be applied to image denoising, as shown by an example in Figure 7. The filters or bases were learned by using 7×7 image patches sampled from the left half of the image, and subsequently used to reconstruct the right half of the image which was distorted by Gaussian noise. A common practice for evaluating the results of image denoising is by looking at the difference between the reconstruction and the original image. If the reconstruction is perfect the difference should look like Gaussian noise. In Figure 7(c) and 7(d) a dictionary (100 bases) was learned by MBDL and orthogonal matching pursuit was used to estimate the sparse solution. 1 For our method (shown in Figure 7(b)), we first get the optimal filters parameter W, a low rank matrix (K0 < K), then from the distorted image patches xm (m = 1, · · · ,M ) we get filter outputs ym = WTxm and the reconstruction x̆m = Bym (parameters: = 0.975 and K0 = K1 = 14). As can be seen from Figure 7, our method worked better than dictionary learning, although we only used 14 bases compared with 100 bases used by dictionary learning. Our method is also more efficient. We can get better optimal bases B by a generative model using our infomax approach (details not shown).\n1Python source code is available at http://scikit-learn.org/stable/ downloads/plot image denoising.py"
    } ],
    "references" : [ {
      "title" : "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Aharon et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Aharon et al\\.",
      "year" : 2006
    }, {
      "title" : "Natural gradient learning for over- and under-complete bases in ica",
      "author" : [ "S. Amari" ],
      "venue" : "Neural Comput., 11(8), 1875–1883.",
      "citeRegEx" : "Amari,? 1999",
      "shortCiteRegEx" : "Amari",
      "year" : 1999
    }, {
      "title" : "Could information theory provide an ecological theory of sensory processing? Network: Comp",
      "author" : [ "J.J. Atick" ],
      "venue" : "Neural., 3(2), 213–251.",
      "citeRegEx" : "Atick,? 1992",
      "shortCiteRegEx" : "Atick",
      "year" : 1992
    }, {
      "title" : "Possible principles underlying the transformation of sensory messages",
      "author" : [ "H.B. Barlow" ],
      "venue" : "Sensory Communication, (pp. 217–234).",
      "citeRegEx" : "Barlow,? 1961",
      "shortCiteRegEx" : "Barlow",
      "year" : 1961
    }, {
      "title" : "An information-maximization approach to blind separation and blind deconvolution",
      "author" : [ "A.J. Bell", "T.J. Sejnowski" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Bell and Sejnowski,? \\Q1995\\E",
      "shortCiteRegEx" : "Bell and Sejnowski",
      "year" : 1995
    }, {
      "title" : "The ”independent components” of natural scenes are edge filters",
      "author" : [ "A.J. Bell", "T.J. Sejnowski" ],
      "venue" : "Vision Res.,",
      "citeRegEx" : "Bell and Sejnowski,? \\Q1997\\E",
      "shortCiteRegEx" : "Bell and Sejnowski",
      "year" : 1997
    }, {
      "title" : "Deep learning of representations for unsupervised and transfer learning",
      "author" : [ "Y. Bengio" ],
      "venue" : "Unsupervised and Transfer Learning Challenges in Machine Learning, 7, 19.",
      "citeRegEx" : "Bengio,? 2012",
      "shortCiteRegEx" : "Bengio",
      "year" : 2012
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2007
    }, {
      "title" : "Information theory and neural coding",
      "author" : [ "A. Borst", "F.E. Theunissen" ],
      "venue" : "Nature neuroscience,",
      "citeRegEx" : "Borst and Theunissen,? \\Q1999\\E",
      "shortCiteRegEx" : "Borst and Theunissen",
      "year" : 1999
    }, {
      "title" : "Structural uniformity of neocortex, revisited",
      "author" : [ "C.N. Carlo", "C.F. Stevens" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Carlo and Stevens,? \\Q2013\\E",
      "shortCiteRegEx" : "Carlo and Stevens",
      "year" : 2013
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "A. Coates", "A.Y. Ng", "H. Lee" ],
      "venue" : "In International conference on artificial intelligence and statistics (pp",
      "citeRegEx" : "Coates et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2011
    }, {
      "title" : "Elements of Information, 2nd Edition",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : null,
      "citeRegEx" : "Cover and Thomas,? \\Q2006\\E",
      "shortCiteRegEx" : "Cover and Thomas",
      "year" : 2006
    }, {
      "title" : "The geometry of algorithms with orthogonality constraints",
      "author" : [ "A. Edelman", "T.A. Arias", "S.T. Smith" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "Edelman et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Edelman et al\\.",
      "year" : 1998
    }, {
      "title" : "Image denoising via sparse and redundant representations over learned dictionaries",
      "author" : [ "M. Elad", "M. Aharon" ],
      "venue" : "Image Processing, IEEE Transactions on,",
      "citeRegEx" : "Elad and Aharon,? \\Q2006\\E",
      "shortCiteRegEx" : "Elad and Aharon",
      "year" : 2006
    }, {
      "title" : "Why does unsupervised pre-training help deep learning",
      "author" : [ "D. Erhan", "Y. Bengio", "A. Courville", "Manzagol", "P.-A", "P. Vincent", "S. Bengio" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2010
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems (pp. 2672–2680)",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "A practical guide to training restricted boltzmann machines",
      "author" : [ "G. Hinton" ],
      "venue" : "Momentum, 9(1), 926.",
      "citeRegEx" : "Hinton,? 2010",
      "shortCiteRegEx" : "Hinton",
      "year" : 2010
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G. Hinton", "S. Osindero", "Teh", "Y.-W" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton and Salakhutdinov,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton and Salakhutdinov",
      "year" : 2006
    }, {
      "title" : "Information-theoretic bounds and approximations in neural population",
      "author" : [ "W. Huang", "K. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Huang and Zhang,? \\Q2017\\E",
      "shortCiteRegEx" : "Huang and Zhang",
      "year" : 2017
    }, {
      "title" : "Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex",
      "author" : [ "D.H. Hubel", "T.N. Wiesel" ],
      "venue" : "The Journal of physiology,",
      "citeRegEx" : "Hubel and Wiesel,? \\Q1962\\E",
      "shortCiteRegEx" : "Hubel and Wiesel",
      "year" : 1962
    }, {
      "title" : "Fast and robust fixed-point algorithms for independent component analysis",
      "author" : [ "A. Hyvärinen" ],
      "venue" : "Neural Networks, IEEE Transactions on, 10(3), 626–634.",
      "citeRegEx" : "Hyvärinen,? 1999",
      "shortCiteRegEx" : "Hyvärinen",
      "year" : 1999
    }, {
      "title" : "Efficient coding of natural images with a population of noisy linear-nonlinear neurons",
      "author" : [ "Y. Karklin", "E.P. Simoncelli" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Karklin and Simoncelli,? \\Q2011\\E",
      "shortCiteRegEx" : "Karklin and Simoncelli",
      "year" : 2011
    }, {
      "title" : "Statistical analysis of effective singular values in matrix rank determination",
      "author" : [ "K. Konstantinides", "K. Yao" ],
      "venue" : "Acoustics, Speech and Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Konstantinides and Yao,? \\Q1988\\E",
      "shortCiteRegEx" : "Konstantinides and Yao",
      "year" : 1988
    }, {
      "title" : "Dictionary learning algorithms for sparse representation",
      "author" : [ "K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.S. Lee", "T.J. Sejnowski" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Kreutz.Delgado et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kreutz.Delgado et al\\.",
      "year" : 2003
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808)",
      "author" : [ "H. Lee", "A. Battle", "R. Raina", "A.Y. Ng" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2006
    }, {
      "title" : "Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880)",
      "author" : [ "H. Lee", "C. Ekanadham", "A.Y. Ng" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2008
    }, {
      "title" : "Probabilistic framework for the adaptation and comparison of image codes",
      "author" : [ "M.S. Lewicki", "B.A. Olshausen" ],
      "venue" : "JOSA A,",
      "citeRegEx" : "Lewicki and Olshausen,? \\Q1999\\E",
      "shortCiteRegEx" : "Lewicki and Olshausen",
      "year" : 1999
    }, {
      "title" : "Learning overcomplete representations",
      "author" : [ "M.S. Lewicki", "T.J. Sejnowski" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Lewicki and Sejnowski,? \\Q2000\\E",
      "shortCiteRegEx" : "Lewicki and Sejnowski",
      "year" : 2000
    }, {
      "title" : "Self-Organization in a perceptual network",
      "author" : [ "R. Linsker" ],
      "venue" : "Computer, 21(3), 105–117.",
      "citeRegEx" : "Linsker,? 1988",
      "shortCiteRegEx" : "Linsker",
      "year" : 1988
    }, {
      "title" : "Online dictionary learning for sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).:",
      "citeRegEx" : "Mairal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2009
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2010
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems (pp. 2924–2932)",
      "citeRegEx" : "Montufar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Montufar et al\\.",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Nair and Hinton,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton",
      "year" : 2010
    }, {
      "title" : "Emergence of simple-cell receptive field properties by learning a sparse code for natural",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : "images. Nature,",
      "citeRegEx" : "Olshausen and Field,? \\Q1996\\E",
      "shortCiteRegEx" : "Olshausen and Field",
      "year" : 1996
    }, {
      "title" : "Sparse coding with an overcomplete basis set: A strategy employed by v1",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : "Vision Res.,",
      "citeRegEx" : "Olshausen and Field,? \\Q1997\\E",
      "shortCiteRegEx" : "Olshausen and Field",
      "year" : 1997
    }, {
      "title" : "Information and accuracy attainable in the estimation of statistical parameters",
      "author" : [ "C.R. Rao" ],
      "venue" : "Bulletin of the Calcutta Mathematical Society, 37(3), 81–91.",
      "citeRegEx" : "Rao,? 1945",
      "shortCiteRegEx" : "Rao",
      "year" : 1945
    }, {
      "title" : "A mathematical theory of communications",
      "author" : [ "C. Shannon" ],
      "venue" : "Bell System Technical Journal, 27, 379–423 and 623–656.",
      "citeRegEx" : "Shannon,? 1948",
      "shortCiteRegEx" : "Shannon",
      "year" : 1948
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "presented a technique to randomly drop units from the neural network during training, which may in fact be regarded as an attempt to reduce the rank of the weight matrix because the dropout can result in a sparser weights (lower rank matrix)",
      "author" : [ "Srivastava" ],
      "venue" : null,
      "citeRegEx" : "Srivastava,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Representation learning has drawn considerable attention in recent years (Bengio et al., 2013).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "One category of algorithms for unsupervised learning of representations is based on probabilistic models (Lewicki & Sejnowski, 2000; Hinton & Salakhutdinov, 2006; Lee et al., 2008), such as maximum likelihood (ML) estimation, maximum a posteriori (MAP) probability estimation, and related methods.",
      "startOffset" : 105,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "Another category of algorithms is based on reconstruction error or generative criterion (Olshausen & Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal et al., 2010; Goodfellow et al., 2014), and the objective functions usually involve squared errors with additional constraints.",
      "startOffset" : 88,
      "endOffset" : 202
    }, {
      "referenceID" : 33,
      "context" : "Another category of algorithms is based on reconstruction error or generative criterion (Olshausen & Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal et al., 2010; Goodfellow et al., 2014), and the objective functions usually involve squared errors with additional constraints.",
      "startOffset" : 88,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "Another category of algorithms is based on reconstruction error or generative criterion (Olshausen & Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal et al., 2010; Goodfellow et al., 2014), and the objective functions usually involve squared errors with additional constraints.",
      "startOffset" : 88,
      "endOffset" : 202
    }, {
      "referenceID" : 39,
      "context" : "However, computational difficulties associated with Shannon’s mutual information (MI) (Shannon, 1948) have hindered its wider applications.",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : "Bell and Sejnowski (Bell & Sejnowski, 1995; 1997) have directly applied the infomax approach (Linsker, 1988) to independent component analysis (ICA) of data with independent non-Gaussian components assuming additive noise, but their method requires that the number of outputs be equal to the number of inputs.",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "The extensions of ICA to overcomplete or undercomplete bases incur increased algorithm complexity and difficulty in learning of parameters (Lewicki & Sejnowski, 2000; Kreutz-Delgado et al., 2003; Karklin & Simoncelli, 2011).",
      "startOffset" : 139,
      "endOffset" : 223
    }, {
      "referenceID" : 38,
      "context" : "Representation learning based on reconstruction error could be accommodated also by information theory, because the inverse of Fisher information (FI) is the Cramér-Rao lower bound on the mean square decoding error of any unbiased decoder (Rao, 1945).",
      "startOffset" : 239,
      "endOffset" : 250
    }, {
      "referenceID" : 3,
      "context" : "It has long been suggested that the real nervous systems might approach an information-theoretic optimum for neural coding and computation (Barlow, 1961; Atick, 1992; Borst & Theunissen, 1999).",
      "startOffset" : 139,
      "endOffset" : 192
    }, {
      "referenceID" : 2,
      "context" : "It has long been suggested that the real nervous systems might approach an information-theoretic optimum for neural coding and computation (Barlow, 1961; Atick, 1992; Borst & Theunissen, 1999).",
      "startOffset" : 139,
      "endOffset" : 192
    }, {
      "referenceID" : 34,
      "context" : "More generally, consider a highly nonlinear feedforward neural network that maps the input x to output z, with z = F (x;θ) = hL ◦ · · · ◦ h1 (x), where hl (l = 1, · · · , L) is a linear or nonlinear function (Montufar et al., 2014).",
      "startOffset" : 208,
      "endOffset" : 231
    }, {
      "referenceID" : 26,
      "context" : "We have applied our methods to the natural images from Olshausen’s image dataset (Olshausen & Field, 1996) and the images of handwritten digits from MNIST dataset (LeCun et al., 1998) using Matlab 2016a on a computer with 12 Intel CPU cores (2.",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 22,
      "context" : "We also compared with the fast ICA algorithm (FICA) (Hyvärinen, 1999), which is faster than IICA.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "We also tested the restricted Boltzmann machine (RBM) (Hinton et al., 2006) for a unsupervised learning of representations, and found that it could not easily learn Gabor-like filters from Olshausen’s image dataset as trained by contrastive divergence.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ", sparse RBM (SRBM) (Lee et al., 2008) or sparse autoencoder (Hinton, 2010), could attain Gabor-like filters from this dataset.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : ", 2008) or sparse autoencoder (Hinton, 2010), could attain Gabor-like filters from this dataset.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 27,
      "context" : "However, its training is time consuming due to its expensive computational cost, although many new training algorithms have emerged (e.g. Aharon et al., 2006; Elad & Aharon, 2006; Lee et al., 2006; Mairal et al., 2010).",
      "startOffset" : 132,
      "endOffset" : 218
    }, {
      "referenceID" : 33,
      "context" : "However, its training is time consuming due to its expensive computational cost, although many new training algorithms have emerged (e.g. Aharon et al., 2006; Elad & Aharon, 2006; Lee et al., 2006; Mairal et al., 2010).",
      "startOffset" : 132,
      "endOffset" : 218
    }, {
      "referenceID" : 8,
      "context" : "From the viewpoint of information theory, the unsupervised pre-training for deep learning (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) may be reinterpreted as a process of hierarchical infomax, which might help explain why unsupervised pre-training helps deep learning (Erhan et al.",
      "startOffset" : 90,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : ", 2007) may be reinterpreted as a process of hierarchical infomax, which might help explain why unsupervised pre-training helps deep learning (Erhan et al., 2010).",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "In our framework, a pre-whitening step can emerge naturally by the hierarchical infomax, which might also explain why a pre-whitening step is useful for training in many learning algorithms (Coates et al., 2011; Bengio, 2012).",
      "startOffset" : 190,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "In our framework, a pre-whitening step can emerge naturally by the hierarchical infomax, which might also explain why a pre-whitening step is useful for training in many learning algorithms (Coates et al., 2011; Bengio, 2012).",
      "startOffset" : 190,
      "endOffset" : 225
    }, {
      "referenceID" : 20,
      "context" : "Hyvärinen, A. (1999). Fast and robust fixed-point algorithms for independent component analysis.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "Hyvärinen, A. (1999). Fast and robust fixed-point algorithms for independent component analysis. Neural Networks, IEEE Transactions on, 10(3), 626–634. Karklin, Y. & Simoncelli, E. P. (2011). Efficient coding of natural images with a population of noisy linear-nonlinear neurons.",
      "startOffset" : 0,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : "Hyvärinen, A. (1999). Fast and robust fixed-point algorithms for independent component analysis. Neural Networks, IEEE Transactions on, 10(3), 626–634. Karklin, Y. & Simoncelli, E. P. (2011). Efficient coding of natural images with a population of noisy linear-nonlinear neurons. In Advances in neural information processing systems, volume 24 (pp. 999–1007). Konstantinides, K. & Yao, K. (1988). Statistical analysis of effective singular values in matrix rank determination.",
      "startOffset" : 0,
      "endOffset" : 396
    }, {
      "referenceID" : 20,
      "context" : "Hyvärinen, A. (1999). Fast and robust fixed-point algorithms for independent component analysis. Neural Networks, IEEE Transactions on, 10(3), 626–634. Karklin, Y. & Simoncelli, E. P. (2011). Efficient coding of natural images with a population of noisy linear-nonlinear neurons. In Advances in neural information processing systems, volume 24 (pp. 999–1007). Konstantinides, K. & Yao, K. (1988). Statistical analysis of effective singular values in matrix rank determination. Acoustics, Speech and Signal Processing, IEEE Transactions on, 36(5), 757–763. Kreutz-Delgado, K., Murray, J. F., Rao, B. D., Engan, K., Lee, T. S., & Sejnowski, T. J. (2003). Dictionary learning algorithms for sparse representation.",
      "startOffset" : 0,
      "endOffset" : 652
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition.",
      "startOffset" : 2,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms.",
      "startOffset" : 2,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2.",
      "startOffset" : 2,
      "endOffset" : 336
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes.",
      "startOffset" : 2,
      "endOffset" : 495
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations.",
      "startOffset" : 2,
      "endOffset" : 637
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network.",
      "startOffset" : 2,
      "endOffset" : 732
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding.",
      "startOffset" : 2,
      "endOffset" : 855
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding.",
      "startOffset" : 2,
      "endOffset" : 1055
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks.",
      "startOffset" : 2,
      "endOffset" : 1228
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932). Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines.",
      "startOffset" : 2,
      "endOffset" : 1388
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932). Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807– 814). Olshausen, B. A. & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images.",
      "startOffset" : 2,
      "endOffset" : 1588
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932). Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807– 814). Olshausen, B. A. & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583), 607–609. Olshausen, B. A. & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Res.",
      "startOffset" : 2,
      "endOffset" : 1754
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932). Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807– 814). Olshausen, B. A. & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583), 607–609. Olshausen, B. A. & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Res., 37(23), 3311–3325. Rao, C. R. (1945). Information and accuracy attainable in the estimation of statistical parameters.",
      "startOffset" : 2,
      "endOffset" : 1878
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932). Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807– 814). Olshausen, B. A. & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583), 607–609. Olshausen, B. A. & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Res., 37(23), 3311–3325. Rao, C. R. (1945). Information and accuracy attainable in the estimation of statistical parameters. Bulletin of the Calcutta Mathematical Society, 37(3), 81–91. Shannon, C. (1948). A mathematical theory of communications.",
      "startOffset" : 2,
      "endOffset" : 2040
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. Lee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efficient sparse coding algorithms. In Advances in neural information processing systems (pp. 801–808). Lee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In Advances in neural information processing systems (pp. 873–880). Lewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and comparison of image codes. JOSA A, 16(7), 1587–1601. Lewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural computation, 12(2), 337–365. Linsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning (pp. 689–696).: ACM. Mairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 19–60. Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932). Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807– 814). Olshausen, B. A. & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583), 607–609. Olshausen, B. A. & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Res., 37(23), 3311–3325. Rao, C. R. (1945). Information and accuracy attainable in the estimation of statistical parameters. Bulletin of the Calcutta Mathematical Society, 37(3), 81–91. Shannon, C. (1948). A mathematical theory of communications. Bell System Technical Journal, 27, 379–423 and 623–656. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting.",
      "startOffset" : 2,
      "endOffset" : 2224
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.",
      "startOffset" : 2,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : ", Bengio, Y., & Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11, 3371–3408. Yarrow, S., Challis, E., & Series, P. (2012). Fisher and shannon information in finite neural populations.",
      "startOffset" : 2,
      "endOffset" : 258
    }, {
      "referenceID" : 17,
      "context" : "47) is equivalent to a weight-decay regularization term used in many other optimization problems (Cortes & Vapnik, 1995; Hinton, 2010), which can reduce overfitting to the training data.",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "47) is equivalent to a weight-decay regularization term used in many other optimization problems (Cortes & Vapnik, 1995; Hinton, 2010), which can reduce overfitting to the training data. To prevent the neural networks from overfitting, Srivastava et al. (2014) presented a technique to randomly drop units from the neural network during training, which may in fact be regarded as an attempt to reduce the rank of the weight matrix because the dropout can result in a sparser weights (lower rank matrix).",
      "startOffset" : 121,
      "endOffset" : 261
    }, {
      "referenceID" : 13,
      "context" : "84), we can use the following update rule for learning C (Edelman et al., 1998; Amari, 1999): C = C + μt dC dt , (A.",
      "startOffset" : 57,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "84), we can use the following update rule for learning C (Edelman et al., 1998; Amari, 1999): C = C + μt dC dt , (A.",
      "startOffset" : 57,
      "endOffset" : 92
    }, {
      "referenceID" : 32,
      "context" : "2 COMPARISON OF BASIS VECTORS We compared our algorithm with an up-to-date sparse coding algorithm, the mini-batch dictionary learning (MBDL) as given in (Mairal et al., 2009; 2010) and integrated in Python library, i.",
      "startOffset" : 154,
      "endOffset" : 181
    }, {
      "referenceID" : 26,
      "context" : "We have also trained our model on 60,000 images of handwritten digits from MNIST dataset (LeCun et al., 1998) and the resultant 400 typical optimal filters and bases are shown in Figure 5(c) and Figure 5(d), respectively.",
      "startOffset" : 89,
      "endOffset" : 109
    } ],
    "year" : 2017,
    "abstractText" : "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon’s mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",
    "creator" : "LaTeX with hyperref package"
  }
}