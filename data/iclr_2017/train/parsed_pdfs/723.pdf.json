{
  "name" : "723.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Théodore Bluche", "Christopher Kermorvant" ],
    "emails" : [ "tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing openbigrams, i.e. couple of letters that are not necessarily next to each other. In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction. For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this representation and the bagged RNN prediction to retrieve the vocabulary word. We compare this method to standard word recognition techniques based on sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable to more conventional decoding methods based on sequences of letters."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Taking inspiration in Biology is sometimes very efficient. For example, deep neural networks (NN) – which are outperforming all other methods (including support vector machines, SVM) in image recognition – are based on a series of several (usually 5 to 15) neurons layers, each layer involving sparsity in the activation pattern (a biological trait of the cortical map). The analogy continues with the modeling of the cortex as a hierarchy of cortical maps. Thanks to the analysis of reaction time in cognitive psychology experiments, the minimal number of cortical maps involved in a cognitive process is estimated to about ten, the same order of magnitude as the number of layers in deep neural networks for computer vision tasks. In the case of handwritten word recognition, Dehaene et al. have proposed a biologically plausible model of the cortical organization of reading (Dehaene et al., 2005) that assumes seven successive steps of increasing complexity, from the retinal ganglion cells to a cortical map of the orthographic word forms (Figure 1). One of the most recent successes of experimental psychology was the demonstration that human visual word recognition uses an explicit representation of letter position order based on letter pairs: the open-bigram coding (Whitney et al., 2012; Gomez et al., 2008; Grainger & Van Heuven, 2003; Glotin et al., 2010; Dufau, 2008).\nAs demonstrated in (Touzet et al., 2014), open-bigrams (OB) allow an over-coding of the orthographic form of words that facilitates recognition. OB coding favors same length words (i.e., neighbors of similar lengths). In the context of learning to read, the existence of the OB layer just before the orthographic word representation has been used to explain the lack of efficiency of whole language method (today banned from reading teaching) compared to the phonics method which explic-\nitly supervises the organization of the OB map (with syllables), where the global method does not (Figure 1).\nSince cognitive psychology has demonstrated the existence of the OB layer, the hypothesis has been put forward (Touzet et al., 2014) that the orthographic representation of words may have evolved in order to take into account the topology of the OB space, instead of the topology of the single letter space. Our goal here is to test this hypothesis, comparing OB vs sequential character recognition for word recognition. A state-of-the-art decoder based on a Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) is used on two public databases of handwritten words (Rimes and IAM).\nThe remaining of this paper will be divided as follows. In Section 2, we present related methods for handwritten word recognition. Then, we describe the open-bigram representation of words and the proposed decoder in Section 3. The experimental setup, including the data and the bigram prediction model, is explained in Section 4. Finally, we present our results in Section 5, before concluding in Section 6."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In this section, we give a brief overview of existing techniques for handwritten word recognition. Historically, the methods may be divided in three broad categories. The first approach is whole word recognition, where the image of the full word is directly classified into word classes, without relying on the character level (e.g. in (Parisse, 1996; Madhvanath & Govindaraju, 1996)). In the second method, the word image is segmented into parts of characters (stokes or graphemes). The segments are grouped and scored, and character sequences are obtained with a graph search (e.g. in (Bengio et al., 1995)) or with hidden Markov models (HMMs, e.g. in (Knerr et al., 1998)). The last method, most popular nowadays, is a segmentation-free approach. The goal is to predict a character sequence from the image without segmenting it first. The techniques include scanning a sliding window to extract features used in an HMM (e.g. in (Kaltenmeier et al., 1993)), or to feed the image to a neural network able to output sequences of character predictions (e.g. SDNNs (LeCun et al., 1998) or MDLSTM-RNN (Graves & Schmidhuber, 2009)).\nMore recently, different approaches have been proposed to recognize words using character bigrams, and therefore closer to the method we propose in this paper. Jaderberg et al. (2014) propose\nto predict both the characters and ngrams of characters with two distinct convolutional neural networks (CNNs) to recognize text in natural images. Their approach includes a conditional random field as decoder. Similarly, Poznanski & Wolf (2016) train a CNN with a cross-entropy loss to detect common unigrams, bigrams or trigrams of character in a handwritten word image. The output of the network is matched against the lexicon using canonical correlation analysis. Almazán et al. (2014) use Fisher vectors from images and pyramidal character histograms, to learn a feature space shared by the word images and labels, for word spotting, also using canonical correlation analysis."
    }, {
      "heading" : "3 PROPOSED METHOD",
      "text" : ""
    }, {
      "heading" : "3.1 AN OPEN-BIGRAM REPRESENTATION OF WORDS",
      "text" : "The letter bigrams of a word w is the set of pairs of consecutive letters. The open-bigram of order d is the set of pairs of letters separated by d other letters in the word, which we call Bd(w):\nBd(w) = {wiwi+d : i ∈ {1 . . . |w| − d}}. (1)\nThe usual bigrams are open-bigrams of order 1. By extension, we call B0(w) the set of letters in the word w. For example, for word word, we have:\nB1(word) = {or, rd, wo} ; B2(word) = {od, wr} ; B3(word) = {wd}.\nThe general open-bigram representation of a word is the union of\nBd1,...,dn(w) = Bd1(w) ∪ . . . ∪ Bdn(w). (2)\nFor example, B1,2,3(word) = {od, or, rd, wd, wo, wr}. We extend B into B′ by including special bigrams for the letters at the beginning and end of a word:\nB′(w) = B(w) ∪ { w0, w|w| }. (3) So, for example,\nB′1,2,3(word) = { w, d , od, or, rd, wd, wo, wr}. (4)\nIn this paper, we will call B the set of all bigrams, and W the set of all words. We will represent a word of the vocabulary w ∈W as a normalized binary vector vw∈W ∈ <|B|\nvw = [δ(b ∈ B(w))]b∈B√\n|B(w)| , (5)\ni.e. the vector with 0 everywhere and 1/ √ |B(w)| at indices corresponding to bigrams of the word. The stacking of the vector representation of all the words in the vocabulary yields the vocabulary matrix V ∈ <|W |×|B|. Note that in this representation, the bigrams form an unordered set. We do not know: (i) where the bigrams are, (ii) what is the order of a given bigram, (iii) how many times it occurs. The goal is to build a word recognition decoder in the bigram space."
    }, {
      "heading" : "3.2 AN OPEN-BIGRAM DECODER",
      "text" : "While the trivial representation of a word is an ordered sequence of letters, the order in the bigram space is locally embedded in the bigram representation. Most state-of-the-art word recognition systems recognize sequences of letters, and organize the vocabulary for a constrained search as directed graphs, such as prefix trees, or Finite-State Transducers. On the other hand, we can interpret the bigram representation as encoding directed edges in a graph, although we will not explicitly build such a graph for decoding.\nOn Figure 2, we show the graph for a representation of the word into a sequence of letters. Gray edges show the potential risk of a misrecognition in the letter sequences. On Figure 2(b), we display the conceptual representation of bigrams as edges. We observe that a global order of letters can emerge from the local representation. Moreover, the constituent information of a word in the\nbigram space is redundant, potentially making this representation more robust to mispredictions of the optical model.\nThe optical model is the system which provides the predictions of bigrams from the image (or, in the classical approach sequences of character predictions). That is, it provides a confidence measure that each bigram b is present in image x: 0 ≤ pb(x) ≤ 1. This is transformed into a vector in the bigram space:\nqx = [pb(x)]b∈B√∑\nb p 2 b(x)\n. (6)\nFor decoding, we chose the very simple cosine similarity between the query (qx) and a vocabulary word (vw). Since we normalized both vectors, this is simply the dot product:\nd(qx,vw) = v T wqx, (7)\nso the similarity with all words of the vocabulary can be computed with a matrix-vector product:\nDV (x) = V Tqx. (8)\nThe recognized word is the one with maximum similarity with the query:\nw∗ = argmaxDV (x) = argmax w ∑ b∈B(w) pb(x)√ |B(w)| ∑ b p 2 b(x) . (9)\nWe carried out a few preliminary experiments to justify the open-bigram decoder. First, we considered the famous sentence with mixed up letters:\n“aoccdrnig to a rscheearch at cmabrigde uinervtisy it deos not mttaer in waht oredr the ltteers in a wrod are the olny iprmoatnt tihng is taht the frist and lsat ltteers be at the rghit pclae the rset can be a toatl mses and you can sitll raed it wouthit porbelm tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef but the wrod as a wlohe”.\nAlthough the origin and validity of this statement when letters are put in the right order has been discussed 1, it is true that most of us can read it without trouble. For each word of more than one letter in this sentence, we computed the open-bigram representation (d = 0..3), and replaced it with the word having the highest cosine similarity in the English vocabulary described in the next section. The result was:\n“according to a researcher at abridged university it does not matter in what ordered the letters in a word are the only important thing is that the first and last letters be at the right place the rest can be a total messes and you can still read it outwith problem this is because the human mind does not read every letter by itself but the word as a whole”.\nNote that the word “cambridge” was not in the vocabulary. Although the task in this paper is not to recognize mixed up words, it shows the ability of our decoder to perform a reading task that we naturally do.\n1http://www.mrc-cbu.cam.ac.uk/people/matt.davis/cmabridge/\nOn Figure 3, we show the English vocabulary in bigram space (d = 1..3), reduced to two dimensions with t-SNE (Van der Maaten & Hinton, 2008). We observe that words which are close in the bigram space also have a close orthographic form."
    }, {
      "heading" : "4 EXPERIMENTAL SETUP",
      "text" : ""
    }, {
      "heading" : "4.1 DATA PREPARATION",
      "text" : "We carried out the experiments on two public handwritten word databases: Rimes (Augustin et al., 2006) (French), and IAM (Marti & Bunke, 2002) (English). We simplified the problem by limiting ourselves to words of at least two lowercase characters (a to z). This selection removed approximately 30% of the words. The number of words and bigrams of different orders in the different sets are reported on Table 4, in Appendix A.1.\nWe applied deslanting (Buse et al., 1997), contrast enhancement, and padded the images with 10px of white pixels to account for empty context on the left and right of words. From the preprocessed images, we extracted sequences of feature vectors with a sliding window of width 3px. The features are geometrical and statistical features described in (Bianne et al., 2011), which give state-of-the-art results in handwritten text line recognition (Bluche et al., 2014).\nWe downloaded word frequency lists for French and English2. These lists were built from film subtitles 3 written by many contributors, and they contain many misspellings. We removed the misspelled words using GNU Aspell (Atkinson).\nWe selected 50,000 words for each language. They are the most frequent words (length ≥ 2) and made only of lowercase characters between a and z, making sure to also include all the words of the database. For example, the 50,000 most frequent French words fulfilling these condition miss about 200 words of the Rimes database, so we selected the most frequent 49,800 and added the missing 200. Note that most of the words that were removed from the dataset are not shorter words, but words with special or upper case characters. The distribution of lengths of filtered-out words is shown on Figure 5 in Appendix A.1."
    }, {
      "heading" : "4.2 RECOGNITION OF OPEN-BIGRAMS WITH RECURRENT NEURAL NETWORKS (RNNS)",
      "text" : "To predict bigrams, we chose Bidirectional Long Short-Term Memory RNNs (BLSTM-RNNs) for their ability to consider the whole sequence of input vectors to make predictions. We trained one RNN for each order-d bigram, with the Connectionist Temporal Classification (CTC (Graves et al., 2006)) criterion. The CTC framework defines a sequence labeling problem, with an output sequence of labels, of smaller length than the input sequence of observations.\n2http://invokeit.wordpress.com/frequency-word-lists/. 3http://opensubtitles.org\nWe built the target sequences for training as sequences of bigrams, ordered according to the first letter of the bigram. For example, for d = 2, the target for example is ea-xm-ap-ml-pe. The CTC training criterion optimizes the Negative Log-Likelihood (NLL) of the correct label sequence. We set the learning rate to 0.001, and stopped the training when the NLL on the validation set did not decrease for 20 epochs. We kept the network yielding the best NLL on the validation set.\nWe trained one RNN for each order d = 0 to 3, including the special bigrams for word extremities or not. We will refer to each of these RNNs with rnnd for order d (rnnd′ when extremities are included). The architecture of the networks is described in Appendix A.3. These RNNs are trained to predict sequences of fixed order bigrams. Here, we are interested in a word representation as a bag of bigrams, which does not carry any information about the sequence in which the bigrams appear, the number of times each bigram appears, or the order of each individual bigram. That is, we are interested in a decoder which considers an unordered set of bigrams predictions across bigram orders.\nWe forget the temporal aspect of bigram predictions by taking the maximum value of a given bigram prediction by the RNN (where rnnd(x, t) if the output of the RNN for order d, input image x at timestep t):\npd,b(x) = max t rnnd(x, t), (10)\nand we forget the bigram order by taking the maximum output across different values of d:\npb(x) = max d max t rnnd(x, t). (11)\nIt would have been more satisfying for this experiment to train an optical model to predict a set of bigrams for all orders. However, this work is focused on the decoder. Moreover, even the simpler task of predicting a sequence of bigrams of fixed order is challenging (the sequence error rates of these networks are detailed in Appendix B.2). On Figure 4, we show the hypothetical context needed to make two consecutive predictions, for bigram order d = 0..3. RNNs are popular for handwriting recognition, and can consider a context size of variable length – but still local – to predict characters (d = 0).\nFor d = 1, the required context is still local (and would span two consecutive characters), but overlap, because each character is involved in two bigrams. For d > 1, the context is even split into two areas (covering the involved characters) that might be far apart depending on d. Contexts for different predictions are entangled: the whole area between two characters forming a bigram is not relevant for this bigram (and might be of varying size), but will be important to predict other bigrams. It means that the RNN will have to remember a character observation for some time, until it sees the second character of the bigram, while ignoring the area in between for this bigram prediction, but remembering it since it will be useful in order to predict other bigrams. The number of classes for bigrams is also 26 times larger than the number of characters, making the classification problem harder, and the number of examples per class in training smaller."
    }, {
      "heading" : "5 RESULTS",
      "text" : "In this paper, we focused on a subset of Rimes and IAM word databases, which makes the comparison with published results difficult. Instead, we compared the bigram decoder approach to decoding with standard models, consisting of a beam search with Viterbi algorithm in the lexicon. However, these standard models yield state-of-the art results on the reference task for the two considered database (Bluche et al., 2014)."
    }, {
      "heading" : "5.1 BASELINE SYSTEMS BASED ON HMMS AND VITERBI DECODING",
      "text" : "We built several models and used the same vocabulary as for the bigram decoder, and no language model (all words have the same prior probability). These baseline systems are based on HMMs, with emission models made either of Gaussian mixtures (GMM/HMM), Multi-Layer Perceptrons (MLP/HMM) or Recurrent Neural Networks (rnn0/HMM). They are almost identical to those presented in a previous work (Bluche et al., 2014), where a comparison is made with state-of-the-art systems for handwritten text line recognition. More details about these models and their training procedure are presented in Appendix A.2.\nOn Table 1, we report the percentages of word errors on the validation and test sets of Rimes and IAM. The best word error rates are around 10% (17.5% on the test set of IAM), and constitute the baseline performance to which the bigram approach is to be compared."
    }, {
      "heading" : "5.2 MEASURING THE QUALITY OF BIGRAM PREDICTIONS",
      "text" : "Since we keep a confidence value for all bigrams in the prediction vector, rather than using a binary vector (cf. Eq. 6), we modified the formulation of precision and recall. A bigram b ∈ B(w) is correctly retrieved with confidence pb(x), and missed with confidence (1 − pb(x)). Similarly, a bigram not in the representation B(w) of word w is falsely recognized with confidence pb(x), and correctly ignored with confidence (1 − pb(x)). It gives us the following expressions for precision and recall\nprecision =\n∑ (x,w) ∑ b∈B(w) pb(x)∑\nx ∑ b′∈B pb(x) , recall =\n∑ (x,w) ∑ b∈B(w) pb(x)∑\nw∈W |B(w)| , (12)\nwhich are the usual ones when pb(x) ∈ {0, 1}. The F-measure is calculated from precision and recall with the usual formula.\nThe results for all RNNs, and for the combination of orders, are reported on Table 2. We observe that the precision and recall results are correlated to the performance in terms of edit distance or sequence error rates. Namely, they decrease as the bigram order increases, which is not surprising, given that higher order bigrams are more difficult to recognize with these sequence models. We also see that including the special bigrams for word beginnings and endings generally improves the results. This is not surprising either: the RNNs are good at recognizing them.\nDespite this performance decrease, the precision remains above 70%, which limits the amount of noise that will be included in the bigram representation for recognition. Combining the recognition across orders, we obtain a precision of around 84% on Rimes and 80% on IAM. The recall tends to be higher than the precision, staying around or above 80% in all configurations. Across orders, the\nrecall is above 88% on Rimes and 86% on IAM. The high recall will limit the amount of missing information in the bigram representation.\nOverall, the F-measure for bigram recognition is above 80%, which is a good starting point, given that (i) the vocabulary used in decoding will add constraints and may help recovering from some mistakes in the bigram recognition, and (ii) the redundancy and order encoded in the bigram may limit the impact of misrecognitions."
    }, {
      "heading" : "5.3 WORD RECOGNITION USING BIGRAM PREDICTIONS",
      "text" : "On Table 3, we report the results of bigram decoding. For each word image in the validation and test sets, we computed the bigram predictions with the RNNs described above. We combined the different orders as explained previously, and either added the special bigrams for word boundaries and/or the single character predictions or not. We computed the cosine similarity to the bigram decomposition of all words in the vocabularies in the same representation space (i.e. same orders, and same choices for the inclusion of special bigrams and single characters) by computing the product of the vocabulary matrix V by the recognition vector. We counted the number of times the correct word was not the most similar one.\nWe see that adding the special bigrams for word boundaries improves the results, especially when single characters are not included in the representation. A possible explanation, besides the fact that they tend to be recognized more easily, could be that they provide a very useful information to disambiguate words having a similar bigram representation (e.g. them and theme). Adding single characters also improves the performance of the decoder, especially when the boundary bigrams are not included in the representation. The gain obtained with the single characters is about the same – sometimes a little better – as the gain with boundaries. It might be due to the much better recognition of the RNN for single characters (precision and recall over 90%), as well as the added redundancy and complementary information provided. The results of decoding with different combinations of orders are presented in the appendices in Table 7. They confirm those observations. The best performance is achieved with both single characters and word boundaries, although the gain compared to adding only one of them is slight. The error rates are competitive or better than the best error rates obtained by classical character sequence modeling and Viterbi decoding."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "State-of-the-art systems, as well as most of the systems for handwritten word recognition found in the literature, either try to model words as a whole, or as a sequence of characters. The latter, which currently gives the best results, is widely adopted by the community, and benefits from a lot of attention. In this paper, we have proposed a simple alternative model, inspired by the recent findings in cognitive neurosciences research on reading.\nWe focused on the representation of words in the open-bigram space and built an handwritten word recognition system operating in that space. We were interested in observing how a simple decoding scheme, based on a mere cosine similarity measure in the bigram space, compared to traditional methods. The main apparent difficulty arises from the fact that the global ordering of characters and the distance between bigram constituents are lost in this representation.\nThe qualitative results presented in the first section showed that the envisioned approach was viable. With the letter reordering example, we have seen that the correct orthographic form of words can be retrieved with a limited and local knowledge of character orders. Moreover, we validated that words\nthat are close in orthographic form are also close in the bigram space. Thus, we demonstrated that the open-bigram representation shows interesting and competitive metric properties for the word recognition. Current work consists in learning most discriminant open-bigram at different order, possibly higher than three according to the length of the word and its similarity to others."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was conducted in COGNILEGO project 2012-15, supported by the French Research Agency under the contract ANR 2010-CORD-013 http://cognilego.univ-tln.fr."
    }, {
      "heading" : "A DATA AND MODELS",
      "text" : "A.2 BASELINE SYSTEMS\nWe built several models and used the same vocabulary as for the bigram decoder, and no language model (all words have the same prior probability). The first one is a Hidden Markov Model, with 5 (Rimes) or 6 (IAM) states per characters, and an emission model based on Gaussiam Mixture Models. This system is trained with the Maximum Likelihood criterion, following the usual ExpectationMaximization procedure. At each iteration, the number of Gaussians is increased, until no improvement is observed on the validation set. The forced alignments with the GMM/HMM system are used to build a labeled dataset for training a Multi-Layer Perceptron (MLP) with 4 hidden layers of 1,024 sigmoid units. We optimized the cross-entropy criterion to train the network to predict the HMM states from the concatenation of 11 input frames, with a learning rate of 0.008. The learning rate was halved when the relative improvement was smaller than 1% from one epoch to the next. The MLP is integrated in the hybrid NN/HMM scheme by dividing the predicted state posteriors p(s|x) by the state priors p(s), estimated from the forced alignments. It is then further trained with a sequence-discriminative criterion: state-level Minimum Bayes Risk (Kingsbury, 2009) (sMBR) for 5 epochs. Finally, we also performed the sequential decoding with vocabulary constraints using rnn0. The input features for all systems are the same, described in 4.1. The results of similar\nsystems for handwritten text line recognition can for example be found in a previous work (Bluche et al., 2014), where a comparison is made with state-of-the-art systems.\nA.3 RECURRENT NEURAL NETWORK\nThe RNNs, depicted on Figure 6, have seven hidden layers, alternating Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) recurrent layers in both direction and feed-forward layers. The first LSTM layers have 100 hidden LSTM units. The BLSTM outputs of two consecutive timesteps of both directions are fed to a feed-forward layer with 100 nodes, halving the size of the sequence. The next LSTM layers have 150 units, and are connected to a feed-forward layer with 150 units. The last two LSTM layers and feed-forward layer have 200 hidden units.\nThe inputs of the networks are sequences of geometrical and statistical features (described in (Bianne et al., 2011)), extracted with a sliding window of width 3px. The outputs are sequences of openbigram predictions.\nThe networks are trained with stochastic gradient descent to minimize the Connectionist Temporal Classification (CTC (Graves et al., 2006)) criterion, i.e. the Negative Log-Likelihood (NLL) of the correct label sequence. We set the learning rate to 0.001, and stopped the training when the NLL on the validation set did not decrease for 20 epochs, and kept the network yielding the best NLL on the validation set."
    }, {
      "heading" : "B RESULTS",
      "text" : "B.1 CORRELATION BETWEEN CHARACTER EDIT DISTANCES AND OB COSINE SIMILARITIES\nOn Figure 7, we randomly selected pairs of words, and pairs of words with high cosine similarity in the French and English dictionary, and plotted their cosine similarity in the bigram space against\nthe edit distance between the two words, normalized by the length of the longest word. We note that words with high cosine similarity also have short edit distance, supporting the idea that the bigram representation encode some global letter order, and therefore might favor word recognition.\nB.2 EVALUATION OF PERFORMANCE OF RNN ALONE\nWe trained RNNs to predict sequences of bigrams of a given order. Their performance to accomplish this task can be measured, on the validation set, with the edit distance (edit.dist) between the recognized sequence and the true sequence, and with the percentage of sequences with at least one error: the Sequence Error Rate (SER).\nThe results are reported on Table 6. We observe that the performance decreases as d increase. Yet the errors remain in a reasonable range compared to the simple case d = 0, and one should keep in mind that a small degradation should be expected from the larger number of classes and smaller number of training examples per class.\nWe have shown that RNNs can perform the apparently difficult task of recognizing sequences of open-bigrams. Thus, we may use the RNN bigram predictions in our decoder, rather than building bigram predictions from letter ones, which would have been less satisfying considering the supposed interest of the redundancy of the bigram representation that allows error corrections.\nB.3 EVALUATION OF PERFORMANCE OF THE DECODER\nIn Table 7, we report the results of the cosine decoder applied to the outputs of OB RNNs with different bigram orders.\nIn Table 8, we evaluate the cosine decoder by applying it to the ground-truth OB decomposition with varying order of the validation sets, i.e. the performance assuming the RNN optical models are perfect.\nB.4 ERROR ANALYSIS\nB.4.1 COMPARISON OF ERRORS OF THE SEQUENTIAL AND OPEN-BIGRAM MODELS"
    } ],
    "references" : [ {
      "title" : "Word spotting and recognition with embedded attributes",
      "author" : [ "Jon Almazán", "Albert Gordo", "Alicia Fornés", "Ernest Valveny" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Almazán et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Almazán et al\\.",
      "year" : 2014
    }, {
      "title" : "Preteux. RIMES evaluation campaign for handwritten mail processing",
      "author" : [ "E. Augustin", "M. Carré", "E. Grosicki", "J.-M. Brodin", "E. Geoffrois" ],
      "venue" : "In Proceedings of the Workshop on Frontiers in Handwriting Recognition,",
      "citeRegEx" : "Augustin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Augustin et al\\.",
      "year" : 2006
    }, {
      "title" : "Lerec: A NN/HMM hybrid for on-line handwriting recognition",
      "author" : [ "Yoshua Bengio", "Yann LeCun", "Craig Nohl", "Chris Burges" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1995
    }, {
      "title" : "Dynamic and Contextual Information in HMM modeling for Handwriting Recognition",
      "author" : [ "A.-L. Bianne", "F. Menasri", "R. Al-Hajj", "C. Mokbel", "C. Kermorvant", "L. Likforman-Sulem" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Bianne et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bianne et al\\.",
      "year" : 2011
    }, {
      "title" : "A Comparison of Sequence-Trained Deep Neural Networks and Recurrent Neural Networks Optical Modeling for Handwriting Recognition",
      "author" : [ "Thodore Bluche", "Hermann Ney", "Christopher Kermorvant" ],
      "venue" : "In International Conference on Statistical Language and Speech Processing,",
      "citeRegEx" : "Bluche et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bluche et al\\.",
      "year" : 2014
    }, {
      "title" : "A structural and relational approach to handwritten word recognition",
      "author" : [ "R. Buse", "Z Q Liu", "T. Caelli" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics,",
      "citeRegEx" : "Buse et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Buse et al\\.",
      "year" : 1997
    }, {
      "title" : "The neural code for written words: a proposal",
      "author" : [ "Stanislas Dehaene", "Laurent Cohen", "Mariano Sigman", "Fabien Vinckier" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "Dehaene et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dehaene et al\\.",
      "year" : 2005
    }, {
      "title" : "Auto-organisation des représentations lexicales au cours de l’apprentissage de la lecture: approches comportementale électrophysiologique et neuro-computationnelle",
      "author" : [ "Stéphane Dufau" ],
      "venue" : "PhD thesis, Université de Provence,",
      "citeRegEx" : "Dufau.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dufau.",
      "year" : 2008
    }, {
      "title" : "An adaptive resonance theory account of the implicit learning of orthographic word forms",
      "author" : [ "H Glotin", "P Warnier", "F Dandurand", "S Dufau", "B Lété", "C Touzet", "JC Ziegler", "J Grainger" ],
      "venue" : "Journal of Physiology-Paris,",
      "citeRegEx" : "Glotin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glotin et al\\.",
      "year" : 2010
    }, {
      "title" : "The overlap model: a model of letter position coding",
      "author" : [ "Pablo Gomez", "Roger Ratcliff", "Manuel Perea" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Gomez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gomez et al\\.",
      "year" : 2008
    }, {
      "title" : "Modeling letter position coding in printed word perception",
      "author" : [ "Jonathan Grainger", "W Van Heuven" ],
      "venue" : "The mental lexicon,",
      "citeRegEx" : "Grainger and Heuven.,? \\Q2003\\E",
      "shortCiteRegEx" : "Grainger and Heuven.",
      "year" : 2003
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "A Graves", "S Fernández", "F Gomez", "J Schmidhuber" ],
      "venue" : "In International Conference on Machine learning,",
      "citeRegEx" : "Graves et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks",
      "author" : [ "Alex Graves", "Juergen Schmidhuber" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Graves and Schmidhuber.,? \\Q2009\\E",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2009
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Deep structured output learning for unconstrained text recognition",
      "author" : [ "Max Jaderberg", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman" ],
      "venue" : "CoRR, abs/1412.5903,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Sophisticated topology of hidden Markov models for cursive script recognition",
      "author" : [ "Alfred Kaltenmeier", "Torsten Caesar", "Joachim M Gloger", "Eberhard Mandler" ],
      "venue" : "In Document Analysis and Recognition,",
      "citeRegEx" : "Kaltenmeier et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kaltenmeier et al\\.",
      "year" : 1993
    }, {
      "title" : "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling",
      "author" : [ "Brian Kingsbury" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP",
      "citeRegEx" : "Kingsbury.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kingsbury.",
      "year" : 2009
    }, {
      "title" : "Hidden Markov model based word recognition and its application to legal amount reading on French checks",
      "author" : [ "Stefan Knerr", "Emmanuel Augustin", "Olivier Baret", "David Price" ],
      "venue" : "Computer Vision and Image Understanding,",
      "citeRegEx" : "Knerr et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Knerr et al\\.",
      "year" : 1998
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Holistic lexicon reduction for handwritten word recognition",
      "author" : [ "Sriganesh Madhvanath", "Venu Govindaraju" ],
      "venue" : "In Electronic Imaging: Science & Technology, pp. 224–234. International Society for Optics and Photonics,",
      "citeRegEx" : "Madhvanath and Govindaraju.,? \\Q1996\\E",
      "shortCiteRegEx" : "Madhvanath and Govindaraju.",
      "year" : 1996
    }, {
      "title" : "The IAM-database: an English sentence database for offline handwriting recognition",
      "author" : [ "U.-V. Marti", "H. Bunke" ],
      "venue" : "International Journal on Document Analysis and Recognition,",
      "citeRegEx" : "Marti and Bunke.,? \\Q2002\\E",
      "shortCiteRegEx" : "Marti and Bunke.",
      "year" : 2002
    }, {
      "title" : "Global word shape processing in off-line recognition of handwriting",
      "author" : [ "Christophe Parisse" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Parisse.,? \\Q1996\\E",
      "shortCiteRegEx" : "Parisse.",
      "year" : 1996
    }, {
      "title" : "Cnn-n-gram for handwriting word recognition",
      "author" : [ "Arik Poznanski", "Lior Wolf" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Poznanski and Wolf.,? \\Q2016\\E",
      "shortCiteRegEx" : "Poznanski and Wolf.",
      "year" : 2016
    }, {
      "title" : "The Theory of neural Cognition applied to Robotics",
      "author" : [ "C Touzet" ],
      "venue" : "International Journal of Advanced Robotic Systems,",
      "citeRegEx" : "Touzet.,? \\Q2015\\E",
      "shortCiteRegEx" : "Touzet.",
      "year" : 2015
    }, {
      "title" : "A Biologically Plausible SOM Representation of the Orthographic Form of 50000 French Words. In Advances in Self-Organizing Maps and Learning Vector Quantization",
      "author" : [ "Claude Touzet", "Christopher Kermorvant", "Hervé Glotin" ],
      "venue" : null,
      "citeRegEx" : "Touzet et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Touzet et al\\.",
      "year" : 2014
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten and Hinton.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "On coding the position of letters in words: a test of two models",
      "author" : [ "Carol Whitney", "Daisy Bertrand", "Jonathan Grainger" ],
      "venue" : "Experimental psychology,",
      "citeRegEx" : "Whitney et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Whitney et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "have proposed a biologically plausible model of the cortical organization of reading (Dehaene et al., 2005) that assumes seven successive steps of increasing complexity, from the retinal ganglion cells to a cortical map of the orthographic word forms (Figure 1).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "One of the most recent successes of experimental psychology was the demonstration that human visual word recognition uses an explicit representation of letter position order based on letter pairs: the open-bigram coding (Whitney et al., 2012; Gomez et al., 2008; Grainger & Van Heuven, 2003; Glotin et al., 2010; Dufau, 2008).",
      "startOffset" : 220,
      "endOffset" : 325
    }, {
      "referenceID" : 9,
      "context" : "One of the most recent successes of experimental psychology was the demonstration that human visual word recognition uses an explicit representation of letter position order based on letter pairs: the open-bigram coding (Whitney et al., 2012; Gomez et al., 2008; Grainger & Van Heuven, 2003; Glotin et al., 2010; Dufau, 2008).",
      "startOffset" : 220,
      "endOffset" : 325
    }, {
      "referenceID" : 8,
      "context" : "One of the most recent successes of experimental psychology was the demonstration that human visual word recognition uses an explicit representation of letter position order based on letter pairs: the open-bigram coding (Whitney et al., 2012; Gomez et al., 2008; Grainger & Van Heuven, 2003; Glotin et al., 2010; Dufau, 2008).",
      "startOffset" : 220,
      "endOffset" : 325
    }, {
      "referenceID" : 7,
      "context" : "One of the most recent successes of experimental psychology was the demonstration that human visual word recognition uses an explicit representation of letter position order based on letter pairs: the open-bigram coding (Whitney et al., 2012; Gomez et al., 2008; Grainger & Van Heuven, 2003; Glotin et al., 2010; Dufau, 2008).",
      "startOffset" : 220,
      "endOffset" : 325
    }, {
      "referenceID" : 24,
      "context" : "As demonstrated in (Touzet et al., 2014), open-bigrams (OB) allow an over-coding of the orthographic form of words that facilitates recognition.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "Additional information helps the organization of levels 4 and 5, when using a phonics method, but not a whole language method (today banned from reading teaching for lack of efficiency, adapted from (Dehaene et al., 2005) and (Touzet, 2015).",
      "startOffset" : 199,
      "endOffset" : 221
    }, {
      "referenceID" : 23,
      "context" : ", 2005) and (Touzet, 2015).",
      "startOffset" : 12,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "Since cognitive psychology has demonstrated the existence of the OB layer, the hypothesis has been put forward (Touzet et al., 2014) that the orthographic representation of words may have evolved in order to take into account the topology of the OB space, instead of the topology of the single letter space.",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "in (Parisse, 1996; Madhvanath & Govindaraju, 1996)).",
      "startOffset" : 3,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "in (Bengio et al., 1995)) or with hidden Markov models (HMMs, e.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "in (Knerr et al., 1998)).",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "in (Kaltenmeier et al., 1993)), or to feed the image to a neural network able to output sequences of character predictions (e.",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "SDNNs (LeCun et al., 1998) or MDLSTM-RNN (Graves & Schmidhuber, 2009)).",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "in (Bengio et al., 1995)) or with hidden Markov models (HMMs, e.g. in (Knerr et al., 1998)). The last method, most popular nowadays, is a segmentation-free approach. The goal is to predict a character sequence from the image without segmenting it first. The techniques include scanning a sliding window to extract features used in an HMM (e.g. in (Kaltenmeier et al., 1993)), or to feed the image to a neural network able to output sequences of character predictions (e.g. SDNNs (LeCun et al., 1998) or MDLSTM-RNN (Graves & Schmidhuber, 2009)). More recently, different approaches have been proposed to recognize words using character bigrams, and therefore closer to the method we propose in this paper. Jaderberg et al. (2014) propose",
      "startOffset" : 4,
      "endOffset" : 729
    }, {
      "referenceID" : 0,
      "context" : "Almazán et al. (2014) use Fisher vectors from images and pyramidal character histograms, to learn a feature space shared by the word images and labels, for word spotting, also using canonical correlation analysis.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 24,
      "context" : "3 (Touzet et al., 2014) (left), vs after t-SNE (Van der Maaten & Hinton, 2008) (right).",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "We carried out the experiments on two public handwritten word databases: Rimes (Augustin et al., 2006) (French), and IAM (Marti & Bunke, 2002) (English).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "We applied deslanting (Buse et al., 1997), contrast enhancement, and padded the images with 10px of white pixels to account for empty context on the left and right of words.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "The features are geometrical and statistical features described in (Bianne et al., 2011), which give state-of-the-art results in handwritten text line recognition (Bluche et al.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : ", 2011), which give state-of-the-art results in handwritten text line recognition (Bluche et al., 2014).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "We trained one RNN for each order-d bigram, with the Connectionist Temporal Classification (CTC (Graves et al., 2006)) criterion.",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "However, these standard models yield state-of-the art results on the reference task for the two considered database (Bluche et al., 2014).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 4,
      "context" : "They are almost identical to those presented in a previous work (Bluche et al., 2014), where a comparison is made with state-of-the-art systems for handwritten text line recognition.",
      "startOffset" : 64,
      "endOffset" : 85
    } ],
    "year" : 2017,
    "abstractText" : "Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing openbigrams, i.e. couple of letters that are not necessarily next to each other. In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction. For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this representation and the bagged RNN prediction to retrieve the vocabulary word. We compare this method to standard word recognition techniques based on sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable to more conventional decoding methods based on sequences of letters.",
    "creator" : "LaTeX with hyperref package"
  }
}