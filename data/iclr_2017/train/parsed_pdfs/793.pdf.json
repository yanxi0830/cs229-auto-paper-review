{
  "name" : "793.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Kamil Rocki" ],
    "emails" : [ "kmrocki@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Based on human performance on the same task, it is believed that an important ingredient which is missing in state-of-the-art variants of recurrent networks is top-down feedback. Despite evidence of its existence, it is not entirely clear how mammalian brain might implement such a mechanism. It is important to understand what kind of top-down interaction contributes to improved prediction capability in order to tackle more challenging AI problems requiring interpretation of deeper contextual information. Furthermore, it might provide clues as what makes human cognitive abilities so unique. Existing approaches which consider top-down feedback in neural networks are primarily focused on stacked layers of neurons, where higher-level representations constitute a top-down signal source. In this paper, we propose that the discrepancy between most recent predictions and observations might be effectively used as a feedback signal affecting further predictions. It is very common to use such a discrepancy during learning phase as the error which is subject to minimization, but not during inference. We show that is also possible to use such top-down signal without losing generality of the algorithm and that it improves generalization capabilities when applied to Long-Short Term Memory (Hochreiter & Schmidhuber, 1997) architecture. It is important to point out that the feedback idea presented here applies only to temporal data."
    }, {
      "heading" : "1.1 SUMMARY OF CONTRIBUTIONS",
      "text" : "The main contributions of this work are:\n• the introduction of a novel way of incorporating most recent misprediction measure as an additional input signal\n• extending state-of-the-art performance on character-level text modeling using Hutter Wikipedia dataset."
    }, {
      "heading" : "1.2 RELATED WORK",
      "text" : "There exist other approaches which attempted to introduce top-down input for improving predictions. One such architecture is Gated-Feedback RNN (Chung et al., 2015). An important difference between architecture proposed here and theirs is the source of the feedback signal. In GF-RNN it is assumed that there exist higher level representation layers and they constitute the feedback source.\nOn the other hand, here, feedback depends directly on the discrepancy between past predictions and current observation and operates even within a single layer. Another related concept is Ladder Networks (Rasmus et al., 2015), where top-down connections contribute to improved semi-supervised learning performance."
    }, {
      "heading" : "2 FEEDBACK: MISPREDICTION-DRIVEN PREDICTION",
      "text" : ""
    }, {
      "heading" : "2.1 NOTATION",
      "text" : "The following notation is used throughout the section:\nx - inputs h - hidden units y - outputs p - output probabilities (normalized y) s - surprisal t - time step W - feedforward x→ h connection matrix U - recurrent h→ h connection matrix V - feedback s→ h connection matrix S - truncated BPTT length M - number of inputs N - number of hidden units\n· denotes matrix multiplication denotes elementwise multiplication σ(·), tanh(·) - elementwise nonlinearities δx = ∂E∂x\nIn case of LSTM, the following concatenated representations are used:\ngt =  itftot ut  b = b i bf bo bu U = U i Uf Uo Uu W = W i Wf Wo Wu V = V i Vf Vo Vu  (1)"
    }, {
      "heading" : "2.2 SIMPLE RNN WITHOUT FEEDBACK",
      "text" : "First, we show a simple recurrent neural network architecture without feedback which serves as a basis for demonstrating our approach. It is illustrated in Fig. 2 and formulated as follows:\nht = tanh(W · xt + U · ht−1 + b) (2)"
    }, {
      "heading" : "2.3 FEEDBACK AUGMENTED RECURRENT NETWORKS",
      "text" : "Figure 3 presents the main idea of surprisal-driven feedback in recurrent networks. In addition to feedforward and recurrent connections W and U , we added one additional matrix V . One more input signal, namely V · st is being considered when updating hidden states of the network. We propose that the discrepancy st between most recent predictions pt−1 and observations xt might be effectively used as a feedback signal affecting further predictions. Such information is usually used during learning phase as an error signal, but not during inference. Our hypothesis is that it represents an important source of information which can be used during the inference phase, should be used and that it bring benefits in the form of improved generalization capability. Figure 1 presents examples of feedback signal being considered. Intuitively, when surprisal is near zero, the sum of input signals is the same as in a typical RNN. Next subsections provide mathematical description of the feedback architecture in terms of forward and backward passes for the Back Propagation Through Time (BPTT) (Werbos, 1990) algorithm."
    }, {
      "heading" : "2.4 FORWARD PASS",
      "text" : "Set h0, c0 to zero and p0 to uniform distribution or carry over the last state to emulate full BPTT.\n∀i, pi0 = 1\nM , i ∈ {0, 1, ..,M − 1}, t = 0 (3)\nfor t = 1:1:S-1\nI. Surprisal part\nst = − i∑ log pit−1 xit (4)\nIIa. Computing hidden activities, Simple RNN\nht = tanh(W · xt + U · ht−1 + V · st + b) (5)\nIIb. Computing hidden activities, LSTM (to be used instead of IIa)\nft = σ(Wf · xt + Uf · ht−1 + Vf · st + bf ) (6)\nit = σ(W i · xt + U i · ht−1 + Vi · st + bi) (7)\not = σ(Wo · xt + Uo · ht−1 + Vo · st + bo) (8)\nut = tanh(Wu · xt + Uu · ht−1 + Vu · st + bu) (9)\nct = (1− ft) ct−1 + it ut (10)\nĉt = tanh(ct) (11)\nht = ot ĉt (12)\nIII. Outputs\nyit =Wy · ht + by (13)\nSoftmax normalization\npit = ey i t∑i ey i t\n(14)"
    }, {
      "heading" : "2.5 BACKWARD PASS",
      "text" : "for t = S-1:-1:1\nI. Backprop through predictions\nBackprop through softmax, cross-entropy error, accumulate\n∂Et ∂yt = ∂Et ∂yt + pt−1 − xt (15)\nδy → δWy, δby ∂E\n∂Wy =\n∂E\n∂Wy + hTt · ∂Et ∂yt\n(16)\n∂E ∂by = ∂E ∂by + M∑ i=1 ∂Eit ∂yit\n(17)\nδy → δh ∂Et ∂ht = ∂Et ∂ht + ∂Et ∂yt ·WTy (18)\nIIa. Backprop through hidden nonlinearity (simple RNN version)\n∂Et ∂ht = ∂Et ∂ht + ∂Et ∂ht tanh′(ht) (19)\n∂Et ∂gt = ∂Et ∂ht\n(20)\nIIb. Backprop through c, h, g (LSTM version)\nBackprop through memory cells, (keep gradients from the previous iteration)\n∂Et ∂ct = ∂Et ∂ct + ∂Et ∂ht ot tanh′(ĉt) (21)\nCarry error over to ∂Et∂ct−1\n∂Et ∂ct−1 = ∂Et ∂ct−1 + ∂Et ∂ct (1− ft) (22)\nPropagate error through the gates\n∂Et ∂ot = ∂Et ∂ht ĉt σ′(ot) (23)\n∂Et ∂it = ∂Et ∂ct ut σ′(it) (24)\n∂Et ∂ft = −∂Et ∂ct ct−1 σ′(ft) (25)\n∂Et ∂ut = ∂Et ∂ct it tanh′(ut) (26)\nCarry error over to ∂Et∂ht−1 ∂Et ∂ht−1 = ∂Et ∂gt · UT (27)\nIII. Backprop through linearities\n∂Et ∂b = ∂Et ∂b + N∑ i=1 ∂Et ∂git\n(28)\n∂E ∂U = ∂E ∂U + hTt−1 · ∂Et ∂gt\n(29)\n∂E ∂W = ∂E ∂W + xTt · ∂Et ∂gt\n(30)\n∂E ∂x = ∂E ∂x + ∂Et ∂gt ·WT (31)\nIV. Surprisal part\n∂E ∂V = ∂E ∂V + sTt · ∂Et ∂gt\n(32)\n∂E ∂st = ∂E ∂gt · V T (33)\n∂Et ∂pt−1 = ∂Et ∂st xt (34)\nAdjust ∂Et∂pt−1 according to the sum of gradients and carry over to ∂Et ∂yt−1\n∂Et ∂yt−1 = ∂Et ∂pt−1 − pt−1 M∑ i=1 ∂Et ∂pit−1\n(35)"
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "We ran experiments on the enwik8 dataset. It constitutes first 108 bytes of English Wikipedia dump (with all extra symbols present in XML), also known as Hutter Prize challenge dataset2. First 90% of each corpus was used for training, the next 5% for validation and the last 5% for reporting test accuracy. In each iteration sequences of length 10000 were randomly selected. The learning algorithm used was Adagrad1 with a learning rate of 0.001. Weights were initialized using so-called Xavier initialization Glorot & Bengio (2010). Sequence length for BPTT was 100 and batch size 128, states were carried over for the entire sequence of 10000 emulating full BPTT. Forget bias was set initially to 1. Other parameters set to zero. The algorithm was written in C++ and CUDA 8 and ran on GTX Titan GPU for up to 10 days. Table 1 presents results comparing existing state-of-theart approaches to the introduced Feedback LSTM algorithm which outperforms all other methods despite not having any regularizer."
    }, {
      "heading" : "4 SUMMARY",
      "text" : "We introduced feedback recurrent network architecture, which takes advantage of temporal nature of the data and monitors the discrepancy between predictions and observations. This prediction error\n1with a modification taking into consideration only recent window of gradient updates 2http://mattmahoney.net/dc/text.html 3This method does not belong to the ’dynamic evaluation’ group: 1. It never actually sees test data during\ntraining. 2. It does not adapt weights during testing 4our implementation\ninformation, also known as surprisal, is used when making new guesses. We showed that combining commonly used feedforward, recurrent and such feedback signals improves generalization capabilities of Long-Short Term Memory network. It outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction achieving 1.37 BPC."
    }, {
      "heading" : "5 FURTHER WORK",
      "text" : "It is still an open question what the feedback should really constitute as well as how it should interact with lower-level neurons (additive, multiplicative or another type of connection). Further improvements may be possible with the addition of regularization. Another research direction is incorporating sparsity in order improve disentangling sources of variation in temporal data."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "This work has been supported in part by the Defense Advanced Research Projects Agency (DARPA)."
    } ],
    "references" : [ {
      "title" : "Gated feedback recurrent neural networks",
      "author" : [ "Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1502.02367,",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves" ],
      "venue" : "CoRR, abs/1507.01526,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2015
    }, {
      "title" : "Zoneout: Regularizing rnns by randomly preserving hidden activations",
      "author" : [ "David Krueger", "Tegan Maharaj", "János Kramár", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron C. Courville", "Chris Pal" ],
      "venue" : "CoRR, abs/1606.01305,",
      "citeRegEx" : "Krueger et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2016
    }, {
      "title" : "Semisupervised learning with ladder",
      "author" : [ "Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko" ],
      "venue" : "network. CoRR,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent memory array structures",
      "author" : [ "Kamil Rocki" ],
      "venue" : "arXiv preprint arXiv:1607.03085,",
      "citeRegEx" : "Rocki.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rocki.",
      "year" : 2016
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey Hinton" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Backpropagation through time: what does it do and how to do it",
      "author" : [ "P. Werbos" ],
      "venue" : "In Proceedings of IEEE,",
      "citeRegEx" : "Werbos.,? \\Q1990\\E",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "On multiplicative integration with recurrent neural networks",
      "author" : [ "Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov" ],
      "venue" : "CoRR, abs/1606.06630,",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent highway networks, 2016",
      "author" : [ "Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutnk", "Jrgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "Zilly et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zilly et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One such architecture is Gated-Feedback RNN (Chung et al., 2015).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Another related concept is Ladder Networks (Rasmus et al., 2015), where top-down connections contribute to improved semi-supervised learning performance.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "Next subsections provide mathematical description of the feedback architecture in terms of forward and backward passes for the Back Propagation Through Time (BPTT) (Werbos, 1990) algorithm.",
      "startOffset" : 164,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "BPC mRNN(Sutskever et al., 2011) 1.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "60 GF-RNN (Chung et al., 2015) 1.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "58 Grid LSTM (Kalchbrenner et al., 2015) 1.",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "45 MI-LSTM (Wu et al., 2016) 1.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "44 Recurrent Highway Networks (Zilly et al., 2016) 1.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "42 Array LSTM (Rocki, 2016) 1.",
      "startOffset" : 14,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "38 Feedback LSTM + Zoneout (Krueger et al., 2016) 1.",
      "startOffset" : 27,
      "endOffset" : 49
    } ],
    "year" : 2016,
    "abstractText" : "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.",
    "creator" : "LaTeX with hyperref package"
  }
}