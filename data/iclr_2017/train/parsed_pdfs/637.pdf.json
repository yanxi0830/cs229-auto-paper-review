{
  "name" : "637.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "FEATURE SPACE", "Stefano Teso", "Andrea Passerini" ],
    "emails" : [ "passerini}@disi.unitn.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Relational embeddings have emerged as an excellent tool for inferring novel facts from partially observed knowledge bases. Recently, it was shown that some classes of embeddings can also be exploited to perform a simplified form of rule mining. By interpreting logical conjunction as a form of composition between relation embeddings, simplified logical theories can be mined directly in the space of latent representations. In this paper, we present a method to mine full-fledged logical theories, which are significantly more expressive, by casting the semantics of the logical operators to the space of the embeddings. In order to extract relevant rules in the space of relation compositions we borrow sparse reconstruction procedures from the field of compressed sensing. Our empirical analysis showcases the advantages of our approach."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Knowledge Bases (KB) capture relational knowledge about a domain of choice by modelling entities and facts relating them. In so doing, KBs allow for rich answers to user queries, as happens with the knowledge panels powered by the Google Knowledge Graph. Furthermore, KBs can be mined for rules, i.e. patterns of relations which are frequently found to hold in the KB. Mining theories from data is the task of Rule Mining (Dzeroski & Lavrac, 2000) and Inductive Logic Programming (Dzeroski & Lavrac, 1994; Muggleton et al., 1992).\nClassical ILP methods mine theories by searching over the (exponentially large) space of logical theories, resorting to language biases and heuristics to simplify the learning problem. While powerful, pure ILP methods do not scale to large relational datasets, preventing them from mining Web-scale KBs such as YAGO (Hoffart et al., 2013) and DBpedia (Auer et al., 2007). Further, purely logical methods can not gracefully deal with noise. Next-generation miners that specialize on large KBs, such as AMIE (Galárraga et al., 2015), work around these issues by trading off theory expressiveness for runtime efficiency.\nA general strategy for processing huge datasets is dimensionality reduction: instead of working on the original KB directly, one first squeezes it to a summary of manageable size, and then performs the required operations on the summary itself. Common summarization techniques for relational data include relational factorization (Nickel et al., 2011; London et al., 2013; Riedel et al., 2013) and representation learning (Bordes et al., 2011; Socher et al., 2013). The core idea is to learn compressed latent representations, or embeddings, of entities and relations able to reconstruct the original KB by minimizing a suitable reconstruction loss. Until recently, relational embeddings have been mostly employed for link prediction and knowledge base completion (Nickel et al., 2016).\nHowever, Yang et al. (2015) have shown that low-dimensional representations can also be exploited to perform a simplified form of theory learning. Their paper shows that, under reasonable assumptions, a simple nearest neighbor algorithm can recover logical rules directly from the fixed size embeddings of a KB, with potential runtime benefits. Furthermore, since the embeddings generalize beyond the observed facts, the rules are implicitly mined over a completion of the KB. Despite the novelty of their insight, their proposed method has several major downsides. First, their simple approach is limited to extracting rules as conjunctions of relations, with no support for logical dis-\njunction and negation. Second, the rules are mined independently of one another, which can lead to redundant theories and compromise generalization ability and interpretability.\nBuilding on the insights of Yang et al. (2015), we propose a novel approach to theory learning from low-dimensional representations. We view theory learning as a special sparse recovery problem. In this setting, a logical theory is merely an algebraic combination of embedded relations that best reconstructs the original KB, in a sense that will be made clear later. The recovery problem can be solved with specialized compressed sensing algorithms, such as Orthogonal Matching Pursuit (Pati et al., 1993) or variants thereof. Our approach offers two key advantages: it automatically models the inter-dependency between different rules, discouraging redundancy in the learned theory, and it supports all propositional logic connectives, i.e. conjunction, disjunction, and negation. Our empirical analysis indicates that our method can mine satisfactory theories in realistic KBs, demonstrating its ability to discover diverse and interpretable sets of rules. Additionally, our method can in principle be applied to “deeper” embeddings, that is, embeddings produced by deep models that take into consideration both relational and feature-level aspects of the data.\nThe paper is structured as follows. In the next section we introduce the required background material. We proceed by detailing our approach in Section 3 and evaluating it empirically in Section 4. We discuss relevant related work in Section 5, and conclude with some final remarks in Section 6."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "In this section we briefly overview the required background. Let us start with the notation we will use. We write column vectors x in bold-face, matricesX in upper-case, and third-order tensors X in calligraphic upper-case. Xk is the kth frontal slice of the tensor X , and vec(X) is the vectorization (flattening) of X . We denote the usual Frobenius matrix norm as ‖X‖F := √∑ ij x 2 ij , the number of non-zero entries as ‖X‖0, the set {1, . . . , n} as [n] and the Cartesian product of ` sets {1, . . . , n} as [n]`. We reserve typewriter fonts for logical entities Ann and relations motherOf.\nKnowledge Bases and Theories. A knowledge base (KB) is a collection of known true facts about a domain of interest. As an example, a KB about kinship relations may include facts such as (Ann,motherOf,Bob), which states that Ann is known to be the mother of Bob. In the following we will use n and m to denote the number of distinct entities and relations in the KB, respectively. With a slight abuse of notation, we will refer to logical constants and relations (e.g. Ann and motherOf) by their index in the KB (e ∈ [n] or r ∈ [m], respectively). Triples not occurring in the KB are unobserved, i.e. neither true nor false.\nGiven an input KB, the goal of theory learning, also known as Inductive Logic Programming (Muggleton et al., 1992), is to induce a compact logical theory that both explains the observed facts and generalizes to the unobserved ones. Most ILP methods extract theories in definite clausal form, which offers a good compromise between expressiveness and efficiency. A theory in this form is an implicitly conjoined set of Horn rules, i.e. rules like:\n∀ e, e′ ∈ [n] (e,uncleOf, e′)⇐ ∃ e′′ ∈ [n] (e,brotherOf, e′′) ∧ (e′′,parentOf, e′)\nHere ⇐ represents logical entailment. The left-hand side is called the head of the rule, while the right-hand side is the body. The semantics of Horn rules are simple: whenever the body is satisfied by a given set of entities and relations, so is the head. The length of a rule is the number of relations appearing in its body; the above is a length 2 rule.\nClassical ILP approaches cast theory learning as a search problem over the (exponentially large) space of candidate theories. When there are no negative facts, as in our case, the quality of a theory is given by the number of true facts it entails. In practice, learning is regularized by the size of the theory (number and length of the rules) to encourage compression, generalization and interpretability. Due to the combinatorial nature of the problem, the search task is solved heuristically, e.g. by searching individual Horn rules either independently or sequentially, or by optimizing surrogate objective functions. A language bias, provided by a domain expert, is often employed to guide the search toward more promising theories. Please see (Dzeroski & Lavrac, 1994; Muggleton et al., 1992) for more details.\nRelational embeddings. Relational embedding techniques learn a low-dimensional latent representation of a KB. In order to ground the discussion, we focus on a prototypical factorization method, RESCAL (Nickel et al., 2011; 2012); many alternative formulations can be seen as variations or generalizations thereof. We stress, however, that our method can be applied to other kinds of relational embeddings, as sketched in Section 6. For a general treatment of the subject, see Nickel et al. (2016).\nIn RESCAL, each entity e ∈ [n] in the KB is mapped to a vector xe ∈ Rd, and each binary relation r ∈ [m] to a matrix W r ∈ Rd×d. These parameters are learned from data. Here d ∈ [n] is a user-specified constant (the rank) controlling the amount of compression. The key idea is to model the plausibility, or score, of each fact as a function of its embedding. In particular, in RESCAL the score of a fact (e, r, e′) is given by the bilinear product:\nscore(e, r, e′) := (xe)>W rxe ′ = d∑ i=1 d∑ j=1 xeiW r ijx e′ j\nThe bilinear product measures how similar xe andW rxe ′\nare: the higher the dot product, the higher the score.\nThe embeddings can be expressed compactly in tensor form by grouping the entity vectors sideby-side into a matrix X ∈ Rd×n, and stacking the relation matrices into a tensor W ∈ Rd×d×m. The embeddings (X,W) are learned so as to reconstruct the original KB as accurately as possible, modulo regularization. More formally, let Y ∈ {0, 1}n×n×m be a tensor such that Y ree′ evaluates to 1 if the fact (e, r, e′) appears in the KB, and to 0 otherwise. The learned embeddings should satisfy Y ree′ ≈ score(e, r, e′) for all possible triples (e, r, e′). Learning equates to solving the optimization problem:\nmin W,X m∑ r=1 ‖Y r −X>W rX‖2F + λ ( ‖X‖2F + m∑ r=1 ‖W r‖2F ) (1)\nThe second summand is a quadratic regularization term, whose impact is modulated by the λ > 0 hyperparameter. Note that the entity embeddings X are shared between relations. Choosing d n forces RESCAL to learn more compressed latent features, that hopefully better generalize over distinct facts, at the cost of a potentially larger reconstruction error. While the optimization problem is non-convex and can not be solved exactly in general, RESCAL pairs clever initialization with an alternating least squares procedure to obtain good quality solutions (Nickel et al., 2011).\nIn the next section we will see how theory learning can be generalized to work directly on the embeddings produced by RESCAL and analogous models."
    }, {
      "heading" : "3 RULE MINING IN FEATURE SPACE",
      "text" : "In this section we detail our take on rule mining. Given a knowledge base in tensor form Y , our goal is to learn a theory T that (1) entails many of the observed facts and few of the unobserved ones, and (2) is composed of few, diverse rules, for improved generalization.\nThe theory T includes rules for all possible relations h ∈ [m], where the relation is the head of the rule and the body is an “explanation” of the relation as a (logical) combination of relations. Let Th be the set of rules for head h. In our setting, Th is a conjunction of Horn rules, where each rule is at most ` long 1; ` is provided by the user. Following Yang et al. (2015), we require the rules to be closed paths, i.e. to be in the following form:\n(e1, h, e`+1)⇐ (e1, b1, e2) ∧ (e2, b2, e3) ∧ . . . ∧ (e`, b`, e`+1) (2)\nHere h is the head relation, and b1, . . . , bl are the body relations; quantifiers have been left implicit. Formally, a Horn rule is a closed path if (i) consecutive relations share the middle argument, and (ii) the left argument of the head appears as the first argument of the body (and conversely for the right argument). This special form enables us to cast theory learning in terms of Boolean matrix operations, as follows.\n1For the sake of exposition, in the following we only consider rules exactly ` long; as a matter of fact, the miners we consider can return rules of length ` or shorter.\nLet Y be a knowledge base and h ∈ [m] the target head relation. Note that the conjunction of Horn rules with the same head relation h amounts to the disjunction of their bodies. Due to requirement (1), the set of rules targeting h should approximate the truth values of h, i.e.\nY h ≈ ∨ B∈Th ∧ b∈B Y b\nHere B is the body of a rule, and the logical connectives operate element-wise. In order to learn T from Y , we define a loss function that encourages the above condition. We define the loss ∆(Y h, Th) as the accuracy of reconstruction of Y h w.r.t. Th, written as:\n∆(Y h, Th) := ∥∥Y h ⊕∨B∈Th ∧b∈B Y b∥∥0 (3)\nwhere ⊕ is the element-wise exclusive OR operator and ‖ · ‖0 computes the misclassification error of Th over Y h. Minimizing Eq. (3) unfortunately is a hard combinatorial problem. We will next show how to approximate the latter as a continuous sparse reconstruction problem.\nThe relaxed reconstruction problem. Our goal is to approximate Eq. (3) in terms of algebraic matrix operations over the relation embeddings W . First, we replace conjunctions with products between the embeddings of the relations along the path in the body of the rule, i.e.∧\nb∈B Y b ≈ X> (∏ b∈BW b ) X\nThe idea is that a linear operator W b maps the embedding of the left argument of relation b to vectors similar to the embedding of the right one, as per Eq. 1. For instance, W motherOf will map the embedding of Ann to a vector with high dot product w.r.t. the embedding of Bob. The closed path represented by the conjunction of the relations in the body B is emulated by composition of embeddings and obtained by repeated applications of this mapping (Yang et al., 2015).\nSecond, we replace disjunctions with sums: Y h ≈ X>WhX ≈ X> [∑ B∈Th ∏ b∈BW b ] X\nIntuitively, each path should represent an alternative explanation for the head relation, so that two entities are in relation h if at least one path (approximately) maps the left entity to the right one. Diversity between these alternatives will be enforced by imposing orthogonality between the mappings of the corresponding paths during the mining procedure, as explained later on in the section.\nClearly, the set of rules Th is unknown and needs to be learned in solving the reconstruction problem. We thus let the summation run over all possible paths of length `, i.e. [m]`, adding a coefficient αB for each candidate path. The problem boils down to learning these alphas:\nminα ∥∥∥X>WhX −∑B∈[m]` αBX>∏b∈BW bX∥∥∥ F\n(4)\nIn principle, the coefficients αB should be zero-one; however, we relax them to be real-valued to obtain a tractable optimization problem. This choice has another beneficial side effect: the relaxed formulation gives us a straightforward way to introduce negations in formulas, thus augmenting the expressiveness of our approach beyond purely Horn clauses. The idea builds on the concept of set difference from set theory. A relation like brotherOf can be explained by the rule “a sibling who is not a sister”. This could be represented in the space of the embeddings as the difference between the siblingOf mapping (accounting for both brothers and sisters) and the sisterOf one. More specifically, siblingOf ∧¬ sisterOf would be encoded as W siblingOf − W sisterOf. We thus allow α to also take negative values, with the interpretation that negative bodies are negated and conjoint (rather than disjoint) with the rest of the formula.\nThe last step is to get rid of the instances X , and mine rules for head h only in terms of their ability to reconstruct its embedding Wh. This is justified by the observation (Yang et al., 2015; Gu et al., 2015; Neelakantan et al., 2015; Garcı́a-Durán et al., 2015) that the embeddings are learned so that their composition is close to that of the embedding of the head.\nPutting everything together, we obtain an optimization problem of the form:\nminα ∥∥∥Wh −∑B∈[m]` αB∏b∈BW b∥∥∥ F\n(5)\nfor each target head h. Upon finding the coefficients α, we convert them into a logic theory based on their sign and magnitude. First, only bodies with absolute coefficients larger than a threshold τ > 0 are retained. Each body is then converted to the conjuction of the relations it contains. Bodies with positive coefficients are disjunctively combined with the rest of the formula, while bodies with negative coefficients are added as conjunctions of their negations. The final theory for the mined rule can be written as:\nY h ≈ (∨\nB:αB>τ ∧ b∈B Y b ) ∧ ¬ (∨ B:αB<−τ ∧ b∈B Y b )\n(6)\nSolving the reconstruction problem. Equation 5 is a matrix recovery problem in Frobenius norm. Instead of solving it directly, we leverage the norm equivalence ‖A−B‖F = ‖vec(A)− vec(B)‖2 to reinterpret it as a simpler vector recovery problem. Most importantly, since most of the candidate paths B can not explain the head h, the recovery problem is typically sparse. Sparse recovery problems are a main subject of study in compressed sensing (Candès et al., 2006), and a multitude of algorithms can be employed to solve them, including Orthogonal Matching Pursuit (OMP) (Pati et al., 1993), Basis Pursuit (Chen et al., 1998), and many recent alternatives. In Appendix A we show how minimizing the sparse recovery problem in Eq. 5 equates to minimizing an upper bound of the total loss.\nTwo features of the above problem stand out. First, if the target theory is sparse enough, existing recovery algorithms can solve the reconstruction to global optimality with high probability (Candès et al., 2006). We do not explicitly leverage this perk; we leave finding conditions guaranteeing perfect theory recovery to future work. Second and most importantly, reconstruction algorithms choose the non-zero coefficients αB so that the corresponding path embeddings ∏ b∈BW\nb are mutually orthogonal. This means that similar paths will not be mined together, thus encouraging rule diversity, as per requirement (2)."
    }, {
      "heading" : "4 EMPIRICAL EVALUATION",
      "text" : "We compare our method, dubbed Feature Rule Miner (FRM for short), against two variants of the kNN-based theory miner of Yang et al. (2015) on four publicly available knowledge bases: Nations, Kinship and UMLS from Kemp et al. (2006), and Family from Fang et al. (2013). The KB statistics can be found in Table 1. Given that FRM requires the relational embeddings W to be normalized (with respect to the Frobenius norm), we compare it against both the original kNN-based miner, which mines the unnormalized embeddings, and a variant that uses the normalized embeddings instead, for the sake of fairness.\nThe miners were tested in a 10-fold cross-validation setting. We computed the relational embeddings over the training sets using non-negative RESCAL (Krompaß et al., 2013) 2 variant with the default parameters (500 maximum iterations, convergence threshold 10−5). The size of the embeddings d was set to a reasonable value for each KB: 100 for Family, 25 for Kinship and UMLS, and 5 for Nations. We configured all competitors to mine at most 100 rules for each head relation. The kNN distance threshold was set to 100 (although the actual value used is chosen dynamically, as done by Yang et al. (2015)). The desired reconstruction threshold of OMP was set ot 10−3. Finally, the coefficient threshold τ was set to 0.2.\nWe evaluate both the F-score and the per-rule recall of all the methods. The F-score measures how well the mined rules reconstruct the test facts in terms of both precision and recall. The per-rule recall is simply the recall over the number of rules mined for the target head; it favors methods that\n2Standard RESCAL tends to penalize the kNN-based competitors.\nfocus on few rules with high coverage, and penalizes those that mine many irrelevant rules. The results on the four KBs (averaged over all target relations) are reported in Figures 1 and 2, and an example of mined rules in Figure 3. More detailed per-head results can be found in Appendix B. (Unfortunately, the normalized kNN method failed to work with the UMLS dataset; we left a blank in the plots.)\nThe plots show a clear trend: FRM performs better than the kNN-based methods in all four knowledge bases, both in terms of F-score and in terms of per-rule recall. Further, the normalized kNN variant tends to outperform the original, unnormalized version, providing support for our use of normalized relation embeddings.\nNotably, the three methods mine similar amounts of rules. While OMP stops automatically when the mined body reconstructs the target head sufficiently well, the kNN methods compensate for the lack of a proper termination criterion by employing a distance-based pruning heuristict (as discussed by Yang et al. (2015)). Rather, the poor per-rule recall performance of the kNN methods can be imputed to insufficient rule diversity. The kNN miners discover the rules independently of each other, leading to theory redundancy. This is a well known problem in rule mining. On the contrary, OMP avoids this issue by enforcing orthogonality between the mined bodies. The resulting theories performs much better especially in terms of per-rule recall.\nThe phenomenon is also visible in Figure 3. The theory found by FRM contains many diverse bodies, while the one found by kNN does not. The two rules also show the power of negation: the FRM theory includes the “perfect” definition of a brother, i.e. siblingOf ∧ ¬sisterOf (as well as an obvious error, i.e. that a brother can not be a sibling of a sibling). In contrast the theory found by kNN completely ignores the complementarity of brotherOf and sisterOf, and includes the rule brotherOf⇐ sisterOf."
    }, {
      "heading" : "5 RELATED WORK",
      "text" : "There is a huge body of work on theory learning, historically studied in Inductive Logic Programming (Dzeroski & Lavrac, 1994; Muggleton et al., 1992). For the sake of brevity, we focus on techniques that are more closely related to our proposal.\nThe core of most ILP methods, e.g. FOIL (Quinlan, 1990), Progol (Muggleton, 1995), and Aleph3, is a search loop over the space of candidate theories. Bottom-up methods start from an initially empty theory, and add one Horn rule at a time. Individual rules are constructed by conjoining firstorder relations so as to maximize the number of covered positive facts, while trying to keep covered negative facts to a minimum. After each rule is constructed, all covered facts are removed from the KB. These methods are extremely expressive, and can handle general nary relations. Instead, FRM focuses on binary relations only, which are more common in today’s Web-centric knowledge bases. ILP methods are designed to operate on the original KB only; this fact, paired with the sheer magnitude of the search space, makes standard ILP methods highly non-scalable. More recent extensions (e.g. kFOIL (Landwehr et al., 2006)) adopt a feature-space view of relational facts, but are still based on the classical search loop and can not be trivially adapted to working on the relational embeddings directly. Finally, rule elongation can be hindered by the presence of plateaus in the cost function.\nOur path-based learning procedure is closely related to Relational Pathfinding (RP) (Richards & Mooney, 1992). RP is based on the observation that ground relation paths (that is, conjunctions of true relation instances) do act as support for arbitrary-length rules. It follows that mining these paths directly allows to detect longer rules with high support, avoiding the rule elongation problem entirely. There are many commonalities between RP and FRM. Both approaches are centered around relation paths, although in different representations (original versus compressed), and focus on pathbased theories. The major drawback of RP is that it requires exhaustive enumeration of relation paths (up to a maximum length), which can be impractical depending on the size of the KB. FRM sidesteps this issue by leveraging efficient online decoding techniques, namely Online Search OMP (Weinstein & Wakin, 2012).\nTo alleviate its computational requirements, a lifting procedure for RP was presented in Kok & Domingos (2009). Similarly to FRM, lifted RP is composed of separate compression and learning stages. In the first stage, the original KB is “lifted” by clustering functionally identical relation paths together, producing a smaller KB as output. In the second stage, standard RP is applied to the compressed KB. A major difference with FRM is that lifting is exact, while RESCAL is typically lossy. Consequently, lifted RP guarantees equivalence of the original and compressed learning problems, but it also ignores the potential generalization benefit provided by the embeddings. Additionally, the first step of lifted RP relies on a (rather complex) agglomerative clustering procedure, while FRM can make use of state-of-the-art representation learning methods. Note that, just like lifted RP, FRM can be straightforwardly employed for structure learning of statistical relational models.\nThe work of Malioutov & Varshney (2013) is concerned with mining one-level rules from binary data. Like in FRM, rule learning is viewed as a recovery problem, and solved using compressed sensing techniques. Two major differences with FRM exist. In Malioutov & Varshney (2013) the truth value matrix is recovered with an extension of Basis Pursuit that handles 0-1 coefficients through\n3http://www.cs.ox.ac.uk/activities/machinelearning/Aleph/\na mixed-integer linear programming (MILP) formulation, which is however solved approximately using linear relaxations. BP however requires the dictionary to be explicitly grounded, which is not the case for FRM. Additionally, their method is limited to one-level rules, i.e. either conjunctions or disjunctions of relations, but not both. An extension to two-level rules has been presented by Su et al. (2015), where BP is combined with heuristics to aggregate individual rules into two-level theories. In contrast, FRM natively supports mining two-level rules via efficient online search.\nThe only other theory learning method that is explicitly designed for working on embeddings is the one of Yang et al. (2015). It is based on the observation (also made by Gu et al. (2015)) that closed path Horn rules can be converted to path queries, which can be answered approximately by searching the space of (type-compatible) compositions of relation embeddings. They propose to perform a simple nearest neighbor search around the embedding of the head relation, Wh, while avoiding type-incompatible relation compositions. Unfortunately, rules are searched for independently of one another, which seriously affects both quality and interpretability of the results as shown by our experimental evaluation."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We presented a novel approach for performing rule mining directly over a compressed summary of a KB. A major advantage over purely logical alternatives is that the relational embeddings automatically generalize beyond the observed facts; as consequence, our method implicitly mines a completion of the knowledge base. The key idea is that theory learning can be approximated by a recovery problem in the space of relation embeddings, which can be solved efficiently using well-known sparse recovery algorithms. This novel formulation enables our method to deal with all propositional logic connectives (conjunction, disjunction, and negation), unlike previous techniques. We presented experimental results highlighting the ability of our miner to discover relevant and, most importantly, diverse rules.\nOne difficulty in applying our methods is that classical sparse recovery algorithm require the complete enumeration of the candidate rule bodies, which is exponential in rule length. In order to solve this issue, we plan to apply recent online recovery algorithms, like Online Search OMP (Weinstein & Wakin, 2012), which can explore the space of alternative bodies on-the-fly.\nAs the quality of relational embedding techniques improves, for instance thanks to path-based Gu et al. (2015); Neelakantan et al. (2015); Garcı́a-Durán et al. (2015) and logic-based Rocktäschel et al. (2015) training techniques, we expect the reliability and performance of theory learning in feature space to substantially improve as well."
    }, {
      "heading" : "APPENDIX A: ERROR DERIVATION",
      "text" : "Fix a target head relation h. Let Eh denote the RESCAL error matrix Eh := Y h − X>WhX , and Ẽh denote the error matrix of Wh due to FRM, namely Ẽh := Wh − ∑ B∈Th α\nBWB where WB = ∏ b∈BW b. Putting the two definitions together, we obtain:\nEh = Y h −X> [∑\nB∈Th W B + Ẽh\n] X = Y h −X> [∑ B∈Th W B ] X −X>ẼhX\nThen, the Frobenius norm of the reconstruction error of head h is: ‖Y h−X> ∑ B∈Th αBW BX‖ = ‖X>ẼhX+Eh‖ ≤ ‖X>ẼhX‖+‖Eh‖ ≤ ‖X‖2‖Ẽh‖+‖Eh‖\nwhere the last step follows from the sub-multiplicativity of the Frobenius norm. Now, FRM minimizes ‖Ẽh‖, and therefore minimizes an upper bound of the misclassification error of Th over Y h.\nWe note in passing that the bound can be tightened by reducing the norm of the entity embeddings X , for instance by choosing the proper embedding method. The question of how to find an optimal choice, however, is left as future work."
    }, {
      "heading" : "APPENDIX B: EXTENDED RESULTS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Dbpedia: A nucleus for a web of open data",
      "author" : [ "S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio" ],
      "venue" : "In Proceedings of AAAI,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "Emmanuel J Candès", "Justin Romberg", "Terence Tao" ],
      "venue" : "IEEE Transactions on information theory,",
      "citeRegEx" : "Candès et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2006
    }, {
      "title" : "Atomic decomposition by basis pursuit",
      "author" : [ "S.S. Chen", "David L. Donoho", "M.A. Saunders" ],
      "venue" : "SIAM journal on scientific computing,",
      "citeRegEx" : "Chen et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 1998
    }, {
      "title" : "Inductive logic programming: Techniques and applications",
      "author" : [ "Sašo Dzeroski", "Nada Lavrac" ],
      "venue" : null,
      "citeRegEx" : "Dzeroski and Lavrac.,? \\Q1994\\E",
      "shortCiteRegEx" : "Dzeroski and Lavrac.",
      "year" : 1994
    }, {
      "title" : "Relational Data Mining, New York",
      "author" : [ "Sašo Dzeroski", "Nada Lavrac (eds" ],
      "venue" : "NY, USA,",
      "citeRegEx" : "Dzeroski and .eds...,? \\Q2000\\E",
      "shortCiteRegEx" : "Dzeroski and .eds...",
      "year" : 2000
    }, {
      "title" : "Kinship classification by modeling facial feature heredity",
      "author" : [ "R. Fang", "A. Gallagher", "T. Chen", "A. Loui" ],
      "venue" : "In Proceedings of ICIP,",
      "citeRegEx" : "Fang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast rule mining in ontological knowledge bases with amie+",
      "author" : [ "L. Galárraga", "C. Teflioudi", "K. Hose", "F.M. Suchanek" ],
      "venue" : "The VLDB Journal,",
      "citeRegEx" : "Galárraga et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2015
    }, {
      "title" : "Composing relationships with translations",
      "author" : [ "A. Garcı́a-Durán", "A. Bordes", "N. Usunier" ],
      "venue" : "In Proceedings of EMNLP, pp",
      "citeRegEx" : "Garcı́a.Durán et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Garcı́a.Durán et al\\.",
      "year" : 2015
    }, {
      "title" : "Traversing knowledge graphs in vector space",
      "author" : [ "K. Gu", "J. Miller", "P. Liang" ],
      "venue" : "arXiv preprint arXiv:1506.01094,",
      "citeRegEx" : "Gu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2015
    }, {
      "title" : "Yago2: A spatially and temporally enhanced knowledge base from wikipedia",
      "author" : [ "J. Hoffart", "F.M. Suchanek", "K. Berberich", "G. Weikum" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Hoffart et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hoffart et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning systems of concepts with an infinite relational model",
      "author" : [ "Charles Kemp", "Joshua B Tenenbaum", "Thomas L Griffiths", "Takeshi Yamada", "Naonori Ueda" ],
      "venue" : "In Proceedings of AAAI,",
      "citeRegEx" : "Kemp et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kemp et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning markov logic network structure via hypergraph lifting",
      "author" : [ "S. Kok", "P. Domingos" ],
      "venue" : "In Proceedings of ICML, pp",
      "citeRegEx" : "Kok and Domingos.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kok and Domingos.",
      "year" : 2009
    }, {
      "title" : "Non-negative tensor factorization with rescal",
      "author" : [ "Denis Krompaß", "Maximilian Nickel", "Xueyan Jiang", "Volker Tresp" ],
      "venue" : "In Tensor Methods for Machine Learning, ECML workshop,",
      "citeRegEx" : "Krompaß et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Krompaß et al\\.",
      "year" : 2013
    }, {
      "title" : "kfoil: Learning simple relational kernels",
      "author" : [ "N. Landwehr", "A. Passerini", "L. De Raedt", "P. Frasconi" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Landwehr et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Landwehr et al\\.",
      "year" : 2006
    }, {
      "title" : "Multi-relational learning using weighted tensor decomposition with modular loss",
      "author" : [ "B. London", "T. Rekatsinas", "B. Huang", "L. Getoor" ],
      "venue" : "arXiv preprint arXiv:1303.1733,",
      "citeRegEx" : "London et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "London et al\\.",
      "year" : 2013
    }, {
      "title" : "Exact rule learning via boolean compressed sensing",
      "author" : [ "D. Malioutov", "K. Varshney" ],
      "venue" : "In Proceedings of ICML, pp",
      "citeRegEx" : "Malioutov and Varshney.,? \\Q2013\\E",
      "shortCiteRegEx" : "Malioutov and Varshney.",
      "year" : 2013
    }, {
      "title" : "Inverse entailment and progol",
      "author" : [ "S. Muggleton" ],
      "venue" : "New generation computing,",
      "citeRegEx" : "Muggleton.,? \\Q1995\\E",
      "shortCiteRegEx" : "Muggleton.",
      "year" : 1995
    }, {
      "title" : "Inductive logic programming, volume 168",
      "author" : [ "Stephen Muggleton", "Ramon Otero", "Alireza Tamaddoni-Nezhad" ],
      "venue" : null,
      "citeRegEx" : "Muggleton et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Muggleton et al\\.",
      "year" : 1992
    }, {
      "title" : "Compositional vector space models for knowledge base completion",
      "author" : [ "A. Neelakantan", "B. Roth", "A. McCallum" ],
      "venue" : "arXiv preprint arXiv:1504.06662,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "A three-way model for collective learning on multi-relational data",
      "author" : [ "M. Nickel", "V. Tresp", "H-P Kriegel" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2011
    }, {
      "title" : "Factorizing yago: scalable machine learning for linked data",
      "author" : [ "Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel" ],
      "venue" : "In Proceedings of WWW,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2012
    }, {
      "title" : "A review of relational machine learning for knowledge graphs",
      "author" : [ "Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2016
    }, {
      "title" : "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition",
      "author" : [ "Y. Chandra Pati", "R. Rezaiifar", "P.S. Krishnaprasad" ],
      "venue" : "In Proceedings of Asilomar Conference on Signals, Systems and Computers,",
      "citeRegEx" : "Pati et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Pati et al\\.",
      "year" : 1993
    }, {
      "title" : "Learning logical definitions from relations",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Quinlan.,? \\Q1990\\E",
      "shortCiteRegEx" : "Quinlan.",
      "year" : 1990
    }, {
      "title" : "Learning relations by pathfinding",
      "author" : [ "B.L. Richards", "R.J. Mooney" ],
      "venue" : "In Proceedings of AAAI,",
      "citeRegEx" : "Richards and Mooney.,? \\Q1992\\E",
      "shortCiteRegEx" : "Richards and Mooney.",
      "year" : 1992
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "S. Riedel", "L. Yao", "A. McCallum", "B.M. Marlin" ],
      "venue" : "In Proceedings of NAACL-HLT, pp",
      "citeRegEx" : "Riedel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2013
    }, {
      "title" : "Injecting logical background knowledge into embeddings for relation extraction",
      "author" : [ "T. Rocktäschel", "S. Singh", "S. Riedel" ],
      "venue" : "In Proceedings of NAACL HTL,",
      "citeRegEx" : "Rocktäschel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "R. Socher", "D. Chen", "C.D. Manning", "A. Ng" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Interpretable two-level boolean rule learning for classification",
      "author" : [ "G. Su", "D. Wei", "K.R. Varshney", "D.M. Malioutov" ],
      "venue" : "arXiv preprint arXiv:1511.07361,",
      "citeRegEx" : "Su et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2015
    }, {
      "title" : "Online search orthogonal matching pursuit",
      "author" : [ "A.J. Weinstein", "M.B. Wakin" ],
      "venue" : "In Proceedings of IEEE SSP Workshop,",
      "citeRegEx" : "Weinstein and Wakin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Weinstein and Wakin.",
      "year" : 2012
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "B. Yang", "W. Yih", "X. He", "J. Gao", "L. Deng" ],
      "venue" : "In Proceedings of ICLR,",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Mining theories from data is the task of Rule Mining (Dzeroski & Lavrac, 2000) and Inductive Logic Programming (Dzeroski & Lavrac, 1994; Muggleton et al., 1992).",
      "startOffset" : 111,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "While powerful, pure ILP methods do not scale to large relational datasets, preventing them from mining Web-scale KBs such as YAGO (Hoffart et al., 2013) and DBpedia (Auer et al.",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : ", 2013) and DBpedia (Auer et al., 2007).",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Next-generation miners that specialize on large KBs, such as AMIE (Galárraga et al., 2015), work around these issues by trading off theory expressiveness for runtime efficiency.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Common summarization techniques for relational data include relational factorization (Nickel et al., 2011; London et al., 2013; Riedel et al., 2013) and representation learning (Bordes et al.",
      "startOffset" : 85,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Common summarization techniques for relational data include relational factorization (Nickel et al., 2011; London et al., 2013; Riedel et al., 2013) and representation learning (Bordes et al.",
      "startOffset" : 85,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : "Common summarization techniques for relational data include relational factorization (Nickel et al., 2011; London et al., 2013; Riedel et al., 2013) and representation learning (Bordes et al.",
      "startOffset" : 85,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : ", 2013) and representation learning (Bordes et al., 2011; Socher et al., 2013).",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : ", 2013) and representation learning (Bordes et al., 2011; Socher et al., 2013).",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "Until recently, relational embeddings have been mostly employed for link prediction and knowledge base completion (Nickel et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : ", 2013) and DBpedia (Auer et al., 2007). Further, purely logical methods can not gracefully deal with noise. Next-generation miners that specialize on large KBs, such as AMIE (Galárraga et al., 2015), work around these issues by trading off theory expressiveness for runtime efficiency. A general strategy for processing huge datasets is dimensionality reduction: instead of working on the original KB directly, one first squeezes it to a summary of manageable size, and then performs the required operations on the summary itself. Common summarization techniques for relational data include relational factorization (Nickel et al., 2011; London et al., 2013; Riedel et al., 2013) and representation learning (Bordes et al., 2011; Socher et al., 2013). The core idea is to learn compressed latent representations, or embeddings, of entities and relations able to reconstruct the original KB by minimizing a suitable reconstruction loss. Until recently, relational embeddings have been mostly employed for link prediction and knowledge base completion (Nickel et al., 2016). However, Yang et al. (2015) have shown that low-dimensional representations can also be exploited to perform a simplified form of theory learning.",
      "startOffset" : 21,
      "endOffset" : 1102
    }, {
      "referenceID" : 23,
      "context" : "The recovery problem can be solved with specialized compressed sensing algorithms, such as Orthogonal Matching Pursuit (Pati et al., 1993) or variants thereof.",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 30,
      "context" : "Building on the insights of Yang et al. (2015), we propose a novel approach to theory learning from low-dimensional representations.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Given an input KB, the goal of theory learning, also known as Inductive Logic Programming (Muggleton et al., 1992), is to induce a compact logical theory that both explains the observed facts and generalizes to the unobserved ones.",
      "startOffset" : 90,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "Please see (Dzeroski & Lavrac, 1994; Muggleton et al., 1992) for more details.",
      "startOffset" : 11,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "In order to ground the discussion, we focus on a prototypical factorization method, RESCAL (Nickel et al., 2011; 2012); many alternative formulations can be seen as variations or generalizations thereof.",
      "startOffset" : 91,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "In order to ground the discussion, we focus on a prototypical factorization method, RESCAL (Nickel et al., 2011; 2012); many alternative formulations can be seen as variations or generalizations thereof. We stress, however, that our method can be applied to other kinds of relational embeddings, as sketched in Section 6. For a general treatment of the subject, see Nickel et al. (2016). In RESCAL, each entity e ∈ [n] in the KB is mapped to a vector x ∈ R, and each binary relation r ∈ [m] to a matrix W r ∈ Rd×d.",
      "startOffset" : 92,
      "endOffset" : 387
    }, {
      "referenceID" : 20,
      "context" : "While the optimization problem is non-convex and can not be solved exactly in general, RESCAL pairs clever initialization with an alternating least squares procedure to obtain good quality solutions (Nickel et al., 2011).",
      "startOffset" : 199,
      "endOffset" : 220
    }, {
      "referenceID" : 31,
      "context" : "Following Yang et al. (2015), we require the rules to be closed paths, i.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 31,
      "context" : "The closed path represented by the conjunction of the relations in the body B is emulated by composition of embeddings and obtained by repeated applications of this mapping (Yang et al., 2015).",
      "startOffset" : 173,
      "endOffset" : 192
    }, {
      "referenceID" : 31,
      "context" : "This is justified by the observation (Yang et al., 2015; Gu et al., 2015; Neelakantan et al., 2015; Garcı́a-Durán et al., 2015) that the embeddings are learned so that their composition is close to that of the embedding of the head.",
      "startOffset" : 37,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "This is justified by the observation (Yang et al., 2015; Gu et al., 2015; Neelakantan et al., 2015; Garcı́a-Durán et al., 2015) that the embeddings are learned so that their composition is close to that of the embedding of the head.",
      "startOffset" : 37,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "This is justified by the observation (Yang et al., 2015; Gu et al., 2015; Neelakantan et al., 2015; Garcı́a-Durán et al., 2015) that the embeddings are learned so that their composition is close to that of the embedding of the head.",
      "startOffset" : 37,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "This is justified by the observation (Yang et al., 2015; Gu et al., 2015; Neelakantan et al., 2015; Garcı́a-Durán et al., 2015) that the embeddings are learned so that their composition is close to that of the embedding of the head.",
      "startOffset" : 37,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "Sparse recovery problems are a main subject of study in compressed sensing (Candès et al., 2006), and a multitude of algorithms can be employed to solve them, including Orthogonal Matching Pursuit (OMP) (Pati et al.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : ", 2006), and a multitude of algorithms can be employed to solve them, including Orthogonal Matching Pursuit (OMP) (Pati et al., 1993), Basis Pursuit (Chen et al.",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : ", 1993), Basis Pursuit (Chen et al., 1998), and many recent alternatives.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "First, if the target theory is sparse enough, existing recovery algorithms can solve the reconstruction to global optimality with high probability (Candès et al., 2006).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "We computed the relational embeddings over the training sets using non-negative RESCAL (Krompaß et al., 2013) 2 variant with the default parameters (500 maximum iterations, convergence threshold 10−5).",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "We compare our method, dubbed Feature Rule Miner (FRM for short), against two variants of the kNN-based theory miner of Yang et al. (2015) on four publicly available knowledge bases: Nations, Kinship and UMLS from Kemp et al.",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "(2015) on four publicly available knowledge bases: Nations, Kinship and UMLS from Kemp et al. (2006), and Family from Fang et al.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "(2006), and Family from Fang et al. (2013). The KB statistics can be found in Table 1.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "(2006), and Family from Fang et al. (2013). The KB statistics can be found in Table 1. Given that FRM requires the relational embeddings W to be normalized (with respect to the Frobenius norm), we compare it against both the original kNN-based miner, which mines the unnormalized embeddings, and a variant that uses the normalized embeddings instead, for the sake of fairness. The miners were tested in a 10-fold cross-validation setting. We computed the relational embeddings over the training sets using non-negative RESCAL (Krompaß et al., 2013) 2 variant with the default parameters (500 maximum iterations, convergence threshold 10−5). The size of the embeddings d was set to a reasonable value for each KB: 100 for Family, 25 for Kinship and UMLS, and 5 for Nations. We configured all competitors to mine at most 100 rules for each head relation. The kNN distance threshold was set to 100 (although the actual value used is chosen dynamically, as done by Yang et al. (2015)).",
      "startOffset" : 24,
      "endOffset" : 980
    }, {
      "referenceID" : 31,
      "context" : "While OMP stops automatically when the mined body reconstructs the target head sufficiently well, the kNN methods compensate for the lack of a proper termination criterion by employing a distance-based pruning heuristict (as discussed by Yang et al. (2015)).",
      "startOffset" : 238,
      "endOffset" : 257
    }, {
      "referenceID" : 18,
      "context" : "There is a huge body of work on theory learning, historically studied in Inductive Logic Programming (Dzeroski & Lavrac, 1994; Muggleton et al., 1992).",
      "startOffset" : 101,
      "endOffset" : 150
    }, {
      "referenceID" : 24,
      "context" : "FOIL (Quinlan, 1990), Progol (Muggleton, 1995), and Aleph3, is a search loop over the space of candidate theories.",
      "startOffset" : 5,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "FOIL (Quinlan, 1990), Progol (Muggleton, 1995), and Aleph3, is a search loop over the space of candidate theories.",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "kFOIL (Landwehr et al., 2006)) adopt a feature-space view of relational facts, but are still based on the classical search loop and can not be trivially adapted to working on the relational embeddings directly.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "kFOIL (Landwehr et al., 2006)) adopt a feature-space view of relational facts, but are still based on the classical search loop and can not be trivially adapted to working on the relational embeddings directly. Finally, rule elongation can be hindered by the presence of plateaus in the cost function. Our path-based learning procedure is closely related to Relational Pathfinding (RP) (Richards & Mooney, 1992). RP is based on the observation that ground relation paths (that is, conjunctions of true relation instances) do act as support for arbitrary-length rules. It follows that mining these paths directly allows to detect longer rules with high support, avoiding the rule elongation problem entirely. There are many commonalities between RP and FRM. Both approaches are centered around relation paths, although in different representations (original versus compressed), and focus on pathbased theories. The major drawback of RP is that it requires exhaustive enumeration of relation paths (up to a maximum length), which can be impractical depending on the size of the KB. FRM sidesteps this issue by leveraging efficient online decoding techniques, namely Online Search OMP (Weinstein & Wakin, 2012). To alleviate its computational requirements, a lifting procedure for RP was presented in Kok & Domingos (2009). Similarly to FRM, lifted RP is composed of separate compression and learning stages.",
      "startOffset" : 7,
      "endOffset" : 1320
    }, {
      "referenceID" : 14,
      "context" : "kFOIL (Landwehr et al., 2006)) adopt a feature-space view of relational facts, but are still based on the classical search loop and can not be trivially adapted to working on the relational embeddings directly. Finally, rule elongation can be hindered by the presence of plateaus in the cost function. Our path-based learning procedure is closely related to Relational Pathfinding (RP) (Richards & Mooney, 1992). RP is based on the observation that ground relation paths (that is, conjunctions of true relation instances) do act as support for arbitrary-length rules. It follows that mining these paths directly allows to detect longer rules with high support, avoiding the rule elongation problem entirely. There are many commonalities between RP and FRM. Both approaches are centered around relation paths, although in different representations (original versus compressed), and focus on pathbased theories. The major drawback of RP is that it requires exhaustive enumeration of relation paths (up to a maximum length), which can be impractical depending on the size of the KB. FRM sidesteps this issue by leveraging efficient online decoding techniques, namely Online Search OMP (Weinstein & Wakin, 2012). To alleviate its computational requirements, a lifting procedure for RP was presented in Kok & Domingos (2009). Similarly to FRM, lifted RP is composed of separate compression and learning stages. In the first stage, the original KB is “lifted” by clustering functionally identical relation paths together, producing a smaller KB as output. In the second stage, standard RP is applied to the compressed KB. A major difference with FRM is that lifting is exact, while RESCAL is typically lossy. Consequently, lifted RP guarantees equivalence of the original and compressed learning problems, but it also ignores the potential generalization benefit provided by the embeddings. Additionally, the first step of lifted RP relies on a (rather complex) agglomerative clustering procedure, while FRM can make use of state-of-the-art representation learning methods. Note that, just like lifted RP, FRM can be straightforwardly employed for structure learning of statistical relational models. The work of Malioutov & Varshney (2013) is concerned with mining one-level rules from binary data.",
      "startOffset" : 7,
      "endOffset" : 2235
    }, {
      "referenceID" : 14,
      "context" : "kFOIL (Landwehr et al., 2006)) adopt a feature-space view of relational facts, but are still based on the classical search loop and can not be trivially adapted to working on the relational embeddings directly. Finally, rule elongation can be hindered by the presence of plateaus in the cost function. Our path-based learning procedure is closely related to Relational Pathfinding (RP) (Richards & Mooney, 1992). RP is based on the observation that ground relation paths (that is, conjunctions of true relation instances) do act as support for arbitrary-length rules. It follows that mining these paths directly allows to detect longer rules with high support, avoiding the rule elongation problem entirely. There are many commonalities between RP and FRM. Both approaches are centered around relation paths, although in different representations (original versus compressed), and focus on pathbased theories. The major drawback of RP is that it requires exhaustive enumeration of relation paths (up to a maximum length), which can be impractical depending on the size of the KB. FRM sidesteps this issue by leveraging efficient online decoding techniques, namely Online Search OMP (Weinstein & Wakin, 2012). To alleviate its computational requirements, a lifting procedure for RP was presented in Kok & Domingos (2009). Similarly to FRM, lifted RP is composed of separate compression and learning stages. In the first stage, the original KB is “lifted” by clustering functionally identical relation paths together, producing a smaller KB as output. In the second stage, standard RP is applied to the compressed KB. A major difference with FRM is that lifting is exact, while RESCAL is typically lossy. Consequently, lifted RP guarantees equivalence of the original and compressed learning problems, but it also ignores the potential generalization benefit provided by the embeddings. Additionally, the first step of lifted RP relies on a (rather complex) agglomerative clustering procedure, while FRM can make use of state-of-the-art representation learning methods. Note that, just like lifted RP, FRM can be straightforwardly employed for structure learning of statistical relational models. The work of Malioutov & Varshney (2013) is concerned with mining one-level rules from binary data. Like in FRM, rule learning is viewed as a recovery problem, and solved using compressed sensing techniques. Two major differences with FRM exist. In Malioutov & Varshney (2013) the truth value matrix is recovered with an extension of Basis Pursuit that handles 0-1 coefficients through",
      "startOffset" : 7,
      "endOffset" : 2471
    }, {
      "referenceID" : 28,
      "context" : "An extension to two-level rules has been presented by Su et al. (2015), where BP is combined with heuristics to aggregate individual rules into two-level theories.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "An extension to two-level rules has been presented by Su et al. (2015), where BP is combined with heuristics to aggregate individual rules into two-level theories. In contrast, FRM natively supports mining two-level rules via efficient online search. The only other theory learning method that is explicitly designed for working on embeddings is the one of Yang et al. (2015). It is based on the observation (also made by Gu et al.",
      "startOffset" : 54,
      "endOffset" : 376
    }, {
      "referenceID" : 9,
      "context" : "It is based on the observation (also made by Gu et al. (2015)) that closed path Horn rules can be converted to path queries, which can be answered approximately by searching the space of (type-compatible) compositions of relation embeddings.",
      "startOffset" : 45,
      "endOffset" : 62
    } ],
    "year" : 2016,
    "abstractText" : "Relational embeddings have emerged as an excellent tool for inferring novel facts from partially observed knowledge bases. Recently, it was shown that some classes of embeddings can also be exploited to perform a simplified form of rule mining. By interpreting logical conjunction as a form of composition between relation embeddings, simplified logical theories can be mined directly in the space of latent representations. In this paper, we present a method to mine full-fledged logical theories, which are significantly more expressive, by casting the semantics of the logical operators to the space of the embeddings. In order to extract relevant rules in the space of relation compositions we borrow sparse reconstruction procedures from the field of compressed sensing. Our empirical analysis showcases the advantages of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}