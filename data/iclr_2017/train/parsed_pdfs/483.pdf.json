{
  "name" : "483.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "RECURRENT MIXTURE DENSITY NETWORK FOR SPATIOTEMPORAL VISUAL ATTENTION",
    "authors" : [ "Loris Bazzani", "Hugo Larochelle" ],
    "emails" : [ "bazzanil@amazon.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Attentional modeling and saliency prediction in images has been an active research topic in computer vision over the last decade. Interest in attentional models is primarily motivated by their ability to eliminate or down-weight pixels that are not important for the task at hand, as for example shown in prior work using visual attention for image recognition and caption generation (Sermanet et al., 2014; Xu et al., 2015; Mnih et al., 2014). Integrating visual attention in an image analysis model can potentially lead to improved overall accuracy, as the system can focus on the most salient regions in the photo without being disturbed by irrelevant information.\nRecently, we have witnessed a shift of trend from image saliency prediction (Borji & Itti, 2013) to the modeling of saliency in videos (Rudoy et al., 2013). Since human fixation patterns are strongly correlated over time (Coull, 2004), it appears critical to model the relations between saliency maps of consecutive frames. In this scenario, attention can be defined as a spatiotemporal volume, where each saliency map (one for each frame) depends on the frames at the previous times. The saliency map can be interpreted as a probability distribution over pixels and the actual fixation patterns can be generated by sampling from the the map.\n∗This work was done when Loris Bazzani was at Dartmouth College. †Hugo Larochelle is now at Google Brain.\nGoing from images to videos is not straightforward, since videos bring up many challenges. First of all, videos have an additional dimension (time), compared to images. This causes a dramatic growth in the number of pixels to be processed and poses a significantly higher computational cost for analysis. At the same time, there are strong redundancies present in such data, which implies that visual attention may be particularly beneficial for the video setting. For example, typically the objects or people in a video do not change significantly in appearance over time. Yet, for analysis tasks such as action recognition (Wang & Schmid, 2013) or video description (Yao et al., 2015), it is imperative to properly model the dynamical properties of these objects in the video. This suggests that, in order to identify spatiotemporal volumes that are salient for video analysis, an attentional model must take into account high-level image semantics as well as the history of past fixations.\nIn order to cope with these challenges, we propose an efficient spatiotemporal attentional model (see Fig. 1) that leverages deep 3D convolutional features (Tran et al., 2015) as semantic, spatiotemporal representation of short clips in the video. This clip-level representation is then aggregated by a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997), that expands the temporal range of analysis from few frames to seconds. The LSTM model connects into a Mixture Density Network (MDN) (Bishop, 1994) that at each frame outputs the parameters of a Gaussian mixture model expressing the saliency map. We refer to this model as Recurrent Mixture Density Network (RMDN). RMDN is trained via maximum likelihood estimation using human fixations as training data, without knowledge of the actions in the videos.\nThe potential applications of automatic saliency map prediction from videos are many. They include attention-based video compression (Gitman et al., 2014), visual attention for robots (Yu et al., 2010), crowd analysis for video surveillance (Jiang et al., 2014), salient object detection (Li & Yu, 2015; Karthikeyan et al., 2015) and activity recognition (Vig et al., 2012; Sapienza et al., 2014). In this work we focus on a study of how visual attention may improve action recognition by leveraging the saliency map generated by RMDN for video classification. The idea is akin to soft attention and consists in re-weighting the pixel values of the input video by the estimated saliency map. Despite its simplicity, we show that the combination of features extracted from this modified version of the video and those computed from the original input lead to a significant improvement in action recognition, compared to a model that does not use attention.\nThe primary contribution of this work is a spatiotemporal saliency estimation network optimized to reproduce human fixations. The proposed approach offers several advantages: 1) the model can be trained without having to engineer spatiotemporal features; 2) RMDN is directly trained on examples of human fixations and thus learns to mimic human visual attention; 3) prediction of the saliency map is very fast (it takes 0.08s per 16-frame clip on a GPU); 4) the method outperforms the state-of-the-art (Mathe & Sminchisescu, 2015) in saliency accuracy; 5) our predicted saliency maps lead to to improvements in action classification accuracy."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Broadly speaking, the literature on attentional models can be split into two categories: task-agnostic approaches which model the bottom-up, free-viewing properties of attention, and task-specific methods which model its top-down, task-driven properties. Researchers have devoted many years to create datasets, collecting human fixations and proposing solutions for biologically-plausible saliency estimators, built using low-level cues such as edge detectors and color filters (e.g. see Borji et al. (2013); Judd et al. (2009); Harel et al. (2006) for recent examples). We refer to Borji & Itti (2013) and Bruce et al. (2016) for an interesting analysis and comparison of existing methods. Most of the techniques in the literature are focused on extracting features in a bottom-up and/or top-down manner and use them to estimate the saliency map. In this context, motion features are introduced when extending saliency methods from images to videos (Guo et al., 2008; Zhao et al., 2015; Zhai & Shah, 2006). However, there is no explicit modeling of the temporal dimension that can capture long-term relations. In fact, motion features (e.g., optical flow) describe short-term associations at the temporal scale of only a few consecutive frames.\nPrior approaches can also be categorized into soft-attentional versus hard-attentional models. Softattentional models use the predicted saliency maps to down-weight pixels that are not relevant or salient, e.g., Song et al. (2016). Specifically deep networks have been used in this context to assign a weight to each pixel in order to extract “glimpses” from images (Xu et al., 2015; Gregor et al., 2015)\nor videos (Yao et al., 2015) in the form of weighted pixel averages. One strength of such approaches is that they can backpropagate through the attentional component and tune it in the context of its use in a deep network. Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training.\nAll of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images. Pan & i Nieto (2015) followed the same principle but without using any pre-trained network for initialization. Liu et al. (2015) proposed a multi-scale architecture for saliency prediction, and Li & Yu (2015) added a refinement step in order to enforce spatial coherence of the output. Simonyan et al. (2014) and Mahendran & Vedaldi (2016) proposed to reverse deep networks using deconvolutions for visualization and to estimate image saliency. However, these methods estimate saliency from still images and do not consider the temporal aspect of video. Chaabouni et al. (2016a;b) trained a ConvNet for saliency prediction on optical flow features and individual frames. However the model uses only the very short-term temporal relations of two consecutive frames.\nIn this paper, we explore the following question: can deep networks be trained to reliably predict spatiotemporal attentional patterns, specifically in such a way that these predictions can be leveraged successfully by a recognition system? To our knowledge, our work distinguishes itself from the aforementioned literature by being the first application of deep networks to the prediction of spatiotemporal human saliency in videos."
    }, {
      "heading" : "3 PROPOSED MODEL",
      "text" : "We start with a high-level description of our attentional model. We then formalize it in Sec. 3.1, and describe its training in Sec. 3.2. Sec. 3.3 reports how prediction is efficiently carried out at test time. Sec. 3.4 describes how to leverage the predicted saliency map to improve action recognition.\nThe proposed RMDN model for saliency estimation is depicted in Fig. 1. At time t, the input of the model is a sequence of the lastK = 16 frames, i.e., from time t−K+1 to current time t. We refer to this sequence as the input “clip.” The first part of the model (Fig. 1, blue layers above the input clip) consists of a 3D convolutional network that provides a feature representation of the clip. Our choice of a clip-based representation rather than a single-frame descriptor is motivated by the fact that these features allow us to explicitly capture short-term information that is then aggregated for long-term spatiotemporal visual attention by RMDN. Furthermore, there is recently growing evidence (Tran et al., 2015; Srivastava et al., 2015; Yue-Hei Ng et al., 2015) that by modeling the temporal information it is possible to obtain improved performances in high-level video analysis tasks, such as action recognition. For the computation of spatiotemporal features from the input clip, we use the “C3D” architecture proposed by Tran et al. (2015), which has been shown to provide competitive results on video recognition tasks across different datasets. The C3D architecture is defined as: C64-P-C128-P-C256-C256-P-C512-C512-P-C512-C512-P-FC4096-FC4096-softmax, where C is a 3D convolutional layer, P is the pooling layer, FC is a fully-connected layer, and the number specifies the number of kernels of the layer (e.g. C64 has 64 kernels). For the details about the size and stride of the convolutional and pooling kernels, we refer to (Tran et al., 2015).\nThe convolutional network has access to a limited window of the video since it uses a fixed-size clip of 16 frames as input. In order to empower the visual attention model with the ability to take into account longer temporal extents, we need a mechanism that performs temporal aggregation of past clip-level signals. To this end, we propose to connect the internal representation of the C3D model to a recurrent neural network, as shown in Fig. 1 (green module). The aim of the temporal\nconnections of the recurrent neural network is to propagate the clip-level features through time via memory units that can capture long-term dependencies. Our model uses LSTMs (Hochreiter & Schmidhuber, 1997) as memory blocks.\nThe saliency map at each time t is expressed in terms of a Gaussian Mixture Model (GMM) with C components. We denote its parameters with {(µc, πc,Σc)}Cc=1, where µc, πc and Σc are the mean, the mixture coefficient and the covariance of the c-th Gaussian component, respectively. The LSTM directly outputs these parameters (see details below). The resulting network is known as a Mixture Density Network (MDN) (Bishop, 1994; Graves, 2013).\nSince the model is recurrent, there is a direct connection between the inner representation of the LSTM at time t and the one at time t + 1. This favors temporal consistency between the saliency maps at adjacent times."
    }, {
      "heading" : "3.1 FORMALIZATION OF THE MODEL",
      "text" : "Let D = {(vi,ai)}Ni=1 be a dataset of videos and human fixations pairs. vi = (cit) Ti−1 t=0 is a video consisting of Ti temporally overlapping clips cit (i.e., sampled with stride 1) and a i = (ait) Ti−1 t=0 is the sequence of ground-truth fixations for the i-th video aligned with the clips. Since we use C3D to represent each clip, cit has a fixed length of K = 16 frames and t = 0 means that the first K frames are used to build the 0-th clip. The fixations ait = {ait,j}Aj=1 are a set of (x, y) image positions that are normalized to [0, 1] in order to deal with videos of different resolutions. The number of fixations vary from frame to frame in general, but in our experiments we control it via subsampling in order to obtain for each frame a set of fixed size A.\nLet xt = C3D(ct) be the internal representation of C3D for an input clip ct. In our model we use the last convolutional layer, before the fully-connected layers. We choose a convolutional layer instead of a fully-connected layer because the latter discards spatial information, which is crucial to estimate a spatially-variant saliency map over the image.\nThe LSTM network (Hochreiter & Schmidhuber, 1997) is defined as follows:\nft = σ(Wf · [ht−1,xt] + bf ), it = σ(Wi · [ht−1,xt] + bi) (1) ot = σ(Wo · [ht−1,xt] + bo), C̃t = tanh (WC · [ht−1,xt] + bC) (2) Ct = ft ∗ Ct−1 + it ∗ C̃t, ht = ot ∗ tanh (Ct) (3)\nwhere ft, it, ot, Ct and ht are the forget gate, the input gate, the output gate, the memory cell, and the hidden representation, respectively. The learning parameters that need to be estimated during the training phase are Wz and bz ,where z ∈ {f, i, o, C}. The MDN (Graves, 2013; Bishop, 1994) takes its inputs from the hidden representation of the LSTM network. Since the output space is 2D (the space of image locations), we can reparametrize the model as {(µct , πct , σct , ρct)}Cc=1, where µct , πct , σct and ρct are the 2D mean position, the weight, the 2D variance and the correlation of the c-th Gaussian component, respectively. The MDN is therefore defined as follows:\nyt = {(µ̃ct , π̃ct , σ̃ct , ρ̃ct)}Cc=1 = Wy · ht + by (4) where Wy and by are the parameters of the linear layer and ht is the hidden representation of the LSTM network. The parameters of the GMM in Eq. 4 are normalized as follows in order to obtain a valid probability distribution:\nµct = µ̃ c t , π c t = exp(π̃ct )∑C i=1 exp(π̃ i t) , σct = exp(σ̃ c t ), ρ c t = tanh (ρ̃ c t). (5)\nThe composition of the LSTM and the MDN results in the RMDN."
    }, {
      "heading" : "3.2 TRAINING",
      "text" : "The proposed model can be trained by optimizing the log-likelihood of the training ground truth fixations, ai, under the GMM. The loss function for the i-th video, vi, is defined as the negative log-likelihood of the fixations under the GMM, as follows:\nL(vi,ai) = Ti−1∑ t=0 A∑ j=1 − log\n( C∑\nc=1\nπctN (ait,j ;µct , σct , ρct)\n) (6)\nwhereN is the Gaussian distribution. Note that the parameters of the Gaussian components depend on the input video vi, but we do not make this explicit in the equation in order to keep notation simple.\nThe log-likelihood of the RMDN is optimized using backpropagation through time, since it is a composition of continuous functions (e.g. linear transformations and element-wise non-linearities) and the LSTM, for which we can compute the gradients. In particular, we refer to the paper of Graves (2013) for the derivation of the gradients for the MDN using the loss function of Eq. 6. In practice, due to the limited training data, we freeze the layers of the C3D network to the values pretrained by Tran et al. (2015) for action recognition. This implies that the low-level representation xt is fixed. We jointly train the LSTM and MDN from randomly initialized parameters."
    }, {
      "heading" : "3.3 PREDICTION",
      "text" : "The inference stage is straightforward by following the equations of Sec. 3.1. At a given time t, the clip from time t − K + 1 to t is fed into the C3D network to produce the representation xt. This vector is passed to the LSTM (Eqs. 1, 2, and 3) whose hidden representation is passed to the MDN, which outputs the GMM parameters (Eqs. 4 and 5). In order to generate the final saliency map, we compute the probability of each pixel position under the GMM model. We normalize the probability map to sum up to 1 over the image pixels in order to produce a normalized saliency map."
    }, {
      "heading" : "3.4 SALIENCY FOR CLASSIFICATION",
      "text" : "For the task of video classification, we generate a modified version of the video by using a softattentional mechanism: the idea is to weight each pixel value by the estimated saliency at that\nposition. This operation effectively down-weights regions that are deemed not salient. The intuition is that then the classifier will be able to focus on the parts of the frame which are most relevant without being distracted by the non-salient regions (see Fig. 2 in Appendix A).\nAt each time t, we extract two representations: the “context” branch is given by the C3D representation of the original clip, while the “soft attentional” branch is given by the C3D representation of the input clip weighted by the saliency map. The rationale is that the context branch considers the global evolution of the activity in the video while the soft attentional branch is focused on the most-salient local evolution of the activity. The two representations are then concatenated at the clip level and max-pooled over the video to obtain the final video-level descriptor. This video-level representation is then used as input to train the video classifier, which is a linear SVM in all our experiments."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section, we evaluate the proposed method for both saliency prediction and action recognition on two challenging datasets: Hollywood2 (Marszałek et al., 2009) and UCF101 (Soomro et al., 2012). Section 4.1 reports a quantitative analysis for the task of saliency prediction. Section 4.2 shows the results for the action recognition task in two scenarios: 1) using the same dataset that was used to train the saliency predictor and 2) applying the pretrained attentional model to a neverseen dataset and a different set of actions. We reported the implementation details in Appendix B. We invite the reader to watch the qualitative results of the proposed method in the form of a video available at https://youtu.be/aXOwc17nx_s."
    }, {
      "heading" : "4.1 SALIENCY PREDICTION",
      "text" : "The proposed model is trained using human fixation data. Few datasets provide both human fixations and class labels, which we need for the action recognition experiment discussed in the next section. Therefore, we used the Hollywood2 dataset, which was augmented with eye tracking data by Mathe & Sminchisescu (2015). We follow the same evaluation protocol (i.e., same training/test splits) of Mathe & Sminchisescu (2015) and their validation procedure to compute the final results in order to compare with their work. Mathe & Sminchisescu (2015) generate the ground truth saliency from a binary fixation map where the only non-zero values are at fixations points. The final saliency map is produced by convolving the binary map with an isotropic Gaussian filter with standard deviation σ and then adding to it a uniform distribution with probability parameter p. As in Mathe & Sminchisescu (2015), the values of these two parameters are chosen from σ ∈ {1.5, 3.0} and p ∈ {0.25, 0.5} via hold-out validation. We use a validation set consisting of 20% of the training set. We use the remaining 80% of the training data to learn our models, and use the hold-out validation set to choose the hyperpameters of our model.\nWe evaluate all models on the test set, using popular metrics proposed in the literature of saliency map prediction for still images (Judd et al., 2012; Borji et al., 2013), such as Area Under the ROC Curve (AUC), Normalized Scanpath Saliency (NSS), linear Correlation Coefficient (CC) and the Similarity score (Sim). We refer to the papers of Judd et al. (2012) and Borji et al. (2013) for their detailed description.\nTable 1 shows results achieved with different variants of our model and a simple baseline method, which we refer to as Trained Central Bias (TCB). The TCB model is a single GMM trained using the fixations of all the videos in the training sets. TCB predicts the same saliency map for each testing frame, thus it discards completely the temporal information and the image input. This experiment shows that all versions of our RMDN consistently outperform TCB under all metrics, even when using a smaller number of fixations per frame during training.\nThe different variants of RMDN in Table 1 explore the following design choices in our model: 1) the impact of using LSTM hidden units as opposed to regular RNN units (second and third row) and 2) the number of fixations per frame used for training (third and fourth row). These experiments show that LSTM (third row) is better than an RNN (second row) in terms of AUC and NSS, but in order to have better CC and Sim we need to use more fixations per frame (fourth row). This is intuitive: since the LSTM has many more parameters than the RNN, it needs more training data to be properly optimized.\nThe last row in Table 1 shows the results obtained by retraining our model using the full training set of Mathe & Sminchisescu (2015) instead of just the 80% subset. For this case (RMDN full) we used the hyper-parameter values selected via hold out-validation for the experiment in the fourth row. This gives the best result for saliency prediction reported in our work and it is the model we used for all the subsequent experiments described below.\nAll of the experiments reported in Table 1 were obtained using C = 20 components in the GMM. We have also studied how the accuracy varies by reducing the value of C. For example, using the RMDN variant of row 5 in Table 1 but with C = 10 components (instead of 20), the performance does not change dramatically, yielding AUC= 0.8966 and NSS= 2.4392. On the other hand, the AUC and the NSS decrease considerably, by 1.3% and 0.3 points respectively, when using only C = 2 components (AUC= 0.8836 and NSS= 2.1385). Based on this analysis, in all our subsequent experiments we used C = 20 as we noticed that our approach implements automatically a sort of Occam’s razor, setting the weights πc of many components close to zero when necessary.\nWe have also carried out a few side experiments and discovered that using the fully-connected features of C3D instead of the convolutional representation gives results that are at least 1.5% lower in terms of AUC. Moreover, we tried to finetune the C3D network for action categorization on Hollywood2. However we did not obtain any significant improvement, confirming the findings of Tran et al. (2015): the C3D representation is already general enough to perform effectively on different action recognition tasks and fine-tuning the model on smaller-scale datasets (such as Hollywood2) does not seem beneficial. We also experimented with deep LSTMs, but we obtained an insignificant improvement of performance. For this reason and also because deep LSTMs have more parameters and are more computationally expensive to train, we chose to use a shallow one-layer LSTM. Finally, we run the ablation study where the recurrent link between time t − 1 and t of the RMDN is removed: the results in terms of AUC are 1.2% and 2.4% lower with respect to the RMDN which uses RNN (second row of Table 1) and LSTM (third row of Table 1), respectively.\nWe also compared our approach to the state-of-the-art in saliency prediction from video. Table 2 includes the results of the best methods taken from the extensive analysis done in Mathe & Sminchisescu (2015). The table reports also some useful baselines, such as the central bias (CB) and the human accuracy for the task. Note that: 1) CB differs from TCB, since it does not use any training fixations; and 2) the human accuracy is computed in (Mathe & Sminchisescu, 2015) by deriving a saliency map from half of our human subjects and is evaluated with respect to fixations of the remaining ones. Furthermore, Table 2 contrasts the use of static features, motion features and their combination. The last row reports the results obtained with our RMDN model. It is interesting to see that the results obtained with a single type of features (static or motion) have an AUC lower than 0.75, which is even lower than the one obtained by the central bias (0.84). Moreover, the combination reaches the best results when the central bias is combined with engineered features (SF+MF+CB). On the other hand, our method outperforms all the methods evaluated in Mathe & Sminchisescu (2015) by a large margin and our results are very close to human performance (the difference is only 3.2%). In addition to being the best method in Table 2, our method has several advantages: 1) it does not require any hand-engineering of spatiotemporal features, 2) it performs joint training of the LSTM and the saliency predictor, 3) it is very efficient. Specifically, although we cannot estimate the runtime for prior approaches, we believe that our method is much faster than most of the methods reported in Table 2 as these depend on features that are computationally expensive to extract. Our proposed method takes only 0.08s per clip for inference on GPU: 0.07s to compute C3D features and 0.01s to evaluate the RMDN."
    }, {
      "heading" : "4.2 ACTION RECOGNITION",
      "text" : "In order to show how saliency can be used for action recognition we carried out a set of experiments covering two scenarios: 1) using the same dataset where the saliency predictor was trained (Hollywood2) and 2) using a never-seen dataset with a different set of actions (UCF101).\nThe results on Hollywood2 are reported in terms of mean Average Precision (mAP) as done by Mathe & Sminchisescu (2015). Table 3 shows an analysis of 1) the impact of using different feature representations as well as 2) the effect of the saliency map. As in Tran et al. (2015), we experimented with different features, namely CONV5 and FC6, which correspond to the fifth convolutional layer and the first fully-connected representation of C3D, respectively. We also tested two ways to use the saliency maps, called in the second column: “feature” and “clip”. In the feature mode (first row, experiments (2-5)), the convolutional representation is multiplied by the saliency map, after resizing it accordingly. In other words, the saliency weights directly the feature representation, similarly to the work of Sharma et al. (2016). In the clip mode (second and third row, experiments (2-5)), we adopted the model presented in Sec. 3.4, where the saliency maps are used to weight the input video pixels.\nThe third column of Table 3 (experiment (1)) reports the results using only the original video as input to C3D (referred to as context in Fig. 2). Experiment (2) uses the ground truth saliency maps as soft attention to weight the input of C3D, while in experiment (3) this vector is concatenated with the context features. The last two columns (experiment (4) and (5)) represent the same setup, but in this case we use the saliency maps predicted by our model instead of the ground truth.\nTable 3 shows that the results of CONV5 and FC6 are very close when considering the original video (experiment (1)). The table also shows that the feature mode has lower performance compared to the clip mode (experiment (2)). Moreover, the concatenation (experiment (3)) is effective only when visual attention is used to weight pixels rather than features. Based on the poor performance of the features mode, we decided to experiment only with the clip mode in our study with predicted saliency (experiment (4) and (5)). Also, we decided to use FC6 features for the rest of the paper because the representation is more compact and therefore allows to train the classifiers more quickly. We can notice that even in the case of predicted saliency, the concatenation of FC6 context features and those obtained by weighting the input video with soft-attention (experiment (5)) produces a significant improvement over the original CONV5/FC6 features without attention. Furthermore, a pleasant surprise is represented by the the small difference in results between using the predicted saliency (experiment (5)) versus the ground truth maps (experiment (3)): only 0.27% for FC6 (last row).\nThe SVM model complexity for experiment (3) in Table 3 is twice as large as the complexity for (1) and (2) since the feature dimensionality is doubled by construction. The same applies when comparing (5) against (1) and (4). In order to have a more fair comparison, we added PCA dimensionality reduction to experiment (5) in order to match the same feature dimensionality as (1) (and (4)). Although the validation accuracies are very similar, the testing mAP drops from 54.85% of experiment (5) to 51.82% of the PCA experiment. This is not surprising, since the extra dimensions\nprovided by the use of the saliency map are not redundant with respect to the context representation (1). Therefore, concatenation seems to be an effective way to make use of the saliency map.\nTable 4 compares our action categorization results with those presented in Mathe & Sminchisescu (2015). As we did before, we separate experiments that use the ground truth maps and those that use predicted saliency. The results of Table 4 show that the performance our method (second and fifth column) is around 2% lower than Mathe & Sminchisescu (2015). However this is most likely explained by the differences in the type of features and classifier, and not by the differences in saliency map prediction methods. Indeed, we already established in Table 2 that our proposed saliency map predictor is more accurate than the one proposed in Mathe & Sminchisescu (2015). On the other hand, Mathe & Sminchisescu (2015) use a combination of many different features and a kernel chi-square SVM, while our method uses C3D features with a simple linear SVM classifier. Adding more non-linearities, especially for the concatenation experiment, would probably help. But we consider the experimentation with different types of action recognition features and classifiers out of the scope of this paper.\nFinally, we perform an experiment to assess the generalization abilities of the learned saliency model to a different dataset, with classes and videos that have not been seen during its training. To this end, we used the attentional model trained on the Hollywood2 dataset to extract saliency maps on the UCF101 dataset. As saliency ground truth is not available for UCF101, we evaluate performance in terms of action recognition accuracy using the evaluation protocol and splits by Soomro et al. (2012). Table 5 summarizes the results. The proposed method (C3D + RMDN, eight row) corresponds to the concatenation of the original C3D descriptor and the C3D descriptor with the input weighted by the saliency map, as was done in the Hollywood2 experiments. We compare our method with the results obtained using the C3D descriptor computed from the context only (seventh row) and other state-ofthe-art methods (first row through sixth row). A linear SVM trained on C3D features computed from the context already outperforms most of the other methods (first row to forth row). But training the linear SVM on a concatenation of context C3D features and those obtained by reweighting the video\ninput with the RMDN saliency maps (seventh row) leads to a further improvement of 1.1%. This is an impressive result since the RMDN was trained on the separate and small Hollywood2 dataset.\nSince we noticed that the saliency maps of RMDN for UCF101 tend to be highly peaked around a single location in each frame, we added the trained central bias (already analyzed in Table 1). This has the effect of diffusing the saliency map with the central bias, thus enlarging the area of attention used by the recognition system. The result of this experiment, which is reported in the last row of Table 5, further improves the accuracy by 1.3%."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "In this paper, we proposed a recurrent mixture density network for spatiotemporal visual attention. We showed that our model outperforms state-of-the-art methods for saliency prediction in videos. We have also shown that the saliency maps generated by our model can be leveraged to improve action categorization using a very simple procedure. This suggests that saliency can enrich the original video representation. The runtime overhead to estimate the saliency map is very small: only 0.01s added to the feature extraction time of 0.07s.\nAs future work, we plan to close the gap between RMDN and action recognition with a joint network. The idea is to have as output of the model both the saliency map at each time and the class of the action for the entire video. This can be combined with the idea of using the saliency map estimated at the previous time to weight the input for the current time. Putting together these two ideas in a single network would result in a joint model for saliency prediction and action recognition."
    }, {
      "heading" : "6 ACKNOWLEDGMENTS",
      "text" : "We thank Du Tran for helpful discussion about the code of the C3D network and its usage. We are grateful to Stefan Mathe for explaining the format of the eyetracking data and the protocol of the Hollywood2 experiment. This work was funded in part by NSF award CNS-1205521. We gratefully acknowledge NVIDIA for the donation of GPUs used for portions of this work."
    }, {
      "heading" : "A SALIENCY FOR CLASSIFICATION",
      "text" : "The proposed model for recognition is presented in Fig. 2. At each time t, we extract two representations: the context branch is given by the C3D representation of the original clip, while the soft attentional branch is given by the C3D representation of the input clip weighted by the saliency map. The two representations are then concatenated at the clip level and max-pooled over the video to obtain the final video-level descriptor. This video-level representation is then used as input to train the video classifier which is a linear SVM in our experiments.\nIn our experiments, we also evaluated the option of weighting the convolutional feature map xt instead of the input, as for example done by Sharma et al. (2016). However, we will see that soft-masking the input gives higher accuracy, probably because applying C3D’s non-linear transformation after the soft-weighting produces a representation that is less redundant with the original (non-masked) C3D representation.\nB IMPLEMENTATION DETAILS\nWe used the pretrained C3D network (Tran et al., 2015) as feature representation which is the input of the LSTM network. The convolutional layer before the fully-connected layers is used for saliency prediction, while the last fully-connected layer before the softmax is used for classification, since Tran et al. (2015) showed to obtain the best performance.\nThe training of the RMDN is performed using RMSprop with adaptive learning rate and gradient clipping. We start from a learning rate of 0.0003 and after 8 epochs it is reduced at each epoch with a decay factor of 0.95. The gradient is clipped with a threshold of 20. Dropout with a ratio of 0.5 is applied only on the hidden layer of the LSTM network before the MDN. We trained for 40 epochs, but training is stopped if there is no significative improvement of the loss. During training, temporal data augmentation is performed by clipping the videos to shorter videos of length 65 frames (which corresponds to 50 C3D descriptors since it needs a buffer of 16 frames for the first descriptor). The number of components of the GMM C is fixed to 20 for all the experiments. All the experiments were carried out using an NVIDIA Tesla K40 card.\nAfter extracting the saliency maps and the feature representations on GPU, our recognition experiments were performed on CPU using a linear SVM. In order to compute the video-level representation, we performed max pooling of the clip-level representations of the video. For all the experiments, we used 20% of the training data as validation set to find the regularization parameter of the SVM. We searched the parameter space on a grid between 10−9 to 103 with a step of 10 1 2 . Finally, we retrain the SVM on all the training set (including the validation set) using the best cross-validated parameter."
    } ],
    "references" : [ {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Ba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning attentional policies for object tracking and recognition in video with deep networks",
      "author" : [ "L. Bazzani", "N. de Freitas", "H. Larochelle", "V. Murino", "J-A Ting" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Bazzani et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bazzani et al\\.",
      "year" : 2011
    }, {
      "title" : "Mixture density networks",
      "author" : [ "Christopher M Bishop" ],
      "venue" : "Technical Report. Aston University,",
      "citeRegEx" : "Bishop.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1994
    }, {
      "title" : "State-of-the-art in visual attention modeling",
      "author" : [ "A. Borji", "L. Itti" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Borji and Itti.,? \\Q2013\\E",
      "shortCiteRegEx" : "Borji and Itti.",
      "year" : 2013
    }, {
      "title" : "Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study",
      "author" : [ "Ali Borji", "Dicky N Sihite", "Laurent Itti" ],
      "venue" : "Image Processing, IEEE Transactions on,",
      "citeRegEx" : "Borji et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Borji et al\\.",
      "year" : 2013
    }, {
      "title" : "A deeper look at saliency: Feature contrast, semantics, and beyond",
      "author" : [ "Neil DB Bruce", "Christopher Catton", "Sasa Janjic" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Bruce et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bruce et al\\.",
      "year" : 2016
    }, {
      "title" : "Transfer learning with deep networks for saliency prediction in natural video",
      "author" : [ "S. Chaabouni", "J. Benois-Pineau", "C. Ben Amar" ],
      "venue" : "IEEE International Conference on Image Processing (ICIP),",
      "citeRegEx" : "Chaabouni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chaabouni et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep learning for saliency prediction in natural video",
      "author" : [ "Souad Chaabouni", "Jenny Benois-Pineau", "Ofer Hadar", "Chokri Ben Amar" ],
      "venue" : "CoRR, abs/1604.08010,",
      "citeRegEx" : "Chaabouni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chaabouni et al\\.",
      "year" : 2016
    }, {
      "title" : "fmri studies of temporal attention: allocating attention within, or towards, time",
      "author" : [ "Jennifer T. Coull" ],
      "venue" : "Cognitive Brain Research,",
      "citeRegEx" : "Coull.,? \\Q2004\\E",
      "shortCiteRegEx" : "Coull.",
      "year" : 2004
    }, {
      "title" : "Learning where to attend with deep architectures for image tracking",
      "author" : [ "M. Denil", "L. Bazzani", "H. Larochelle", "N. de Freitas" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Denil et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2012
    }, {
      "title" : "Semiautomatic visual-attention modeling and its application to video compression",
      "author" : [ "Y. Gitman", "M. Erofeev", "D. Vatolin", "B. Andrey", "F. Alexey" ],
      "venue" : "In Image Processing (ICIP),",
      "citeRegEx" : "Gitman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gitman et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "CoRR, abs/1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra" ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform",
      "author" : [ "Chenlei Guo", "Qi Ma", "Liming Zhang" ],
      "venue" : "In Computer vision and pattern recognition,",
      "citeRegEx" : "Guo et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2008
    }, {
      "title" : "Graph-based visual saliency",
      "author" : [ "Jonathan Harel", "Christof Koch", "Pietro Perona" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Harel et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Harel et al\\.",
      "year" : 2006
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "How many bits does it take for a stimulus to be salient",
      "author" : [ "Sayed Hossein Khatoonabadi", "Nuno Vasconcelos", "Ivan V. Bajic", "Yufeng Shan" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Khatoonabadi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Khatoonabadi et al\\.",
      "year" : 2015
    }, {
      "title" : "Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks",
      "author" : [ "Xun Huang", "Chengyao Shen", "Xavier Boix", "Qi Zhao" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end saliency mapping via probability distribution prediction",
      "author" : [ "Saumya Jetley", "Naila Murray", "Eleonora Vig" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Jetley et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jetley et al\\.",
      "year" : 2016
    }, {
      "title" : "Saliency in crowd",
      "author" : [ "Ming Jiang", "Juan Xu", "Qi Zhao" ],
      "venue" : "In Computer Vision–ECCV",
      "citeRegEx" : "Jiang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to predict where humans look",
      "author" : [ "Tilke Judd", "Krista Ehinger", "Frédo Durand", "Antonio Torralba" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "Judd et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Judd et al\\.",
      "year" : 2009
    }, {
      "title" : "A benchmark of computational models of saliency to predict human fixations",
      "author" : [ "Tilke Judd", "Frédo Durand", "Antonio Torralba" ],
      "venue" : "In MIT Technical Report,",
      "citeRegEx" : "Judd et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Judd et al\\.",
      "year" : 2012
    }, {
      "title" : "Eye tracking assisted extraction of attentionally important objects from videos",
      "author" : [ "S. Karthikeyan", "Thuyen Ngo", "Miguel Eckstein", "B.S. Manjunath" ],
      "venue" : "In IEEE International Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Karthikeyan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karthikeyan et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep gaze I: boosting saliency prediction with feature maps trained on imagenet",
      "author" : [ "Matthias Kümmerer", "Lucas Theis", "Matthias Bethge" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kümmerer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kümmerer et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to combine foveal glimpses with a third-order Boltzmann machine",
      "author" : [ "Hugo Larochelle", "Geoffrey E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Larochelle and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Larochelle and Hinton.",
      "year" : 2010
    }, {
      "title" : "Visual saliency based on multiscale deep features",
      "author" : [ "Guanbin Li", "Yizhou Yu" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Li and Yu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li and Yu.",
      "year" : 2015
    }, {
      "title" : "Predicting eye fixations using convolutional neural networks",
      "author" : [ "Nian Liu", "Junwei Han", "Dingwen Zhang", "Shifeng Wen", "Tianming Liu" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Salient deconvolutional networks",
      "author" : [ "Aravindh Mahendran", "Andrea Vedaldi" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Mahendran and Vedaldi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mahendran and Vedaldi.",
      "year" : 2016
    }, {
      "title" : "Actions in context",
      "author" : [ "Marcin Marszałek", "Ivan Laptev", "Cordelia Schmid" ],
      "venue" : "In IEEE Conference on Computer Vision & Pattern Recognition,",
      "citeRegEx" : "Marszałek et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Marszałek et al\\.",
      "year" : 2009
    }, {
      "title" : "Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition",
      "author" : [ "S. Mathe", "C. Sminchisescu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Mathe and Sminchisescu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mathe and Sminchisescu.",
      "year" : 2015
    }, {
      "title" : "Encoding based saliency detection for videos and images",
      "author" : [ "Thomas Mauthner", "Horst Possegger", "Georg Waltner", "Horst Bischof" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Mauthner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mauthner et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "Aude Oliva", "Antonio Torralba" ],
      "venue" : "Int. J. Comput. Vision,",
      "citeRegEx" : "Oliva and Torralba.,? \\Q2001\\E",
      "shortCiteRegEx" : "Oliva and Torralba.",
      "year" : 2001
    }, {
      "title" : "End-to-end convolutional network for saliency prediction",
      "author" : [ "Junting Pan", "Xavier Giró i Nieto" ],
      "venue" : "CoRR, abs/1507.01422,",
      "citeRegEx" : "Pan and Nieto.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pan and Nieto.",
      "year" : 2015
    }, {
      "title" : "Learning video saliency from human gaze using candidate selection",
      "author" : [ "Dmitry Rudoy", "Dan B. Goldman", "Eli Shechtman", "Lihi Zelnik-Manor" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Rudoy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rudoy et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning discriminative space–time action parts from weakly labelled videos",
      "author" : [ "Michael Sapienza", "Fabio Cuzzolin", "Philip HS Torr" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Sapienza et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sapienza et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention for fine-grained categorization",
      "author" : [ "Pierre Sermanet", "Andrea Frome", "Esteban Real" ],
      "venue" : "CoRR, abs/1412.7054,",
      "citeRegEx" : "Sermanet et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sermanet et al\\.",
      "year" : 2014
    }, {
      "title" : "Action recognition using visual attention",
      "author" : [ "Shikhar Sharma", "Ryan Kiros", "Ruslan Salakhutdinov" ],
      "venue" : "In ICLR workshops,",
      "citeRegEx" : "Sharma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Two-stream convolutional networks for action recognition in videos",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Beyond frame-level cnn: Saliency-aware 3d cnn with lstm for video action recognition",
      "author" : [ "Jingkuan Song", "Heng Shen" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "Song and Shen,? \\Q2016\\E",
      "shortCiteRegEx" : "Song and Shen",
      "year" : 2016
    }, {
      "title" : "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "author" : [ "Khurram Soomro", "Amir Roshan Zamir", "Mubarak Shah" ],
      "venue" : "arXiv preprint arXiv:1212.0402,",
      "citeRegEx" : "Soomro et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Soomro et al\\.",
      "year" : 2012
    }, {
      "title" : "Unsupervised learning of video representations using lstms",
      "author" : [ "Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhudinov" ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Action from still image dataset and inverse optimal control to learn task specific visual scanpaths",
      "author" : [ ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mathe.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mathe.",
      "year" : 2013
    }, {
      "title" : "Learning spatiotemporal features with 3d convolutional networks",
      "author" : [ "Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Tran et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2015
    }, {
      "title" : "Space-variant descriptor sampling for action recognition based on saliency and eye movements",
      "author" : [ "Eleonora Vig", "Michael Dorr", "David Cox" ],
      "venue" : "In European conference on computer vision,",
      "citeRegEx" : "Vig et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vig et al\\.",
      "year" : 2012
    }, {
      "title" : "Action recognition with improved trajectories",
      "author" : [ "Heng Wang", "Cordelia Schmid" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Wang and Schmid.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang and Schmid.",
      "year" : 2013
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Describing Videos by Exploiting Temporal Structure",
      "author" : [ "Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "In IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Yao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2015
    }, {
      "title" : "Attentionnet: Aggregating weak directions for accurate object detection",
      "author" : [ "Donggeun Yoo", "Sunggyun Park", "Joon-Young Lee", "Anthony S. Paek", "In So Kweon" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Yoo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yoo et al\\.",
      "year" : 2015
    }, {
      "title" : "An object-based visual attention model for robotic applications",
      "author" : [ "Y. Yu", "G.K.I. Mann", "R.G. Gosine" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),",
      "citeRegEx" : "Yu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2010
    }, {
      "title" : "Beyond short snippets: Deep networks for video classification",
      "author" : [ "Joe Yue-Hei Ng", "Matthew Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Ng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual attention detection in video sequences using spatiotemporal cues",
      "author" : [ "Yun Zhai", "Mubarak Shah" ],
      "venue" : "In Proceedings of the 14th annual ACM international conference on Multimedia,",
      "citeRegEx" : "Zhai and Shah.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhai and Shah.",
      "year" : 2006
    }, {
      "title" : "Fixation bank: Learning to reweight fixation candidates",
      "author" : [ "Jiaping Zhao", "Christian Siagian", "Laurent Itti" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "A Neural Autoregressive Approach to Attentionbased Recognition",
      "author" : [ "Yin Zheng", "Yu-Jin Zhang", "Hugo Larochelle" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Zheng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "Interest in attentional models is primarily motivated by their ability to eliminate or down-weight pixels that are not important for the task at hand, as for example shown in prior work using visual attention for image recognition and caption generation (Sermanet et al., 2014; Xu et al., 2015; Mnih et al., 2014).",
      "startOffset" : 254,
      "endOffset" : 313
    }, {
      "referenceID" : 47,
      "context" : "Interest in attentional models is primarily motivated by their ability to eliminate or down-weight pixels that are not important for the task at hand, as for example shown in prior work using visual attention for image recognition and caption generation (Sermanet et al., 2014; Xu et al., 2015; Mnih et al., 2014).",
      "startOffset" : 254,
      "endOffset" : 313
    }, {
      "referenceID" : 31,
      "context" : "Interest in attentional models is primarily motivated by their ability to eliminate or down-weight pixels that are not important for the task at hand, as for example shown in prior work using visual attention for image recognition and caption generation (Sermanet et al., 2014; Xu et al., 2015; Mnih et al., 2014).",
      "startOffset" : 254,
      "endOffset" : 313
    }, {
      "referenceID" : 34,
      "context" : "Recently, we have witnessed a shift of trend from image saliency prediction (Borji & Itti, 2013) to the modeling of saliency in videos (Rudoy et al., 2013).",
      "startOffset" : 135,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "Since human fixation patterns are strongly correlated over time (Coull, 2004), it appears critical to model the relations between saliency maps of consecutive frames.",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 48,
      "context" : "Yet, for analysis tasks such as action recognition (Wang & Schmid, 2013) or video description (Yao et al., 2015), it is imperative to properly model the dynamical properties of these objects in the video.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 44,
      "context" : "1) that leverages deep 3D convolutional features (Tran et al., 2015) as semantic, spatiotemporal representation of short clips in the video.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "The LSTM model connects into a Mixture Density Network (MDN) (Bishop, 1994) that at each frame outputs the parameters of a Gaussian mixture model expressing the saliency map.",
      "startOffset" : 61,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "They include attention-based video compression (Gitman et al., 2014), visual attention for robots (Yu et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 50,
      "context" : ", 2014), visual attention for robots (Yu et al., 2010), crowd analysis for video surveillance (Jiang et al.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : ", 2010), crowd analysis for video surveillance (Jiang et al., 2014), salient object detection (Li & Yu, 2015; Karthikeyan et al.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : ", 2014), salient object detection (Li & Yu, 2015; Karthikeyan et al., 2015) and activity recognition (Vig et al.",
      "startOffset" : 34,
      "endOffset" : 75
    }, {
      "referenceID" : 45,
      "context" : ", 2015) and activity recognition (Vig et al., 2012; Sapienza et al., 2014).",
      "startOffset" : 33,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : ", 2015) and activity recognition (Vig et al., 2012; Sapienza et al., 2014).",
      "startOffset" : 33,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "In this context, motion features are introduced when extending saliency methods from images to videos (Guo et al., 2008; Zhao et al., 2015; Zhai & Shah, 2006).",
      "startOffset" : 102,
      "endOffset" : 158
    }, {
      "referenceID" : 53,
      "context" : "In this context, motion features are introduced when extending saliency methods from images to videos (Guo et al., 2008; Zhao et al., 2015; Zhai & Shah, 2006).",
      "startOffset" : 102,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "see Borji et al. (2013); Judd et al.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "see Borji et al. (2013); Judd et al. (2009); Harel et al.",
      "startOffset" : 4,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "see Borji et al. (2013); Judd et al. (2009); Harel et al. (2006) for recent examples).",
      "startOffset" : 4,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "see Borji et al. (2013); Judd et al. (2009); Harel et al. (2006) for recent examples). We refer to Borji & Itti (2013) and Bruce et al.",
      "startOffset" : 4,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "see Borji et al. (2013); Judd et al. (2009); Harel et al. (2006) for recent examples). We refer to Borji & Itti (2013) and Bruce et al. (2016) for an interesting analysis and comparison of existing methods.",
      "startOffset" : 4,
      "endOffset" : 143
    }, {
      "referenceID" : 47,
      "context" : "Specifically deep networks have been used in this context to assign a weight to each pixel in order to extract “glimpses” from images (Xu et al., 2015; Gregor et al., 2015)",
      "startOffset" : 134,
      "endOffset" : 172
    }, {
      "referenceID" : 12,
      "context" : "Specifically deep networks have been used in this context to assign a weight to each pixel in order to extract “glimpses” from images (Xu et al., 2015; Gregor et al., 2015)",
      "startOffset" : 134,
      "endOffset" : 172
    }, {
      "referenceID" : 48,
      "context" : "or videos (Yao et al., 2015) in the form of weighted pixel averages.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 9,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 31,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 47,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 36,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 0,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 49,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 54,
      "context" : "Other work has been geared towards learning hard-attentional models, which explicitly ignore and discard parts of the input (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Mnih et al., 2014; Xu et al., 2015; Sermanet et al., 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings.",
      "startOffset" : 124,
      "endOffset" : 307
    }, {
      "referenceID" : 30,
      "context" : "Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos.",
      "startOffset" : 12,
      "endOffset" : 161
    }, {
      "referenceID" : 23,
      "context" : "Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos.",
      "startOffset" : 12,
      "endOffset" : 161
    }, {
      "referenceID" : 34,
      "context" : "Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos.",
      "startOffset" : 12,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al.",
      "startOffset" : 8,
      "endOffset" : 889
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al.",
      "startOffset" : 8,
      "endOffset" : 910
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images.",
      "startOffset" : 8,
      "endOffset" : 934
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images. Pan & i Nieto (2015) followed the same principle but without using any pre-trained network for initialization.",
      "startOffset" : 8,
      "endOffset" : 1081
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images. Pan & i Nieto (2015) followed the same principle but without using any pre-trained network for initialization. Liu et al. (2015) proposed a multi-scale architecture for saliency prediction, and Li & Yu (2015) added a refinement step in order to enforce spatial coherence of the output.",
      "startOffset" : 8,
      "endOffset" : 1189
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images. Pan & i Nieto (2015) followed the same principle but without using any pre-trained network for initialization. Liu et al. (2015) proposed a multi-scale architecture for saliency prediction, and Li & Yu (2015) added a refinement step in order to enforce spatial coherence of the output.",
      "startOffset" : 8,
      "endOffset" : 1269
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images. Pan & i Nieto (2015) followed the same principle but without using any pre-trained network for initialization. Liu et al. (2015) proposed a multi-scale architecture for saliency prediction, and Li & Yu (2015) added a refinement step in order to enforce spatial coherence of the output. Simonyan et al. (2014) and Mahendran & Vedaldi (2016) proposed to reverse deep networks using deconvolutions for visualization and to estimate image saliency.",
      "startOffset" : 8,
      "endOffset" : 1369
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ba et al., 2015; Yoo et al., 2015; Zheng et al., 2015), thus providing significant computational savings. Unfortunately, such models are often hard to train because they rely on reinforcement learning techniques to generate the image/video locations during training. All of the aforementioned prior work attempts to learn attentional models indirectly rather than from explicit information of where humans look. Recent work (Mauthner et al., 2015; Hossein Khatoonabadi et al., 2015; Mathe & Sminchisescu, 2015; Stefan Mathe, 2013; Kümmerer et al., 2015; Rudoy et al., 2013) has shown that it may be possible to accurately reproduce gazing patterns of human subjects attending to images and videos. However, these prior approaches rely on hand-crafted features to estimate the saliency maps. Attempts at removing hand-engineering of features are represented by Jetley et al. (2016); Huang et al. (2015); Kümmerer et al. (2015) where networks pre-trained for object recognition were subsequently finetuned using saliency-based loss functions for images. Pan & i Nieto (2015) followed the same principle but without using any pre-trained network for initialization. Liu et al. (2015) proposed a multi-scale architecture for saliency prediction, and Li & Yu (2015) added a refinement step in order to enforce spatial coherence of the output. Simonyan et al. (2014) and Mahendran & Vedaldi (2016) proposed to reverse deep networks using deconvolutions for visualization and to estimate image saliency.",
      "startOffset" : 8,
      "endOffset" : 1400
    }, {
      "referenceID" : 44,
      "context" : "Furthermore, there is recently growing evidence (Tran et al., 2015; Srivastava et al., 2015; Yue-Hei Ng et al., 2015) that by modeling the temporal information it is possible to obtain improved performances in high-level video analysis tasks, such as action recognition.",
      "startOffset" : 48,
      "endOffset" : 117
    }, {
      "referenceID" : 42,
      "context" : "Furthermore, there is recently growing evidence (Tran et al., 2015; Srivastava et al., 2015; Yue-Hei Ng et al., 2015) that by modeling the temporal information it is possible to obtain improved performances in high-level video analysis tasks, such as action recognition.",
      "startOffset" : 48,
      "endOffset" : 117
    }, {
      "referenceID" : 44,
      "context" : "For the details about the size and stride of the convolutional and pooling kernels, we refer to (Tran et al., 2015).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : ", 2015; Srivastava et al., 2015; Yue-Hei Ng et al., 2015) that by modeling the temporal information it is possible to obtain improved performances in high-level video analysis tasks, such as action recognition. For the computation of spatiotemporal features from the input clip, we use the “C3D” architecture proposed by Tran et al. (2015), which has been shown to provide competitive results on video recognition tasks across different datasets.",
      "startOffset" : 8,
      "endOffset" : 340
    }, {
      "referenceID" : 2,
      "context" : "The resulting network is known as a Mixture Density Network (MDN) (Bishop, 1994; Graves, 2013).",
      "startOffset" : 66,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "The resulting network is known as a Mixture Density Network (MDN) (Bishop, 1994; Graves, 2013).",
      "startOffset" : 66,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "The MDN (Graves, 2013; Bishop, 1994) takes its inputs from the hidden representation of the LSTM network.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "The MDN (Graves, 2013; Bishop, 1994) takes its inputs from the hidden representation of the LSTM network.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "In particular, we refer to the paper of Graves (2013) for the derivation of the gradients for the MDN using the loss function of Eq.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "In particular, we refer to the paper of Graves (2013) for the derivation of the gradients for the MDN using the loss function of Eq. 6. In practice, due to the limited training data, we freeze the layers of the C3D network to the values pretrained by Tran et al. (2015) for action recognition.",
      "startOffset" : 40,
      "endOffset" : 270
    }, {
      "referenceID" : 28,
      "context" : "In this section, we evaluate the proposed method for both saliency prediction and action recognition on two challenging datasets: Hollywood2 (Marszałek et al., 2009) and UCF101 (Soomro et al.",
      "startOffset" : 141,
      "endOffset" : 165
    }, {
      "referenceID" : 41,
      "context" : ", 2009) and UCF101 (Soomro et al., 2012).",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "We evaluate all models on the test set, using popular metrics proposed in the literature of saliency map prediction for still images (Judd et al., 2012; Borji et al., 2013), such as Area Under the ROC Curve (AUC), Normalized Scanpath Saliency (NSS), linear Correlation Coefficient (CC) and the Similarity score (Sim).",
      "startOffset" : 133,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "We evaluate all models on the test set, using popular metrics proposed in the literature of saliency map prediction for still images (Judd et al., 2012; Borji et al., 2013), such as Area Under the ROC Curve (AUC), Normalized Scanpath Saliency (NSS), linear Correlation Coefficient (CC) and the Similarity score (Sim).",
      "startOffset" : 133,
      "endOffset" : 172
    }, {
      "referenceID" : 40,
      "context" : "Therefore, we used the Hollywood2 dataset, which was augmented with eye tracking data by Mathe & Sminchisescu (2015). We follow the same evaluation protocol (i.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 40,
      "context" : "Therefore, we used the Hollywood2 dataset, which was augmented with eye tracking data by Mathe & Sminchisescu (2015). We follow the same evaluation protocol (i.e., same training/test splits) of Mathe & Sminchisescu (2015) and their validation procedure to compute the final results in order to compare with their work.",
      "startOffset" : 89,
      "endOffset" : 222
    }, {
      "referenceID" : 40,
      "context" : "Therefore, we used the Hollywood2 dataset, which was augmented with eye tracking data by Mathe & Sminchisescu (2015). We follow the same evaluation protocol (i.e., same training/test splits) of Mathe & Sminchisescu (2015) and their validation procedure to compute the final results in order to compare with their work. Mathe & Sminchisescu (2015) generate the ground truth saliency from a binary fixation map where the only non-zero values are at fixations points.",
      "startOffset" : 89,
      "endOffset" : 347
    }, {
      "referenceID" : 40,
      "context" : "Therefore, we used the Hollywood2 dataset, which was augmented with eye tracking data by Mathe & Sminchisescu (2015). We follow the same evaluation protocol (i.e., same training/test splits) of Mathe & Sminchisescu (2015) and their validation procedure to compute the final results in order to compare with their work. Mathe & Sminchisescu (2015) generate the ground truth saliency from a binary fixation map where the only non-zero values are at fixations points. The final saliency map is produced by convolving the binary map with an isotropic Gaussian filter with standard deviation σ and then adding to it a uniform distribution with probability parameter p. As in Mathe & Sminchisescu (2015), the values of these two parameters are chosen from σ ∈ {1.",
      "startOffset" : 89,
      "endOffset" : 698
    }, {
      "referenceID" : 4,
      "context" : ", 2012; Borji et al., 2013), such as Area Under the ROC Curve (AUC), Normalized Scanpath Saliency (NSS), linear Correlation Coefficient (CC) and the Similarity score (Sim). We refer to the papers of Judd et al. (2012) and Borji et al.",
      "startOffset" : 8,
      "endOffset" : 218
    }, {
      "referenceID" : 4,
      "context" : ", 2012; Borji et al., 2013), such as Area Under the ROC Curve (AUC), Normalized Scanpath Saliency (NSS), linear Correlation Coefficient (CC) and the Similarity score (Sim). We refer to the papers of Judd et al. (2012) and Borji et al. (2013) for their detailed description.",
      "startOffset" : 8,
      "endOffset" : 242
    }, {
      "referenceID" : 43,
      "context" : "The last row in Table 1 shows the results obtained by retraining our model using the full training set of Mathe & Sminchisescu (2015) instead of just the 80% subset.",
      "startOffset" : 106,
      "endOffset" : 134
    }, {
      "referenceID" : 43,
      "context" : "The last row in Table 1 shows the results obtained by retraining our model using the full training set of Mathe & Sminchisescu (2015) instead of just the 80% subset. For this case (RMDN full) we used the hyper-parameter values selected via hold out-validation for the experiment in the fourth row. This gives the best result for saliency prediction reported in our work and it is the model we used for all the subsequent experiments described below. All of the experiments reported in Table 1 were obtained using C = 20 components in the GMM. We have also studied how the accuracy varies by reducing the value of C. For example, using the RMDN variant of row 5 in Table 1 but with C = 10 components (instead of 20), the performance does not change dramatically, yielding AUC= 0.8966 and NSS= 2.4392. On the other hand, the AUC and the NSS decrease considerably, by 1.3% and 0.3 points respectively, when using only C = 2 components (AUC= 0.8836 and NSS= 2.1385). Based on this analysis, in all our subsequent experiments we used C = 20 as we noticed that our approach implements automatically a sort of Occam’s razor, setting the weights πc of many components close to zero when necessary. We have also carried out a few side experiments and discovered that using the fully-connected features of C3D instead of the convolutional representation gives results that are at least 1.5% lower in terms of AUC. Moreover, we tried to finetune the C3D network for action categorization on Hollywood2. However we did not obtain any significant improvement, confirming the findings of Tran et al. (2015): the C3D representation is already general enough to perform effectively on different action recognition tasks and fine-tuning the model on smaller-scale datasets (such as Hollywood2) does not seem beneficial.",
      "startOffset" : 106,
      "endOffset" : 1593
    }, {
      "referenceID" : 43,
      "context" : "Table 2 includes the results of the best methods taken from the extensive analysis done in Mathe & Sminchisescu (2015). The table reports also some useful baselines, such as the central bias (CB) and the human accuracy for the task.",
      "startOffset" : 91,
      "endOffset" : 119
    }, {
      "referenceID" : 43,
      "context" : "Table 2 includes the results of the best methods taken from the extensive analysis done in Mathe & Sminchisescu (2015). The table reports also some useful baselines, such as the central bias (CB) and the human accuracy for the task. Note that: 1) CB differs from TCB, since it does not use any training fixations; and 2) the human accuracy is computed in (Mathe & Sminchisescu, 2015) by deriving a saliency map from half of our human subjects and is evaluated with respect to fixations of the remaining ones. Furthermore, Table 2 contrasts the use of static features, motion features and their combination. The last row reports the results obtained with our RMDN model. It is interesting to see that the results obtained with a single type of features (static or motion) have an AUC lower than 0.75, which is even lower than the one obtained by the central bias (0.84). Moreover, the combination reaches the best results when the central bias is combined with engineered features (SF+MF+CB). On the other hand, our method outperforms all the methods evaluated in Mathe & Sminchisescu (2015) by a large margin and our results are very close to human performance (the difference is only 3.",
      "startOffset" : 91,
      "endOffset" : 1091
    }, {
      "referenceID" : 20,
      "context" : "SF = Static Features Color features (Judd et al., 2009) 0.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "Combo (Mathe & Sminchisescu, 2015) SF (Judd et al., 2009) 0.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 42,
      "context" : "The results on Hollywood2 are reported in terms of mean Average Precision (mAP) as done by Mathe & Sminchisescu (2015). Table 3 shows an analysis of 1) the impact of using different feature representations as well as 2) the effect of the saliency map.",
      "startOffset" : 91,
      "endOffset" : 119
    }, {
      "referenceID" : 42,
      "context" : "The results on Hollywood2 are reported in terms of mean Average Precision (mAP) as done by Mathe & Sminchisescu (2015). Table 3 shows an analysis of 1) the impact of using different feature representations as well as 2) the effect of the saliency map. As in Tran et al. (2015), we experimented with different features, namely CONV5 and FC6, which correspond to the fifth convolutional layer and the first fully-connected representation of C3D, respectively.",
      "startOffset" : 91,
      "endOffset" : 277
    }, {
      "referenceID" : 37,
      "context" : "In other words, the saliency weights directly the feature representation, similarly to the work of Sharma et al. (2016). In the clip mode (second and third row, experiments (2-5)), we adopted the model presented in Sec.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 43,
      "context" : "The proposed method (RMDN) is compared to the approaches reported by Mathe & Sminchisescu (2015) (named as central bias and saliency sampling).",
      "startOffset" : 69,
      "endOffset" : 97
    }, {
      "referenceID" : 43,
      "context" : "The proposed method (RMDN) is compared to the approaches reported by Mathe & Sminchisescu (2015) (named as central bias and saliency sampling). Note that Mathe & Sminchisescu (2015) and RDMN do not use the same video classification model.",
      "startOffset" : 69,
      "endOffset" : 182
    }, {
      "referenceID" : 43,
      "context" : "Table 4 compares our action categorization results with those presented in Mathe & Sminchisescu (2015). As we did before, we separate experiments that use the ground truth maps and those that use predicted saliency.",
      "startOffset" : 75,
      "endOffset" : 103
    }, {
      "referenceID" : 43,
      "context" : "Table 4 compares our action categorization results with those presented in Mathe & Sminchisescu (2015). As we did before, we separate experiments that use the ground truth maps and those that use predicted saliency. The results of Table 4 show that the performance our method (second and fifth column) is around 2% lower than Mathe & Sminchisescu (2015). However this is most likely explained by the differences in the type of features and classifier, and not by the differences in saliency map prediction methods.",
      "startOffset" : 75,
      "endOffset" : 354
    }, {
      "referenceID" : 43,
      "context" : "Table 4 compares our action categorization results with those presented in Mathe & Sminchisescu (2015). As we did before, we separate experiments that use the ground truth maps and those that use predicted saliency. The results of Table 4 show that the performance our method (second and fifth column) is around 2% lower than Mathe & Sminchisescu (2015). However this is most likely explained by the differences in the type of features and classifier, and not by the differences in saliency map prediction methods. Indeed, we already established in Table 2 that our proposed saliency map predictor is more accurate than the one proposed in Mathe & Sminchisescu (2015). On the other hand, Mathe & Sminchisescu (2015) use a combination of many different features and a kernel chi-square SVM, while our method uses C3D features with a simple linear SVM classifier.",
      "startOffset" : 75,
      "endOffset" : 668
    }, {
      "referenceID" : 43,
      "context" : "Table 4 compares our action categorization results with those presented in Mathe & Sminchisescu (2015). As we did before, we separate experiments that use the ground truth maps and those that use predicted saliency. The results of Table 4 show that the performance our method (second and fifth column) is around 2% lower than Mathe & Sminchisescu (2015). However this is most likely explained by the differences in the type of features and classifier, and not by the differences in saliency map prediction methods. Indeed, we already established in Table 2 that our proposed saliency map predictor is more accurate than the one proposed in Mathe & Sminchisescu (2015). On the other hand, Mathe & Sminchisescu (2015) use a combination of many different features and a kernel chi-square SVM, while our method uses C3D features with a simple linear SVM classifier.",
      "startOffset" : 75,
      "endOffset" : 716
    }, {
      "referenceID" : 41,
      "context" : "As saliency ground truth is not available for UCF101, we evaluate performance in terms of action recognition accuracy using the evaluation protocol and splits by Soomro et al. (2012). Table 5 summarizes the results.",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 42,
      "context" : "6% LSTM composite model (Srivastava et al., 2015) 75.",
      "startOffset" : 24,
      "endOffset" : 49
    } ],
    "year" : 2017,
    "abstractText" : "In many computer vision tasks, the relevant information to solve the problem at hand is mixed with irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations at clip level and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}