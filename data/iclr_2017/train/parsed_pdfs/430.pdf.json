{
  "name" : "430.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LATENT SEQUENCE DECOMPOSITIONS",
    "authors" : [ "William Chan", "Yu Zhang", "Quoc V. Le", "Navdeep Jaitly" ],
    "emails" : [ "williamchan@cmu.edu", "yzhang87@mit.edu", "qvl@google.com", "ndjaitly@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Sequence-to-sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014) with attention (Bahdanau et al., 2015) have been successfully applied to many applications including machine translation (Luong et al., 2015; Jean et al., 2015), parsing (Vinyals et al., 2015a), image captioning (Vinyals et al., 2015b; Xu et al., 2015) and Automatic Speech Recognition (ASR) (Chan et al., 2016; Bahdanau et al., 2016a).\nPrevious work has assumed a fixed deterministic decomposition for each output sequence. The output representation is usually a fixed sequence of words (Sutskever et al., 2014; Cho et al., 2014), phonemes (Chorowski et al., 2015), characters (Chan et al., 2016; Bahdanau et al., 2016a) or even a mixture of characters and words (Luong & Manning, 2016). However, in all these cases, the models are trained towards one fixed decomposition for each output sequence.\nWe argue against using fixed deterministic decompositions of a sequence that has been defined a priori. Word segmented models (Luong et al., 2015; Jean et al., 2015) often have to deal with large softmax sizes, rare words and Out-of-Vocabulary (OOV) words. Character models (Chan et al., 2016; Bahdanau et al., 2016a) overcome the OOV problem by modelling the smallest output unit, however this typically results in long decoder lengths and computationally expensive inference. And even with mixed (but fixed) character-word models (Luong & Manning, 2016), it is unclear whether such a predefined segmentation is optimal. In all these examples, the output decomposition is only\n∗Work done at Google Brain.\na function of the output sequence. This may be acceptable for problems such as translations, but inappropriate for tasks such as speech recognition, where segmentation should also be informed by the characteristics of the inputs, such as audio.\nWe want our model to have the capacity and flexibility to learn a distribution of sequence decompositions. Additionally, the decomposition should be a sequence of variable length tokens as deemed most probable. For example, language may be more naturally represented as word pieces (Schuster & Nakajima, 2012) rather than individual characters. In many speech and language tasks, it is probably more efficient to model “qu” as one output unit rather than “q” + “u” as separate output units (since in English, “q” is almost always followed by “u”). Word piece models also naturally solve rare word and OOV problems similar to character models.\nThe output sequence decomposition should be a function of both the input sequence and the output sequence (rather than output sequence alone). For example, in speech, the choice of emitting “ing” as one word piece or as separate tokens of “i” + “n” + “g” should be a function of the current output word as well as the audio signal (i.e., speaking style).\nWe present the Latent Sequence Decompositions (LSD) framework. LSD does not assume a fixed decomposition for an output sequence, but rather learns to decompose sequences as function of both the input and the output sequence. Each output sequence can be decomposed to a set of latent sequence decompositions using a dictionary of variable length output tokens. The LSD framework produces a distribution over the latent sequence decompositions and marginalizes over them during training. During test inference, we find the best decomposition and output sequence, by using beam search to find the most likely output sequence from the model."
    }, {
      "heading" : "2 LATENT SEQUENCE DECOMPOSITIONS",
      "text" : "In this section, we describe LSD more formally. Let x be our input sequence, y be our output sequence and z be a latent sequence decomposition of y. The latent sequence decomposition z consists of a sequence of zi ∈ Z where Z is the constructed token space. Each token zi need not be the same length, but rather in our framework, we expect the tokens to have different lengths. Specifically, Z ⊆ ∪ni=1Ci where C is the set of singleton tokens and n is the length of the largest output token. In ASR , C would typically be the set of English characters, while Z would be word pieces (i.e., n-grams of characters).\nTo give a concrete example, consider a set of tokens {“a”, “b”, “c”, “at”, “ca”, “cat”}. With this set of tokens, the word “cat” may be represented as the sequence “c”, “a”, “t”, or the sequence “ca”, “t”, or alternatively as the single token “cat”. Since the appropriate decomposition of the word “cat” is not known a priori, the decomposition itself is latent.\nNote that the length |za| of a decomposition za need not be the same as the length of the output sequence, |y| (for example “ca”, “t” has a length of 2, whereas the sequence is 3 characters long). Similarly, a different decomposition zb (for example the 3-gram token “cat”) of the same sequence may be of a different length (in this case 1).\nEach decomposition, collapses to the target output sequence using a trivial collapsing function y = collapse(z). Clearly, the set of decompositions, {z : collapse(z) = y}, of a sequence, y, using a non-trivial token set, Z , can be combinatorially large. If there was a known, unique, correct segmentation z∗ for a given pair, (x,y), one could simply train the model to output the fixed deterministic decomposition z∗. However, in most problems, we do not know the best possible decomposition z∗; indeed it may be possible that the output can be correctly decomposed into multiple alternative but valid segmentations. For example, in end-to-end ASR we typically use characters as the output unit of choice (Chan et al., 2016; Bahdanau et al., 2016a) but word pieces may be better units as they more closely align with the acoustic entities such as syllables. However, the most appropriate decomposition z∗ for a given is (x,y) pair is often unknown. Given a particular y, the best z∗ could even change depending on the input sequence x (i.e., speaking style).\nIn LSD, we want to learn a probabilistic segmentation mapping from x → z → y. The model produces a distribution of decompositions, z, given an input sequence x, and the objective is to maximize the log-likelihood of the ground truth sequence y. We can accomplish this by factorizing\nand marginalizing over all possible z latent sequence decompositions under our model p(z|x; θ) with parameters θ:\nlog p(y|x; θ) = log ∑ z p(y, z|x; θ) (1)\n= log ∑ z p(y|z,x)p(z|x; θ) (2)\n= log ∑ z p(y|z)p(z|x; θ) (3)\nwhere p(y|z) = 1(collapse(z) = y) captures path decompositions z that collapses to y. Due to the exponential number of decompositions of y, exact inference and search is intractable for any nontrivial token set Z and sequence length |y|. We describe a beam search algorithm to do approximate inference decoding in Section 4.\nSimilarly, computing the exact gradient is intractable. However, we can derive a gradient estimator by differentiating w.r.t. to θ and taking its expectation:\n∂ ∂θ log p(y|x; θ) = 1 p(y|x; θ) ∂ ∂θ ∑ z p(y|x, z)p(z|x; θ) (4)\n= 1 p(y|x; θ) ∑ z p(y|x, z)∇θp(z|x; θ) (5)\n= 1 p(y|x; θ) ∑ z p(y|x, z)p(z|x; θ)∇θ log p(z|x; θ) (6)\n= Ez∼p(z|x,y;θ) [∇θ log p(z|x; θ)] (7)\nEquation 6 uses the identity ∇θfθ(x) = fθ(x)∇θ log fθ(x) assuming fθ(x) 6= 0 ∀ x. Equation 7 gives us an unbiased estimator of our gradient. It tells us to sample some latent sequence decomposition z ∼ p(z|y,x; θ) under our model’s posterior, where z is constraint to be a valid sequence that collapses to y, i.e. z ∈ {z′ : collapse(z′) = y}. To train the model, we sample z ∼ p(z|y,x; θ) and compute the gradient of ∇θ log p(z|x; θ) using backpropagation. However, sampling z ∼ p(z|y,x; θ) is difficult. Doing this exactly is computationally expensive, because it would require sampling correctly from the posterior – it would be possible to do this using a particle filtering like algorithm, but would require a full forward pass through the output sequence to do this.\nInstead, in our implementation we use a heuristic to sample z ∼ p(z|y,x; θ). At each output time step t when producing tokens z1, z2 · · · z(t−1), we sample from zt ∼ p (zt|x,y, z<t, θ) in a left-toright fashion. In other words, we sample valid extensions at each time step t. At the start of the training, this left-to-right sampling procedure is not a good approximation to the posterior, since the next step probabilities at a time step include probabilities of all future paths from that point.\nFor example, consider the case when the target word is “cat”, and the vocabulary includes all possible characters and the tokens “ca”, and “cat”. At time step 1, when the valid next step options are “c”, “ca”, “cat”, their relative probabilities reflect all possible sequences “c*”, “ca*”, “cat*” respectively, that start from the first time step of the model. These sets of sequences include sequences other than the target sequence “cat”. Thus sampling from the distribution at step 1 is a biased procedure.\nHowever, as training proceeds the model places more and more mass only on the correct hypotheses, and the relative probabilities that the model produces between valid extensions gets closer to the posterior. In practice, we find that the when the model is trained with this method, it quickly collapses to using single character targets, and never escapes from this local minima1. Thus, we follow an -greedy exploration strategy commonly found in reinforcement learning literature (Sutton & Barto, 1998) – we sample zt from a mixture of a uniform distribution over valid next tokens and p (zt|x,y, z<t, θ). The relative probability of using a uniform distribution vs. p (·|x,y, z<t, θ) is varied over training. With this modification the model learns to use the longer n-grams of characters appropriately, as shown in later sections.\n1One notable exception was the word piece “qu” (“u” is almost always followed by “q” in English). The model does learn to consistently emit “qu” as one token and never produce “q” + “u” as separate tokens."
    }, {
      "heading" : "3 MODEL",
      "text" : "In this work, we model the latent sequence decompositions p(z|x) with an attention-based seq2seq model (Bahdanau et al., 2015). Each output token zi is modelled as a conditional distribution over all previously emitted tokens z<i and the input sequence x using the chain rule:\np(z|x; θ) = ∏ i p(zi|x, z<i) (8)\nThe input sequence x is processed through an EncodeRNN network. The EncodeRNN function transforms the features x into some higher level representation h. In our experimental implementation EncodeRNN is a stacked Bidirectional LSTM (BLSTM) (Schuster & Paliwal, 1997; Graves et al., 2013) with hierarchical subsampling (Hihi & Bengio, 1996; Koutnik et al., 2014):\nh = EncodeRNN(x) (9)\nThe output sequence z is generated with an attention-based transducer (Bahdanau et al., 2015) one zi token at a time:\nsi = DecodeRNN([zi−1, ci−1], si−1) (10) ci = AttentionContext(si,h) (11)\np(zi|x, z<i) = TokenDistribution(si, ci) (12)\nThe DecodeRNN produces a transducer state si as a function of the previously emitted token zi−1, previous attention context ci−1 and previous transducer state si−1. In our implementation, DecodeRNN is a LSTM (Hochreiter & Schmidhuber, 1997) function without peephole connections.\nThe AttentionContext function generates ci with a content-based MLP attention network (Bahdanau et al., 2015). Energies ei are computed as a function of the encoder features h and current transducer state si. The energies are normalized into an attention distribution αi. The attention context ci is created as a αi weighted linear sum over h:\nei,j = 〈v, tanh(φ(si, hj))〉 (13) αi,j = exp(ei,j)∑ j′ exp(ei,j′) (14)\nci = ∑ j αi,jhj (15)\nwhere φ is linear transform function. TokenDistribution is a MLP function with softmax outputs modelling the distribution p(zi|x, z<i)."
    }, {
      "heading" : "4 DECODING",
      "text" : "During inference we want to find the most likely word sequence given the input acoustics:\nŷ = argmax y ∑ z log p(y|z)p(z|x) (16)\nhowever this is obviously intractable for any non-trivial token space and sequence lengths. We simply approximate this by decoding for the best word piece sequence ẑ and then collapsing it to its corresponding word sequence ŷ:\nẑ = argmax z\nlog p(z|x) (17)\nŷ = collapse(ẑ) (18)\nWe approximate for the best ẑ sequence by doing a left-to-right beam search (Chan et al., 2016)."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We experimented with the Wall Street Journal (WSJ) ASR task. We used the standard configuration of train si284 dataset for training, dev93 for validation and eval92 for test evaluation. Our input features were 80 dimensional filterbanks computed every 10ms with delta and delta-delta acceleration normalized with per speaker mean and variance as generated by Kaldi (Povey et al., 2011). The EncodeRNN function is a 3 layer BLSTM with 256 LSTM units per-direction (or 512 total) and 4 = 22 time factor reduction. The DecodeRNN is a 1 layer LSTM with 256 LSTM units. All the weight matrices were initialized with a uniform distribution U(−0.075, 0.075) and bias vectors to 0. Gradient norm clipping of 1 was used, gaussian weight noise N (0, 0.075) and L2 weight decay 1e−5 (Graves, 2011). We used ADAM with the default hyperparameters described in (Kingma & Ba, 2015), however we decayed the learning rate from 1e−3 to 1e−4. We used 8 GPU workers for asynchronous SGD under the TensorFlow framework (Abadi et al., 2015). We monitor the dev93 Word Error Rate (WER) until convergence and report the corresponding eval92 WER. The models took around 5 days to converge.\nWe created our token vocabularyZ by looking at the n-gram character counts of the training dataset. We explored n ∈ {2, 3, 4, 5} and took the top {256, 512, 1024} tokens based on their count frequencies (since taking the full n-cartesian exponent of the unigrams would result in an intractable number of tokens for n > 2). We found very minor differences in WER based on the vocabulary size, for our n = {2, 3} word piece experiments we used a vocabulary size of 256 while our n = {4, 5} word piece experiments used a vocabulary size of 512. Additionally, we restrict 〈space〉 to be a unigram token and not included in any other word pieces, this forces the decompositions to break on word boundaries.\nTable 1 compares the effect of varying the n sized word piece vocabulary. The Latent Sequence Decompositions (LSD) models were trained with the framework described in Section 2 and the (Maximum Extension) MaxExt decomposition is a fixed decomposition. MaxExt is generated in a left-to-right fashion, where at each step the longest word piece extension is selected from the vocabulary. The MaxExt decomposition is not the shortest |z| possible sequence, however it is a deterministic decomposition that can be easily generated in linear time on-the-fly. We decoded these models with simple n-best list beam search without any external dictionary or Language Model (LM).\nThe baseline model is simply the unigram or character model and achieves 14.76% WER. We find the LSD n = 4 word piece vocabulary model to perform the best at 12.88% WER or yielding a 12.7% relative improvement over the baseline character model. None of our MaxExt models beat our character model baseline, suggesting the maximum extension decomposition to be a poor decomposition choice. However, all our LSD models perform better than the baseline suggesting the LSD framework is able to learn a decomposition better than the baseline character decomposition.\nWe also look at the distribution of the characters covered based on the word piece lengths during inference across different n sized word piece vocabulary used in training. We define the distribution of the characters covered as the percentage of characters covered by the set of word pieces with the same length across the test set, and we exclude 〈space〉 in this statistic. Figure 1 plots the\ndistribution of the {1, 2, 3, 4, 5}-ngram word pieces the model decides to use to decompose the sequences. When the model is trained to use the bigram word piece vocabulary, we found the model to prefer bigrams (55% of the characters emitted) over characters (45% of the characters emitted) in the LSD decomposition. This suggest that a character only vocabulary may not be the best vocabulary to learn from. Our best model, LSD with n = 4 word piece vocabulary, covered the word characters with 42.16%, 39.35%, 14.83% and 3.66% of the time using 1, 2, 3, 4 sized word pieces respectively. In the n = 5 word piece vocabulary model, the LSD model uses the n = 5 sized word pieces to cover approximately 2% of the characters. We suspect if we used a larger dataset, we could extend the vocabulary to cover even larger n ≥ 5. The MaxExt model were trained to greedily emit the longest possible word piece, consequently this prior meant the model will prefer to emit long word pieces over characters. While this decomposition results in the shorter |z| length, the WER is slightly worse than the character baseline. This suggest the much shorter decompositions generated by the MaxExt prior may not be best decomposition. This falls onto the principle that the best z∗ decomposition is not only a function of y∗ but as a function of (x,y∗). In the case of ASR, the segmentation is a function of the acoustics as well as the text.\nTable 2 compares our WSJ results with other published end-to-end models. The best CTC model achieved 27.3% WER with REINFORCE optimization on WER (Graves & Jaitly, 2014). The previously best reported basic seq2seq model on WSJ WER achieved 18.0% WER (Bahdanau et al., 2016b) with Task Loss Estimation (TLE). Our baseline, also a seq2seq model, achieved 14.8% WER. Main differences between our models is that we did not use convolutional locational-based\npriors and we used weight noise during training. The deep CNN model with residual connections, batch normalization and convolutions achieved a WER of 11.8% (Zhang et al., 2017) 2.\nOur LSD model using a n = 4 word piece vocabulary achieves a WER of 12.9% or 12.7% relatively better over the baseline seq2seq model. If we combine our LSD model with the CNN (Zhang et al., 2017) model, we achieve a combined WER of 9.6% WER or 35.1% relatively better over the baseline seq2seq model. These numbers are all reported without the use of any language model.\nPlease see Appendix A for the decompositions generated by our model. The LSD model learns multiple word piece decompositions for the same word sequence."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "Singh et al. (2002); McGraw et al. (2013); Lu et al. (2013) built probabilistic pronunciation models for Hidden Markov Model (HMM) based systems. However, such models are still constraint to the conditional independence and Markovian assumptions of HMM-based systems.\nConnectionist Temporal Classification (CTC) (Graves et al., 2006; Graves & Jaitly, 2014) based models assume conditional independence, and can rely on dynamic programming for exact inference. Similarly, Ling et al. (2016) use latent codes to generate text, and also assume conditional independence and leverage on dynamic programming for exact maximum likelihood gradients. Such models can not learn the output language if the language distribution is multimodal. Our seq2seq models makes no such Markovian assumptions and can learn multimodal output distributions. Collobert et al. (2016) and Zweig et al. (2016) developed extensions of CTC where they used some word pieces. However, the word pieces are only used in repeated characters and the decompositions are fixed.\nWord piece models with seq2seq have also been recently used in machine translation. Sennrich et al. (2016) used word pieces in rare words, while Wu et al. (2016) used word pieces for all the words, however the decomposition is fixed and defined by heuristics or another model. The decompositions in these models are also only a function of the output sequence, while in LSD the decomposition is a\n2 For our CNN architectures, we use and compare to the “(C (3 × 3) / 2) × 2 + NiN” architecture from Table 2 line 4.\nfunction of both the input and output sequence. The LSD framework allows us to learn a distribution of decompositions rather than learning just one decomposition defined by a priori.\nVinyals et al. (2016) used seq2seq to outputs sets, the output sequence is unordered and used fixed length output units; in our decompositions we maintain ordering use variable lengthed output units. Reinforcement learning (i.e., REINFORCE and other task loss estimators) (Sutton & Barto, 1998; Graves & Jaitly, 2014; Ranzato et al., 2016) learn different output sequences can yield different task losses. However, these methods don’t directly learn different decompositions of the same sequence. Future work should incorporate LSD with task loss optimization methods."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We presented the Latent Sequence Decompositions (LSD) framework. LSD allows us to learn decompositions of sequences that are a function of both the input and output sequence. We presented a biased training algorithm based on sampling valid extensions with an -greedy strategy, and an approximate decoding algorithm. On the Wall Street Journal speech recognition task, the sequenceto-sequence character model baseline achieves 14.8% WER while the LSD model achieves 12.9%. Using a a deep convolutional neural network on the encoder with LSD, we achieve 9.6% WER."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Ashish Agarwal, Philip Bachman, Dzmitry Bahdanau, Eugene Brevdo, Jan Chorowski, Jeff Dean, Chris Dyer, Gilbert Leung, Mohammad Norouzi, Noam Shazeer, Xin Pan, Luke Vilnis, Oriol Vinyals and the Google Brain team for many insightful discussions and technical assistance."
    }, {
      "heading" : "A LEARNING THE DECOMPOSITIONS",
      "text" : "We give the top 8 hypothesis generated by a baseline seq2seq character model, a Latent Sequence Decompositions (LSD) word piece model and a Maximum Extension (MaxExt) word piece model. We note that “shamrock’s” is an out-of-vocabulary word while “shamrock” is in-vocabulary. The ground truth is “shamrock’s pretax profit from the sale was one hundred twenty five million dollars a spokeswoman said”. Note how the LSD model generates multiple decompostions for the same word sequence, this does not happen with the MaxExt model.\nTa bl\ne 3:\nTo p\nhy po\nth es\nis co\nm pa\nrs io\nn be\ntw ee\nn se\nq2 se\nq ch\nar ac\nte rm\nod el\n,L SD\nw or\nd pi\nec e\nm od\nel an\nd M\nax E\nxt w\nor d\npi ec\ne m\nod el . n H yp ot he si s\nL og\nPr ob\nR ef\ner en ce - sh am\nro ck\n’s pr\net ax\npr ofi\ntf ro\nm th\ne sa\nle w\nas on\ne hu\nnd re\nd tw\nen ty\nfiv e\nm ill\nio n\ndo lla\nrs a\nsp ok\nes w\nom an\nsa id\n-\nC ha\nra ct\ner se\nq2 se q 1 c| h| a| m |r| o| c| k|\n’|s ||\np| r|e\n|t| a|\nx| |p\n|r| o|\nf|i |t|\n|f| r|o\n|m ||\nt|h |e\n|| s|a\n|l| e|\n|w |a\n|s| |o\n|n |e\n|| h|\nu| n|\nd| r|e\n|d ||\nt|w |e\n|n |t|\ny| |f|\ni|v |e\n|| m\n|i| l|l\n|i| o|\nn| |d\n|o |l|\nl|a |r|\ns| |a\n|| s|p\n|o |k\n|e |s|\nw |o\n|m |a\n|n ||\ns|a |i|\nd -1\n.3 73\n86 8\n2 c|\nh| a|\nm |r|\no| x|\n|p |r|\ne| t|a\n|x ||\np| r|o\n|f| i|t\n|| f|r\n|o |m\n|| t|h\n|e ||\ns|a |l|\ne| |w\n|a |s|\n|o |n\n|e ||\nh| u|\nn| d|\nr|e |d\n|| t|w\n|e |n\n|t| y|\n|f| i|v\n|e ||\nm |i|\nl|l |i|\no| n|\n|d |o\n|l| l|a\n|r| s|\n|a ||\ns|p |o\n|k |e\n|s| w\n|o |m\n|a |n\n|| s|a\n|i| d\n-2 .2\n53 58 1 3 c| h| a| m |r| o| c| k| s| |p |r| e| t|a |x || p| r|o |f| i|t || f|r |o |m || t|h |e || s|a |l| e| |w |a |s| |o |n |e || h| u| n| d| r|e |d || t|w |e |n |t| y| |f| i|v |e || m |i| l|l |i| o| n| |d |o |l| l|a |r| s| |a || s|p |o |k |e |s| w |o |m |a |n || s|a |i| d -3 .4 82 71 3 4 c| h| a| m |r| o| c| k| ’|s || p| r|e |t| a| x| |p |r| o| f|i |t| |f| r|o |m || t|h |e || s|a |l| e| |w |a |s| |o |n |e || h| u| n| d| r|e |d || t|w |e |n |t| y| |f| i|v |e || m |i| l|l |i| o| n| |d |o |l| l|a |r| s| |o |f| |s| p| o| k| e| s|w |o |m |a |n || s|a |i| d -3 .4 93 95 7 5 c| h| a| m |r| o| d| ’|s || p| r|e |t| a| x| |p |r| o| f|i |t| |f| r|o |m || t|h |e || s|a |l| e| |w |a |s| |o |n |e || h| u| n| d| r|e |d || t|w |e |n |t| y| |f| i|v |e || m |i| l|l |i| o| n| |d |o |l| l|a |r| s| |a || s|p |o |k |e |s| w |o |m |a |n || s|a |i| d -3 .8 85 18 5 6 c| h| a| m |r| o| x| |p |r| e| t|a |x || p| r|o |f| i|t || f|r |o |m || t|h |e || s|a |l| e| |w |a |s| |o |n |e || h| u| n| d| r|e |d || t|w |e |n |t| y| |f| i|v |e || m |i| l|l |i| o| n| |d |o |l| l|a |r| s| |o |f| |s| p| o| k| e| s|w |o |m |a |n || s|a |i| d -4 .3 73 68 7 6 c| h| a| m |r| o| c| ’|s || p| r|e |t| a| x| |p |r| o| f|i |t| |f| r|o |m || t|h |e || s|a |l| e| |w |a |s| |o |n |e || h| u| n| d| r|e |d || t|w |e |n |t| y| |f| i|v |e || m |i| l|l |i| o| n| |d |o |l| l|a |r| s| |a || s|p |o |k |e |s| w |o |m |a |n || s|a |i| d -5 .1 48 48 4 8 c| h| a| m |r| o| c| k| s| |p |r| e| t|a |x || p| r|o |f| i|t || f|r |o |m || t|h |e || s|a |l| e| |w |a |s| |o |n |e || h| u| n| d| r|e |d || t|w |e |n |t| y| |f| i|v |e || m |i| l|l |i| o| n| |d |o |l| l|a |r| s| |o |f| |s| p| o| k| e| s|w |o |m |a |n || s|a |i| d -5 .6 02 79 3 W or d Pi ec e M od el M ax im um E xt en si on 1 sh |a m |ro |c k| ’s || pr e| ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -1 .1 55 20 3 2 sh |a m |ro |x || pr e| ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -3 .0 31 33 0 3 sh |a r|r o| x| |p re |ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -3 .0 74 76 2 4 sh |e || m || ro |x || pr e| ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -3 .8 15 66 2 5 sh |e || m ar |x || pr e| ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -3 .8 80 76 0 6 sh |a r|r o| ck |s| |p re |ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -4 .0 83 27 4 7 sh |e || m || ro |c k| ed || pr e| ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -4 .8 78 02 5 8 sh |e || m || ro |c k| s| |p re |ta |x || pr o| fi| t| |fr om || th e| |sa |le || w as || on e| |h u| nd |re d| |tw |e nt |y || fiv e| |m il| lio n| |d ol l|a rs || a| |sp |o k| es |w o| m an || sa id -5 .1 21 49 0 W or d Pi ec e M od el L at en tS eq ue nc e D ec om po si tio ns 1 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| lio |n || do ll| a| r|s || a| |sp |o k| e| s|w o| m a| n| |sa id -2 8. 11 14 85 2 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| li| o| n| |d ol l|a r|s || a| |sp |o k| e| s|w o| m a| n| |sa id -2 8. 17 28 78 3 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| lio |n || do ll| a| r|s || a| |sp |o k| e| s|w |o m |a |n || sa id -2 8. 45 33 81 4 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| li| o| n| |d ol l|a |r| s| |a || sp |o k| e| s|w |o m |a |n || sa id -2 9. 10 31 84 5 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| lio |n || do ll| a| r|s || a| |sp |o k| e| s|w |o m |a |n || sa |id -2 9. 15 96 60 6 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| lio |n || do ll| a| r|s || a| |sp |o |k |e |s| w |o |m a| n| |sa id -2 9. 16 41 41 7 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| li| o| n| |d ol l|a |r| s| |a || sp |o k| e| s|w |o m |a |n || sa i|d -2 9. 16 93 10 8 sh |a |m |ro |c |k |’s || pr e| ta |x || pr o| fi| t| |fr o| m || t|h |e || sa |l| e| |w as || on |e || hu |n |d r|e |d || t|w e| nt |y || fiv |e || m il| li| o| n| |d ol l|a |r| s| |a || sp |o k| e| s|w |o m |a |n || sa |id -2 9. 80 99 37"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "cent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation by Jointly Learning to Align and Translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Endto-end Attention-based Large Vocabulary Speech Recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Task Loss Estimation for Sequence Prediction",
      "author" : [ "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Philemon Brakel", "Nan Rosemary Ke", "Jan Chorowski", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In International Conference on Learning Representations Workshop,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Chan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwen", "Yoshua Bengio" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention-Based Models for Speech Recognition",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Chorowski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Wav2Letter: an End-to-End ConvNetbased Speech Recognition System",
      "author" : [ "Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve" ],
      "venue" : "In arXiv:1609.03193,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2016
    }, {
      "title" : "Practical Variational Inference for Neural Networks",
      "author" : [ "Alex Graves" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Graves.,? \\Q2011\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2011
    }, {
      "title" : "Towards End-to-End Speech Recognition with Recurrent Neural Networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Graves and Jaitly.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves and Jaitly.",
      "year" : 2014
    }, {
      "title" : "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
      "author" : [ "Alex Graves", "Santiago Fernandez", "Faustino Gomez", "Jurgen Schmiduber" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Graves et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Hybrid Speech Recognition with Bidirectional LSTM",
      "author" : [ "Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed" ],
      "venue" : "In Automatic Speech Recognition and Understanding Workshop,",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs",
      "author" : [ "Awni Hannun", "Andrew Maas", "Daniel Jurafsky", "Andrew Ng" ],
      "venue" : null,
      "citeRegEx" : "Hannun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hannun et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical Recurrent Neural Networks for Long-Term Dependencies",
      "author" : [ "Salah Hihi", "Yoshua Bengio" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Hihi and Bengio.,? \\Q1996\\E",
      "shortCiteRegEx" : "Hihi and Bengio.",
      "year" : 1996
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jurgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "On Using Very Large Target Vocabulary for Neural Machine Translation",
      "author" : [ "Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A Clockwork RNN",
      "author" : [ "Jan Koutnik", "Klaus Greff", "Faustino Gomez", "Jurgen Schmidhuber" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Koutnik et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Koutnik et al\\.",
      "year" : 2014
    }, {
      "title" : "Latent Predictor Networks for Code Generation",
      "author" : [ "Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Andrew Senior", "FUmin Wang", "Phil Blunsom" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Ling et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2016
    }, {
      "title" : "Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition",
      "author" : [ "Liang Lu", "Arnab Ghoshal", "Steve Renals" ],
      "venue" : "In Automatic Speech Recognition and Understanding Workshop,",
      "citeRegEx" : "Lu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2013
    }, {
      "title" : "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models",
      "author" : [ "Minh-Thang Luong", "Christopher Manning" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Luong and Manning.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "Addressing the Rare Word Problem in Neural Machine Translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Lexicons From Speech Using a Pronunciation Mixture Model",
      "author" : [ "Ian McGraw", "Ibrahim Badr", "James Glass" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "McGraw et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "McGraw et al\\.",
      "year" : 2013
    }, {
      "title" : "The Kaldi Speech Recognition Toolkit",
      "author" : [ "Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannenmann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely" ],
      "venue" : "In Automatic Speech Recognition and Understanding Workshop,",
      "citeRegEx" : "Povey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequence Level Training with Recurrent Neural Networks",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Japanese and Korean Voice Search",
      "author" : [ "Mike Schuster", "Kaisuke Nakajima" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Schuster and Nakajima.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schuster and Nakajima.",
      "year" : 2012
    }, {
      "title" : "Bidirectional Recurrent Neural Networks",
      "author" : [ "Mike Schuster", "Kuldip Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Schuster and Paliwal.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic generation of subword units for speech recognition systems",
      "author" : [ "Rita Singh", "Bhiksha Raj", "Richard Stern" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing,",
      "citeRegEx" : "Singh et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2002
    }, {
      "title" : "Sequence to Sequence Learning with Neural Networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc Le" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard Sutton", "Andrew Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and Tell: A Neural Image Caption Generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Order Matters: Sequence to sequence for sets",
      "author" : [ "Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for end-to-end speech recognition",
      "author" : [ "Yu Zhang", "William Chan", "Navdeep Jaitly" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Advances in All-Neural Speech Recognition",
      "author" : [ "Geoffrey Zweig", "Chengzhu Yu", "Jasha Droppo", "Andreas Stolcke" ],
      "venue" : "In arXiv:1609.05935,",
      "citeRegEx" : "Zweig et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zweig et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Sequence-to-sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014) with attention (Bahdanau et al.",
      "startOffset" : 38,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "Sequence-to-sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014) with attention (Bahdanau et al.",
      "startOffset" : 38,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : ", 2014) with attention (Bahdanau et al., 2015) have been successfully applied to many applications including machine translation (Luong et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : ", 2015) have been successfully applied to many applications including machine translation (Luong et al., 2015; Jean et al., 2015), parsing (Vinyals et al.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : ", 2015) have been successfully applied to many applications including machine translation (Luong et al., 2015; Jean et al., 2015), parsing (Vinyals et al.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 34,
      "context" : ", 2015a), image captioning (Vinyals et al., 2015b; Xu et al., 2015) and Automatic Speech Recognition (ASR) (Chan et al.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : ", 2015) and Automatic Speech Recognition (ASR) (Chan et al., 2016; Bahdanau et al., 2016a).",
      "startOffset" : 47,
      "endOffset" : 90
    }, {
      "referenceID" : 29,
      "context" : "The output representation is usually a fixed sequence of words (Sutskever et al., 2014; Cho et al., 2014), phonemes (Chorowski et al.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "The output representation is usually a fixed sequence of words (Sutskever et al., 2014; Cho et al., 2014), phonemes (Chorowski et al.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : ", 2014), phonemes (Chorowski et al., 2015), characters (Chan et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : ", 2015), characters (Chan et al., 2016; Bahdanau et al., 2016a) or even a mixture of characters and words (Luong & Manning, 2016).",
      "startOffset" : 20,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "Word segmented models (Luong et al., 2015; Jean et al., 2015) often have to deal with large softmax sizes, rare words and Out-of-Vocabulary (OOV) words.",
      "startOffset" : 22,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "Word segmented models (Luong et al., 2015; Jean et al., 2015) often have to deal with large softmax sizes, rare words and Out-of-Vocabulary (OOV) words.",
      "startOffset" : 22,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "Character models (Chan et al., 2016; Bahdanau et al., 2016a) overcome the OOV problem by modelling the smallest output unit, however this typically results in long decoder lengths and computationally expensive inference.",
      "startOffset" : 17,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "For example, in end-to-end ASR we typically use characters as the output unit of choice (Chan et al., 2016; Bahdanau et al., 2016a) but word pieces may be better units as they more closely align with the acoustic entities such as syllables.",
      "startOffset" : 88,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "In this work, we model the latent sequence decompositions p(z|x) with an attention-based seq2seq model (Bahdanau et al., 2015).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "In our experimental implementation EncodeRNN is a stacked Bidirectional LSTM (BLSTM) (Schuster & Paliwal, 1997; Graves et al., 2013) with hierarchical subsampling (Hihi & Bengio, 1996; Koutnik et al.",
      "startOffset" : 85,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : ", 2013) with hierarchical subsampling (Hihi & Bengio, 1996; Koutnik et al., 2014):",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "The output sequence z is generated with an attention-based transducer (Bahdanau et al., 2015) one zi token at a time: si = DecodeRNN([zi−1, ci−1], si−1) (10) ci = AttentionContext(si,h) (11) p(zi|x, z<i) = TokenDistribution(si, ci) (12)",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "The AttentionContext function generates ci with a content-based MLP attention network (Bahdanau et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "ŷ = collapse(ẑ) (18) We approximate for the best ẑ sequence by doing a left-to-right beam search (Chan et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "Our input features were 80 dimensional filterbanks computed every 10ms with delta and delta-delta acceleration normalized with per speaker mean and variance as generated by Kaldi (Povey et al., 2011).",
      "startOffset" : 179,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "075) and L2 weight decay 1e−5 (Graves, 2011).",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "Model WER Graves & Jaitly (2014) CTC 30.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Model WER Graves & Jaitly (2014) CTC 30.1 CTC + WER 27.3 Hannun et al. (2014) CTC 35.",
      "startOffset" : 10,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "8 Bahdanau et al. (2016a) seq2seq 18.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "8 Bahdanau et al. (2016a) seq2seq 18.6 Bahdanau et al. (2016b) seq2seq + TLE 18.",
      "startOffset" : 2,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "8 Bahdanau et al. (2016a) seq2seq 18.6 Bahdanau et al. (2016b) seq2seq + TLE 18.0 Zhang et al. (2017) seq2seq + CNN 2 11.",
      "startOffset" : 2,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "8% (Zhang et al., 2017) 2.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "If we combine our LSD model with the CNN (Zhang et al., 2017) model, we achieve a combined WER of 9.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Connectionist Temporal Classification (CTC) (Graves et al., 2006; Graves & Jaitly, 2014) based models assume conditional independence, and can rely on dynamic programming for exact inference.",
      "startOffset" : 44,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "(2002); McGraw et al. (2013); Lu et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "(2013); Lu et al. (2013) built probabilistic pronunciation models for Hidden Markov Model (HMM) based systems.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "Connectionist Temporal Classification (CTC) (Graves et al., 2006; Graves & Jaitly, 2014) based models assume conditional independence, and can rely on dynamic programming for exact inference. Similarly, Ling et al. (2016) use latent codes to generate text, and also assume conditional independence and leverage on dynamic programming for exact maximum likelihood gradients.",
      "startOffset" : 45,
      "endOffset" : 222
    }, {
      "referenceID" : 7,
      "context" : "Collobert et al. (2016) and Zweig et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "Collobert et al. (2016) and Zweig et al. (2016) developed extensions of CTC where they used some word pieces.",
      "startOffset" : 0,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "Collobert et al. (2016) and Zweig et al. (2016) developed extensions of CTC where they used some word pieces. However, the word pieces are only used in repeated characters and the decompositions are fixed. Word piece models with seq2seq have also been recently used in machine translation. Sennrich et al. (2016) used word pieces in rare words, while Wu et al.",
      "startOffset" : 0,
      "endOffset" : 313
    }, {
      "referenceID" : 7,
      "context" : "Collobert et al. (2016) and Zweig et al. (2016) developed extensions of CTC where they used some word pieces. However, the word pieces are only used in repeated characters and the decompositions are fixed. Word piece models with seq2seq have also been recently used in machine translation. Sennrich et al. (2016) used word pieces in rare words, while Wu et al. (2016) used word pieces for all the words, however the decomposition is fixed and defined by heuristics or another model.",
      "startOffset" : 0,
      "endOffset" : 368
    }, {
      "referenceID" : 24,
      "context" : ", REINFORCE and other task loss estimators) (Sutton & Barto, 1998; Graves & Jaitly, 2014; Ranzato et al., 2016) learn different output sequences can yield different task losses.",
      "startOffset" : 44,
      "endOffset" : 111
    }, {
      "referenceID" : 29,
      "context" : "Vinyals et al. (2016) used seq2seq to outputs sets, the output sequence is unordered and used fixed length output units; in our decompositions we maintain ordering use variable lengthed output units.",
      "startOffset" : 0,
      "endOffset" : 22
    } ],
    "year" : 2017,
    "abstractText" : "Sequence-to-sequence models rely on a fixed decomposition of the target sequences into a sequence of tokens that may be words, word-pieces or characters. The choice of these tokens and the decomposition of the target sequences into a sequence of tokens is often static, and independent of the input, output data domains. This can potentially lead to a sub-optimal choice of token dictionaries, as the decomposition is not informed by the particular problem being solved. In this paper we present Latent Sequence Decompositions (LSD), a framework in which the decomposition of sequences into constituent tokens is learnt during the training of the model. The decomposition depends both on the input sequence and on the output sequence. In LSD, during training, the model samples decompositions incrementally, from left to right by locally sampling between valid extensions. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve a WER of 9.6%.",
    "creator" : "LaTeX with hyperref package"
  }
}