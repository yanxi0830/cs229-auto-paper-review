{
  "name" : "726.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Stefano Soatto" ],
    "emails" : [ "achille@cs.ucla.edu", "soatto@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the general supervised setting, we want to learn the conditional distribution p(y|x) of some random variable y, which we refer to as the task, given (samples of the) input data x. In typical applications, x is often high dimensional (for example an image or a video), while y is low dimensional, such as a label or a coarsely-quantized location. In such cases, a large part of the variability in x is actually due to nuisance factors that affect the data, but are otherwise irrelevant for the task. Since by definition these nuisance factors are not predictive of the task, they should be disregarded during the inference process. However, it often happens that modern machine learning algorithms, in part due to their high flexibility, will fit spurious correlations, present in the training data, between the nuisances and the task, thus leading to poor generalization performance.\nIn view of this, Tishby & Zaslavsky (2015) argue that the success of deep learning is in part due to the capability of neural networks to build incrementally better representations that expose the relevant information, while at the same time discarding nuisance variability. This interpretation is intriguing, as it establishes a connection between machine learning, probabilistic inference, and information theory. However, the commonly used training methods do not seem to stem from this insight, and indeed deep networks may maintain even in the topmost layers dependencies on easily ignorable nuisances (see for example Figure 2).\nTo bring the practice in line with the theory, and to better understand these connections, we introduce a new layer, that we call Information Dropout, which allows the network to selectively introduce multiplicative noise in the layer activations, and thus to control the flow of information. We then introduce a modified cost function, that can be seen as an approximation of the Information Bottleneck Lagrangian of Tishby et al. (1999), which encourages the creation of representations of the data which are increasingly insensitive to the action of nuisances. In practice, this prevents the network from overfitting to the nuisances. As we show in various experiments, our method improves the\n∗Dedicated to Naftali Tishby in the occasion of the conference Information, Control and Learning held in his honor in Jerusalem, September 26-28, 2016.\ngeneralization performance, and is comparable or better than the dropout baseline. In particular, it considerably improves over binary dropout on smaller models, since, unlike dropout, Information Dropout also adapts the noise to the structure of the network and to the individual sample at test time.\nApart from the practical interest of Information Dropout, one of our main results is that Information Dropout can be seen as a generalization to several existing dropout methods, providing a unified framework to analyze them, together with some additional insights on empirical results. As we discuss in Section 2, the introduction of noise to prevent overfitting has already been studied from several points of view. For example the original formulation of dropout of Srivastava et al. (2014), which introduces binary multiplicative noise, was motivated as a way of efficiently training an ensemble of exponentially many networks, that would be averaged at testing time. Kingma et al. (2015) introduce Variational Dropout, a dropout method which closely resemble ours, and is instead derived from a Bayesian analysis of neural networks. Information Dropout gives an alternative information-theoretic interpretation of those methods.\nAs we show in Section 5, other than being very closely related to Variational Dropout, Information Dropout directly yields a variational autoencoder as a special case when the task is the reconstruction of the input. This result is in part expected, since by construction Information Dropout seeks an optimal representation of the input for the task of reconstruction, and the representation given by the latent variables of a variational autoencoder fits the criteria. However, it still rises the question of exactly what and how deep are the links between information theory, representation learning, variational inference and nuisance invariance. This work can be seen as a small step in answering this question."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "The main contribution of our work is to establish how two seemingly different areas of research, namely the study of noise and dropout methods to prevent overfitting, and the study of optimal representations, can be linked through the Information Bottleneck principle.\nDropout was introduced by Srivastava et al. (2014). The original motivation was that by randomly dropping the activations during training, we can effectively train an ensemble of exponentially many networks, that are then averaged during testing, therefore reducing overfitting. Wang & Manning (2013) suggested that dropout could be seen as performing a Monte-Carlo approximation of an implicit loss function, and suggest that instead of multiplying the activations by binary noise, like in the original dropout, multiplicative Gaussian noise with mean 1 can be used as a way of better approximating the implicit loss function. This leads to a comparable performance but faster training than binary dropout.\nKingma et al. (2015) take a similar view of dropout as introducing multiplicative (Gaussian) noise, but instead study the problem from a Bayesian point of view. Given a training dataset D = {(xi,yi)}i=1,...,N and a prior distribution p(w), we want to compute the posterior distribution p(w|D) of the weights w of the network. As is customary in variational inference, the true posterior can be approximated by minimizing the negative variational lower bound L(θ) of the marginal log-likelihood of the data,\nL(θ) = 1 N N∑ i=1 Ew∼pθ(w|D)[− log p(yi|xi,w)] + 1 N KL(pθ(w|D) ‖ p(w)).\nThis minimization is difficult to perform, since it requires to repeatedly sample new weights for each sample of the dataset. As an alternative, Kingma et al. (2015) suggest that the uncertainty about the weights that is expressed by the posterior distribution pθ(w|D) can equivalently be encoded as a multiplicative noise in the activations of the layers (the so called local reparametrization trick). As we will see in the following sections, this loss function closely resemble the one of Information Dropout, which however is derived from a purely information theoretic argument based on the Information Bottleneck principle. One difference is that we allow the parameters of the noise to change on a per-sample basis (which, as we show in the experiments, can be useful to deal with nuisances), and that we allow a scaling constant β in front of the KL-divergence term, which can be changed freely. Interestingly, even if the Bayesian derivation does not allow a rescaling of the KL-divergence, Kingma et al. (2015) notice that choosing a different scale for the KL-divergence term can indeed lead to improvements in practice.\nThe interpretation of deep neural network as a way of creating successively better representations of the data has already been suggested and explored by many, as described in Tishby & Zaslavsky (2015). Some have focused on creating representations that are maximally invariant to nuisances, especially when they have the structure of a (possibly infinite-dimensional) group acting on the data, like Sundaramoorthi et al. (2009), or, when the nuisance is a locally compact group acting on each layer, by successive approximations implemented by hierarchical convolutional architectures, like Anselmi et al. (2016) and Bruna & Mallat (2011). In these cases, which cover common nuisances such as translations and rotations of an image (affine group), or small diffeomorphic deformations due to a slight change of point of view (group of diffeomorphisms), the representation is equivalent to the data modulo the action of the group. However, when the nuisances are not a group, as is the case for occlusions, it is not possible to achieve such equivalence, that is, there is a loss. To address this problem, Soatto & Chiuso (2016) defined optimal representations not in terms of maximality, but in terms of sufficiency, and characterized representations that are both sufficient and invariant. They argue that the management of nuisance factors common in visual data, such as change of viewpoint, local deformations, and changes of illumination, is directly tied to the specific structure of deep convolutional networks, where local marginalization of simple nuisances at each layer results in marginalization of complex nuisances in the network as a whole.\nOur work fits in this last line of thinking, where the goal is not equivalence to the data up to the action of (group) nuisances, but instead sufficiency for the task. Our main contribution in this sense is to show that injecting noise into the layers, and therefore using a non-deterministic function of the data, can actually simplify the theoretical analysis and lead to improved invariance to nuisances. This is an alternate explanation to that put forth by the references above."
    }, {
      "heading" : "3 OPTIMAL REPRESENTATIONS AND THE INFORMATION BOTTLENECK LOSS",
      "text" : "Given some input data x, we want to compute some (possibly nondeterministic) function of x, called a representation, that has some desirable properties in view of the task y, for instance by being more convenient to work with, exposing relevant statistics, or being easier to store. Ideally, we want this representation to be as good as the original data for the task, and not squander resources modeling parts of the data that are irrelevant to the task. Formally, this means that we want to find a random variable z satisfying the following conditions:\n(i) z is a representation of x; that is, its distribution depends only on x, as expressed by the following Markov chain:\ny x z\n(ii) z is sufficient for the task y, that is I(x;y) = I(z;y), expressed by the Markov chain:\ny z x\n(iii) among all random variables satisfying these requirements, the mutual information I(x; z) is minimal. This means that z discards all the variability that is not relevant to the task.\nUsing the identity I(x;y)− I(z;y) = H(y|z)−H(y|x), where H denotes the entropy and I the mutual information, it is easy to see that the above conditions are equivalent to finding a distribution p(z|x) which solves the optimization problem\nminimize I(x; z)\ns.t. H(y|z) = H(y|x).\nSince the above minimization is difficult to perform in general due to the non-linear constraint, Tishby et al. (1999) propose to minimize instead the following relaxation, known as the Information Bottleneck Lagrangian, L = H(y|z) + βI(x; z). (1) where β is some positive constant. It is easy to see that, in the limit β → 0+, this is equivalent to the original problem. When all random variables are discrete and z = T (x) is a deterministic function of x, the algorithm proposed by Tishby et al. (1999) can be used to minimize the IB Lagrangian\nefficiently. However, no algorithm is known to minimize the IB Lagrangian for non-Gaussian, high-dimensional continuous random variables.\nOur main result is that, when we restrict to the family of distributions obtained by injecting noise to one layer of a neural network, we can efficiently approximate and minimize the IB Lagrangian. Since we restrict the family of distributions, this does not in general guarantee that the resulting representation will be optimal. We can however iterate the process to obtain incrementally improved representations. As we will show, this process can be effectively implemented through a generalization of the dropout layer that we call Information Dropout.\nIn preparation for this, we rewrite the IB Lagrangian as a per-sample loss function. Let p(x,y) denote the true distribution of the data, from which the training set {(xi,yi)}i=1,...,N is sampled, and let pθ(z|x) and pθ(y|z) denote the unknown distributions that we need to estimate, parametrized by θ. Then, we can write the two terms in the IB Lagrangian as\nH(y|z) ' Ex,y∼p(x,y) [ Ez∼pθ(z|x)[− log pθ(y|z)] ] I(x; z) = Ex∼p(x)[KL(pθ(z|x) ‖ pθ(z))],\nwhereKL denotes the Kullback-Leibler divergence. We can therefore approximate the IB Lagrangian empirically as\nL = 1 N N∑ i=1 Ez∼p(z|xi)[− log p(yi|z)] + βKL(pθ(z|xi) ‖ pθ(z)). (2)\nNotice that the first term simply is the average cross-entropy loss, which is the most commonly used loss function in deep learning. The second term can then be seen as a regularization term that penalizes the transfer of information from x to z. In the next section, we discuss ways to control such information transfer through the injection of noise.\nRemark. Aside from being easier to work with, stochastic representations of the data can realize a lower value of the IB Lagrangian than any deterministic representation. For an example, consider the task of reconstructing single random bit y given a noisy observation x. The only deterministic representations are equivalent to the either the noisy observation itself or to the trivial constant map. It is not difficult to check that for opportune values of β and of the noise, neither realize the optimal tradeoff reached by an opportune stochastic representation."
    }, {
      "heading" : "4 INFORMATION DROPOUT",
      "text" : "Inspired by the effectiveness of dropout, we propose the following way of constructing a representation z: first, we compute a function f(x) of the input through a sequence of convolutional or fullyconnected layers, followed by a nonlinear activation function. Ideally, this should help “disentangle” the nuisances from the (random) function of the data that is relevant to the task, as we illustrate in Appendix C. Subsequently, we selectively let the relevant information flow by applying multiplicative noise from a noise distribution pα(x)(ε) with mean 1 and whose parameters α(x) = αθ(x), such as the variance, are selected by the network itself:\nε ∼ pα(x)(ε), z = ε f(x),\nwhere “ ” denotes the element-wise product. Notice that, if pα(x)(ε) is a Bernoulli distribution rescaled to have mean 1, this reduces exactly to the classic binary dropout layer. As we discussed in Section 2, there are also variants of dropout that use different distributions.\nx f(x) z = ε f(x)\nε\ny\nA natural choice for the multiplicative noise distribution pα(x)(ε), which also simplifies the theoretical analysis, is the log-normal distribution pα(x)(ε) = logN (0, α2θ(x)). Once we fix this noise distribution, given the above expression for z, we can easily compute the distribution pθ(z|x) that appears in eq. (2). However, to be able to compute the KL-divergence term, we still need to fix a prior distribution pθ(z). The choice of this prior largely depends on the expected distribution of the activations f(x). In the following, we assume for simplicity that all the activations are approximately independent and identically distributed. In Appendix B we show that, even if the activations are not independent, optimizing the loss in Equation (2) under the assumption of a factorized prior still makes sense, and it actually encourages the components to become independent. We concentrate on two of the most common activation functions, the rectified linear unit (ReLU), which is easy to compute and works well in practice, and the Softplus function, which can be seen as a strictly positive and differentiable approximation of ReLU.\nA network implemented using only ReLU and a final Softmax layer has the remarkable property of being scale-invariant, meaning that multiplying all weights, biases, and activations by a constant does not change the final result. Therefore, from a theoretical point of view, it would be desirable to use a scale-invariant prior. The only such prior is the improper log-uniform, p(log(z)) = c, or equivalently p(z) = c/z, which was also suggested by Kingma et al. (2015), but as a prior for the weights of the network, rather than the activations. Since the ReLU activations are frequently zero, we also assume p(z = 0) = q for some constant 0 ≤ q ≤ 1. Therefore, the final prior has the form p(z) = qδ0(z) + c/z, where δ0 is the Dirac delta in zero. In Figure 1a, we compare this prior distribution with the actual empirical distribution p(z) of a network with ReLU activations.\nIn a network implemented using Softplus activations, a log-normal is a good fit of the distribution of the activations. This is to be expected, especially when using batch-normalization, since the pre-activations will approximately follow a normal distribution with zero mean, and the Softplus approximately resembles a scaled exponential near zero. Therefore, in this case we suggest using a log-normal distribution as our prior p(z). In Figure 1b, we compare this prior with the empirical distribution p(z) of a network with Softplus activations.\nUsing these priors, we can finally compute the KL divergence term in eq. (2) for both ReLU activations and Softplus activations. We prove the following two propositions in Appendix A. Proposition 1 (Information dropout cost for ReLU activations). Let z = ε · f(x), where ε ∼ pα(ε), and assume p(z) = qδ0(z) + c/z. Then, assuming f(x) 6= 0, we have\nKL(pθ(z|x) ‖ p(z)) = −H(pα(x)(log ε)) + log c\nIn particular, if pα(ε) is chosen to be the log-normal distribution pα(ε) = logN (0, α2θ(x)), we have\nKL(pθ(z|x) ‖ p(z)) = − logαθ(x) + const. (3)\nIf instead f(x) = 0, we have KL(pθ(z|x) ‖ p(z)) = − log q.\nProposition 2 (Information dropout cost for Softplus activations). Let z = ε · f(x), where ε ∼ pα(ε) = logN (0, α2θ(x)), and assume pθ(z) = logN (µ, σ2). Then, we have\nKL(pθ(z|x) ‖ p(z)) = 1 2σ2 ( α2(x) + µ2 ) − log α(x) σ − 1 2 . (4)\nSubstituting the expression for the KL divergence in eq. (3) inside eq. (2), and ignoring for simplicity the special case f(x) = 0, we obtain the following loss function for ReLU activations\nL = 1 N N∑ i=1 Ez∼pθ(z|xi)[log p(yi|z)] + β logαθ(xi), (5)\nand a similar expression for the Softplus. Notice that the first expectation can be approximated by sampling (in the experiments we use one single sample, as customary for dropout), and is just the average cross-entropy term that is typical in deep learning. The second term, which is new, penalizes the network for choosing a low variance for the noise, i.e. for letting more information pass through to the next layer. This loss can be optimized easily using stochastic gradient descent and the reparametrization trick of Kingma & Welling (2013) to back-propagate the gradient through the sampling operation."
    }, {
      "heading" : "5 VARIATIONAL AUTOENCODERS AND INFORMATION DROPOUT",
      "text" : "In this section, we briefly outline a link between variational autoencoders (Kingma & Welling, 2013) and Information Dropout. A variational autoencoder (VAE) aims to reconstruct, given a training dataset D = {xi}, a latent random variable z such that the observed data x can be thought as being generated by the, usually simpler, variable z through some unknown generative process pθ(x|z). In practice, this is done by minimizing the negative variational lower-bound to the marginal log-likelihood of the data\nL(θ) = 1 N N∑ i=1 Ez∼pθ(z|xi)[− log pθ(xi|z)] + KL(pθ(z|xi) ‖ p(z)),\nwhich can be optimized easily using the SGVB method of Kingma & Welling (2013). Interestingly, when the task is reconstruction, that is when y = x, the IB loss function in eq. (2) reduces to\nL(θ) = 1 N N∑ i=1 Ez∼pθ(z|xi)[− log pθ(xi|z)] + βKL(pθ(z|xi) ‖ p(z)).\nTherefore, by letting β = 1 in the previous expression, we obtain exactly the loss function of a variational autoencoder, that is, the representation z computed by the Information Dropout layer coincides with the latent variable z computed by the VAE. This is in part to be expected, since the objective of Information Dropout is to create a representation of the data that is minimal sufficient for the task of reconstruction, and the latent variables of a VAE can be thought as such a representation. The term β in this case can be seen as managing the trade off between the fidelity of the reconstruction of the input from the representation (measured by the cross-entropy), against the compression factor (complexity) of the representation (measured by the KL-divergence)."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "We compare our method with the Dropout baseline on several standard benchmark datasets using different networks architecture, and highlight a few key properties of Information Dropout. All the models were implemented using TensorFlow (Abadi et al., 2015). As Kingma et al. (2015) also notice, letting the variance of the noise increase too much leads to poor local minima in the optimization process. To avoid this problem, we put the constraint α(x) < 0.7, so that the maximum variance of the log-normal error distribution will be approximatively 1, the same as binary dropout when using a drop probability of 0.5. In all experiments we divide the KL-divergence term by the number of training samples, so that for β = 1 the scaling of the KL-divergence term in similar to the one used by Variational Dropout (see Section 2).\nCluttered MNIST. To visually asses the ability of Information Dropout to create a representation of the input that is increasingly insensitive to nuisance factors, we train the All-CNN-96 network (Table 2) for classification on a Cluttered MNIST dataset (cf. Mnih et al., 2014), consisting of 96x96 images containing a single MNIST digit together with 21 distraction objects. The dataset is divided in 50.000 training images and 10.000 testing images. As shown in Figure 2, for small values of β, the network lets through both the objects of interest (digits) and distractors, to upper layers. By increasing the value of β, we force the network to disregard something, and the network decides to disregard the least discriminative components of the data, thereby building a better representation for the task. This behavior depends on the ability of Information Dropout to learn the structure of the nuisances in the dataset which, unlike other methods, is facilitated by the ability to select noise level on a per-sample basis.\nOccluded CIFAR. Occlusions are a fundamental type of nuisance in vision for which it is difficult to hand-design invariant representations. To assess that the approximate minimal sufficient representation produced by Information Dropout has this invariance property, we created a new dataset by occluding images from CIFAR-10 with digits from MNIST (Figure 4a). We train the All-CNN-32 network (Table 2) to classify the CIFAR image. The information relative to the occluding MNIST digit is then a nuisance for the task, and therefore should be excluded from the final representation. To test this, we train a secondary network to classify the nuisance MNIST digit using only the the representation learned for the main task. When training with small values of β, the network has very little pressure to limit the presence of nuisance information in the representation, so we expect the nuisance classifier to perform better. On the other hand, increasing the value of β we expect the performance to degrade, since the representation will become increasingly minimal, and therefore invariant to nuisances. The results in Figure 4b confirm this intuition.\nMNIST and CIFAR-10. Similar to Kingma et al. (2015), to see the effect of Information Dropout on different network sizes and architectures, we train on MNIST a network with 3 fully connected hidden layers with a variable number of hidden units, and we train on CIFAR-10 (Krizhevsky & Hinton, 2009) the All-CNN-32 convolutional network described in Table 2, using a variable percentage of all the filters. The fully connected network was trained for 80 epochs, using stochastic gradient descent with momentum with initial learning rate 0.07 and dropping the learning rate by 0.1 at 30 and 70 epochs. The CNN was trained for 160 epochs with initial learning rate 0.1 and dropping the learning rate by 0.1 at 80 and 120 epochs. We show the results in Figure 3. Information Dropout is comparable or outperforms binary dropout, especially on smaller networks. A possible explanation is\nthat dropout severely reduces the already limited capacity of the network, while Information Dropout can adapt the amount of noise to the data and to the size of the network so that the relevant information can still flow to the successive layers.\nVAE. To validate what we said in Section 5, we replicate the basic variational autoencoder of Kingma & Welling (2013), implementing it both with Gaussian latent variables, as in the original, and with an Information Dropout layer. We trained both implementations for 300 epochs dropping the learning rate by 0.1 at 30 and 120 epochs. We report the results in the following table. The Information Dropout implementation has similar performance to the original, confirming that a variational autoencoder can be considered a special case of Information Dropout."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We introduced a new dropout method that can be seen as an efficient implementation of the Information Bottleneck principle and that helps the network learn the structure of the nuisance factors affecting the data and build representations that are insensitive to those nuisances, therefore improving generalization performance. We also analyzed from an information theoretic viewpoint the role that noise injected in a network has in learning nuisance invariance. Experimental evidence confirms the insights stemming from the theory thus developed.\nAnother interpretation of Information Dropout is as a way of biasing the network towards reconstructing representations of the data that are compatible with a Markov chain generative model,\nmaking it more suited to data coming from hierarchical models, and in this sense is complementary to architectural constraint, such as convolutions, that instead bias the model toward geometrical tasks.\nAn important topic in representation learning, which we did not discuss explicitly, is whether we can also learn a “disentangled” representation of the data, and whether the factors of this representation are related to the latent factors of the real generative model. In Appendix B and Appendix C, we show that by adding independent (multiplicative) noise to the activations and by using the IB loss, we naturally favor representations which have mutually independent components and, in some restricted situations, we prove that we can disentangle the relevant part of the information from the nuisance variability. We leave proving more general results in this direction to a future work."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "Work supported by ARO, ONR, AFOSR."
    }, {
      "heading" : "A COMPUTATION OF THE KL-DIVERGENCE",
      "text" : "Proposition (Information dropout cost for ReLU activations). Let z = ε · f(x), where ε ∼ pα(ε), and assume p(z) = qδ0(z) + c/z. Then, assuming f(x) 6= 0, we have\nKL(pθ(z|x) ‖ p(z)) = −H(pα(x)(log ε)) + log(c)\nIn particular, if pα(ε) is chosen to be the log-normal distribution pα(ε) = logN (0, α2θ(x)), we have\nKL(pθ(z|x) ‖ p(z)) = − logαθ(x) + const.\nIf instead f(x) = 0, we have KL(pθ(z|x) ‖ p(z)) = − log q.\nProof. If f(x) 6= 0, then we also have z 6= 0. Since the KL-divergence is invariant under parameter transformations we can write\nKL(pθ(z|x) ‖ pθ(z)) = KL(pθ(log z|x) ‖ pθ(log z))\n= ∫ log ( pθ(log z|x) pθ(log z) ) pθ(log z|x)dz\n= ∫ log ( pα(x)(log ε) ) pα(x)(log ε)dε− log c\n= −H(pα(x)(log ε))− log c.\nFor the second part, notice that by definition pα(x) = N (0, α2θ(x)) and\nH(N (0, α)) = logαθ(x) + 1\n2 log(2πe).\nFinally, if f(x) = 0, then also z = 0, so p(z|x) = δ0(z). It is then easy to see that\nKL(pθ(z|x) ‖ p(z)) = − log p(z = 0) = − log q.\nProposition (Information dropout cost for Softplus activations). Let z = ε·f(x), where ε ∼ pα(ε) = logN (0, α2θ(x)), and assume pθ(z) = logN (µ, σ2). Then, we have\nKL(pθ(z|x) ‖ p(z)) = 1 2σ2 ( α2(x) + µ2 ) − log α(x) σ − 1 2 .\nProof. Since the KL divergence is invariant for reparametrizations, the divergence between two log-normal distributions is equal to the divergence between the corresponding normal distributions. Therefore, using the known formula for the KL divergence of normals, we get the desired result."
    }, {
      "heading" : "B EFFECTS OF USING A FACTORIZED APPROXIMATE PRIOR",
      "text" : "In Information Dropout, we want to find a representation z ∼ q(z|x) that minimizes the objective\nHq(y|z) + βI(z;x).\nThis objective can be rewritten as\nHq(y|z) + βEx[KL(q(z|x) ‖ q(z))],\nwhere the parameters of the posterior distribution q(z|x) are learned using a neural network. The prior distribution q(z) should in theory be computed from the posterior by integrating over x. However, since this is not feasible, we instead introduce a factorized approximation of the prior in the form p(z) = ∏ j pj(zj), and solve the problem\nHq(y|z) + βEx[KL(q(z|x) ‖ p(z))]. In this appendix, we show that the latter problem is indeed equivalent to the former when the components of the representation are mutually independent, and that assuming a factorized prior automatically favors the creation of such representations. In the following proposition, for simplicity, we concentrate on discrete random variables. Recall that the total correlation, or multivariate mutual information, of a variable z = (z1, . . . , zn) is defined as\nTC(z) = ∑ j H(zj)−H(z)\n= KL(q(z) ‖ ∏\nj qj(zj)),\nand that TC(z) = 0 if and only if the components of z are mutually independent. Proposition 3. Let z = (z1, . . . , zn) be a discrete random variable, let q(z|x) be a generic probability distribution, and let p(z) = ∏ i=1n p(zi) be a factorized prior distribution. Then, for any function F (q), a minimization problem in the form minimizeq,p F (q) + βEx[KL(q(z|x) ‖ p(z))],\nis equivalent to minimizeq F (q) + β {Iq(z;x) + TCq(z)} , where Iq(z;x) is the mutual information and TCq(z) is the total correlation of z, assuming z ∼ q(z).\nProof. To prove the proposition, we just need to minimize with respect to p and substitute back the solution. Adding a Lagrange multiplier for the constrain ∑ zi pi(zi) = 1, the problem can be rewritten as\nL(q, p) = F (q) + βEx [∑ z q(z|x) log q(z|x) p(z) dz ] + λ (∑ zi pi(zi)− 1 )\n= F (q) + βEx [∑ z q(z|x) log q(z|x)∏n j=1 pj(zj) dz ] + λ (∑ zi pi(zi)− 1 ) .\nTaking the derivative with respect to to pi(z̄i) we have\n∂L(q, p) ∂pi(z̄i) = βEx [∑ z q(z|x) log q(z|x)∏n j=1 pj(zj) ] + λ\n= −β ∑ zi=z̄i Ex[q(z|x)] pi(z̄i) + λ\n= −β q(z̄i) p(z̄i) + λ.\nSetting it to zero, we obtain p(zi) = q(zi), that is, the optimal factorized prior is the product of the marginals. Substituting it back in the second term (the only one containing p), we obtain\nEx[KL(q(z|x) ‖ p(z))] = Ex [∑ z q(z|x) log q(z|x)∏n j=1 qj(zj) ]\n= Ex [∑ z q(z|x) ( log q(z|x) q(z) + log q(z)∏n j=1 qj(zj) )]\n= Ex [∑ z q(z|x) log q(z|x) q(z) ] + ∑ z Ex[q(z|x)] log q(z)∏n j=1 qj(zj)\n= Iq(z;x) + KL(q(z) ‖ ∏ j qj(zj))\n= Iq(z;x) + TCq(z).\nC INFORMATION BOTTLENECK PRINCIPLE AND DISENTANGLEMENT\nIn Appendix B, we showed that the loss function we use, in conjunction with a factorized prior, automatically favors representations of the data in which all the components are independent. In this appendix we show that, in the simple case of the data x coming from a Gaussian distribution, where our task is to recover one component of the data from a noisy observation, the solution of the Information Bottleneck Lagrangian when noise is added to the computation process naturally leads to recovering the independent components of the generative model. For simplicity, here we will use additive noise from a Gaussian distribution, but it is easily seen by exponentiating each random variable that this is exactly equivalent to using multiplicative noise from a log-normal distribution, which is the same case treated in the main paper.\nSuppose we have a 2-dimensional Gaussian random variable\nx = (x, y) ∼ N (0, I), and that our task is to recover y = eT2 x, given a noisy observation\nx̂ = Ax+ ξ,\nwhere A is an orthogonal matrix and ξ ∼ N (0, σ2ξ ) is some additive noise. We want to find an optimal representation of the input x̂ for the task y. To simplify the problem, we restrict to the representations in the form\nz = Bx̂,\nwhere B is an orthogonal matrix. Intuitively, the best representation for the task should be obtained by choosing B = A−1, since then we would have z = A−1x̂ = x+A−1ξ, and the component of x̂ relevant to the task, that is y, would be disentangled from the nuisance x. However, as we show in the following proposition, if we evaluate the representation using the conditional cross-entropy loss L = H(y|z), then all the choices of B are equivalent, while if we add noise to the representation and use the IB loss, we naturally obtain that the optimal representation is disentangled. Proposition 4. Let x ∼ N (0, I), x̂ = Ax+ ξ, where A is orthogonal and ξ ∼ N (0, σ2ξ ). We want to find a representation z of x for the task y = eT2 x.\n(i) Assume the representation z is in the form z = Bx̂, where B is an orthogonal matrix. Then the cross-entropy loss L = H(y|z) does not depend on choice of B, so all the representations are equivalent for the cross-entropy loss.\n(ii) Assume the representation z is in the form z = Bx̂+ , where B is an orthogonal matrix and the noise has distribution ∼ N (0,Σ = diag(σ21 , σ22)). Without loss of generality, assume that σ2 ≤ σ1. Then, for β small enough, the representation that minimizes the IB Lagrangian\nL = H(y|z) + βI(x̂, z), is the disentangled representation obtained by choosing B = A−1 and, for β → 0+, we have σ1 → +∞ and σ2 → 0, so the added noise selectively preserve only information relative to the task.\nProof. For (i), notice that, since y = eT2 x = e T 2 (C −1z−Bξ) we have\ny|z ∼ N (eT2 C−1z, σ2ξ ), therefore,\nH(y|z) = log σξ + 1\n2 log(2πe)\nis independent from B.\nFor (ii), reasoning as before, we have\ny|z ∼ N (eT2 C−1z, σ2ξ + eT2 CTΣCe2),\nwhere we have defined C = BA. Since C is orthogonal, we can write Ce2 = (sin θ, cos θ) for some θ. Notice in particular that for θ = 0 we would have C = I , and so B = A−1. Using this, we can rewrite the expression above as\ny|z ∼ N (eT2 C−1z, σ2ξ + σ21 sin2 θ + σ22 cos2 θ).\nTherefore, the conditional entropy is\nH(y|z) = log(σ2ξ + σ21 sin2 θ + σ22 cos2 θ) + const.\nWe now need to compute the mutual information I(x̂; z) term of the IB Lagrangian. Recall that the mutual information between two N-dimensional Gaussian variables x and z is\nI(x; z) = 1\n2 log\n( |Σxx||Σzz|\n|Σ|\n) ,\nwhere Σxx and Σzz are the covariances matrixes of x and z respectively and Σ is the covariance matrix of the pair (x, z). Using this, we obtain\nI(x̂, z) = 1\n2\n[ log ( 1 +\n1 + σ2ξ σ21\n) + log ( 1 +\n1 + σ2ξ σ22\n)] .\nPutting everything together, the IB loss function is\nL = log(σ2ξ + σ21 cos2 θ + σ22 sin2 θ) + β\n2\n[ log ( 1 +\n1 + σ2ξ σ21\n) + log ( 1 +\n1 + σ2ξ σ22\n)] .\nSince θ appears only in the first term, we can immediately minimize the loss with respect to θ. Recall that we are assuming σ2 ≤ σ1. Then, there are two possibilities: if σ2 < σ1, then we must have θ = 0 (or equivalently C = I and hence B = A−1). If instead σ1 = σ2, then all θ are equivalent. In both cases, the loss function simplifies to\nL = log(σ2ξ + σ22) + β\n2\n[ log ( 1 +\n1 + σ2ξ σ21\n) + log ( 1 +\n1 + σ2ξ σ22\n)] .\nThe terms containing σ1 are now separate from those containing σ2 and we can minimize them independently. Doing so we obtain\nσ1 → +∞, σ2 = f(β)\nwhere f(β) is a function of β such that f(β)→ 0 as β → 0+. Finally, since we have now established that for β small enough we have σ2 < σ1, we can conclude from what we said before that we must have θ = 0, and therefore that B = A−1 and the representation is disentangled, as we wanted.\nWhile the previous proposition deals with a very simple case, under restrictive hypotheses on the form of the representation, we conjecture that a similar property should hold more generally for any representation. For example, assume that we can find a mapping f(x) = (T (x), n), where T (x) is a sufficient statistic of the data for the task y, while n is independent from y. That is, f(x) can “disentangle” the component of the data relevant to the task from the nuisances. Then, it would be easy to minimize the IB Lagrangian by routing all the noise on n, while leaving T (x) untouched. We claim that the opposite should also happen: When the IB Lagrangian is minimized, the representation should be decomposed in a part relevant for the task, and a part which is independent. We leave further study of this problem to a future work."
    }, {
      "heading" : "D ADDITIONAL PLOTS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "On invariance and selectivity in representation learning",
      "author" : [ "Fabio Anselmi", "Lorenzo Rosasco", "Tomaso Poggio" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Anselmi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2016
    }, {
      "title" : "Classification with scattering operators",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Bruna and Mallat.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bruna and Mallat.",
      "year" : 2011
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Variational dropout and the local reparameterization trick",
      "author" : [ "Diederik P. Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "In Proceedings of the 28th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual representations: Defining properties and deep approximations",
      "author" : [ "Stefano Soatto", "Alessandro Chiuso" ],
      "venue" : "Proc. of the Intl. Conf. on Learning Representations (ICLR); ArXiv:",
      "citeRegEx" : "Soatto and Chiuso.,? \\Q2016\\E",
      "shortCiteRegEx" : "Soatto and Chiuso.",
      "year" : 2016
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1412.6806,",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "On the set of images modulo viewpoint and contrast changes",
      "author" : [ "G. Sundaramoorthi", "P. Petersen", "V.S. Varadarajan", "S. Soatto" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Sundaramoorthi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sundaramoorthi et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep learning and the information bottleneck principle",
      "author" : [ "Naftali Tishby", "Noga Zaslavsky" ],
      "venue" : "In Information Theory Workshop (ITW),",
      "citeRegEx" : "Tishby and Zaslavsky.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tishby and Zaslavsky.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "For example the original formulation of dropout of Srivastava et al. (2014), which introduces binary multiplicative noise, was motivated as a way of efficiently training an ensemble of exponentially many networks, that would be averaged at testing time.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Kingma et al. (2015) introduce Variational Dropout, a dropout method which closely resemble ours, and is instead derived from a Bayesian analysis of neural networks.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "Dropout was introduced by Srivastava et al. (2014). The original motivation was that by randomly dropping the activations during training, we can effectively train an ensemble of exponentially many networks, that are then averaged during testing, therefore reducing overfitting.",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "Dropout was introduced by Srivastava et al. (2014). The original motivation was that by randomly dropping the activations during training, we can effectively train an ensemble of exponentially many networks, that are then averaged during testing, therefore reducing overfitting. Wang & Manning (2013) suggested that dropout could be seen as performing a Monte-Carlo approximation of an implicit loss function, and suggest that instead of multiplying the activations by binary noise, like in the original dropout, multiplicative Gaussian noise with mean 1 can be used as a way of better approximating the implicit loss function.",
      "startOffset" : 26,
      "endOffset" : 301
    }, {
      "referenceID" : 4,
      "context" : "Kingma et al. (2015) take a similar view of dropout as introducing multiplicative (Gaussian) noise, but instead study the problem from a Bayesian point of view.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "As an alternative, Kingma et al. (2015) suggest that the uncertainty about the weights that is expressed by the posterior distribution pθ(w|D) can equivalently be encoded as a multiplicative noise in the activations of the layers (the so called local reparametrization trick).",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "As an alternative, Kingma et al. (2015) suggest that the uncertainty about the weights that is expressed by the posterior distribution pθ(w|D) can equivalently be encoded as a multiplicative noise in the activations of the layers (the so called local reparametrization trick). As we will see in the following sections, this loss function closely resemble the one of Information Dropout, which however is derived from a purely information theoretic argument based on the Information Bottleneck principle. One difference is that we allow the parameters of the noise to change on a per-sample basis (which, as we show in the experiments, can be useful to deal with nuisances), and that we allow a scaling constant β in front of the KL-divergence term, which can be changed freely. Interestingly, even if the Bayesian derivation does not allow a rescaling of the KL-divergence, Kingma et al. (2015) notice that choosing a different scale for the KL-divergence term can indeed lead to improvements in practice.",
      "startOffset" : 19,
      "endOffset" : 895
    }, {
      "referenceID" : 9,
      "context" : "Some have focused on creating representations that are maximally invariant to nuisances, especially when they have the structure of a (possibly infinite-dimensional) group acting on the data, like Sundaramoorthi et al. (2009), or, when the nuisance is a locally compact group acting on each layer, by successive approximations implemented by hierarchical convolutional architectures, like Anselmi et al.",
      "startOffset" : 197,
      "endOffset" : 226
    }, {
      "referenceID" : 1,
      "context" : "(2009), or, when the nuisance is a locally compact group acting on each layer, by successive approximations implemented by hierarchical convolutional architectures, like Anselmi et al. (2016) and Bruna & Mallat (2011).",
      "startOffset" : 170,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "(2009), or, when the nuisance is a locally compact group acting on each layer, by successive approximations implemented by hierarchical convolutional architectures, like Anselmi et al. (2016) and Bruna & Mallat (2011). In these cases, which cover common nuisances such as translations and rotations of an image (affine group), or small diffeomorphic deformations due to a slight change of point of view (group of diffeomorphisms), the representation is equivalent to the data modulo the action of the group.",
      "startOffset" : 170,
      "endOffset" : 218
    }, {
      "referenceID" : 1,
      "context" : "(2009), or, when the nuisance is a locally compact group acting on each layer, by successive approximations implemented by hierarchical convolutional architectures, like Anselmi et al. (2016) and Bruna & Mallat (2011). In these cases, which cover common nuisances such as translations and rotations of an image (affine group), or small diffeomorphic deformations due to a slight change of point of view (group of diffeomorphisms), the representation is equivalent to the data modulo the action of the group. However, when the nuisances are not a group, as is the case for occlusions, it is not possible to achieve such equivalence, that is, there is a loss. To address this problem, Soatto & Chiuso (2016) defined optimal representations not in terms of maximality, but in terms of sufficiency, and characterized representations that are both sufficient and invariant.",
      "startOffset" : 170,
      "endOffset" : 706
    }, {
      "referenceID" : 4,
      "context" : "The only such prior is the improper log-uniform, p(log(z)) = c, or equivalently p(z) = c/z, which was also suggested by Kingma et al. (2015), but as a prior for the weights of the network, rather than the activations.",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "As Kingma et al. (2015) also notice, letting the variance of the noise increase too much leads to poor local minima in the optimization process.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "Similar to Kingma et al. (2015), to see the effect of Information Dropout on different network sizes and architectures, we train on MNIST a network with 3 fully connected hidden layers with a variable number of hidden units, and we train on CIFAR-10 (Krizhevsky & Hinton, 2009) the All-CNN-32 convolutional network described in Table 2, using a variable percentage of all the filters.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "The design of network is based on Springenberg et al. (2014), but we also add batch normalization before the activations of each layer.",
      "startOffset" : 34,
      "endOffset" : 61
    } ],
    "year" : 2017,
    "abstractText" : "We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.",
    "creator" : "LaTeX with hyperref package"
  }
}