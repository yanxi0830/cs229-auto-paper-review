{
  "name" : "615.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "L-SR1: A SECOND ORDER OPTIMIZATION METHOD",
    "authors" : [ "DEEP LEARNING", "Vivek Ramamurthy" ],
    "emails" : [ "vivek.ramamurthy@sentient.ai", "nigel.duffy@sentient.ai" ],
    "sections" : [ {
      "heading" : "1 MOTIVATION",
      "text" : "Second order methods hold great potential for distributing the training of deep neural networks. Due to their use of curvature information, they can often find good minima in far fewer steps than first order methods such as stochastic gradient descent (SGD). Moreover, stochastic second order methods can benefit from larger mini-batches (Le et al., 2011). This is because they estimate second derivatives via differences between estimated gradients. The gradient estimates need to have less variance, so that when we take their differences, the result has low variance. As a result they provide a different trade-off between number of steps and mini-batch size than do SGD-like methods. This trade-off is interesting, because while steps must be evaluated sequentially, a mini-batch may be evaluated in parallel. Thus, second order methods present an opportunity to extract more parallelism in neural network training. In particular, when mini-batches are sufficiently large, their evaluation may be distributed. Furthermore, there are relatively fewer hyperparameters to tune in second order methods, compared to variants of stochastic gradient descent.\nL-BFGS (Nocedal, 1980; Liu & Nocedal, 1989) is perhaps the most commonly used second order method in machine learning. BFGS is a quasi-Newton method that maintains an approximation to the inverse Hessian of the function being optimized. L-BFGS is a limited memory version of BFGS that stores the most recent updates to the inverse Hessian approximation and can therefore be used practically for large scale problems. L-BFGS is typically combined with a line search technique to choose an appropriate step size at each iteration. L-BFGS has been used to good effect in convex optimization problems in machine learning, but has not found effective use in large scale non-convex problems such as deep learning.\nThree critical weaknesses have been identified. First, we know that training deep neural networks involves minimizing non-convex error functions over continuous, high dimensional spaces. It has been argued that the proliferation of saddle points in these problems presents a deep and profound difficulty for quasi-Newton optimization methods (Dauphin et al., 2014). Furthermore, it has been argued that curvature matrices generated in second order methods are often ill-conditioned, and these need to be carefully repaired. A variety of approaches to this have been suggested, including the use of an empirical Fisher diagonal matrix (Martens, 2016). Finally, popular quasi-Newton\napproaches, such as L-BFGS (in their default form), require line search to make parameter updates, which requires many more gradient and/or function evaluations.\nWe propose L-SR1, a second order method that addresses each of these concerns. SR1 (Symmetric Rank One) is a quasi-Newton method that uses a rank one update for updating the Hessian approximation of the function being optimized (Nocedal & Wright, 2006). Unlike BFGS, the SR1 update does not guarantee positive definiteness of the updated matrix. This was considered a major problem in the early days of nonlinear optimization when only line search iterations were used, and possibly led to the obscurity of SR1 outside the optimization community. However, with the development of trust-region methods, the SR1 updating formula is potentially very useful, and its ability to generate indefinite Hessian approximations can actually prove to be advantageous.\nWe believe that it is possible to overcome saddle points using rank-one update based second order methods. The more common rank-two methods, e.g. L-BFGS, maintain a positive definite approximation to the inverse of the Hessian, by design (Nocedal & Wright, 2006). At saddle-points, the true Hessian cannot be well approximated by a positive definite matrix, causing commonly used second order methods to go uphill (Dauphin et al., 2014). On the other hand, rank-one approaches such as SR1 don’t maintain this invariant, so they can go downhill at saddle points. Numerical experiments (Conn et al., 1991) suggest that the approximate Hessian matrices generated by the SR1 method show faster progress towards the true Hessian than those generated by BFGS. This suggests that a limited memory SR1 method (L-SR1, if you like) could potentially outperform L-BFGS in the task of high dimensional optimization in neural network training. The building blocks needed to construct an L-SR1 method have been suggested in the literature (Byrd et al., 1994; Khalfan et al., 1993). To the best of our knowledge, however, there is no complete L-SR1 method previously described in the literature 1. This prompted us to develop and test the approach, specifically in the large scale non-convex problems that arise in deep learning.\nTwo other insights make L-SR1 practical by removing the requirement for a line search and addressing the conditioning problem. First, we replace the line search using a trust region approach. While L-BFGS using line search is well studied, recently, an L-BFGS method that uses a trustregion framework has also been proposed (Burke et al., 2008). Second, we combine L-SR1 with batch normalization. Batch normalization is a technique of normalizing inputs to layers of a neural network, used to address a phenomenon known as internal covariate shift during training (Ioffe & Szegedy, 2015). Our hypothesis is that batch normalization may cause parameters of a neural network to be suitably scaled so that the Hessian becomes better conditioned. We tested this hypothesis empirically and outline the results below."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "We now briefly summarize some other second order approaches that have been suggested in the literature, in order to place our approach in context. Pearlmutter (1994) derived a technique that directly calculated the product of the Hessian with an arbitrary vector, and applied this technique to a few variants of backpropagation, thereby showing a way to use the full Hessian without needing to compute and store it. Martens (2010) used a generalization of this technique, introduced by Schraudolph (2002), to develop a second order optimization method based on the “Hessian-free” approach, using it to train deep auto-encoders (Martens, 2010), as well as recurrent neural networks (Martens & Sutskever, 2011). The “Hessian-free” approach is essentially a line search Newton-CG (Conjugate Gradient) method, also known as the truncated Newton method (Nocedal & Wright, 2006), in which the search direction is computed by applying CG to the Newton method, and terminating it once it has made sufficient progress. This approach differs from ours in its use of line search instead of a trust region method. Moreover, it computes Hessian-vector products using finite differencing, as opposed to the limited-memory symmetric rank one update with trust region method, used in our approach. The cost of skipping the Hessian calculation in a truncated Newton method is one additional gradient evaluation per CG iteration (Nocedal & Wright, 2006). As mentioned previously, Dauphin et al. (2014) argue, that in high dimensional problems of practical interest, the proliferation of saddle points poses greater difficulty than local minima. In a bid to escape these saddle points, they propose second order optimization method called the saddle-free Newton method. Key to this\n1 The reference Brust et al. (2016) describes an approach to solve the trust region sub-problem encountered in an L-SR1 method, but does not describe the L-SR1 method itself.\napproach is the definition of a class of generalized trust region methods. This class extends classical trust region methods in a couple of ways. A first order Taylor expansion of the function is minimized, instead of the second order Taylor expansion. Moreover, the constraint on the step norm is replaced by generalized constraint on the distance between consecutive iterates. Our approach, by contrast, uses a a classical trust-region method. Rather than compute the Hessian exactly, Dauphin et al. (2014) use an approach similar Krylov subspace descent (Vinyals & Povey, 2012). The function is optimized in a lower-dimensional Krylov subspace, which is determined through Lanczos iteration of the Hessian (Vinyals & Povey, 2012). The Lanczos method may be considered a generalization of the CG method that can be applied to indefinite systems, and may be used to aid the CG method by gathering negative curvature information (Nocedal & Wright, 2006). The Lanczos method also involves finding an approximate solution to a trust-region subproblem in the range of a Krylov basis that it generates. This trust region problem differs from the one we solve, in that the Krylov basis generated has a special structure due to its mapping to a tridiagonal matrix (Nocedal & Wright, 2006).\nIt is worth noting that several approaches have been proposed to overcome the weaknesses of LBFGS. First, it has been proposed to initialize L-BFGS with a number of SGD steps. However, this diminishes the potential for parallelism (Dean et al., 2012; Le et al., 2011). Second, it has been proposed to use “forgetting”, where every few (say, for example, 5) steps, the history for L-BFGS is discarded. However, this greatly reduces the ability to use second order curvature information. There has also been a recent spurt of work on stochastic quasi-Newton methods for optimization. Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration. Mokhtari & Ribeiro (2014) propose RES, a regularized stochastic version of BFGS to solve convex optimization problems with stochastic objectives, and prove its convergence for bounded Hessian eigenvalues. Mokhtari & Ribeiro (2015) propose an online L-BFGS method for solving optimization problems with strongly convex stochastic objectives, and establish global almost sure convergence of their approach for bounded Hessian eigenvalues of sample functions. In the case of nonconvex stochastic optimization, Wang et al. (2014) propose, based on a general framework, two concrete stochastic quasi-Newton update strategies, namely stochastic damped-BFGS update and stochastic cyclic Barzilai-Borwein-like update, to adaptively generate positive definite Hessian approximations. They also analyze the almost sure convergence of these updates to stationary points. Keskar & Berahas (2015) propose ADAQN, a stochastic quasiNewton algorithm for training RNNs. This approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method also uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. Finally, Curtis (2016) proposes a variable-metric algorithm for stochastic nonconvex optimization which exploits fundamental self-correcting properties of BFGS-type updating, and uses it to solve a few machine learning problems. As one may notice, all of these approaches adapt the BFGS-style rank two updates in different ways to solve convex and non-convex problems. In contrast, our approach uses SR1-type updates, which we think can help better navigate the pathological saddle points present in the non-convex loss functions found in deep learning, by not constraining the Hessian approximation to be positive definite, as in the case of BFGS-style updates. Comparison of our approach with one of these recent stochastic second order methods is an interesting next step. In the Appendix, we provide a brief primer on line search and trust region methods, as well as on quasi-Newton methods and their limited memory variants."
    }, {
      "heading" : "3 ALGORITHM",
      "text" : "Our algorithm is synthesized as follows. We take the basic SR1 algorithm described in Nocedal & Wright (2006) (Algorithm 6.2), and represent the relevant input matrices using the limited-memory representations described in Byrd et al. (1994). The particular limited-memory representations used in the algorithm vary, depending on whether we use trust region or line search methods as subroutines to make parameter updates, as does some of the internal logic. For instance, if k updates are made to the symmetric matrix B0 using the vector pairs {si, yi}k−1i=0 and the SR1 formula, the resulting matrix Bk can be expressed as (Nocedal & Wright, 2006)\nBk = B0 + (Yk −B0Sk)(Dk + Lk + LTk − STk B0Sk)−1(Yk −B0Sk)T\nwhere Sk, Yk, Dk, and Lk are defined as follows:\nSk = [so, · · · , sk−1], andYk = [y0, · · · , yk−1] ,\n(Lk)i,j = { sTi−1yj−1 if i > j 0 otherwise\nDk = diag[sT0 y0, · · · , sTk−1yk−1] The self-duality of the SR1 method (Nocedal & Wright, 2006) allows the inverse formula Hk to be obtained simply by replacing B, s, and y by H , y, and s, respectively, using standard matrix identities. Limited-memory SR1 methods can be derived exactly like in the case of the BFGS method. Additional details are present in the pseudocode provided in the Appendix. The algorithm we develop is general enough to work with any line search or trust region method. While we tested the algorithm with line search approaches described in Dennis Jr. & Schnabel (1983), and with the trust region approach described in Brust et al. (2016), in this paper, we focus our experimental investigations on using the trust region approach, and the advantage that provides over using other first and second order optimization methods.\nWe also make a note here about the space and time complexity of our algorithm. We respectively denote by m and n, the memory size, and parameter dimensions. We assume m << n. As discussed in Section 7.2 of Nocedal & Wright (2006), the limited-memory updating procedure of Bk requires approximately 2mn+O(m3) operations, and matrix vector products of the form Bkv can be performed at a cost of (4m+ 1)n+O(m2) multiplications. Moreover, the Cholesky and eigenvalue decompositions we perform within our trust-region method form×mmatrices requireO(m3) operations. It follows quite easily2 from this that the space complexity of our algorithm is O(mn), and the per iteration time complexity of our algorithm is O(mn)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In the following, we summarize the results of training standard neural networks on the MNIST and CIFAR10 datasets using our approach, and benchmarking the performance with respect to other first and second order methods. First, we compared our L-SR1 (with trust region) approach, with Nesterov’s Accelerated Gradient Descent (NAG), L-BFGS with forgetting every 5 steps, default SGD, AdaDelta, and SGD with momentum, by training small standard networks on the MNIST and CIFAR10 datasets. On these problems, we also studied the effect of varying the minibatch size, for L-SR1, Adam (Kingma & Ba, 2014), and NAG. Next, we compared our L-SR1 with trust region approach with default hyperparameters, with a benchmark SGD with momentum, and Adam, by training a 20-layer deep residual network on the CIFAR10 dataset. Following that, we varied each hyperparameter of the L-SR1 with trust region approach to observe its effect on training the residual network on CIFAR10."
    }, {
      "heading" : "4.1 LENET-LIKE NETWORKS",
      "text" : "For each approach, and for each dataset, we considered the case where our networks had batch normalization layers within them, and the case where they did not. The parameters of the networks were randomly initialized. All experiments were repeated 10 times to generate error bars."
    }, {
      "heading" : "4.1.1 MNIST",
      "text" : "We considered the LeNet5 architecture in this case, which comprised 2 convolutional layers, followed by a fully connected layer and an outer output layer. Each convolutional layer was followed by a max-pooling layer. In the case where we used batch-normalization, each convolutional and fully connected layer was followed by a spatial batch normalization layer. We used a mini-batch size of 20 for the first order methods like NAG, SGD, AdaDelta and SGD with momentum, and a mini-batch size of 400 for the second order methods like L-SR1 and L-BFGS. The memory size was set to 5 for both L-SR1 and L-BFGS. The networks were trained for 20 epochs. Further details on the network architecture and other parameter settings are provided in the Appendix.\n2Deep neural networks typically have paramater dimensions in the tens of millions, while the memory size typically does not exceed 10. So n is indeed several orders of magnitude larger than m."
    }, {
      "heading" : "4.1.2 CIFAR10",
      "text" : "We considered a slight modification to the ‘LeNet5’ architecture described above. We used a minibatch size of 96 for NAG, SGD, AdaDelta and SGD with momentum. The other mini-batch sizes and memory sizes for L-SR1 and L-BFGS were as above. As above, the networks were trained for 20 epochs. Further details on the network architecture and other parameter settings are provided in the Appendix."
    }, {
      "heading" : "4.1.3 VARIATION OF MINIBATCH SIZE",
      "text" : "We also compared the variation of test loss between L-SR1, Adam and NAG, as we varied the mini-batch size from 500 to 1000 to 10000, in the presence of batch normalization. The network architectures were as above. For minibatch sizes 500 and 1000, we trained the networks for 50 epochs, while for the minibatch size of 10000, the networks were trained for 200 epochs."
    }, {
      "heading" : "4.1.4 DISCUSSION",
      "text" : "Our first set of experiments (Figures 1, 2) suggest that L-SR1 performs as well as, or slightly better than all the first order methods on both the MNIST and CIFAR10 datasets, with or without batch normalization. L-SR1 is substantially better than L-BFGS in all settings, with or without forgetting. Forgetting appears to be necessary in order to get L-BFGS to work. Without forgetting, the approach appears to be stuck where it is initialized. For this reason, the plots for L-BFGS without forgetting have not been included. Batch normalization appears to improve the performance of all approaches, particularly the early performance of second order approaches like L-SR1 and L-BFGS.\nThe experiments with variation of minibatch sizes (Figures 3, 4), seem to provide compelling evidence of the potential for distributed training of deep networks, as may be seen from Table 1. First, we note that first order methods like NAG are not as sensitive to size of the minibatch, as commonly understood. For example, a 20 fold increase in minibatch size did not decrease the speed of convergence by the same or higher order of magnitude. Furthermore, approaches like L-SR1 and Adam appear to be much less sensitive to increasing minibatch size than NAG. This strengthens the case for their application to distributed training of deep neural networks. Finally, while Adam makes much faster initial progress than the other approaches, its final test loss by the end of training is worse than in the case of L-SR1.\nOne of the limitations of SR1 updating is that the denominator in the update can vanish. The literature however suggests that this happens rarely enough that the updates can be skipped when this phenomenon occurs, without affecting performance. In this regard, we had some interesting observations from our experiments. While in most cases, updates were either never skipped, or skipped less than 2.5% of the time, the cases of MNIST training with batch normalization, yielded abnor-\nmally high levels of skipped updates, ranging all the way from 7% to higher than 60% (for minibatch size 10000). While this did not seem to affect performance adversely, it certainly warrants future investigation. Moreover, a better understanding of the interplay between batch normalization and optimization could help inform potential improvements in optimization approaches."
    }, {
      "heading" : "4.2 RESIDUAL NETWORKS",
      "text" : "We next considered a deeper residual network architecture described in section 4.2 of He et al. (2015b), with n = 3. This led to a 20-layer residual network including 9 shortcut connections. As in He et al. (2015b), we used batch normalization (Ioffe & Szegedy, 2015) and the same initialization method (He et al., 2015a)."
    }, {
      "heading" : "4.2.1 COMPARISON WITH SGD WITH MOMENTUM, AND ADAM",
      "text" : "We trained the residual network using the benchmark SGD with momentum, and other parameter settings as described in He et al. (2015b). We also trained the network using L-SR1 with default settings. These included, a memory size of 5, a trust-region radius decrease factor of 0.5, and a trust-region radius increase factor of 2.0. Finally, we also compared with Adam, with default settings (Kingma & Ba, 2014). We used the same mini-batch size of 128 for all algorithms. Based on the learning rate schedule used, the learning rate was equal to 0.1 through the first 80 epochs, 0.01 up to 120 epochs, and 0.001 thereafter, for SGD with momentum. Figure 5 shows variation of test loss, over epochs, and by time. It needs to be noted that default L-SR1, with no parameter tuning at all, has a superior final test loss to Adam, and is competitive with SGD with momentum, which used custom parameters that were tuned carefully. L-SR1 does make slower progress over time, which can be further optimized. Finally, we note that the test loss for L-SR1 bounces around a lot more than the test loss for the other algorithms. This bears further exploration."
    }, {
      "heading" : "4.2.2 VARIATION OF L-SR1 HYPERPARAMETERS",
      "text" : "We varied the hyperparameters of L-SR1 in turn, keeping the remaining fixed. In each case, we trained the network for 200 epochs. We first considered varying the increase and decrease factors together. We considered a trust-region radius decrease factor of 0.2, 0.5 and 0.8, and a trust-region radius increase factor 1.2 and 2.0. The respective default values of these factors are 0.5 and 2.0 respectively. This led to six different combinations of decrease and increase factors. We kept the memory size and mini-batch size fixed at 5 and 128 respectively. Next, we considered memory sizes of 2 and 10 (in addition to 5, which we tried earlier), keeping the mini-batch size, decrease factor, and increase factor fixed at 128, 0.5, and 2.0 respectively. Finally, we considered mini-batch sizes of 512, 2048 and 8192 (in addition to 128, which we tried earlier), keeping the memory size, decrease factor, and increase factor fixed at 5, 0.5, and 2.0 respectively. Figure 6 shows the results.\nThe following may be noted, based on the experiments with L-SR1 for training a residual network on CIFAR10. While there is potential value in increasing and decreasing the trust region radius at different rates, our experiments suggest that it may not be necessary to tune these hyperparameters. There is no noticeable performance gain from using a higher memory size in L-SR1. Furthermore, using a smaller memory size performs at least as well as in the default case. This is good news, due to the consequent savings in storage and computational resources. L-SR1 is relatively insensitive to a 4-fold increase in mini-batch size from 128 to 512, and a further 4-fold increase to 2048. The minibatch sensitivity of L-SR1 seems to be higher in the case of the residual network, compared\nwith the Le-Net like networks seen earlier. Finally, we found the proportion of skipped updates in the case of residual networks to be less than 0.5% in all cases."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "In this paper, we have described L-SR1, a new second order method to train deep neural networks. Our experiments suggest that this approach is at the very least, competitive, with other first order methods, and substantially better than L-BFGS, a well-known second order method. Our experiments also appear to validate our intuition about the ability of L-SR1 to overcome key challenges associated with second order methods, such as inappropriate handling of saddle points, and poor conditioning of the Hessian. Our experimentation with the hyperparameters of L-SR1 suggested that it is relatively robust with respect to them, and requires minimal tuning. Furthermore, we have evidence to suggest that L-SR1 is much more insensitive to larger minibatch sizes than a first order method like NAG. This suggests that L-SR1 holds promise for distributed training of deep networks, and we see our work as an important step toward that goal."
    }, {
      "heading" : "APPENDIX",
      "text" : ""
    }, {
      "heading" : "BACKGROUND",
      "text" : "In the following, we provide a brief primer on line search and trust region methods, as well as on quasi-Newton methods and their limited memory variants. Further details may be found in Nocedal & Wright (2006)."
    }, {
      "heading" : "LINE SEARCH AND TRUST REGION METHODS",
      "text" : "In any optimization algorithm, there are two main ways of moving from the current point xk to a new iterate xk+1. One of them is line search. In it, the algorithm picks a descent direction pk and searches along this direction from the current iterate xk for a new iterate with a lower function value. The distance α to move along pk can be found by solving the following one-dimensional minimization problem:\nmin α>0 f(xk + αpk)\nInstead of an exact minimization which may be expensive, the line search algorithm generates a limited number of trial step lengths until it finds one that generates a sufficient decrease in function\nvalue. At the new point, the process of computing the descent direction and step length is repeated. The other way is to use a trust region method. In a trust region method, the information about f is used to construct a model function mk, which is supposed to approximate f near the current point xk. Since the model mk may not approximate f well when x is far from xk, the search for a minimizer of mk is restricted to some trust region within a radius ∆k around xk. To wit, the candidate step p approximately solves the following sub-problem:\nmin p:‖p‖≤∆k mk(xk + p),\nIf the candidate solution does not produce a sufficient decrease in f , the trust region is considered too large for the model function to approximate f well. So we shrink the trust region and re-solve. Essentially, the line search and trust region approaches differ in the order in which they choose the direction and magnitude of the move to the next iterate. In line search, the descent direction pk is fixed first, and then the step length αk to be taken along that direction is computed. In trust region, a maximum distance equal to the trust-region radius ∆k is first set, and then a direction is determined within this radius, that achieves the best improvement in the objective value. If such a direction does not yield sufficient improvement, the model function is determined to be a poor approximation to the function, and the trust-region radius ∆k is reduced until the approximation is deemed good enough. Conversely, as long as the model function appears to approximate the objective function well, the trust region radius is increased until the approximation is not good enough."
    }, {
      "heading" : "LIMITED MEMORY QUASI-NEWTON METHODS",
      "text" : "Quasi-Newton methods are a useful alternative to Newton’s method in that they do not require computation of the exact Hessian, and yet still attain good convergence. In place of the true Hessian ∇2fk, they use an approximation Bk, which is updated after each step based on information gained during the step. At each step, the new Hessian approximation Bk+1 is required to satisfy the following condition, known as the secant equation:\nBk+1sk = yk\nwhere sk = xk+1 − xk, yk = ∇fk+1 −∇fk\nTypically, Bk+1, is also required to be symmetric (like the exact Hessian), and the difference between successive approximations Bk and Bk+1 is constrained to have low rank. One of the most popular formulae for updating the Hessian approximation Bk is the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by\nBk+1 = Bk − Bksks\nT kBk\nsTkBksk + yky\nT k yTk sk\nA less well known formula, particularly in the machine learning community, is the symmetric-rankone (SR1) formula, defined by\nBk+1 = Bk + (yk −Bksk)(yk −Bksk)T\n(yk −Bksk)T sk The former update is a rank-two update, while the latter is a rank-one update. Both updates satisfy the secant equation and maintain symmetry. The BFGS update always generates positive definite approximations whenever the initial approximation B0 is positive definite and sTk yk > 0. Often, in practical implementations of quasi-Newton methods, the inverse Hessian approximation Hk is used instead of the Bk,and the corresponding update formulae can be generated using the ShermanMorrison-Woodbury matrix identity (Hager, 1989).\nLimited-memory quasi-Newton methods are useful for solving large problems where computation of Hessian matrices is costly or when these matrices are dense. Instead of storing fully dense n× n approximations, these methods save only a few vectors of length n that capture the approximations. Despite these modest storage requirements, they often converge well. The most popular limited memory quasi-Newton method is L-BFGS, which uses curvature information from only the most recent iterations to construct the inverse Hessian approximation. Curvature information from earlier\niterations, which is less likely to be useful to modeling the actual behavior of the Hessian at the current iteration, is discarded in order to save memory.\nLimited-memory quasi-Newton approximations can be used with line search or trust region methods. As described in Byrd et al. (1994), we can derive efficient limited memory implementations of several quasi-Newton update formulae, and their inverses."
    }, {
      "heading" : "NETWORK ARCHITECTURES AND HYPERPARAMETER SETTINGS",
      "text" : ""
    }, {
      "heading" : "MNIST",
      "text" : "The layers of the LeNet5 architecture used, are described below. All the batch normalization layers were removed, in the ‘without batch normalization’ case.\n• Convolutional Layer - filter size 5 × 5, 20 feature maps, stride 1, padding 0, and a ReLU activation function with bias 0 and Gaussian noise with mean 0 and standard deviation 0.1\n• Spatial Batch Normalization Layer • Max Pooling Layer - filter size 2 • Convolutional Layer - filter size 5 × 5, 50 feature maps, stride 1, padding 0, and a ReLU\nactivation function with bias 0 and Gaussian noise with mean 0 and standard deviation 0.1 • Spatial Batch Normalization Layer • Max Pooling Layer - filter size 2 • Fully Connected Layer - 500 hidden units, and a tangent hyperbolic activation function • Spatial Batch Normalization Layer • Outer Output Layer - 10 outputs and output standard deviation of 0.1\nAdditionally, the network was trained with L2 regularization with parameter 0.0001. Training loss was measured as softmax cross entropy, while test loss was measured as multi-class error count. In the case of the first order methods, the learning rate was set to 0.003 where needed, and the momentum was set to 0.9, where needed. AdaDelta did not take any parameters."
    }, {
      "heading" : "CIFAR10",
      "text" : "The layers of the architecture used, are described below. All the batch normalization layers were removed, in the ‘without batch normalization’ case.\n• Convolutional Layer - filter size 5 × 5, 32 feature maps, stride 1, padding 2, and a ReLU activation function with bias 0 and Gaussian noise with mean 0 and standard deviation 0.01\n• Spatial Batch Normalization Layer • Max Pooling Layer - filter size 2 • Activation Layer - ReLU activation function with bias 0 and Gaussian noise with mean 0\nand standard deviation 0.1 • Convolutional Layer - filter size 5 × 5, 32 feature maps, stride 1, padding 2, and a ReLU\nactivation function with bias 0 and Gaussian noise with mean 0 and standard deviation 0.01 • Spatial Batch Normalization Layer • Max Pooling Layer - filter size 2 • Convolutional Layer - filter size 5 × 5, 64 feature maps, stride 1, padding 2, and a ReLU\nactivation function with bias 0 and Gaussian noise with mean 0 and standard deviation 0.01 • Spatial Batch Normalization Layer • Max Pooling Layer - filter size 2 • Fully Connected Layer - 64 hidden units, and a ReLU activation function with bias 0 and\nGaussian noise with mean 0 and standard deviation 0.1 • Spatial Batch Normalization Layer\n• Outer Output Layer - 10 outputs and output standard deviation of 0.1\nAdditionally, the network was trained with L2 regularization with parameter 0.001. Training loss was measured as softmax cross entropy, while test loss was measured as multi-class error count. In the case of the first order methods, the learning rate was set to 0.01 where needed, and the momentum was set to 0.9, where needed. AdaDelta did not take any parameters."
    }, {
      "heading" : "PSEUDOCODE",
      "text" : "Algorithm 1 provides the pseudocode for L-SR1 with trust region method, while Algorithm 2 provides the pseudocode for L-SR1 with line search.\nAlgorithm 1 L-SR1 with Trust Region Method Require: Sk = [s0, · · · , sk−1], Yk = [y0, · · · , yk−1], starting point x0 ∈ Rn, limited memory size\nm << n, initial Hessian approximation B0 (a diagonal matrix, typically γIn, γ 6= 0), initial trust-region radius ∆0 = ‖∇f(x0)‖2, convergence tolerance t > 0, maximum iterations K, parameters η ∈ (0, 10−3), r ∈ (0, 1), and column dimension colDim; 1: k ← 0 2: while k < K and ‖∇f(xk)‖2 > t and ‖sk‖2 > t do 3: if k = 0 or Sk.colDim = 0 then 4: sk ← −∇f(xk) 5: Bk ← B0 6: else 7: sk ← TrustRegionMethod(Ψk,M−1k ,∇f(xk),∆k, γ, B0) (Solve the trust-region sub-\nproblem) 8: Bksk ← B0sk + ΨkMk(ΨTk sk) 9: end if\n10: pred← − ( ∇f(xk)T sk + 12s T kBksk ) (predicted reduction) 11: ared← f(xk)− f(xk + sk) (actual reduction) 12: yk ← ∇f(xk + sk)−∇f(xk) 13: if ared/pred > η then 14: xk+1 ← xk + sk 15: else 16: xk+1 ← xk 17: end if 18: ∆k+1 ← ∆k 19: if ared/pred > u then (u = 0.75 by default) 20: if ‖sk‖ > ρ∆k then (ρ = 0.8 by default) 21: ∆k+1 ← 2∆k 22: end if 23: else if ared/pred < l then (l = 0.1 by default) 24: ∆k+1 ← 0.5∆k 25: end if 26: if |sTk (yk −Bksk)| ≥ r‖sk‖‖yk −Bksk‖ then 27: Sk+1 ← [Sk, sk], Yk+1 ← [Yk, yk] 28: if Sk+1.colDim > m then 29: Sk+1 ← Sk+1[, 2 : m+ 1], Yk+1 ← Yk+1[, 2 : m+ 1] 30: end if 31: while Sk+1.colDim > 0 do 32: Ψk+1 ← Yk+1 −B0Sk+1 33: M−1k+1 ← STk+1Yk+1 − STk+1B0Sk+1 34: if (ΨTk+1Ψk+1 0 and |M −1 k+1| 6= 0) then 35: break 36: else 37: Remove the first columns of Sk+1 and Yk+1 38: end if 39: end while 40: end if 41: k ← k + 1 42: end while\nAlgorithm 2 L-SR1 with Line Search Require: Sk = [s0, · · · , sk−1], Yk = [y0, · · · , yk−1], starting point x0 ∈ Rn, limited memory size\nm << n, initial inverse Hessian approximationH0 (a diagonal matrix, typically In), initial step length λ0 = 1, convergence tolerance t > 0, maximum iterations K, r ∈ (0, 1), and column dimension colDim;\n1: k ← 0 2: while k < K and (‖∇f(xk)‖2 > t or ‖sk‖2 > t) do 3: if k = 0 or Sk.colDim = 0 then 4: dk ← −∇f(xk) 5: Hk ← H0 6: else 7: dk ← −H0∇f(xk)−ΨkMk(ΨTk∇f(xk)) 8: end if 9: if k > 0 then\n10: λ0 = min{1, 2λk−1} 11: end if 12: λk ←computeStepLength(f, xk, dk, λ0) (perform line search) 13: sk ← λkdk 14: xk+1 ← xk + sk 15: yk ← ∇f(xk+1)−∇f(xk) 16: if k > 0 and Sk.colDim > 0 then 17: Hkyk ← H0yk + ΨkMk(ΨTk yk) 18: end if 19: if |yTk (sk −Hkyk)| ≥ r‖yk‖‖sk −Hkyk‖ then 20: Sk+1 ← [Sk, sk], Yk+1 ← [Yk, yk] 21: if Sk+1.colDim > m then 22: Sk+1 ← Sk+1[, 2 : m+ 1], Yk+1 ← Yk+1[, 2 : m+ 1] 23: end if 24: while Sk+1.colDim > 0 do 25: Ψk+1 ← Sk+1 −H0Yk+1 26: M−1k+1 ← Y Tk+1Sk+1 − Y Tk+1H0Yk+1 27: if (|M−1k+1| 6= 0) then 28: break 29: else 30: Remove the first columns of Sk+1 and Yk+1 31: end if 32: end while 33: end if 34: k ← k + 1 35: end while"
    } ],
    "references" : [ {
      "title" : "On solving l-sr1 trust-region",
      "author" : [ "Johannes Brust", "Jennifer B. Erway", "Roummel F. Marcia" ],
      "venue" : "subproblems. arXiv.org,",
      "citeRegEx" : "Brust et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Brust et al\\.",
      "year" : 2016
    }, {
      "title" : "Limited memory bfgs updating in a trust-region framework",
      "author" : [ "J.V. Burke", "A. Wiegman", "L. Xu" ],
      "venue" : null,
      "citeRegEx" : "Burke et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Burke et al\\.",
      "year" : 2008
    }, {
      "title" : "Representations of quasi-newton matrices and their use in limited-memory methods",
      "author" : [ "Richard H. Byrd", "Jorge Nocedal", "Robert B. Schnabel" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Byrd et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 1994
    }, {
      "title" : "A stochastic quasi-newton method for large-scale optimization",
      "author" : [ "Richard H. Byrd", "S.L. Hansen", "Jorge Nocedal", "Yoram Singer" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Byrd et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 2016
    }, {
      "title" : "Convergence of quasi-newton matrices generated by the symmetric rank one update",
      "author" : [ "A.R. Conn", "N.I.M. Gould", "Ph. L. Toint" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Conn et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Conn et al\\.",
      "year" : 1991
    }, {
      "title" : "A self-correcting variable-metric algorithm for stochastic optimization",
      "author" : [ "Frank Curtis" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "Curtis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Curtis.",
      "year" : 2016
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann Dauphin", "Razvan Pascanu", "Çaglar Gülçehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1406.2572,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Marc Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le", "Andrew Y. Ng" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dean et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2012
    }, {
      "title" : "Numerical methods for unconstrained optimization and nonlinear equations",
      "author" : [ "John E. Dennis Jr.", "Robert B. Schnabel" ],
      "venue" : null,
      "citeRegEx" : "Jr. and Schnabel.,? \\Q1983\\E",
      "shortCiteRegEx" : "Jr. and Schnabel.",
      "year" : 1983
    }, {
      "title" : "Updating the inverse of a matrix",
      "author" : [ "William W. Hager" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Hager.,? \\Q1989\\E",
      "shortCiteRegEx" : "Hager.",
      "year" : 1989
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1502.01852,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "shift. CoRR,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "adaqn: An adaptive quasi-newton algorithm for training rnns",
      "author" : [ "Nitish Shirish Keskar", "Albert S. Berahas" ],
      "venue" : "CoRR, abs/1511.01169,",
      "citeRegEx" : "Keskar and Berahas.,? \\Q2015\\E",
      "shortCiteRegEx" : "Keskar and Berahas.",
      "year" : 2015
    }, {
      "title" : "A theoretical and experimental study of the symmetric rank one update",
      "author" : [ "Humaid Khalfan", "Richard H. Byrd", "Robert B. Schnabel" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Khalfan et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Khalfan et al\\.",
      "year" : 1993
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "On optimization methods for deep learning",
      "author" : [ "Quoc V. Le", "Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Andrew Y. Ng" ],
      "venue" : "In Lise Getoor and Tobias Scheffer (eds.),",
      "citeRegEx" : "Le et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2011
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "Dong C. Liu", "Jorge Nocedal" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Liu and Nocedal.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu and Nocedal.",
      "year" : 1989
    }, {
      "title" : "Deep learning via hessian-free optimization",
      "author" : [ "James Martens" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Martens.,? \\Q2010\\E",
      "shortCiteRegEx" : "Martens.",
      "year" : 2010
    }, {
      "title" : "Second-Order Optimization for Neural Networks",
      "author" : [ "James Martens" ],
      "venue" : "PhD thesis, Graduate Department of Computer Science, University of Toronto,",
      "citeRegEx" : "Martens.,? \\Q2016\\E",
      "shortCiteRegEx" : "Martens.",
      "year" : 2016
    }, {
      "title" : "Learning recurrent neural networks with hessian-free optimization",
      "author" : [ "James Martens", "Ilya Sutskever" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Martens and Sutskever.,? \\Q2011\\E",
      "shortCiteRegEx" : "Martens and Sutskever.",
      "year" : 2011
    }, {
      "title" : "RES: regularized stochastic BFGS algorithm",
      "author" : [ "Aryan Mokhtari", "Alejandro Ribeiro" ],
      "venue" : "IEEE Trans. Signal Processing,",
      "citeRegEx" : "Mokhtari and Ribeiro.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mokhtari and Ribeiro.",
      "year" : 2014
    }, {
      "title" : "Global convergence of online limited memory bfgs",
      "author" : [ "Aryan Mokhtari", "Alejandro Ribeiro" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Mokhtari and Ribeiro.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mokhtari and Ribeiro.",
      "year" : 2015
    }, {
      "title" : "Updating quasi-newton matrices with limited storage",
      "author" : [ "Jorge Nocedal" ],
      "venue" : "Mathematics of Computation, 35(151):773–782,",
      "citeRegEx" : "Nocedal.,? \\Q1980\\E",
      "shortCiteRegEx" : "Nocedal.",
      "year" : 1980
    }, {
      "title" : "Numerical Optimization. Springer-Verlag, New York, 2 edition",
      "author" : [ "Jorge Nocedal", "Stephen J. Wright" ],
      "venue" : null,
      "citeRegEx" : "Nocedal and Wright.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nocedal and Wright.",
      "year" : 2006
    }, {
      "title" : "Fast exact multiplication by the hessian",
      "author" : [ "Barak A. Pearlmutter" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Pearlmutter.,? \\Q1994\\E",
      "shortCiteRegEx" : "Pearlmutter.",
      "year" : 1994
    }, {
      "title" : "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent",
      "author" : [ "Nicol N. Schraudolph" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schraudolph.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schraudolph.",
      "year" : 2002
    }, {
      "title" : "Krylov subspace descent for deep learning",
      "author" : [ "Oriol Vinyals", "Daniel Povey" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Vinyals and Povey.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vinyals and Povey.",
      "year" : 2012
    }, {
      "title" : "Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization",
      "author" : [ "X. Wang", "S. Ma", "W. Liu" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Moreover, stochastic second order methods can benefit from larger mini-batches (Le et al., 2011).",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "L-BFGS (Nocedal, 1980; Liu & Nocedal, 1989) is perhaps the most commonly used second order method in machine learning.",
      "startOffset" : 7,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "It has been argued that the proliferation of saddle points in these problems presents a deep and profound difficulty for quasi-Newton optimization methods (Dauphin et al., 2014).",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 19,
      "context" : "A variety of approaches to this have been suggested, including the use of an empirical Fisher diagonal matrix (Martens, 2016).",
      "startOffset" : 110,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "At saddle-points, the true Hessian cannot be well approximated by a positive definite matrix, causing commonly used second order methods to go uphill (Dauphin et al., 2014).",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "Numerical experiments (Conn et al., 1991) suggest that the approximate Hessian matrices generated by the SR1 method show faster progress towards the true Hessian than those generated by BFGS.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "The building blocks needed to construct an L-SR1 method have been suggested in the literature (Byrd et al., 1994; Khalfan et al., 1993).",
      "startOffset" : 94,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "The building blocks needed to construct an L-SR1 method have been suggested in the literature (Byrd et al., 1994; Khalfan et al., 1993).",
      "startOffset" : 94,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "While L-BFGS using line search is well studied, recently, an L-BFGS method that uses a trustregion framework has also been proposed (Burke et al., 2008).",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "Martens (2010) used a generalization of this technique, introduced by Schraudolph (2002), to develop a second order optimization method based on the “Hessian-free” approach, using it to train deep auto-encoders (Martens, 2010), as well as recurrent neural networks (Martens & Sutskever, 2011).",
      "startOffset" : 211,
      "endOffset" : 226
    }, {
      "referenceID" : 21,
      "context" : "Pearlmutter (1994) derived a technique that directly calculated the product of the Hessian with an arbitrary vector, and applied this technique to a few variants of backpropagation, thereby showing a way to use the full Hessian without needing to compute and store it.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "Martens (2010) used a generalization of this technique, introduced by Schraudolph (2002), to develop a second order optimization method based on the “Hessian-free” approach, using it to train deep auto-encoders (Martens, 2010), as well as recurrent neural networks (Martens & Sutskever, 2011).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 17,
      "context" : "Martens (2010) used a generalization of this technique, introduced by Schraudolph (2002), to develop a second order optimization method based on the “Hessian-free” approach, using it to train deep auto-encoders (Martens, 2010), as well as recurrent neural networks (Martens & Sutskever, 2011).",
      "startOffset" : 0,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "As mentioned previously, Dauphin et al. (2014) argue, that in high dimensional problems of practical interest, the proliferation of saddle points poses greater difficulty than local minima.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "1 The reference Brust et al. (2016) describes an approach to solve the trust region sub-problem encountered in an L-SR1 method, but does not describe the L-SR1 method itself.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Rather than compute the Hessian exactly, Dauphin et al. (2014) use an approach similar Krylov subspace descent (Vinyals & Povey, 2012).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "However, this diminishes the potential for parallelism (Dean et al., 2012; Le et al., 2011).",
      "startOffset" : 55,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "However, this diminishes the potential for parallelism (Dean et al., 2012; Le et al., 2011).",
      "startOffset" : 55,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration. Mokhtari & Ribeiro (2014) propose RES, a regularized stochastic version of BFGS to solve convex optimization problems with stochastic objectives, and prove its convergence for bounded Hessian eigenvalues.",
      "startOffset" : 0,
      "endOffset" : 272
    }, {
      "referenceID" : 2,
      "context" : "Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration. Mokhtari & Ribeiro (2014) propose RES, a regularized stochastic version of BFGS to solve convex optimization problems with stochastic objectives, and prove its convergence for bounded Hessian eigenvalues. Mokhtari & Ribeiro (2015) propose an online L-BFGS method for solving optimization problems with strongly convex stochastic objectives, and establish global almost sure convergence of their approach for bounded Hessian eigenvalues of sample functions.",
      "startOffset" : 0,
      "endOffset" : 477
    }, {
      "referenceID" : 2,
      "context" : "Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration. Mokhtari & Ribeiro (2014) propose RES, a regularized stochastic version of BFGS to solve convex optimization problems with stochastic objectives, and prove its convergence for bounded Hessian eigenvalues. Mokhtari & Ribeiro (2015) propose an online L-BFGS method for solving optimization problems with strongly convex stochastic objectives, and establish global almost sure convergence of their approach for bounded Hessian eigenvalues of sample functions. In the case of nonconvex stochastic optimization, Wang et al. (2014) propose, based on a general framework, two concrete stochastic quasi-Newton update strategies, namely stochastic damped-BFGS update and stochastic cyclic Barzilai-Borwein-like update, to adaptively generate positive definite Hessian approximations.",
      "startOffset" : 0,
      "endOffset" : 772
    }, {
      "referenceID" : 2,
      "context" : "Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration. Mokhtari & Ribeiro (2014) propose RES, a regularized stochastic version of BFGS to solve convex optimization problems with stochastic objectives, and prove its convergence for bounded Hessian eigenvalues. Mokhtari & Ribeiro (2015) propose an online L-BFGS method for solving optimization problems with strongly convex stochastic objectives, and establish global almost sure convergence of their approach for bounded Hessian eigenvalues of sample functions. In the case of nonconvex stochastic optimization, Wang et al. (2014) propose, based on a general framework, two concrete stochastic quasi-Newton update strategies, namely stochastic damped-BFGS update and stochastic cyclic Barzilai-Borwein-like update, to adaptively generate positive definite Hessian approximations. They also analyze the almost sure convergence of these updates to stationary points. Keskar & Berahas (2015) propose ADAQN, a stochastic quasiNewton algorithm for training RNNs.",
      "startOffset" : 0,
      "endOffset" : 1130
    }, {
      "referenceID" : 2,
      "context" : "Byrd et al. (2016) propose a stochastic quasi-Newton method which uses the classical L-BFGS formula, but collects curvature information pointwise, at regular intervals, through sub-sampled Hessian vector products, rather than at every iteration. Mokhtari & Ribeiro (2014) propose RES, a regularized stochastic version of BFGS to solve convex optimization problems with stochastic objectives, and prove its convergence for bounded Hessian eigenvalues. Mokhtari & Ribeiro (2015) propose an online L-BFGS method for solving optimization problems with strongly convex stochastic objectives, and establish global almost sure convergence of their approach for bounded Hessian eigenvalues of sample functions. In the case of nonconvex stochastic optimization, Wang et al. (2014) propose, based on a general framework, two concrete stochastic quasi-Newton update strategies, namely stochastic damped-BFGS update and stochastic cyclic Barzilai-Borwein-like update, to adaptively generate positive definite Hessian approximations. They also analyze the almost sure convergence of these updates to stationary points. Keskar & Berahas (2015) propose ADAQN, a stochastic quasiNewton algorithm for training RNNs. This approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method also uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. Finally, Curtis (2016) proposes a variable-metric algorithm for stochastic nonconvex optimization which exploits fundamental self-correcting properties of BFGS-type updating, and uses it to solve a few machine learning problems.",
      "startOffset" : 0,
      "endOffset" : 1486
    }, {
      "referenceID" : 21,
      "context" : "We take the basic SR1 algorithm described in Nocedal & Wright (2006) (Algorithm 6.",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "2), and represent the relevant input matrices using the limited-memory representations described in Byrd et al. (1994). The particular limited-memory representations used in the algorithm vary, depending on whether we use trust region or line search methods as subroutines to make parameter updates, as does some of the internal logic.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : "Dk = diag[s0 y0, · · · , sk−1yk−1] The self-duality of the SR1 method (Nocedal & Wright, 2006) allows the inverse formula Hk to be obtained simply by replacing B, s, and y by H , y, and s, respectively, using standard matrix identities. Limited-memory SR1 methods can be derived exactly like in the case of the BFGS method. Additional details are present in the pseudocode provided in the Appendix. The algorithm we develop is general enough to work with any line search or trust region method. While we tested the algorithm with line search approaches described in Dennis Jr. & Schnabel (1983), and with the trust region approach described in Brust et al.",
      "startOffset" : 71,
      "endOffset" : 595
    }, {
      "referenceID" : 0,
      "context" : "& Schnabel (1983), and with the trust region approach described in Brust et al. (2016), in this paper, we focus our experimental investigations on using the trust region approach, and the advantage that provides over using other first and second order optimization methods.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "2 of Nocedal & Wright (2006), the limited-memory updating procedure of Bk requires approximately 2mn+O(m) operations, and matrix vector products of the form Bkv can be performed at a cost of (4m+ 1)n+O(m) multiplications.",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "2 of He et al. (2015b), with n = 3.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "2 of He et al. (2015b), with n = 3. This led to a 20-layer residual network including 9 shortcut connections. As in He et al. (2015b), we used batch normalization (Ioffe & Szegedy, 2015) and the same initialization method (He et al.",
      "startOffset" : 5,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "We trained the residual network using the benchmark SGD with momentum, and other parameter settings as described in He et al. (2015b). We also trained the network using L-SR1 with default settings.",
      "startOffset" : 116,
      "endOffset" : 134
    } ],
    "year" : 2017,
    "abstractText" : "We describe L-SR1, a new second order method to train deep neural networks. Second order methods hold great promise for distributed training of deep networks. Unfortunately, they have not proven practical. Two significant barriers to their success are inappropriate handling of saddle points, and poor conditioning of the Hessian. L-SR1 is a practical second order method that addresses these concerns. We provide experimental results showing that L-SR1 performs at least as well as Nesterov’s Accelerated Gradient Descent, on the MNIST and CIFAR10 datasets. For the CIFAR10 dataset, we see competitive performance on shallow networks like LeNet5, as well as on deeper networks like residual networks. Furthermore, we perform an experimental analysis of L-SR1 with respect to its hyperparameters to gain greater intuition. Finally, we outline the potential usefulness of L-SR1 in distributed training of deep neural networks.",
    "creator" : "LaTeX with hyperref package"
  }
}