{
  "name" : "433.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "REAL NVP", "Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The domain of representation learning has undergone tremendous advances due to improved supervised learning techniques. However, unsupervised learning has the potential to leverage large pools of unlabeled data, and extend these advances to modalities that are otherwise impractical or impossible.\nOne principled approach to unsupervised learning is generative probabilistic modeling. Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [61, 46, 59], denoising [3], colorization [71], and super-resolution [9].\nAs data of interest are generally high-dimensional and highly structured, the challenge in this domain is building models that are powerful enough to capture its complexity yet still trainable. We address this challenge by introducing real-valued non-volume preserving (real NVP) transformations, a tractable yet expressive approach to modeling high-dimensional data.\nThis model can perform efficient and exact inference, sampling and log-density estimation of data points. Moreover, the architecture presented in this paper enables exact and efficient reconstruction of input images from the hierarchical features extracted by this model."
    }, {
      "heading" : "2 Related work",
      "text" : "Substantial work on probabilistic generative models has focused on training models using maximum likelihood. One class of maximum likelihood models are those described by probabilistic undirected graphs, such as Restricted Boltzmann Machines [58] and Deep Boltzmann Machines [53]. These models are trained by taking advantage of the conditional independence property of their bipartite structure to allow efficient exact or approximate posterior inference on latent variables. However, because of the intractability of the associated marginal distribution over latent variables, their training, evaluation, and sampling procedures necessitate the use of approximations like Mean Field inference and Markov Chain Monte Carlo, whose convergence time for such complex models ∗Work was done when author was at Google Brain.\nremains undetermined, often resulting in generation of highly correlated samples. Furthermore, these approximations can often hinder their performance [7].\nDirected graphical models are instead defined in terms of an ancestral sampling procedure, which is appealing both for its conceptual and computational simplicity. They lack, however, the conditional independence structure of undirected models, making exact and approximate posterior inference on latent variables cumbersome [56]. Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45]. In particular, the variational autoencoder algorithm [35, 49] simultaneously learns a generative network, that maps gaussian latent variables z to samples x, and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z, by exploiting the reparametrization trick [68]. Its success in leveraging recent advances in backpropagation [51, 39] in deep neural networks resulted in its adoption for several applications ranging from speech synthesis [12] to language modeling [8]. Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].\nSuch approximations can be avoided altogether by abstaining from using latent variables. Autoregressive models [18, 6, 37, 20] can implement this strategy while typically retaining a great deal of flexibility. This class of algorithms tractably models the joint distribution by decomposing it into a product of conditionals using the probability chain rule according to a fixed ordering over dimensions, simplifying log-likelihood evaluation and sampling. Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32]. The ordering of the dimensions, although often arbitrary, can be critical to the training of the model [66]. The sequential nature of this model limits its computational efficiency. For example, its sampling procedure is sequential and non-parallelizable, which can become cumbersome in applications like speech and music synthesis, or real-time rendering. Additionally, there is no natural latent representation associated with autoregressive models, and they have not yet been shown to be useful for semi-supervised learning.\nGenerative Adversarial Networks (GANs) [21] on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether. Instead, the generative network is associated with a discriminator network whose task is to distinguish between samples and real data. Rather than using an intractable log-likelihood, this discriminator network provides the training signal in an adversarial fashion. Successfully trained GAN models [21, 15, 47] can consistently generate sharp and realistically looking samples [38]. However, metrics that measure the diversity in the generated samples are currently intractable [62, 22, 30]. Additionally, instability in their training process [47] requires careful hyperparameter tuning to avoid diverging behavior.\nTraining such a generative network g that maps latent variable z ∼ pZ to a sample x ∼ pX does not in theory require a discriminator network as in GANs, or approximate inference as in variational autoencoders. Indeed, if g is bijective, it can be trained through maximum likelihood using the change of variable formula:\npX(x) = pZ(z) ∣∣∣∣det(∂g(z)∂zT )∣∣∣∣−1 . (1)\nThis formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3]. As the existence proof of nonlinear ICA solutions [29] suggests, auto-regressive models can be seen as tractable instance of maximum likelihood nonlinear ICA, where the residual corresponds to the independent components. However, naive application of the change of variable formula produces models which are computationally expensive and poorly conditioned, and so large scale models of this type have not entered general use."
    }, {
      "heading" : "3 Model definition",
      "text" : "In this paper, we will tackle the problem of learning highly nonlinear models in high-dimensional continuous spaces through maximum likelihood. In order to optimize the log-likelihood, we introduce a more flexible class of architectures that enables the computation of log-likelihood on continuous data using the change of variable formula. Building on our previous work in [17], we define a powerful class of bijective functions which enable exact and tractable density evaluation and exact and tractable inference. Moreover, the resulting cost function does not to rely on a fixed form reconstruction cost such as square error [38, 47], and generates sharper samples as a result. Also, this flexibility helps us leverage recent advances in batch normalization [31] and residual networks [24, 25] to define a very deep multi-scale architecture with multiple levels of abstraction."
    }, {
      "heading" : "3.1 Change of variable formula",
      "text" : "Given an observed data variable x ∈ X , a simple prior probability distribution pZ on a latent variable z ∈ Z, and a bijection f : X → Z (with g = f−1), the change of variable formula defines a model distribution on X by\npX(x) = pZ ( f(x) ) ∣∣∣∣det(∂f(x)∂xT )∣∣∣∣ (2)\nlog (pX(x)) = log ( pZ ( f(x) )) + log (∣∣∣∣det(∂f(x)∂xT )∣∣∣∣) , (3)\nwhere ∂f(x) ∂xT is the Jacobian of f at x.\nExact samples from the resulting distribution can be generated by using the inverse transform sampling rule [16]. A sample z ∼ pZ is drawn in the latent space, and its inverse image x = f−1(z) = g(z) generates a sample in the original space. Computing the density on a point x is accomplished by computing the density of its image f(x) and multiplying by the associated Jacobian determinant det ( ∂f(x) ∂xT ) . See also Figure 1. Exact and efficient inference enables the accurate and fast evaluation of the model."
    }, {
      "heading" : "3.2 Coupling layers",
      "text" : "Computing the Jacobian of functions with high-dimensional domain and codomain and computing the determinants of large matrices are in general computationally very expensive. This combined with the restriction to bijective functions makes Equation 2 appear impractical for modeling arbitrary distributions.\nAs shown however in [17], by careful design of the function f , a bijective model can be learned which is both tractable and extremely flexible. As computing the Jacobian determinant of the transformation is crucial to effectively train using this principle, this work exploits the simple observation that the determinant of a triangular matrix can be efficiently computed as the product of its diagonal terms.\nWe will build a flexible and tractable bijective function by stacking a sequence of simple bijections. In each simple bijection, part of the input vector is updated using a function which is simple to invert, but which depends on the remainder of the input vector in a complex way. We refer to each of these simple bijections as an affine coupling layer. Given a D dimensional input x and d < D, the output y of an affine coupling layer follows the equations\ny1:d = x1:d (4) yd+1:D = xd+1:D exp ( s(x1:d) ) + t(x1:d), (5)\nwhere s and t stand for scale and translation, and are functions from Rd 7→ RD−d, and is the Hadamard product or element-wise product (see Figure 2(a))."
    }, {
      "heading" : "3.3 Properties",
      "text" : "The Jacobian of this transformation is\n∂y\n∂xT =\n[ Id 0\n∂yd+1:D ∂xT1:d\ndiag ( exp [s (x1:d)] ) ] , (6) where diag ( exp [s (x1:d)] ) is the diagonal matrix whose diagonal elements correspond to the vector exp [s (x1:d)]. Given the observation that this Jacobian is triangular, we can efficiently compute its determinant as exp [∑ j s (x1:d)j ] . Since computing the Jacobian determinant of the coupling layer operation does not involve computing the Jacobian of s or t, those functions can be arbitrarily complex. We will make them deep convolutional neural networks. Note that the hidden layers of s and t can have more features than their input and output layers.\nAnother interesting property of these coupling layers in the context of defining probabilistic models is their invertibility. Indeed, computing the inverse is no more complex than the forward propagation\n(see Figure 2(b)), { y1:d = x1:d yd+1:D = xd+1:D exp ( s(x1:d) ) + t(x1:d)\n(7)\n⇔ { x1:d = y1:d xd+1:D = ( yd+1:D − t(y1:d) ) exp ( − s(y1:d) ) ,\n(8)\nmeaning that sampling is as efficient as inference for this model. Note again that computing the inverse of the coupling layer does not require computing the inverse of s or t, so these functions can be arbitrarily complex and difficult to invert."
    }, {
      "heading" : "3.4 Masked convolution",
      "text" : "Partitioning can be implemented using a binary mask b, and using the functional form for y, y = b x+ (1− b) ( x exp ( s(b x) ) + t(b x) ) . (9)\nWe use two partitionings that exploit the local correlation structure of images: spatial checkerboard patterns, and channel-wise masking (see Figure 3). The spatial checkerboard pattern mask has value 1 where the sum of spatial coordinates is odd, and 0 otherwise. The channel-wise mask b is 1 for the first half of the channel dimensions and 0 for the second half. For the models presented here, both s(·) and t(·) are rectified convolutional networks."
    }, {
      "heading" : "3.5 Combining coupling layers",
      "text" : "Although coupling layers can be powerful, their forward transformation leaves some components unchanged. This difficulty can be overcome by composing coupling layers in an alternating pattern, such that the components that are left unchanged in one coupling layer are updated in the next (see Figure 4(a)).\nThe Jacobian determinant of the resulting function remains tractable, relying on the fact that\n∂(fb ◦ fa) ∂xTa (xa) = ∂fa ∂xTa (xa) · ∂fb ∂xTb\n( xb = fa(xa) ) (10)\ndet(A ·B) = det(A) det(B). (11)\nSimilarly, its inverse can be computed easily as\n(fb ◦ fa)−1 = f−1a ◦ f−1b . (12)"
    }, {
      "heading" : "3.6 Multi-scale architecture",
      "text" : "We implement a multi-scale architecture using a squeezing operation: for each channel, it divides the image into subsquares of shape 2× 2× c, then reshapes them into subsquares of shape 1× 1× 4c. The squeezing operation transforms an s × s × c tensor into an s2 × s 2 × 4c tensor (see Figure 3), effectively trading spatial size for number of channels.\nAt each scale, we combine several operations into a sequence: we first apply three coupling layers with alternating checkerboard masks, then perform a squeezing operation, and finally apply three more coupling layers with alternating channel-wise masking. The channel-wise masking is chosen so that the resulting partitioning is not redundant with the previous checkerboard masking (see Figure 3). For the final scale, we only apply four coupling layers with alternating checkerboard masks.\nPropagating a D dimensional vector through all the coupling layers would be cumbersome, in terms of computational and memory cost, and in terms of the number of parameters that would need to be trained. For this reason we follow the design choice of [57] and factor out half of the dimensions at regular intervals (see Equation 14). We can define this operation recursively (see Figure 4(b)),\nh(0) = x (13)\n(z(i+1), h(i+1)) = f (i+1)(h(i)) (14)\nz(L) = f (L)(h(L−1)) (15)\nz = (z(1), . . . , z(L)). (16)\nIn our experiments, we use this operation for i < L. The sequence of coupling-squeezing-coupling operations described above is performed per layer when computing f (i) (Equation 14). At each layer, as the spatial resolution is reduced, the number of hidden layer features in s and t is doubled. All variables which have been factored out at different scales are concatenated to obtain the final transformed output (Equation 16).\nAs a consequence, the model must Gaussianize units which are factored out at a finer scale (in an earlier layer) before those which are factored out at a coarser scale (in a later layer). This results in the definition of intermediary levels of representation [53, 49] corresponding to more local, fine-grained features as shown in Appendix D.\nMoreover, Gaussianizing and factoring out units in earlier layers has the practical benefit of distributing the loss function throughout the network, following the philosophy similar to guiding intermediate layers using intermediate classifiers [40]. It also reduces significantly the amount of computation and memory used by the model, allowing us to train larger models."
    }, {
      "heading" : "3.7 Batch normalization",
      "text" : "To further improve the propagation of training signal, we use deep residual networks [24, 25] with batch normalization [31] and weight normalization [2, 54] in s and t. As described in Appendix E we introduce and use a novel variant of batch normalization which is based on a running average over recent minibatches, and is thus more robust when training with very small minibatches.\nWe also apply batch normalization to the whole coupling layer output. The effects of batch normalization are easily included in the Jacobian computation, since it acts as a linear rescaling on each dimension. That is, given the estimated batch statistics µ̃ and σ̃2, the rescaling function\nx 7→ x− µ̃√ σ̃2 +\n(17)\nhas a Jacobian determinant (∏ i (σ̃2i + ) )− 12 . (18)\nThis form of batch normalization can be seen as similar to reward normalization in deep reinforcement learning [44, 65].\nWe found that the use of this technique not only allowed training with a deeper stack of coupling layers, but also alleviated the instability problem that practitioners often encounter when training conditional distributions with a scale parameter through a gradient-based approach."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Procedure",
      "text" : "The algorithm described in Equation 2 shows how to learn distributions on unbounded space. In general, the data of interest have bounded magnitude. For examples, the pixel values of an image typically lie in [0, 256]D after application of the recommended jittering procedure [64, 62]. In order to reduce the impact of boundary effects, we instead model the density of logit(α+(1−α) x256 ), where α is picked here as .05. We take into account this transformation when computing log-likelihood and bits per dimension. We also augment the CIFAR-10, CelebA and LSUN datasets during training to also include horizontal flips of the training examples.\nWe train our model on four natural image datasets: CIFAR-10 [36], Imagenet [52], Large-scale Scene Understanding (LSUN) [70], CelebFaces Attributes (CelebA) [41]. More specifically, we train on the downsampled to 32× 32 and 64× 64 versions of Imagenet [46]. For the LSUN dataset, we train on the bedroom, tower and church outdoor categories. The procedure for LSUN is the same as in [47]: we downsample the image so that the smallest side is 96 pixels and take random crops of 64×64. For CelebA, we use the same procedure as in [38]: we take an approximately central crop of 148× 148 then resize it to 64× 64. We use the multi-scale architecture described in Section 3.6 and use deep convolutional residual networks in the coupling layers with rectifier nonlinearity and skip-connections as suggested by [46]. To compute the scaling functions s, we use a hyperbolic tangent function multiplied by a learned scale, whereas the translation function t has an affine output. Our multi-scale architecture is repeated recursively until the input of the last recursion is a 4× 4× c tensor. For datasets of images of size 32 × 32, we use 4 residual blocks with 32 hidden feature maps for the first coupling layers with checkerboard masking. Only 2 residual blocks are used for images of size 64× 64. We use a batch size of 64. For CIFAR-10, we use 8 residual blocks, 64 feature maps, and downscale only once. We optimize with ADAM [33] with default hyperparameters and use an L2 regularization on the weight scale parameters with coefficient 5 · 10−5. We set the prior pZ to be an isotropic unit norm Gaussian. However, any distribution could be used for pZ , including distributions that are also learned during training, such as from an auto-regressive model, or (with slight modifications to the training objective) a variational autoencoder."
    }, {
      "heading" : "4.2 Results",
      "text" : "We show in Table 1 that the number of bits per dimension, while not improving over the Pixel RNN [46] baseline, is competitive with other generative methods. As we notice that our performance increases with the number of parameters, larger models are likely to further improve performance. For CelebA and LSUN, the bits per dimension for the validation set was decreasing throughout training, so little overfitting is expected.\nWe show in Figure 5 samples generated from the model with training examples from the dataset for comparison. As mentioned in [62, 22], maximum likelihood is a principle that values diversity\nover sample quality in a limited capacity setting. As a result, our model outputs sometimes highly improbable samples as we can notice especially on CelebA. As opposed to variational autoencoders, the samples generated from our model look not only globally coherent but also sharp. Our hypothesis is that as opposed to these models, real NVP does not rely on fixed form reconstruction cost like an L2 norm which tends to reward capturing low frequency components more heavily than high frequency components. Unlike autoregressive models, sampling from our model is done very efficiently as it is parallelized over input dimensions. On Imagenet and LSUN, our model seems to have captured well the notion of background/foreground and lighting interactions such as luminosity and consistent light source direction for reflectance and shadows.\nWe also illustrate the smooth semantically consistent meaning of our latent variables. In the latent space, we define a manifold based on four validation examples z(1), z(2), z(3), z(4), and parametrized by two parameters φ and φ′ by,\nz = cos(φ) ( cos(φ′)z(1) + sin(φ ′)z(2) ) + sin(φ) ( cos(φ′)z(3) + sin(φ ′)z(4) ) . (19)\nWe project the resulting manifold back into the data space by computing g(z). Results are shown Figure 6. We observe that the model seems to have organized the latent space with a notion of meaning that goes well beyond pixel space interpolation. More manifold visualization are shown in the Appendix. To further test whether the latent space has a consistent semantic interpretation, we trained a class-conditional model on CelebA, and found that the learned representation had a consistent semantic meaning across class labels (see Appendix F)."
    }, {
      "heading" : "5 Discussion and conclusion",
      "text" : "In this paper, we have defined a class of invertible functions with tractable Jacobian determinant, enabling exact and tractable log-likelihood evaluation, inference, and sampling. We have shown that this class of generative model achieves competitive performances, both in terms of sample quality and log-likelihood. Many avenues exist to further improve the functional form of the transformations, for instance by exploiting the latest advances in dilated convolutions [69] and residual networks architectures [60].\nThis paper presented a technique bridging the gap between auto-regressive models, variational autoencoders, and generative adversarial networks. Like auto-regressive models, it allows tractable and exact log-likelihood evaluation for training. It allows however a much more flexible functional form, similar to that in the generative model of variational autoencoders. This allows for fast and exact sampling from the model distribution. Like GANs, and unlike variational autoencoders, our technique does not require the use of a fixed form reconstruction cost, and instead defines a cost in terms of higher level features, generating sharper images. Finally, unlike both variational\nautoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space. This may make the algorithm particularly well suited to semi-supervised learning tasks, as we hope to explore in future work.\nReal NVP generative models can additionally be conditioned on additional variables (for instance class labels) to create a structured output algorithm. More so, as the resulting class of invertible transformations can be treated as a probability distribution in a modular way, it can also be used to improve upon other probabilistic models like auto-regressive models and variational autoencoders. For variational autoencoders, these transformations could be used both to enable a more flexible reconstruction cost [38] and a more flexible stochastic inference distribution [48]. Probabilistic models in general can also benefit from batch normalization techniques as applied in this paper.\nThe definition of powerful and trainable invertible functions can also benefit domains other than generative unsupervised learning. For example, in reinforcement learning, these invertible functions can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning [23] or find representation where local linear Gaussian approximations are more appropriate [67]."
    }, {
      "heading" : "6 Acknowledgments",
      "text" : "The authors thank the developers of Tensorflow [1]. We thank Sherry Moore, David Andersen and Jon Shlens for their help in implementing the model. We thank Aäron van den Oord, Yann Dauphin, Kyle Kastner, Chelsea Finn, Maithra Raghu, David Warde-Farley, Daniel Jiwoong Im and Oriol Vinyals for fruitful discussions. Finally, we thank Ben Poole, Rafal Jozefowicz and George Dahl for their input on a draft of the paper."
    }, {
      "heading" : "A Samples",
      "text" : ""
    }, {
      "heading" : "B Manifold",
      "text" : ""
    }, {
      "heading" : "C Extrapolation",
      "text" : "Inspired by the texture generation work by [19, 61] and extrapolation test with DCGAN [47], we also evaluate the statistics captured by our model by generating images twice or ten times as large as present in the dataset. As we can observe in the following figures, our model seems to successfully create a “texture” representation of the dataset while maintaining a spatial smoothness through the image. Our convolutional architecture is only aware of the position of considered pixel through edge effects in convolutions, therefore our model is similar to a stationary process. This also explains why these samples are more consistent in LSUN, where the training data was obtained using random crops."
    }, {
      "heading" : "D Latent variables semantic",
      "text" : "As in [22], we further try to grasp the semantic of our learned layers latent variables by doing ablation tests. We infer the latent variables and resample the lowest levels of latent variables from a standard gaussian, increasing the highest level affected by this resampling. As we can see in the following figures, the semantic of our latent space seems to be more on a graphic level rather than higher level concept. Although the heavy use of convolution improves learning by exploiting image prior knowledge, it is also likely to be responsible for this limitation."
    }, {
      "heading" : "E Batch normalization",
      "text" : "We further experimented with batch normalization by using a weighted average of a moving average of the layer statistics µ̃t, σ̃2t and the current batch batch statistics µ̂t, σ̂2t ,\nµ̃t+1 = ρµ̃t + (1− ρ)µ̂t (20) σ̃2t+1 = ρσ̃ 2 t + (1− ρ)σ̂2t , (21)\nwhere ρ is the momentum. When using µ̃t+1, σ̃2t+1, we only propagate gradient through the current batch statistics µ̂t, σ̂2t . We observe that using this lag helps the model train with very small minibatches.\nWe used batch normalization with a moving average for our results on CIFAR-10."
    }, {
      "heading" : "F Attribute change",
      "text" : "Additionally, we exploit the attribute information y in CelebA to build a conditional model, i.e. the invertible function f from image to latent variable uses the labels in y to define its parameters. In order to observe the information stored in the latent variables, we choose to encode a batch of images x with their original attribute y and decode them using a new set of attributes y′, build by shuffling the original attributes inside the batch. We obtain the new images x′ = g ( f(x; y); y′ ) .\nWe observe that, although the faces are changed as to respect the new attributes, several properties remain unchanged like position and background."
    } ],
    "references" : [ {
      "title" : "Jozefowicz and George Dahl for their input on a draft of the paper",
      "author" : [ "Sherry Moore", "David Andersen", "Jon Shlens" ],
      "venue" : "arXiv preprint arXiv:1603.04467,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Understanding symmetries in deep networks",
      "author" : [ "Vijay Badrinarayanan", "Bamdev Mishra", "Roberto Cipolla" ],
      "venue" : "arXiv preprint arXiv:1511.01029,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Density modeling of images using a generalized normalization transformation",
      "author" : [ "Johannes Ballé", "Valero Laparra", "Eero P Simoncelli" ],
      "venue" : "arXiv preprint arXiv:1511.06281,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "An information-maximization approach to blind separation and blind deconvolution",
      "author" : [ "Anthony J Bell", "Terrence J Sejnowski" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Artificial neural networks and their application to sequence recognition",
      "author" : [ "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1991
    }, {
      "title" : "Modeling high-dimensional discrete data with multi-layer neural networks",
      "author" : [ "Yoshua Bengio", "Samy Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1999
    }, {
      "title" : "Stochastic gradient estimate variance in contrastive divergence and persistent contrastive divergence",
      "author" : [ "Mathias Berglund", "Tapani Raiko" ],
      "venue" : "arXiv preprint arXiv:1312.6002,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06349,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Super-resolution with deep convolutional sufficient statistics",
      "author" : [ "Joan Bruna", "Pablo Sprechmann", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1511.05666,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Importance weighted autoencoders",
      "author" : [ "Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1509.00519,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "A Gopinath. Gaussianization",
      "author" : [ "Scott Shaobing Chen", "Ramesh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "A recurrent latent variable model for sequential data",
      "author" : [ "Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "The helmholtz machine",
      "author" : [ "Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1995
    }, {
      "title" : "Higher order statistical decorrelation without information loss",
      "author" : [ "Gustavo Deco", "Wilfried Brauer" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1995
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems 28: 10 Published as a conference paper at ICLR",
      "author" : [ "Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus" ],
      "venue" : "Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "Sample-based non-uniform random variate generation",
      "author" : [ "Luc Devroye" ],
      "venue" : "In Proceedings of the 18th conference on Winter simulation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "Nice: non-linear independent components estimation",
      "author" : [ "Laurent Dinh", "David Krueger", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1410.8516,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Graphical models for machine learning and digital communication",
      "author" : [ "Brendan J Frey" ],
      "venue" : "MIT press,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems",
      "author" : [ "Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge" ],
      "venue" : "Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "MADE: masked autoencoder for distribution estimation",
      "author" : [ "Mathieu Germain", "Karol Gregor", "Iain Murray", "Hugo Larochelle" ],
      "venue" : "CoRR, abs/1502.03509,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Towards conceptual compression",
      "author" : [ "Karol Gregor", "Frederic Besse", "Danilo Jimenez Rezende", "Ivo Danihelka", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1604.08772,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Continuous deep q-learning with model-based acceleration",
      "author" : [ "Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine" ],
      "venue" : "arXiv preprint arXiv:1603.00748,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1512.03385,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1997
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Independent component analysis, volume 46",
      "author" : [ "Aapo Hyvärinen", "Juha Karhunen", "Erkki Oja" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2004
    }, {
      "title" : "Nonlinear independent component analysis: Existence and uniqueness results",
      "author" : [ "Aapo Hyvärinen", "Petteri Pajunen" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1999
    }, {
      "title" : "Generating images with recurrent adversarial networks",
      "author" : [ "Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic" ],
      "venue" : "arXiv preprint arXiv:1602.05110,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Józefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Diederik P Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1606.04934,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "Learning multiple layers of features from tiny",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2009
    }, {
      "title" : "The neural autoregressive distribution estimator",
      "author" : [ "Hugo Larochelle", "Iain Murray" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2011
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity",
      "author" : [ "Anders Boesen Lindbo Larsen", "Søren Kaae Sønderby", "Ole Winther" ],
      "venue" : "metric. CoRR,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2015
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Yann A LeCun", "Léon Bottou", "Genevieve B Orr", "Klaus-Robert Müller" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2012
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In Proceedings of International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2015
    }, {
      "title" : "Auxiliary deep generative models",
      "author" : [ "Lars Maaløe", "Casper Kaae Sønderby", "Søren Kaae Sønderby", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1602.05473,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2016
    }, {
      "title" : "Neural variational inference and learning in belief networks",
      "author" : [ "Andriy Mnih", "Karol Gregor" ],
      "venue" : "arXiv preprint arXiv:1402.0030,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "A view of the em algorithm that justifies incremental, sparse, and other variants. In Learning in graphical models, pages 355–368",
      "author" : [ "Radford M Neal", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1998
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1601.06759,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "CoRR, abs/1511.06434,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2015
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed" ],
      "venue" : "arXiv preprint arXiv:1505.05770,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1401.4082,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2014
    }, {
      "title" : "High-dimensional probability estimation with deep density models",
      "author" : [ "Oren Rippel", "Ryan Prescott Adams" ],
      "venue" : "arXiv preprint arXiv:1302.5125,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2013
    }, {
      "title" : "Learning representations by backpropagating errors",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Cognitive modeling,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1988
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2015
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey E Hinton" ],
      "venue" : "In International conference on artificial intelligence and statistics,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2009
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "author" : [ "Tim Salimans", "Diederik P Kingma" ],
      "venue" : "arXiv preprint arXiv:1602.07868,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2016
    }, {
      "title" : "Markov chain monte carlo and variational inference: Bridging the gap",
      "author" : [ "Tim Salimans", "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1410.6460,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2014
    }, {
      "title" : "Mean field theory for sigmoid belief networks",
      "author" : [ "Lawrence K Saul", "Tommi Jaakkola", "Michael I Jordan" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1996
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2014
    }, {
      "title" : "Information processing in dynamical systems: Foundations of harmony theory",
      "author" : [ "Paul Smolensky" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 1986
    }, {
      "title" : "Deep unsupervised learning using nonequilibrium thermodynamics",
      "author" : [ "Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2015
    }, {
      "title" : "Resnet in resnet",
      "author" : [ "Sasha Targ", "Diogo Almeida", "Kevin Lyman" ],
      "venue" : "Generalizing residual architectures. CoRR,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2016
    }, {
      "title" : "Generative image modeling using spatial lstms",
      "author" : [ "Lucas Theis", "Matthias Bethge" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2015
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Lucas Theis", "Aäron Van Den Oord", "Matthias Bethge" ],
      "venue" : "CoRR, abs/1511.01844,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2015
    }, {
      "title" : "Variational gaussian process",
      "author" : [ "Dustin Tran", "Rajesh Ranganath", "David M Blei" ],
      "venue" : "arXiv preprint arXiv:1511.06499,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2015
    }, {
      "title" : "Rnade: The real-valued neural autoregressive densityestimator",
      "author" : [ "Benigno Uria", "Iain Murray", "Hugo Larochelle" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2013
    }, {
      "title" : "Learning functions across many orders of magnitudes",
      "author" : [ "Hado van Hasselt", "Arthur Guez", "Matteo Hessel", "David Silver" ],
      "venue" : "arXiv preprint arXiv:1602.07714,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2016
    }, {
      "title" : "Order matters: Sequence to sequence for sets",
      "author" : [ "Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur" ],
      "venue" : "arXiv preprint arXiv:1511.06391,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2015
    }, {
      "title" : "Embed to control: A locally linear latent dynamics model for control from raw images",
      "author" : [ "Manuel Watter", "Jost Springenberg", "Joschka Boedecker", "Martin Riedmiller" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2015
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 1992
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "Fisher Yu", "Vladlen Koltun" ],
      "venue" : "arXiv preprint arXiv:1511.07122,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2015
    }, {
      "title" : "Construction of a large-scale image dataset using deep learning with humans in the loop",
      "author" : [ "Fisher Yu", "Yinda Zhang", "Shuran Song", "Ari Seff", "Jianxiong Xiao" ],
      "venue" : "arXiv preprint arXiv:1506.03365,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 59,
      "context" : "Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [61, 46, 59], denoising [3], colorization [71], and super-resolution [9].",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 44,
      "context" : "Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [61, 46, 59], denoising [3], colorization [71], and super-resolution [9].",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 57,
      "context" : "Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [61, 46, 59], denoising [3], colorization [71], and super-resolution [9].",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [61, 46, 59], denoising [3], colorization [71], and super-resolution [9].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [61, 46, 59], denoising [3], colorization [71], and super-resolution [9].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 56,
      "context" : "One class of maximum likelihood models are those described by probabilistic undirected graphs, such as Restricted Boltzmann Machines [58] and Deep Boltzmann Machines [53].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 51,
      "context" : "One class of maximum likelihood models are those described by probabilistic undirected graphs, such as Restricted Boltzmann Machines [58] and Deep Boltzmann Machines [53].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, these approximations can often hinder their performance [7].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 54,
      "context" : "They lack, however, the conditional independence structure of undirected models, making exact and approximate posterior inference on latent variables cumbersome [56].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 26,
      "context" : "Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 41,
      "context" : "Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : "Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 47,
      "context" : "Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 43,
      "context" : "Recent advances in stochastic variational inference [27] and amortized inference [13, 43, 35, 49], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [45].",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 34,
      "context" : "In particular, the variational autoencoder algorithm [35, 49] simultaneously learns a generative network, that maps gaussian latent variables z to samples x, and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z, by exploiting the reparametrization trick [68].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 47,
      "context" : "In particular, the variational autoencoder algorithm [35, 49] simultaneously learns a generative network, that maps gaussian latent variables z to samples x, and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z, by exploiting the reparametrization trick [68].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 66,
      "context" : "In particular, the variational autoencoder algorithm [35, 49] simultaneously learns a generative network, that maps gaussian latent variables z to samples x, and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z, by exploiting the reparametrization trick [68].",
      "startOffset" : 318,
      "endOffset" : 322
    }, {
      "referenceID" : 49,
      "context" : "Its success in leveraging recent advances in backpropagation [51, 39] in deep neural networks resulted in its adoption for several applications ranging from speech synthesis [12] to language modeling [8].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 38,
      "context" : "Its success in leveraging recent advances in backpropagation [51, 39] in deep neural networks resulted in its adoption for several applications ranging from speech synthesis [12] to language modeling [8].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Its success in leveraging recent advances in backpropagation [51, 39] in deep neural networks resulted in its adoption for several applications ranging from speech synthesis [12] to language modeling [8].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "Its success in leveraging recent advances in backpropagation [51, 39] in deep neural networks resulted in its adoption for several applications ranging from speech synthesis [12] to language modeling [8].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 40,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 46,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 53,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 61,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 9,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 57,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 33,
      "context" : "Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [42, 48, 55, 63, 10, 59, 34].",
      "startOffset" : 175,
      "endOffset" : 203
    }, {
      "referenceID" : 17,
      "context" : "Autoregressive models [18, 6, 37, 20] can implement this strategy while typically retaining a great deal of flexibility.",
      "startOffset" : 22,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Autoregressive models [18, 6, 37, 20] can implement this strategy while typically retaining a great deal of flexibility.",
      "startOffset" : 22,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : "Autoregressive models [18, 6, 37, 20] can implement this strategy while typically retaining a great deal of flexibility.",
      "startOffset" : 22,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "Autoregressive models [18, 6, 37, 20] can implement this strategy while typically retaining a great deal of flexibility.",
      "startOffset" : 22,
      "endOffset" : 37
    }, {
      "referenceID" : 49,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 59,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 237,
      "endOffset" : 245
    }, {
      "referenceID" : 44,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 237,
      "endOffset" : 245
    }, {
      "referenceID" : 31,
      "context" : "Recent work in this line of research has taken advantage of recent advances in recurrent networks [51], in particular long-short term memory [26], and residual networks [25, 24] in order to learn state-of-the-art generative image models [61, 46] and language models [32].",
      "startOffset" : 266,
      "endOffset" : 270
    }, {
      "referenceID" : 64,
      "context" : "The ordering of the dimensions, although often arbitrary, can be critical to the training of the model [66].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Generative Adversarial Networks (GANs) [21] on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "Successfully trained GAN models [21, 15, 47] can consistently generate sharp and realistically looking samples [38].",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "Successfully trained GAN models [21, 15, 47] can consistently generate sharp and realistically looking samples [38].",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 45,
      "context" : "Successfully trained GAN models [21, 15, 47] can consistently generate sharp and realistically looking samples [38].",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 37,
      "context" : "Successfully trained GAN models [21, 15, 47] can consistently generate sharp and realistically looking samples [38].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 60,
      "context" : "However, metrics that measure the diversity in the generated samples are currently intractable [62, 22, 30].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "However, metrics that measure the diversity in the generated samples are currently intractable [62, 22, 30].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "However, metrics that measure the diversity in the generated samples are currently intractable [62, 22, 30].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 45,
      "context" : "Additionally, instability in their training process [47] requires careful hyperparameter tuning to avoid diverging behavior.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 27,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 194,
      "endOffset" : 208
    }, {
      "referenceID" : 48,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 194,
      "endOffset" : 208
    }, {
      "referenceID" : 16,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 194,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 28], gaussianization [14, 11] and deep density models [5, 50, 17, 3].",
      "startOffset" : 194,
      "endOffset" : 208
    }, {
      "referenceID" : 28,
      "context" : "As the existence proof of nonlinear ICA solutions [29] suggests, auto-regressive models can be seen as tractable instance of maximum likelihood nonlinear ICA, where the residual corresponds to the independent components.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "Building on our previous work in [17], we define a powerful class of bijective functions which enable exact and tractable density evaluation and exact and tractable inference.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 37,
      "context" : "Moreover, the resulting cost function does not to rely on a fixed form reconstruction cost such as square error [38, 47], and generates sharper samples as a result.",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 45,
      "context" : "Moreover, the resulting cost function does not to rely on a fixed form reconstruction cost such as square error [38, 47], and generates sharper samples as a result.",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 30,
      "context" : "Also, this flexibility helps us leverage recent advances in batch normalization [31] and residual networks [24, 25] to define a very deep multi-scale architecture with multiple levels of abstraction.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "Also, this flexibility helps us leverage recent advances in batch normalization [31] and residual networks [24, 25] to define a very deep multi-scale architecture with multiple levels of abstraction.",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "Also, this flexibility helps us leverage recent advances in batch normalization [31] and residual networks [24, 25] to define a very deep multi-scale architecture with multiple levels of abstraction.",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "Exact samples from the resulting distribution can be generated by using the inverse transform sampling rule [16].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "As shown however in [17], by careful design of the function f , a bijective model can be learned which is both tractable and extremely flexible.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 55,
      "context" : "For this reason we follow the design choice of [57] and factor out half of the dimensions at regular intervals (see Equation 14).",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 51,
      "context" : "This results in the definition of intermediary levels of representation [53, 49] corresponding to more local, fine-grained features as shown in Appendix D.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 47,
      "context" : "This results in the definition of intermediary levels of representation [53, 49] corresponding to more local, fine-grained features as shown in Appendix D.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "To further improve the propagation of training signal, we use deep residual networks [24, 25] with batch normalization [31] and weight normalization [2, 54] in s and t.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "To further improve the propagation of training signal, we use deep residual networks [24, 25] with batch normalization [31] and weight normalization [2, 54] in s and t.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "To further improve the propagation of training signal, we use deep residual networks [24, 25] with batch normalization [31] and weight normalization [2, 54] in s and t.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "To further improve the propagation of training signal, we use deep residual networks [24, 25] with batch normalization [31] and weight normalization [2, 54] in s and t.",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 52,
      "context" : "To further improve the propagation of training signal, we use deep residual networks [24, 25] with batch normalization [31] and weight normalization [2, 54] in s and t.",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 42,
      "context" : "This form of batch normalization can be seen as similar to reward normalization in deep reinforcement learning [44, 65].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 63,
      "context" : "This form of batch normalization can be seen as similar to reward normalization in deep reinforcement learning [44, 65].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 62,
      "context" : "For examples, the pixel values of an image typically lie in [0, 256] after application of the recommended jittering procedure [64, 62].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 60,
      "context" : "For examples, the pixel values of an image typically lie in [0, 256] after application of the recommended jittering procedure [64, 62].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 35,
      "context" : "We train our model on four natural image datasets: CIFAR-10 [36], Imagenet [52], Large-scale Scene Understanding (LSUN) [70], CelebFaces Attributes (CelebA) [41].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 50,
      "context" : "We train our model on four natural image datasets: CIFAR-10 [36], Imagenet [52], Large-scale Scene Understanding (LSUN) [70], CelebFaces Attributes (CelebA) [41].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 68,
      "context" : "We train our model on four natural image datasets: CIFAR-10 [36], Imagenet [52], Large-scale Scene Understanding (LSUN) [70], CelebFaces Attributes (CelebA) [41].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 39,
      "context" : "We train our model on four natural image datasets: CIFAR-10 [36], Imagenet [52], Large-scale Scene Understanding (LSUN) [70], CelebFaces Attributes (CelebA) [41].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 44,
      "context" : "More specifically, we train on the downsampled to 32× 32 and 64× 64 versions of Imagenet [46].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 45,
      "context" : "The procedure for LSUN is the same as in [47]: we downsample the image so that the smallest side is 96 pixels and take random crops of 64×64.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 37,
      "context" : "For CelebA, we use the same procedure as in [38]: we take an approximately central crop of 148× 148 then resize it to 64× 64.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 44,
      "context" : "6 and use deep convolutional residual networks in the coupling layers with rectifier nonlinearity and skip-connections as suggested by [46].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "We optimize with ADAM [33] with default hyperparameters and use an L2 regularization on the weight scale parameters with coefficient 5 · 10−5.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 44,
      "context" : "Dataset PixelRNN [46] Real NVP Conv DRAW [22] IAF-VAE [34] CIFAR-10 3.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "Dataset PixelRNN [46] Real NVP Conv DRAW [22] IAF-VAE [34] CIFAR-10 3.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : "Dataset PixelRNN [46] Real NVP Conv DRAW [22] IAF-VAE [34] CIFAR-10 3.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 44,
      "context" : "We show in Table 1 that the number of bits per dimension, while not improving over the Pixel RNN [46] baseline, is competitive with other generative methods.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 60,
      "context" : "As mentioned in [62, 22], maximum likelihood is a principle that values diversity",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "As mentioned in [62, 22], maximum likelihood is a principle that values diversity",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 67,
      "context" : "Many avenues exist to further improve the functional form of the transformations, for instance by exploiting the latest advances in dilated convolutions [69] and residual networks architectures [60].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 58,
      "context" : "Many avenues exist to further improve the functional form of the transformations, for instance by exploiting the latest advances in dilated convolutions [69] and residual networks architectures [60].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 37,
      "context" : "For variational autoencoders, these transformations could be used both to enable a more flexible reconstruction cost [38] and a more flexible stochastic inference distribution [48].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 46,
      "context" : "For variational autoencoders, these transformations could be used both to enable a more flexible reconstruction cost [38] and a more flexible stochastic inference distribution [48].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 22,
      "context" : "For example, in reinforcement learning, these invertible functions can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning [23] or find representation where local linear Gaussian approximations are more appropriate [67].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 65,
      "context" : "For example, in reinforcement learning, these invertible functions can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning [23] or find representation where local linear Gaussian approximations are more appropriate [67].",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 0,
      "context" : "The authors thank the developers of Tensorflow [1].",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2017,
    "abstractText" : "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.",
    "creator" : "LaTeX with hyperref package"
  }
}