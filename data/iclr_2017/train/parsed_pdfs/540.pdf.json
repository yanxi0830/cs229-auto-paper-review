{
  "name" : "540.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "MODULARIZED MORPHING", "Tao Wei", "Changhu Wang", "Chang Wen Chen" ],
    "emails" : [ "taowei@buffalo.edu", "chw@microsoft.com", "chencw@buffalo.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep convolutional neural networks have continuously demonstrated their excellent performances on diverse computer vision problems. In image classification, the milestones of such networks can be roughly represented by LeNet (LeCun et al., 1989), AlexNet (Krizhevsky et al., 2012), VGG net (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2014), and ResNet (He et al., 2015), with networks becoming deeper and deeper. However, the architectures of these network are significantly altered and hence are not backward-compatible. Considering a life-long learning system, it is highly desired that the system is able to update itself from the original version established initially, and then evolve into a more powerful one, rather than re-learning a brand new one from scratch.\nNetwork morphism (Wei et al., 2016) is an effective way towards such an ambitious goal. It can morph a well-trained network to a new one with the knowledge entirely inherited, and hence is able to update the original system to a compatible and more powerful one based on further training. Network morphism is also a performance booster and architecture explorer for convolutional neural networks, allowing us to quickly investigate new models with significantly less computational and human resources. However, the network morphism operations proposed in (Wei et al., 2016), including depth, width, and kernel size changes, are quite primitive and have been limited to the level of layer in a network. For practical applications where neural networks usually consist of dozens or even hundreds of layers, the morphing space would be too large for researchers to practically design the architectures of target morphed networks, when based on these primitive morphing operations only.\nDifferent from previous work, we investigate in this research the network morphism from a higher level of viewpoint, and systematically study the central problem of network morphism on the module level, i.e., whether and how a convolutional layer can be morphed into an arbitrary module1, where a module refers to a single-source, single-sink acyclic subnet of a neural network. With this\n†Tao Wei performed this work while being an intern at Microsoft Research Asia. 1Although network morphism generally does not impose constraints on the architecture of the child network,\nin this work we limit the investigation to the expanding mode.\nmodularized network morphing, instead of morphing in the layer level where numerous variations exist in a deep neural network, we focus on the changes of basic modules of networks, and explore the morphing space in a more efficient way. The necessities for this study are two folds. First, we wish to explore the capability of the network morphism operations and obtain a theoretical upper bound for what we are able to do with this learning scheme. Second, modern state-of-the-art convolutional neural networks have been developed with modularized architectures (Szegedy et al., 2014; He et al., 2015), which stack the construction units following the same module design. It is highly desired that the morphing operations could be directly applied to these networks.\nTo study the morphing capability of network morphism and figure out the morphing process, we introduce a simplified graph-based representation for a module. Thus, the network morphing process can be formulated as a graph transformation process. In this representation, the module of a neural network is abstracted as a directed acyclic graph (DAG), with data blobs in the network represented as vertices and convolutional layers as edges. Furthermore, a vertex with more than one outdegree (or indegree) implicitly includes a split of multiple copies of blobs (or a joint of addition). Indeed, the proposed graph abstraction suffers from the problem of dimension compatibility of blobs, for different kernel filters may result in totally different blob dimensions. We solve this problem by extending the blob and filter dimensions from finite to infinite, and the convergence properties will also be carefully investigated.\nTwo atomic morphing operations are adopted as the basis for the proposed graph transformation, based on which a large family of modules can be transformed from a convolutional layer. This family of modules are called simple morphable modules in this work. A novel algorithm is proposed to identify the morphing steps by reducing the module into a single convolutional layer. For any module outside the simple morphable family, i.e., complex module, we first apply the same reduction process and reduce it to an irreducible module. A practical algorithm is then proposed to solve for the network morphism equation of the irreducible module. Therefore, we not only verify the morphability to an arbitrary module, but also provide a unified morphing solution. This demonstrates the generalization ability and thus practicality of this learning scheme.\nExtensive experiments have been conducted based on ResNet (He et al., 2015) to show the effectiveness of the proposed morphing solution. With only 1.2x or less computation, the morphed network can achieve up to 25% relative performance improvement over the original ResNet. Such an improvement is significant in the sense that the morphed 20-layered network is able to achieve an error rate of 6.60% which is even better than a 110-layered ResNet (6.61%) on the CIFAR10 dataset, with only around 1/5 of the computational cost. It is also exciting that the morphed 56-layered network is able to achieve 5.37% error rate, which is even lower than those of ResNet-110 (6.61%) and ResNet-164 (5.46%). The effectiveness of the proposed learning scheme has also been verified on the CIFAR100 and ImageNet datasets."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Knowledge Transfer. Network morphism originated from knowledge transferring for convolutional neural networks. Early attempts were only able to transfer partial knowledge of a well-trained network. For example, a series of model compression techniques (Bucilu et al., 2006; Ba & Caruana, 2014; Hinton et al., 2015; Romero et al., 2014) were proposed to fit a lighter network to predict the output of a heavier network. Pre-training (Simonyan & Zisserman, 2014) was adopted to preinitialize certain layers of a deeper network with weights learned from a shallower network. However, network morphism requires the knowledge being fully transferred, and existing work includes Net2Net (Chen et al., 2015) and NetMorph (Wei et al., 2016). Net2Net achieved this goal by padding identity mapping layers into the neural network, while NetMorph decomposed a convolutional layer into two layers by deconvolution. Note that the network morphism operations in (Chen et al., 2015; Wei et al., 2016) are quite primitive and at a micro-scale layer level. In this research, we study the network morphism at a meso-scale module level, and in particular, we investigate its morphing capability.\nModularized Network Architecture. The evolution of convolutional neural networks has been from sequential to modularized. For example, LeNet (LeCun et al., 1998), AlexNet (Krizhevsky et al., 2012), and VGG net (Simonyan & Zisserman, 2014) are sequential networks, and their difference is primarily on the number of layers, which is 5, 8, and up to 19 respectively. However, recently\nproposed networks, such as GoogLeNet (Szegedy et al., 2014; 2015) and ResNet (He et al., 2015), follow a modularized architecture design, and have achieved the state-of-the-art performance. This is why we wish to study network morphism at the module level, so that its operations are able to directly apply to these modularized network architectures."
    }, {
      "heading" : "3 NETWORK MORPHISM VIA GRAPH ABSTRACTION",
      "text" : "In this section, we present a systematic study on the capability of network morphism learning scheme. We shall verify that a convolutional layer is able to be morphed into any single-source, single-sink DAG subnet, named as a module here. We shall also present the corresponding morphing algorithms.\nFor simplicity, we first consider convolutional neural networks with only convolutional layers. All other layers, including the non-linearity and batch normalization layers, will be discussed later in this paper."
    }, {
      "heading" : "3.1 BACKGROUND AND BASIC NOTATIONS",
      "text" : "For a 2D deep convolutional neural network (DCNN), as shown in Fig. 1a, the convolution is defined by: Bj(cj) = ∑ ci Bi(ci) ∗Gl(cj , ci), (1)\nwhere the blob B∗ is a 3D tensor of shape (C∗, H∗,W∗) and the convolutional filter Gl is a 4D tensor of shape (Cj , Ci,Kl,Kl). In addition, C∗, H∗, and W∗ represent the number of channels, height and width of B∗. Kl is the convolutional kernel size2.\nIn a network morphism process, the convolutional layer Gl in the parent network is morphed into two convolutional layers Fl and Fl+1 (Fig. 1a), where the filters Fl and Fl+1 are 4D tensors of shapes (Cl, Ci,K1,K1) and (Cj , Cl,K2,K2). This process should follow the morphism equation:\nG̃l(cj , ci) = ∑ cl Fl(cl, ci) ∗ Fl+1(cj , cl), (2)\nwhere G̃l is a zero-padded version of Gl whose effective kernel size is K̃l = K1 +K2 − 1 ≥ Kl. (Wei et al., 2016) showed a sufficient condition for exact network morphism:\nmax(ClCiK 2 1 , CjClK 2 2 ) ≥ CjCi(K1 +K2 − 1)2. (3)\nFor simplicity, we shall denote equations (1) and (2) as Bj = Gl ~Bi and G̃l = Fl+1 ~ Fl, where ~ is a non-communicative multi-channel convolution operator. We can also rewrite equation (3) as max(|Fl|, |Fl+1|) ≥ |G̃l|, where | ∗ | measures the size of the convolutional filter.\n2Generally speaking, Gl is a 4D tensor of shape (Cj , Ci,KHl ,K W l ), where convolutional kernel sizes for blob height and width are not necessary to be the same. However, in order to simplify the notations, we assume that KHl = K W l , but the claims and theorems in this paper apply equally well when they are different."
    }, {
      "heading" : "3.2 ATOMIC NETWORK MORPHISM",
      "text" : "We start with the simplest cases. Two atomic morphing types are considered, as shown in Fig. 1b: 1) a convolutional layer is morphed into two convolutional layers (TYPE-I); 2) a convolutional layer is morphed into two-way convolutional layers (TYPE-II). For the TYPE-I atomic morphing operation, equation (2) is satisfied, while For TYPE-II, the filter split is set to satisfy\nGl = F 1 l + F 2 l . (4)\nIn addition, for TYPE-II, at the source end, the blob is split with multiple copies; while at the sink end, the blobs are joined by addition."
    }, {
      "heading" : "3.3 GRAPH ABSTRACTION",
      "text" : "To simplify the representation, we introduce the following graph abstraction for network morphism. For a convolutional neural network, we are able to abstract it as a graph, with the blobs represented by vertices, and convolutional layers by edges. Formally, a DCNN is represented as a DAG M = (V,E), where V = {Bi}Ni=1 are blobs and E = {el = (Bi, Bj)}Ll=1are convolutional layers. Each convolutional layer el connects two blobs Bi and Bj , and is associated with a convolutional filter Fl. Furthermore, in this graph, if outdegree(Bi) > 1, it implicitly means a split of multiple copies; and if indegree(Bi) > 1, it is a joint of addition.\nBased on this abstraction, we formally introduce the following definition for modular network morphism: Definition 1. Let M0 = ({s, t}, e0) represent the graph with only a single edge e0 that connects the source vertex s and sink vertex t. M = (V,E) represents any single-source, single-sink DAG with the same source vertex s and the same sink vertex t. We call such an M as a module. If there exists a process that we are able to morph M0 to M , then we say that module M is morphable, and the morphing process is called modular network morphism.\nHence, based on this abstraction, modular network morphism can be represented as a graph transformation problem. As shown in Fig. 2b, module (C) in Fig. 2a can be transformed from module M0 by applying the illustrated network morphism operations.\nFor each modular network morphing, a modular network morphism equation is associated: Definition 2. Each module essentially corresponds to a function from s to t, which is called a module function. For a modular network morphism process from M0 to M , the equation that guarantees the module function unchanged is called modular network morphism equation.\nIt is obvious that equations (2) and (4) are the modular network morphism equations for TYPE-I and TYPE-II atomic morphing types. In general, the modular network morphism equation for a module M is able to be written as the sum of all convolutional filter compositions, in which each composition is actually a path from s to t in the module M . Let {(Fp,1, Fp,2, · · · , Fp,ip) : p = 1, · · · , P, and ip is the length of path p} be the set of all such paths represented by the convolutional filters. Then the modular network morphism equation for module M can be written as\nGl = ∑ p Fp,ip ~ Fp,ip−1 ~ · · ·~ Fp,1. (5)\nAs an example illustrated in Fig. 2a, there are four paths in module (D), and its modular network morphism equation can be written as\nGl = F5 ~ F1 + F6 ~ (F3 ~ F1 + F4 ~ F2) + F7 ~ F2, (6)\nwhere Gl is the convolutional filter associated with e0 in module M0."
    }, {
      "heading" : "3.4 THE COMPATIBILITY OF NETWORK MORPHISM EQUATION",
      "text" : "One difficulty in this graph abstraction is in the dimensional compatibility of convolutional filters or blobs. For example, for the TYPE-II atomic morphing in Fig. 1b, we have to satisfy Gl = F 1l +F 2 l . Suppose that Gl and F 2l are of shape (64, 64, 3, 3), while F 1 l is (64, 64, 1, 1), they are actually not addable. Formally, we define the compatibility of modular network morphism equation as follows:\nDefinition 3. The modular network morphism equation for a module M is compatible if and only if the mathematical operators between the convolutional filters involved in this equation are welldefined.\nIn order to solve this compatibility problem, we need not to assume that blobs {Bi} and filters {Fl} are finite dimension tensors. Instead they are considered as infinite dimension tensors defined with a finite support3, and we call this as an extended definition. An instant advantage when we adopt this extended definition is that we will no longer need to differentiate Gl and G̃l in equation (2), since G̃l is simply a zero-padded version of Gl.\nLemma 4. The operations + and ~ are well-defined for the modular network morphism equation. Namely, if F 1 and F 2 are infinite dimension 4D tensors with finite support, let G = F 1 + F 2 and H = F 2 ~ F 1, then both G and H are uniquely defined and also have finite support.\nSketch of Proof. It is quite obvious that this lemma holds for the operator +. For the operator ~, if we have this extended definition, the sum in equation (2) will become infinite over the index cl. It is straightforward to show that this infinite sum converges, and also that H is finitely supported with respect to the indices cj and ci. Hence H has finite support.\nAs a corollary, we have:\nCorollary 5. The modular network morphism equation for any module M is always compatible if the filters involved in M are considered as infinite dimension tensors with finite support."
    }, {
      "heading" : "3.5 SIMPLE MORPHABLE MODULES",
      "text" : "In this section, we introduce a large family of modules, i.e, simple morphable modules, and then provide their morphing solutions. We first introduce the following definition:\nDefinition 6. A module M is simple morphable if and only if it is able to be morphed with only combinations of atomic morphing operations.\nSeveral example modules are shown in Fig. 2a. It is obvious that modules (A)-(C) are simple morphable, and the morphing process for module (C) is also illustrated in Fig. 2b.\nFor a simple morphable module M , we are able to identity a morphing sequence from M0 to M . The algorithm is illustrated in Algorithm 1. The core idea is to use the reverse operations of atomic morphing types to reduce M to M0. Hence, the morphing process is just the reverse of the reduction process. In Algorithm 1, we use a four-element tuple (M, e1, {e2, e3}, type) to represent the process of morphing edge e1 in module M to {e2, e3} using TYPE-<TYPE> atomic operation. Two auxiliary functions CHECKTYPEI and CHECKTYPEII are further introduced. Both\n3A support of a function is defined as the set of points where the function value is non-zero, i.e., support(f) = {x|f(x) 6= 0}.\nAlgorithm 1 Algorithm for Simple Morphable Modules Input: M0; a simple morphable module M Output: The morphing sequence Q that morphs M0 to M using atomic morphing operations Q = ∅ while M 6= M0 do\nwhile CHECKTYPEI(M ) is not FALSE do // Let (Mtemp, e1, {e2, e3}, type) be the return value of CHECKTYPEI(M ) Q.prepend((Mtemp, e1, {e2, e3}, type)) and M ←Mtemp end while while CHECKTYPEII(M ) is not FALSE do\n// Let (Mtemp, e1, {e2, e3}, type) be the return value of CHECKTYPEII(M ) Q.prepend((Mtemp, e1, {e2, e3}, type)) and M ←Mtemp\nend while end while\nAlgorithm 2 Algorithm for Irreducible Modules Input: Gl; an irreducible module M Output: Convolutional filters {Fi}ni=1 of M Initialize {Fi}ni=1 with random noise. Calculate the effective kernel size of M , expand Gl to G̃l by padding zeros. repeat\nfor j = 1 to n do Fix {Fi : i 6= j}, and calculate Fj = deconv(G̃l, {Fi : i 6= j}) Calculate loss l = ‖G̃l − conv({Fi}ni=1)‖2\nend for until l = 0 or maxIter is reached\nof them return either FALSE if there is no such atomic sub-module in M , or a morphing tuple (M, e1, {e2, e3}, type) if there is. The algorithm of CHECKTYPEI only needs to find a vertex satisfying indegree(Bi) = outdegree(Bi) = 1, while CHECKTYPEII looks for the matrix elements > 1 in the adjacent matrix representation of module M .\nIs there a module not simple morphable? The answer is yes, and an example is the module (D) in Fig. 2a. A simple try does not work as shown in Fig. 2b. In fact, we have the following proposition:\nProposition 7. Module (D) in Fig. 2a is not simple morphable.\nSketch of Proof. A simple morphable module M is always able to be reverted back to M0. However, for module (D) in Fig. 2a, both CHECKTYPEI and CHECKTYPEII return FALSE."
    }, {
      "heading" : "3.6 MODULAR NETWORK MORPHISM THEOREM",
      "text" : "For a module that is not simple morphable, which is called a complex module, we are able to apply Algorithm 1 to reduce it to an irreducible module M first. For M , we propose Algorithm 2 to solve the modular network morphism equation. The core idea of this algorithm is that, if only one convolutional filter is allowed to change with all others fixed, the modular network morphism equation will reduce to a linear system. The following argument guarantees the correctness of Algorithm 2.\nCorrectness of Algorithm 2. Let Gl and {Fi}ni=1 be the convolutional filter(s) associated with M0 and M . We further assume that one of {Fi}, e.g., Fj , is larger or equal to G̃l, where G̃l is the zeropadded version of Gl (this assumption is a strong condition in the expanding mode). The module network morphism equation for M can be written as\nG̃l = C1 ~ Fj ~ C2 + C3, (7)\nwhere C1, C2, and C3 are composed of other filters {Fi : i 6= j}. It can be checked that equation (7) is a linear system with |G̃l| constraints and |Fj | free variables. Since we have |Fj | ≥ |G̃l|, the system is non-deterministic and hence solvable as random matrices are rarely inconsistent.\nFor a general module M , whether simple morphable or not, we apply Algorithm 1 to reduce M to an irreducible module M ′, and then apply Algorithm 2 to M ′. Hence we have the following theorem:\nTheorem 8. A convolutional layer can be morphed to any module (any single-source, single-sink DAG subnet).\nThis theorem answers the core question of network morphism, and provides a theoretical upper bound for the capability of this learning scheme."
    }, {
      "heading" : "3.7 NON-LINEARITY AND BATCH NORMALIZATION IN MODULAR NETWORK MORPHISM",
      "text" : "Besides the convolutional layers, a neural network module typically also involves non-linearity layers and batch normalization layers, as illustrated in Fig. 3. In this section, we shall describe how do we handle these layers for modular network morphism.\nFor the non-linear activation layers, we adopt the solution proposed in (Wei et al., 2016). Instead of directly applying the non-linear activations, we are using their parametric forms. Let ϕ be any non-linear activation function, and its parametric form is defined to be\nP -ϕ = {ϕa}|a∈[0,1] = {(1− a) · ϕ+ aϕid}|a∈[0,1]. (8)\nThe shapes of the parametric form of the non-linear activation ϕ is controlled by the parameter a. When a is initialized (a = 1), the parametric form is equivalent to an identity function, and when the value of a has been learned, the parametric form will become a non-linear activation. In Fig. 3b, the non-linear activation for the morphing process is annotated as PReLU to differentiate itself with the\nother ReLU activations. In the proposed experiments, for simplicity, all ReLUs are replaced with PReLUs.\nThe batch normalization layers (Ioffe & Szegedy, 2015) can be represented as\nnewdata = data−mean√\nvar + eps · gamma+ beta. (9)\nIt is obvious that if we set gamma = √ var + eps and beta = mean, then a batch normalization layer is reduced to an identity mapping layer, and hence it can be inserted anywhere in the network. Although it is possible to calculate the values of gamma and beta from the training data, in this research, we adopt another simpler approach by setting gamma = 1 and beta = 0. In fact, the value of gamma can be set to any nonzero number, since the scale is then normalized by the latter batch normalization layer (lower right one in Fig. 3b). Mathematically and strictly speaking, when we set gamma = 0, the network function is actually changed. However, since the morphed filters for the convolutional layers are roughly randomized, even though the mean of data is not strictly zero, it is still approximately zero. Plus with the fact that the data is then normalized by the latter batch normalization layer, such small perturbation for the network function change can be neglected. In the proposed experiments, only statistical variances in performance are observed for the morphed network when we adopt setting gamma to zero. The reason we prefer such an approach to using the training data is that it is easier to implement and also yields slightly better results when we continue to train the morphed network."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS",
      "text" : "In this section, we report the results of the proposed morphing algorithms based on current state-ofthe-art ResNet (He et al., 2015), which is the winner of 2015 ImageNet classification task."
    }, {
      "heading" : "4.1 NETWORK ARCHITECTURES OF MODULAR NETWORK MORPHISM",
      "text" : "We first introduce the network architectures used in the proposed experiments. Fig. 3a shows the module template in the design of ResNet (He et al., 2015), which is actually a simple morphable two-way module. The first path consists of two convolutional layers, and the second path is a shortcut connection of identity mapping. The architecture of the ResNet module can be abstracted as the graph in Fig. 4a. For the morphed networks, we first split the identity mapping layer in the ResNet module into two layers with a scaling factor of 0.5. Then each of the scaled identity mapping layers is able to be further morphed into two convolutional layers. Fig. 3b illustrates the case with only one scaled identity mapping layer morphed into two convolutional layers, and its equivalent graph abstraction is shown in Fig. 4b. To differentiate network architectures adopted in this research, the notation morph <k1>c<k2> is introduced, where k1 and k2 are kernel sizes in the morphed network. If both of scaled identity mapping branches are morphed, we append a suffix of ‘ 2branch’. Some examples of morphed modules are illustrated in Fig. 4. We also use the suffix ‘ half’ to indicate that only one half (odd-indexed) of the modules are morphed, and the other half are left as original ResNet modules."
    }, {
      "heading" : "4.2 EXPERIMENTAL RESULTS ON THE CIFAR10 DATASET",
      "text" : "CIFAR10 (Krizhevsky & Hinton, 2009) is a benchmark dataset on image classification and neural network investigation. It consists of 32×32 color images in 10 categories, with 50,000 training images and 10,000 testing images. In the training process, we follow the same setup as in (He et al., 2015). We use a decay of 0.0001 and a momentum of 0.9. We adopt the simple data augmentation with a pad of 4 pixels on each side of the original image. A 32×32 view is randomly cropped from the padded image and a random horizontal flip is optionally applied.\nTable 1 shows the results of different networks morphed from ResNet (He et al., 2015). Notice that it is very challenging to further improve the performance, for ResNet has already boosted the number to a very high level. E.g., ResNet (He et al., 2015) made only 0.36% performance improvement by extending the model from 56 to 110 layers (Table 1). From Table 1 we can see that, with only 1.2x or less computational cost, the morphed networks achieved 2.15%, 1.60%, 1.11% performance improvements over the original ResNet-20, ResNet-56, and ResNet-110 respectively. Notice that the relative performance improvement can be up to 25%. Table 1 also compares the number of parameters of the original network architectures and the ones after morphing. As can be seen, the morphed ones only have a little more parameters than the original ones, typically less than 1.2x.\nExcept for large error rate reduction achieved by the morphed network, one exciting indication from Table 1 is that the morphed 20-layered network morph20 3c3 is able to achieve slightly lower error rate than the 110-layered ResNet (6.60% vs 6.61%), and its computational cost is actually less than 1/5 of the latter one. Similar results have also been observed from the morphed 56-layered network. It is able to achieve a 5.37% error rate, which is even lower than those of ResNet-110 (6.61%) and ResNet-164 (5.46%) (He et al., 2016). These results are also illustrated in Fig. 5(a).\nSeveral different architectures of the morphed networks were also explored, as illustrated in Fig. 4 and Table 1. First, when the kernel sizes were expanded from 1 × 1 to 3 × 3, the morphed networks (morph20 3c1 and morph20 3c3) achieved better performances. Similar results were reported in (Simonyan & Zisserman, 2014) (Table 1 for models C and D). However, because the morphed networks almost double the computational cost, we did not adopt this approach. Second, we also tried to morph the other scaled identity mapping layer into two convolutional layers (morph20 1c1 2branch), the error rate was further lowered for the 20-layered network. However, for the 56-layered and 110-layered networks, this strategy did not yield better results.\nWe also found that the morphed network learned with multiple phases could achieve a lower error rate than that learned with single phase. For example, the networks morph20 3c1 and morph20 3c3 learned with intermediate phases achieved better results in Table 1. This is quite reasonable as it divides the optimization problem into sequential phases, and thus is possible to avoid being trapped into a local minimum to some extent. Inspired by this observation, we then used a 1c1 half network as an intermediate phase for the morph56 1c1 and morph110 1c1 networks, and better results have been achieved.\nWe compared the proposed learning scheme against learning from scratch for the networks with the same architectures. These results are illustrated in Table 2. As can be seen, networks learned by morphing is able to achieve up to 2.66% absolute performance improvement and 32.6% relative performance improvement comparing against learning from scratch for the morph110 1c1 network architecture. These results are quite reasonable as when networks are learned by the proposed morphing scheme, they have already been regularized and shall have lower probability to be trapped into a bad-performing local minimum in the continual training process than the learning from scratch scheme. One may also notice that, morph110 1c1 actually performed worse than resnet110 when learned from scratch. This is because the network architecture morph 1c1 is proposed for morphing, and the identity shortcut connection is scaled with a factor of 0.5. It was also reported that residual networks with a constant scaling factor of 0.5 actually led to a worse performance in (He et al., 2016) (Table 1), while this performance degradation problem could be avoided by the proposed morphing scheme.\nFinally, it is worth noting that another advantage of the proposed learning scheme against the learning from scratch scheme is on model exploration. One can quickly check whether a morphed architecture deserves further exploration by continuing to train the morphed network in a finer learning rate (e.g. 1e-5), to see if the performance is improved. Hence, one does not have to wait for days or even months of training time to tell whether the new network architecture is able to achieve a\nbetter performance. This could save human time for deciding which network architecture is worth for exploring."
    }, {
      "heading" : "4.3 EXPERIMENTAL RESULTS ON THE CIFAR100 DATASET",
      "text" : "CIFAR100 (Krizhevsky & Hinton, 2009) is another benchmark dataset for tiny images that consists of 100 categories. There are 500 training images and 100 testing images per category. The proposed experiments on CIFAR100 follows the same setup as in the experiments on CIFAR10. The experimental results are illustrated in Table 3 and Fig. 5(b). As shown, the performance improvement is also significant: with only around 1.1x computational cost, the absolute performance improvement can be up to 2% and the relative performance improvement can be up to 8%. For the morphed 56-layered network, it also achieves better performance than the 110-layered ResNet (27.52% vs 28.46%), and with only around one half of the computation. Table 4 also compares the proposed learning scheme against learning from scratch. More than 5% absolute performance improvement and around 16% relative performance improvement were achieved."
    }, {
      "heading" : "4.4 EXPERIMENTAL RESULTS ON THE IMAGENET DATASET",
      "text" : "We also evaluate the proposed scheme on the ImageNet dataset (Russakovsky et al., 2014). This dataset consists of 1,000 object categories, with 1.28 million training images and 50K validation images. For the training process, we use a decay of 0.0001 and a momentum of 0.9. The image is resized to guarantee its shorter edge is randomly sampled from [256,480] for scale augmentation. A 224× 224 patch or its horizontal flip is randomly cropped from the re-\nsized image, with the image data per-channel normalized. We train the networks using SGD with a batch size of 256. The learning rate starts from 0.1 and is decreased with a factor of 0.1 for every 30 epochs. The networks are trained for a total of 70 epochs.\nThe comparison results of the morphed and original ResNets for both 18-layer and 34-layer networks are illustrated in Table 5 and Fig. 6. As shown in Table 5, morph18 1c1 and morph34 1c1 are able to achieve lower error rates than ResNet-18 and ResNet-34 respectively, and the absolute performance improvements can be up to 1.2%. We also draw the evaluation error curves in Fig. 6, which shows that the morphed networks morph18 1c1 and morph34 1c1 are much more effective than the original ResNet-18 and ResNet-34 respectively."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "This paper presented a systematic study on the problem of network morphism at a higher level, and tried to answer the central question of such learning scheme, i.e., whether and how a convolutional layer can be morphed into an arbitrary module. To facilitate the study, we abstracted a modular network as a graph, and formulated the process of network morphism as a graph transformation process. Based on this formulation, both simple morphable modules and complex modules have been defined and corresponding morphing algorithms have been proposed. We have shown that a convolutional layer can be morphed into any module of a network. We have also carried out experiments to illustrate how to achieve a better performing model based on the state-of-the-art ResNet with minimal extra computational cost on benchmark datasets. The experimental results have demonstrated the effectiveness of the proposed morphing approach."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Jimmy Ba", "Rich Caruana" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ba and Caruana.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba and Caruana.",
      "year" : 2014
    }, {
      "title" : "Net2net: Accelerating learning via knowledge transfer",
      "author" : [ "Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens" ],
      "venue" : "arXiv preprint arXiv:1511.05641,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.6550,",
      "citeRegEx" : "Romero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "arXiv preprint arXiv:1409.4842,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "arXiv preprint arXiv:1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "In image classification, the milestones of such networks can be roughly represented by LeNet (LeCun et al., 1989), AlexNet (Krizhevsky et al.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : ", 1989), AlexNet (Krizhevsky et al., 2012), VGG net (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al.",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : ", 2012), VGG net (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2014), and ResNet (He et al.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : ", 2014), and ResNet (He et al., 2015), with networks becoming deeper and deeper.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "Second, modern state-of-the-art convolutional neural networks have been developed with modularized architectures (Szegedy et al., 2014; He et al., 2015), which stack the construction units following the same module design.",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Second, modern state-of-the-art convolutional neural networks have been developed with modularized architectures (Szegedy et al., 2014; He et al., 2015), which stack the construction units following the same module design.",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Extensive experiments have been conducted based on ResNet (He et al., 2015) to show the effectiveness of the proposed morphing solution.",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "For example, a series of model compression techniques (Bucilu et al., 2006; Ba & Caruana, 2014; Hinton et al., 2015; Romero et al., 2014) were proposed to fit a lighter network to predict the output of a heavier network.",
      "startOffset" : 54,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "For example, a series of model compression techniques (Bucilu et al., 2006; Ba & Caruana, 2014; Hinton et al., 2015; Romero et al., 2014) were proposed to fit a lighter network to predict the output of a heavier network.",
      "startOffset" : 54,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "However, network morphism requires the knowledge being fully transferred, and existing work includes Net2Net (Chen et al., 2015) and NetMorph (Wei et al.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Note that the network morphism operations in (Chen et al., 2015; Wei et al., 2016) are quite primitive and at a micro-scale layer level.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "For example, LeNet (LeCun et al., 1998), AlexNet (Krizhevsky et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : ", 1998), AlexNet (Krizhevsky et al., 2012), and VGG net (Simonyan & Zisserman, 2014) are sequential networks, and their difference is primarily on the number of layers, which is 5, 8, and up to 19 respectively.",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "proposed networks, such as GoogLeNet (Szegedy et al., 2014; 2015) and ResNet (He et al.",
      "startOffset" : 37,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : ", 2014; 2015) and ResNet (He et al., 2015), follow a modularized architecture design, and have achieved the state-of-the-art performance.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "Results annotated with † are from (He et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "In this section, we report the results of the proposed morphing algorithms based on current state-ofthe-art ResNet (He et al., 2015), which is the winner of 2015 ImageNet classification task.",
      "startOffset" : 115,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "3a shows the module template in the design of ResNet (He et al., 2015), which is actually a simple morphable two-way module.",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "In the training process, we follow the same setup as in (He et al., 2015).",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Table 1 shows the results of different networks morphed from ResNet (He et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : ", ResNet (He et al., 2015) made only 0.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "46%) (He et al., 2016).",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "5 actually led to a worse performance in (He et al., 2016) (Table 1), while this performance degradation problem could be avoided by the proposed morphing scheme.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "We also evaluate the proposed scheme on the ImageNet dataset (Russakovsky et al., 2014).",
      "startOffset" : 61,
      "endOffset" : 87
    } ],
    "year" : 2017,
    "abstractText" : "In this work we study the problem of network morphism, an effective learning scheme to morph a well-trained neural network to a new one with the network function completely preserved. Different from existing work where basic morphing types on the layer level were addressed, we target at the central problem of network morphism at a higher level, i.e., how a convolutional layer can be morphed into an arbitrary module of a neural network. To simplify the representation of a network, we abstract a module as a graph with blobs as vertices and convolutional layers as edges, based on which the morphing process is able to be formulated as a graph transformation problem. Two atomic morphing operations are introduced to compose the graphs, based on which modules are classified into two families, i.e., simple morphable modules and complex modules. We present practical morphing solutions for both of these two families, and prove that any reasonable module can be morphed from a single convolutional layer. Extensive experiments have been conducted based on the state-of-the-art ResNet on benchmark datasets, and the effectiveness of the proposed solution has been verified.",
    "creator" : "LaTeX with hyperref package"
  }
}