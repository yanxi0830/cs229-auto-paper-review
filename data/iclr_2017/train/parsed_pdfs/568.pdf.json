{
  "name" : "568.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Xin Zheng", "Zhenzhou Wu" ],
    "emails" : [ "xzheng008@e.ntu.edu.sg,", "xin.zheng@sap.com", "zhenzhou.wu@sap.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "For text classification, a popular feature representation method is bag-of-word. However, this representation has an intrinsic disadvantage that two separate features will be generated for two words with the same root or of different tenses. Lemmatization and stemming could be applied to partially address this problem, but may not always leads to correct results. For example, “meaningful” and “meaningless” would both be considered as “meaning” after applying lemmatization or stemming algorithms, while they are of opposite meanings. Thus, word morphology could also provide useful information in document understanding, particular in short text where the information redundancy is low.\nFor short text, an important issue is data sparsity, particularly when utilizing feature representation method like bag-of-word, regardless the weighting scheme. Therefore, various distributed word representation like Word2Vec (Mikolov et al., 2013) and document representation Doc2Vec (Le & Mikolov, 2014) have been proposed to address the problem. However, this kind of method miss the word morphology information and word combination information. To deal with these issues, we propose a model which could capture various kinds of features that could benefit classification task.\nIn this paper, we look deep into characters. We learn character representation and combine both character-level (Zhang et al., 2015) and word-level embedding to represent a word. Thus both morphology and semantic properties of the word are captured. As we know, not all the words in a sentence contribute the same when predicting the sentence’s label. Therefore, highlight the relatively pertinent information would give better chance of correct prediction. Attention mechanism (Mnih et al., 2014; Bahdanau et al., 2016) which focuses on specific part of input could help achieve this goal. The applications of attention mechanism are mostly on sequential model, while we employ\n∗The two authors contribute the same for the work.\nthe idea of attention on a feed-forward network (Raffel & Ellis, 2015). By multiplying the weight assigned by attention mechanism to its corresponding word vector, a weighted feature matrix could be constructed by concatenating the sequence of word embeddings in a sentence.\nShort text usually could not provide much useful information for class prediction. We try different views to extract as much information as possible to construct an enriched sentence representation vector. Specifically, to convert a sentence representation matrix to an enriched vector, we draw two types of features. The first one is based on word feature space and the other one is based on n-gram. However, not all the features contribute the same on sentence classification. Attention mechanism is applied to focus on the significant features. Since these features come from different views, we need a method to make the elements consistent. The residual network proposed in (He et al., 2015; 2016) achieve much better results on image classification task. In other words, the residual mechanism could construct better image representation. Therefore, we adopt residual network to refine the sentence representation vector. Once we obtain a good quality representation for the sentence, it will be delivered to a classifier."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "There are many traditional machine learning methods for text classification and most of them could achieve quite good results on formal text datasets. Recently, many deep learning methods are proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).\nDeep convolutional neural network suggests benefits in image classification (Krizhevsky et al., 2012; Sermanet et al., 2013). Therefore, many research also try to apply it on text classification problem. Kim (2014) propose a model similar to Collobert et al. (2011) architecture. However, they employ two channels of word vectors. One is static throughout training and the other is fine-tuned via backpropagation. Various size of filters are conducted on both channel, and the results are concatenated together. Then max-pooling over time is taken to select the most significant feature among each filter. The selected features are concatenated as the sentence vector.\nSimilarly, Zhang et al. (2015) also employ the convolutional networks but add character-level information for text classification. They design two networks, one large and one small. Both of them have nine layers including six convolutional layer and three fully-connected layers. Between the three fully connected layers they insert two dropout for regularization. For both convolution and max-pooling layers, they employ 1-D version (Boureau et al., 2010). After convolution, they add the sum over all the results from one filter as the output. Specially, they claim 1-D max-pooling enable them to train a relatively deep network (Boureau et al.).\nBesides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. (2015) and most of them achieve quite good performance. dos Santos & Zadrozny (2014) take word morphology and shape into consideration which have been ignored for part-of-speech tagging task. They suggest the intra-word information is extremely useful when dealing with morphologically rich languages. They adopt neural network model to learn the character-level representation which is further delivered to help word embedding learning. Kim et al. (2016) construct neural language model by analysis of word representation obtained from character composition. Results suggest the model could be encode semantic and orthographic information from character level.\nAttention model is also utilized in our model, which is used to assign weights for each parts of components. Usually, attention model is used in sequential model (Rocktäschel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016). The attention mechanism includes sensor, internal state, actions, and reward. At each time, the sensor will capture a glimpse network which only focus on a small part of the network. Internal state will summarize the extracted information. Actions decides the location for the next step and reward suggests the benefit when taking the action. In our condition, we adopt a simplified attention network as (Raffel & Ellis, 2015; 2016). We do not need to guess the next step location and just give a weight on each components which indicates the significance of the element.\nResidual network (He et al., 2015; 2016; Chen et al., 2016) is known to be able to make neural network deeper and relieve degradation problem at the same time. And residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition. He et al. (2016) introduces\nhow to make the residual block more efficient on image classification. Similarly, for short text classification problem, the quality of sentence representation is also quite important for the final result. Thus, we try to adopt the residual block as in (He et al., 2015; 2016) to refine the sentence vector."
    }, {
      "heading" : "3 CHARACTER-AWARE ATTENTION RESIDUAL NETWORK",
      "text" : "In this paper, we propose a character-aware attention residual network to generate sentence representation. Figure 1 illustrates the model. For each word, the word representation vector is constructed by concatenating both character-level embedding and word semantic embedding. Thus a sentence is represented by a matrix. Then two types of features are extracted from the sentence matrix to construct the enriched sentence representation vector for short text. However, not all the features contribute the same for classification. Attention mechanism is employed to target on pertinent parts. To make features extracted from different views consistent, a residual network is adopt to refine the sentence representation vector. Thus, an enriched sentence vector is obtained to do text classification."
    }, {
      "heading" : "3.1 WORD REPRESENTATION CONSTRUCTION",
      "text" : "Let C be the vocabulary of characters, and E ∈ Rdc×|C| is the character embedding matrix, where dc is the dimensionality of character embedding. Given a word, which is composed of a sequence of characters [c1, c2, ..., cnc ], its corresponding character-level embedding matrix would be E\nw ∈ Rdc×nc . Herein, Ew·i = E · vi (1) where vi is a binary column vector with 1 only at the ci-th place and 0 for other positions. Here, we fix the word length dc and take zero-padding when necessary.\nFor each of such matrix Ew, a convolution operation (Le Cun et al., 1990) with m filters (i.e., kernels) P ∈ Rdc×k is applied on Ew, and a set of feature maps could be obtained. Instead of adopting max-pooling over time (Collobert et al., 2011), we adopt max-pooling over filters operation to capture local information of words as shown in Figure 1. Similar operation is adopted in (Shen et al., 2014). That is we get the max feature value over results of m filters at the same window position, which depicts the most significant feature over the k characters. Thus, a vector qc for the word which captures the character-level information is constructed.\nNote that embedding vector qc could only capture the word morphological features, while it can not reflect word semantic and syntactic characteristics. Therefore, we concatenate the distributed word representative vector qw (i.e., Word2Vec) (Mikolov et al., 2013) to qc as the word’s final representation q ∈ R(dc+dw), where dw is the dimensionality of Word2Vec. Given a sentence, which consists of a sequence of words [w1, w2, ..., wnw ], its representation matrix is Q ∈ R(dc+dw)×nw ."
    }, {
      "heading" : "3.2 SENTENCE REPRESENTATION VECTOR CONSTRUCTION",
      "text" : "To overcome the lack of information issue for short text, we explore various kinds of useful information from limited context. From higher level, we adopt two types of features as shown in\nFigure 2 (i.e., type 1 feature and type 2 feature). They capture different views of information for the short text, which could be considered as results from horizontal view and vertical view on sentence representation matrix Q separately.\nType 1 feature takes word’s feature space (i.e., horizontal view on Q) into consideration. The feature space is the composition of both character-level embedding and word semantic embedding. Each word is a point in the feature space. We formulate the summation over all words appearing in the sentence as the sentence’s representation, inspired by (Zhang et al., 2015). In fact, not all the words in a sentence contribute the same for prediction. Therefore, we want to highlight the significant words and this is realized by weighting the word’s representation features. To assign the weights, we employ attention mechanism, and multiply the weight to the word feature vector as Equation 2. Specifically, we follow Raffel & Ellis (2015) and Bahdanau et al. (2014) as shown in Figure 3(a). For each word representation vector qi, we apply a Tanh function on the linear transformation of qi as g(qi) = Tanh(Wqhqi + bqh), where Wth ∈ R1×(dc+dw), bth ∈ R. Then a softmax function on g(qi) is used to assign a weight si for each qi, which indicates the significance of word i in the sentence.\nsi = exp(g(qi))∑j=1 nw exp(g(qj)) , q̃i = siqi. (2)\nAs a result, we can get a weighted sentence representation matrix Q̃ ∈ R(dc+dw)×nw . Then we employ an average over words in the sentence at the same feature position and obtain a sentence representation vector r0.\nType 2 feature models the word level features (i.e., vertical view on Q). As we know, sometimes continuous words combination is meaningful and pertinent for sentence classification. To capture n-gram information, we apply convolution operation on Q, which is followed by a max-pooling over time. We adopt several different kernel sizes to model various n-grams. Different n-grams contribute differently. The attention mechanism is utilized again on the vectors of n-gram representations, and the resulting weights indicate their significance. We get the weighted feature vectors r1, r2, r3. Concatenating r0, r1, r2, r3, the complete sentence vector v is constructed."
    }, {
      "heading" : "3.3 RESIDUAL NETWORK FOR REFINING SENTENCE REPRESENTATION",
      "text" : "The residual learning (He et al., 2015; 2016) is reported to outperform state-of-the-art models in image classification task and object detection task. This suggests residual learning could help to capture and refine the embedding. To make the features of sentence vector v from different views consistent, we employ residual learning to v.\nLet the desired mapping as H(v), instead of making each layer directly optimize H(v), residual learning (He et al., 2015) turns to fit the residual function:\nF(v) := H(v)− v. (3)\nThus, the original target mapping becomes:\ny = F(v) + v. (4)\nBoth residual function F(v) and the added input form v are flexible. In our model, we construct the building block by two fully connected layers connected by a ReLU (Nair & Hinton, 2010) operation as shown in Figure 3(b). Meanwhile, the identity mapping is adopted by performing a shortcut connection and element-wise addition:\nv′ = F(v, G) + v (5)\nwhere v′ is the refined sentence vector, G is the weight matrix to be learned. After getting the sentence embedding v′ from the building block, it is further delivered to a softmax classifier for text classification."
    }, {
      "heading" : "4 EXPERIMENT",
      "text" : ""
    }, {
      "heading" : "4.1 DATASETS",
      "text" : "We adopt testing datasets from different sources. There are three datasets, including Tweets, Question, AG news. All of them are relatively short.\nTweets are typical short text with only 140 characters limitation. We crawl the tweets from Twitter with a set of keywords, which is specifically about some products. We label them as positive, negative, neutral, question and spam.\nQuestion dataset is a small dataset. The content is short questions, and the labels are question types.\nAG News dataset is from (Zhang et al., 2015). The reason we choose this is because the length of text is much shorter than others. The news here only contains the title and description fields."
    }, {
      "heading" : "4.2 EXPERIMENT SETTINGS",
      "text" : "In this paper, we take 128 ASCII characters as character set, by which most of the testing documents are composite. We define word length nc as 20 and character embedding length dc as 100. If a word with characters less than 20, zero padding is taken. If the length is larger than 20, just take the first 20 characters. We train the word distributed embedding using training data and the feature dimension is 300. We take sentence length as 20, which is enough to cover most of crucial words. We add 5 residual blocks to refine the sentence vector."
    }, {
      "heading" : "4.3 BASELINE MODEL",
      "text" : "We select both traditional models and deep learning models on classification as baselines.\nTF-SVM is the bag-of-word feature weighted by counting the term frequency in a sentence. Then deliver the feature matrix to a SVM classifier.\nTFIDF-SVM is taken as traditional baseline model. Since SVM classifier is robust and state-ofthe-art traditional classifier, and TFIDF usually assign good weights for bag-of-words in documents even for tough inputs. So this is a competitive baseline model.\nLg. Conv, Sm. Conv are proposed in (Zhang et al., 2015) which also consider character-level embedding, and they concatenate all the characters’ embeddings in a sentence in order as sentence’s representation matrix. For fair comparison, we do not include thesaurus to help clear documents."
    }, {
      "heading" : "4.4 COMPARISON RESULTS",
      "text" : "Table 3 shows the comparison results on the testing datasets. As we can see, the proposed model could outperform baseline models on Tweets and Question datasets. For AG news dataset, our method could give comparable results as the best baseline model, TFIDF-SVM. The TFIDF-SVM model can achieve relatively better results than others. However, both Lg. Conv and Sm. Conv do not perform well on Tweets dataset. This may because these two models are relatively deep network with several down sampling operations (i.e., max-pooling) and this dramatically decreases the short text representation. And short text does not contain much information. Thus Lg. Conv and Sm. Conv could not give good results. The TF-SVM model also does not perform well on Tweets dataset. This may because the tweet text is too short and term frequencies are mostly 1 which is not enough to provide information on classification. Similar to result of CAR-1 on Tweets data. When removing type 1 feature, the performance drops dramatically. However, for other datasets, in which the document length is longer and the content is relatively formal, removing type 1 feature does not influence the performance that much. Hence, these results suggest the word character-level feature and semantic feature (i.e., type 1 feature) are rather important for short, free-style text. On the other hand, by adding type 2 features can also improve the performance according to results of CAR2. Consequently, when dealing with short text, either formal or informal, including character-level feature, word-semantic feature and n-gram feature would benefit the performance.\nAnother comparison is adding the residual network or not. As we can see from Table 3, residual network could refine the vector representation. When removing residual block, performances on three datasets all decrease. In particular, the improvement for shorter and noisy text (Tweets dataset) is more than those relatively longer and formatted documents. Thus, for short noisy text classification problem, one adopts residual building block would improve the performance."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We propose a character-aware attention residual network for short text classification. We construct the sentence representation vector by two kinds of features. The first is focusing on feature space, which include both character-level characteristics and semantic characteristics. The other is n-gram features. To make them consistent, the residual network helps refine the vector representation. Experiment results suggest both extracted features and the residual network helps on short text clas-\nsification. Our proposed method could outperform the state-of-the-art traditional models and deep learning models."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Endto-end attention-based large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning mid-level features for recognition",
      "author" : [ "Y-Lan Boureau", "Francis R. Bach", "Yann LeCun", "Jean Ponce" ],
      "venue" : "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Boureau et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Boureau et al\\.",
      "year" : 2010
    }, {
      "title" : "Semantic segmentation with modified deep residual networks",
      "author" : [ "Xinze Chen", "Guangliang Cheng", "Yinghao Cai", "Dayong Wen", "Heping Li" ],
      "venue" : "In Pattern Recognition - 7th Chinese Conference, CCPR,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep convolutional neural networks for sentiment analysis of short texts",
      "author" : [ "Cı́cero Nogueira dos Santos", "Maira Gatti" ],
      "venue" : "In COLING, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers,",
      "citeRegEx" : "Santos and Gatti.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Gatti.",
      "year" : 2014
    }, {
      "title" : "Learning character-level representations for partof-speech tagging",
      "author" : [ "Cı́cero Nogueira dos Santos", "Bianca Zadrozny" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Santos and Zadrozny.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Computer Vision - ECCV - 14th European Conference,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Text understanding with the attention sum reader network",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kadlec et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, A meeting of SIGDAT, a Special Interest Group of the ACL,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V. Le", "Tomas Mikolov" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Le and Mikolov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Handwritten digit recognition with a back-propagation network",
      "author" : [ "B Boser Le Cun", "John S Denker", "D Henderson", "Richard E Howard", "W Hubbard", "Lawrence D Jackel" ],
      "venue" : "In Advances in neural information processing systems. Citeseer,",
      "citeRegEx" : "Cun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1990
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning ICML,",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Feed-forward networks with attention can solve some long-term memory problems",
      "author" : [ "Colin Raffel", "Daniel P.W. Ellis" ],
      "venue" : "CoRR, abs/1512.08756,",
      "citeRegEx" : "Raffel and Ellis.,? \\Q2015\\E",
      "shortCiteRegEx" : "Raffel and Ellis.",
      "year" : 2015
    }, {
      "title" : "Pruning subsequence search with attention-based embedding",
      "author" : [ "Colin Raffel", "Daniel P.W. Ellis" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP,",
      "citeRegEx" : "Raffel and Ellis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Raffel and Ellis.",
      "year" : 2016
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomás Kociský", "Phil Blunsom" ],
      "venue" : "CoRR, abs/1509.06664,",
      "citeRegEx" : "Rocktäschel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michaël Mathieu", "Rob Fergus", "Yann LeCun" ],
      "venue" : "CoRR, abs/1312.6229,",
      "citeRegEx" : "Sermanet et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sermanet et al\\.",
      "year" : 2013
    }, {
      "title" : "A latent semantic model with convolutional-pooling structure for information retrieval",
      "author" : [ "Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Grégoire Mesnil" ],
      "venue" : "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,",
      "citeRegEx" : "Shen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2014
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Therefore, various distributed word representation like Word2Vec (Mikolov et al., 2013) and document representation Doc2Vec (Le & Mikolov, 2014) have been proposed to address the problem.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "We learn character representation and combine both character-level (Zhang et al., 2015) and word-level embedding to represent a word.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "Attention mechanism (Mnih et al., 2014; Bahdanau et al., 2016) which focuses on specific part of input could help achieve this goal.",
      "startOffset" : 20,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Attention mechanism (Mnih et al., 2014; Bahdanau et al., 2016) which focuses on specific part of input could help achieve this goal.",
      "startOffset" : 20,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "The residual network proposed in (He et al., 2015; 2016) achieve much better results on image classification task.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "Recently, many deep learning methods are proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).",
      "startOffset" : 88,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "Recently, many deep learning methods are proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).",
      "startOffset" : 88,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "Deep convolutional neural network suggests benefits in image classification (Krizhevsky et al., 2012; Sermanet et al., 2013).",
      "startOffset" : 76,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "Deep convolutional neural network suggests benefits in image classification (Krizhevsky et al., 2012; Sermanet et al., 2013).",
      "startOffset" : 76,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "For both convolution and max-pooling layers, they employ 1-D version (Boureau et al., 2010).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "Usually, attention model is used in sequential model (Rocktäschel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).",
      "startOffset" : 53,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "Usually, attention model is used in sequential model (Rocktäschel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).",
      "startOffset" : 53,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "Usually, attention model is used in sequential model (Rocktäschel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).",
      "startOffset" : 53,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "Usually, attention model is used in sequential model (Rocktäschel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).",
      "startOffset" : 53,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "Residual network (He et al., 2015; 2016; Chen et al., 2016) is known to be able to make neural network deeper and relieve degradation problem at the same time.",
      "startOffset" : 17,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "Residual network (He et al., 2015; 2016; Chen et al., 2016) is known to be able to make neural network deeper and relieve degradation problem at the same time.",
      "startOffset" : 17,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "And residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : ", 2015; dos Santos & Gatti, 2014; Kim, 2014). Deep convolutional neural network suggests benefits in image classification (Krizhevsky et al., 2012; Sermanet et al., 2013). Therefore, many research also try to apply it on text classification problem. Kim (2014) propose a model similar to Collobert et al.",
      "startOffset" : 34,
      "endOffset" : 261
    }, {
      "referenceID" : 0,
      "context" : "Kim (2014) propose a model similar to Collobert et al. (2011) architecture.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Kim (2014) propose a model similar to Collobert et al. (2011) architecture. However, they employ two channels of word vectors. One is static throughout training and the other is fine-tuned via backpropagation. Various size of filters are conducted on both channel, and the results are concatenated together. Then max-pooling over time is taken to select the most significant feature among each filter. The selected features are concatenated as the sentence vector. Similarly, Zhang et al. (2015) also employ the convolutional networks but add character-level information for text classification.",
      "startOffset" : 38,
      "endOffset" : 496
    }, {
      "referenceID" : 0,
      "context" : "For both convolution and max-pooling layers, they employ 1-D version (Boureau et al., 2010). After convolution, they add the sum over all the results from one filter as the output. Specially, they claim 1-D max-pooling enable them to train a relatively deep network (Boureau et al.). Besides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. (2015) and most of them achieve quite good performance.",
      "startOffset" : 70,
      "endOffset" : 469
    }, {
      "referenceID" : 0,
      "context" : "For both convolution and max-pooling layers, they employ 1-D version (Boureau et al., 2010). After convolution, they add the sum over all the results from one filter as the output. Specially, they claim 1-D max-pooling enable them to train a relatively deep network (Boureau et al.). Besides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. (2015) and most of them achieve quite good performance. dos Santos & Zadrozny (2014) take word morphology and shape into consideration which have been ignored for part-of-speech tagging task.",
      "startOffset" : 70,
      "endOffset" : 547
    }, {
      "referenceID" : 0,
      "context" : "For both convolution and max-pooling layers, they employ 1-D version (Boureau et al., 2010). After convolution, they add the sum over all the results from one filter as the output. Specially, they claim 1-D max-pooling enable them to train a relatively deep network (Boureau et al.). Besides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. (2015) and most of them achieve quite good performance. dos Santos & Zadrozny (2014) take word morphology and shape into consideration which have been ignored for part-of-speech tagging task. They suggest the intra-word information is extremely useful when dealing with morphologically rich languages. They adopt neural network model to learn the character-level representation which is further delivered to help word embedding learning. Kim et al. (2016) construct neural language model by analysis of word representation obtained from character composition.",
      "startOffset" : 70,
      "endOffset" : 918
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2016; Kadlec et al., 2016). The attention mechanism includes sensor, internal state, actions, and reward. At each time, the sensor will capture a glimpse network which only focus on a small part of the network. Internal state will summarize the extracted information. Actions decides the location for the next step and reward suggests the benefit when taking the action. In our condition, we adopt a simplified attention network as (Raffel & Ellis, 2015; 2016). We do not need to guess the next step location and just give a weight on each components which indicates the significance of the element. Residual network (He et al., 2015; 2016; Chen et al., 2016) is known to be able to make neural network deeper and relieve degradation problem at the same time. And residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition. He et al. (2016) introduces",
      "startOffset" : 8,
      "endOffset" : 906
    }, {
      "referenceID" : 15,
      "context" : "qc is the character-level embedding vector for the word, and qw is the word embedding generated according to (Mikolov et al., 2013).",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Thus, we try to adopt the residual block as in (He et al., 2015; 2016) to refine the sentence vector.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Instead of adopting max-pooling over time (Collobert et al., 2011), we adopt max-pooling over filters operation to capture local information of words as shown in Figure 1.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "Similar operation is adopted in (Shen et al., 2014).",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : ", Word2Vec) (Mikolov et al., 2013) to q as the word’s final representation q ∈ Rcw, where dw is the dimensionality of Word2Vec.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : "We formulate the summation over all words appearing in the sentence as the sentence’s representation, inspired by (Zhang et al., 2015).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "We formulate the summation over all words appearing in the sentence as the sentence’s representation, inspired by (Zhang et al., 2015). In fact, not all the words in a sentence contribute the same for prediction. Therefore, we want to highlight the significant words and this is realized by weighting the word’s representation features. To assign the weights, we employ attention mechanism, and multiply the weight to the word feature vector as Equation 2. Specifically, we follow Raffel & Ellis (2015) and Bahdanau et al.",
      "startOffset" : 115,
      "endOffset" : 503
    }, {
      "referenceID" : 0,
      "context" : "Specifically, we follow Raffel & Ellis (2015) and Bahdanau et al. (2014) as shown in Figure 3(a).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "The residual learning (He et al., 2015; 2016) is reported to outperform state-of-the-art models in image classification task and object detection task.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "Let the desired mapping as H(v), instead of making each layer directly optimize H(v), residual learning (He et al., 2015) turns to fit the residual function: F(v) := H(v)− v.",
      "startOffset" : 104,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "AG News dataset is from (Zhang et al., 2015).",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "Conv are proposed in (Zhang et al., 2015) which also consider character-level embedding, and they concatenate all the characters’ embeddings in a sentence in order as sentence’s representation matrix.",
      "startOffset" : 21,
      "endOffset" : 41
    } ],
    "year" : 2017,
    "abstractText" : "Text classification in general is a well studied area. However, classifying short and noisy text remains challenging. Feature sparsity is a major issue. The quality of document representation here has a great impact on the classification accuracy. Existing methods represent text using bag-of-word model, with TFIDF or other weighting schemes. Recently word embedding and even document embedding are proposed to represent text. The purpose is to capture features at both word level and sentence level. However, the character level information are usually ignored. In this paper, we take word morphology and word semantic meaning into consideration, which are represented by character-aware embedding and word distributed embedding. By concatenating both character-level and word distributed embedding together and arranging words in order, a sentence representation matrix could be obtained. To overcome data sparsity problem of short text, sentence representation vector is then derived based on different views from sentence representation matrix. The various views contributes to the construction of an enriched sentence embedding. We employ a residual network on the sentence embedding to get a consistent and refined sentence representation. Evaluated on a few short text datasets, our model outperforms state-of-the-art models.",
    "creator" : "LaTeX with hyperref package"
  }
}