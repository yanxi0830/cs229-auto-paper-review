{
  "name" : "641.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TWO METHODS FOR WILD VARIATIONAL INFERENCE",
    "authors" : [ "Qiang Liu", "Yihao Feng" ],
    "emails" : [ "yihao.feng.gr}@dartmouth.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Probabilistic modeling provides a principled approach for reasoning under uncertainty, and has been increasingly dominant in modern machine learning where highly complex, structured probabilistic models are often the essential components for solving complex problems with increasingly larger datasets. A key challenge, however, is to develop computationally efficient Bayesian inference methods to approximate, or draw samples from the posterior distributions. Variational inference (VI) provides a powerful tool for scaling Bayesian inference to complex models and big data. The basic idea of VI is to approximate the true distribution with a simpler distribution by minimizing the KL divergence, transforming the inference problem into an optimization problem, which is often then solved efficiently using stochastic optimization techniques (e.g., Hoffman et al., 2013; Kingma & Welling, 2013). However, the practical design and application of VI are still largely restricted by the requirement of using simple approximation families, as we explain in the sequel.\nLet p(z) be a distribution of interest, such as the posterior distribution in Bayesian inference. VI approximates p(z) with a simpler distribution q∗(z) found in a set Q = {qη(z)} of distributions indexed by parameter η by minimizing the KL divergence objective:\nmin η\n{ KL(qη || p) ≡ Ez∼qη [log(qη(z)/p(z))] } , (1)\nwhere we can get exact result p = q∗ if Q is chosen to be broad enough to actually include p. In practice, however, Q should be chosen carefully to make the optimization in (1) computationally tractable; this casts two constraints on Q: 1. A minimum requirement is that we should be able to sample from qη efficiently, which allows us to make estimates and predictions based on qη in placement of the more intractable p. The samples from qη can also be used to approximate the expectation Eq[·] in (1) during optimization. This means that there should exist some computable function f(η; ξ), called the inference network, which takes a random seed ξ, whose distribution is denoted by q0, and outputs a random variable z = f(η; ξ) whose distribution is qη .\n2. We should also be able to calculate the density qη(z) or it is derivative in order to optimize the KL divergence in (1). This, however, casts a much more restrictive condition, since it requires us to use only simple inference network f(η; ξ) and input distributions q0 to ensure a tractable form for the density qη of the output z = f(η; ξ).\nIn fact, it is this requirement of calculating qη(z) that has been the major constraint for the design of state-of-the-art variational inference methods. The traditional VI methods are often limited to\nusing simple mean field, or Gaussian-based distributions as qη and do not perform well for approximating complex target distributions. There is a line of recent work on variational inference with rich approximation families (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015, to name only a few), all based on handcrafting special inference networks to ensure the computational tractability of qη(z) while simultaneously obtaining high approximation accuracy. These approaches require substantial mathematical insights and research effects, and can be difficult to understand or use for practitioners without a strong research background in VI. Methods that allow us to use arbitrary inference networks without substantial constraints can significantly simplify the design and applications of VI methods, allowing practical users to focus more on choosing proposals that work best with their specific tasks.\nWe use the term wild variational inference to refer to variants of variational methods working with general inference networks f(η, ξ) without tractability constraints on its output density qη(z); this should be distinguished with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(z) without significant model-by-model consideration (but still require to calculate the proposal density qη(z)). Essentially, wild variational inference makes it possible to “learn to draw samples”, constructing black-box neural samplers for given distributions. This enables more adaptive and automatic design of efficient Bayesian inference procedures, replacing the hand-designed inference algorithms with more efficient ones that can improve their efficiency adaptively over time based on past tasks they performed.\nIn this work, we discuss two methods for wild variational inference, both based on recent works that combine kernel techniques with Stein’s method (e.g., Liu & Wang, 2016; Liu et al., 2016). The first method, also discussed in Wang & Liu (2016), is based on iteratively adjusting parameter η to make the random output z = f(η; ξ) mimic a Stein variational gradient direction (SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution. The second method is based on minimizing a kernelized Stein discrepancy, which, unlike KL divergence, does not require to calculate density qη(z) for the optimization thanks to its special form.\nAnother critical problem is to design good network architectures well suited for Bayesian inference. Ideally, the network design should leverage the information of the target distribution p(z) in a convenient way. One useful perspective is that we can view the existing MC/MCMC methods as (hand-designed) stochastic neural networks which can be used to construct native inference networks for given target distributions. On the other hand, using existing MC/MCMC methods as inference networks also allow us to adaptively adjust the hyper-parameters of these algorithms; this enables amortized inference which leverages the experience on past tasks to accelerate the Bayesian computation, providing a powerful approach for designing efficient algorithms in settings when a large number of similar tasks are needed.\nAs an example, we leverage stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as the inference network, which can be treated as a special deep residential network (He et al., 2016), in which important gradient information ∇z log p(z) is fed into each layer to allow efficient approximation for the target distribution p(z). In our case, the network parameter η are the step sizes of SGLD, and our method provides a way to adaptively improve the step sizes, providing speed-up on future tasks with similar structures. We show that the adaptively estimated step sizes significantly outperform the hand-designed schemes such as Adagrad.\nRelated Works The idea of amortized inference (Gershman & Goodman, 2014) has been recently applied in various domains of probabilistic reasoning, including both amortized variational inference\n(e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a) and date-driven designs of Monte Carlo based methods (e.g., Paige & Wood, 2016), to name only a few. Most of these methods, however, require to explicitly calculate qη(z) (or its gradient).\nOne well exception is a very recent work (Ranganath et al., 2016) that also avoids calculating qη(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.2.\nThe auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative way when the variational distribution qη(z) can be represented as a hidden variable model. In particular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a variational approximation. These approaches, however, still require to write down the likelihood function on the augmented spaces, and need to introduce an additional inference network related to the auxiliary variables.\nThere is a large literature on traditional adaptive MCMC methods (e.g., Andrieu & Thoms, 2008; Roberts & Rosenthal, 2009) which can be used to adaptively adjust the proposal distribution of MCMC by exploiting the special theoretical properties of MCMC (e.g., by minimizing the autocorrelation). Our method is simpler, more generic, and works efficiently in practice thanks to the use of gradient-based back-propagation. Finally, connections between stochastic gradient descent and variational inference have been discussed and exploited in Mandt et al. (2016); Maclaurin et al. (2015).\nOutline Section 2 introduces background on Stein discrepancy and Stein variational gradient descent. Section 3 discusses two methods for wild variational inference. Section 4 discuss using stochastic gradient Langevin dynamics (SGLD) as the inference network. Empirical results are shown in Section 5."
    }, {
      "heading" : "2 STEIN’S IDENTITY, STEIN DISCREPANCY, STEIN VARIATIONAL GRADIENT",
      "text" : "Stein’s identity Stein’s identity plays a fundamental role in our framework. Let p(z) be a positive differentiable density on Rd, and φ(z) = [φ1(z), · · · , φd(z)]> is a differentiable vector-valued function. Define∇z · φ = ∑ i ∂ziφ. Stein’s identity is\nEz∼p[〈∇z log p(z), φ(z)〉+∇z · φ(z)] = ∫ X ∇z · (p(z)φ(z))dx = 0, (2)\nwhich holds once p(z)φ(z) vanishes on the boundary of X by integration by parts or Stokes’ theorem; It is useful to rewrite Stein’s identity in a more compact way:\nEz∼p[Tpφ(z)] = 0, with Tpφ def = 〈∇z log p, φ〉+∇z · φ, (3)\nwhere Tp is called a Stein operator, which acts on function φ and returns a zero-mean function Tpφ(z) under z ∼ p. A key computational advantage of Stein’s identity and Stein operator is that they depend on p only through the derivative of the log-density ∇z log p(z), which does not depend on the cumbersome normalization constant of p, that is, when p(z) = p̄(z)/Z, we have ∇z log p(z) = ∇z log p̄(z), independent of the normalization constant Z. This property makes Stein’s identity a powerful practical tool for handling unnormalized distributions widely appeared in machine learning and statistics.\nStein Discrepancy Although Stein’s identity ensures that Tpφ has zero expectation under p, its expectation is generally non-zero under a different distribution q. Instead, for p 6= q, there must exist a φ which distinguishes p and q in the sense that Ez∼q[Tpφ(z)] 6= 0. Stein discrepancy leverages this fact to measure the difference between p and q by considering the “maximum violation of Stein’s identity” for φ in certain function set F :\nD(q || p) = max φ∈F\n{ Ez∼q[Tpφ(z)] } , (4)\nwhere F is the set of functions φ that we optimize over, and decides both the discriminative power and computational tractability of Stein discrepancy. Kernelized Stein discrepancy (KSD) is a special\nStein discrepancy that takes F to be the unit ball of vector-valued reproducing kernel Hilbert spaces (RKHS), that is,\nF = {φ ∈ Hd : ||φ||Hd ≤ 1}, (5)\nwhere H is a real-valued RKHS with kernel k(z, z′). This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017):\nD(q || p) = max φ∈Hd\n{ Ez∼q[Tpφ(z)], s.t. ||φ||Hd ≤ 1 } , (6)\n= √ Ez,z′∼q[κp(z, z′)], (7)\nwhere κp(z, z′) is a positive definite kernel obtained by applying Stein operator on k(z, z′) twice:\nκp(z, z ′) = T z\n′\np (T zp ⊗ k(z, z′)), = sp(z)sp(z\n′)k(z, z′) + sp(z)∇z′k(z, z′) + sp(z′)∇zk(z, z′) +∇z · (∇z′k(z, z′)), (8)\nwhere sp(z) = ∇z log p(z) and T zp and T zp denote the Stein operator when treating k(z, z′) as a function of z and z′, respectively; here we defined T zp ⊗k(z, z′) = ∇x log p(x)k(z, z′)+∇xk(z, z′) which returns a d× 1 vector-valued function. It can be shown that D(q || p) = 0 if and only if q = p when k(z, z′) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016). D(q || p) can treated as a variant of maximum mean discrepancy equipped with kernel κp(z, z\n′) which depends on p (which makes D(q || p) asymmetric on q and p). The form of KSD in (6) allows us to estimate the discrepancy between a set of sample {zi} (e.g., drawn from q) and a distribution p specified by∇z log p(z),\nD̂2u({zi} || p) = 1 n(n− 1) ∑ i6=j [κp(zi, zj)], D̂2v({zi} || p) = 1 n2 ∑ i,j [κp(zi, zj)], (9)\nwhere D̂2u(q || p) provides an unbiased estimator (hence called a U -statistic) for D2(q || p), and D̂2v(q || p), called V -statistic, provides a biased estimator but is guaranteed to be always nonnegative: D̂2v({zi} || p) ≥ 0.\nStein Variational Gradient Descent (SVGD) Stein operator and Stein discrepancy have a close connection with KL divergence, which is exploited in Liu & Wang (2016) to provide a general purpose deterministic approximate sampling method. Assume that {zi}ni=1 is a sample (or a set of particles) drawn from q, and we want to update {zi}ni=1 to make it “move closer” to the target distribution p to improve the approximation quality. We consider updates of form\nzi ← zi + φ∗(zi), ∀i = 1, . . . , n, (10)\nwhere φ∗ is a perturbation direction, or velocity field, chosen to maximumly decrease the KL divergence between the distribution of updated particles and the target distribution, in the sense that\nφ∗ = arg max φ∈F { − d d KL(q[ φ] || p) ∣∣ =0 } , (11)\nwhere q[ φ] denotes the density of the updated particle z′ = z + φ(z) when the density of the original particle z is q, and F is the set of perturbation directions that we optimize over. A key observation (Liu & Wang, 2016) is that the optimization in (11) is in fact equivalent to the optimization for KSD in (4); we have\n− d d KL(q[ φ] || p) ∣∣ =0 = Ez∼q[Tpφ(z)], (12)\nthat is, the Stein operator transforms the perturbation φ on the random variable (the particles) to the change of the KL divergence. Taking F to be unit ball of Hd as in (5), the optimal solution φ∗ of (11) equals that of (6), which is shown to be (e.g., Liu et al., 2016)\nφ∗(z′) ∝ Ez∼q[T zp k(z, z′)] = Ez∼q[∇z log p(z)k(z, z′) +∇zk(z, z′)].\nAlgorithm 1 Amortized SVGD and KSD Minimization for Wild Variational Inference for iteration t do\n1. Draw random {ξi}ni=1, calculate zi = f(η; ξi), and the Stein variational gradient ∆zi in (13). 2. Update parameter η using (14) or (15) for amortized SVGD, or (17) for KSD minimization.\nend for\nBy approximating the expectation under q with the empirical mean of the current particles {zi}ni=1, SVGD admits a simple form of update that iteratively moves the particles towards the target distribution,\nzi ← zi + ∆zi, ∀i = 1, . . . , n, ∆zi = Êz∈{zi}ni=1 [∇z log p(z)k(z, zi) +∇zk(z, zi)], (13)\nwhere Êz∼{zi}ni=1 [f(z)] = ∑ i f(zi)/n. The two terms in ∆zi play two different roles: the term with the gradient ∇z log p(z) drives the particles towards the high probability regions of p(z), while the term with ∇zk(z, zi) serves as a repulsive force to encourage diversity; to see this, consider a stationary kernel k(z, z′) = k(z − z′), then the second term reduces to Êz∇zk(z, zi) = −Êz∇zik(z, zi), which can be treated as the negative gradient for minimizing the average similarity Êzk(z, zi) in terms of zi.\nIt is easy to see from (13) that ∆zi reduces to the typical gradient ∇z log p(zi) when there is only a single particle (n = 1) and ∇zk(z, zi) when z = zi, in which case SVGD reduces to the standard gradient ascent for maximizing log p(z) (i.e., maximum a posteriori (MAP))."
    }, {
      "heading" : "3 TWO METHODS FOR WILD VARIATIONAL INFERENCE",
      "text" : "Since the direct parametric optimization of the KL divergence (1) requires calculating qη(z), there are two essential ways to avoid calculating qη(z): either using alternative (approximate) optimization approaches, or using different divergence objective functions. We discuss two possible approaches in this work: one based on “amortizing SVGD” (Wang & Liu, 2016) which trains the inference network f(η, ξ) so that its output mimic the SVGD dynamics in order to decrease the KL divergence; another based on minimizing the KSD objective (9) which does not require to evaluate q(z) thanks to its special form."
    }, {
      "heading" : "3.1 AMORTIZED SVGD",
      "text" : "SVGD provides an optimal updating direction to iteratively move a set of particles {zi} towards the target distribution p(z). We can leverage it to train an inference network f(η; ξ) by iteratively adjusting η so that the output of f(η; ξ) changes along the Stein variational gradient direction in order to maximumly decrease its KL divergence with the target distribution. By doing this, we “amortize” SVGD into a neural network, which allows us to leverage the past experience to adaptively improve the computational efficiency and generalize to new tasks with similar structures. Amortized SVGD is also presented in Wang & Liu (2016); here we present some additional discussion.\nTo be specific, assume {ξi} are drawn from q0 and zi = f(η; ξi) the corresponding random output based on the current estimation of η. We want to adjust η so that zi changes along the Stein variational gradient direction ∆zi in (13) so as to maximumly decrease the KL divergence with target distribution. This can be done by updating η via\nη ← arg min η n∑ i=1 ||f(η; ξi)− zi − ∆zi||22. (14)\nEssentially, this projects the non-parametric perturbation direction ∆zi to the change of the finite dimensional network parameter η. If we take the step size to be small, then the updated η by (14) should be very close to the old value, and a single step of gradient descent of (14) can provide a\ngood approximation for (14). This gives a simpler update rule: η ← η + ∑ i ∂ηf(η; ξi)∆zi, (15)\nwhich can be intuitively interpreted as a form of chain rule that back-propagates the SVGD gradient to the network parameter η. In fact, when we have only one particle, (15) reduces to the standard gradient ascent for maxη log p(f(η; ξ)), in which fη is trained to “learn to optimize” (e.g., Andrychowicz et al., 2016), instead of “learn to sample” p(z). Importantly, as we have more than one particles, the repulsive term ∇zk(z, zi) in ∆zi becomes active, and enforces an amount of diversity on the network output that is consistent with the variation in p(z). The full algorithm is summarized in Algorithm 1.\nAmortized SVGD can be treated as minimizing the KL divergence using a rather special algorithm: it leverages the non-parametric SVGD which can be treated as approximately solving the infinite dimensional optimization minq KL(q || p) without explicitly assuming a parametric form on q, and iteratively projecting the non-parametric update back to the finite dimensional parameter space of η. It is an interesting direction to extend this idea to “amortize” other MC/MCMC-based inference algorithms. For example, given a MCMC with transition probability T (z′|z) whose stationary distribution is p(z), we may adjust η to make the network output move towards the updated values z′ drawn from the transition probability T (z′|z). The advantage of using SVGD is that it provides a deterministic gradient direction which we can back-propagate conveniently and is particle efficient in that it reduces to “learning to optimize” with a single particle. We have been using the simple L2 loss in (14) mainly for convenience; it is possible to use other two-sample discrepancy measures such as maximum mean discrepancy."
    }, {
      "heading" : "3.2 KSD VARIATIONAL INFERENCE",
      "text" : "Amortized SVGD attends to minimize the KL divergence objective, but can not be interpreted as a typical finite dimensional optimization on parameter η. Here we provide an alternative method based on directly minimizing the kernelized Stein discrepancy (KSD) objective, for which, thanks to its special form, the typical gradient-based optimization can be performed without needing to estimate q(z) explicitly.\nTo be specific, take qη to be the density of the random output z = f(η; ξ) when ξ ∼ q0, and we want to find η to minimize D(qη || p). Assuming {ξi} is i.i.d. drawn from q0, we can approximate D2(qη || p) unbiasedly with a U-statistics:\nD2(qη || p) ≈ 1 n(n− 1) ∑ i 6=j κp(f(η; ξi), f(η; ξj)), (16)\nfor which a standard gradient descent can be derived for optimizing η:\nη ← η − 2 n(n− 1) ∑ i 6=j ∂ηf(η; ξi)∇ziκp(zi, zj), where zi = f(η; ξi). (17)\nThis enables a wild variational inference method based on directly minimizing η with standard (stochastic) gradient descent. See Algorithm 1. Note that (17) is similar to (15) in form, but replaces ∆zi with a ∆̃zi ∝ − ∑ j : i 6=j ∇ziκp(zi, zj). It is also possible to use the V -statistic in (9), but we find that the U -statistic performs much better in practice, possibly because of its unbiasedness property.\nMinimizing KSD can be viewed as minimizing a constrastive divergence objective function. To see this, recall that q[ φ] denotes the density of z′ = z + φ(z) when z ∼ q. Combining (11) and (6), we can show that\nD2(q || p) ≈ 1\n(KL(q || p)−KL(q[ φ] || p)).\nThat is, KSD measures the amount of decrease of KL divergence when we update the particles along the optimal SVGD perturbation direction φ given by (11). If q = p, then the decrease of KL\ndivergence equals zero and D2(q || p) equals zero. In fact, as shown in Liu & Wang (2016) KSD can be explicitly represented as the magnitude of a functional gradient of KL divergence:\nD(q || p) = ∣∣∣∣∣∣ d dφ KL(q[φ] || p) ∣∣ φ=0 ∣∣∣∣∣∣ Hd ,\nwhere q[φ] is the density of z = z +φ(z) when z ∼ q, and ddφF (φ) denotes the functional gradient of functional F (φ) w.r.t. φ defined in RKHSHd, and ddφF (φ) is also an element inH\nd. Therefore, KSD variational inference can be treated as explicitly minimizing the magnitude of the gradient of KL divergence, in contract with amortized SVGD which attends to minimize the KL divergence objective itself.\nThis idea is also similar to the contrastive divergence used for learning restricted Boltzmann machine (RBM) (Hinton, 2002) (which, however, optimizes p with fixed q). It is possible to extend this approach by replacing z′ = z + φ(z) with other transforms, such as these given by a transition probability of a Markov chain whose stationary distribution is p. In fact, according the so called generator method for constructing Stein operator (Barbour, 1988), any generator of a Markov process defines a Stein operator that can be used to define a corresponding Stein discrepancy.\nThis idea is related to a very recent work by Ranganath et al. (2016), which is based on directly minimizing the variational form of Stein discrepancy in (4); Ranganath et al. (2016) assumes F consists of a neural network φτ (z) parametrized by τ , and find η by solving the following min-max problem:\nmin η max τ Ez∼q[Tpφτ (z)].\nIn contrast, our method leverages the closed form solution by taking F to be an RKHS and hence obtains an explicit optimization problem, instead of a min-max problem that can be computationally more expensive, or have difficulty in achieving convergence.\nBecause κp(x, x′) (defined in (8)) depends on the derivative ∇x log p(x) of the target distribution, the gradient in (17) depends on the Hessian matrix ∇2x log p(x) and is hence less convenient to implement compared with amortized SVGD (the method by Ranganath et al. (2016) also has the same problem). However, this problem can be alleviated using automatic differentiation tools, which be used to directly take the derivative of the objective in (16) without manually deriving its derivatives."
    }, {
      "heading" : "4 LANGEVIN INFERENCE NETWORK",
      "text" : "With wild variational inference, we can choose more complex inference network structures to obtain better approximation accuracy. Ideally, the best network structure should leverage the special properties of the target distribution p(z) in a convenient way. One way to achieve this by viewing existing MC/MCMC methods as inference networks with hand-designed (and hence potentially suboptimal) parameters, but good architectures that take the information of the target distribution p(z) into account. By applying wild variational inference on networks constructed based on existing MCMC methods, we effectively provide an hyper-parameter optimization for these existing methods. This allows us to fully optimize the potential of existing Bayesian inference methods, significantly improving the result with less computation cost, and decreasing the need for hyper-parameter tuning by human experts. This is particularly useful when we need to solve a large number of similar tasks, where the computation cost spent on optimizing the hyper-parameters can significantly improve the performance on the future tasks.\nStochastic Gradient Langevin Dynamics We first take the original stochastic gradient Langevin dynamics (SGLD) algorithm (Welling & Teh, 2011) as an example. SGLD starts with a random initialization z0, and perform iterative update of form\nzt+1 ← zt + ηt ∇z log p̂(zt; Mt) + √ 2ηt ξt, ∀t = 1, · · ·T, (18)\nwhere log p̂(zt; Mt) denotes an approximation of log p(zt) based on, e.g., a random mini-batch Mt of observed data at t-th iteration, and ξt is a standard Gaussian random vector of the same size as z, and ηt denotes a (vector) step-size at t-th iteration; here “ ” denotes element-wise product. When running SGLD for T iterations, we can treat zT as the output of a T -layer neural network\nparametrized by the collection of step sizes η = {ηt}Tt=1, whose random inputs include the random initialization z0, the mini-batch Mt and Gaussian noise ξt at each iteration t. We can see that this defines a rather complex network structure with several different types of random inputs (z0, Mt and ξt). This makes it intractable to explicitly calculate the density of zT and traditional variational inference methods can not be applied directly. But wild variational inference can still allow us to adaptively improve the optimal step-size η in this case.\nGeneral Langevin Networks Based on the original formula of SGLD, we proposed a more general langevin network structure, and each layer of the network has a form\nzt+1 ← Atzt + h(BtBt>∇z log p̂(zt; Mt) +Btξt +Dt), ∀t = 1, · · ·T, (19)\nwhere At, Bt and Dt are network parameters at t-th iteration(whose size is d× d, and d is the size of zt), and h(·) denotes a smooth element-wise non-linearity function; here ξt is still a standard gaussian random vector with the same size as z. With this more complex network, we can use fewer layers to construct more powerful back-box samplers."
    }, {
      "heading" : "5 EMPIRICAL RESULTS",
      "text" : ""
    }, {
      "heading" : "5.1 SGLD INFERENCE NETWORK",
      "text" : "We first test our algorithm with SGLD inference network with (18) formula on both a toy Gaussian mixture model and a Bayesian logistic regression example. We find that we can adaptively learn step sizes that significantly outperform the existing hand-designed step size schemes, and hence save computational cost in the testing phase. In particular, we compare with the following step size schemes, for all of which we report the best results (testing accuracy in Figure 3(a); testing likelihood in Figure 3(b)) among a range of hyper-parameters:\n1. Constant Step Size. We select a best constant step size in {1, 2, 23, . . . , 229} × 10−6. 2. Power Decay Step Size. We consider t = 10a × (b + t)−γ where γ = 0.55, a ∈ {−6,−5, . . . , 1, 2}, b ∈ {0, 1, . . . , 9}. 3. Adagrad, Rmsprop, Adadelta, all with the master step size selected in {1, 2, 23, . . . , 229}× 10−6, with the other parameters chosen by default values.\nGaussian Mixture We start with a simple 1D Gaussian mixture example shown in Figure 2 where the target distribution p(z) is shown by the red dashed curve. We use amortized SVGD and KSD to optimize the step size parameter of the Langevin inference network in (18) with T = 20 layers (i.e., SGLD with T = 20 iterations), with an initial z0 drawn from a q0 far away from the target distribution (see the green curve in Figure 2(a)); this makes it critical to choose a proper step size to achieve close approximation within T = 20 iterations. We find that amortized SVGD and KSD allow us to achieve good performance with 20 steps of SGLD updates (Figure 2(b)-(c)), while the result of the best constant step size and power decay step-size are much worse (Figure 2(d)-(e)).\nBayesian Logistic Regression We consider Bayesian logistic regression for binary classification using the same setting as Gershman et al. (2012), which assigns the regression weights w with a Gaussian prior p0(w|α) = N (w,α−1) and p0(α) = Gamma(α, 1, 0.01). The inference is applied on the posterior of z = [w, logα]. We test this model on the binary Covertype dataset1 with 581,012 data points and 54 features.\nTo demonstrate that our estimated learning rate can work well on new datasets never seen by the algorithm. We partition the dataset into mini-datasets of size 50, 000, and use 80% of them for training and 20% for testing. We adapt our amortized SVGD/KSD to train on the whole population of the training mini-datasets by randomly selecting a mini-dataset at each iteration of Algorithm 1, and evaluate the performance of the estimated step sizes on the remaining 20% testing mini-datasets.\nFigure 3 reports the testing accuracy and likelihood on the 20% testing mini-datasets when we train the Langevin network with T = 10, 50, 100 layers, respectively. We find that our methods outperform all the hand-designed learning rates, and allow us to get performance closer to the fully converged SGLD and SVGD with a small number T of iterations.\nFigure 4 shows the testing accuracy and testing likelihood of all the intermediate results when training Langevin network with T = 100 layers. It is interesting to observe that amortized SVGD and KSD learn rather different behavior: KSD tends to increase the performance quickly at the first few iterations but saturate quickly, while amortized SVGD tends to increase slowly in the beginning and boost the performance quickly in the last few iterations. Note that both algorithms are set up to optimize the performance of the last layers, while need to decide how to make progress on the intermediate layers to achieve the best final performance."
    }, {
      "heading" : "5.2 GENERAL LANGEVIN INFERENCE NETWORK",
      "text" : "We further test our algorithm with general Langevin inference network. We firstly construct one single layer general Langevin network to approach the posterior of Bayesian logistic regression parameters and we can achieve 74.58% average accuracy and −0.5216 average testing log-likelihood in 100 repeat experiments. This result proves the proposed general Langevin Inference Network is quite competitive and worth to explore. Moreover, we use it as a black-box sampler to approach more complicate Gaussian Mixture distributions.\nGaussian Mixture We consider 10 components Gaussian Mixture Models with mean and covariance matrix of each component randomly drawed from a uniform distribution, and we test our methods on different dimensions models.\nWe construct 6 layers of general Langevin networks as a black-box sampler, and our proposed two methods to train the black-box sampler to approximate the target distribution. Figure 5 shows our\n1https://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/binary.html\nresults on 50 dimension Gaussian Mixture case and figure 6 shows results of different dimensions of Gaussian Mixture. From the figures we can know that our proposed sampling structure is quite competive comparing with NUT sampler(Hoffman & Gelman, 2014), and these two variational inference methods can both train a good black-box sampler."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We consider two methods for wild variational inference that allows us to train general inference networks with intractable density functions, and apply it to adaptively estimate step sizes of stochastic gradient Langevin dynamics. More studies are needed to develop better methods, more applications and theoretical understandings for wild variational inference, and we hope that the two methods we discussed in the paper can motivate more ideas and studies in the field."
    } ],
    "references" : [ {
      "title" : "An auxiliary variational method",
      "author" : [ "Agakov", "Felix V", "Barber", "David" ],
      "venue" : "In International Conference on Neural Information Processing,",
      "citeRegEx" : "Agakov et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Agakov et al\\.",
      "year" : 2004
    }, {
      "title" : "A tutorial on adaptive mcmc",
      "author" : [ "Andrieu", "Christophe", "Thoms", "Johannes" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Andrieu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Andrieu et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning to learn by gradient descent by gradient descent",
      "author" : [ "Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando" ],
      "venue" : "arXiv preprint arXiv:1606.04474,",
      "citeRegEx" : "Andrychowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andrychowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Stein’s method and poisson process convergence",
      "author" : [ "Barbour", "Andrew D" ],
      "venue" : "Journal of Applied Probability,",
      "citeRegEx" : "Barbour and D.,? \\Q1988\\E",
      "shortCiteRegEx" : "Barbour and D.",
      "year" : 1988
    }, {
      "title" : "A kernel test of goodness of fit",
      "author" : [ "Chwialkowski", "Kacper", "Strathmann", "Heiko", "Gretton", "Arthur" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Chwialkowski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chwialkowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Nonparametric variational inference",
      "author" : [ "Gershman", "Samuel", "Hoffman", "Matt", "Blei", "David" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Gershman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gershman et al\\.",
      "year" : 2012
    }, {
      "title" : "Amortized inference in probabilistic reasoning",
      "author" : [ "Gershman", "Samuel J", "Goodman", "Noah D" ],
      "venue" : "In Proceedings of the 36th Annual Conference of the Cognitive Science Society,",
      "citeRegEx" : "Gershman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gershman et al\\.",
      "year" : 2014
    }, {
      "title" : "Measuring sample quality with Stein’s method",
      "author" : [ "Gorham", "Jack", "Mackey", "Lester" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Gorham et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gorham et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "Hinton", "Geoffrey E" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton and E.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hinton and E.",
      "year" : 2002
    }, {
      "title" : "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo",
      "author" : [ "Hoffman", "Matthew D", "Gelman", "Andrew" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John" ],
      "venue" : null,
      "citeRegEx" : "Hoffman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2013
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "Stein variational gradient descent: A general purpose bayesian inference algorithm",
      "author" : [ "Liu", "Qiang", "Wang", "Dilin" ],
      "venue" : "arXiv preprint arXiv:1608.04471,",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "A kernelized Stein discrepancy for goodness-of-fit tests",
      "author" : [ "Liu", "Qiang", "Lee", "Jason D", "Jordan", "Michael I" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Early stopping is nonparametric variational inference",
      "author" : [ "Maclaurin", "Dougal", "Duvenaud", "David", "Adams", "Ryan P" ],
      "venue" : "arXiv preprint arXiv:1504.01344,",
      "citeRegEx" : "Maclaurin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maclaurin et al\\.",
      "year" : 2015
    }, {
      "title" : "A variational analysis of stochastic gradient algorithms",
      "author" : [ "Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M" ],
      "venue" : "arXiv preprint arXiv:1602.02666,",
      "citeRegEx" : "Mandt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mandt et al\\.",
      "year" : 2016
    }, {
      "title" : "Control functionals for Monte Carlo integration",
      "author" : [ "Oates", "Chris J", "Girolami", "Mark", "Chopin", "Nicolas" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Oates et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Oates et al\\.",
      "year" : 2017
    }, {
      "title" : "Inference networks for sequential monte carlo in graphical models",
      "author" : [ "Paige", "Brooks", "Wood", "Frank" ],
      "venue" : "arXiv preprint arXiv:1602.06701,",
      "citeRegEx" : "Paige et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Paige et al\\.",
      "year" : 2016
    }, {
      "title" : "Operator variational inference",
      "author" : [ "R. Ranganath", "J. Altosaar", "D. Tran", "D.M. Blei" ],
      "venue" : null,
      "citeRegEx" : "Ranganath et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2016
    }, {
      "title" : "Black box variational inference",
      "author" : [ "Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M" ],
      "venue" : "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Ranganath et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical variational models",
      "author" : [ "Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David M" ],
      "venue" : "arXiv preprint arXiv:1511.02386,",
      "citeRegEx" : "Ranganath et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Rezende et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir" ],
      "venue" : "arXiv preprint arXiv:1505.05770,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2015
    }, {
      "title" : "Examples of adaptive mcmc",
      "author" : [ "Roberts", "Gareth O", "Rosenthal", "Jeffrey S" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Roberts et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2009
    }, {
      "title" : "Markov chain monte carlo and variational inference: Bridging the gap",
      "author" : [ "Salimans", "Tim" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Salimans and Tim,? \\Q2015\\E",
      "shortCiteRegEx" : "Salimans and Tim",
      "year" : 2015
    }, {
      "title" : "Variational gaussian process",
      "author" : [ "Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M" ],
      "venue" : "arXiv preprint arXiv:1511.06499,",
      "citeRegEx" : "Tran et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to draw samples: With application to amortized mle for generative adversarial learning",
      "author" : [ "Wang", "Dilin", "Liu", "Qiang" ],
      "venue" : "Submitted to ICLR",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Bayesian learning via stochastic gradient Langevin dynamics",
      "author" : [ "Welling", "Max", "Teh", "Yee W" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Welling et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Welling et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "We use the term wild variational inference to refer to variants of variational methods working with general inference networks f(η, ξ) without tractability constraints on its output density qη(z); this should be distinguished with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(z) without significant model-by-model consideration (but still require to calculate the proposal density qη(z)).",
      "startOffset" : 267,
      "endOffset" : 291
    }, {
      "referenceID" : 13,
      "context" : "In this work, we discuss two methods for wild variational inference, both based on recent works that combine kernel techniques with Stein’s method (e.g., Liu & Wang, 2016; Liu et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "As an example, we leverage stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as the inference network, which can be treated as a special deep residential network (He et al., 2016), in which important gradient information ∇z log p(z) is fed into each layer to allow efficient approximation for the target distribution p(z).",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : ", Liu & Wang, 2016; Liu et al., 2016). The first method, also discussed in Wang & Liu (2016), is based on iteratively adjusting parameter η to make the random output z = f(η; ξ) mimic a Stein variational gradient direction (SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution.",
      "startOffset" : 20,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "One well exception is a very recent work (Ranganath et al., 2016) that also avoids calculating qη(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : ", 2016) that also avoids calculating qη(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.",
      "startOffset" : 160,
      "endOffset" : 248
    }, {
      "referenceID" : 17,
      "context" : ", 2016) that also avoids calculating qη(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.",
      "startOffset" : 160,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : ", 2016) that also avoids calculating qη(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.",
      "startOffset" : 160,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : ", 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.2. The auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative way when the variational distribution qη(z) can be represented as a hidden variable model. In particular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a variational approximation.",
      "startOffset" : 8,
      "endOffset" : 349
    }, {
      "referenceID" : 4,
      "context" : ", 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.2. The auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative way when the variational distribution qη(z) can be represented as a hidden variable model. In particular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a variational approximation. These approaches, however, still require to write down the likelihood function on the augmented spaces, and need to introduce an additional inference network related to the auxiliary variables. There is a large literature on traditional adaptive MCMC methods (e.g., Andrieu & Thoms, 2008; Roberts & Rosenthal, 2009) which can be used to adaptively adjust the proposal distribution of MCMC by exploiting the special theoretical properties of MCMC (e.g., by minimizing the autocorrelation). Our method is simpler, more generic, and works efficiently in practice thanks to the use of gradient-based back-propagation. Finally, connections between stochastic gradient descent and variational inference have been discussed and exploited in Mandt et al. (2016); Maclaurin et al.",
      "startOffset" : 8,
      "endOffset" : 1192
    }, {
      "referenceID" : 4,
      "context" : ", 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.2. The auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative way when the variational distribution qη(z) can be represented as a hidden variable model. In particular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a variational approximation. These approaches, however, still require to write down the likelihood function on the augmented spaces, and need to introduce an additional inference network related to the auxiliary variables. There is a large literature on traditional adaptive MCMC methods (e.g., Andrieu & Thoms, 2008; Roberts & Rosenthal, 2009) which can be used to adaptively adjust the proposal distribution of MCMC by exploiting the special theoretical properties of MCMC (e.g., by minimizing the autocorrelation). Our method is simpler, more generic, and works efficiently in practice thanks to the use of gradient-based back-propagation. Finally, connections between stochastic gradient descent and variational inference have been discussed and exploited in Mandt et al. (2016); Maclaurin et al. (2015).",
      "startOffset" : 8,
      "endOffset" : 1217
    }, {
      "referenceID" : 13,
      "context" : "This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max φ∈Hd { Ez∼q[Tpφ(z)], s.",
      "startOffset" : 93,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max φ∈Hd { Ez∼q[Tpφ(z)], s.",
      "startOffset" : 93,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max φ∈Hd { Ez∼q[Tpφ(z)], s.",
      "startOffset" : 93,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "It can be shown that D(q || p) = 0 if and only if q = p when k(z, z′) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016).",
      "startOffset" : 118,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "It can be shown that D(q || p) = 0 if and only if q = p when k(z, z′) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016).",
      "startOffset" : 118,
      "endOffset" : 163
    }, {
      "referenceID" : 19,
      "context" : "This idea is related to a very recent work by Ranganath et al. (2016), which is based on directly minimizing the variational form of Stein discrepancy in (4); Ranganath et al.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "This idea is related to a very recent work by Ranganath et al. (2016), which is based on directly minimizing the variational form of Stein discrepancy in (4); Ranganath et al. (2016) assumes F consists of a neural network φτ (z) parametrized by τ , and find η by solving the following min-max problem: min η max τ Ez∼q[Tpφτ (z)].",
      "startOffset" : 46,
      "endOffset" : 183
    }, {
      "referenceID" : 19,
      "context" : "This idea is related to a very recent work by Ranganath et al. (2016), which is based on directly minimizing the variational form of Stein discrepancy in (4); Ranganath et al. (2016) assumes F consists of a neural network φτ (z) parametrized by τ , and find η by solving the following min-max problem: min η max τ Ez∼q[Tpφτ (z)]. In contrast, our method leverages the closed form solution by taking F to be an RKHS and hence obtains an explicit optimization problem, instead of a min-max problem that can be computationally more expensive, or have difficulty in achieving convergence. Because κp(x, x′) (defined in (8)) depends on the derivative ∇x log p(x) of the target distribution, the gradient in (17) depends on the Hessian matrix ∇x log p(x) and is hence less convenient to implement compared with amortized SVGD (the method by Ranganath et al. (2016) also has the same problem).",
      "startOffset" : 46,
      "endOffset" : 859
    }, {
      "referenceID" : 5,
      "context" : "Bayesian Logistic Regression We consider Bayesian logistic regression for binary classification using the same setting as Gershman et al. (2012), which assigns the regression weights w with a Gaussian prior p0(w|α) = N (w,α−1) and p0(α) = Gamma(α, 1, 0.",
      "startOffset" : 122,
      "endOffset" : 145
    } ],
    "year" : 2017,
    "abstractText" : "Variational inference provides a powerful tool for approximate probabilistic inference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable probability density functions. This largely limits the design and implementation of variational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD, yielding significant improvement over the hand-designed step size schemes.",
    "creator" : "LaTeX with hyperref package"
  }
}