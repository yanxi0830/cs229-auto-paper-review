{
  "name" : "685.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "OPENING THE VOCABULARY OF NEURAL LANGUAGE MODELS WITH CHARACTER-LEVEL WORD REPRESEN- TATIONS",
    "authors" : [ "Matthieu Labeau", "Alexandre Allauzen" ],
    "emails" : [ "labeau@limsi.fr", "allauzen@limsi.fr" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Most of neural language models, such as n-gram models Bengio et al. (2003) are word based and rely on the definition of a finite vocabulary V . As a consequence, a Look-up table is associated to V in which each word w ∈ V is mapped to a vector of dE real valued features stored in a matrix L ∈ R|V|∗dE . While this approach has proven successful for a variety of tasks and languages, see for instance Schwenk (2007) in speech recognition and Le et al. (2012); Devlin et al. (2014); Bahdanau et al. (2014) in machine translation, it induces several limitations.\nFor morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data. On the one hand, growing the Look-up table is not a solution, since it would increase the number of parameters without having enough training example for a proper estimation. On the other hand, rare words can be replaced by a special token. Nevertheless, this acts as a word class merging very different words without any distinction and using different word classes to handle outof-vocabulary words Allauzen & Gauvain (2005) does not really solve this issue, since rare words are difficult to classify.\nMoreover, for most inflected or agglutinative forms, as well as for compound words, the word structure is overlooked, wasting parameters for modeling forms that could be more efficiently handled by word decomposition. While the use of subword units Botha & Blunsom (2014); Sennrich et al. (2016) could improve the generalization power of such models, it relies on a proper and efficient method to induce these subword units.\nTo overcome these issues, we propose to investigate a word based language model with an open vocabulary. Since most of existing models and training criteria rely on the assumption of a finite vocabulary, the definition of an open vocabulary model, along with a training criterion, constitutes a scientific challenge. Our goal is to build word representations every words. Word representations are inferred on-the-fly from its character sequence, using convolution filters which implicitly capture subword patterns, as described in section 2. The architecture is based on a neural ngram model inspired from Bengio et al. (2003), while this idea can be extended to other kind of models. By relaxing the normalized constraint, the objective function borrows from the noise contrastive estimation Gutmann & Hyvärinen (2012) to allow our model to consider a possibly infinite vocabulary. This paper focusses on this challenge and its related training issues. To assess the efficiency of\nthis approach, the experimental setup described in section 3 uses a large scale translation task in a reranking setting. The experimental results summarized in section 4 show promising results as well as training issues."
    }, {
      "heading" : "2 MODEL DESCRIPTION",
      "text" : "Word embeddings are parameters, stored in a Look-up matrix L. The embedding ewordw of a word w is simply the column of L corresponding to its index in the vocabulary:\newordw = [L]w"
    }, {
      "heading" : "2.1 CHARACTER-LEVEL WORD EMBEDDINGS",
      "text" : "To infer a word embedding from its character embeddings, we use a convolution layer Waibel et al. (1990); Collobert et al. (2011), similar to layers used in Santos & Zadrozny (2014); Kim et al. (2015). As illustrated in figure 1, a word w is a character sequence {c1, .., c|w|} represented by their embeddings {Cc1 , ..,Cc|w|}, where Cci denotes the vector associated to the character ci. A convolution filter Wconv ∈ Rde × Rdc∗nc is applied over a sliding window of nc characters, producing local features :\nxn = W conv(Ccn−nc+1 : .. : Ccn) T + bconv\nwhere xn is a vector of size de obtained for each position n in the word1. The notation (Ccn−1 : Ccn ) denotes the concatenation of two embeddings. The i-th element of the embedding of w is the mean over the i-th elements of the feature vectors, passed by the activation function φ :\n[echar]i = φ |w|−nc+1∑ n=1 [xn]i |w| − nc + 1  (1) Using a mean after a sliding convolution window ensures that the embedding combines local features from the whole word, and that the gradient is redistributed at scale for each character n-gram. The parameters of the layer are the matrices C and Wconv and the bias bconv ."
    }, {
      "heading" : "2.2 MODELS",
      "text" : "Our model follows the classic n-gram feedforward architecture. The input of the network is a nwords context Hi = (wi−1, . . . , wN−i+1), and its output the probability P (w|Hi) for each word w ∈ V . The embeddings of the word in the context are concatenated and fed into a hidden layer:\nhHi = φ(Whidden(ei−1 : . . . : eN−i+1) + b hidden)\nA second hidden layer my be added. Finally, the output layer computes scores for each word:\nsHi = exp (WouthHi + bout)\nWhidden, bhidden, Wout and bout are the parameters of the model. As the input Lookup-matrix L, the output weight matrix Wout contains word embeddings, that are output representations of the words in the vocabulary:\neoutw = [W out]w\nThen, the output probabilities are expressed as:\nP (w|Hi) = exp eoutw h Hi∑ 1<j<|V| exp eoutj h Hi\nLater, we will use three different input layer to obtain word representations: 1Two padding character tokens are used to deal with border effects. The first is added at the beginning and the second at the end of the word, as many times as it is necessary to obtain the same number of windows than the length of the word. Their embeddings are added to C.\n• A classic NLM using word-level embeddings only, that we will note WE, which uses |V| ∗ de parameters. • A NLM using embeddings constructed from character n-grams by convolution + pooling,\nthat we will note CE, which uses |Vc| ∗ dc + dc ∗ nc ∗ de parameters. • A NLM using a concatenation of these two types of embeddings as word representation,\nthat we will note CWE."
    }, {
      "heading" : "2.3 OBJECTIVE FUNCTION FOR OPEN VOCABULARY MODELS",
      "text" : "Usually, such a model is trained by maximizing the log-likelihood. For a given word given its context, the model parameters θ are estimated in order to maximize the following function for all the n-grams observed in the training data:\nLL(θ) = ∑\n1<i<|D|\nlogPθ(wi|Hi).\nThis objective function raises two important issues. For conventional word models, it implies a very costly summation imposed by the softmax activation of the output layer. More importantly, this objective requires the definition of a finite vocabulary, while the proposed model may use characterbased word embeddings, especially at the output, making the notion of vocabulary obsolete.\nTherefore, the parameters estimation relies on Noise Contrastive Estimation (NCE) introduced in Gutmann & Hyvärinen (2012); Mnih & Teh (2012). This criterion allows us to train both types of models based on conventional word embeddings, along with character-based embeddings. The NCE objective function aims to discriminate between examples sampled from the real data and from a noise distribution. When presented with examples coming from a mixture of one sample from the data distribution Pd and k from the noise distribution Pn, PH(w ∈ D) denotes the posterior probability of a word w given its context H to be sampled from the training data D. This probability can be expressed as follows:\nPH(w ∈ D) = P H d (w)\nPHd (w) + kPn(w)\nAs suggested in Mnih & Teh (2012), Pn only depends on w here, since we chose the unigram distribution estimated on the training data. If\nsHθ (w) = exp (e outhH + bout) (2)\ndenotes the non-normalized score given by the model to a specific word w, as a function of the parameters θ and the context H , the final NCE objective function has the following form Gutmann\n& Hyvärinen (2012):\nJHθ = EsHθ\n[ log\nsHθ (w)\nsH(w) + kPn(w)\n] + kEPn [ log\nkPn(w)\nsHθ (w) + kPn(w)\n] ,\nwhere sHθ will tend to P H d without the need for an explicit normalization."
    }, {
      "heading" : "2.4 CHARACTER-BASED OUTPUT WEIGHTS WITH NOISE-CONTRASTIVE ESTIMATION",
      "text" : "The output weights representing each word in the vocabulary eout can also be replaced by embeddings computed by a convolution layer on character n-grams. In this case the model can efficiently represent and infer a score to any word, observed during the training process or not, while with conventional word embeddings, out of vocabulary words only share the same representation and distribution. Instead of using a parameter matrix Wout to estimate the score like in equation 2, the output representation of a word w, eoutw can be replaced by a vector e char−out w estimated on the fly based on its character sequence as described in equation 1, using |Vc| ∗dc +dc ∗nc ∗dh parameters. With this extension the model does not rely on a vocabulary anymore, hence motivating our choice of the NCE. This unnormalized objective allows us to handle an open vocabulary, since we only need to compute k+ 1 word representations for each training examples. Models that use character-based embeddings both for input and output words are denoted by CWE-CWE.\nMoreover, with this extension, the representations of words sharing character n-grams are tied. This is an important property to let the model generalize to unseen words. However, it can be also an issue: the limited number of updates for output representations (k+ 1 words) has a “rich get richer” effect: the most frequent words are usually short and will get most of the update. They may therefore ”contaminate” the representation of longer words with which they share character n-grams, even if these words are not related. This issue is further addressed in section 4.1."
    }, {
      "heading" : "3 EXPERIMENTAL SET-UP",
      "text" : "The impact of the models described in section 2 is evaluated within the machine translation (MT) shared task of IWSLT-20162 from Englih to Czech. This language pair is highly challenging since Czech is a morphologically-rich language. Neural language models are integrated in a two steps approach: the first step uses a conventional MT system to produce an n-best list (the n most likely translations); in the second step, these hypothesis are re-ranked by adding the score of the neural language model. To better benefit from the open vocabulary models introduced in section 2.1, a more complex system is also used: first an MT system is used to translate from English to a simplified form of Czech which is reinflected. With this pipeline we expect n-best lists with more diversity and also words unseen during the training process. The neural language models are then used to re-rank the reinflected n-best lists."
    }, {
      "heading" : "3.1 DATA",
      "text" : "The IWSLT16 MT task is focused on the translation of TED talks. The translation systems are trained on parallel data from the TED, QED and europarl. Our Neural language models are trained on the same data, but training examples are sampled from these corpora given weights that are computed to balance between in-domain parallel data (TED), out-of domain parallel data, and additional monolingual data. Finally, we use the concatenation of TED.dev2010, TED.dev2011 and TED.tst2010 as development set, while TED.tst2012 and TED.tst2013 provide the test set."
    }, {
      "heading" : "3.2 CZECH RE-INFLECTION",
      "text" : "In Czech, a morphologically rich language, each lemma can take a lot of possible word forms. Most of them won’t appear - or with a very low frequency - in training data. For an important part of the words found in test data and unseen during training, their lemmas however can be observed but with a different morphological derivation.\n2http://workshop2016.iwslt.org\nA non-observed word form can’t be generated by the translation system, and one seen too rarely won’t be used in a relevant way. To circumvent this limitation, in a similar fashion as the method described in Marie et al. (2015), each noun, pronoun and adjective is replaced in the training corpora by its lemma along with some morphological features. These word forms are considered in factored way, where some of the POS tags are discarded to reduce the vocabulary. After the translation process, a cascade of Conditional Random Fields (CRF) are used to reintroduce the discarded features, such as gender, number and case, and to generate a new word form.\nFormally, the MT system translates English into a simplified version of Czech, that is reinflected. Within this process, the MT system can produce a n-best list, that can be extended to a nk-best list, considering for each translation hypothesis the k-best reinflected sentences given by the factorized CRF. Intuitively, this process can introduce word forms potentially not yet seen in training data, but based on known paradigms, which can give an advantage to language models able to build a word representation from character n-grams."
    }, {
      "heading" : "3.3 BASELINE TRANSLATION SYSTEM",
      "text" : "Our baseline is built with a Statistical Machine Translation system based on bilingual n-grams, NCODE3, described in Crego et al. (2011). We follow the same setup as in Marie et al. (2015)."
    }, {
      "heading" : "3.4 NLM TRAINING AND OPTIMIZATION",
      "text" : "First, some comparative experiments on a smaller dataset are carried out to better understand how open vocabulary NLM behave and to set the hyper-parameters. First trained using stochastic gradient descent, we observed a quite unstable training process, restricting a proper hyper-parameters choices. We found that especially the embedding dimensions, and the activation functions used could make the NCE-objective hard to optimize. This was aggravated in Czech, which we found more difficult to work with than other morphologically complex languages, like German and Russian. The use of Adagrad Duchi et al. (2010) clearly helps to solve most of these issues, but adds consequent computation time. Following preliminary results on our work with a similar model on a different task Labeau et al. (2015), we made the choice of not implementing LSTMs to obtain character-level word representations. It gave similar results, at the cost of unstable training and extended computation time. We then train using batches of 128, for various context sizes, WE, CWE, and CWE-CWE models. The ReLu activation function is used, along with an embedding size of de = 128. When relevant, we used a character embedding size of dc = 32 and a convolution on nc = 5-grams of characters for all experiments4. Concerning the NCE training, we sampled k = 25 examples from the unigram distribution obtained from the training data, for each example sampled from the data. The models were implemented using C++5."
    }, {
      "heading" : "3.5 RERANKING",
      "text" : "The re-ranking step uses additional features to find a better translation among the n-best generated by the decoder (in our case, n = 300): we use the score (probability) of WE, CWE and CWECWE models given to each sentence by our models as such a feature. Tuning for re-ranking was performed with KB-MIRA Cherry & Foster (2012), and evaluation using BLEU score."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS",
      "text" : "The first set of experiments investigates the impact of the padding design on the character-level representation followed by a study of the learning behavior of our proposed models and training criterion. Then, the proposed models are evaluated within the MT task. The final set of experiments analyzes the issues of the model based on character-level representation for output words, in order to propose remedies.\n3http://ncode.limsi.fr 4Results did not differ significantly when increasing these embedding sizes, with an impact on convergence\nspeed and computation time. 5Implementation will be made available."
    }, {
      "heading" : "4.1 TIES BETWEEN CHARACTER-LEVEL REPRESENTATION OF OUTPUT WORDS",
      "text" : "Preliminary results on smaller dataset are quite poor for models using character-level representation, and far worse when used for the output layer. We suspect that groups of characters are updated far more together, yielding a ”contamination” of several character n-grams by very frequent short words.\nIndeed, our simple padding scheme, as shown in the left part of table 1, makes words sharing first or last letter(s) systematically share at least one character n-gram: we suppose it gives the models more chance to detect similarities in word forms sharing prefixes and suffixes.\nThe representations of any of the character n-grams that are included in the frequent words will thus be re-used in a large part of the other words in the corpus. A huge number of word forms are affected: a little more than one third of the training data shares its first character n-gram with one of the ten most frequent words, and a little more than one quarter shares its last.\nWhile considering varying size of character n-grams when building our word representation, as in Kim et al. (2015), would certainly help, it would increase our computation time. We thus choose to alleviate our padding scheme, as shown on the right part of table 1. We add only one character token at the beginning of the word, and one at the end6. While it may inhibit the capacity of the model to build links between words sharing prefixes or suffixes, it improves results drastically, especially when using character-level outputs, as shown in figure 3. This limited padding scheme is used for the following experiments."
    }, {
      "heading" : "4.2 NLM TRAINING",
      "text" : "While the perplexity of our language models is not our main focus, it is still related to the quantity that our training seeks to optimize - since the NCE gradient approaches the maximum likelihood gradient Mnih & Teh (2012). On figure 2 are shown perplexity values of each model during training. These values are based on a vocabulary containing the 250K most frequent words on the training data - it is also the vocabulary used in the model when relevant. They are computed on the development set after each epoch. An epoch includes 2,5M N-grams sampled from the training data. On table 2 are shown the best perplexity obtained on the development set by each model, during training.\nFigure 4: Model perplexity measured on the development set during training. The context size is 3 words. Figure 3 shows models based on character-level word representations, with and without complete padding. Models are trained on the same data than Figure 2 but on smaller epochs (250K n-grams).\nsame vocabulary as for other models, and use the ’unknown’ tokens for words and characters-based representations. Hence, the perplexity computed is difficult to interpret. The main downside of Adagrad is that the learning rate determined by accumulating the history of past gradients is usually too aggressive and stops learning rather early. We simply reset this history every five epochs to give the model a chance to improve, which explains the flattening followed by small improvements we see for WE and CWE models. We choose to do that reset 2 times, based on previous experiments. Despite adaptive gradient, training of CWE-CWE models stays unstable."
    }, {
      "heading" : "4.3 RERANKING",
      "text" : "The reranking results are shown in table 3. The first line corresponds to experiments with a direct translation from English to Czech, where n-best lists generated by the MT system are simply rescored by our models. The best result is given by the longest-context CWE model, which produces a +0.7 BLEU score improvement. CWE models gives on average +0.1 BLEU point compared to WE models, while CWE-CWE are−0.2 BLEU point under. Doubling the context size consistently improves results of +0.2 BLEU point.\nExperimental results on reinflected Czech seems to follow a similar trend: CWE models behave a little better than WE models, while CWE-CWE models are under. While simply reranking n-best lists is not as efficient as doing it directly in Czech, reranking nk-best lists extended by the factorized CRF gives a small improvement, reaching an improvement of +0.7 BLEU point. As a general rule, small context models seem to have difficulties with reinflected Czech. The main advantage given by the CWE model is an ability to better rerank nk-best lists. These results suggest that, while the normalization + reinflection procedure may introduce diversity in the output to be reranked, our models are not able to draw any significant advantage from it."
    }, {
      "heading" : "4.4 ANALYSIS OF CHARACTER-LEVEL OUTPUT REPRESENTATIONS PERFORMANCE",
      "text" : "Models using character-level output representations gave sub-par results on re-ranking. It is surprising, especially for re-inflected Czech: such a model is supposed to behave better on unknown words, and thus should benefit from diversity given by generating new words. However, as we can see in table 4, re-inflection doesn’t add that much diversity (About 0.1 % of OOV words, and about 0.001 % of words never seen by the model before). Diversity is also inhibited by our training algorithm: while we train open-vocabulary models, the negative examples used with Noise-contrastive estimation come from a closed vocabulary.\nThis can related to the nature of the unigram distribution used to sample negative examples. As explained in section 4.1, it makes frequent short words completely outweigh the others in number of updates, and we are forced to reduce the ability of the model to find common morphological attributes between words to avoid ’contamination’ of character n-gram representations."
    }, {
      "heading" : "5 RELATED WORKS",
      "text" : "There is a number of different strategies to efficiently train NNLMs with large vocabularies, such as different types of hierarchical softmax Mnih & Hinton (2009); Le et al. (2011), importance sampling Bengio & Sénécal (2003), and Noise contrastive estimation Gutmann & Hyvärinen (2012); Mnih & Teh (2012). Vaswani et al. (2013) has showed the interest of training a NLM with NCE to re-rank k-best lists, while Devlin et al. (2014) uses a self-normalization. Recently, a comparative study Chen et al. (2016) has been made on how to deal with a large vocabulary. However, the purpose of this paper is to explore models with open vocabulary rather large vocabulary.\nThere is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016).\nIn language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging.\nThe recent work of Kim et al. (2015), that uses convolutional networks and pooling to construct a word representation from character n-grams, coupled with highway networks Srivastava et al. (2015), showed on various languages that using characters improves results on the language modeling task (for a small corpus), even more so for languages with complex morphology. A similar architecture was used Józefowicz et al. (2016) on a larger dataset, conjointly with bi-directional LSTMs, and trained with importance sampling, showing great results.\nOn the study of NNLMs in the context of Machine Translation, we can mention the work of Luong et al. (2015) on the effect of the number of layers on reranking n-best lists. Finally, while not directly related to our work, Luong & Manning (2016) very recently showed great improvements on a translation task by handling rare words with character-level recurrent networks, with a neural translation model."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this work, we addressed the challenge of designing an open vocabulary Neural Language Model. For that purpose, word representations are estimated on-the-fly from n-grams of characters. Two kinds of models are introduced: first, NLMs using word and character-level embeddings to represent the input context (CWE); then its extension to an open-vocabulary even for the predicted words (CWE-CWE). These models were used to re-rank outputs of translation systems from English to Czech. We also carried out experiments on translation systems from English to a simplified Czech, which is then re-inflected into Czech before re-ranking.\nWe obtained a slight improvement in BLEU score using a CWE model, which, given the little variety of the words generated by translation systems, makes us suppose there is room for more. We plan to investigate with more complex translation systems, as well as with other applications, such as morphological re-inflection.\nWhile the performance of our open-vocabulary models are to some extent disappointing, they open questions about the learned representations we will explore. We also plan to investigate on a more fitted noise distribution to use with NCE when training open-vocabulary models."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Open vocabulary asr for audiovisual document indexation",
      "author" : [ "A. Allauzen", "J.L Gauvain" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Allauzen and Gauvain.,? \\Q2005\\E",
      "shortCiteRegEx" : "Allauzen and Gauvain.",
      "year" : 2005
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith" ],
      "venue" : "In Llus Mrquez,",
      "citeRegEx" : "Ballesteros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Quick training of probabilistic neural nets by importance sampling",
      "author" : [ "Yoshua Bengio", "Jean-Sébastien Sénécal" ],
      "venue" : "In Proceedings of the conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Bengio and Sénécal.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio and Sénécal.",
      "year" : 2003
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Compositional Morphology for Word Representations and Language Modelling",
      "author" : [ "Jan A. Botha", "Phil Blunsom" ],
      "venue" : "In Proceedings of the International Conference of Machine Learning (ICML),",
      "citeRegEx" : "Botha and Blunsom.,? \\Q2014\\E",
      "shortCiteRegEx" : "Botha and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Strategies for training large vocabulary neural language models. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016",
      "author" : [ "Wenlin Chen", "David Grangier", "Michael Auli" ],
      "venue" : "August 7-12,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch tuning strategies for statistical machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
      "author" : [ "Colin Cherry", "George Foster" ],
      "venue" : null,
      "citeRegEx" : "Cherry and Foster.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cherry and Foster.",
      "year" : 2012
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "N-code: an open-source Bilingual N-gram SMT Toolkit",
      "author" : [ "Josep Maria Crego", "Franois Yvon", "Jos B. Mariño" ],
      "venue" : "Prague Bulletin of Mathematical Linguistics,",
      "citeRegEx" : "Crego et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Crego et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast and robust neural network joint models for statistical machine translation",
      "author" : [ "Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Devlin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Technical Report UCB/EECS-2010-24, EECS Department,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "CoRR, abs/1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Hybrid speech recognition with deep bidirectional LSTM",
      "author" : [ "Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed" ],
      "venue" : "Czech Republic, December",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
      "author" : [ "Michael U. Gutmann", "Aapo Hyvärinen" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Gutmann and Hyvärinen.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gutmann and Hyvärinen.",
      "year" : 2012
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Józefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : "CoRR, abs/1602.02410,",
      "citeRegEx" : "Józefowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Józefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush" ],
      "venue" : "arXiv preprint arXiv:1508.06615,",
      "citeRegEx" : "Kim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Non-lexical neural architecture for finegrained pos tagging",
      "author" : [ "Matthieu Labeau", "Kevin Löser", "Alexandre Allauzen" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Labeau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Labeau et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer" ],
      "venue" : "In In proceedings of NAACL-HLT (NAACL 2016).,",
      "citeRegEx" : "Lample et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Structured output layer neural network language model",
      "author" : [ "Hai Son Le", "Ilya Oparin", "Alexandre Allauzen", "Jean-Luc Gauvain", "François Yvon" ],
      "venue" : "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Le et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2011
    }, {
      "title" : "Continuous space translation models with neural networks. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
      "author" : [ "Hai-Son Le", "Alexandre Allauzen", "François Yvon" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2012
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Lus Marujo", "Tiago Lus" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D. Manning" ],
      "venue" : "CoRR, abs/1604.00788,",
      "citeRegEx" : "Luong and Manning.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "Deep neural language models for machine translation",
      "author" : [ "Thang Luong", "Michael Kayser", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the 19th Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Limsi@wmt’15 : Translation task",
      "author" : [ "Benjamin Marie", "Alexandre Allauzen", "Franck Burlot", "Quoc-Khanh Do", "Julia Ive", "elena knyazeva", "Matthieu Labeau", "Thomas Lavergne", "Kevin Löser", "Nicolas Pécheux", "François Yvon" ],
      "venue" : "In Proceedings of the Tenth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Marie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Marie et al\\.",
      "year" : 2015
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "Jan Cernocky" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2009
    }, {
      "title" : "A fast and simple algorithm for training neural probabilistic language models",
      "author" : [ "Andriy Mnih", "Yee Whye Teh" ],
      "venue" : "In ICML. icml.cc / Omnipress,",
      "citeRegEx" : "Mnih and Teh.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mnih and Teh.",
      "year" : 2012
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Cicero D. Santos", "Bianca Zadrozny" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Santos and Zadrozny.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Continuous space language models",
      "author" : [ "Holger Schwenk" ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "Schwenk.,? \\Q2007\\E",
      "shortCiteRegEx" : "Schwenk.",
      "year" : 2007
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch" ],
      "venue" : "In Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey Hinton" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Decoding with large-scale neural language models improves translation",
      "author" : [ "Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang" ],
      "venue" : "In EMNLP, pp. 1387–1392",
      "citeRegEx" : "Vaswani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2013
    }, {
      "title" : "Readings in Speech Recognition, chapter Phoneme Recognition Using Time-delay Neural Networks, pp. 393–404",
      "author" : [ "Alexander Waibel", "Toshiyuki Hanazawa", "Geofrey Hinton", "Kiyohiro Shikano", "Kevin J. Lang" ],
      "venue" : null,
      "citeRegEx" : "Waibel et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Waibel et al\\.",
      "year" : 1990
    }, {
      "title" : "Text understanding from scratch",
      "author" : [ "Xiang Zhang", "Yann LeCun" ],
      "venue" : "CoRR, abs/1502.01710,",
      "citeRegEx" : "Zhang and LeCun.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang and LeCun.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Most of neural language models, such as n-gram models Bengio et al. (2003) are word based and rely on the definition of a finite vocabulary V .",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Most of neural language models, such as n-gram models Bengio et al. (2003) are word based and rely on the definition of a finite vocabulary V . As a consequence, a Look-up table is associated to V in which each word w ∈ V is mapped to a vector of dE real valued features stored in a matrix L ∈ R|V|∗dE . While this approach has proven successful for a variety of tasks and languages, see for instance Schwenk (2007) in speech recognition and Le et al.",
      "startOffset" : 54,
      "endOffset" : 416
    }, {
      "referenceID" : 3,
      "context" : "Most of neural language models, such as n-gram models Bengio et al. (2003) are word based and rely on the definition of a finite vocabulary V . As a consequence, a Look-up table is associated to V in which each word w ∈ V is mapped to a vector of dE real valued features stored in a matrix L ∈ R|V|∗dE . While this approach has proven successful for a variety of tasks and languages, see for instance Schwenk (2007) in speech recognition and Le et al. (2012); Devlin et al.",
      "startOffset" : 54,
      "endOffset" : 459
    }, {
      "referenceID" : 3,
      "context" : "Most of neural language models, such as n-gram models Bengio et al. (2003) are word based and rely on the definition of a finite vocabulary V . As a consequence, a Look-up table is associated to V in which each word w ∈ V is mapped to a vector of dE real valued features stored in a matrix L ∈ R|V|∗dE . While this approach has proven successful for a variety of tasks and languages, see for instance Schwenk (2007) in speech recognition and Le et al. (2012); Devlin et al. (2014); Bahdanau et al.",
      "startOffset" : 54,
      "endOffset" : 481
    }, {
      "referenceID" : 1,
      "context" : "(2014); Bahdanau et al. (2014) in machine translation, it induces several limitations.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "(2014); Bahdanau et al. (2014) in machine translation, it induces several limitations. For morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data. On the one hand, growing the Look-up table is not a solution, since it would increase the number of parameters without having enough training example for a proper estimation. On the other hand, rare words can be replaced by a special token. Nevertheless, this acts as a word class merging very different words without any distinction and using different word classes to handle outof-vocabulary words Allauzen & Gauvain (2005) does not really solve this issue, since rare words are difficult to classify.",
      "startOffset" : 8,
      "endOffset" : 729
    }, {
      "referenceID" : 1,
      "context" : "(2014); Bahdanau et al. (2014) in machine translation, it induces several limitations. For morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data. On the one hand, growing the Look-up table is not a solution, since it would increase the number of parameters without having enough training example for a proper estimation. On the other hand, rare words can be replaced by a special token. Nevertheless, this acts as a word class merging very different words without any distinction and using different word classes to handle outof-vocabulary words Allauzen & Gauvain (2005) does not really solve this issue, since rare words are difficult to classify. Moreover, for most inflected or agglutinative forms, as well as for compound words, the word structure is overlooked, wasting parameters for modeling forms that could be more efficiently handled by word decomposition. While the use of subword units Botha & Blunsom (2014); Sennrich et al.",
      "startOffset" : 8,
      "endOffset" : 1079
    }, {
      "referenceID" : 1,
      "context" : "(2014); Bahdanau et al. (2014) in machine translation, it induces several limitations. For morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data. On the one hand, growing the Look-up table is not a solution, since it would increase the number of parameters without having enough training example for a proper estimation. On the other hand, rare words can be replaced by a special token. Nevertheless, this acts as a word class merging very different words without any distinction and using different word classes to handle outof-vocabulary words Allauzen & Gauvain (2005) does not really solve this issue, since rare words are difficult to classify. Moreover, for most inflected or agglutinative forms, as well as for compound words, the word structure is overlooked, wasting parameters for modeling forms that could be more efficiently handled by word decomposition. While the use of subword units Botha & Blunsom (2014); Sennrich et al. (2016) could improve the generalization power of such models, it relies on a proper and efficient method to induce these subword units.",
      "startOffset" : 8,
      "endOffset" : 1103
    }, {
      "referenceID" : 1,
      "context" : "(2014); Bahdanau et al. (2014) in machine translation, it induces several limitations. For morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data. On the one hand, growing the Look-up table is not a solution, since it would increase the number of parameters without having enough training example for a proper estimation. On the other hand, rare words can be replaced by a special token. Nevertheless, this acts as a word class merging very different words without any distinction and using different word classes to handle outof-vocabulary words Allauzen & Gauvain (2005) does not really solve this issue, since rare words are difficult to classify. Moreover, for most inflected or agglutinative forms, as well as for compound words, the word structure is overlooked, wasting parameters for modeling forms that could be more efficiently handled by word decomposition. While the use of subword units Botha & Blunsom (2014); Sennrich et al. (2016) could improve the generalization power of such models, it relies on a proper and efficient method to induce these subword units. To overcome these issues, we propose to investigate a word based language model with an open vocabulary. Since most of existing models and training criteria rely on the assumption of a finite vocabulary, the definition of an open vocabulary model, along with a training criterion, constitutes a scientific challenge. Our goal is to build word representations every words. Word representations are inferred on-the-fly from its character sequence, using convolution filters which implicitly capture subword patterns, as described in section 2. The architecture is based on a neural ngram model inspired from Bengio et al. (2003), while this idea can be extended to other kind of models.",
      "startOffset" : 8,
      "endOffset" : 1859
    }, {
      "referenceID" : 1,
      "context" : "(2014); Bahdanau et al. (2014) in machine translation, it induces several limitations. For morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data. On the one hand, growing the Look-up table is not a solution, since it would increase the number of parameters without having enough training example for a proper estimation. On the other hand, rare words can be replaced by a special token. Nevertheless, this acts as a word class merging very different words without any distinction and using different word classes to handle outof-vocabulary words Allauzen & Gauvain (2005) does not really solve this issue, since rare words are difficult to classify. Moreover, for most inflected or agglutinative forms, as well as for compound words, the word structure is overlooked, wasting parameters for modeling forms that could be more efficiently handled by word decomposition. While the use of subword units Botha & Blunsom (2014); Sennrich et al. (2016) could improve the generalization power of such models, it relies on a proper and efficient method to induce these subword units. To overcome these issues, we propose to investigate a word based language model with an open vocabulary. Since most of existing models and training criteria rely on the assumption of a finite vocabulary, the definition of an open vocabulary model, along with a training criterion, constitutes a scientific challenge. Our goal is to build word representations every words. Word representations are inferred on-the-fly from its character sequence, using convolution filters which implicitly capture subword patterns, as described in section 2. The architecture is based on a neural ngram model inspired from Bengio et al. (2003), while this idea can be extended to other kind of models. By relaxing the normalized constraint, the objective function borrows from the noise contrastive estimation Gutmann & Hyvärinen (2012) to allow our model to consider a possibly infinite vocabulary.",
      "startOffset" : 8,
      "endOffset" : 2052
    }, {
      "referenceID" : 31,
      "context" : "To infer a word embedding from its character embeddings, we use a convolution layer Waibel et al. (1990); Collobert et al.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "(1990); Collobert et al. (2011), similar to layers used in Santos & Zadrozny (2014); Kim et al.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "(1990); Collobert et al. (2011), similar to layers used in Santos & Zadrozny (2014); Kim et al.",
      "startOffset" : 8,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "(1990); Collobert et al. (2011), similar to layers used in Santos & Zadrozny (2014); Kim et al. (2015). As illustrated in figure 1, a word w is a character sequence {c1, .",
      "startOffset" : 8,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "To circumvent this limitation, in a similar fashion as the method described in Marie et al. (2015), each noun, pronoun and adjective is replaced in the training corpora by its lemma along with some morphological features.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Our baseline is built with a Statistical Machine Translation system based on bilingual n-grams, NCODE3, described in Crego et al. (2011). We follow the same setup as in Marie et al.",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "Our baseline is built with a Statistical Machine Translation system based on bilingual n-grams, NCODE3, described in Crego et al. (2011). We follow the same setup as in Marie et al. (2015).",
      "startOffset" : 117,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "The use of Adagrad Duchi et al. (2010) clearly helps to solve most of these issues, but adds consequent computation time.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "The use of Adagrad Duchi et al. (2010) clearly helps to solve most of these issues, but adds consequent computation time. Following preliminary results on our work with a similar model on a different task Labeau et al. (2015), we made the choice of not implementing LSTMs to obtain character-level word representations.",
      "startOffset" : 19,
      "endOffset" : 226
    }, {
      "referenceID" : 16,
      "context" : "While considering varying size of character n-grams when building our word representation, as in Kim et al. (2015), would certainly help, it would increase our computation time.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "There is a number of different strategies to efficiently train NNLMs with large vocabularies, such as different types of hierarchical softmax Mnih & Hinton (2009); Le et al. (2011), importance sampling Bengio & Sénécal (2003), and Noise contrastive estimation Gutmann & Hyvärinen (2012); Mnih & Teh (2012).",
      "startOffset" : 164,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "There is a number of different strategies to efficiently train NNLMs with large vocabularies, such as different types of hierarchical softmax Mnih & Hinton (2009); Le et al. (2011), importance sampling Bengio & Sénécal (2003), and Noise contrastive estimation Gutmann & Hyvärinen (2012); Mnih & Teh (2012).",
      "startOffset" : 164,
      "endOffset" : 226
    }, {
      "referenceID" : 11,
      "context" : "There is a number of different strategies to efficiently train NNLMs with large vocabularies, such as different types of hierarchical softmax Mnih & Hinton (2009); Le et al. (2011), importance sampling Bengio & Sénécal (2003), and Noise contrastive estimation Gutmann & Hyvärinen (2012); Mnih & Teh (2012).",
      "startOffset" : 164,
      "endOffset" : 287
    }, {
      "referenceID" : 11,
      "context" : "There is a number of different strategies to efficiently train NNLMs with large vocabularies, such as different types of hierarchical softmax Mnih & Hinton (2009); Le et al. (2011), importance sampling Bengio & Sénécal (2003), and Noise contrastive estimation Gutmann & Hyvärinen (2012); Mnih & Teh (2012). Vaswani et al.",
      "startOffset" : 164,
      "endOffset" : 306
    }, {
      "referenceID" : 11,
      "context" : "There is a number of different strategies to efficiently train NNLMs with large vocabularies, such as different types of hierarchical softmax Mnih & Hinton (2009); Le et al. (2011), importance sampling Bengio & Sénécal (2003), and Noise contrastive estimation Gutmann & Hyvärinen (2012); Mnih & Teh (2012). Vaswani et al. (2013) has showed the interest of training a NLM with NCE to re-rank k-best lists, while Devlin et al.",
      "startOffset" : 164,
      "endOffset" : 329
    }, {
      "referenceID" : 8,
      "context" : "(2013) has showed the interest of training a NLM with NCE to re-rank k-best lists, while Devlin et al. (2014) uses a self-normalization.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "Recently, a comparative study Chen et al. (2016) has been made on how to deal with a large vocabulary.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Recently, a comparative study Chen et al. (2016) has been made on how to deal with a large vocabulary. However, the purpose of this paper is to explore models with open vocabulary rather large vocabulary. There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al.",
      "startOffset" : 30,
      "endOffset" : 366
    }, {
      "referenceID" : 5,
      "context" : "Recently, a comparative study Chen et al. (2016) has been made on how to deal with a large vocabulary. However, the purpose of this paper is to explore models with open vocabulary rather large vocabulary. There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al.",
      "startOffset" : 30,
      "endOffset" : 408
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al.",
      "startOffset" : 212,
      "endOffset" : 238
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al.",
      "startOffset" : 212,
      "endOffset" : 285
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al.",
      "startOffset" : 212,
      "endOffset" : 444
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al.",
      "startOffset" : 212,
      "endOffset" : 522
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al.",
      "startOffset" : 212,
      "endOffset" : 537
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al.",
      "startOffset" : 212,
      "endOffset" : 585
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging.",
      "startOffset" : 212,
      "endOffset" : 615
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging. The recent work of Kim et al. (2015), that uses convolutional networks and pooling to construct a word representation from character n-grams, coupled with highway networks Srivastava et al.",
      "startOffset" : 212,
      "endOffset" : 784
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging. The recent work of Kim et al. (2015), that uses convolutional networks and pooling to construct a word representation from character n-grams, coupled with highway networks Srivastava et al. (2015), showed on various languages that using characters improves results on the language modeling task (for a small corpus), even more so for languages with complex morphology.",
      "startOffset" : 212,
      "endOffset" : 944
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging. The recent work of Kim et al. (2015), that uses convolutional networks and pooling to construct a word representation from character n-grams, coupled with highway networks Srivastava et al. (2015), showed on various languages that using characters improves results on the language modeling task (for a small corpus), even more so for languages with complex morphology. A similar architecture was used Józefowicz et al. (2016) on a larger dataset, conjointly with bi-directional LSTMs, and trained with importance sampling, showing great results.",
      "startOffset" : 212,
      "endOffset" : 1173
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging. The recent work of Kim et al. (2015), that uses convolutional networks and pooling to construct a word representation from character n-grams, coupled with highway networks Srivastava et al. (2015), showed on various languages that using characters improves results on the language modeling task (for a small corpus), even more so for languages with complex morphology. A similar architecture was used Józefowicz et al. (2016) on a larger dataset, conjointly with bi-directional LSTMs, and trained with importance sampling, showing great results. On the study of NNLMs in the context of Machine Translation, we can mention the work of Luong et al. (2015) on the effect of the number of layers on reranking n-best lists.",
      "startOffset" : 212,
      "endOffset" : 1401
    }, {
      "referenceID" : 2,
      "context" : "There is a surge of interest into using character-level information for a wide range of NLP tasks, with improved results in POS Tagging Santos & Zadrozny (2014), Text classification Zhang & LeCun (2015), Parsing Ballesteros et al. (2015), Named entity recognition Lample et al. (2016). In language modeling, first applications to language modeling were strictly using characters, and performed less than word-level models Mikolov et al. (2012), while showing impressive results for text generation Sutskever et al. (2011); Graves (2013), using bi-directional LSTM Graves et al. (2013). Recently, Ling et al. (2015) has used bi-directional LSTM to build word representations from characters, with improvements in language modeling and POS-tagging. The recent work of Kim et al. (2015), that uses convolutional networks and pooling to construct a word representation from character n-grams, coupled with highway networks Srivastava et al. (2015), showed on various languages that using characters improves results on the language modeling task (for a small corpus), even more so for languages with complex morphology. A similar architecture was used Józefowicz et al. (2016) on a larger dataset, conjointly with bi-directional LSTMs, and trained with importance sampling, showing great results. On the study of NNLMs in the context of Machine Translation, we can mention the work of Luong et al. (2015) on the effect of the number of layers on reranking n-best lists. Finally, while not directly related to our work, Luong & Manning (2016) very recently showed great improvements on a translation task by handling rare words with character-level recurrent networks, with a neural translation model.",
      "startOffset" : 212,
      "endOffset" : 1538
    } ],
    "year" : 2016,
    "abstractText" : "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the NoiseContrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.",
    "creator" : "LaTeX with hyperref package"
  }
}