{
  "name" : "512.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NONPARAMETRICALLY LEARNING ACTIVATION FUNCTIONS IN DEEP NEURAL NETS",
    "authors" : [ "Carson Eisenach", "Han Liu", "Zhaoran Wang" ],
    "emails" : [ "eisenach@princeton.edu", "hanliu@princeton.edu", "zhaoran@princeton.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep learning techniques have proven particularly useful in the classification setting, recently surpassing even human performance on some image-classification tasks. We seek to advance the state of the art by learning not only weights between neurons in the network but also part of the network structure itself – the activation functions. Current deep learning literature largely focuses on improving architectures and adding regularization to the training process.\nBy contrast, the realm of learning the network structure itself is relatively unexplored. Current best practice often treats network size, shape and choice of activation function as a hyper-parameter to be chosen empirically. Instead, we propose to learn the activation functions through nonparametric estimation. We introduce a class of nonparametric models for activation functions. Importantly, we ensure that our technique can be incorporated into the back-propagation framework. This is crucial because it means our method can be easily added to current practice. To this end, we propose learning functions via basis expansion. In particular, we find that using a Fourier basis works well in practice and offers improved performance over the baseline on several benchmark datasets. We see relative improvements in test error rates of up to 15%. Nonparametric activation functions in dropout nets are especially successful.\nWe also introduce a two-stage training process. First, a network without nonparametric activation functions is trained. Then, the learned network is used as an initialization for an identical network with nonparametric activation functions. This method of initializing the network yields considerable improvements in performance for convolution neural networks on image classification tasks.\nLastly, we consider the algorithmic stability approach to accessing generalization bounds. We use this to demonstrate that feed-forward networks with our method of nonparametrically estimating activation functions generalize well.\nTo summarize, our contributions are the following:\n• A theoretically justified framework for learning activation functions in a neural network,\n• Provable bounds on the generalization error of networks where the activation functions are learned, and\n• An optional two-stage training process that can greatly improve results for certain network architectures, specifically those with convolution layers."
    }, {
      "heading" : "2 RELATED WORKS",
      "text" : "Recent work from Agostinelli et al. (2015) describes piecewise linear activation functions. Theoretically speaking, our approach improves upon this by fully exploring the space of possible activation functions – not merely considering those which are piecewise linear.\nLearning activation functions, in conjunction with the weights in the layer transformations, allows for the fitting of an arbitrary function of the output of the previous layer. A similar idea is Network in Network, due to Lin et al. (2013). There the authors propose to replace standard convolution layers with what they call mlpconv layers. Traditional convolution layers, if using a sigmoid activation, are essentially learning a generalized linear model of the previous layer’s output. Their innovation is to use a more general nonlinear function estimator as the filter – a multilayer perceptron. Thus, their scheme fits in the back-propagation framework and so is easy to implement. Our approach is different because though we are learning arbitrary functions of the previous layer, our activation function approach can be used in any layer of the network and in networks without convolution layers, which theirs cannot.\nMaclaurin et al. (2015) propose to learn the hyper-parameters of a neural net through reverse gradient descent. This is similar to the work we present here in that they also provide a method for learning parameters of the network."
    }, {
      "heading" : "3 NONPARAMETRIC ACTIVATION FUNCTIONS",
      "text" : "Here we describe our main contribution: the nonparametric activation function model. We also provide justification for a weight tying scheme to use nonparametric activations in conjunction with convolution layers, a regularization which proves powerful empirically (see Section 5)."
    }, {
      "heading" : "3.1 FOURIER BASIS EXPANSION FOR ACTIVATION FUNCTIONS",
      "text" : "For the nonparametric estimation we use a Fourier series basis expansion. One concern, however, with Fourier basis expansion is that function approximation over a fixed interval often exhibits poor tail behavior. To combat that, we fit the approximation over an interval [−L,L] and then use it over truncation of the interval, [−L+T, L+T ]. The designation NPF stands for Nonparametric Fourier Basis Expansion for Activation Functions.\nDefinition 3.1. Given some sample size n, an interval [−L,L] and tail truncation T where 0 ≤ T ≤ L, the activation function NPF(L, T ) is parametrized by (a0, . . . , ak) and (b1, . . . , bk) given by\nf(x) =  a0 + ∑k i=1 ai cos((−L+ T )iπx/L) + bi sin((−L+ T )iπx/L) x < −L+ T a0 + ∑k i=1 ai cos(iπx/L) + bi sin(iπx/L) −L+ T ≤ x ≤ L− T\na0 + ∑k i=1 ai cos((L− T )iπx/L) + bi sin((L− T )iπx/L) x > L− T\nk grows with sample size n and is given by k = dn1/7e.\nWe choose to present basis expansion of arbitrary functions on the interval [−L,L] because it is thought to be advantageous in practice to train neural networks with activation functions that are antisymmetric through the origin (LeCun et al., 1998). Though the nonparametric activation functions learned may not be symmetric about the origin, this approach will hopefully result in them being as close to symmetric as possible.\nEach node in the fully connected layers estimates a potentially different activation function. However, in the convolution layers one activation function is learned per filter. The power of a convolution net is the weight tying scheme in the convolution layer, and we preserve that by only estimating one activation function per filter. In Section 5 the effectiveness of this approach can be seen.\nNPFC(L, T ) is the notation for the nonparametrically estimated activation functions in convolution layers. Figure 1 shows some examples of activation functions learned on the CIFAR-10 dataset."
    }, {
      "heading" : "3.2 TRAINING WITH LEARNED ACTIVATION UNITS",
      "text" : "A practical difficulty is that training nonparametric deep nets often proved unstable, specifically for networks with convolution layers. To remedy this our two-stage procedure is given as a generic description in Algorithm 1.\nAlgorithm 1 Generic Two Stage Training for Deep Convolutional Neural Networks Input: A network architecture, hyper parameters T ,L\n1: Instantiate a network with ReLU activations in convolution layers and NPF(T, L) in all others. 2: Run training algorithm of choice. 3: Instantiate a new network with NPFC(T, L) activations in convolution layers and NPF(T, L) in\nall others. Instantiate all weights from the trained network. Instantiate NPFC(T, L) as desired. 4: Run training algorithm of choice. 5: return Network weights resulting from the second training.\nOne point that is not immediately clear is how to initialize NPF(L, T ) activation functions. A good initialization, of any parameter in the network, is crucial to the success of fitting the model. We choose to initialize them to the Fourier series approximation of the tanh function. This works quite well as will be seen in Section 5.\nAlso left to choice is how to perform both stages of training. Of particular interest is the training method used for the second stage. We explored the following two approaches:\n• In stage 2, train only the nonparametric activation functions, holding the remainder of the network constant.\n• In stage 2, train all weights in the network together.\nWe discovered that both methods are successful, but allowing for the activations and weights in the network to be trained together gave slightly better results empirically, so it is those that are reported in Section 5.\nFigure 2a shows the training path for a convolution network on the CIFAR-10 dataset. The network has three convolution layers followed by two fully connected layers. It is initially trained with rectified linear units as activations in the convolution layers and with NPF(4, 1) activations in the fully connected layers. The stage two network, instead, has NPFC(4, 1) activations in the convolution layer.\n0 20 40 60 80 100\nEpochs Trained\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\nE rr\no r\nR a te\nBegin Stage Two\nTest and Training Error Rate on CIFAR-10 for Two Stage Training\nTraining Error\nTest Error\n(a) The training and testing error paths during training. Notice the jump in error rate after stage two begins, but that ultimately it settles into better generalization performance. This is for a convolutional neural net with three convolution and two fully connected layers trained on CIFAR-10.\n0 5000 10000 15000 20000\nNumber of Weight Updates\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\nE rr\no r\nR a te\nTest Error Rate on CIFAR-10\nSingle-Stage Training with NPF(4, 1) Activations\nBaseline CIFAR-10\n(b) Test errors for a convolution neural net trained on CIFAR-10. Each line represents one network trained with random initialization. Five random initializations and training paths for each network type are shown."
    }, {
      "heading" : "4 ALGORITHMIC STABILITY APPROACH TO GENERALIZATION BOUNDS",
      "text" : "For our model to be theoretically sound, its generalization error should vanish with a sufficiently large sample size. This is true for standard neural networks, and such generalization bounds are well known (for example via VC bounds (Bartlett & Anthony, 1999)). However, because we expand the function class, we can not naively apply the traditional results. Instead of the VC bound route, we take the algorithmic stability approach.\nIn recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from X×Y , the space of data and labels. A is an algorithm and D is the training data. Denote w = A(D) as the parameters of the model learned by the algorithm A. We take the following definition from Hardt et al. (2015).\nDefinition 4.1. An algorithm is -uniformly stable if for all data sets D,D′ ∈ (X × Y )n such that D,D′ differ in at most one example, we have\nsup z\nEA[f(A(D), z)− f(A(D′), z)] ≤ .\nThe expectation is taken with respect to any randomness present in the algorithm itself. By stab(A,n) be the infimum over all for which the statement holds.\nBoth Bousquet & Elisseeff (2002); Hardt et al. (2015) present Theorem 4.2 regarding the generalization ability of -uniformly stable algorithms is given\nTheorem 4.2. Take some algorithm -uniformly stable algorithm A. Then |ED,A[RD[A(D)]−R[A(D)]]| ≤ .\nThe notation RD[w] signifies the empirical risk when the loss function depends on w in addition to z. Likewise for the notation RD[A(D)]. Lastly, we have Theorem 4.5 also from Hardt et al. (2015) that will allow us to derive a generalization bound for our model trained with stochastic gradient descent.\nDefinition 4.3. A function f is L-Lipschitz if for all points u, v ∈ dom(f), |f(u)− f(v)| ≤ L||u− v||.\nDefinition 4.4. A function f : Ω→ R is β-smooth if for all u, v ∈ Ω, ||∇f(u)−∇f(v)|| ≤ β||u− v||.\nTheorem 4.5. Assume that f(,̇z) ∈ [0, 1] is an L−Lipschitz and β-smooth loss function for every z. Suppose that we run SGM for T steps with monotonically non increasing step sizes αt ≤ c/t. Then SGM has uniform stability with\nstab ≤ 1 + 1/βc\nn− 1 (2cL2)\n1 βc+1T βc βc+1 .\nIn particular, up to some constant factors, the following relationship approximately holds\nstab ≤ T 1−1/(βc+1)\nn .\n(Hardt et al., 2015)\nIn order to apply the theorem, we demonstrate our network architecture and choice of loss function meets this requirement. Once -stability is established, generalization follows by Theorem 4.2.\n4.1 FEED-FORWARD NETWORKS WITH NPF(T, L) ACTIVATIONS\nFor the full proofs of results in this section, see Appendix A. Assume a network architecture of Q fully connected hidden layers (affine transformation layers) fed into a softmax layer which classifies into one of c classes. The weight vector for the network w should be viewed as the all parameters of the affine transformations and activation functions.\nThe loss function is the negative log-likelihood of the network output, which is a softmax over c classes. We treat the softmax layer as two layers for the purposes of demonstrating the generalization\nbound. One layer is an affine transformation of the output of the lower layers in the network to c nodes. We will denote the result of this computation at each node i in the affine transformation layer within the softmax layer as gQ+1(w, x)i. The second layer maps the outputs at these c nodes to c nodes – this will be the network output. It computes at the the ith node\noi(w, x) = exp gQ+1(w, x)i∑c i=1 exp gQ+1(w, x)i .\nThus, on some training example (x, y), the negative log-likelihood of the network output is\nf(w, (x, y)) = − log(oi(w, x)) = −gQ+1(w, x)y + log ( c∑ i=1 exp gQ+1(w, x)c ) .\nIf we can demonstrate that this f has the desired properties, then the generalization bound given in Section 4 will apply.\nOne point to note is that in order to control the gradients of the various transformations in the layers in the network, we need to control the norm of the weight matrices W and biases b in the affine layers and the coefficients of the activations NPF(L, T ). This is a common technique and is compatible with the framework in Hardt et al. (2015), as it only makes updates less expansive. Note that though f does not have range [0, 1], it is bounded due to the max-norm constraints and can be rescaled. Some smoothing is assumed in order achieve the bound, but in practice the smoothing is not a concern as it can be assumed to take place in an -neighborhood of the transition from one piece of the activation function to another. The smoothed nonparametric activation functions are denoted NPF (L, T ). Recall that k = dn1/7e. Theorem 4.6. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then there exists some K such that the loss function as given in Section 4.1 is K-Lipschitz. Specifically, for any such network where the number of hidden layers Q ≥ 1 and there are p nodes per layer,\nK = max { O (√ Q(pQ+1k3Q+p(2Q+1)/2k3Q−1L) Q ) ,O (√ Q(pQ+1k3Q+p(2Q+1)/2k3Q−1L) QLQ )} .\nProof. The full proof is in Appendix A. The proof idea is to bound the first partial derivatives of the loss function f with respect to each parameter in the network. Then we can proceed recursively down through the network to obtain a bound. From there the gradient can be bounded, implying the loss function f is Lipschitz.\nTheorem 4.7. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then there exists some K such that the loss function as given in Section 4.1 is β-smooth. Specifically, for any such network where the number of hidden layers Q ≥ 1 and there are p nodes per layer,\nβ = max { O ( Q p3Q+5/2k9Q−2\nL2Q 3Q−1\n) ,O ( Q p2 Q+Q+2k5×2 Q+3QL2 Q−1+1\n2Q+Q\n)} .\nProof. The full proof is in Appendix A. The proof idea is to bound the second partial derivatives of f with respect to each of the parameters in the network. Then we can bound |λmax(∇2f)| and thus the largest singular value. This gives an upper bound of β on ||∇2f ||2, implying f is β-smooth.\nTheorem 4.8. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then, for all practical values of L,T , ,Q and p, the resulting Lipschitz constant K and smoothness constant β are sufficiently large that\nstab ≤ T\nn− 1 .\nThus if T = O( √ n), stab → 0 as sample size n→∞.\nProof. By Theorems 4.5, 4.6, and 4.7 the result is immediate."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS",
      "text" : "In this section we provide empirical results for our novel nonparametric deep learning model. The goal of these experiments is to demonstrate its efficacy when used with several different architectures and on several different datasets, not to beat the current best result on either dataset. Crucially, our experiments provide an apples to apples comparison between networks with identical architectures and employing the same regularization techniques where the only difference is choice of activation function. The generalization results from the Section 4 are directly relevant to the experimental results here. In all experiments listed in this section, the reported testing error is an estimation of the generalization error because the final training error was zero in all cases."
    }, {
      "heading" : "5.1 IMPLEMENTATION, REGULARIZATION AND ARCHITECTURES",
      "text" : "To implement the various deep architectures, we use the Python CPU and GPU computing library Theano (Bergstra et al., 2010). We use Theano as an interface to the powerful CUDA and CuDNN libraries (John Nickolls, 2008; Chetlur et al., 2014) enabling fast training for our neural networks. We ran our simulations on Princeton’s SMILE server and TIGER computing cluster. The SMILE server was equipped with a single Tesla K40c GPU, while the TIGER cluster has 200 K20 GPUs.\nWe use early stopping and dropout as regularization techniques (Srivastava et al., 2014; Hardt et al., 2015). The error rate reported for each experiment is the lowest testing error seen during training.\nA common deep network architecture consists 3 stacked convolutional layers with 2 fully connected layers on top (Srivastava et al., 2014; Agostinelli et al., 2015). The convolutional layers have 96, 128, and 256 filters each and each layer has a 5 × 5 filter size that is applied with stride 1 in both directions. The max pooling layers pool 3 × 3 fields and are applied with a stride of 2 in both directions. We use this architecture for the more challenging CIFAR-10 dataset."
    }, {
      "heading" : "5.2 MNIST",
      "text" : "This dataset consists of 60,000 training and 10,000 testing images. Images are 28 × 28 rasterized grayscale images – that is they have one channel. All data points are normalized before training by taking the entire combined dataset, finding the maximum and minimum entry, then centering and rescaling. The dataset is from Lecun & Cortes (1999).\nThe experiments investigate the effects of our techniques on standard multilayer neural networks as well as convolutional neural networks. Specifically, for the standard neural networks we use networks with three fully connected hidden layers with 2048 units in each layer fed into a 10-way softmax.\nThe convolutional neural networks in these experiments have two convolutional layers with 32 and 64 filters respectively. Filters are 5 × 5 and these layers are followed by a max pooling layer with 2×2 pool size. The feed-forward networks consist of 3 fully connected layers with 2048 units each. A mini-batch size of 250 was used for all experiments.\nTable 1 contains the results from the MNIST baseline experiments.\nFor the experiments with NPF(L, T ) activations, we use the same fully connected and convolutional architectures as in the MNIST baseline experiments. Table 2 contains the results of the experiments.\nLearning activation functions via Fourier basis expansion offers sizable improvements on the MNIST dataset as can be seen in Table 2. No experiments were done using our two-stage tech-\nnique, as it proved unnecessary on this comparatively easy to learn dataset. Notice that the weight tying in the NPFC activation makes a considerable difference for the convolution nets, especially when dropout is used in conjunction with learning the activation functions. The biggest improvement is in learning activations for both convolution and fully connected layers and using dropout – a relative 15% improvement over the baseline. Using ReLU activations in the convolution layers with nonparametric activations in the fully connected layers also offered sizable performance improvements over the baseline.\nIn feedforward networks with three fully connected layers there also is an improvement in the test error rate. Here again the improvement is more pronounced with dropout, from 1.35% to 1.10% versus 1.66% to 1.60%."
    }, {
      "heading" : "5.3 CIFAR-10",
      "text" : "The CIFAR-10 dataset is due to Krizhevsky (2009). This dataset consists of 50,000 training and 10,000 test images. The images are three channel color images with dimension 32× 32. Thus each image can be viewed either as a 3× 32× 32 tensor or a vector of length 3072. These images belong to one of ten classes. We apply ZCA whitening and global contrast normalization to the dataset as in Srivastava et al. (2014). The architecture described at the beginning of Section 5 is used for all experiments. For dropout nets we follow Srivastava et al. (2014) and use a dropout of 0.9 on the input, 0.75 in the convolution layers and 0.5 in the fully connected layers. The baseline results can be found in Table 3. Mini-batch sizes between 125 and 250 are used, depending upon memory constraints.\nIn Table 4, one-stage training corresponds to standard back-propagation as in the baseline experiments. The two-stage procedure is the one we introduce to train convolution nets with learned activation functions described in Section 3.2. As can be seen in Table 4, one stage training does not work when learning nonparametric activations in convolution layers – test error is worse than the baseline result. However, by using the two-stage process we see a relative 5% boost in performance without dropout and a relative 9% boost with dropout! In absolute terms we achieve up to a 1.3% improvement in generalization performance using learned activations.\nAlso in Table 4, we can see that we achieved improved performance both with and without dropout when using ReLU activations for convolution layers and NPF(4, 1) for fully connected layers. Furthermore, Figure 2b shows that in terms of number of weight updates required, adding nonparametric activations did not slow training."
    }, {
      "heading" : "6 DISCUSSION AND CONCLUSIONS",
      "text" : "As can be seen in Section 5, nonparametric activation functions offer a meaningful improvement in the generalization performance of deep neural nets in practice. We achieve relative improvements in performance of up to 15% and absolute improvements of up to 1.3% on two benchmark datasets. Equally importantly, networks with the activation functions NPF(L, T ) and NPFC(L, T ) can be trained as fast as their baseline comparison. To realize these improvements on more challenging datasets such as CIFAR-10 we introduced a two-stage training procedure. Our nonparametric activation functions are principled in that they have the necessary properties to guarantee vanishing generalization error in the sense of Bousquet & Elisseeff (2002). Specifically, it was shown in Theorem 4.8, from Section 4.1, that for any feed-forward network architecture using NPF(L, T ) activation functions, we achieve vanishing generalization error as sample size increases.\nOne interesting direction for future work is to investigate why Fourier basis expansion is successful where other methods, such as polynomial basis expansion (which we also explored), were not. Both can be theoretically justified, yet only one works well in practice. Further study of how to eliminate the two-stage process is needed.\nUltimately, we improve upon other approaches of expanding the function class which can be learned by deep neural networks. An important previous work on learning more general function approximations at each node is that of Lin et al. (2013). Their result offers a new type of convolution filter that can learn a broader function class, and in practice their technique also works well. It is limited however in that its use is restricted to convolution layers in convolution neural networks. Ours is applicable to any network architecture and, in Section 5, we demonstrate its success on multiple datasets."
    }, {
      "heading" : "A GENERALIZATION BOUND FOR FULLY CONNECTED NETWORKS WITH FOURIER BASIS ACTIVATION FUNCTIONS",
      "text" : "This is an extension of Section 4.1. For the purposes of this section assume that all data x are vectors in [−1, 1]m, which is permissible because we classify input that is bounded. Furthermore, we assume that the network architecture is a series of Q fully connected hidden layers (affine transformation layers) fed into a softmax layer which classifies into one of c classes. The weight vector for the network w should be viewed as the all parameters of the affine transformations and activation functions.\nThe loss function is the negative log-likelihood of the network output, which is a softmax over c classes. The softmax layer in our framework should be treated as two layers for the purposes of demonstrating the generalization bound. One layer is an affine transformation of the output of the lower layers in the network to c nodes. We will denote the result of this computation at each node i in the affine transformation layer within the softmax layer as gQ+1(w, x)i. The second layer maps the outputs at these c nodes to c nodes – this will be the network output. It computes at the the ith node\noi(w, x) = exp gQ+1(w, x)i∑c i=1 exp gQ+1(w, x)i .\nThus, on some training example (x, y), the negative log-likelihood of the network output is\nf(w, (x, y)) = − log(oi(w, x)) = −gQ+1(w, x)y + log ( c∑ i=1 exp gQ+1(w, x)c ) .\nIf we can demonstrate that this f has the desired properties, then the generalization bound given in Section 4 will apply.\nFor clarity, we refer to network of stacked affine transformation layers as a fully connected network. These layers compute Wx + b, where x is either the input layer (so x ∈ Rm) or it is the output of the previous layer in the network (in which case x ∈ Rp, p being the number of nodes in that layer). Regarding notation:\n• w is a vector denoting all the parameters in the network, so it consists of all parameters noted below.\n• Wi denotes a portion of the parameter vector, corresponding to the affine transformation of the pi−1-dimensional output of the (i− 1)th layer to the pi-dimensional input to the ith layer in the network.\n• Wi,j denotes the jth row of the matrix Wi.\n• Wi,j,k refers to the weight of the edge from node j in the (i − 1)th layer to edge c in the ith layer.\n• bi denotes the constant term in the affine transformation from the (i− 1)th layer to the ith layer.\n• bi,j denotes the jth component of the vector bi.\n• ai,j0 , (a i,j 1 , . . . , a i,j k ) and (b i,j 1 , . . . , b i,j k ) denote the parameters of activation function of the\njth node in the ith layer.\nThe 0th layer is the input layer. Let Q be the number of layers in the network; if i = Q+ 1, it refers to weights on edges in the affine transformation contained within the softmax layer.\nIntuitively, our choice of network topology and use of NPF(L, T ) activation function should have the necessary properties to apply Theorem 4.5. The definition of NPF(L, T ) is repeated below for clarity.\nDefinition A.1. Given some sample size n, an interval [−L,L] and tail truncation T where 0 ≤ T ≤ L, the activation function NPF(L, T ) is parametrized by (a0, . . . , ak) and (b1, . . . , bk) given\nby\nf(x) =  a0 + ∑k i=1 ai cos((−L+ T )iπ/L) + bi sin((−L+ T )iπ/L) x < −L+ T a0 + ∑k i=1 ai cos(iπx/L) + bi sin(iπx/L) −L+ T ≤ x ≤ L− T\na0 + ∑k i=1 ai cos((L− T )iπ/L) + bi sin((L− T )iπ/L) x > L− T\nk grows with sample size n and is given by k = dn1/7e.\nWe need to control the norm of the weight matrices W and biases b in the affine transformations. In addition, we also need to control the magnitude of the coefficients in the basis expansion. This does not affect the applicability of the results from Hardt et al. (2015), as it at most reduces the expansivity of the updates.\nFurthermore, for the analysis of their properties we assume a slightly modified version of the activation functions – in an epsilon neighborhood about the points where the pieces of the function connect we smooth their joint by taking a convex combination of the two pieces to smoothly transition from one to the other. To this end define the following functions\nL(x) = a0 + k∑ i=1 ai cos((−L+ T )iπ/L) + bi sin((−L+ T )iπ/L),\nM(x) = a0 + k∑ i=1 ai cos(iπx/L) + bi sin(iπx/L),\nU(x) = a0 + k∑ i=1 ai cos((L− T )iπ/L) + bi sin((L− T )iπ/L).\nfor given L, T, k, (a0, . . . , ak), and (b1, . . . , bk). Then the -neighborhood smoothed nonparametric activation functions are given by Definition A.2.\nDefinition A.2. Given some sample size n, an interval [−L,L] and tail truncation T where 0 ≤ T ≤ L, the activation function NPF (L, T ) is parametrized by (a0, . . . , ak) and (b1, . . . , bk) given by\nf(x) =  L(x) x < −L+ T − [Case 1] x+L−T+ 2 M(x) + −L+T+ −x 2 L(x) −L+ T − ≤ x ≤ −L+ T + [Case 2] M(x) −L+ T + ≤ x ≤ L− T − [Case 3] L−T+ −x 2 M(x) + x−L+T+\n2 U(x) L− T − ≤ x ≤ L− T + [Case 4] U(x) x > L− T + [Case 5]\nk grows with sample size n and is given by k = dn1/7e.\nSince we are concerned with behavior as is made small – because we are approximating NPF(T, L) with NPF (T, L) – assume without loss of generality that ≤ 1.\nA.1 LIPSCHITZ CONSTANT\nWe proceed layer by layer to obtain bounds on the first partial derivatives of the loss function – which will allow us to bound the gradient. Lemma A.3 gives bounds for the first layer. The notation a(w, x) denotes the output of a layer-wide activation transformation. g(w, x) denotes the output of a layer-wide affine transformation.\nLemma A.3. Assume x ∈ [−a, a]m. Now, consider the affine transformation g1(w) = W1x + b1 from Rm → Rp, thus W1 ∈ Rm×p and b ∈ Rp. Denoting by g1(w, x)i = W1,ix + b1,i, it follows that g1 has first partial derivatives bounded by max{1, a}.\nProof. Clearly,∇g1(w, x)i = [1, . . . , 1;x; 0, . . . , 0]. Thus it has first partial derivatives bounded by max{1, a}.\nWhat if we have instead as input to the affine transformation a function of x rather than x itself? Then we have the Lemma A.4.\nLemma A.4. Let ai−1(w, x) ∈ [−a, a]pi−1 , the output of the (i − 1)th layer in the network. Now, consider the affine transformation\ngi(w, x) = Wiai−1(w, x) + bi.\nfrom Rpi−1 → Rpi . bi ∈ Rd. If we have the following assumptions:\n• ai(w, x)q has first partial derivatives bounded by B, and\n• ||Wi,j ||2 ≤ N .\nDenoting by gi(w, x)j = Wi,jai−1(w, x) + bi,j , it follows that gi(w, x)j has first partial derivatives bounded by max{1, a, √ nNL}.\nProof. We need to first examine\n∂gi(w, x)j ∂bi,j = ∂bi,j ∂bi,j + ∂ ∂bi,j ( n∑ q=1 Wi,j,qai−1(w, x)q ) = 1 + 0 = 1.\nNext we have for any m,\n∂gi(w, x)j ∂Wi,j,m = ∂bi,j ∂Wi,j,m +\n∂\n∂Wi,j,m ( n∑ q=1 Wi,j,qai−1(w, x)q )\n= 0 + ai−1(w, x)m + ∑ q 6=m Wi,j,q ∂ai−1(w, x)q ∂Wi,j,m\n= ai−1(w, x)m.\nPartial derivatives with respect to all other parameters from the ith layer are clearly 0. If l > i, partial derivatives with respect to all parameters from the lth layer are clearly 0. If l < i,∣∣∣∣∂gi(w, x)j∂Wl,m,q ∣∣∣∣ = ∣∣∣∣∣ ∂bi,j∂Wl,m,q + ∂∂Wl,m,q ( pi−1∑ r=1 Wi,j,rai−1(w, x)r )∣∣∣∣∣ =\n∣∣∣∣∣0 + pi−1∑ r=1 Wi,j,r ∂ai−1(w, x)r ∂Wl,m,q ∣∣∣∣∣ ≤ N\n√√√√pi−1∑ r=1 ( ∂ai−1(w, x)r ∂Wl,m,q )2 (Cauchy-Schwartz)\n≤ √pi−1NB. Likewise,\n|∂gi(w, x)j ∂bl,q | ≤ √pi−1NB\nand for the partial derivatives with respect to activation function parameters. It follows that the transformations gi(w, x)j have first partial derivatives bounded by max{1, a, √ pi−1NB}.\nLemma A.5 provides a bound on the range of affine transformations of outputs of NPF (T, L) activations.\nLemma A.5. Consider the affine transformation in layer i, gi(w, x). Assume\n• All layers in the network have NPF (L, T ) activations and that coefficients in these activations are bounded by M ,\n• For all i and j, ||Wi,j ||2 ≤ N , and\n• For all i and j, |bi,j | ≤ O.\nThen, for all j, |gi(w, x)j | ≤ √ pi−1(2k + 1)NM +O = O( √ pi−1k).\nProof. Recall that gi(w, x)j = Wi,jai−1(w, x)+bi,j , thus by Cauchy-Schwartz and that im(sin) = im(cos) = [−1, 1],\n|gi(w, x)j | ≤ √ pi−1(2k + 1)NM +O = O( √ pi−1k).\nNext, we bound the component functions of NPF (L, T ). In addition, to derive asymptotic bounds, we assume N ,M , and O are constants as they are part of the training procedure and not the network structure. We are more concerned with how the smoothness and continuity properties change as n,k,L,T ,Q and pi change. Also note that for the activation function NPF (L, T ) to be well defined, it must be that ≤ L. We also assume ≤ 1. Lemma A.6. Assume gi(w, x) ∈ [−a, a]pi and that gi(w, x)j has first partial derivatives bounded by B. Further assume that |ad| ≤ M and define f(w, x) = ad cos ( dπgi(w,x)j\nL\n) . Then f(w, x) has\nfirst partial derivatives bounded by max{dπMBL , 1}.\nProof. Taking the derivative of f with respect to ad yields∣∣∣∣ ∂f∂ad ∣∣∣∣ = ∣∣∣∣cos(dπgi(w, x)jL )∣∣∣∣ ≤ 1. Next if we take the derivative of f with respect to any weight from elsewhere in the network – some z – we get\n∂f ∂z = −dπad L ∂gi(w, x)j ∂z sin\n( dπgi(w, x)j\nL\n) ,\nand therefore that ∣∣∣∣∂f∂z ∣∣∣∣ ≤ ∣∣∣∣dπadL ∂gi(w, x)j∂z ∣∣∣∣ ≤ dπMBL , and so the partial derivatives of f are bounded by max{dπMBL , 1}.\nImmediately from the proof to Lemma A.6 we see that an identical result holds if we replaced cos with sin.\nLemma A.7. Let gi(w, x) ∈ [−a, a]pi−1 and assume\n• gi(w, x)j has first partial derivatives bounded by B,\n• All layers in the network have NPF (L, T ) activations and that coefficients in these activations are bounded by M ,\n• For all i and j, ||Wi,j ||2 ≤ N , and\n• For all i and j, |bi,j | ≤ O.\nConsider the layer-wide activation transformation ai(w, x) where ai(w, x)j is given by a function of the type NPF (L, T ). ai(w, x)j then has first partial derivatives with bound\nB′ = O ( max {√ pi−1k 3B + k2L\nL ,\n√ pi−1k 3B + k2L }) .\nProof. To bound the partial derivatives, first consider Cases 1 and 5, clearly the derivative with respect to some parameter z is 0. For Case 3, by Lemma A.6,∣∣∣∣∂M(gi(w, x)j)∂z ∣∣∣∣ ≤ (k2 + k)MBπL + (k2 + k) = (k2 + k)(MB + L)πL = O ( k2 + k2B L ) .\nNext consider Case 2, we have ∂\n∂z\n( gi(w, x)j + L− T +\n2 M(gi(w, x)j) + −L+ T + − gi(w, x)j 2 L(gi(w, x)j) ) = ∂\n∂z\n( gi(w, x)j + L− T +\n2 M(gi(w, x)j)\n) − ∂gi(w, x)j\n∂z\n1\n2 L(gi(w, x)j)\n= 1\n2 ∂gi(w, x)j ∂z M(gi(w, x)j) +\n( gi(w, x)j + L− T +\n2\n) ∂M(gi(w, x)j)\n∂z − ∂gi(w, x)j ∂z 1 2 L(gi(w, x)j)\n=\n( M(gi(w, x)j)− L(gi(w, x)j)\n2\n) ∂gi(w, x)j\n∂z +\n( gi(w, x)j + L− T +\n2\n) ∂M(gi(w, x)j)\n∂z .\nBoth M(x) and L(x) are trivially bounded above by (k2 + k + 1)M . Therefore∣∣∣∣ ∂∂z ( gi(w, x)j + L− T + 2 M(gi(w, x)j) + −L+ T + − gi(w, x)j 2 L(gi(w, x)j) )∣∣∣∣ = ∣∣∣∣(M(gi(w, x)j)− L(gi(w, x)j)2 ) ∂gi(w, x)j ∂z + ( gi(w, x)j + L− T + 2 ) ∂M(gi(w, x)j) ∂z\n∣∣∣∣ ≤ (k 2 + k + 1)BM + (√ pi−1(2k + 1)NM +O + L+\n2\n)( (k2 + k)(MB + L)π\nL ) = O ( max {√ pi−1k 3B + k2L\nL ,\n√ pi−1k 3B + k2L }) .\nBy symmetry, we have the same bound for Case 4. Thus overall, the bound on the first partial derivatives of ai(w, x)j is\nO(max {√ pi−1k 3B + k2L\nL ,\n√ pi−1k 3B + k2L } ),\nas for all values of k,B,L, and pi−1 the bound for Cases 2 and 4 dominates the bound for Case 3.\nFinally, we turn to the top layer of the network.\nLemma A.8. With the notation established in Section A, assume there are c classes, the network has Q layers of affine transformations, and each gL(w, x)j has first partial derivatives bounded by B. Then the loss function f is 2B √ (pl + 1)c+ ∑Q i=1 pi(pi−1 + 1)−Lipschitz. If all layers have\np nodes, then f is 2B √\n(Q− 1)p2 + (c+m+Q)p+ c-Lipschitz. In asymptotic notation, it is K-Lipschitz with K = O(Bp √ Q).\nProof. Consider the partial derivative of f with respect to an arbitrary parameter z\n∂f ∂z = ∂gL(w, x)y ∂z + 1∑c j=1 exp(gL(w, x)j) c∑ j=1 ∂gL(w, x)j ∂z exp(gL(w, x)j).\nBy the assumption |∂gL(w,x)y∂z | ≤ Q. Therefore the following holds∣∣∣∣∂f∂z ∣∣∣∣ = ∣∣∣∣∣∣∂gL(w, x)y∂z + 1∑cj=1 exp(gL(w, x)j) c∑ j=1 ∂gL(w, x)j ∂z exp(gj) ∣∣∣∣∣∣ ≤ ∣∣∣∣∂gL(w, x)y∂z ∣∣∣∣+ ∣∣∣∣∣∣ 1∑cj=1 exp(gL(w, x)j) k∑ j=1 ∂gL(w, x)j ∂z exp(gL(w, x)j)\n∣∣∣∣∣∣ ≤ B + ∣∣∣∣maxj ∂gL(w, x)j∂z ∣∣∣∣\n≤ 2B. If layer i has pi nodes there are pi × pi−1 + pi = pi(pi−1 + 1) parameters corresponding to the affine transformation from layer i − 1 to layer i. Recall layer 0 is the input layer, so p0 = m.\nFurthermore, the affine transformation in the softmax has pl × c + c parameters. Thus the total number of parameters in the network is\n(pl + 1)c+ Q∑ i=1 pi(pi−1 + 1),\nwhich implies that f is 2B √ (pl + 1)c+ ∑Q i=1 pi(pi−1 + 1)−Lipschitz. If all layers have the same number of nodes, then the total number of parameters in the network is (Q− 1)p2 + (c+m+Q)p+ c\nand so in this case f is 2B √\n(Q− 1)p2 + (c+m+Q)p+ c-Lipschitz. In asymptotic notation, it is K-Lipschitz with K = O(Bp √ Q).\nCombining all the results above we have Theorem A.9.\nTheorem A.9. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then there exists some K such that the loss function as given in Section A is K-Lipschitz. Specifically, for any such network, where the number of hidden layers Q ≥ 1 and there are p nodes per layer,\nK = max { O (√ Q(pQ+1k3Q + p(2Q+1)/2k3Q−1L)\nQ\n) ,O (√ Q(pQ+1k3Q + p(2Q+1)/2k3Q−1L)\nQLQ\n)} .\nProof. By Lemmas A.3, A.4, A.7 and A.8 such an K must exist. Note that the output of a NPF (T, L) activation is in [−(k2 +k+ 1)N, (k2 +k+ 1)N ]. Then, the computation is as follows, using the aforementioned theorems:\n• g1 – B = 1 • a1 – B = max { O (√ pk3+k2L ) ,O (√ pk3+k2L\nL )} • g2 – B = max { O ( pk3+ √ pk2L ) ,O ( pk3+ √ pk2L\nL )} • a2 – B = max { O ( p3/2k6+pk5L 2 ) ,O ( p3/2k6+pk5L\n2L2 )} • g3 – B = max { O ( p2k6+p3/2k5L 2 ) ,O ( p2k6+p3/2k5L\n2L2 )} • a3 – B = max { O ( p5/2k9+p2k8L 3 ) ,O ( p5/2k9+p2k8L\n3L3 )} Above the transformations are listed with the bounds on their first partial derivatives. A simple inductive argument yields for i ≥ 2\n• gi –\nB = max { O ( pi−1k3(i−1) + p(2i−3)/2k3i−4L\ni−1\n) ,O ( pi−1k3(i−1) + p(2i−3)/2k3i−4L\ni−1Li−1 )} • ai –\nB = max { O ( p(2i−1)/2k3i + pi−1k3i−1\ni\n) ,O ( p(2i−1)/2k3i + pi−1k3i−1\niLi )} Using Lemma A.8 gives\nK = max { O (√ Q(pQ+1k3Q + p(2Q+1)/2k3Q−1L)\nQ\n) ,O (√ Q(pQ+1k3Q + p(2Q+1)/2k3Q−1L)\nQLQ\n)} .\nconcluding the proof.\nA.2 β-SMOOTHNESS\nTo derive the smoothness constant, we begin at the top of the network and work down, bounding partial second derivatives of transformations in the network. Lemma A.10. With the notation established in Section A, assume there are c classes and each gj has first partial derivatives bounded by B1 and second partial derivatives bounded by B2. Then the loss function f is 2((pl + 1)c+ ∑Q i=1 pi(pi−1 + 1))(B 2 1 +B2)-smooth. If all layers have p nodes, then f is 2((Q− 1)p2 + (c+m+Q)p+ c)(B21 +B2)-smooth.\nProof. We begin by recalling from the proof of Lemma A.8 that for some arbitrary parameter z,\n∂f ∂z = ∂gL(w, x)y ∂z + 1∑c l=1 exp(gj) c∑ l=1 ∂gL(w, x)l ∂z exp(gL(w, x)l).\nTherefore differentiating again with respect to another parameter x yields\n∂2f ∂z∂x = ∂2gL(w, x)y ∂z∂x − [ c∑ l=1 ∂gL(w, x)l ∂z exp(gL(w, x)l) ][ c∑ l=1 ∂gL(w, x)l ∂x exp(gL(w, x)l) ] ×\n[ c∑ l=1 exp(gL(w, x)l) ]−2 + [ c∑ l=1 ( ∂2gL(w, x)l ∂z∂x + ∂gL(w, x)l ∂z ∂gL(w, x)l ∂x ) exp(gL(w, x)l) ] ×\n[ c∑ l=1 exp(gL(w, x)l) ]−1 .\nIf we take the absolute value of both sides we can see that∣∣∣∣ ∂2f∂z∂x ∣∣∣∣ ≤ 2B21 + 2B2.\nAnd therefore by Theorem C.2 it follows that\n|λmax(∇2f)| ≤ 2((pl + 1)c+ Q∑ i=1 pi(pi−1 + 1))(B 2 1 +B2).\nBecause∇2f is symmetric, its singular values are just the absolute values of its eigenvalues. Therefore ||∇2f ||2 ≤ 2n(B21 + B2). By Lemma D.2, we have that f is 2(2(pl + 1)c+ ∑Q i=1 pi(pi−1 + 1))(B21 +B2)-smooth. If all layers have the same number of nodes, then |λmax(∇2f)| ≤ 2((Q− 1)p2 + (c+m+Q)p+ c)(B21 +B2) and so f is 2((Q− 1)p2 + (c+m+Q)p+ c)(B21 +B2)-smooth.\nNext, it is necessary to control the second partial derivatives of the affine transformations as the first derivatives are bounded in Lemma A.4. This leads to Lemma A.11. Lemma A.11. Let ai−1(w, x) ∈ [−a, a]pi−1 , the output of the (i− 1)th layer in the network. Now, consider the affine transformation\ngi(w, x) = Wiai−1(w, x) + bi,\nfrom Rpi−1 → Rpi . bi ∈ Rd. If we have the following assumptions:\n• ai(w, x)q has first partial derivatives bounded by B1,\n• ai(w, x)q has second partial derivatives bounded by B2, and\n• ||Wi,j ||2 ≤ N .\nDenoting by gi(w, x)j = Wi,jai−1(w, x) + bi,j , it follows that gi(w, x)j has second partial derivatives bounded by max{B1, N √ pi−1B2}.\nProof. From the proof of Lemma A.4 we have that ∂gi(w, x)j ∂bi,j = 1\nand for any m ∂gi(w, x)j ∂Wi,j,m = ai−1(w, x)m.\nFor some m 6= j and any q, ∂gi(w, x)j ∂Wi,m,q = 0.\nAnd, lastly, that the partial derivatives with respect to any parameter z in layer l < i\n∂gi(w, x)j ∂z = pi−1∑ q=1 Wi,j,q ∂ai−1(w, x)q ∂z .\nWe see then that for the first and third cases the second partial derivatives are zero. For the second case above, the second partial derivatives are either 0 or bounded by B1. What remains then is to check the fourth case. If we differentiate with respect to bi,j ,\n∂2gi(w, x)j ∂z∂bi,j = 0.\nIf we differentiate with respect to Wi,m,q , for m 6= j, ∂2gi(w, x)j ∂z∂Wi,m,q = 0.\nIf we differentiate with respect to Wi,j,q , ∂2gi(w, x)j ∂z∂Wi,j,q = ∂ai−1(w, x)q ∂z . Thus the only remaining case is if we differentiate with respect to some parameter z′ from a layer l < i:\n∂2gi(w, x)j ∂z∂z′ = pi−1∑ q=1 ∂ ∂z′ ( Wi,j,q ∂ai−1(w, x)q ∂z )\n= pi−1∑ q=1 Wi,j,q ∂2ai−1(w, x)q ∂z∂z′ .\nIf we take the absolute value of both sides we get that∣∣∣∣∂2gi(w, x)j∂z∂z′ ∣∣∣∣ = ∣∣∣∣∣ pi−1∑ q=1 Wi,j,q ∂2ai−1(w, x)q ∂z∂z′ ∣∣∣∣∣ ≤ N\n√√√√pi−1∑ q=1 ( ∂2ai−1(w, x)q ∂z∂z′ )2 (Cauchy-Schwartz)\n≤ N√pi−1B2 And therefore gi(w, x)j has second partial derivatives bounded by max{B1, N √ pi−1B2}.\nWe also consider affine transformations on the input – although this is covered by Lemma A.11, we can obtain a better bound directly.\nLemma A.12. Assume x ∈ [−a, a]m. Now, consider the affine transformation g1(w) = W1x+ b1 from Rm → Rp, thus W1 ∈ Rm×p and b ∈ Rp. Denoting by g1(w, x)i = W1,ix + b1,i, it follows that g1 has second partial derivatives bounded by 0.\nProof. Recall that from the proof of Lemma A.3 ∇g1(w, x)i = [1, . . . , 1;x; 0, . . . , 0]. Thus its second derivatives are all 0.\nFinally, a bound is needed for the second partial derivatives of the NPF (L, T ) activations.\nLemma A.13. Assume gi(w, x) ∈ [−a, a]pi and that gi(w, x)j has first partial derivatives bounded by B1 and second partial derivatives bounded by B2. Further assume that |ad| ≤ M and define f(w, x) = ad cos ( dπgi(w,x)j\nL\n) . Then f(w, x) has second partial derivatives bounded by\nmax { dπMLB2+d\n2π2MB21 L2 , dπB1 L\n} .\nProof. Recall from the proof of Lemma A.6 that taking the derivative of f(w, x) with respect to ad yields\n∂f(w, x)\n∂ad = cos\n( dπgi(w, x)j\nL\n) ,\nand that if we take the derivative of f(w, x) with respect to any weight from elsewhere in the network – some z – we get\n∂f(w, x) ∂z = −dπad L ∂gi(w, x)j ∂z sin\n( dπgi(w, x)j\nL ) Now, if we take the derivative of ∂f(w,x)∂ad with respect to ad we get 0. If we take the derivative of ∂f(w,x) ∂ad with respect to some z we get\n∂2f(w, x)\n∂ad∂z = − sin\n( dπgi(w, x)j\nL\n)( dπ\nL ∂gi(w, x)j ∂z ) and we can bound this as ∣∣∣∣∂2f(w, x)∂ad∂z\n∣∣∣∣ ≤ dπB1L . Next if we take the derivative of ∂f(w,x)∂z with respect to ad we get the same as above. Lastly we take the derivative of ∂f(w,x)∂z with respect to some z ′ which gives ∂2f(w, x)\n∂z∂z′ = −dπad L ∂2gi(w, x)j ∂z∂z′ sin\n( dπgi(w, x)j\nL\n) −d\n2π2ad L2 ∂gi(w, x)j ∂z ∂gi(w, x)j ∂z′ cos\n( dπgi(w, x)j\nL\n) ,\nwhich we can bound as∣∣∣∣∂2f(w, x)∂z∂z′ ∣∣∣∣ ≤ dπMB2L + d2π2MB21L2 = dπMLB2 + d2π2MB21L2 .\nTherefore the second partial derivatives of f are bounded by max { dπMLB2+d\n2π2MB21 L2 , dπB1 L\n} .\nLemma A.14. Let gi(w, x) ∈ [−a, a]pi−1 and assume\n• gi(w, x)j has first partial derivatives bounded by B1,\n• gi(w, x)j has first partial derivatives bounded by B2,\n• All layers in the network have NPF (L, T ) activations and that coefficients in these activations are bounded by M ,\n• For all i and j, ||Wi,j ||2 ≤ N , and\n• For all i and j, |bi,j | ≤ O.\nConsider the layer-wide activation transformation ai(w, x) where ai(w, x)j is given by a function of the type NPF (L, T ). ai(w, x)j then has second partial derivatives bounded by\nB′ = O ( B22k 2 + B1k 2 + √ pi−1k 3(B1 +B2)\nL + k4B21 L2 + k3B21 L\n) .\nProof. Without loss of generality we can assume that if Bi 6= 0 that Bi ≥ 1, as these are upper bounds. To bound the second partial derivatives, first note that Cases 1 and 5 have partial second\nderivatives of 0 with respect to all parameters in the network. Next, in Case 3 we see from Lemma A.13 that∣∣∣∣∂2M(gi(w, x)j)∂z∂z′ ∣∣∣∣ ≤ 2 k∑ d=1 ( dπMLB2 + d 2π2MB21 L2 + dπB1 L ) =\n(k2 + k)πMLB2 L2 +\n( k3\n3 + k2 2 + k 6 ) π2MB21 L2 + (k2 + k)πB1 L\n= O ( k2(B1 +B2)\nL + k3B21 L2 ) for any two parameters z and z′. Denote\nc1 = L− T + and c2 = −L+ T + . In Case 5 then, we can compute\n∂2ai(w, x)j ∂z∂z′ = ∂ ∂z′\n( ∂gi(w, x)j\n∂z′ M(gi(w, x)j) 2 + ∂M(gi(w, x)j) ∂z′ gi(w, x)j + c1 2\n−∂gi(w, x)j ∂z′ L(gi(w, x)j) 2 ) = ∂2gi(w, x)j ∂z∂z′ M(gi(w, x)j) 2 + ∂gi(w, x)j ∂z′ ∂M(gi(w, x)j) ∂z 1 2\n+ ∂gi(w, x)j\n∂z\n∂M(gi(w, x)j) ∂z′ 1 2 + ∂2M(gi(w, x)j) ∂z∂z′ gi(w, x)j + c1 2\n− ∂ 2gi(w, x)j ∂z∂z′ L(gi(w, x)j)\n2 and then bound it, using B3 and B4 as bounds on the first and second partial derivatives of M , respectively,∣∣∣∣∂2ai(w, x)j∂z∂z′\n∣∣∣∣ ≤ B22 (k2 + k + 1)M + B1B3 +B4 a+ c12 = O ( B22k 2 + B1k 2 + √ pi−1k 3(B1 +B2)\nL + k4B21 L2 + k3B21 L + k2(B1 +B2) L\n+ k3B21 L2 ) = O ( B22k 2 + B1k 2 + √ pi−1k 3(B1 +B2)\nL + k4B21 L2 + k3B21 L\n) .\nSince this bound dominates the one for Case 3, we are done.\nCombining all the results above we have Theorem A.15. Theorem A.15. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then there exists someK such that the loss function as given in Section A is β-smooth. Specifically, for any such network where the number of hidden layers Q ≥ 1 and and there are p nodes per layer,\nβ = max { O ( Q p3Q+5/2k9Q−2\nL2Q 3Q−1\n) ,O ( Q p2 Q+Q+2k5×2 Q+3QL2 Q−1+1\n2Q+Q\n)} .\nProof. By Lemmas A.3, A.4, A.7, A.10, A.11, A.14 and A.12 such a β must exist.\nTo compute this β, the aforementioned results can be used as well as the computations from the proof of Theorem A.9. The computation is as follows, using the aforementioned results,\n• g1 – B = 0 • a1 – B = max { O (√ pk3 L + k4 L2 ) ,O ( k2 + √ pk3 L + k4 L2 )}\n• g2 – B = max { O ( pk3 L + √ pk4 L2 ) ,O (√ pk3 + k2L + pk3 L + √ pk4 L2 )} • a2 – B = max { O ( p2k10 L4 3 ) ,O ( p2k10L2 2\n)} • g3 – B = max { O ( p4k16 L6 5 ) ,O ( p5/2k10L2 2\n)} Above, next to each transformation, is the bound B on its second partial derivatives. The bounds are computed with respect to L ≤ 1 and L > 1, and then the maximum is taken over both cases. By a simple inductive argument for i ≥ 2,\n• gi –\nB = max { O ( p2i−3/2k6i−8\nL2i−2 2i−3\n) ,O ( p2 i−1−1/2k5×2 i−1 L2 i−2\n2i−1\n)}\n• ai –\nB = max { O ( p2i−2k6i−2\nL2i 2i−1\n) ,O ( p2 i−1k5×2 i L2 i−1\n2i\n)}\nUsing the bounds derived above, the bounds from Theorem A.9 and Lemma A.10 gives\nβ = max { O ( Q p3Q+5/2k9Q−2\nL2Q 3Q−1\n) ,O ( Q p2 Q+Q+2k5×2 Q+3QL2 Q−1+1\n2Q+Q\n)} .\nA.3 GENERALIZABILITY\nWith the necessary properties demonstrated, we arrive at Theorem A.16. By p denote the number of nodes per layer and Q the number of hidden layers.\nTheorem A.16. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then, for all practical values of L,T , ,Q and p, the resulting Lipschitz constant K and smoothness constant β are sufficiently large that\nstab ≤ T\nn− 1 .\nThus if T = O( √ n), stab → 0 as sample size n→∞.\nProof. By Theorems 4.5, A.9, and A.15 the result is immediate.\nB RESULTS FOR FUNCTIONS OVER Rn\nWithout proof we give the mean value inequality for vector valued function in Theorem B.1.\nTheorem B.1. Let f be a differentiable vector valued function, that is f : A ⊂ Rn → Rm. Then for x, y ∈ A, we have that\n||f(x)− f(y)|| ≤ ||Df(z)(x− y)||,\nwhere z is some point on the line segment connecting x, y. HereDf represents the Frechet derivative of f , which can be represented in matrix form by the Jacobian."
    }, {
      "heading" : "C RESULTS FOR MATRICES",
      "text" : "When we want to bound the largest eigenvalue of the matrix, we can get a loose bound by the sum of the entries of the matrix as in Theorems C.1 and C.2. Theorem C.1. For some matrix norm || · ||, the spectral radius of the matrix A is upper bounded by ||A||.\nProof. Take the eigenvector corresponding to the largest eigenvalue in magnitude of A. Denote this pair by x, λ. Next define the matrix X = [x| · · · |x]. It follows then that AX = λX . Therefore\n|λ|||X|| = ||λX|| = ||AX|| ≤ ||A||||X|| because matrix norms are sub-multiplicative, demonstrating |λ| ≤ ||A||.\nTheorem C.2. Let A be a symmetric n× n matrix. Let the entries of the matrix be bounded above by some constant k. It follows then that maxk |λk(A)| ≤ nk.\nProof. By Theorem C.1 we can see that\nmax k |λk(A)| ≤ √∑ i,j (Aij)2 ≤ √ n2k2 = nk."
    }, {
      "heading" : "D RESULTS FOR LIPSCHITZ AND SMOOTH FUNCTIONS",
      "text" : "First note that if ∇f(x) is well defined everywhere and if ||∇f(x)|| ≤ L over the entire domain, it is L-Lipschitz by the mean value theorem and an application of Cauchy-Schwartz. In the convex case, the reverse is also true. Lemma D.1. Take g to be a function from Rn → Rd. If g is L-Lipschitz in each of its components, g is √ nL-Lipschitz.\nProof. Consider any x, y ∈ Rn. By the assumption we know that for i = 1, . . . , n |gi(x)−gi(y)|2 ≤ L2||x− y||22. Therefore\nn∑ i=1 |gi(x)− gi(y)|2 ≤ nL2||x− y||22,\nand, thus, the desired result that\n||g(x)− g(y)||2 = √√√√ n∑ i=1 |gi(x)− gi(y)|2 ≤ √ nL||x− y||2.\nDirectly from the definition it is difficult to prove a gradient to be Lipschitz continuous. However we can utilize Lemma D.2. Lemma D.2. For some twice differentiable function f : A ⊂ Rn → R, f is β-smooth if its Hessian is bounded by β with respect to the euclidean norm.\nProof. Take the gradient ∇f which is a mapping A ⊂ Rn → Rn. We can apply the mean value inequality for vector valued functions, Theorem B.1, which gives us\n||∇f(x)−∇f(y)||2 ≤ ||∇2f(z)(x− y)||2, where z is on the line segment connecting x and y. Because of how the induced euclidean norm on a matrix is defined it follows that\n||∇2f(z)(x− y)||2 ≤ ||∇2f(z)||2||x− y||2 ≤ β||x− y||2. Combining the two equations above gives the desired result,\n||∇f(x)−∇f(y)||2 ≤ β||x− y||2.\nThe insight to take this approach to demonstrating β-smoothness is from Bach & Moulines (2011)."
    } ],
    "references" : [ {
      "title" : "Learning activation functions to improve deep neural networks",
      "author" : [ "Agostinelli", "Hoffman", "Sadowski", "Baldi" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Agostinelli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agostinelli et al\\.",
      "year" : 2015
    }, {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "Francis Bach", "Eric Moulines" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Bach and Moulines.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bach and Moulines.",
      "year" : 2011
    }, {
      "title" : "Neural Network Learning: Theoretical Foundations",
      "author" : [ "Peter Bartlett", "Martin. Anthony" ],
      "venue" : null,
      "citeRegEx" : "Bartlett and Anthony.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bartlett and Anthony.",
      "year" : 1999
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "James Bergstra", "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Python for Scientific Computing Conference (SciPy),",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Stability and generalization",
      "author" : [ "Olivier Bousquet", "Andre Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bousquet and Elisseeff.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bousquet and Elisseeff.",
      "year" : 2002
    }, {
      "title" : "cudnn: Efficient primitives for deep learning",
      "author" : [ "Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer" ],
      "venue" : null,
      "citeRegEx" : "Chetlur et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chetlur et al\\.",
      "year" : 2014
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "Moritz Hardt", "Benjamin Recht", "Yoram Singer" ],
      "venue" : "CoRR, abs/1509.01240,",
      "citeRegEx" : "Hardt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2015
    }, {
      "title" : "Scalable parallel programming with cuda",
      "author" : [ "Ian Buck" ],
      "venue" : "ACM Queue,",
      "citeRegEx" : "Nickolls and Buck.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nickolls and Buck.",
      "year" : 2008
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y. LeCun", "L. Bottou", "G. Orr", "K. Muller" ],
      "venue" : "Neural Networks: Tricks of the trade. Springer,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "The MNIST database of handwritten digits",
      "author" : [ "Yann Lecun", "Corinna Cortes" ],
      "venue" : "URL http: //yann.lecun.com/exdb/mnist/",
      "citeRegEx" : "Lecun and Cortes.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lecun and Cortes.",
      "year" : 1999
    }, {
      "title" : "Gradient-based hyperparameter optimization through reversible learning",
      "author" : [ "D. Maclaurin", "D. Duvenaud", "R. Adams" ],
      "venue" : "In JMLR,",
      "citeRegEx" : "Maclaurin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maclaurin et al\\.",
      "year" : 2015
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Hinton", "Krizhevsky", "Sutskever", "Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "cos((L− T )iπ/L) + bi sin((L− T )iπ/L) x > L− T k grows with sample size n and is given by k = dne. We need to control the norm of the weight matrices W and biases b in the affine transformations. In addition, we also need to control the magnitude of the coefficients in the basis expansion",
      "author" : [ "Hardt" ],
      "venue" : null,
      "citeRegEx" : "Hardt,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recent work from Agostinelli et al. (2015) describes piecewise linear activation functions.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Recent work from Agostinelli et al. (2015) describes piecewise linear activation functions. Theoretically speaking, our approach improves upon this by fully exploring the space of possible activation functions – not merely considering those which are piecewise linear. Learning activation functions, in conjunction with the weights in the layer transformations, allows for the fitting of an arbitrary function of the output of the previous layer. A similar idea is Network in Network, due to Lin et al. (2013). There the authors propose to replace standard convolution layers with what they call mlpconv layers.",
      "startOffset" : 17,
      "endOffset" : 510
    }, {
      "referenceID" : 0,
      "context" : "Recent work from Agostinelli et al. (2015) describes piecewise linear activation functions. Theoretically speaking, our approach improves upon this by fully exploring the space of possible activation functions – not merely considering those which are piecewise linear. Learning activation functions, in conjunction with the weights in the layer transformations, allows for the fitting of an arbitrary function of the output of the previous layer. A similar idea is Network in Network, due to Lin et al. (2013). There the authors propose to replace standard convolution layers with what they call mlpconv layers. Traditional convolution layers, if using a sigmoid activation, are essentially learning a generalized linear model of the previous layer’s output. Their innovation is to use a more general nonlinear function estimator as the filter – a multilayer perceptron. Thus, their scheme fits in the back-propagation framework and so is easy to implement. Our approach is different because though we are learning arbitrary functions of the previous layer, our activation function approach can be used in any layer of the network and in networks without convolution layers, which theirs cannot. Maclaurin et al. (2015) propose to learn the hyper-parameters of a neural net through reverse gradient descent.",
      "startOffset" : 17,
      "endOffset" : 1220
    }, {
      "referenceID" : 9,
      "context" : "We choose to present basis expansion of arbitrary functions on the interval [−L,L] because it is thought to be advantageous in practice to train neural networks with activation functions that are antisymmetric through the origin (LeCun et al., 1998).",
      "startOffset" : 229,
      "endOffset" : 249
    }, {
      "referenceID" : 6,
      "context" : "In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from X×Y , the space of data and labels.",
      "startOffset" : 20,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from X×Y , the space of data and labels. A is an algorithm and D is the training data. Denote w = A(D) as the parameters of the model learned by the algorithm A. We take the following definition from Hardt et al. (2015). Definition 4.",
      "startOffset" : 20,
      "endOffset" : 548
    }, {
      "referenceID" : 6,
      "context" : "In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from X×Y , the space of data and labels. A is an algorithm and D is the training data. Denote w = A(D) as the parameters of the model learned by the algorithm A. We take the following definition from Hardt et al. (2015). Definition 4.1. An algorithm is -uniformly stable if for all data sets D,D′ ∈ (X × Y ) such that D,D′ differ in at most one example, we have sup z EA[f(A(D), z)− f(A(D′), z)] ≤ . The expectation is taken with respect to any randomness present in the algorithm itself. By stab(A,n) be the infimum over all for which the statement holds. Both Bousquet & Elisseeff (2002); Hardt et al.",
      "startOffset" : 20,
      "endOffset" : 918
    }, {
      "referenceID" : 6,
      "context" : "In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from X×Y , the space of data and labels. A is an algorithm and D is the training data. Denote w = A(D) as the parameters of the model learned by the algorithm A. We take the following definition from Hardt et al. (2015). Definition 4.1. An algorithm is -uniformly stable if for all data sets D,D′ ∈ (X × Y ) such that D,D′ differ in at most one example, we have sup z EA[f(A(D), z)− f(A(D′), z)] ≤ . The expectation is taken with respect to any randomness present in the algorithm itself. By stab(A,n) be the infimum over all for which the statement holds. Both Bousquet & Elisseeff (2002); Hardt et al. (2015) present Theorem 4.",
      "startOffset" : 20,
      "endOffset" : 939
    }, {
      "referenceID" : 6,
      "context" : "In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from X×Y , the space of data and labels. A is an algorithm and D is the training data. Denote w = A(D) as the parameters of the model learned by the algorithm A. We take the following definition from Hardt et al. (2015). Definition 4.1. An algorithm is -uniformly stable if for all data sets D,D′ ∈ (X × Y ) such that D,D′ differ in at most one example, we have sup z EA[f(A(D), z)− f(A(D′), z)] ≤ . The expectation is taken with respect to any randomness present in the algorithm itself. By stab(A,n) be the infimum over all for which the statement holds. Both Bousquet & Elisseeff (2002); Hardt et al. (2015) present Theorem 4.2 regarding the generalization ability of -uniformly stable algorithms is given Theorem 4.2. Take some algorithm -uniformly stable algorithm A. Then |ED,A[RD[A(D)]−R[A(D)]]| ≤ . The notation RD[w] signifies the empirical risk when the loss function depends on w in addition to z. Likewise for the notation RD[A(D)]. Lastly, we have Theorem 4.5 also from Hardt et al. (2015) that will allow us to derive a generalization bound for our model trained with stochastic gradient descent.",
      "startOffset" : 20,
      "endOffset" : 1331
    }, {
      "referenceID" : 6,
      "context" : "(Hardt et al., 2015)",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "This is a common technique and is compatible with the framework in Hardt et al. (2015), as it only makes updates less expansive.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "1 IMPLEMENTATION, REGULARIZATION AND ARCHITECTURES To implement the various deep architectures, we use the Python CPU and GPU computing library Theano (Bergstra et al., 2010).",
      "startOffset" : 151,
      "endOffset" : 174
    }, {
      "referenceID" : 5,
      "context" : "We use Theano as an interface to the powerful CUDA and CuDNN libraries (John Nickolls, 2008; Chetlur et al., 2014) enabling fast training for our neural networks.",
      "startOffset" : 71,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "We use early stopping and dropout as regularization techniques (Srivastava et al., 2014; Hardt et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "We use early stopping and dropout as regularization techniques (Srivastava et al., 2014; Hardt et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "A common deep network architecture consists 3 stacked convolutional layers with 2 fully connected layers on top (Srivastava et al., 2014; Agostinelli et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "A common deep network architecture consists 3 stacked convolutional layers with 2 fully connected layers on top (Srivastava et al., 2014; Agostinelli et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "The CIFAR-10 dataset is due to Krizhevsky (2009). This dataset consists of 50,000 training and 10,000 test images.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "The CIFAR-10 dataset is due to Krizhevsky (2009). This dataset consists of 50,000 training and 10,000 test images. The images are three channel color images with dimension 32× 32. Thus each image can be viewed either as a 3× 32× 32 tensor or a vector of length 3072. These images belong to one of ten classes. We apply ZCA whitening and global contrast normalization to the dataset as in Srivastava et al. (2014). The architecture described at the beginning of Section 5 is used for all experiments.",
      "startOffset" : 31,
      "endOffset" : 413
    }, {
      "referenceID" : 8,
      "context" : "The CIFAR-10 dataset is due to Krizhevsky (2009). This dataset consists of 50,000 training and 10,000 test images. The images are three channel color images with dimension 32× 32. Thus each image can be viewed either as a 3× 32× 32 tensor or a vector of length 3072. These images belong to one of ten classes. We apply ZCA whitening and global contrast normalization to the dataset as in Srivastava et al. (2014). The architecture described at the beginning of Section 5 is used for all experiments. For dropout nets we follow Srivastava et al. (2014) and use a dropout of 0.",
      "startOffset" : 31,
      "endOffset" : 552
    }, {
      "referenceID" : 6,
      "context" : "This does not affect the applicability of the results from Hardt et al. (2015), as it at most reduces the expansivity of the updates.",
      "startOffset" : 59,
      "endOffset" : 79
    } ],
    "year" : 2016,
    "abstractText" : "We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justification for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.",
    "creator" : "LaTeX with hyperref package"
  }
}