{
  "name" : "424.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "MOLLIFYING NETWORKS", "Caglar Gulcehre", "Marcin Moczulski", "Francesco Visin", "Yoshua Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the last few years, deep neural networks – i.e. convolutional networks (LeCun et al., 1989), LSTMs (Hochreiter & Schmidhuber, 1997a) or GRUs (Cho et al., 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016). However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al., 2014; Dauphin et al., 2014).\nA number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe & Szegedy, 2015), learning with a curriculum (Bengio et al., 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al., 2015). The impact of noise injection on the behavior of modern deep learning methods has been explored by Neelakantan et al. (2015a). Hazan et al. (2015) have shown that injecting a particular noise and scheduling it carefully, can guarantee the convergence in O(1/σ2 2) steps for -optimal and σ-nice functions. Similar to our work graduated optimization optimizes a smoothed objective function without performing expensive convolutions. Injecting noise to the activation functions and scheduling it have been recently shown to improve the performance on a wide variety of tasks (Gulcehre et al., 2016).\nWe connect the ideas of curriculum learning and continuation methods with those arising from models with skip connections and using layers that compute near-identity transformations. Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers. Similarly, it has been shown that stochastically changing the depth of a network during training (Huang et al., 2016b) does not prevent convergence and enables better generalization performance.\nWe discuss the idea of mollification for neural networks – a form of differentiable smoothing of the loss function connected to noisy activations – which in our case can be interpreted as a form of adaptive noise injection which is controlled by a single hyperparameter. Inspired by Huang et al. (2016b), we use a hyperparameter to stochastically control the depth of our network. This allows us to start the optimization from a convex objective function (as long as the optimized criterion is convex, e.g. linear or logistic regression) and to slowly introduce more complexity into the model by annealing the hyperparameter, thus making the network deeper and increasingly non-linear.\n∗ This work was done while these students were interning at the MILA lab in University of Montreal.\nAn important difference of our work compared to injecting noise to the gradients as it is explored in (Hazan et al., 2015; Neelakantan et al., 2015b) is that we inject the noise in the forward computation of the graph and we shape the cost function directly. As a result the cost function for the mollified network both at the test time and during the training are consistent and this makes the early-stopping much easier."
    }, {
      "heading" : "2 MOLLIFYING OBJECTIVE FUNCTIONS",
      "text" : ""
    }, {
      "heading" : "2.1 CONTINUATION AND ANNEALING METHODS",
      "text" : "Continuation methods and simulated annealing provide a general strategy to reduce the impact of local minima and deal with non-convex, continuous, but not necessarily everywhere differentiable objective functions by smoothing the original objective function and gradually reducing the amount of smoothing during training (Allgower & Georg, 1980) (see Fig. 1).\nIn machine learning, approaches based on curriculum learning (Bengio et al., 2009) are inspired by this principle and define a sequence of gradually more difficult training tasks (or training distributions) that eventually converge to the task of interest.\nIn the context of stochastic gradient descent, we use a stochastic estimation of the gradient for the smoothed objective function. This is convenient because it may not be analytically feasible to compute the smoothed function, but a Monte-Carlo estimate can often be obtained easily.\nIn this paper we construct a sequence of smoothed objective functions obtained with a form of mollification and we progressively optimize them. The training procedure iterates over the sequence of objective functions starting from the simpler ones – i.e. with a smoother loss surface – and moving towards more complex ones until the last, original, objective function is reached.1"
    }, {
      "heading" : "2.2 MOLLIFIERS AND WEAK GRADIENTS",
      "text" : "We smooth the loss function L, which is parametrized by θ ∈ Rn, by convolving it with another function K(·) with stride τ ∈ Rn:\nLK(θ) = (L ∗K)(θ) = ∫ +∞ −∞ L(θ − τ )K(τ )dτ (1)\nAlthough there are many choices for the function K(·), we focus on those that satisfy the definition of a mollifier.\nA mollifier is an infinitely differentiable function that behaves like an approximate identity in the group of convolutions of integrable functions. If K(·) is an infinitely differentiable function, that converges to the Dirac delta function when appropriately rescaled and for any integrable function L, then it is a mollifier:\nL(θ) = lim →0\n∫ −nK(τ/ )L(θ − τ )dτ . (2)\n1We plan to release the source code of the models and experiments under, http://github.com/ caglar/molly_nets/.\nIf we choose K(·) to be a mollifier and obtain the smoothed loss function LK as in Eqn. 1, we can take its gradient with respect to θ using directly the result from Evans (1998):\n∇θLK(θ) = ∇θ(L ∗K)(θ) = (L ∗ ∇K)(θ). (3)\nTo relate the resulting gradient ∇θLK to the gradient of the original function L, we introduce the notion of weak gradient, i.e. an extension to the idea of weak/distributional derivatives to functions with multidimensional arguments, such as loss functions of neural networks.\nFor an integrable function L in space L ∈ L([a, b]), g ∈ L([a, b]n) is a n-dimensional weak gradient of L if it satisfies: ∫\nC g(τ )K(τ )dτ = − ∫ C L(τ )∇K(τ )dτ , (4)\nwhere K(τ ) is an infinitely differentiable function vanishing at infinity, C ∈ [a, b]n and τ ∈ Rn.\nAs long as the chosen K(·) fulfills the definition of a mollifier we can use Eqn. 3 and Eqn. 4 2 to rewrite the gradient as:\n∇θLK(θ) = (L ∗ ∇K)(θ) by Eqn. 3 (5)\n= ∫ C L(θ − τ )∇K(τ )dτ (6)\n= − ∫ C g(θ − τ )K(τ )dτ by Eqn. 4 (7)\nFor a differentiable almost everywhere function L, the weak gradient g(θ) is equal to ∇θL almost everywhere. With a slight abuse of notation we can therefore write:\n∇θLK(θ) = − ∫ C ∇θL(θ − τ )K(τ )dτ (8)"
    }, {
      "heading" : "2.3 GAUSSIAN MOLLIFIERS",
      "text" : "It is possible to use the standard Gaussian distribution N (0, I) as a mollifier K(·), as it satisfies the desired properties: it is infinitely differentiable, a sequence of properly rescaled Gaussian distributions converges to the Dirac delta function and it vanishes in infinity. With such a K(·) the gradient becomes:\n∇θLK=N (θ) = − ∫ C ∇θL(θ − τ )p(τ )dτ (9)\n= Eτ [∇θL(θ − τ ) ], with τ ∼ N (0, I) (10)\nExploiting the fact that a Gaussian distribution is a mollifier, we can focus on a sequence of mollifications indexed by scaling parameter introduced in Eqn. 2. A single element of this sequence takes the following form:\n∇θLN , (θ) = − ∫ C ∇θL(θ − τ ) −1p(τ/ )dτ (11)\n= Eτ [∇θL(θ − τ ) ], with τ ∼ N (0, 2I) (12)\nReplacing with σ yields a sequence of mollifications indexed by σ:\n∇θLN ,σ(θ) = Eτ [∇θL(θ − τ ) ], with τ ∼ N (0, σ2I) (13) with the following property (by Eqn. 2):\nlim σ→0 ∇θLN ,σ(θ) = ∇θL(θ) (14)\nAn intuitive interpretation of the result is that σ determines the standard deviation of a mollifying Gaussian and is annealed in order to construct a sequence of gradually less \"blurred\" and closer\n2We omit for brevity the algebraic details involved with a translation of the argument.\napproximations to L. This is consistent with the property that when σ is annealed to zero we are optimizing the original function L. So far we obtained the mollified version LK(θ) of the cost function L(θ) by convolving it with a mollifier K(θ). The kernel K(θ) corresponds to the average effect of injecting noise ξ sampled from standard Normal distribution. The amount of noise controls the amount of smoothing. Gradually reducing the noise during training is related to a form of simulated annealing (Kirkpatrick et al., 1983). Similarly to the analysis in Mobahi (2016), we can write a Monte-Carlo estimate of LK(θ) = (L ∗K)(θ) ≈ 1N ∑N i=1 L(θ − ξ(i)). We provide the derivation and the gradient of this equation in Appendix A.\nThe Monte-Carlo estimators of the mollifiers can be easily implemented with neural networks, where the layers typically have the form:\nhl = f(Wlhl−1) (15)\nwith hl−1 a vector of activations from the previous layer in the hierarchy, Wl a matrix representing a linear transformation and f an element-wise non-linearity of choice.\nA mollification of such a layer can be formulated as:\nhl = f((Wl − ξl)hl−1), where ξl ∼ N (µ, σ2) (16)\nFrom Eqn. 16, it is easy to see that both weight noise methods proposed by Hinton & van Camp (1993) and Graves (2011) can be seen as a variation of Monte-Carlo estimate of mollifiers."
    }, {
      "heading" : "2.4 GENERALIZED AND NOISY MOLLIFIERS",
      "text" : "We introduce a generalization of the concept of mollifiers that encompasses the approach we explored here and that is targeted during optimization via a continuation method using stochastic gradient descent. Definition 2.1. (Generalized Mollifier). A generalized mollifier is an operator, where Tσ(f) defines a mapping between two functions, such that Tσ : f → f∗.\nlim σ→0 Tσf = f, (17)\nf0 = lim σ→∞ Tσf is an identity function (18)\n∂(Tσf)(x)\n∂x exists ∀x, σ > 0 (19)\nIn addition, we consider noisy mollifiers which can be defined as an expected value of a stochastic function φ(x, ξ) under some noise source ξ with variance σ:\n(Tσf)(x) = Eξ[φ(x, ξσ)] (20)\nDefinition 2.2. (Noisy Mollifier). We call a stochastic function φ(x, ξσ) with input x and noise ξ a noisy mollifier if its expected value corresponds to the application of a generalized mollifier Tσ, as per Eqn. 20.\nThe composition of two noisy mollifiers sharing the same σ is also a noisy mollifier, since the three properties in the definition (Eqns. 17,18,19) are still satisfied. When σ = 0 no noise is injected and therefore the original function will be optimized. If σ → ∞ instead, the function will become an identity function. Thus, for instance, if we mollify each layer of a feed-forward network except the output layer, when σ →∞ all the mollified layers will become identity function and the objective function of the network with respect to its inputs will be convex.\nConsequently, corrupting separately the activation function of each level of a deep neural network (but with a shared noise level σ) and annealing σ yields a noisy mollifier for the objective function. This is related to the work of Mobahi (2016), who recently introduced a way of analytically smoothing of the non-linearities to help the training of recurrent networks. The differences of that approach from our algorithm is two-fold: we use a noisy mollifier (rather than an analytic smoothing of the network’s non-linearities) and we introduce (in the next section) a particular form of the noisy mollifier that empirically proved to work well.\nShaping the cost function to define a sequence of costs that are progressing from easier to more difficult ones can be related to the reward shaping (Ng et al., 1999; Ng, 2003) algorithms. In our algorithm, we shape the cost and the model architecture itself, rather than rewards or the targets in order to make the optimization easier. In that sense, reward shaping can be considered to be more closer to curriculum learning."
    }, {
      "heading" : "3 METHOD",
      "text" : "We propose an algorithm to mollify the cost of a neural network which also addresses an important drawback of the previously proposed noisy training procedures: as the noise gets larger, it can dominate the learning process and lead the algorithm to perform a random walk on the energy landscape of the objective function. Conversely in our algorithm, as the noise gets larger gradient descent minimizes a simpler (e.g. convex) but still meaningful objective function.\nWe define the desired behavior of the network in the limit cases where the noise is very large or very small, and modify the model architecture accordingly. Specifically, during training we minimize a sequence of increasingly complex noisy objectives L = (L1(θ; ξσ1),L2(θ; ξσ2), · · · ,Lk(θ; ξσk)) that we obtain by annealing the scale (variance) of the noise σi. Let us note that our algorithm satisfies the fundamental properties of the generalized and noisy mollifiers that we introduced earlier.\nWe use a noisy mollifier based on our definition in Section 2.4. Instead of convolving the objective function with a kernel:\n1. We start the training by optimizing a convex objective function that is obtained by configuring all the layers between the input and the last cost layer to compute an identity function, i.e., by skipping both the affine transformations and the blocks followed by nonlinearities. 2. During training, the magnitude of noise which is proportional to p is annealed, allowing to gradually evolve from identity transformations to linear transformations between the layers. 3. Simultaneously, as we decrease the p, the noisy mollification procedure allows the element-wise activation functions to gradually change from linear to be nonlinear."
    }, {
      "heading" : "4 SIMPLIFYING THE OBJECTIVE FUNCTION FOR FEEDFORWARD NETWORKS",
      "text" : "For every unit of each layer, we either copy the activation (output) of the corresponding unit of the previous layer (the identity path in Figure 2) or output a noisy activation h̃l of a non-linear transformation of it ψ(hl−1, ξ;Wl), where ξ is noise, Wl is a weight matrix applied on hl−1 and π is a vector of binary decisions for each unit (the convolutional path in Figure 2):\nh̃l = ψ(hl−1, ξ;Wl) (21)\nφ(hl−1, ξ,πl;Wl) = πl hl−1 + (1− πl) h̃l (22) hl = φ(hl−1, ξ,πl;Wl). (23)\nTo decide which path to take, for each unit in the network, a binary stochastic decision is taken by drawing from a Bernoulli distribution with probability dependent on the decaying value of pl:\nπl ∼ Bernoulli(pl) (24)\nIf the number of hidden units of layer l−1 and layer l+1 is not the same, we can either zero-pad layer l−1 before feeding it into the next layer or apply a linear projection to obtain the right dimensionality.\nFor pl = 1, the layer computes the identity function leading to a convex objective. If pl = 0 the layer computes the original non-linear transformation unfolding the full capacity of the model. We call the connections that are introduced this way as unitwise stochastic skip connections(USSC).\nIn DropIn layers (Smith et al., 2016) a binary random vector is sampled from a Bernoulli distribution to decide whether to introduce skip a connection from the layer below l − 1 for each layer l and they use this as a regularization. As opposed to the USSC, DropIn layers do not necessarily do not necessarily achieve a convex objective function as the DropIn ratio (pl) increases.\nThe pseudo-code for the mollified activations is reported in Algorithm 1.\nAlgorithm 1 Activation of a unit i at layer l. 1: xi ← w>i hl−1 + bi . an affine transformation of hl−1 2: ∆i ← u(xi)− f(xi) . ∆i is a measure of a saturation of a unit 3: σ(xi)← (sigmoid(ai∆i)− 0.5)2 . std of the injected noise depends on ∆i 4: ξi ∼ N (0, 1) . sampling the noise from a basic Normal distribution 5: si ← pl c σ(xi)|ξi| . Half-Normal noise controlled by σ(xi), const. c and prob-ty pl 6: ψ(xi, ξi)← sgn(u∗(xi))min(|u∗(xi)|, |f∗(xi) + sgn(u∗(xi))|si||) + u(0) . noisy activation 7: πli ∼ Bernoulli(pl) . pl controls the variance of the noise AND the prob of skipping a unit 8: h̃li = ψ(xi, ξi) . h̃ l i is a noisy activation candidate\n9: φ(hl−1, ξi, πli;wi) = π l ih l−1 i + (1− πli)h̃li . make a HARD decision between h l−1 i and h̃ l i"
    }, {
      "heading" : "5 LINEARIZING THE NETWORK",
      "text" : "In Section 2, we show that convolving the objective function with a particular kernel can be approximated by adding noise to the activation function. This method may suffer from excessive random exploration when the noise is very large.\nWe address this issue by bounding the element-wise activation function f(·) with its linear approximation when the variance of the noise is very large, after centering it at the origin. The resulting function f∗(·) is bounded and centered around the origin. Note that centering the sigmoid or hard-sigmoid will make them symmetric with respect to the origin. With a proper choice of the standard deviation σ(h), the noisy activation function becomes a linear function of the input when p is large, as illustrated by Figure 10.\nLet u∗(x) = u(x)−u(0), where u(0) is the offset of the function from the origin, and xi the i-th dimension of an affine transformation of the output of the previous layer hl−1: xi = w>i h l−1 + bi. Then:\nψ(xi, ξi;wi) = sgn(u∗(xi))min(|u∗(xi)|, |f∗(xi) + sgn(u∗(xi))|si||) + u(0) (25) The noise is sampled from a Normal distribution with mean 0 and whose standard deviation depends on c:\nsi ∼ N (0, p c σ(xi))"
    }, {
      "heading" : "5.1 LINEARIZING RELU ACTIVATION FUNCTION",
      "text" : "We have a simpler form of the equations to linearize ReLU (Nair & Hinton, 2010) activation function when pl →∞. Instead of the complicated Eqn. 23. We can use a simpler equation as in Eqn. 26 to\nachieve the linearization of the activation function when we have a very large noise in the activation function:\nsi = minimum(|xi|, pσ(xi)|ξ|) (26) ψ(xi, ξi,wi) = f(xi)− si (27)"
    }, {
      "heading" : "6 MOLLIFYING LSTMS AND GRUS",
      "text" : "In a similar vein it is possible to smooth the objective functions of LSTM and GRU networks by starting the optimization procedure with a simpler objective function such as optimizing a word2vec, BoW-LM or CRF objective function at the beginning of training and gradually increasing the difficulty of the optimization by increasing the capacity of the network.\nFor GRUs we set the update gate to 1t – where t is the time-step index – and reset the gate to 1 if the noise is very large, using Algorithm 1. Similarly for LSTMs, we can set the output gate to 1 and input gate to 1t and forget gate to 1 − 1 t when the noise is very large. The output gate is 1 or close to 1 when the noise is very large. This way the LSTM will behave like a BOW model. In order to achieve this behavior, the activations ψ(xt, ξi) of the gates can be formulated as:\nψ(xlt, ξ) = f(x l t + p lσ(x)|ξ|) By using a particular formulation of σ(x) that constraints it to be in expectation over ξ when pl = 1, we can obtain a function for γ ∈ R within the range of f(·) that is discrete in expectation, but still per sample differentiable:\nσ(xlt) = f−1(γ)− xlt\nEξ[|ξ|] (28)\nWe provide the derivation of Eqn. 28 in Appendix B. The gradient of the Eqn. 28 will be a Monte-Carlo approximation to the gradient of f(xlt).\n7 ANNEALING SCHEDULES FOR p\nWe used a different schedule for each layer of the network, such that the noise in the lower layers will anneal faster. This is similar to the linearly decaying probability of layers in Huang et al. (2016b).\nExponential Decay In our experiments, we focused on using an annealing schedule similar to the inverse sigmoid rule in Bengio et al. (2015) with plt,\nplt = 1− e− kvtl tL (29)\nwith hyper-parameter k ≥ 0 at tth update for the lth layer, where L is the number of layers of the model. We stop annealing when the expected depth pt = ∑L i=1 p l t reaches some threshold δ. In our experiment we set vt to be a moving average of the loss3 of the network, but for some of our experimnets that resulted unstable behaviors in the training and thus we have to fix vt to 1. An advantage of using running average of loss for the vt is because the behavior of the loss/optimization can directly influence the annealing behavior of the network. Because we will have:\nlim vt→∞ plt = 1 and, lim vt→0 plt = 0. (30)\nThis has a desirable property: when the training-loss is high, the noise injected into the system will be large as well. As a result, the model is encouraged to do more exploration, while if the model converges the noise injected into the system by the mollification procedure will be zero.\nSquare-root Decay The square root decaying of the learning rate is very popular. In our experiments we have tried to decay the plt according to min(pmin, 1 −\n√ t\nNepochs ) where t is the\nindex of the iteration/epoch over the data and Nepochs corresponds to the total number of iterations that the model will do on the dataset. pmin is hyperparameter to define the lower bound on the plt.\nLinear Decay We have also experimented with linearly decaying the plt via min(pmin, 1− tNepochs ).\nWe have compared the plot of different annealing methods described in this paper as in Figure 4.\nFurthermore, in our experiments we observe that training with noisy mollifiers can potentially be helpful for the generalization. This can be due to the noise induced to the backpropagation through the noisy mollification, that makes SGD more likely to converge to a flatter-minima (Hochreiter & Schmidhuber, 1997b) because the noise will help it escape from sharper local minima."
    }, {
      "heading" : "8 EXPERIMENTS",
      "text" : "In this section we mainly focus on training of difficult to optimize models, in particular deep MLPs with sigmoid or tanh activation functions. The details of the experimental procedure is provided in Appendix C."
    }, {
      "heading" : "8.1 EXPLORATORY STUDY ON MNIST",
      "text" : "We train a thin deep neural network on MNIST (LeCun & Cortes, 1998) dataset with 72 hidden layers and 100 hidden units. We train our model with Adam(Kingma & Ba, 2014) optimizer and fix the\n3Depending on whether the model overfits or not, this can be a moving average of training or validation loss.\nlearning rate of all the models to 3e−4. We have used the same learning-rate for all the models in order to factor out the possibility that a model converges faster, due to the fact of using a larger learning rate.\nFirstly, in Figure 5, we investigate the effect of using different annealing schedules. Exponential decay converges faster compared to linear decay and square root decay of p. We find it to be very unstable to train our model with linear and square root decay in particular for large c values. Thus we have to use smaller c value (20 instead of 100) to be able to train the model with causing it to diverge.\nIn Figure 6, we show the effect of using the noisy training procedure that we have introduced by sampling a mask from Bernoulli and Gaussian distributions versu using the deterministic approximation of this noisy procedure which we also use for the test time but during the training as well.\nIn Figure 7, we compare the results obtained for the model using mollification obtained with or without batch normalization and feed-forward residual networks. The mollified model performs very closely to the MLP trained with residual connections and the batchnorm. However, using residual connections and batch-norm does not seem to improve the results.\nWe have tried to run experiment with the Monte-Carlo approximation of the mollification which is derived in Appendix A, however when we start with large noise and anneal the noise during the training, the model was very unstable and training was diverging. If we start with small noise and anneal the magnitude of the noise during the training, we could not observe any effect of it on the training."
    }, {
      "heading" : "8.2 DEEP MLP EXPERIMENTS",
      "text" : "Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015). We experiment on forty dimensional (bits) parity problem with 6-layer MLP using sigmoid activation function. All the models are initialized with Glorot initialization Glorot et al. (2011) and trained with SGD with momentum. We compare an MLP with residual connections using batch normalization and a mollified network with sigmoid activation function. As can be seen in Figure 8, the mollified network converges faster.\nDeep Pentomino Pentomino is a toy-image dataset where each image has 3 Pentomino blocks. The task is to predict whether if there is a different shape in the image or not (Gülçehre & Bengio, 2013). The best reported result on this task with MLPs is 68.15% accuracy (Gulcehre et al., 2014). The same model as ours trained without noisy activation function and vanilla residual connections scored 69.5% accuracy, while our mollified version scored 75.15% accuracy after 100 epochs of training on the 80k dataset.\nCIFAR10 We experimented with deep convolutional neural networks of 110-layers with residual blocks and residual connections comparing our model against ResNet and Stochastic depth. We adapted the hyperparameters of the Stochastic depth network from Huang et al. (2016a) and we used the same hyperparameters for our algorithm. We report the training and validation curves of the three models in Figure 10 and the best test accuracy obtained early stopping on validation accuracy over 500 epochs in Table 1. Our model achieves better generalization than ResNet. Stochastic depth achieves better generalization, but it might be possible to combine both and obtain better results.\nTest PPL\nLSTM 119.4 Mollified LSTM 115.7\nTable 2: 3-layered LSTM network on word-level language modeling for PTB."
    }, {
      "heading" : "9 LSTM EXPERIMENTS",
      "text" : "Predicting the Character Embeddings from Characters Learning the mapping from sequences of characters to the word-embeddings is a difficult problem. Thus one needs to use a highly non-linear function. We trained a word2vec model on Wikipedia with embeddings of size 500 (Mikolov et al., 2014) with a vocabulary of size 374557.\nLSTM Language Modeling We evaluate our model on LSTM language modeling. Our baseline model is a 3-layer stacked LSTM without any regularization. We observed that mollified model converges faster and achieves better results. We provide the results for PTB language modeling in Table 2."
    }, {
      "heading" : "10 CONCLUSION",
      "text" : "We propose a novel method for training neural networks inspired by an idea of continuation, smoothing techniques and recent advances in non-convex optimization algorithms. The method makes learning easier by starting from a simpler model, solving a well-behaved problem, and gradually transitioning to a more complicated setting. We show improvements on very deep models, difficult to optimize tasks and compare with powerful techniques such as batch-normalization and residual connections. We also show that the mollification procedure improves the generalization performance of the model on two tasks.\nOur future work includes testing this method on large-scale language tasks that require long training time, e.g., machine translation and language modeling. Moreover, (Kaiser & Sutskever, 2015) observed that the training of Neural-GPU model can be improved significantly by using gradient noise which can be related to the smoothing of the loss surface, it would be interesting to try mollification on this model to see if the training of Neural GPU can be made easier by using mollification procedure."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We thank Nicholas Ballas and Misha Denil for the valuable discussions and their feedback. We would like to also thank the developers of Theano 4, for developing such a powerful tool for scientific computing Theano Development Team (2016). We acknowledge the support of the following organizations for research funding and computing support: NSERC, Samsung, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR."
    }, {
      "heading" : "A MONTE-CARLO ESTIMATE OF MOLLIFICATION",
      "text" : "LK(θ) = (L ∗K)(θ) = ∫ C L(θ − ξ)K(ξ)dξ which can be estimated by a Monte Carlo:\n≈ 1 N N∑ i=1 L(θ − ξ(i)), where ξ(i) is a realization of the noise random variable ξ\nyielding ∂LK(θ) ∂θ\n≈ 1 N N∑ i=1 ∂L(θ − ξ(i)) ∂θ .\n(31)\nTherefore introducing additive noise to the input of L(θ) is equivalent to mollification."
    }, {
      "heading" : "B DERIVATION OF THE NOISY ACTIVATIONS FOR THE GATING",
      "text" : "Assume that zlt = x l t + p l tσ(x)|ξlt| and Eξ[ψ(xlt, ξ)] = t. Thus for all zlt,\nEξ[ψ(x l t, ξ l t)] = Eξ[f(z l t)], (32)\nt = Eξ[f(zlt)], assuming f(·) behaves similar to a linear function: (33) Eξ[f(zlt)] ≈ f(Eξ[zlt]) since we use hard-sigmoid for f(·) this will hold. (34)\nf−1(t) ≈ Eξ[zlt] (35) (36)\nAs in Eqn. 32, we can write the expectation of this equation as:\nf−1(t) ≈ xlt + pltσ(x)Eξ[ξlt] Corollary, the value that σ(xlt) should take in expectation for p l t = 1 would be:\nσ(xlt) ≈ f−1(t)− xlt\nEξ[ξlt]\nIn our experiments for f(·) we used the hard-sigmoid activation function. We used the following piecewise activation function in order to use it as f−1(x) = 4(x − 0.5). During inference we use the expected value of random variables π and ξ."
    }, {
      "heading" : "C EXPERIMENTAL DETAILS",
      "text" : "C.1 MNIST\nThe weights of the models are initialized with Glorot & Bengio initialization Glorot et al. (2011). We use the learning rate of 4e − 4 along with RMSProp. We initialize ai parameters of mollified activation function by sampling it from a uniform distribution, U[−2, 2]. We used 100 hidden units at each layer with a minibatches of size 500.\nC.2 PENTOMINO\nWe train a 6−layer MLP with sigmoid activation function using SGD and momentum. We used 200 units per layer with sigmoid activation functions. We use a learning rate of 1e− 3.\nC.3 CIFAR10\nWe use the same model with the same hyperparameters for both ResNet, mollified network and the stochastic depth. We borrowed the hyperparameters of the model from Huang et al. (2016a). Our mollified convnet model has residual connections coming from its layer below.\nC.4 PARITY\nThe n-dimensional parity task is the task to figure out whether the sum of n-bits in a binary vector is even or odd. We use SGD with Nesterov momentum and initialize the weight matrices by using Glorot&Bengio initializationGlorot et al. (2011). For all models, we use the learning rate of 1e− 3 and momentum of 0.92. ai is the parameters of mollified activation function are initialized by sampling from uniform distribution, U [−2, 2].\nC.5 LSTM LANGUAGE MODELING\nWe trained 2-layered LSTM language models on PTB word-level. We used the models with the same hyperparameters as in Zaremba & Sutskever (2014). We used the same hyperparameters for both the mollified LSTM language model and the LSTM. We use hard-sigmoid activation function for both the LSTM and mollified LSTM language model. We use hard-sigmoid activation function for the gates of the LSTM.\nC.6 PREDICTING THE CHARACTER EMBEDDINGS FROM CHARACTERS\nWe use 10k of these words as a validation and another 10k word embeddings as test set. We train a bidirectional-LSTM on top of each sequence of characters for each word and on top of the representation of bidirectional LSTM, we use a 5-layered tanh-MLP to predict the word-embedding.\nWe train our models using RMSProp and momentum with learning rate of 6e− 4 and momentum 0.92. The size of the minibatches, we used is 64. As seen in Figure 9, mollified LSTM network converges faster."
    } ],
    "references" : [ {
      "title" : "Numerical Continuation Methods",
      "author" : [ "E.L. Allgower", "K. Georg" ],
      "venue" : "An Introduction. Springer-Verlag,",
      "citeRegEx" : "Allgower and Georg.,? \\Q1980\\E",
      "shortCiteRegEx" : "Allgower and Georg.",
      "year" : 1980
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Online algorithms and stochastic approximations",
      "author" : [ "Léon Bottou" ],
      "venue" : null,
      "citeRegEx" : "Bottou.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bottou.",
      "year" : 1998
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "The loss surface of multilayer",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Choromanska et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2014
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "In NIPS’2014,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Partial differential equations",
      "author" : [ "Lawrence C Evans" ],
      "venue" : "Graduate Studies in Mathematics,",
      "citeRegEx" : "Evans.,? \\Q1998\\E",
      "shortCiteRegEx" : "Evans.",
      "year" : 1998
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Graves.,? \\Q2011\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2011
    }, {
      "title" : "Adaptive computation time for recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1603.08983,",
      "citeRegEx" : "Graves.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2016
    }, {
      "title" : "Knowledge matters: Importance of prior information for optimization",
      "author" : [ "Çağlar Gülçehre", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1301.4083,",
      "citeRegEx" : "Gülçehre and Bengio.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gülçehre and Bengio.",
      "year" : 2013
    }, {
      "title" : "Learned-norm pooling for deep feedforward and recurrent neural networks",
      "author" : [ "Caglar Gulcehre", "Kyunghyun Cho", "Razvan Pascanu", "Yoshua Bengio" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2014
    }, {
      "title" : "Noisy activation functions",
      "author" : [ "Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2016
    }, {
      "title" : "On graduated optimization for stochastic non-convex problems",
      "author" : [ "Elad Hazan", "Kfir Y Levy", "Shai Shalev-Shwartz" ],
      "venue" : "arXiv preprint arXiv:1503.03712,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George Dahl", "Abdel rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury" ],
      "venue" : "Signal Processing Magazine,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Keeping neural networks simple",
      "author" : [ "Geoffrey E Hinton", "Drew van Camp" ],
      "venue" : "In ICANN’93,",
      "citeRegEx" : "Hinton and Camp.,? \\Q1993\\E",
      "shortCiteRegEx" : "Hinton and Camp.",
      "year" : 1993
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q. Weinberger" ],
      "venue" : "CoRR, abs/1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "shift. CoRR,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Neural gpus learn algorithms",
      "author" : [ "Łukasz Kaiser", "Ilya Sutskever" ],
      "venue" : "arXiv preprint arXiv:1511.08228,",
      "citeRegEx" : "Kaiser and Sutskever.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kaiser and Sutskever.",
      "year" : 2015
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1507.01526,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Optimization by simulated annealing",
      "author" : [ "S. Kirkpatrick", "C.D. Gelatt Jr.", "M.P. Vecchi" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 1983
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Yann LeCun", "Corinna Cortes" ],
      "venue" : null,
      "citeRegEx" : "LeCun and Cortes.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun and Cortes.",
      "year" : 1998
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Training recurrent neural networks by diffusion",
      "author" : [ "Hossein Mobahi" ],
      "venue" : "arXiv preprint arXiv:1601.04114,",
      "citeRegEx" : "Mobahi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mobahi.",
      "year" : 2016
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Adding gradient noise improves learning for very deep",
      "author" : [ "Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens" ],
      "venue" : "networks. CoRR,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Adding gradient noise improves learning for very deep networks",
      "author" : [ "Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens" ],
      "venue" : "arXiv preprint arXiv:1511.06807,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Shaping and policy search in reinforcement learning",
      "author" : [ "Andrew Y Ng" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Ng.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ng.",
      "year" : 2003
    }, {
      "title" : "Policy invariance under reward transformations: Theory and application to reward shaping",
      "author" : [ "Andrew Y Ng", "Daishi Harada", "Stuart Russell" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ng et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 1999
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradual dropin of layers to train very deep neural networks",
      "author" : [ "Leslie N Smith", "Emily M Hand", "Timothy Doster" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Smith et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2016
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh K Srivastava", "Klaus Greff", "Jürgen Schmidhuber" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Francesco Visin", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio", "Matteo Matteucci", "Kyunghyun Cho" ],
      "venue" : "arXiv e-prints,",
      "citeRegEx" : "Visin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Visin et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "convolutional networks (LeCun et al., 1989), LSTMs (Hochreiter & Schmidhuber, 1997a) or GRUs (Cho et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : ", 1989), LSTMs (Hochreiter & Schmidhuber, 1997a) or GRUs (Cho et al., 2014) – set the state of the art on a range of challenging tasks (Szegedy et al.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 40,
      "context" : ", 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 217
    }, {
      "referenceID" : 17,
      "context" : ", 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 217
    }, {
      "referenceID" : 39,
      "context" : ", 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 217
    }, {
      "referenceID" : 1,
      "context" : ", 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : ", 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 217
    }, {
      "referenceID" : 36,
      "context" : ", 2014) – set the state of the art on a range of challenging tasks (Szegedy et al., 2014; Visin et al., 2015; Hinton et al., 2012; Sutskever et al., 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 217
    }, {
      "referenceID" : 4,
      "context" : "However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al.",
      "startOffset" : 42,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al., 2014; Dauphin et al., 2014).",
      "startOffset" : 153,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : "However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al., 2014; Dauphin et al., 2014).",
      "startOffset" : 153,
      "endOffset" : 201
    }, {
      "referenceID" : 3,
      "context" : "A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe & Szegedy, 2015), learning with a curriculum (Bengio et al., 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al.",
      "startOffset" : 214,
      "endOffset" : 235
    }, {
      "referenceID" : 30,
      "context" : ", 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al.",
      "startOffset" : 67,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : ", 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 14,
      "context" : "Injecting noise to the activation functions and scheduling it have been recently shown to improve the performance on a wide variety of tasks (Gulcehre et al., 2016).",
      "startOffset" : 141,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 38,
      "context" : "Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016). However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al., 2014; Dauphin et al., 2014). A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe & Szegedy, 2015), learning with a curriculum (Bengio et al., 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al., 2015). The impact of noise injection on the behavior of modern deep learning methods has been explored by Neelakantan et al. (2015a). Hazan et al.",
      "startOffset" : 8,
      "endOffset" : 761
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016). However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al., 2014; Dauphin et al., 2014). A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe & Szegedy, 2015), learning with a curriculum (Bengio et al., 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al., 2015). The impact of noise injection on the behavior of modern deep learning methods has been explored by Neelakantan et al. (2015a). Hazan et al. (2015) have shown that injecting a particular noise and scheduling it carefully, can guarantee the convergence in O(1/σ ) steps for -optimal and σ-nice functions.",
      "startOffset" : 8,
      "endOffset" : 782
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Bahdanau et al., 2014; Mnih et al., 2013; Silver et al., 2016). However when trained with variants of SGD (Bottou, 1998) deep networks can be difficult to optimize due to their highly non-linear and non-convex nature (Choromanska et al., 2014; Dauphin et al., 2014). A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe & Szegedy, 2015), learning with a curriculum (Bengio et al., 2009), recently an approach to train RNNs with diffusion process (Mobahi, 2016), and graduated optimization (Hazan et al., 2015). The impact of noise injection on the behavior of modern deep learning methods has been explored by Neelakantan et al. (2015a). Hazan et al. (2015) have shown that injecting a particular noise and scheduling it carefully, can guarantee the convergence in O(1/σ ) steps for -optimal and σ-nice functions. Similar to our work graduated optimization optimizes a smoothed objective function without performing expensive convolutions. Injecting noise to the activation functions and scheduling it have been recently shown to improve the performance on a wide variety of tasks (Gulcehre et al., 2016). We connect the ideas of curriculum learning and continuation methods with those arising from models with skip connections and using layers that compute near-identity transformations. Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers. Similarly, it has been shown that stochastically changing the depth of a network during training (Huang et al., 2016b) does not prevent convergence and enables better generalization performance. We discuss the idea of mollification for neural networks – a form of differentiable smoothing of the loss function connected to noisy activations – which in our case can be interpreted as a form of adaptive noise injection which is controlled by a single hyperparameter. Inspired by Huang et al. (2016b), we use a hyperparameter to stochastically control the depth of our network.",
      "startOffset" : 8,
      "endOffset" : 2071
    }, {
      "referenceID" : 15,
      "context" : "An important difference of our work compared to injecting noise to the gradients as it is explored in (Hazan et al., 2015; Neelakantan et al., 2015b) is that we inject the noise in the forward computation of the graph and we shape the cost function directly.",
      "startOffset" : 102,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "In machine learning, approaches based on curriculum learning (Bengio et al., 2009) are inspired by this principle and define a sequence of gradually more difficult training tasks (or training distributions) that eventually converge to the task of interest.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "1, we can take its gradient with respect to θ using directly the result from Evans (1998): ∇θLK(θ) = ∇θ(L ∗K)(θ) = (L ∗ ∇K)(θ).",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : "Gradually reducing the noise during training is related to a form of simulated annealing (Kirkpatrick et al., 1983).",
      "startOffset" : 89,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "Gradually reducing the noise during training is related to a form of simulated annealing (Kirkpatrick et al., 1983). Similarly to the analysis in Mobahi (2016), we can write a Monte-Carlo estimate of LK(θ) = (L ∗K)(θ) ≈ 1 N ∑N i=1 L(θ − ξ).",
      "startOffset" : 90,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "16, it is easy to see that both weight noise methods proposed by Hinton & van Camp (1993) and Graves (2011) can be seen as a variation of Monte-Carlo estimate of mollifiers.",
      "startOffset" : 94,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "This is related to the work of Mobahi (2016), who recently introduced a way of analytically smoothing of the non-linearities to help the training of recurrent networks.",
      "startOffset" : 31,
      "endOffset" : 45
    }, {
      "referenceID" : 35,
      "context" : "Shaping the cost function to define a sequence of costs that are progressing from easier to more difficult ones can be related to the reward shaping (Ng et al., 1999; Ng, 2003) algorithms.",
      "startOffset" : 149,
      "endOffset" : 176
    }, {
      "referenceID" : 34,
      "context" : "Shaping the cost function to define a sequence of costs that are progressing from easier to more difficult ones can be related to the reward shaping (Ng et al., 1999; Ng, 2003) algorithms.",
      "startOffset" : 149,
      "endOffset" : 176
    }, {
      "referenceID" : 37,
      "context" : "In DropIn layers (Smith et al., 2016) a binary random vector is sampled from a Bernoulli distribution to decide whether to introduce skip a connection from the layer below l − 1 for each layer l and they use this as a regularization.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "This is similar to the linearly decaying probability of layers in Huang et al. (2016b).",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Exponential Decay In our experiments, we focused on using an annealing schedule similar to the inverse sigmoid rule in Bengio et al. (2015) with pt, pt = 1− e− kvtl tL (29)",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : "Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015).",
      "startOffset" : 105,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015).",
      "startOffset" : 105,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "All the models are initialized with Glorot initialization Glorot et al. (2011) and trained with SGD with momentum.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "15% accuracy (Gulcehre et al., 2014).",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "We adapted the hyperparameters of the Stochastic depth network from Huang et al. (2016a) and we used the same hyperparameters for our algorithm.",
      "startOffset" : 68,
      "endOffset" : 89
    } ],
    "year" : 2017,
    "abstractText" : "The optimization of deep neural networks can be more challenging than the traditional convex optimization problems due to highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape from for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks objectives by starting with a smoothed – or mollified – objective function which becomes more complex as the training proceeds. Our proposition is inspired by the recent studies in continuation methods: similarly to curriculum methods, we begin by learning an easier (possibly convex) objective function and let it evolve during training until it eventually becomes the original, difficult to optimize objective function. The complexity of the mollified networks is controlled by a single hyperparameter that is annealed during training. We show improvements on various difficult optimization tasks and establish a relationship between recent works on continuation methods for neural networks and mollifiers.",
    "creator" : "LaTeX with hyperref package"
  }
}