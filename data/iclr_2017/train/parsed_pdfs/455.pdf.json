{
  "name" : "455.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Junbo Zhao", "Michael Mathieu", "Yann LeCun" ],
    "emails" : [ "yann}@cs.nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We introduce the “Energy-based Generative Adversarial Network” model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : ""
    }, {
      "heading" : "1.1 ENERGY-BASED MODEL",
      "text" : "The essence of the energy-based model (LeCun et al., 2006) is to build a function that maps each point of an input space to a single scalar, which is called “energy”. The learning phase is a datadriven process that shapes the energy surface in such a way that the desired configurations get assigned low energies, while the incorrect ones are given high energies. Supervised learning falls into this framework: for each X in the training set, the energy of the pair (X,Y ) takes low values when Y is the correct label and higher values for incorrect Y ’s. Similarly, when modeling X alone within an unsupervised learning setting, lower energy is attributed to the data manifold. The term contrastive sample is often used to refer to a data point causing an energy pull-up, such as the incorrect Y ’s in supervised learning and points from low data density regions in unsupervised learning."
    }, {
      "heading" : "1.2 GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) have led to significant improvements in image generation (Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016), video prediction (Mathieu et al., 2015) and a number of other domains. The basic idea of GAN is to simultaneously train a discriminator and a generator. The discriminator is trained to distinguish real samples of a dataset from fake samples produced by the generator. The generator uses input from an easy-to-sample random source, and is trained to produce fake samples that the discriminator cannot distinguish from real data samples. During training, the generator receives the gradient of the output of the discriminator with respect to the fake sample. In the original formulation of GAN in Goodfellow et al. (2014), the discriminator produces a probability and, under certain conditions, convergence occurs when the distribution produced by the generator matches the data distribution. From a game theory point of view, the convergence of a GAN is reached when the generator and the discriminator reach a Nash equilibrium."
    }, {
      "heading" : "1.3 ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "In this work, we propose to view the discriminator as an energy function (or a contrast function) without explicit probabilistic interpretation. The energy function computed by the discriminator can be viewed as a trainable cost function for the generator. The discriminator is trained to assign low energy values to the regions of high data density, and higher energy values outside these regions. Conversely, the generator can be viewed as a trainable parameterized function that produces samples in regions of the space to which the discriminator assigns low energy. While it is often possible to convert energies into probabilities through a Gibbs distribution (LeCun et al., 2006), the absence of normalization in this energy-based form of GAN provides greater flexibility in the choice of architecture of the discriminator and the training procedure.\nThe probabilistic binary discriminator in the original formulation of GAN can be seen as one way among many to define the contrast function and loss functional, as described in LeCun et al. (2006) for the supervised and weakly supervised settings, and Ranzato et al. (2007) for unsupervised learning. We experimentally demonstrate this concept, in the setting where the discriminator is an autoencoder architecture, and the energy is the reconstruction error. More details of the interpretation of EBGAN are provided in the appendix B.\nOur main contributions are summarized as follows:\n• An energy-based formulation for generative adversarial training. • A proof that under a simple hinge loss, when the system reaches convergence, the generator\nof EBGAN produces points that follow the underlying data distribution. • An EBGAN framework with the discriminator using an auto-encoder architecture in which\nthe energy is the reconstruction error. • A set of systematic experiments to explore hyper-parameters and architectural choices that\nproduce good result for both EBGANs and probabilistic GANs. • A demonstration that EBGAN framework can be used to generate reasonable-looking high-\nresolution images from the ImageNet dataset at 256×256 pixel resolution, without a multiscale approach."
    }, {
      "heading" : "2 THE EBGAN MODEL",
      "text" : "Let pdata be the underlying probability density of the distribution that produces the dataset. The generator G is trained to produce a sample G(z), for instance an image, from a random vector z, which is sampled from a known distribution pz , for instance N (0, 1). The discriminator D takes either real or generated images, and estimates the energy value E ∈ R accordingly, as explained later. For simplicity, we assume that D produces non-negative values, but the analysis would hold as long as the values are bounded below."
    }, {
      "heading" : "2.1 OBJECTIVE FUNCTIONAL",
      "text" : "The output of the discriminator goes through an objective functional in order to shape the energy function, attributing low energy to the real data samples and higher energy to the generated (“fake”) ones. In this work, we use a margin loss, but many other choices are possible as explained in LeCun et al. (2006). Similarly to what has been done with the probabilistic GAN (Goodfellow et al., 2014), we use a two different losses, one to train D and the other to train G, in order to get better quality gradients when the generator is far from convergence. Given a positive margin m, a data sample x and a generated sample G(z), the discriminator loss LD and the generator loss LG are formally defined by:\nLD(x, z) = D(x) + [m−D ( G(z) ) ]+ (1)\nLG(z) = D ( G(z) ) (2)\nwhere [·]+ = max(0, ·). Minimizing LG with respect to the parameters of G is similar to maximizing the second term of LD. It has the same minimum but non-zero gradients when D(G(z)) ≥ m."
    }, {
      "heading" : "2.2 OPTIMALITY OF THE SOLUTION",
      "text" : "In this section, we present a theoretical analysis of the system presented in section 2.1. We show that if the system reaches a Nash equilibrium, then the generator G produces samples that are indistinguishable from the distribution of the dataset. This section is done in a non-parametric setting, i.e. we assume that D and G have infinite capacity.\nGiven a generator G, let pG be the density distribution of G(z) where z ∼ pz . In other words, pG is the density distribution of the samples generated by G. We define V (G,D) = ∫ x,z LD(x, z)pdata(x)pz(z)dxdz and U(G,D) = ∫ z LG(z)pz(z)dz. We train the discriminatorD to minimize the quantity V and the generatorG to minimize the quantityU . A Nash equilibrium of the system is a pair (G∗, D∗) that satisfies:\nV (G∗, D∗) ≤ V (G∗, D) ∀D (3) U(G∗, D∗) ≤ U(G,D∗) ∀G (4)\nTheorem 1. If (D∗, G∗) is a Nash equilibrium of the system, then pG∗ = pdata almost everywhere, and V (D∗, G∗) = m.\nProof. First we observe that\nV (G∗, D) = ∫ x pdata(x)D(x)dx+ ∫ z pz(z) [m−D(G∗(z))]+ dz (5)\n= ∫ x ( pdata(x)D(x) + pG∗(x) [m−D(x)]+ ) dx. (6)\nThe analysis of the function ϕ(y) = ay+b(m−y)+ (see lemma 1 in appendix A for details) shows: (a) D∗(x) ≤ m almost everywhere. To verify it, let us assume that there exists a set of measure non-zero such that D∗(x) > m. Let D̃(x) = min(D∗(x),m). Then V (G∗, D̃) < V (G∗, D∗) which violates equation 3. (b) The function ϕ reaches its minimum in m if a < b and in 0 otherwise. So V (G∗, D) reaches its minimum when we replace D∗(x) by these values. We obtain\nV (G∗, D∗) = m ∫ x 1pdata(x)<pG∗ (x)pdata(x)dx+m ∫ x 1pdata(x)≥pG∗ (x)pG∗(x)dx (7)\n= m ∫ x ( 1pdata(x)<pG∗ (x)pdata(x) + ( 1− 1pdata(x)<pG∗ (x) ) pG∗(x) ) dx (8)\n= m ∫ x pG∗(x)dx+m ∫ x 1pdata(x)<pG∗ (x)(pdata(x)− pG∗(x))dx (9)\n= m+m ∫ x 1pdata(x)<pG∗ (x)(pdata(x)− pG∗(x))dx. (10)\nThe second term in equation 10 is non-positive, so V (G∗, D∗) ≤ m. By putting the ideal generator that generates pdata into the right side of equation 4, we get∫\nx\npG∗(x)D ∗(x)dx ≤ ∫ x pdata(x)D ∗(x)dx. (11)\nThus by (6), ∫ x pG∗(x)D ∗(x)dx+ ∫ x pG∗(x)[m−D∗(x)]+dx ≤ V (G∗, D∗) (12)\nand since D∗(x) ≤ m, we get m ≤ V (G∗, D∗). Thus, m ≤ V (G∗, D∗) ≤ m i.e. V (G∗, D∗) = m. Using equation 10, we see that can only happen if ∫ x 1pdata(x)<pG(x)dx = 0, which is true if and only if pG = pdata almost everywhere (this is because pdata and pG are probabilities densities, see lemma 2 in the appendix A for details).\nTheorem 2. A Nash equilibrium of this system exists and is characterized by (a) pG∗ = pdata (almost everywhere) and (b) there exists a constant γ ∈ [0,m] such that D∗(x) = γ (almost everywhere).1.\n1This is assuming there is no region where pdata(x) = 0. If such a region exists, D∗(x) may have any value in [0,m] for x in this region.\nProof. See appendix A."
    }, {
      "heading" : "2.3 USING AUTO-ENCODERS",
      "text" : "In our experiments, the discriminator D is structured as an auto-encoder:\nD(x) = ||Dec(Enc(x))− x||. (13)\nThe diagram of the EBGAN model with an auto-encoder discriminator is depicted in figure 1. The choice of the auto-encoders for D may seem arbitrary at the first glance, yet we postulate that it is conceptually more attractive than a binary logistic network:\n• Rather than using a single bit of target information to train the model, the reconstruction-based output offers a diverse targets for the discriminator. With the binary logistic loss, only two targets are possible, so within a minibatch, the gradients corresponding to different samples are most likely far from orthogonal. This leads to inefficient training, and reducing the minibatch sizes is often not an option on current hardware. On the other hand, the reconstruction loss will likely produce very different gradient directions within the minibatch, allowing for larger minibatch size without loss of efficiency. • Auto-encoders have traditionally been used to represent energy-based model and arise naturally. When trained with some regularization terms (see section 2.3.1), auto-encoders have the ability to learn an energy manifold without supervision or negative examples. This means that even when an EBGAN auto-encoding model is trained to reconstruct a real sample, the discriminator contributes to discovering the data manifold by itself. To the contrary, without the presence of negative examples from the generator, a discriminator trained with binary logistic loss becomes pointless."
    }, {
      "heading" : "2.3.1 CONNECTION TO THE REGULARIZED AUTO-ENCODERS",
      "text" : "One common issue in training auto-encoders is that the model may learn little more than an identity function, meaning that it attributes zero energy to the whole space. In order to avoid this problem, the model must be pushed to give higher energy to points outside the data manifold. Theoretical and experimental results have addressed this issue by regularizing the latent representations (Vincent et al., 2010; Rifai et al., 2011; MarcAurelio Ranzato & Chopra, 2007; Kavukcuoglu et al., 2010). Such regularizers aim at restricting the reconstructing power of the auto-encoder so that it can only attribute low energy to a smaller portion of the input points.\nWe argue that the energy function (the discriminator) in the EBGAN framework is also seen as being regularized by having a generator producing the contrastive samples, to which the discriminator ought to give high reconstruction energies. We further argue that the EBGAN framework allows more flexibility from this perspective, because: (i)-the regularizer (generator) is fully trainable instead of being handcrafted; (ii)-the adversarial training paradigm enables a direct interaction between the duality of producing contrastive sample and learning the energy function."
    }, {
      "heading" : "2.4 REPELLING REGULARIZER",
      "text" : "We propose a “repelling regularizer” which fits well into the EBGAN auto-encoder model, purposely keeping the model from producing samples that are clustered in one or only few modes of pdata. Another technique “minibatch discrimination” was developed by Salimans et al. (2016) from the same philosophy.\nImplementing the repelling regularizer involves a Pulling-away Term (PT) that runs at a representation level. Formally, let S ∈ Rs×N denotes a batch of sample representations taken from the encoder output layer. Let us define PT as:\nfPT (S) = 1 N(N − 1) ∑ i ∑ j 6=i ( STi Sj ‖Si‖‖Sj‖ )2 . (14)\nPT operates on a mini-batch and attempts to orthogonalize the pairwise sample representation. It is inspired by the prior work showing the representational power of the encoder in the auto-encoder alike model such as Rasmus et al. (2015) and Zhao et al. (2015). The rationale for choosing the cosine similarity instead of Euclidean distance is to make the term bounded below and invariant to scale. We use the notation “EBGAN-PT” to refer to the EBGAN auto-encoder model trained with this term. Note the PT is used in the generator loss but not in the discriminator loss."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "Our work primarily casts GANs into an energy-based model scope. On this direction, the approaches studying contrastive samples are relevant to EBGAN, such as the use of noisy samples (Vincent et al., 2010) and noisy gradient descent methods like contrastive divergence (Carreira-Perpinan & Hinton, 2005). From the perspective of GANs, several papers were presented to improve the stability of GAN training, (Salimans et al., 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015).\nKim & Bengio (2016) propose a probabilistic GAN and cast it into an energy-based density estimator by using the Gibbs distribution. Quite unlike EBGAN, this proposed framework doesn’t get rid of the computational challenging partition function, so the choice of the energy function is required to be integratable."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 EXHAUSTIVE GRID SEARCH ON MNIST",
      "text" : "In this section, we study the training stability of EBGANs over GANs on a simple task of MNIST digit generation with fully-connected networks. We run an exhaustive grid search over a set of architectural choices and hyper-parameters for both frameworks.\nFormally, we specify the search grid in table 1. We impose the following restrictions on EBGAN models: (i)-using learning rate 0.001 and Adam (Kingma & Ba, 2014) for both G and D; (ii)nLayerD represents the total number of layers combiningEnc andDec. For simplicity, we fixDec to be one layer and only tune the Enc #layers; (iii)-the margin is set to 10 and not being tuned. To analyze the results, we use the inception score (Salimans et al., 2016) as a numerical means reflecting the generation quality. Some slight modification of the formulation were made to make figure 2 visually more approachable while maintaining the score’s original meaning, I ′ = ExKL(p(y)||p(y|x))2 (more details in appendix C). Briefly, higher I ′ score implies better generation quality.\nHistograms We plot the histogram of I ′ scores in figure 2. We further separated out the optimization related setting from GAN’s grid (optimD, optimG and lr) and plot the histogram of each subgrid individually, together with the EBGAN I ′ scores as a reference, in figure 3. The number of experiments for GANs and EBGANs are both 512 in every subplot. The histograms evidently show that EBGANs are more reliably trained.\n2This form of the “inception score” is only used to better analyze the grid search in the scope of this work, but not to compare with any other published work.\nDigits generated from the configurations presenting the best inception score are shown in figure 4."
    }, {
      "heading" : "4.2 SEMI-SUPERVISED LEARNING ON MNIST",
      "text" : "We explore the potential of using the EBGAN framework for semi-supervised learning on permutation-invariant MNIST, collectively on using 100, 200 and 1000 labels. We utilized a bottom-\nlayer-cost Ladder Network (LN) (Rasmus et al., 2015) with the EGBAN framework (EBGAN-LN). Ladder Network can be categorized as an energy-based model that is built with both feedforward and feedback hierarchies powered by stage-wise lateral connections coupling two pathways.\nOne technique we found crucial in enabling EBGAN framework for semi-supervised learning is to gradually decay the margin value m of the equation 1. The rationale behind is to let discriminator punish generator less when pG gets closer to the data manifold. One can think of the extreme case where the contrastive samples are exactly pinned on the data manifold, such that they are “not contrastive anymore”. This ultimate status happens when m = 0 and the EBGAN-LN model falls back to a normal Ladder Network. The undesirability of a non-decay dynamics for using the discriminator in the GAN or EBGAN framework is also indicated by Theorem 2: on convergence, the discriminator reflects a flat energy surface. However, we posit that the trajectory of learning a EBGAN-LN model does provide the LN (discriminator) more information by letting it see contrastive samples. Yet the optimal way to avoid the mentioned undesirability is to make sure m has been decayed to 0 when the Nash Equilibrium is reached. The margin decaying schedule is found by hyper-parameter search in our experiments (technical details in appendix D).\nFrom table 2, it shows that positioning a bottom-layer-cost LN into an EBGAN framework profitably improves the performance of the LN itself. We postulate that within the scope of the EBGAN framework, iteratively feeding the adversarial contrastive samples produced by the generator to the energy function acts as an effective regularizer; the contrastive samples can be thought as an extension to the dataset that provides more information to the classifier. We notice there was a discrepancy between the reported results between Rasmus et al. (2015) and Pezeshki et al. (2015), so we report both results along with our own implementation of the Ladder Network running the same setting. The specific experimental setting and analysis are available in appendix D."
    }, {
      "heading" : "4.3 LSUN & CELEBA",
      "text" : "We apply the EBGAN framework with deep convolutional architecture to generate 64 × 64 RGB images, a more realistic task, using the LSUN bedroom dataset (Yu et al., 2015) and the large-scale face dataset CelebA under alignment (Liu et al., 2015). To compare EBGANs with DCGANs (Radford et al., 2015), we train a DCGAN model under the same configuration and show its generation side-by-side with the EBGAN model, in figures 5 and 6. The specific settings are listed in appendix C."
    }, {
      "heading" : "4.4 IMAGENET",
      "text" : "Finally, we trained EBGANs to generate high-resolution images on ImageNet (Russakovsky et al., 2015). Compared with the datasets we have experimented so far, ImageNet presents an extensively larger and wilder space, so modeling the data distribution by a generative model becomes very challenging. We devised an experiment to generate 128 × 128 images, trained on the full ImageNet-1k dataset, which contains roughly 1.3 million images from 1000 different categories. We also trained a network to generate images of size 256 × 256, on a dog-breed subset of ImageNet, using the wordNet IDs provided by Vinyals et al. (2016). The results are shown in figures 7 and 8. Despite the difficulty of generating images on a high-resolution level, we observe that EBGANs are able to learn about the fact that objects appear in the foreground, together with various background components resembling grass texture, sea under the horizon, mirrored mountain in the water, buildings, etc. In addition, our 256 × 256 dog-breed generations, although far from realistic, do reflect some knowledge about the appearances of dogs such as their body, furs and eye."
    }, {
      "heading" : "5 OUTLOOK",
      "text" : "We bridge two classes of unsupervised learning methods – GANs and auto-encoders – and revisit the GAN framework from an alternative energy-based perspective. EBGANs show better convergence pattern and scalability to generate high-resolution images. A family of energy-based loss functionals presented in LeCun et al. (2006) can easily be incorporated into the EBGAN framework. For the future work, the conditional setting (Denton et al., 2015; Mathieu et al., 2015) is a promising setup to explore. We hope the future research will raise more attention on a broader view of GANs from the energy-based perspective."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "We thank Emily Denton, Soumith Chitala, Arthur Szlam, Marc’Aurelio Ranzato, Pablo Sprechmann, Ross Goroshin and Ruoyu Sun for fruitful discussions. We also thank Emily Denton and Tian Jiang for their help with the manuscript."
    }, {
      "heading" : "A APPENDIX: TECHNICAL POINTS OF SECTION 2.2",
      "text" : "Lemma 1. Let a, b ≥ 0, ϕ(y) = ay + b [m− y]+. The minimum of ϕ on [0,+∞) exists and is reached in m if a < b, and it is reached in 0 otherwise (the minimum may not be unique).\nProof. The function ϕ is defined on [0,+∞), its derivative is defined on [0,+∞)\\{m} and ϕ′(y) = a− b if y ∈ [0,m) and ϕ′(y) = a if y ∈ (m,+∞). So when a < b, the function is decreasing on [0,m) and increasing on (m,+∞). Since it is continuous, it has a minimum in m. It may not be unique if a = 0 or a− b = 0. On the other hand, if a ≥ b the function ϕ is increasing on [0,+∞), so 0 is a minimum.\nLemma 2. If p and q are probability densities, then ∫ x 1p(x)<q(x)dx = 0 if and only if∫\nx 1p(x) 6=q(x)dx = 0.\nProof. Let’s assume that ∫ x 1p(x)<q(x)dx = 0. Then∫\nx\n1p(x)>q(x)(p(x)− q(x))dx (15)\n= ∫ x (1− 1p(x)≤q(x))(p(x)− q(x))dx (16)\n= ∫ x p(x)dx− ∫ x q(x)dx+ ∫ x 1p(x)≤q(x)(p(x)− q(x))dx (17)\n= 1− 1 + ∫ x ( 1p(x)<q(x) + 1p(x)=q(x) ) (p(x)− q(x))dx (18)\n= ∫ x 1p(x)<q(x)(p(x)− q(x))dx+ ∫ x 1p(x)=q(x)(p(x)− q(x))dx (19) = 0 + 0 = 0 (20)\nSo ∫ x 1p(x)>q(x)(p(x) − q(x))dx = 0 and since the term in the integral is always non-negative, 1p(x)>q(x)(p(x) − q(x)) = 0 for almost all x. And p(x) − q(x) = 0 implies 1p(x)>q(x) = 0, so 1p(x)>q(x) = 0 almost everywhere. Therefore ∫ x 1p(x)>q(x)dx = 0 which completes the proof, given the hypothesis.\nProof of theorem 2 The sufficient conditions are obvious. The necessary condition on G∗ comes from theorem 1, and the necessary condition on D∗(x) ≤ m is from the proof of theorem 1. Let us now assume that D∗(x) is not constant almost everywhere and find a contradiction. If it is not, then there exists a constant C and a set S of non-zero measure such that ∀x ∈ S, D∗(x) ≤ C and ∀x 6∈ S, D∗(X) > C. In addition we can choose S such that there exists a subset S ′ ⊂ S of non-zero measure such that pdata(x) > 0 on S ′ (because of the assumption in the footnote). We can build a generator G0 such that pG0(x) ≤ pdata(x) over S and pG0(x) < pdata(x) over S ′. We compute\nU(G∗, D∗)− U(G0, D∗) = ∫ x (pdata − pG0)D∗(x)dx (21)\n= ∫ x (pdata − pG0)(D∗(x)− C)dx (22)\n= ∫ S (pdata − pG0)(D∗(x)− C)dx+ ∫ RN\\S (pdata − pG0)(D∗(x)− C)dx (23) > 0 (24)\nwhich violates equation 4."
    }, {
      "heading" : "B APPENDIX: MORE INTERPRETATIONS ABOUT GANS AND ENERGY-BASED LEARNING",
      "text" : "TWO INTERPRETATIONS OF GANS\nGANs can be interpreted in two complementary ways. In the first interpretation, the key component is the generator, and the discriminator plays the role of a trainable objective function. Let us imagine that the data lies on a manifold. Until the generator produces samples that are recognized as being on the manifold, it gets a gradient indicating how to modify its output so it could approach the manifold. In such scenario, the discriminator acts to punish the generator when it produces samples that are outside the manifold. This can be understood as a way to train the generator with a set of possible desired outputs (e.g. the manifold) instead of a single desired output as in traditional supervised learning.\nFor the second interpretation, the key component is the discriminator, and the generator is merely trained to produce contrastive samples. We show that by iteratively and interactively feeding contrastive samples, the generator enhances the semi-supervised learning performance of the discriminator (e.g. Ladder Network), in section 4.2."
    }, {
      "heading" : "C APPENDIX: EXPERIMENT SETTINGS",
      "text" : "MORE DETAILS ABOUT THE GRID SEARCH\nFor training both EBGANs and GANs for the grid search, we use the following setting:\n• Batch normalization (Ioffe & Szegedy, 2015) is applied after each weight layer, except for the generator output layer and the discriminator input layer (Radford et al., 2015). • Training images are scaled into range [-1,1]. Correspondingly the generator output layer is followed by a Tanh function. • ReLU is used as the non-linearity function. • Initialization: the weights in D fromN (0, 0.002) and in G fromN (0, 0.02). The bias are initial-\nized to be 0.\nWe evaluate the models from the grid search by calculating a modified version of the inception score, I ′ = ExKL(p(y)||p(y|x)), where x denotes a generated sample and y is the label predicted by a MNIST classifier that is trained off-line using the entire MNIST training set. Two main changes were made upon its original form: (i)-we swap the order of the distribution pair; (ii)-we omit the e(·) operation. The modified score condenses the histogram in figure 2 and figure 3. It is also worth noting that although we inherit the name “inception score” from Salimans et al. (2016), the evaluation isn’t related to the “inception” model trained on ImageNet dataset. The classifier is a regular 3-layer ConvNet trained on MNIST.\nThe generations showed in figure 4 are the best GAN or EBGAN (obtaining the best I ′ score) from the grid search. Their configurations are:\n• figure 4(a): nLayerG=5, nLayerD=2, sizeG=1600, sizeD=1024, dropoutD=0, optimD=SGD, optimG=SGD, lr=0.01. • figure 4(b): nLayerG=5, nLayerD=2, sizeG=800, sizeD=1024, dropoutD=0, optimD=ADAM, optimG=ADAM, lr=0.001, margin=10. • figure 4(c): same as (b), with λPT = 0.1.\nLSUN & CELEBA\nWe use a deep convolutional generator analogous to DCGAN’s and a deep convolutional autoencoder for the discriminator. The auto-encoder is composed of strided convolution modules in the feedforward pathway and fractional-strided convolution modules in the feedback pathway. We leave the usage of upsampling or switches-unpooling (Zhao et al., 2015) to future research. We also followed the guidance suggested by Radford et al. (2015) for training EBGANs. The configuration of the deep auto-encoder is:\n• Encoder: (64)4c2s-(128)4c2s-(256)4c2s • Decoder: (128)4c2s-(64)4c2s-(3)4c2s\nwhere “(64)4c2s” denotes a convolution/deconvolution layer with 64 output feature maps and kernel size 4 with stride 2. The margin m is set to 80 for LSUN and 20 for CelebA.\nIMAGENET\nWe built deeper models in both 128×128 and 256×256 experiments, in a similar fashion to section 4.3,\n• 128× 128 model: – Generator: (1024)4c-(512)4c2s-(256)4c2s-(128)4c2s(64)4c2s-(64)4c2s-(3)3c\n– Noise #planes: 100-64-32-16-8-4 – Encoder: (64)4c2s-(128)4c2s-(256)4c2s-(512)4c2s – Decoder: (256)4c2s-(128)4c2s-(64)4c2s-(3)4c2s – Margin: 40\n• 256× 256 model: – Generator: (2048)4c-(1024)4c2s-(512)4c2s-(256)4c2s-(128)4c2s(64)4c2s-(64)4c2s-(3)3c\n– Noise #planes: 100-64-32-16-8-4-2 – Encoder: (64)4c2s-(128)4c2s-(256)4c2s-(512)4c2s – Decoder: (256)4c2s-(128)4c2s-(64)4c2s-(3)4c2s – Margin: 80\nNote that we feed noise into every layer of the generator where each noise component is initialized into a 4D tensor and concatenated with current feature maps in the feature space. Such strategy is also employed by Salimans et al. (2016)."
    }, {
      "heading" : "D APPENDIX: SEMI-SUPERVISED LEARNING EXPERIMENT SETTING",
      "text" : "BASELINE MODEL\nAs stated in section 4.2, we chose a bottom-layer-cost Ladder Network as our baseline model. Specifically, we utilize an identical architecture as reported in both papers (Rasmus et al., 2015; Pezeshki et al., 2015); namely a fully-connected network of size 784-1000-500-250-250-250, with batch normalization and ReLU following each linear layer. To obtain a strong baseline, we tuned the weight of the reconstruction cost with values from the set { 5000784 , 2000 784 , 1000 784 , 500 784}, while fixing the weight on the classification cost to 1. In the meantime, we also tuned the learning rate with values {0.002, 0.001, 0.0005, 0.0002, 0.0001}. We adopted Adam as the optimizer with β1 being set to 0.5. The minibatch size was set to 100. All the experiments are finished by 120,000 steps. We use the same learning rate decay mechanism as in the published papers – starting from the two-thirds of total steps (i.e., from step #80,000) to linearly decay the learning rate to 0. The result reported in section 4.2 was done by the best tuned setting: λL2 = 1000 784 , lr = 0.0002.\nEBGAN-LN MODEL\nWe place the same Ladder Network architecture into our EBGAN framework and train this EBGANLN model the same way as we train the EBGAN auto-encoder model. For technical details, we started training the EBGAN-LN model from the margin value 16 and gradually decay it to 0 within the first 60,000 steps. By the time, we found that the reconstruction error of the real image had already been low and reached the limitation of the architecture (Ladder Network itself); besides the generated images exhibit good quality (shown in figure 10). Thereafter we turned off training the generator but kept training the discriminator for another 120,000 steps. We set the initial learning rates to be 0.0005 for discriminator and 0.00025 for generator. The other setting is kept consistent with the best baseline LN model. The learning rate decay started at step #120,000 (also two-thirds of the total steps).\nOTHER DETAILS\n• Notice that we used the 28×28 version (unpadded) of the MNIST dataset in the EBGAN-LN experiment. For the EBGAN auto-encoder grid search experiments, we used the zero-padded version, i.e., size 32×32. No phenomenal difference has been found due to the zero-padding. • We generally took the `2 norm of the discrepancy between input and reconstruction for the loss term in the EBGAN auto-encoder model as formally written in section 2.1. However, for the EBGAN-LN experiment, we followed the original implementation of Ladder Network using a vanilla form of `2 loss. • Borrowed from Salimans et al. (2016), the batch normalization is adopted without the learned parameter γ but merely with a bias term β. It still remains unknown whether such trick could affect learning in some non-ignorable way, so this might have made our baseline model not a strict reproduction of the published models by Rasmus et al. (2015) and Pezeshki et al. (2015)."
    }, {
      "heading" : "E APPENDIX: TIPS FOR SETTING A GOOD ENERGY MARGIN VALUE",
      "text" : "It is crucial to set a proper energy margin valuem in the framework of EBGAN, from both theoretical and experimental perspective. Hereby we provide a few tips:\n• Delving into the formulation of the discriminator loss made by equation 1, we suggest a numerical balance between its two terms which concern real and fake sample respectively. The second term is apparently bounded by [0,m] (assuming the energy function D(x) is non-negative). It is desirable to make the first term bounded in a similar range. In theory, the upper bound of the first term is essentially determined by (i)-the capacity of D; (ii)-the complexity of the dataset. • In practice, for the EBGAN auto-encoder model, one can run D (the auto-encoder) alone on the real sample dataset and monitor the loss. When it converges, the consequential loss implies a rough limit on how well such setting of D is capable to fit the dataset. This usually suggests a good start for a hyper-parameter searching on m. • m being overly large results in a training instability/difficulty, while m being too small is prone to the mode-dropping problem. This property of m is depicted in figure 9. • One successful technique, as we introduced in appendix D, is to start from a large m and gradually decayed it to 0 along training proceeds. Unlike the feature matching semisupervised learning technique proposed by Salimans et al. (2016), we show in figure 10 that not only does the EBGAN-LN model achieve a good semi-supervised learning performance, it also produces satisfactory generations.\nAbstracting away from the practical experimental tips, the theoretical understanding of EBGAN in section 2.2 also provides some insight for setting a feasible m. For instance, as implied by Theorem 2, setting a large m results in a broader range of γ to which D∗(x) may converge. Instability may come after an overly large γ because it generates two strong gradients pointing to opposite directions, from loss 1, which would demand more finicky optimization setting."
    }, {
      "heading" : "F APPENDIX: MORE GENERATION",
      "text" : "LSUN AUGMENTED VERSION TRAINING\nFor LSUN bedroom dataset, aside from the experiment on the whole images, we also train an EBGAN auto-encoder model based on dataset augmentation by cropping patches. All the patches are of size 64× 64 and cropped from 96× 96 original images. The generation is shown in figure 11.\nCOMPARISON OF EBGANS AND EBGAN-PTS\nTo further demonstrate how the pull-away term (PT) may influence EBGAN auto-encoder model training, we chose both the whole-image and augmented-patch version of the LSUN bedroom dataset, together with the CelebA dataset to make some further experimentation. The comparison of EBGAN and EBGAN-PT generation are showed in figure 12, figure 13 and figure 14. Note\nthat all comparison pairs adopt identical architectural and hyper-parameter setting as in section 4.3. The cost weight on the PT is set to 0.1."
    } ],
    "references" : [ {
      "title" : "On contrastive divergence learning",
      "author" : [ "Carreira-Perpinan", "Miguel A", "Hinton", "Geoffrey" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Carreira.Perpinan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carreira.Perpinan et al\\.",
      "year" : 2005
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Denton", "Emily L", "Chintala", "Soumith", "Fergus", "Rob" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating images with recurrent adversarial networks",
      "author" : [ "Im", "Daniel Jiwoong", "Kim", "Chris Dongjoo", "Jiang", "Hui", "Memisevic", "Roland" ],
      "venue" : "arXiv preprint arXiv:1602.05110,",
      "citeRegEx" : "Im et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Im et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning convolutional feature hierarchies for visual recognition",
      "author" : [ "Kavukcuoglu", "Koray", "Sermanet", "Pierre", "Boureau", "Y-Lan", "Gregor", "Karol", "Mathieu", "Michaël", "Cun", "Yann L" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Kavukcuoglu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kavukcuoglu et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep directed generative models with energy-based probability estimation",
      "author" : [ "Kim", "Taesup", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1606.03439,",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient learning of sparse representations with an energy-based model",
      "author" : [ "MarcAurelio Ranzato", "Christopher Poultney", "Chopra", "Sumit" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2007
    }, {
      "title" : "Deep multi-scale video prediction beyond mean square error",
      "author" : [ "Mathieu", "Michael", "Couprie", "Camille", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1511.05440,",
      "citeRegEx" : "Mathieu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mathieu et al\\.",
      "year" : 2015
    }, {
      "title" : "Deconstructing the ladder network architecture",
      "author" : [ "Pezeshki", "Mohammad", "Fan", "Linxi", "Brakel", "Philemon", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1511.06430,",
      "citeRegEx" : "Pezeshki et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pezeshki et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "A unified energy-based framework for unsupervised learning",
      "author" : [ "Ranzato", "Marc’Aurelio", "Boureau", "Y-Lan", "Chopra", "Sumit", "LeCun", "Yann" ],
      "venue" : "In Proc. Conference on AI and Statistics (AI-Stats),",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2007
    }, {
      "title" : "Semi-supervised learning with ladder networks",
      "author" : [ "Rasmus", "Antti", "Berglund", "Mathias", "Honkala", "Mikko", "Valpola", "Harri", "Raiko", "Tapani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Contractive auto-encoders: Explicit invariance during feature extraction",
      "author" : [ "Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of the 28th international conference on machine learning",
      "citeRegEx" : "Rifai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rifai et al\\.",
      "year" : 2011
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi" ],
      "venue" : "arXiv preprint arXiv:1606.03498,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Timothy", "Kavukcuoglu", "Koray", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1606.04080,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
      "author" : [ "Yu", "Fisher", "Seff", "Ari", "Zhang", "Yinda", "Song", "Shuran", "Funkhouser", "Thomas", "Xiao", "Jianxiong" ],
      "venue" : "arXiv preprint arXiv:1506.03365,",
      "citeRegEx" : "Yu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    }, {
      "title" : "Stacked what-where auto-encoders",
      "author" : [ "Zhao", "Junbo", "Mathieu", "Michael", "Goroshin", "Ross", "Lecun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1506.02351,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "ExKL(p(y)||p(y|x)), where x denotes a generated sample and y is the label predicted by a MNIST classifier that is trained off-line using the entire MNIST training set. Two main changes were made upon its original form: (i)-we swap the order of the distribution pair; (ii)-we omit the e(·) operation. The modified score condenses the histogram in figure 2 and figure 3",
      "author" : [ "Salimans" ],
      "venue" : null,
      "citeRegEx" : "Salimans,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) have led to significant improvements in image generation (Denton et al.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : ", 2014) have led to significant improvements in image generation (Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016), video prediction (Mathieu et al.",
      "startOffset" : 65,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : ", 2014) have led to significant improvements in image generation (Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016), video prediction (Mathieu et al.",
      "startOffset" : 65,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : ", 2014) have led to significant improvements in image generation (Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016), video prediction (Mathieu et al.",
      "startOffset" : 65,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : ", 2014) have led to significant improvements in image generation (Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016), video prediction (Mathieu et al.",
      "startOffset" : 65,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : ", 2016), video prediction (Mathieu et al., 2015) and a number of other domains.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2014) have led to significant improvements in image generation (Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016), video prediction (Mathieu et al., 2015) and a number of other domains. The basic idea of GAN is to simultaneously train a discriminator and a generator. The discriminator is trained to distinguish real samples of a dataset from fake samples produced by the generator. The generator uses input from an easy-to-sample random source, and is trained to produce fake samples that the discriminator cannot distinguish from real data samples. During training, the generator receives the gradient of the output of the discriminator with respect to the fake sample. In the original formulation of GAN in Goodfellow et al. (2014), the discriminator produces a probability and, under certain conditions, convergence occurs when the distribution produced by the generator matches the data distribution.",
      "startOffset" : 66,
      "endOffset" : 770
    }, {
      "referenceID" : 9,
      "context" : "(2006) for the supervised and weakly supervised settings, and Ranzato et al. (2007) for unsupervised learning.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Similarly to what has been done with the probabilistic GAN (Goodfellow et al., 2014), we use a two different losses, one to train D and the other to train G, in order to get better quality gradients when the generator is far from convergence.",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "Theoretical and experimental results have addressed this issue by regularizing the latent representations (Vincent et al., 2010; Rifai et al., 2011; MarcAurelio Ranzato & Chopra, 2007; Kavukcuoglu et al., 2010).",
      "startOffset" : 106,
      "endOffset" : 210
    }, {
      "referenceID" : 15,
      "context" : "Theoretical and experimental results have addressed this issue by regularizing the latent representations (Vincent et al., 2010; Rifai et al., 2011; MarcAurelio Ranzato & Chopra, 2007; Kavukcuoglu et al., 2010).",
      "startOffset" : 106,
      "endOffset" : 210
    }, {
      "referenceID" : 5,
      "context" : "Theoretical and experimental results have addressed this issue by regularizing the latent representations (Vincent et al., 2010; Rifai et al., 2011; MarcAurelio Ranzato & Chopra, 2007; Kavukcuoglu et al., 2010).",
      "startOffset" : 106,
      "endOffset" : 210
    }, {
      "referenceID" : 16,
      "context" : "Another technique “minibatch discrimination” was developed by Salimans et al. (2016) from the same philosophy.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "It is inspired by the prior work showing the representational power of the encoder in the auto-encoder alike model such as Rasmus et al. (2015) and Zhao et al.",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 14,
      "context" : "It is inspired by the prior work showing the representational power of the encoder in the auto-encoder alike model such as Rasmus et al. (2015) and Zhao et al. (2015). The rationale for choosing the cosine similarity instead of Euclidean distance is to make the term bounded below and invariant to scale.",
      "startOffset" : 123,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "On this direction, the approaches studying contrastive samples are relevant to EBGAN, such as the use of noisy samples (Vincent et al., 2010) and noisy gradient descent methods like contrastive divergence (Carreira-Perpinan & Hinton, 2005).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "From the perspective of GANs, several papers were presented to improve the stability of GAN training, (Salimans et al., 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "From the perspective of GANs, several papers were presented to improve the stability of GAN training, (Salimans et al., 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 207
    }, {
      "referenceID" : 12,
      "context" : "From the perspective of GANs, several papers were presented to improve the stability of GAN training, (Salimans et al., 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "From the perspective of GANs, several papers were presented to improve the stability of GAN training, (Salimans et al., 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 207
    }, {
      "referenceID" : 10,
      "context" : "From the perspective of GANs, several papers were presented to improve the stability of GAN training, (Salimans et al., 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : ", 2016; Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Mathieu et al., 2015). Kim & Bengio (2016) propose a probabilistic GAN and cast it into an energy-based density estimator by using the Gibbs distribution.",
      "startOffset" : 8,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "To analyze the results, we use the inception score (Salimans et al., 2016) as a numerical means reflecting the generation quality.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "layer-cost Ladder Network (LN) (Rasmus et al., 2015) with the EGBAN framework (EBGAN-LN).",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "layer-cost Ladder Network (LN) (Rasmus et al., 2015) with the EGBAN framework (EBGAN-LN). Ladder Network can be categorized as an energy-based model that is built with both feedforward and feedback hierarchies powered by stage-wise lateral connections coupling two pathways. One technique we found crucial in enabling EBGAN framework for semi-supervised learning is to gradually decay the margin value m of the equation 1. The rationale behind is to let discriminator punish generator less when pG gets closer to the data manifold. One can think of the extreme case where the contrastive samples are exactly pinned on the data manifold, such that they are “not contrastive anymore”. This ultimate status happens when m = 0 and the EBGAN-LN model falls back to a normal Ladder Network. The undesirability of a non-decay dynamics for using the discriminator in the GAN or EBGAN framework is also indicated by Theorem 2: on convergence, the discriminator reflects a flat energy surface. However, we posit that the trajectory of learning a EBGAN-LN model does provide the LN (discriminator) more information by letting it see contrastive samples. Yet the optimal way to avoid the mentioned undesirability is to make sure m has been decayed to 0 when the Nash Equilibrium is reached. The margin decaying schedule is found by hyper-parameter search in our experiments (technical details in appendix D). From table 2, it shows that positioning a bottom-layer-cost LN into an EBGAN framework profitably improves the performance of the LN itself. We postulate that within the scope of the EBGAN framework, iteratively feeding the adversarial contrastive samples produced by the generator to the energy function acts as an effective regularizer; the contrastive samples can be thought as an extension to the dataset that provides more information to the classifier. We notice there was a discrepancy between the reported results between Rasmus et al. (2015) and Pezeshki et al.",
      "startOffset" : 32,
      "endOffset" : 1948
    }, {
      "referenceID" : 11,
      "context" : "(2015) and Pezeshki et al. (2015), so we report both results along with our own implementation of the Ladder Network running the same setting.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "model 100 200 1000 LN bottom-layer-cost, reported in Pezeshki et al. (2015) 1.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "model 100 200 1000 LN bottom-layer-cost, reported in Pezeshki et al. (2015) 1.69±0.18 - 1.05±0.02 LN bottom-layer-cost, reported in Rasmus et al. (2015) 1.",
      "startOffset" : 53,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "We apply the EBGAN framework with deep convolutional architecture to generate 64 × 64 RGB images, a more realistic task, using the LSUN bedroom dataset (Yu et al., 2015) and the large-scale face dataset CelebA under alignment (Liu et al.",
      "startOffset" : 152,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : ", 2015) and the large-scale face dataset CelebA under alignment (Liu et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "To compare EBGANs with DCGANs (Radford et al., 2015), we train a DCGAN model under the same configuration and show its generation side-by-side with the EBGAN model, in figures 5 and 6.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "We also trained a network to generate images of size 256 × 256, on a dog-breed subset of ImageNet, using the wordNet IDs provided by Vinyals et al. (2016). The results are shown in figures 7 and 8.",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "For the future work, the conditional setting (Denton et al., 2015; Mathieu et al., 2015) is a promising setup to explore.",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "For the future work, the conditional setting (Denton et al., 2015; Mathieu et al., 2015) is a promising setup to explore.",
      "startOffset" : 45,
      "endOffset" : 88
    } ],
    "year" : 2017,
    "abstractText" : "We introduce the “Energy-based Generative Adversarial Network” model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
    "creator" : "LaTeX with hyperref package"
  }
}