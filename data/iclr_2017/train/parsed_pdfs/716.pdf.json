{
  "name" : "716.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ENET: A DEEP NEURAL NETWORK ARCHITECTURE FOR REAL-TIME SEMANTIC SEGMENTATION",
    "authors" : [ "Adam Paszke" ],
    "emails" : [ "a.paszke@students.mimuw.edu.pl", "euge@purdue.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recent interest in augmented reality wearables, home-automation devices, and self-driving vehicles has created a strong need for semantic-segmentation (or visual scene-understanding) algorithms that can operate in real-time on low-power mobile devices. These algorithms label each and every pixel in the image with one of the object classes. In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al. (2015a)) surpass the performance of many conventional computer vision algorithms (Shotton et al. (2009); Perronnin et al. (2010); van de Sande et al. (2011)). Even though CNNs are increasingly successful at classification and categorization tasks, they provide coarse spatial results when applied to pixel-wise labeling of large images. Therefore, they are often cascaded with other algorithms to refine the results, such as color based segmentation (Farabet et al. (2013)) or conditional random fields (Chen et al. (2014)), to name a few.\nIn order to both spatially classify and finely segment images, several neural network architectures have been proposed, such as SegNet (Badrinarayanan et al. (2015a;b)) or fully convolutional networks (Long et al. (2015)). All these works are based on a VGG16 (Simonyan & Zisserman (2014b)) architecture, which is a very large model designed for multi-class classification. These references use models with a large number of parameters, and slow inference time. In these conditions, they become unusable for many mobile or battery-powered applications, which require processing images at rates higher than 10 fps.\nIn this paper, we propose a new neural network architecture optimized for high-accuracy and also fast inference. In our work, beside neural network processing, we chose not to use any other post-processing steps, in order to focus on the intrinsic performance of an end-to-end CNN approach.\nIn Section 3 we propose a fast and compact encoder-decoder architecture named ENet. It has been designed according to rules and ideas that have appeared in the literature recently, all of which we discuss in Section 4. Performance of the proposed network has been tested on Cityscapes (Cordts et al. (2016)) and CamVid (Brostow et al. (2008)) for driving scenario, whereas SUN dataset (Song et al. (2015)) has been used for testing our network in an indoor situation. We benchmark it on NVIDIA Jetson TX1 Embedded Systems Module as well as on an NVIDIA Titan X GPU. The results can be found in Section 5."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Semantic segmentation is important in fully understanding the content of images, find target objects and segment them. This technique is of utmost importance in applications such as driving and augmented reality. Moreover, real-time operation is a must for these applications, and therefore, designing CNNs carefully is vital. Contemporary computer vision applications extensively use deep neural networks, now one of the most widely used techniques for many different tasks, including semantic segmentation. This work presents a fully trainable neural network architecture, and therefore we aim to compare to other literature that performs the large majority of inference in the same way.\nState-of-the-art scen-parsing CNNs use two separate neural network architectures combined together: an encoder and a decoder. Inspired by probabilistic auto-encoders (Ranzato et al. (2007); Ngiam et al. (2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)). However, these networks are slow during inference due to their large architectures and numerous parameters. Unlike in Noh et al. (2015), fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of operations and memory footprint, making it the smallest of these networks. Still, none of them can operate in real-time.\nOther existing architectures use simpler classifiers and then cascade it with Conditional Random Field (CRF) as a post-processing step (Chen et al. (2014); Sturgess et al. (2009)). As explained in Badrinarayanan et al. (2015b), these techniques use onerous post-processing steps and often fail to label the classes that occupy fewer number of pixels in a frame. CNNs can be combined with recurrent neural networks (Zheng et al. (2015)) for better performance, but suffers from further speed degradation. Also, one has to keep in mind that RNN, used as a post-processing step, can be used in conjunction with any other technique, including the one presented in this work."
    }, {
      "heading" : "3 NETWORK ARCHITECTURE",
      "text" : "The architecture of our network is presented in Table 1. It is divided into several stages, as highlighted by horizontal lines in the table and the first digit after each block name. Output sizes are reported for an example input image resolution of 512× 512. We adopt a view of ResNets (He et al. (2015b)) that describes them as having a single main branch and extensions with convolutional filters that separate from it, and then merge back with an element-wise addition, as shown in Figure 1b. Just as in the original paper, we refer to these as bottleneck modules. They consist of three convolutional layers: a 1×1 projection that reduces the dimensionality, a main convolutional layer (conv in Figure 1b), and a 1× 1 expansion. If the bottleneck is downsampling, a max pooling layer is added to the main branch. We zero pad the activations, to match the number of feature maps. Also, the first 1× 1 projection is replaced with a 2× 2 convolution with stride 2 in both dimensions. conv is either a regular, dilated or full convolution (also known as deconvolution or fractionally strided convolution) with 3 × 3 filters. Sometimes we replace it with asymmetric convolution i.e. a sequence of 5 × 1 and 1× 5 convolutions. For the regularizer, we use Spatial Dropout (Tompson et al. (2015)), with p = 0.01 before bottleneck2.0, and p = 0.1 afterwards.\nThe initial stage contains a single block, that is presented in Figure 1a. Stage 1 consists of 5 bottleneck blocks, while stage 2 and 3 have the same structure, with the exception that stage 3 does not downsample the input at the beginning (we omit the 0th bottleneck). These three first stages are the encoder. Stage 4 and 5 belong to the decoder.\nWe did not use bias terms in any of the projections, in order to reduce the number of kernel calls and overall memory operations, as cuDNN (Chetlur et al. (2014)) uses separate kernels for convolutions and bias addition. This choice didn’t have any impact on the accuracy. Between each convolutional layer and following non-linearity we use Batch Normalization (Ioffe & Szegedy (2015)). In the decoder max pooling is replaced with max unpooling, and padding is replaced with spatial convolution without bias. We did not use unpooling information in the last upsampling module, because the initial block operated on the 3 channels of the input frame, while the final output has C feature maps (the number of object classes). Also, for performance reasons, we decided to place only a bare full convolution as last module of the network, which alone takes up a sizeable portion of the decoder processing time."
    }, {
      "heading" : "4 DESIGN CHOICES",
      "text" : "In this section we will discuss our most important experimental results and intuitions, that have shaped the final architecture of ENet.\nFeature map resolution: Downsampling images during semantic segmentation has two main drawbacks. Firstly, reducing feature map resolution implies loss of spatial information like exact edge shape. Secondly, full pixel segmentation requires that the output has the same resolution as the input. This implies that strong downsampling will require equally strong upsampling, which increase model size and computational cost. The first issue has been addressed in Long et al. (2015) by adding the feature maps produced by encoder, and in SegNet (Badrinarayanan et al. (2015a)) by saving the elements index from the corresponding encoder max-pooling module. We followed the SegNet approach, because it allows to reduce memory requirements, but we found that using a strong downsampling still reduces the final accuracy.\nHowever, downsampling has one big advantage. Filters operating on downsampled images have a bigger receptive field, that allows them to gather more context. This is especially important when trying to differentiate between classes occupying a small portion of the overall image, as, for example, rider and pedestrian in a road scene. It is just not enough that the network learns how people look, the context in which they appear is important as well. At the end, we have found that it is better to use dilated convolutions for the purpose of extending context information (Yu & Koltun (2015)).\nEarly downsampling: One crucial intuition to achieving good performance and real-time operation is realizing that processing large input frames is very expensive. This might sound very obvious, however many popular architectures (Hong et al. (2015); Badrinarayanan et al. (2015b)) do not pay much attention towards optimization of early stages of network, which are often the most expensive.\nENet first two blocks heavily reduce the input size, and use only a small set of feature maps. The idea behind it, is that visual information is highly redundant in space, and thus can be compressed into a more efficient representation. Also, our intuition is that the initial network layers should not be used specifilly only for classification. Instead, they should rather act as good feature extractors and preprocess the input for later portions of the network. This insight worked well in our experiments. Increasing the number of feature maps from 16 to 32 did not improve the accuracy on Cityscapes dataset (Cordts et al. (2016)).\nDecoder size: In this work we would like to provide a different view on encoder-decoder architectures than the one presented in Badrinarayanan et al. (2015b). SegNet is a very symmetric architecture, as the encoder is an exact mirror of the encoder. Instead, our architecture consists of a large encoder, and a small decoder. This is motivated by the idea that the encoder should be able to work in a similar fashion to original classification architectures, i.e. to operate on smaller resolution data and provide for information processing and filtering. Instead, the role of the the decoder, is only to upsample the output of the encoder, fine-tuning the details.\nNonlinear operations: He et al. (2016) report that it is beneficial to use ReLU and Batch Normalization layers before convolutions. We tried applying these ideas to ENet, but this had a detrimental effect on accuracy. Investigating its cause we replaced all ReLUs in the network with PReLUs (He et al. (2015a)), which use an additional parameter per feature map, with the goal of learning the negative slope of non-linearities. We expected that in layers where identity is a preferable transfer function, PReLU weights will have values around 1, and conversely, values around 0 if ReLU is preferable. Results of this experiment can be seen in Figure 2.\nThe first layers weights exhibit a large variance and are slightly biased towards positive values, while in the later portions of the encoder they settle to recurring pattern. All layers in the main branch behave nearly exactly like regular ReLUs, while the weights inside bottleneck modules are negative i.e. the function inverts and scales down negative values. We hypothesize that identity did not work out well in our architecture because of its limited depth. We hypothesize that the reason why such lossy functions are learned is that He et al. (2016) uses networks that are hundreds of layers deep, while our network uses fewer layers, and it needs to quickly filter out information. It is notable that the decoder weights become much more positive and learn functions closer to identity. This confirms our intuitions that the decoder is used only to fine-tune the upsampled output.\nInformation-preserving dimensionality changes: As stated earlier, it is necessary to downsample the input early, but aggressive dimensionality reduction can also hinder the information flow. A very good approach to this problem has been presented in Szegedy et al. (2015b). However, pooling after a convolution, in case of increasing feature map depth, is computationally expensive. Therefore, we prefer to perform pooling operation in parallel with convolution of stride 2, and concatenate resulting feature maps. This technique allowed us to speed up inference time of the initial block 10 times.\nAdditionally, we have found one problem in the original ResNet architecture. When downsampling, the first 1×1 projection of the convolutional branch is performed with a stride of 2 in both dimensions, which effectively discards 75% of the input. Increasing the filter size to 2× 2 allows to take the full input into consideration, and thus improves the information flow and accuracy. Of course, it makes these layers 4× more computationally expensive, however there are so few of these in ENet, that the overhead is not noticeable.\nFactorizing filters: It has been shown that convolutional weights have a fair amount of redundancy, and each n× n convolution can be decomposed into two smaller asymmetric convolutions: one with a n × 1 filter followed by a 1 × n filter (Jin et al. (2014); Szegedy et al. (2015b)). We have used asymmetric convolutions with n = 5 in our network, so cost of these two operations is similar to a single 3× 3 convolution. This allowed to increase the variety of functions learned by each block and increase the receptive field.\nSequence of operations used in the bottleneck module (projection, convolution, projection) can be seen as decomposing one large convolutional layer into a series of smaller and simpler low-rank approximation operations. Such factorization allows for large speedups and reduction in number of parameters, making them less redundant (Jin et al. (2014)).\nDilated convolutions: As argued above, it is very important for the network to have a wide receptive field, so it can perform classification by taking a bigger portion of the image (context) into account. We wanted to avoid overly downsampling the feature maps, and decided to use dilated convolutions (Yu & Koltun (2015)) to improve our model. We have used them inside several bottleneck modules, in particular the ones that operate on the smallest resolutions. These gave a significant accuracy boost, by raising IoU on Cityscapes by around 4 percentage points, with no additional cost. We obtained the best accuracy when we interleaved them with other bottleneck modules (both regular and asymmetric), instead of arranging them in sequence, as has been done in Yu & Koltun (2015).\nRegularization: Most pixel-wise segmentation datasets are relatively small (on order of 103 images), so such expressive models as neural networks quickly begin to overfit them. In initial experiments, we used L2 weight decay with little success. Then, inspired by Huang et al. (2016), we have tried stochastic depth, which increased accuracy. However it became apparent that dropping branches (i.e. setting their output to 0) is in fact a special case of applying Spatial Dropout (Tompson et al. (2015)), where either all of the channels, or none of them are ignored, instead of selecting a random subset. We placed Spatial Dropout at the end of convolutional branches, right before the addition, and it turned out to work much better than stochastic depth."
    }, {
      "heading" : "5 RESULTS",
      "text" : "We benchmarked the performance of ENet on three different datasets to demonstrate real-time and accurate for practical applications. We tested on CamVid and Cityscapes datasets of road scenes, and SUN RGB-D dataset of indoor scenes. We set SegNet (Badrinarayanan et al. (2015b)) as a baseline since it is one of the fastest segmentation model available, that also requires less memory to operate than plain CNNs. All our models, training, testing and performance evaluation scripts were written using the Torch7 machine-learning library. To compare results, we use class average accuracy and intersection-over-union (IoU) metrics."
    }, {
      "heading" : "5.1 PERFORMANCE ANALYSIS",
      "text" : "We report results on inference speed on widely used NVIDIA Titan X GPU as well as on NVIDIA TX1 embedded system module. ENet was designed to achieve more than 10 fps on the NVIDIA TX1 board with an input image size 640× 360 (W,H), which is adequate for practical road scene parsing applications. For inference we merge batch normalization and dropout layers into the convolutional filters, to speed up all networks.\nInference time: Table 2 compares inference time for a single input frame of varying resolution. We also report the number of frames per second that can be processed. Dashes indicate that we could not obtain a measurement, due to lack of memory. ENet is significantly faster than competing architectures, providing high frame rates for real-time applications and allowing for practical use of very deep neural network models with encoder-decoder architecture.\nHardware requirements: Table 3 reports a comparison of number of operations and parameters used by different models. ENet efficiency is evident in the much low number of operations per frame and overall parameters. Please note that we report storage required to store the models in half precision floating point format. ENet has so few parameters that it can be saved into a file of just 0.7MB, which makes it possible to fit the whole network in an extremely fast on-chip memory in embedded processors. This alleviates the need for model compression (Han et al. (2015)), making it possible to use general purpose code for neural network computation. However, if one needs to operate under incredibly strict memory constraints, these techniques can still be applied to ENet as well.\nSoftware limitations: One of the most important techniques that has allowed us to reach these levels of performance is convolutional layer factorization. However, we have found one surprising drawback. Although applying this method allowed us to greatly reduce the number of floating point operations and parameters, it also increased the number of individual kernels calls, making each of them smaller.\nWe have found that some of these operations become so cheap, that the cost of GPU kernel launch starts to outweigh the cost of the actual computation. Also, because kernels do not have access to values that have been kept in registers by previous ones, they have to load all data from global memory at launch, and save it when their work is finished. This means that using a higher number of kernels, increases the number of memory operations, because feature maps have to be constantly saved and reloaded. This becomes especially apparent in case of non-linear operations. In ENet, PReLUs consume more than a quarter of inference time. Since they are only simple point-wise operations and very easy to parallelize, we hypothesize it is caused by the aforementioned data movement.\nThese are serious limitations, however they could be resolved by performing kernel fusion in existing software i.e. create kernels that apply non-linearities to results of convolutions directly, or perform a\nnumber of smaller convolutions in one call. This improvement in GPU libraries, such as CuDNN, could increase the speed and efficiency of our network even further."
    }, {
      "heading" : "5.2 BENCHMARKS",
      "text" : "During training we have used the Adam optimization algorithm (Kingma & Ba (2014)). It allowed ENet to converge very quickly and on every dataset we haver used training took only 3-4 hours on Titan X. Training of ENet was performed in two stages: first we train only the encoder to categorize downsampled regions of the input image, then we appended the decoder and train the network to perform upsampling and pixel-wise categorization. in this work, a learning rate of 5e−4 and L2 weight decay of 2e−4, along with batch size of 10 consistently provided the best results. For categorization, we have used a custom class weighing scheme defined as wclass = 1ln(c+pclass) . In contrast to the inverse class probability weighing, the weights are bounded as the probability approaches 0. c is an additional hyper-parameter, which we set to 1.02 (i.e. we restrict the class weights to be in the interval of [1, 50]).\nCityscapes: This dataset consists of 5000 fine-annotated images, out of which 2975 are available for training, 500 for validation, and the remaining 1525 have been selected as test set (Cordts et al. (2016)). Cityscapes was the most important benchmark for us, because of its outstanding quality and highly varying road scenarios, often featuring many pedestrians and cyclists. We trained on 19 classes that have been selected in the official evaluation scripts (Cordts et al. (2016)). It makes use of an additional metric called instance-level intersection over union metric (iIoU), which is IoU weighed by the average object size. As reported in Table 4, ENet outperforms SegNet in class IoU and iIoU, as well as in category IoU. ENet is currently the fastest model in the Cityscapes benchmark.\nCamVid: Another automotive dataset, on which we have tested ENet, was CamVid. It contains 367 training and 233 testing images (Brostow et al. (2008)). There are eleven different classes such as building, tree, sky, car, road, etc. while the twelfth class contains unlabeled data, which we ignore while training. The original frame resolution for this dataset is 960×720 (W,H) but we downsampled the images to 480×360 before training. In Table 5 we compare the performance of ENet with existing state-of-the-art algorithms. ENet outperforms other models in six classes, which are difficult to learn because they correspond to smaller objects.\nSUN RGB-D: The SUN dataset consists of 5285 training images and 5050 testing images with 37 indoor object classes. We did not make any use of depth information in this work and trained the network only on RGB data. In Table 6 we compare the performance of ENet with SegNet (Badrinarayanan et al. (2015b)), which is the only neural network model that reports accuracy on this dataset. Our results, though inferior in global average accuracy and IoU, are comparable in class average accuracy. Since global average accuracy and IoU are metrics that favor correct classification of classes occupying large image patches, researchers generally emphasize the importance of other metrics in case of semantic segmentation. One notable example is introduction of iIoU metric (Cordts et al. (2016)). Comparable result in class average accuracy indicates, that our network is capable of differentiating smaller objects nearly as well as SegNet. Moreover, the difference in accuracy should not overshadow the huge performance gap between these two networks. ENet can process the images in real-time, and is nearly 20× faster than SegNet on embedded platforms."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We have proposed a novel neural network architecture designed from the ground up specifically for semantic segmentation. Our main aim is to make efficient use of scarce resources available on embedded platforms, compared to fully fledged deep learning workstations. Our work provides large gains in this task, while matching and at times exceeding existing baseline models, that have an order of magnitude larger computational and memory requirements. The application of ENet on the NVIDIA TX1 hardware exemplifies real-time portable embedded solutions.\nEven though the main goal was to run the network on mobile devices, we have found that it is also very efficient on high end GPUs like NVIDIA Titan X. This may prove useful in data-center applications, where there is a need of processing large numbers of high resolution images. ENet allows to perform large-scale computations in a much faster and more efficient manner, which might lead to significant savings."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work is partly supported by the Office of Naval Research (ONR) grants N00014-12-1-0167, N00014-15-1-2791 and MURI N00014-10-1-0278. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the TX1, Titan X, K40 GPUs used for this research."
    } ],
    "references" : [ {
      "title" : "Segnet: A deep convolutional encoderdecoder architecture for robust semantic pixel-wise labelling",
      "author" : [ "Vijay Badrinarayanan", "Ankur Handa", "Roberto Cipolla" ],
      "venue" : "arXiv preprint arXiv:1505.07293,",
      "citeRegEx" : "Badrinarayanan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Badrinarayanan et al\\.",
      "year" : 2015
    }, {
      "title" : "Segnet: A deep convolutional encoderdecoder architecture for image segmentation",
      "author" : [ "Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla" ],
      "venue" : "arXiv preprint arXiv:1511.00561,",
      "citeRegEx" : "Badrinarayanan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Badrinarayanan et al\\.",
      "year" : 2015
    }, {
      "title" : "Segmentation and recognition using structure from motion point clouds",
      "author" : [ "Gabriel J. Brostow", "Jamie Shotton", "Julien Fauqueur", "Roberto Cipolla" ],
      "venue" : "In ECCV",
      "citeRegEx" : "Brostow et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brostow et al\\.",
      "year" : 2008
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille" ],
      "venue" : "arXiv preprint arXiv:1412.7062,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "cudnn: Efficient primitives for deep learning",
      "author" : [ "Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer" ],
      "venue" : "arXiv preprint arXiv:1410.0759,",
      "citeRegEx" : "Chetlur et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chetlur et al\\.",
      "year" : 2014
    }, {
      "title" : "The cityscapes dataset for semantic urban scene understanding",
      "author" : [ "Marius Cordts", "Mohamed Omran", "Sebastian Ramos", "Timo Rehfeld", "Markus Enzweiler", "Rodrigo Benenson", "Uwe Franke", "Stefan Roth", "Bernt Schiele" ],
      "venue" : "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Cordts et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cordts et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
      "author" : [ "David Eigen", "Rob Fergus" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Eigen and Fergus.,? \\Q2015\\E",
      "shortCiteRegEx" : "Eigen and Fergus.",
      "year" : 2015
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Farabet et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Farabet et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J. Dally" ],
      "venue" : "arXiv preprint arXiv:1510.00149,",
      "citeRegEx" : "Han et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Decoupled deep neural network for semisupervised semantic segmentation",
      "author" : [ "Seunghoon Hong", "Hyeonwoo Noh", "Bohyung Han" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Flattened convolutional neural networks for feedforward acceleration",
      "author" : [ "Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello" ],
      "venue" : "arXiv preprint arXiv:1412.5474,",
      "citeRegEx" : "Jin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "Yann LeCun", "Yoshua Bengio" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "LeCun and Bengio.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun and Bengio.",
      "year" : 1998
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 28th international conference on machine learning",
      "citeRegEx" : "Ngiam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning deconvolution network for semantic segmentation",
      "author" : [ "Hyeonwoo Noh", "Seunghoon Hong", "Bohyung Han" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Noh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Noh et al\\.",
      "year" : 2015
    }, {
      "title" : "Large-scale image retrieval with compressed fisher vectors",
      "author" : [ "F. Perronnin", "Y. Liu", "J. Sánchez", "H. Poirier" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Perronnin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Perronnin et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
      "author" : [ "Marc Aurelio Ranzato", "Fu Jie Huang", "Y-Lan Boureau", "Yann LeCun" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2007
    }, {
      "title" : "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context",
      "author" : [ "Jamie Shotton", "John Winn", "Carsten Rother", "Antonio Criminisi" ],
      "venue" : "Int. Journal of Computer Vision (IJCV),",
      "citeRegEx" : "Shotton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shotton et al\\.",
      "year" : 2009
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Sun rgb-d: A rgb-d scene understanding benchmark suite",
      "author" : [ "Shuran Song", "Samuel P Lichtenberg", "Jianxiong Xiao" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Song et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2015
    }, {
      "title" : "Combining appearance and structure from motion features for road scene understanding",
      "author" : [ "Paul Sturgess", "Karteek Alahari", "Lubor Ladicky", "Philip HS Torr" ],
      "venue" : "In BMVC 2012-23rd British Machine Vision Conference,",
      "citeRegEx" : "Sturgess et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sturgess et al\\.",
      "year" : 2009
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "arXiv preprint arXiv:1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient object localization using convolutional networks",
      "author" : [ "Jonathan Tompson", "Ross Goroshin", "Arjun Jain", "Yann LeCun", "Christoph Bregler" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Tompson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tompson et al\\.",
      "year" : 2015
    }, {
      "title" : "Segmentation as selective search for object recognition",
      "author" : [ "K.E.A. van de Sande", "J.R.R. Uijlings", "T. Gevers", "A.W.M. Smeulders" ],
      "venue" : "In IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Sande et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sande et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "Fisher Yu", "Vladlen Koltun" ],
      "venue" : "arXiv preprint arXiv:1511.07122,",
      "citeRegEx" : "Yu and Koltun.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu and Koltun.",
      "year" : 2015
    }, {
      "title" : "Conditional random fields as recurrent neural networks",
      "author" : [ "Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537,",
      "citeRegEx" : "Zheng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al.",
      "startOffset" : 169,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al.",
      "startOffset" : 169,
      "endOffset" : 224
    }, {
      "referenceID" : 13,
      "context" : "In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al. (2015a)) surpass the performance of many conventional computer vision algorithms (Shotton et al.",
      "startOffset" : 169,
      "endOffset" : 248
    }, {
      "referenceID" : 13,
      "context" : "In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al. (2015a)) surpass the performance of many conventional computer vision algorithms (Shotton et al. (2009); Perronnin et al.",
      "startOffset" : 169,
      "endOffset" : 344
    }, {
      "referenceID" : 13,
      "context" : "In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al. (2015a)) surpass the performance of many conventional computer vision algorithms (Shotton et al. (2009); Perronnin et al. (2010); van de Sande et al.",
      "startOffset" : 169,
      "endOffset" : 369
    }, {
      "referenceID" : 13,
      "context" : "In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) (LeCun & Bengio (1998); Krizhevsky et al. (2012); Simonyan & Zisserman (2014a); Szegedy et al. (2015a)) surpass the performance of many conventional computer vision algorithms (Shotton et al. (2009); Perronnin et al. (2010); van de Sande et al. (2011)).",
      "startOffset" : 169,
      "endOffset" : 397
    }, {
      "referenceID" : 4,
      "context" : "Therefore, they are often cascaded with other algorithms to refine the results, such as color based segmentation (Farabet et al. (2013)) or conditional random fields (Chen et al.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "(2013)) or conditional random fields (Chen et al. (2014)), to name a few.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "In order to both spatially classify and finely segment images, several neural network architectures have been proposed, such as SegNet (Badrinarayanan et al. (2015a;b)) or fully convolutional networks (Long et al. (2015)).",
      "startOffset" : 136,
      "endOffset" : 221
    }, {
      "referenceID" : 0,
      "context" : "In order to both spatially classify and finely segment images, several neural network architectures have been proposed, such as SegNet (Badrinarayanan et al. (2015a;b)) or fully convolutional networks (Long et al. (2015)). All these works are based on a VGG16 (Simonyan & Zisserman (2014b)) architecture, which is a very large model designed for multi-class classification.",
      "startOffset" : 136,
      "endOffset" : 290
    }, {
      "referenceID" : 4,
      "context" : "Performance of the proposed network has been tested on Cityscapes (Cordts et al. (2016)) and CamVid (Brostow et al.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "(2016)) and CamVid (Brostow et al. (2008)) for driving scenario, whereas SUN dataset (Song et al.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "(2016)) and CamVid (Brostow et al. (2008)) for driving scenario, whereas SUN dataset (Song et al. (2015)) has been used for testing our network in an indoor situation.",
      "startOffset" : 20,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "Inspired by probabilistic auto-encoders (Ranzato et al. (2007); Ngiam et al.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "(2007); Ngiam et al. (2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al.",
      "startOffset" : 84,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)).",
      "startOffset" : 84,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al.",
      "startOffset" : 84,
      "endOffset" : 258
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al.",
      "startOffset" : 84,
      "endOffset" : 383
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al.",
      "startOffset" : 84,
      "endOffset" : 402
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al.",
      "startOffset" : 84,
      "endOffset" : 423
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al.",
      "startOffset" : 84,
      "endOffset" : 446
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)).",
      "startOffset" : 84,
      "endOffset" : 466
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)). However, these networks are slow during inference due to their large architectures and numerous parameters. Unlike in Noh et al. (2015), fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of operations and memory footprint, making it the smallest of these networks.",
      "startOffset" : 84,
      "endOffset" : 604
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)). However, these networks are slow during inference due to their large architectures and numerous parameters. Unlike in Noh et al. (2015), fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of operations and memory footprint, making it the smallest of these networks. Still, none of them can operate in real-time. Other existing architectures use simpler classifiers and then cascade it with Conditional Random Field (CRF) as a post-processing step (Chen et al. (2014); Sturgess et al.",
      "startOffset" : 84,
      "endOffset" : 998
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)). However, these networks are slow during inference due to their large architectures and numerous parameters. Unlike in Noh et al. (2015), fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of operations and memory footprint, making it the smallest of these networks. Still, none of them can operate in real-time. Other existing architectures use simpler classifiers and then cascade it with Conditional Random Field (CRF) as a post-processing step (Chen et al. (2014); Sturgess et al. (2009)).",
      "startOffset" : 84,
      "endOffset" : 1022
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)). However, these networks are slow during inference due to their large architectures and numerous parameters. Unlike in Noh et al. (2015), fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of operations and memory footprint, making it the smallest of these networks. Still, none of them can operate in real-time. Other existing architectures use simpler classifiers and then cascade it with Conditional Random Field (CRF) as a post-processing step (Chen et al. (2014); Sturgess et al. (2009)). As explained in Badrinarayanan et al. (2015b), these techniques use onerous post-processing steps and often fail to label the classes that occupy fewer number of pixels in a frame.",
      "startOffset" : 84,
      "endOffset" : 1070
    }, {
      "referenceID" : 0,
      "context" : "(2011)), encoder-decoder network architecture have been introduced in SegNet-basic (Badrinarayanan et al. (2015a)), and further improved in SegNet (Badrinarayanan et al. (2015b)). The encoder is a vanilla CNN (such as VGG16 from Simonyan & Zisserman (2014b)) which is trained to classify the input, while the decoder is used to upsample the output of the encoder (Long et al. (2015); Noh et al. (2015); Zheng et al. (2015); Eigen & Fergus (2015); Hong et al. (2015)). However, these networks are slow during inference due to their large architectures and numerous parameters. Unlike in Noh et al. (2015), fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of operations and memory footprint, making it the smallest of these networks. Still, none of them can operate in real-time. Other existing architectures use simpler classifiers and then cascade it with Conditional Random Field (CRF) as a post-processing step (Chen et al. (2014); Sturgess et al. (2009)). As explained in Badrinarayanan et al. (2015b), these techniques use onerous post-processing steps and often fail to label the classes that occupy fewer number of pixels in a frame. CNNs can be combined with recurrent neural networks (Zheng et al. (2015)) for better performance, but suffers from further speed degradation.",
      "startOffset" : 84,
      "endOffset" : 1278
    }, {
      "referenceID" : 9,
      "context" : "We adopt a view of ResNets (He et al. (2015b)) that describes them as having a single main branch and extensions with convolutional filters that separate from it, and then merge back with an element-wise addition, as shown in Figure 1b.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "We adopt a view of ResNets (He et al. (2015b)) that describes them as having a single main branch and extensions with convolutional filters that separate from it, and then merge back with an element-wise addition, as shown in Figure 1b. Just as in the original paper, we refer to these as bottleneck modules. They consist of three convolutional layers: a 1×1 projection that reduces the dimensionality, a main convolutional layer (conv in Figure 1b), and a 1× 1 expansion. If the bottleneck is downsampling, a max pooling layer is added to the main branch. We zero pad the activations, to match the number of feature maps. Also, the first 1× 1 projection is replaced with a 2× 2 convolution with stride 2 in both dimensions. conv is either a regular, dilated or full convolution (also known as deconvolution or fractionally strided convolution) with 3 × 3 filters. Sometimes we replace it with asymmetric convolution i.e. a sequence of 5 × 1 and 1× 5 convolutions. For the regularizer, we use Spatial Dropout (Tompson et al. (2015)), with p = 0.",
      "startOffset" : 28,
      "endOffset" : 1032
    }, {
      "referenceID" : 29,
      "context" : "This is heavily inspired by Szegedy et al. (2015b). (b) ENet bottleneck module.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "1 16× 256× 256 fullconv C × 512× 512 We did not use bias terms in any of the projections, in order to reduce the number of kernel calls and overall memory operations, as cuDNN (Chetlur et al. (2014)) uses separate kernels for convolutions and bias addition.",
      "startOffset" : 177,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "1 16× 256× 256 fullconv C × 512× 512 We did not use bias terms in any of the projections, in order to reduce the number of kernel calls and overall memory operations, as cuDNN (Chetlur et al. (2014)) uses separate kernels for convolutions and bias addition. This choice didn’t have any impact on the accuracy. Between each convolutional layer and following non-linearity we use Batch Normalization (Ioffe & Szegedy (2015)).",
      "startOffset" : 177,
      "endOffset" : 422
    }, {
      "referenceID" : 17,
      "context" : "The first issue has been addressed in Long et al. (2015) by adding the feature maps produced by encoder, and in SegNet (Badrinarayanan et al.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "(2015) by adding the feature maps produced by encoder, and in SegNet (Badrinarayanan et al. (2015a)) by saving the elements index from the corresponding encoder max-pooling module.",
      "startOffset" : 70,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "This might sound very obvious, however many popular architectures (Hong et al. (2015); Badrinarayanan et al.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "(2015); Badrinarayanan et al. (2015b)) do not pay much attention towards optimization of early stages of network, which are often the most expensive.",
      "startOffset" : 8,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "(2015); Badrinarayanan et al. (2015b)) do not pay much attention towards optimization of early stages of network, which are often the most expensive. ENet first two blocks heavily reduce the input size, and use only a small set of feature maps. The idea behind it, is that visual information is highly redundant in space, and thus can be compressed into a more efficient representation. Also, our intuition is that the initial network layers should not be used specifilly only for classification. Instead, they should rather act as good feature extractors and preprocess the input for later portions of the network. This insight worked well in our experiments. Increasing the number of feature maps from 16 to 32 did not improve the accuracy on Cityscapes dataset (Cordts et al. (2016)).",
      "startOffset" : 8,
      "endOffset" : 786
    }, {
      "referenceID" : 0,
      "context" : "Decoder size: In this work we would like to provide a different view on encoder-decoder architectures than the one presented in Badrinarayanan et al. (2015b). SegNet is a very symmetric architecture, as the encoder is an exact mirror of the encoder.",
      "startOffset" : 128,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "Nonlinear operations: He et al. (2016) report that it is beneficial to use ReLU and Batch Normalization layers before convolutions.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "Nonlinear operations: He et al. (2016) report that it is beneficial to use ReLU and Batch Normalization layers before convolutions. We tried applying these ideas to ENet, but this had a detrimental effect on accuracy. Investigating its cause we replaced all ReLUs in the network with PReLUs (He et al. (2015a)), which use an additional parameter per feature map, with the goal of learning the negative slope of non-linearities.",
      "startOffset" : 22,
      "endOffset" : 310
    }, {
      "referenceID" : 9,
      "context" : "Nonlinear operations: He et al. (2016) report that it is beneficial to use ReLU and Batch Normalization layers before convolutions. We tried applying these ideas to ENet, but this had a detrimental effect on accuracy. Investigating its cause we replaced all ReLUs in the network with PReLUs (He et al. (2015a)), which use an additional parameter per feature map, with the goal of learning the negative slope of non-linearities. We expected that in layers where identity is a preferable transfer function, PReLU weights will have values around 1, and conversely, values around 0 if ReLU is preferable. Results of this experiment can be seen in Figure 2. The first layers weights exhibit a large variance and are slightly biased towards positive values, while in the later portions of the encoder they settle to recurring pattern. All layers in the main branch behave nearly exactly like regular ReLUs, while the weights inside bottleneck modules are negative i.e. the function inverts and scales down negative values. We hypothesize that identity did not work out well in our architecture because of its limited depth. We hypothesize that the reason why such lossy functions are learned is that He et al. (2016) uses networks that are hundreds of layers deep, while our network uses fewer layers, and it needs to quickly filter out information.",
      "startOffset" : 22,
      "endOffset" : 1211
    }, {
      "referenceID" : 29,
      "context" : "A very good approach to this problem has been presented in Szegedy et al. (2015b). However, pooling after a convolution, in case of increasing feature map depth, is computationally expensive.",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "Factorizing filters: It has been shown that convolutional weights have a fair amount of redundancy, and each n× n convolution can be decomposed into two smaller asymmetric convolutions: one with a n × 1 filter followed by a 1 × n filter (Jin et al. (2014); Szegedy et al.",
      "startOffset" : 238,
      "endOffset" : 256
    }, {
      "referenceID" : 15,
      "context" : "Factorizing filters: It has been shown that convolutional weights have a fair amount of redundancy, and each n× n convolution can be decomposed into two smaller asymmetric convolutions: one with a n × 1 filter followed by a 1 × n filter (Jin et al. (2014); Szegedy et al. (2015b)).",
      "startOffset" : 238,
      "endOffset" : 280
    }, {
      "referenceID" : 15,
      "context" : "Factorizing filters: It has been shown that convolutional weights have a fair amount of redundancy, and each n× n convolution can be decomposed into two smaller asymmetric convolutions: one with a n × 1 filter followed by a 1 × n filter (Jin et al. (2014); Szegedy et al. (2015b)). We have used asymmetric convolutions with n = 5 in our network, so cost of these two operations is similar to a single 3× 3 convolution. This allowed to increase the variety of functions learned by each block and increase the receptive field. Sequence of operations used in the bottleneck module (projection, convolution, projection) can be seen as decomposing one large convolutional layer into a series of smaller and simpler low-rank approximation operations. Such factorization allows for large speedups and reduction in number of parameters, making them less redundant (Jin et al. (2014)).",
      "startOffset" : 238,
      "endOffset" : 875
    }, {
      "referenceID" : 13,
      "context" : "Then, inspired by Huang et al. (2016), we have tried stochastic depth, which increased accuracy.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Then, inspired by Huang et al. (2016), we have tried stochastic depth, which increased accuracy. However it became apparent that dropping branches (i.e. setting their output to 0) is in fact a special case of applying Spatial Dropout (Tompson et al. (2015)), where either all of the channels, or none of them are ignored, instead of selecting a random subset.",
      "startOffset" : 18,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : "We set SegNet (Badrinarayanan et al. (2015b)) as a baseline since it is one of the fastest segmentation model available, that also requires less memory to operate than plain CNNs.",
      "startOffset" : 15,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "This alleviates the need for model compression (Han et al. (2015)), making it possible to use general purpose code for neural network computation.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Cityscapes: This dataset consists of 5000 fine-annotated images, out of which 2975 are available for training, 500 for validation, and the remaining 1525 have been selected as test set (Cordts et al. (2016)).",
      "startOffset" : 186,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "Cityscapes: This dataset consists of 5000 fine-annotated images, out of which 2975 are available for training, 500 for validation, and the remaining 1525 have been selected as test set (Cordts et al. (2016)). Cityscapes was the most important benchmark for us, because of its outstanding quality and highly varying road scenarios, often featuring many pedestrians and cyclists. We trained on 19 classes that have been selected in the official evaluation scripts (Cordts et al. (2016)).",
      "startOffset" : 186,
      "endOffset" : 484
    }, {
      "referenceID" : 2,
      "context" : "It contains 367 training and 233 testing images (Brostow et al. (2008)).",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "In Table 6 we compare the performance of ENet with SegNet (Badrinarayanan et al. (2015b)), which is the only neural network model that reports accuracy on this dataset.",
      "startOffset" : 59,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "In Table 6 we compare the performance of ENet with SegNet (Badrinarayanan et al. (2015b)), which is the only neural network model that reports accuracy on this dataset. Our results, though inferior in global average accuracy and IoU, are comparable in class average accuracy. Since global average accuracy and IoU are metrics that favor correct classification of classes occupying large image patches, researchers generally emphasize the importance of other metrics in case of semantic segmentation. One notable example is introduction of iIoU metric (Cordts et al. (2016)).",
      "startOffset" : 59,
      "endOffset" : 573
    } ],
    "year" : 2016,
    "abstractText" : "The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in practical mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.",
    "creator" : "LaTeX with hyperref package"
  }
}