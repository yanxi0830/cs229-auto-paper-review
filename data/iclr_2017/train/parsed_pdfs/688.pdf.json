{
  "name" : "688.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "nicolas@le-roux.name" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Recently, reinforcement learning has seen a surge in popularity, in part because of its successes in playing Atari games (Mnih et al., 2013) and Go (Silver et al., 2016). Due to its ability to act in settings where the actions taken influence the environment and, more generally, the input distribution of examples, reinforcement learning is now used in other domains, such as online advertising.\nThe goal is to learn a good policy, i.e. a good mapping from states to actions, which will maximize the final score, in the case of Atari games, the probability of winning, in Go, or the number of sales, in online advertising. We have at our disposal logs of past events, where we know the states we were in, the actions we took and the resulting rewards we obtained. In this paper, we shall focus on how to efficiently use these logs to obtain a good policy. In some cases, in addition to (state, action, reward) triplets, we have access to a teacher which provides the optimal, or at least a good, action for a given state. The use of such a teacher to find a good initial policy is outside the scope of this paper.\nThere are many ways to learn good policies using past data. The two most popular are Q-learning and direct policy optimization. In Q-learning (Sutton & Barto, 1998), we are trying to learn a mapping from a (state, action) pair to the reward. Given this mapping and a state, we can then find the action which leads to the maximal predicted reward. This method has been very successful, especially when the action space is small, since we need to test all the actions, and the reward somewhat predictable, since taking the maximum is unstable and a small error can lead to suboptimal actions.\nDirect policy optimization, rather than trying to estimate the value of a (state, action) pair, directly parameterizes a policy, i.e. a conditional distribution over actions given the current state. More precisely, and using the notation from Kober (2014), we wish to maximize the expected return of a policy p(·|θ) with parameters θ, i.e.\nJ(θ) = ∫ T p(τ |θ)R(τ) dτ , (1.1)\nwhere T is the set of all paths τ and R(τ) is the reward associated with path τ . A rollout τ = [s1:T+1, a1:T ] is a series of states s1:T+1 = [s1, . . . , sT+1] and actions a1:T = [a1, . . . , aT ]. p(τ |θ) is the probability of rollout τ when using a policy with parameters θ and R(τ) is the aggregated return of τ . If we make the Markov assumption that a state only depends on the\nprevious state and the action chosen, we have p(τ |θ) = p(s1) T∏ t=1 p(st+1|st, at)π(at|st, t, θ) where p(st+1|st, at) is the next state distribution and is independent of our policy. The action space can be discrete or continuous.\nWithout the ability to exactly compute J a, we must resort to sampling to get an estimate of both J and its gradient. These samples can come from p(·|θ) or from another distribution. In the latter case, we need to use importance sampling to keep our estimator unbiasedb. Whether they use importance sampling or not, all methods which directly optimize the policy rely on iterative procedures. First, rollouts are performed under a policy to gather samples. Then, these samples are used to update the policy, which is in turn used to gather new samples, until convergence of the policy. These methods continuously gather new samples, mostly because the updates to the policy are more reliable when they are based on fresh samples. However, in a production environment, as we will see in Sec. 2, we will typically release a new policy to many users at once, gathering millions or billions of samples, but the delay between two updates of a policy is of the order of hours or even days. Thus, there is a strong need for policy learning techniques which can achieve a good performance with a limited number of policy updates. There is an analogous issue in robotics where each new rollout induces wear and tear on the robot, making such a method which limits the number of rollouts desirable.\nIn this paper, we shall thus present a method dedicated to achieving high performance while limiting the number of different policies under which samples are gathered. Sec. 3 reviews the relevant literature. Sec. 4 presents a first version of our algorithm, proving its theoretical efficiency and providing a common framework to several existing techniques. Then, Sec. 5 shows how the positiveness assumption for the rewards can slow down learning and proposes an improvement which circumvents the issue. Sec. 6 shows results of the proposed algorithm on both a synthetic dataset and a real-life, large scale dataset in online advertising. Finally, Sec. 7.3 reflects on the current state and the future venues of research."
    }, {
      "heading" : "2 DISPLAY ADVERTISING",
      "text" : "Retargeting is a type of advertising where ads are displayed to users who have already expressed interest in one or multiple products, generally by browsing on merchants’ websites. Since the information used to know which ad to display are not related to a query, like in search advertising, these ads can be displayed on any website, for instance news sites or personal pages. More precisely, every time a user lands on such a website, the website contacts an ad-exchange platform which runs a real-time auction. The highest bidder gets the right to display an ad for this particular user and pays a price depending on many factors, including the opponents’ bids. If the user then clicks on the ad, the retargeter is paid by the merchant whose ad was displayed. To maximize its revenue, the retargeter must thus only display ads leading to a click, and do so at the lowest possible price. Historically, these auctions were second-price, which means that the price paid by the highest bidder was the second highest bid. From a bidder perspective, the optimal strategy was straightforward as the optimal bid was the expected gain of displaying an ad. However, ad-exchanges have recently moved to other types of auctions, where the optimal strategy depends on the (unknown) bids of the other bidders. Worse, the exact type of auction is unknown to the bidders who only know the price they pay when they win the auction.\nThe bidding problem thus fits nicely in the reinforcement learning framework where the state is the set of information known about the user and the current website, the action is the bid and the reward is the payment (if there is a click) minus the cost of displaying the ad. As the reward depends mostly on whether there is a click or not, an event which is highly unpredictable, techniques such as doubly robust estimation (Dudı́k et al., 2011) or based on carefully crafted Q-functions (Lillicrap et al., 2015; Schulman et al., 2015b; Gu et al., 2016) are unlikely to yield significant improvements.\nThere is another major difference with other reinforcement learning works. For quality control, a new policy can only be put in production every few hours or even days. Since large advertising companies display several billion ads each day, each new policy is used to gather about a billion samples. We are thus in a very specific setting where the number of samples is very large but the number of policies with which samples are gathered is limited.\nWe will now review the existing literature and show how no existing work addresses the constraints we face. We will then present our solution which is both simple and leads to good policies. To\naComputing J would require visiting every possible τ at least once, which is impossible, even for moderately long rollouts.\nbUsing a biased estimator can be useful but this is outside the scope of this paper.\nshow its efficiency, we report results on both the Cartpole problem, a synthetic problem common in reinforcement learning, and a real-world example."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "We review here some of the most common techniques in reinforcement learning, limiting ourselves to those who try to directly optimize the policy.\nThe first such method is REINFORCE (Williams, 1992), which performs a single gradient step between two rollouts of the policy. This method has multiple issues. First, one has to do rollouts after each update, ultimately resulting in many rollouts before reaching a good solution. This is further emphasized with the potential poor quality of the gradient update which is not insensitive to a reparametrization of the parameter space. Finally, as with any stochastic method, the choice of the stepsize has a strong influence of the convergence speed. Each of these problems has been treated in separate works. The need to perform rollouts after each update was alleviated by using importance sampling policy gradient (Swaminathan & Joachims, 2015). The update direction can be improved by using natural gradient (Amari, 1998; Kakade, 2001) and doing a line search helps in choosing a correct stepsize (Jie & Abbeel, 2010). These improvements can be computationally expensive and the additional hyperparameters make them less suited to a production environment where simplicity and robustness are key.\nAnother line of research used concave lower bounds on J(θ), which could then be optimized using off-the-shelf classifiers such as L-BFGS (Liu & Nocedal, 1989). Examples of such methods are PoWER (Kober & Peters, 2009) and Natural actor-critic (Peters & Schaal, 2008). These bounds were obtained using an analogy with EM (Dempster et al., 1977; Minka) which required the rewards R(τ) to be positive. In settings where multiple policies can achieve high accuracy, Neumann (2011) proposed another EM-based method which focuses on one of these policies rather than trying to loosely cover all of them, at the expense of a larger computational cost.\nWe will show how these works can be extended to better optimize the policy between two updates. We will then show how the positiveness requirement hurts the optimization, then propose an extension which allows us to use any reward.\nSince, in practice, we do not have access to the full distribution but rather to a set of N samples, we shall optimize a Monte-Carlo estimate of J :\nĴ(θ) = 1\nN ∑ i R(τi) p(τi|θ) p(τi|θ0) , τi ∼ p(τ |θ0) , (3.1)\nwhere p(τ |θ0) is the probability of rollout τ under the distribution used to generate samples. This is the standard importance sampling trick commonly used in policy gradient (Sutton et al., 1999)."
    }, {
      "heading" : "4 CONCAVE APPROXIMATION OF THE EXPECTED LOSS",
      "text" : "In this section, we assume that all returns are nonnegativec. Due to this nonnegativity, the nonconcavity in J stems from the nonconcavity of each p(τ |θ). However, if p(τ |θ) belongs to the exponential family (Wainwright & Jordan, 2008), then it is a log-concave function of its parameters and log p(τ |θ) is a concave function of θ. This suggests the following lower bound:\nLemma 1. Let\npq(τ |θ) = q(τ) ( 1 + log\np(τ |θ) q(τ)\n) (4.1)\nwith q such that q(τ) 6= 0 when p(τ |θ) 6= 0. Then we have pq(τ |θ) ≤ p(τ |θ).\ncOr at least bounded below, in which case they need to be adequately shifted.\nProof.\np(τ |θ) = q(τ)p(τ |θ) q(τ)\n≥ q(τ) ( 1 + log\np(τ |θ) q(τ) ) = pq(τ |θ) .\nThe second line stems from the inequality x ≥ 1 + log x.\nLemma 1 shows that, regardless of the function q chosen, pq(τ |θ) is a lower bound of p(τ |θ) for any value of θ. Thus, provided that p(τ |θ) belongs to the exponential family, we have obtained a log-concave lower bound. Lemma 1, however, does not guarantee the quality of that lower bound. This is addressed by the following lemma:\nLemma 2. If there is a ν such that q(τ) = p(τ |ν), we have\npq(τ |ν) = p(τ |ν) , ∂pq(τ |θ) ∂θ ∣∣∣∣ θ=ν = ∂p(τ |θ) ∂θ ∣∣∣∣ θ=ν .\nProof. pq(τ |ν) = p(τ |ν) is immediate when setting θ = ν in Eq. 4.1. Deriving pq(τ |θ) with respect to θ yields\n∂pq(τ |θ) ∂θ = p(τ |ν)∂ log p(τ |θ) ∂θ\n= p(τ |ν) p(τ |θ) ∂p(τ |θ) ∂θ .\nTaking θ = ν on both sides yields ∂pq(τ |θ)∂θ ∣∣∣∣ θ=ν = ∂p(τ |θ)∂θ ∣∣∣∣ θ=ν .\nTo simplify further notations, we will write directly pν(τ |θ) = p(τ |ν) ( 1 + log\np(τ |θ) p(τ |ν)\n) (4.2)\nto explicitly show the dependency of the bound on the parameter ν.\nThe following result is a direct consequence of these two lemmas:\nLemma 3. (lower bound of the expected reward estimator): Let\nĴν(θ) = 1\nN ∑ i R(τi) p(τi|ν) p(τi|θ0) ( 1 + log p(τi|θ) p(τi|ν) ) . (4.3)\nThen we have Ĵν(θ) ≤ Ĵ(θ) ∀θ , Ĵν(ν) = Ĵ(ν) , ∂Ĵν(θ)\n∂θ ∣∣∣∣ θ=ν = ∂Ĵ(θ) ∂θ ∣∣∣∣ θ=ν . Further, if\np(τ |·) is a log-concave function, then Ĵν is concave for any ν.\nProof. Since each R(τi) is nonnegative, so is the ratio R(τi) p(τi|θ0) . The sum of lower bounds being a lower bound, this concludes the proof.\nIt is now worth going into more detail on the three parameters of Eq. 4.3:\n• θ is the current value of the parameter we are trying to optimize over; • θ0 is the parameter value used to gather samples; • ν is the parameter used to create the lower bound. Any value of ν is valid.\nThere are two special cases of this bound. First, when ν = θ = θ0, this bound becomes an equality and we recover the policy gradient of Williams (1992). However, this equality only holds for the first update of θ. Second, a more interesting case is ν = θ0 6= θ. In this case, Eq. 4.3 simplifies and we get\nĴθ0(θ) = 1\nN ∑ i R(τi) ( 1 + log p(τi|θ) p(τi|θ0) ) . (4.4)\nThis bound is used by multiple authors (Dayan & Hinton, 1997; Peters & Schaal, 2007; 2008; Kober, 2014; Schulman et al., 2015a) and has the attractive property that it is tight at the beginning of the optimization since we have θ = θ0. When we optimize this bound without ever changing the value of ν, we end up with exactly the PoWER algorithm. However, as we optimize θ, this bound becomes looser and it might be useful to change the value of ν.\nThis suggest an iterative scheme where, after the optimization of Eq. 4.4 has ended in θ = θ1, we recompute the bound of Eq. 4.3 with ν = θ1. This yields an iterative version of the PoWER algorithm as described in Algorithm 1.\nThe data: Rewards R(τi), probabilities p(τi|θ0), initial parameters θ0 The result: Final parameters θT for t = 1 to T do\nθt = argmax θ Ĵθt−1(θ)\n= argmax θ ∑ i R(τi) p(τi|θt−1) p(τi|θ0) ( 1 + log p(τi|θ) p(τi|θt−1) ) end\nAlgorithm 1: Iterative PoWER\nWe recall that PoWER is equivalent to Algorithm 1 but with T = 1. As we shall see in the experiments, larger values of T lead to vast improvements.\nOne can also see that Algorithm 1 performs the same optimizations as the PoWER algorithm where new samples would be gathered at each iteration, with the difference that importance sampling is used rather than true samples. Thus, in the spirit of off-policy learning, we have replaced extra sampling with importance sampling. When, and only when, variance starts to be too high, can we sample from the new distribution."
    }, {
      "heading" : "5 CONVEX UPPER BOUND",
      "text" : "Lemma 3 requires positive returns. This has two undesirable consequences. First, this can lead to very slow convergence. One can see this by creating a setting where all rollouts lead to a positive return except for one which leads to a return of −β < 0. To maintain the positivity of the returns, we need to shift all the returns by β which does not change the optimal policy using the following\ntransformation: J(θ) = ∫ T p(τ |θ)R(τ) dτ = ∫ T p(τ |θ)(R(τ) + β) dτ − β. We may now apply lemma 3 and optimize the following lower bound:\nJβν (θ) = ∫ T p(τ |ν) ( 1 + log p(τ |θ) p(τ |ν) ) (R(τ) + β) dτ − β .\nWithout the rollout with a negative return, the returns would not need to be shifted by β and we could have optimized J0ν (θ) instead. The difference between the two is equal to J β ν (θ) − J0ν (θ) = −βKL(p(τ |ν)||p(τ |θ)) where KL is the Kullback-Leibler divergence, which encourages p(τ |θ) to be close to p(τ |ν). Hence, one rollout with a negative return would slow down our optimization with a regularization term proportional to that return. A simple heuristic would be to discard such rollouts but we would lose all guarantees about the improvement of the expected return.\nFurther, the positivity of the returns precludes the use of control variates which are especially useful when the shifted rewards are approximately centered on 0. Thus, it prevents us from benefiting of all the existing techniques based around these control variates which would help reducing the variance.\nThe positivity assumption is required since we multiply our lower bound with the returns. If, instead, we have convex upper bounds of p(τ |θ), then this would provide us with a concave lower bound whenever it is associated with a negative return. Lemma 4 provides such a bound. Lemma 4. Let\nuν(τ |θ) = p(τ |ν) exp [ (θ − ν)> ∂ log p(τ |θ)\n∂θ\n∣∣∣∣ θ=ν ] , (5.1)\nwhere p(τ |θ) is a log-concave function of θ. Then uv(τ |θ) is a convex function of θ and we have:\nuν(τ |ν) = p(τ |ν) , ∂uν(τ |θ)\n∂θ ∣∣∣∣ θ=ν = ∂p(τ |θ) ∂θ ∣∣∣∣ θ=ν\n, uν(τ |θ) ≥ p(τ |θ) ∀θ .\nProof. Both equalities can be verified by setting θ = ν. Since uν is the exponential of a linear function in its argument, it is convex. Finally, using the concavity of log p, we have:\nlog p(τ |θ) ≤ log p(τ |ν) + (θ − ν)> ∂ log p(τ |θ) ∂θ ∣∣∣∣ θ=ν\np(τ |θ) ≤ p(τ |ν) exp (θ−ν)> ∂ log p(τ|θ)∂θ ∣∣∣∣ θ=ν .\nThis concludes the proof.\nWe may now combine the two bounds to get the following lower bound on Ĵ(θ) without any constraint on the rewards:\nĴ(θ) ≥ 1 N ∑ i R(τi) p(τi|ν) p(τi|θ0) zi(θ) (5.2)\nzi(θ) = 1R(τi)≥0\n( 1 + log\np(τi|θ) p(τi|ν)\n) + 1R(τi)<0 exp [ (θ − ν)> ∂ log p(τ |θ)\n∂θ\n∣∣∣∣ θ=ν ] .\nFurther, when p(τ |θ) is log-concave in θ, then zi(θ) is concave in θ. This allows us to deal with positive and negative rewards, which means that the choice of control variate is now free, which can help both in reducing the variance and improving the lower bound and thus the convergence."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "To demonstrate the effectiveness of Iterative PoWER, we provide results obtained on the Cartpole benchmark using the Gym d toolkit. We also show experiments on real online advertising data where we try to maximize advertisers’ revenue while keeping our costs constant."
    }, {
      "heading" : "6.1 CARTPOLE BENCHMARK",
      "text" : "To show how we can achieve good performance with limited rollouts, we ran our experiments on the Cartpole benchmark, which is solved by the PoWER method. We used a stochastic linear policy with a 4-dimensional state (positions and velocities of the cart and the pole) where, at each time step, the cart moves right with probability σ(s>θ) where s is the vector representation of the state. Each experiment was 250 rollouts, in 10 batches of 25 rollouts, of length 400. In between each batch, we retrained the parameters of the model. The average performance over 350 repetitions was computed every batch, varying the number T of iterations of PoWER e. We capped the importance weights at 20.\ndhttps://gym.openai.com/ eEach iteration of PoWER consisted in 5 steps of Newton method\nFigure 3: Dashed blue: improvement in expected merchant value as a function of the number of iterations (arbitrary linear scale). The optimization is very slow at the beginning and the improvement is close to linear for the first 50 iterations. Solid green: Relative change of the expected cost as a function of the number of iterations. The expected spend increased up to 0.0086% but reached 0.0001% when we stopped the optimization.\nWe also evaluated the impact of using control variates on the convergence. To that effect, at each iteration, we computed the control variate minimizing the variance of the total estimator. We then used as control variate fractions of that value. For instance, cv = 0.5 means that the control variate used was half of the “optimal” control variate found by minimizing the variance of the estimator. The results can be seen in Fig. 2 and Fig. 1. We see that doing several iterations of PoWER leads to an increase in performance, up to a point where the variance is too high.\nWe can also see that, without using control variates (cv = 0), it is not beneficial to do too several iterations. The stagnation in performance for cv = 0.25 and cv = 0.5 might be due to the poorer quality of the upper bound compared to the lower bound. Thus, to get the full benefit of the control variate, we need to use larger values, such as cv = 0.99, which yields the highest performance for any value of T ."
    }, {
      "heading" : "6.2 REAL WORLD DATA - ONLINE ADVERTISING",
      "text" : "We also tested Iterative PoWER using real data gathered from a production system. In this system, every time a user lands on a webpage, an auction is run to determine who gets to display an ad to that user. Each participant to the auction chooses in real-time how much to bid for the right to display that ad. If the ad is clicked and then the user buys an item on the merchant’s website, a reward equal to the value of the item is observed. Otherwise, a reward of 0 is observed. The cost of displaying an ad depends on the bid in a way unknown to us as we do not have access to the type of auction nor to the bids of the other participants.\nWe gathered data for over 1.3 billion displays over a period of 2 weeks in April 2016. This data was comprised of information available at the time of the bid, for instance the history of the user, the current URL, or the size of the ad to display. The aggregation of all this data represented the states. When we won the auction, we also logged our bid, which are our actions, the cost of the display and the value generated if there was a sale, both of which are used for the reward.\nRather than learning a full bidding strategy mapping from states to a bid, we used our production system as baseline and learnt a small modifier to account for the non-truthfulness of the auctions. Thus, only a few thousand parameters were learnt, a small number compared to the number of training samples. This allowed us to run many iterations of PoWER without fear of high variance.\nSince our aim was to maximize the value generated, this could lead to the undesirable solution where all ads are bought regardless of their price. Thus, we included the constraint that the total cost of buying the ads had to remain constant. The details of iPoWER with added constraints are in section 7.1\nFig. 3 shows the relative improvement in advertiser value generated as a function of the number T of iterations. We can readily see that the final improvement obtained by Iterative Power is far greater than that obtained after one iteration of PoWER (about 60 times greater). The green curve shows the change in total cost. Since we also used a lower bound for the constraint, it is initially not satisfied, but running the algorithm to convergence leads to a solution in the feasible set. The final improvement represents an increase of the net gain by several percentage points."
    }, {
      "heading" : "7 EXTENSIONS AND CONCLUSION",
      "text" : ""
    }, {
      "heading" : "7.1 APPLICATION TO CONSTRAINED OPTIMIZATION",
      "text" : "There are cases where one wishes to maximize the expected reward while satisfying a constraint on another quantity. In online advertising, for instance, it is interesting to maximize the number of clicks (or sales) while keeping the number of ads displayed constant as this reduces the potential long-term effects on user behaviour, something not captured at the scale of a rollout. To maximize the expected reward Eθ[R] while satisfying the constraint Eθ[S] = S0, we may add a Lagrangian term to Ĵν(θ) and iteratively solve the following problem:\nmax θ min α ∑ i R(τi) p(τi|ν) p(τi|θ0) ( 1 + log p(τi|θ) p(τi|ν) ) + α (∑ i S(τi) p(τi|ν) p(τi|θ0) ( 1 + log p(τi|θ) p(τi|ν) ) − S0 ) where α is the Lagrange multiplier associated with the constraint. Due to the approximation, the constraint will not be exactly satisfied for the first few iterations but the convergence of this algorithm guarantees that the constraint will eventually be satisfied."
    }, {
      "heading" : "7.2 EXTENSION TO NON LOG-CONCAVE POLICIES",
      "text" : "The results of this paper rely on a log-concavity assumption on the policy which can be too strong a constraint. Indeed, in many cases, the policy depends in a complex manner on the state, for instance through a deep network. However, most of these policies can still be written as a log-concave policy on a non-linear transformation of the states, for instance when the last layer of the deep network uses a softmax. iPoWER can then be used to transform the optimization problem into a simpler, albeit still not concave, maximization problem where the non-concavity of the output of the network has been removed and only remains the non-concavity of the nonlinear transformation of the state."
    }, {
      "heading" : "7.3 CONCLUSION",
      "text" : "We proposed a modification to the PoWER algorithm which allows to improve a policy with a reduced number of rollouts. This method is particularly interesting when there are constraints on the number of rollouts, for instance in a robotic environment or when each policy has to be deployed in an industrial production system. We also proposed an extension to existing EM-based methods which allows for the use of control variates, a potentially useful tool to reduce the variance of the estimator. However, several questions remain. In particular, experiments on the Cartpole benchmark indicate that, despite the use of capped importance weights and control variates, as we do more iterations, we might end up in regions of the space with high variance. It is thus important to use additional regularizers, such as normalized weights or penalizing the standard deviation of the estimator. To maintain the simplicity of the overall algorithm, concave lower bounds of these regularizers must also be found, which is still an open problem."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The author would like to thank Vianney Perchet for helpful discussions."
    } ],
    "references" : [ {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Amari.,? \\Q1998\\E",
      "shortCiteRegEx" : "Amari.",
      "year" : 1998
    }, {
      "title" : "Using expectation-maximization for reinforcement learning",
      "author" : [ "Peter Dayan", "Geoffrey E Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Dayan and Hinton.,? \\Q1997\\E",
      "shortCiteRegEx" : "Dayan and Hinton.",
      "year" : 1997
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society",
      "author" : [ "Arthur P. Dempster", "Nan M. Laird", "Donald B. Rubin" ],
      "venue" : "Series B (methodological),",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "Miroslav Dudı́k", "John Langford", "Lihong Li" ],
      "venue" : "CoRR, abs:1103.4601,",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "Continuous deep q-learning with model-based acceleration",
      "author" : [ "Shixiang Gu", "Timothy P. Lillicrap", "Ilya Sutskever", "Sergey Levine" ],
      "venue" : "CoRR, abs/1603.00748,",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "On a connection between importance sampling and the likelihood ratio policy gradient",
      "author" : [ "Tang Jie", "Pieter Abbeel" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Jie and Abbeel.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jie and Abbeel.",
      "year" : 2010
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kakade.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kakade.",
      "year" : 2001
    }, {
      "title" : "Learning motor skills: from algorithms to robot experiments",
      "author" : [ "Jens Kober" ],
      "venue" : "it-Information Technology,",
      "citeRegEx" : "Kober.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kober.",
      "year" : 2014
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "Jens Kober", "Jan R Peters" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Kober and Peters.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kober and Peters.",
      "year" : 2009
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "CoRR, abs/1509.02971,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "Dong C Liu", "Jorge Nocedal" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Liu and Nocedal.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu and Nocedal.",
      "year" : 1989
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller" ],
      "venue" : "CoRR, abs/1312.5602,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Variational inference for policy search in changing situations",
      "author" : [ "Gerhard Neumann" ],
      "venue" : "In Proceedings of the 28th international conference on machine learning",
      "citeRegEx" : "Neumann.,? \\Q2011\\E",
      "shortCiteRegEx" : "Neumann.",
      "year" : 2011
    }, {
      "title" : "Reinforcement learning by reward-weighted regression for operational space control",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Peters and Schaal.,? \\Q2007\\E",
      "shortCiteRegEx" : "Peters and Schaal.",
      "year" : 2007
    }, {
      "title" : "Gradient estimation using stochastic computation graphs. CoRR, abs/1506.05254, 2015a. URL http://arxiv.org/ abs/1506.05254",
      "author" : [ "John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel" ],
      "venue" : null,
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Highdimensional continuous control using generalized advantage estimation",
      "author" : [ "John Schulman", "Philipp Moritz", "Sergey Levine", "Michael I. Jordan", "Pieter Abbeel" ],
      "venue" : "CoRR, abs/1506.02438,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Counterfactual risk minimization: Learning from logged bandit feedback",
      "author" : [ "Adith Swaminathan", "Thorsten Joachims" ],
      "venue" : "CoRR, abs:1502.02362,",
      "citeRegEx" : "Swaminathan and Joachims.,? \\Q2015\\E",
      "shortCiteRegEx" : "Swaminathan and Joachims.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Recently, reinforcement learning has seen a surge in popularity, in part because of its successes in playing Atari games (Mnih et al., 2013) and Go (Silver et al.",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "More precisely, and using the notation from Kober (2014), we wish to maximize the expected return of a policy p(·|θ) with parameters θ, i.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "As the reward depends mostly on whether there is a click or not, an event which is highly unpredictable, techniques such as doubly robust estimation (Dudı́k et al., 2011) or based on carefully crafted Q-functions (Lillicrap et al.",
      "startOffset" : 149,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : ", 2011) or based on carefully crafted Q-functions (Lillicrap et al., 2015; Schulman et al., 2015b; Gu et al., 2016) are unlikely to yield significant improvements.",
      "startOffset" : 50,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : ", 2011) or based on carefully crafted Q-functions (Lillicrap et al., 2015; Schulman et al., 2015b; Gu et al., 2016) are unlikely to yield significant improvements.",
      "startOffset" : 50,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "The update direction can be improved by using natural gradient (Amari, 1998; Kakade, 2001) and doing a line search helps in choosing a correct stepsize (Jie & Abbeel, 2010).",
      "startOffset" : 63,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "The update direction can be improved by using natural gradient (Amari, 1998; Kakade, 2001) and doing a line search helps in choosing a correct stepsize (Jie & Abbeel, 2010).",
      "startOffset" : 63,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "The update direction can be improved by using natural gradient (Amari, 1998; Kakade, 2001) and doing a line search helps in choosing a correct stepsize (Jie & Abbeel, 2010). These improvements can be computationally expensive and the additional hyperparameters make them less suited to a production environment where simplicity and robustness are key. Another line of research used concave lower bounds on J(θ), which could then be optimized using off-the-shelf classifiers such as L-BFGS (Liu & Nocedal, 1989). Examples of such methods are PoWER (Kober & Peters, 2009) and Natural actor-critic (Peters & Schaal, 2008). These bounds were obtained using an analogy with EM (Dempster et al., 1977; Minka) which required the rewards R(τ) to be positive. In settings where multiple policies can achieve high accuracy, Neumann (2011) proposed another EM-based method which focuses on one of these policies rather than trying to loosely cover all of them, at the expense of a larger computational cost.",
      "startOffset" : 64,
      "endOffset" : 829
    }, {
      "referenceID" : 18,
      "context" : "This is the standard importance sampling trick commonly used in policy gradient (Sutton et al., 1999).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "This bound is used by multiple authors (Dayan & Hinton, 1997; Peters & Schaal, 2007; 2008; Kober, 2014; Schulman et al., 2015a) and has the attractive property that it is tight at the beginning of the optimization since we have θ = θ0.",
      "startOffset" : 39,
      "endOffset" : 127
    } ],
    "year" : 2017,
    "abstractText" : "We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates.",
    "creator" : "LaTeX with hyperref package"
  }
}