{
  "name" : "770.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A NEURAL KNOWLEDGE LANGUAGE MODEL",
    "authors" : [ "Sungjin Ahn", "Heeyoul Choi", "Tanel Pärnamaa", "Yoshua Bengio" ],
    "emails" : [ "heeyoul@gmail.com", "tanel.parnamaa@gmail.com", "yoshua.bengio@umontreal.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Kanye West, a famous <unknown> and the husband of <unknown>, released his latest album <unknown> in <unknown>.\nA core purpose of language is to communicate knowledge. Thus, for human-level language understanding, it is important for a language model to take advantage of knowledge. Although traditional language models are good at capturing statistical co-occurrences of entities as long as they are observed frequently in a corpus (e.g., words like verbs, pronouns, and prepositions), they are in general limited in their ability to encode or decode knowledge, which is often represented by named entities such as person names, place names, years, etc. (as shown in the above example sentence of Kanye West.) When trained with a very large corpus, traditional language models have demonstrated to some extent the ability to encode/decode knowledge (Vinyals & Le, 2015; Serban et al., 2015). However, we claim that simply feeding a larger corpus into a bigger model hardly results in a good knowledge language model.\nThe primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge-related words. In particular, for applications such as question answering (Iyyer et al., 2014; Weston et al., 2016; Bordes et al., 2015) and dialogue modeling (Vinyals & Le, 2015; Serban et al., 2015), these words are of our main interest. Specifically, in the recurrent neural network language model (RNNLM) (Mikolov et al., 2010) the computational complexity is linearly dependent on the number of vocabulary words. Thus, including all words of a language is computationally prohibitive. Instead, we typically fill our vocabulary with a limited number of frequent words and regard all the other words as the unknown (UNK) word. Even if we can include a large number of words in the vocabulary, according to Zipf’s law, a large portion of the words will be rarely observed in the corpus and thus learning good representations for these words remains a problem.\nThe fact that languages and knowledge can change over time also makes it difficult to simply rely on a large corpus. Media produce an endless stream of new knowledge every day (e.g., the results of baseball games played yesterday) that is even changing over time (e.g., “the current president of the\n∗This work was done while HC was in Samsung Advanced Institute of Technology\nUnited States is ”). Furthermore, a good language model should exercise some level of reasoning. For example, it may be possible to observe several occurrences of Barack Obama’s year of birth in a large corpus and thus the model may be able to predict it. However, after seeing mentions of his year of birth, presented with a simple reformulation of that piece of knowledge into a sentence such as “Barack Obama’s age is ”, one would not expect current language models to handle the required amount of reasoning in order to predict the next word (i.e. the age) easily. However, a good model should be able to reason the answer from this context1.\nIn this paper, we propose a Neural Knowledge Language Model (NKLM) as a step towards addressing the limitations of traditional language modeling when it comes to exploiting factual knowledge. In particular, we incorporate symbolic knowledge provided by a knowledge graph (Nickel et al., 2015) into the RNNLM. A knowledge graph (KG) is a collection of facts which have a form of (subject, relationship, object). We observe particularly the following properties of KGs that make the connection to the language model sensible. First, facts in KGs are mostly about rare words in text corpora. KGs are managed and updated in a similar way that Wikipedia pages are managed to date. The KG embedding methods (Bordes et al., 2011; 2013) provide distributed representations for the entities in the KG. The graph can be traversed for reasoning (Gu et al., 2015). Finally, facts come along with textual representations which we call the fact description and take advantage of here.\nThere are a few differences between the NKLM and the traditional RNNLM. First, we assume that a word generation is either based on a fact or not. Thus, at each time step, before predicting a word, we predict whether the word to generate has an underlying fact or not. As a result, our model provides the predictions over facts in a topic in addition to the word predictions. Similarly to how context information of previous words flows through the hidden states in the RNNLM, in the NKLM the previous information on both facts and words flow through an RNN and provide richer context. Second, the model has two ways to generate the next word. One option is to generate a “vocabulary word” from the vocabulary softmax as is in the RNNLM. The other option is to generate a “knowledge word” by copying a word contained in the description of the predicted fact. Considering that the fact description is often short and consists of out-of-vocabulary words, we predict the position of the word to copy within the fact description. This knowledge-copy mechanism makes it possible to generate words which are not in the predefined vocabulary. Thus, it does not require to learn explicit embeddings of the words to generate, and consequently resolves the rare/unknown word problem. Lastly, the NKLM can immediately adapt to adding or modifying knowledge because the model learns to predict facts, which can easily be modified without having to retrain the model.\nTraining the above model in a supervised way requires to align words with facts. To this end, we introduce a new dataset, called WikiFacts. For each topic in the dataset, a set of facts from the Freebase KG (Bollacker et al., 2008) and a Wikipedia description of the same topic is provided along with the alignment information. This alignment is done automatically by performing string matching between the fact description and the Wikipedia description."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "There have been remarkable advances in language modeling research based on neural networks (Bengio et al., 2003; Mikolov et al., 2010). In particular, the RNNLMs are interesting for their ability to take advantage of longer-term temporal dependencies without a strong conditional independence assumption. It is especially noteworthy that the RNNLM using the Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) has recently advanced to the level of outperforming carefully-tuned traditional n-gram based language models (Jozefowicz et al., 2016).\nThere have been many efforts to speed up the language models so that they can cover a larger vocabulary. These methods approximate the softmax output using hierarchical softmax (Morin & Bengio, 2005; Mnih & Hinton, 2009), importance sampling (Jean et al., 2015), noise contrastive estimation (Mnih & Teh, 2012), etc. Although helpful to mitigate the computational problem, these approaches still suffer from the statistical problem due to rare or unknown words. Having the UNK word as the output of a generative language model is also inconvenient (e.g, dialogue system).\n1We do not investigate the reasoning ability in this paper but highlight this example because the explicit representation of facts would help to handle such examples.\nTo help deal with the rare/unknown word problem, the pointer networks (Vinyals et al., 2015) have been adopted to implement the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016) and applied to machine translation and text summarization. With this approach, the (unknown) word to copy from the context sentence is inferred from neighboring words. However, because in our case the context can be very short and often contains no known relevant words (e.g., person names), we cannot use the existing approach directly.\nOur knowledge memory is also related to the recent literature on neural networks with external memory (Bahdanau et al., 2014; Weston et al., 2015; Graves et al., 2014). In Weston et al. (2015), given simple sentences as facts which are stored in the external memory, the question answering task is studied. In fact, the tasks that the knowledge-based language model aims to solve (i.e. predict the next word) can be considered as a fill-in-the-blank type of question answering. The idea of jointly using Wikipedia and knowledge graphs has also been used in the context of enriching word embedding (Celikyilmaz et al., 2015; Long et al., 2016)."
    }, {
      "heading" : "3 MODEL",
      "text" : ""
    }, {
      "heading" : "3.1 PRELIMINARY",
      "text" : "A topic2 k in a set of entities E is associated with topic knowledge Fk (e.g., from Freebase) and topic description Wk (e.g., from Wikipedia). Topic knowledge Fk is a set of facts {ak,1, ak,2, . . . , ak,|Fk|} where each fact a is a triple of subject ∈ E , relationship, and object ∈ E , e.g., (Barack Obama, Married-To, Michelle Obama). Topic description Wk is a sequence of words (wk1 , w k 2 , . . . , w k |Wk|) describing the topic (e.g., a description of a topic in Wikipedia). Because the subject entities in Fk are all equal to the topic entity k3 and the words describing relationships can easily be found in the vocabulary, we use the description of the object entity (e.g., Michelle Obama) as our fact description.\nGiven Fk and Wk, we perform simple string matching between words in Wk and words in the fact descriptions in Fk and thereby build a sequence of augmented observations Yk = {ykt = (wt, at, zt)}t=1:|Wk|. Here, wt ∈ Wk is an observed word, at ∈ Fk a fact on which the generated word wt is based, and zt a binary variable indicating whether wt is in the vocabulary V (including UNK) or not. Because not all words are based on a fact (e.g., words like, is, a, the, have), we introduce a special type of fact, called Not-a-Fact (NaF), and assign NaF to such words.\nFor example, a description “Rogers was born in Latrobe, Pennsylvania in 1928” from a topic Fred Rogers in Wikipedia, is augmented to, Y = {(w=“Rogers”, a=0, z=0), (“was”, NaF, 1), (“born”, NaF, 1), (“in”, NaF, 1), (“Latrobe”, 42, 0), (“Pennsylvania”, 42, 1), (“in”, NaF, 1), (“1928”, 83, 0)}. Here, we use facts on Fred Rogers, a42 = (Fred Rogers, Place of Birth, Latrobe Pennsylvania), a83 = (Fred Rogers, Year of Birth, 1928), and a special fact a0 = (Fred Rogers, Topic Itself, Fred Rogers) which we define in order to refer to the topic string itself. We also assume here that the words Rogers, Latrobe and 1928 are not in the vocabulary.\nDuring the inference and training of topic k, we assume that the topic knowledge Fk is loaded in the knowledge memory in a form of a matrix Fk ∈ RDa×|Fk| where the i-th column is a fact embedding ak,i ∈ RDa . The fact embedding is the concatenation of subject, relationship, and object embeddings. We obtain these entity embeddings from a preliminary run of a knowledge graph embedding method such as TransE (Bordes et al., 2013). Note that we fix the fact embedding during the training of our model to help the model predict new facts at test time. But, we learn the embedding of the Topic Itself. For notation, to denote the vector representation of any object of our interest, we use bold lowercase characters. For example, the embedding of a word wt is represented by wt = W[wt] where WDw×|V| is the word embedding matrix, and W[wt] denotes the wt-th column of W.\n2In this work, a topic is one of the entities which exist in both Wikipedia and Freebase. This is different to the concept in topic modeling where a topic is represented by a distribution over words.\n3Although in Freebase the topic entity can be either the subject or the object, for convenience we process them such that the subject is always equal to the topic entity k.\na1 a2 a3 a4 … aN NaF\n\uD835\uDC64\uD835\uDC61\uD835\uDC5C\nO1 O2 O3 O4 … ON\nTopic Knowledge\n\uD835\uDC64\uD835\uDC61- \uD835\uDC67/\nℎ/ ℎ/\uD835\uDC4E/\n\uD835\uDC58/\n\uD835\uDC4E/34 \uD835\uDC64\uD835\uDC6134- \uD835\uDC64\uD835\uDC61345\nℎ/34 ℎ/ \uD835\uDC52 \uD835\uDC65/\nFigure 1: The NKLM model. The input consisting of a word (either wot−1 or w v t−1) and a fact (at−1) goes into LSTM. The LSTM’s output ht together with the knowledge context e generates the fact key kt. Using the fact key, the fact embedding at is retrieved from the topic knowledge memory. Using at and ht, knowledge-copy switch zt is determined, which in turn determines the next word generation source wvt or w o t . The copied word w o t is a symbol taken from the fact description Oat ."
    }, {
      "heading" : "3.2 INFERENCE",
      "text" : "At each time step, the NKLM follows four sub-steps. First, using both the word and fact outputs from the previous time step as the input of the current time step, we update the LSTM controller. Second, given the output of the LSTM, the NKLM predicts a fact (including NaF) and extracts corresponding fact embedding from the knowledge memory. Thirdly, with the extracted fact and the state of the LSTM controller, the NKLM makes a binary decision to choose the source of word generation. Finally, a word is generated according to the chosen source. A model diagram is depicted in Fig. 1. In the following, we describe these four steps in more detail.\n1) Input Representation and LSTM Controller. As shown in Fig. 1, the input at time step t is the concatenation of three embedding vectors corresponding to a fact at−1, a vocabulary word wvt−1, and a copied word wot−1, all predicted in the previous time step. However, because at a time step, the predicted word comes only either from the vocabulary or by copying from the fact description, we set either wvt−1 or w o t−1 to a zero vector when it is not selected in the previous step. As we shall see, we use position embeddings to represent the copied words by its position within the fact description. And, because the dimensions of the vocabulary word embedding and the position embedding for copied words are different, we use such concatenation of wvt−1 and w o t−1 to represent the word input. The resulting input representation xt = fconcat(at−1,wvt−1,w o t−1) is then fed into the LSTM controller, and obtain the output states (ht, ct) = fLSTM(xt,ht−1). Note that at−1 and wot−1 (e.g., corresponding to n-th position) together can deliver information that a symbol in n-th position in the description of fact at−1 was used in the previous time step.\n2) Fact Extraction. Then, we predict a relevant fact at on which the word wt will be based. If the word wt is supposed to be irrelevant to any fact, the NaF type is predicted. Unlike the fact embeddings, we learn the NaF embedding during training.\nPredicting a fact is done in two steps. First, a fact-key kfact ∈ RDa is generated by kfact = ffactkey(ht, ek). Here, ek ∈ RDa is the topic context embedding (or a subgraph embedding of the topic) which encodes information about what facts are available in the knowledge memory so that the key generator adapts to changes in the knowledge memory. For example, if we remove a fact from the memory, without retraining, the fact-key generator should be aware of the absence of that information and thus should not generate a key vector for the removed fact. Although, in the experiments, we use mean-pooling (average of the all fact embeddings in the knowledge memory) to obtain ek, one can also consider using the soft-attention mechanism (Bahdanau et al., 2014). For the fact-key generator ffactkey, we use an MLP with one hidden layer of ReLU nonlinearity.\nThen, using the generated fact-key kfact, we perform key-value lookup over the knowledge memory Fk to predict a fact and retrieve its embedding at,\nP (at|ht) = exp(k>factFk[at])∑ a′ exp(k > factFk[a ′]) , (1)\nat = argmax at∈Fk\nP (at|ht), (2)\nat = Fk[at]. (3)\nNote that in order to perform the copy mechanism, we need to pick a single fact from the knowledge memory instead of using the weighted average of the fact embeddings as in the soft-attention.\n3) Knowledge-Copy Switch. Given the encoding of the context ht and the embedding of the extracted fact at, the model decides the source for the next word generation: either from the vocabulary or from the fact description by copy. As zt = 1 if the word wt is in the vocabulary, we define the probability of selecting copy as:\nẑt = p(1− zt|ht) = sigmoid(fcopy(ht,at)). (4) Here, fcopy is an MLP with one ReLU hidden layer and a single linear output unit. For facts about attributes such as nationality or profession, the words in the fact description (e.g., “American” or “actor”) are likely to be in the vocabulary, but for facts like the year of birth or father name, the model is likely to choose to copy.\n4) Word Generation. Word wt is generated from the source indicated by the copy-switch ẑt as follows:\nwt = { wvt ∈ V, if ẑt < 0.5, wot ∈ Oat , otherwise.\nFor vocabulary word wvt ∈ V , we use the softmax function where each output dimension corresponds to a word in the vocabulary including UNK,\nP (wvt = w|ht) = exp(k>vocaW[w])∑\nw′∈V exp(k > vocaW[w\n′]) . (5)\nwhere kvoca ∈ RDw is obtained by fvoca(ht,at) which is an MLP with a ReLU hidden layer and linear output units of dimension Dw.\nFor knowledge word wot ∈ Oat , we predict the position of the word in the fact description and then copy the word on the predicted position to output. This is because, unlike with the traditional copy mechanism, our context words (i.e., the fact description) often consist of all unknown words and/or are short in length. Copying allows us not to rely on the word embeddings for the knowledge words. Instead, we learn the position embeddings shared among all knowledge words. This makes sense because words in the fact description usually appear one by one in increasing order. Thus, given that the first symbol o1 = “Michelle” was used in the previous time step and prior to that other words such as “President” and “US” were also observed, the model can easily predict that it is time to select the second symbol, i.e., o2 = “Obama”.\nFor this copy-by-position, we first generate the position key kpos ∈ RDo by a function fposkey(ht,at) which is again an MLP with one hidden layer and linear outputs whose dimension is equal to the maximum length of the fact descriptions Nomax = maxa∈F |Oa| where F = ∪kFk. Then, the n-th symbol on ∈ Oat is chosen by\nP (wot = on|ht, at) = exp(k>posP[n])∑ n′ exp(k > posP[n ′]) , (6)\nwith n′ running from 0 to |Oat | − 1. Here, PDo×N o max is the position embedding matrix. Note that Nomax is typically a much smaller number (e.g., 20 in our experiments) than the size of vocabulary. The position embedding matrix P is learned during training.\nAlthough in this paper we find that the simple position prediction performs well, we note that one could also consider a more advanced encoding such as one based on a convolutional network (Kim, 2014) to model the fact description. At test time, to compute p(wkt |wk<t), we can obtain {zk<t, ak<t} from {wk<t} and Fk using the automatic labeling script, and perform the above inference process with hard decisions taken about zt and at based on the model’s predictions."
    }, {
      "heading" : "3.3 LEARNING",
      "text" : "Given word observations {Wk}Kk=1 and knowledge {Fk}Kk=1, our objective is to maximize the log-likelihood of the observed words w.r.t the model parameter θ,\nθ∗ = argmax θ ∑ k logPθ(Wk|Fk). (7)\nBecause, given Wk and Fk, a sequence of Yk = {yt = (wt, zt, at)}t=1:|Wk| is deterministically induced for each word wt, the following equality is satisfied\nPθ(Wk|Fk) = Pθ(Yk|Fk). (8)\nBy the chain rule, we can decompose the probability of the observation Yk as\nlogPθ(Yk|Fk) = |Yk|∑ t=1 logPθ(y k t |yk1:t−1,Fk). (9)\nThen, after omitting Fk and k for simplicity, we can rewrite the single step conditional probability as\nPθ(yt|y1:t−1) = Pθ(wt, at, zt|ht) = Pθ(wt|at, zt, ht)Pθ(at|ht)Pθ(zt|ht). (10)\nWe maximize the above objective using stochastic gradient optimization."
    }, {
      "heading" : "4 EVALUATION",
      "text" : ""
    }, {
      "heading" : "4.1 WIKIFACTS DATASET",
      "text" : "An obstacle in developing the above model is the lack of the dataset where the text corpus is aligned with facts at the word level. To this end, we produced the WikiFacts dataset by aligning Wikipedia descriptions with corresponding Freebase facts. Because many Freebase topics provide a link to its corresponding topic in Wikipedia, we choose a set of topics for which both a Freebase entity and a Wikipedia description exist. In the experiments, we used a version called WikiFacts-FilmActor-v0.1 where the domain is restricted to the /Film/Actor in Freebase.\nFor all object entity descriptions {Oak} associated with Fk, we performed string matching to the Wikipedia description Wk. We used the summary part (first few paragraphs) of the Wikipedia page as text to be modeled but discarded topics for which the number of facts is greater than 1000 or the Wikipedia description is too short (< 3 sentences). For the string matching, we also used the synonyms and alias provided by WordNet (Miller, 1995) and Freebase.\nWe augmented the fact set Fk with the anchor facts Ak whose relationship is all set to UnknownRelation. That is, observing that an anchor (words under hyperlink) in Wikipedia descriptions has a corresponding Freebase entity as well as being semantically closely related to the topic in which the anchor is found, we make a synthetic fact of the form (Topic, UnknownRelation, Anchor). This potentially compensates for some missing facts in Freebase. Because we extract the anchor facts from the full Wikipedia page and they all share the same relation, it is more challenging for the model to use these anchor facts than using the Freebase facts. As a result, for each word w in the dataset, we have a tuple (w, zw, aw, kw). Here, kw is the topic where w appears. We provide a summary of the dataset statistics in Table 1. The dataset will be available on a public webpage4."
    }, {
      "heading" : "4.2 EXPERIMENTS",
      "text" : "Setup. We split the dataset into 80/10/10 for train, validation, and test. As a baseline model, we use the RNNLM. For both the NKLM and the RNNLM, two-layer LSTMs with dropout regularization (Zaremba et al., 2014) are used. We tested models with different numbers of LSTM hidden units [200, 500, 1000], and report results from the 1000 hidden-unit model. For the NKLM, we set the symbol embedding dimension to 40 and word embedding dimension to 400. Under this setting, the number of parameters in the NKLM is slightly smaller than that of the RNNLM. We used\n4https://bitbucket.org/skaasj/wikifact_filmactor\n100-dimension TransE embeddings for Freebase entities and relations, and concatenate the relation and object embeddings to obtain fact embeddings. We averaged all fact embeddings in Fk to obtain the topic context embedding ek. We unrolled the LSTMs for 30 steps and used minibatch size 20. We trained the models using stochastic gradient ascent with gradient clipping range [-5,5]. The initial learning rate was set to 0.5 for the NKLM and 1.5 for the RNNLM, and decayed after every epoch by a factor of 0.98. We trained for 50 epochs and report the results chosen by the best validation set results.\nEvaluation metric. The perplexity exp(− 1N ∑N i=1 log pwi) is the standard performance metric for language modeling. This, however, has a problem in evaluating language models for a corpus containing many named entities: a model can get good perplexity by accurately predicting UNK words. As an extreme example, when all words in a sentence are unknown words, a model predicting everything as UNK will get a good perplexity. Considering that unknown words provide virtually no useful information, this is clearly a problem in tasks such as question answering, dialogue modeling, and knowledge language modeling.\nTo this end, we introduce a new evaluation metric, called the Unknown-Penalized Perplexity (UPP), and evaluate the models on this metric as well as the standard perplexity (PPL). Because the actual word underlying the UNK should be one of the out-of-vocabulary (OOV) words, in UPP, we penalize the likelihood of unknown words as follows:\nPUPP(wunk) = P (wunk)/|Vtotal \\ Vvoca|. Here, Vtotal is a set of all unique words in the corpus, and Vvoca is the vocabulary used in the softmax. In other words, in UPP we assume that the OOV set is equal to |Vtotal\\Vvoca| and thus assign a uniform probability to OOV words. In another version, UPP-fact, we consider the fact that the RNNLM can also use the knowledge given to the NKLM to some extent, but with limited capability (because the model is not designed for it). For this, we assume that the OOV set is equal to the total knowledge vocabulary of a topic k, i.e., PUPP-fact(wunk) = P (wunk)/|Ok|, where Ok = ∪iOak,i . In other words, by using UPP-fact, we assume that, for an unknown word, the RNNLM can pick one of the knowledge words with uniform probability. We describe the detail results and discussion on the experiments in the captions of Table 2, 3, and 4.\nObservations from the experiment results. Our observations from the experiment results are as follows. (a) The NKLM outperforms the RNNLM in all three perplexity measures. (b) The copy mechanism is the key of the significant performance improvement. Without the copy mechanism, the NKLM still performs better than the RNNLM due to its usage of the fact information, but the improvement is not so significant. (c) The NKLM results in a much smaller number of UNKs (roughly, a half of the RNNLM). (d) When no knowledge is available, the NKLM performs as well as the\nRNNLM. (e) KG embedding using TransE is an efficient way to initialize the fact embeddings. (f) The NKLM generates named entities in the provided facts whereas the RNNLM generates many more UNKs. (g) The NKLM shows its ability to adapt immediately to the change of the knowledge. (h) The standard perplexity is significantly affected by the prediction accuracy on the unknown words. Thus, one need carefully consider it as a metric for knowledge-related language models."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we presented a novel Neural Knowledge Language Model (NKLM) that brings the symbolic knowledge from a knowledge graph into the expressive power of RNN language models. The\nNKLM significantly outperforms the RNNLM in terms of perplexity and generates named entities which are not observed during training, as well as immediately adapting to changes in knowledge. We believe that the WikiFact dataset introduced in this paper, can be useful in other knowledge-related language tasks as well. In addition, the Unknown-Penalized Perplexity introduced in this paper in order to resolve the limitation of the standard perplexity, can be useful in evaluating other language tasks. The task that we investigated in this paper is limited in the sense that we assume that the true topic of a given description is known. Relaxing this assumption by making the model search for proper topics on-the-fly will make the model more practical. We believe that there are many more open research challenges related to the knowledge language models."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank Alberto Garcı́a-Durán, Caglar Gulcehre, Chinnadhurai Sankar, Iulian Serban and Sarath Chandar for feedback and discussions as well as the developers of Theano (Bastien et al., 2012), NSERC, CIFAR, Samsung and Canada Research Chairs for funding, and Compute Canada for computing resources."
    }, {
      "heading" : "APPENDIX: HEATMAPS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1211.5590,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor" ],
      "venue" : "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,",
      "citeRegEx" : "Bollacker et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bordes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1506.02075,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems",
      "author" : [ "Asli Celikyilmaz", "Dilek Hakkani-Tur", "Panupong Pasupat", "Ruhi Sarikaya" ],
      "venue" : "In 2015 AAAI Spring Symposium Series,",
      "citeRegEx" : "Celikyilmaz et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Celikyilmaz et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li" ],
      "venue" : "learning. CoRR,",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Traversing knowledge graphs in vector space",
      "author" : [ "Kelvin Gu", "John Miller", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "Gu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A neural network for factoid question answering over paragraphs",
      "author" : [ "Mohit Iyyer", "Jordan L Boyd-Graber", "Leonardo Max Batista Claudino", "Richard Socher", "Hal Daumé III" ],
      "venue" : "EMNLP",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2014
    }, {
      "title" : "On using very large target vocabulary for neural machine",
      "author" : [ "Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : "arXiv preprint arXiv:1602.02410,",
      "citeRegEx" : "Jozefowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "EMNLP 2014,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Leveraging lexical resources for learning entity embeddings in multi-relational data",
      "author" : [ "Teng Long", "Ryan Lowe", "Jackie Chi Kit Cheung", "Doina Precup" ],
      "venue" : null,
      "citeRegEx" : "Long et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In INTERSPEECH 2010,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Miller.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Andriy Mnih", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mnih and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2009
    }, {
      "title" : "A fast and simple algorithm for training neural probabilistic language models",
      "author" : [ "Andriy Mnih", "Yee Whye Teh" ],
      "venue" : null,
      "citeRegEx" : "Mnih and Teh.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mnih and Teh.",
      "year" : 2012
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Frederic Morin", "Yoshua Bengio" ],
      "venue" : "AISTATS",
      "citeRegEx" : "Morin and Bengio.,? \\Q2005\\E",
      "shortCiteRegEx" : "Morin and Bengio.",
      "year" : 2005
    }, {
      "title" : "A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction",
      "author" : [ "Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich" ],
      "venue" : "arXiv preprint arXiv:1503.00759,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2015
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural networks",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau" ],
      "venue" : "30th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Serban et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "arXiv preprint arXiv:1506.05869,",
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Weston et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1409.2329,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : ") When trained with a very large corpus, traditional language models have demonstrated to some extent the ability to encode/decode knowledge (Vinyals & Le, 2015; Serban et al., 2015).",
      "startOffset" : 141,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "In particular, for applications such as question answering (Iyyer et al., 2014; Weston et al., 2016; Bordes et al., 2015) and dialogue modeling (Vinyals & Le, 2015; Serban et al.",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "In particular, for applications such as question answering (Iyyer et al., 2014; Weston et al., 2016; Bordes et al., 2015) and dialogue modeling (Vinyals & Le, 2015; Serban et al.",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "In particular, for applications such as question answering (Iyyer et al., 2014; Weston et al., 2016; Bordes et al., 2015) and dialogue modeling (Vinyals & Le, 2015; Serban et al.",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : ", 2015) and dialogue modeling (Vinyals & Le, 2015; Serban et al., 2015), these words are of our main interest.",
      "startOffset" : 30,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "Specifically, in the recurrent neural network language model (RNNLM) (Mikolov et al., 2010) the computational complexity is linearly dependent on the number of vocabulary words.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "In particular, we incorporate symbolic knowledge provided by a knowledge graph (Nickel et al., 2015) into the RNNLM.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "The KG embedding methods (Bordes et al., 2011; 2013) provide distributed representations for the entities in the KG.",
      "startOffset" : 25,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "The graph can be traversed for reasoning (Gu et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "For each topic in the dataset, a set of facts from the Freebase KG (Bollacker et al., 2008) and a Wikipedia description of the same topic is provided along with the alignment information.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "There have been remarkable advances in language modeling research based on neural networks (Bengio et al., 2003; Mikolov et al., 2010).",
      "startOffset" : 91,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "There have been remarkable advances in language modeling research based on neural networks (Bengio et al., 2003; Mikolov et al., 2010).",
      "startOffset" : 91,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "It is especially noteworthy that the RNNLM using the Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) has recently advanced to the level of outperforming carefully-tuned traditional n-gram based language models (Jozefowicz et al., 2016).",
      "startOffset" : 225,
      "endOffset" : 250
    }, {
      "referenceID" : 13,
      "context" : "These methods approximate the softmax output using hierarchical softmax (Morin & Bengio, 2005; Mnih & Hinton, 2009), importance sampling (Jean et al., 2015), noise contrastive estimation (Mnih & Teh, 2012), etc.",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : ", 2015) have been adopted to implement the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016) and applied to machine translation and text summarization.",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Our knowledge memory is also related to the recent literature on neural networks with external memory (Bahdanau et al., 2014; Weston et al., 2015; Graves et al., 2014).",
      "startOffset" : 102,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "Our knowledge memory is also related to the recent literature on neural networks with external memory (Bahdanau et al., 2014; Weston et al., 2015; Graves et al., 2014).",
      "startOffset" : 102,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "The idea of jointly using Wikipedia and knowledge graphs has also been used in the context of enriching word embedding (Celikyilmaz et al., 2015; Long et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "The idea of jointly using Wikipedia and knowledge graphs has also been used in the context of enriching word embedding (Celikyilmaz et al., 2015; Long et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "Our knowledge memory is also related to the recent literature on neural networks with external memory (Bahdanau et al., 2014; Weston et al., 2015; Graves et al., 2014). In Weston et al. (2015), given simple sentences as facts which are stored in the external memory, the question answering task is studied.",
      "startOffset" : 103,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "We obtain these entity embeddings from a preliminary run of a knowledge graph embedding method such as TransE (Bordes et al., 2013).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Although, in the experiments, we use mean-pooling (average of the all fact embeddings in the knowledge memory) to obtain ek, one can also consider using the soft-attention mechanism (Bahdanau et al., 2014).",
      "startOffset" : 182,
      "endOffset" : 205
    }, {
      "referenceID" : 15,
      "context" : "Although in this paper we find that the simple position prediction performs well, we note that one could also consider a more advanced encoding such as one based on a convolutional network (Kim, 2014) to model the fact description.",
      "startOffset" : 189,
      "endOffset" : 200
    }, {
      "referenceID" : 18,
      "context" : "For the string matching, we also used the synonyms and alias provided by WordNet (Miller, 1995) and Freebase.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "For both the NKLM and the RNNLM, two-layer LSTMs with dropout regularization (Zaremba et al., 2014) are used.",
      "startOffset" : 77,
      "endOffset" : 99
    } ],
    "year" : 2016,
    "abstractText" : "Current language models have significant limitations in their ability to encode and decode factual knowledge. This is mainly because they acquire such knowledge based on statistical co-occurrences, even if most of the knowledge words are rarely observed named entities. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by a knowledge graph with the RNN language model. The model predicts whether the word to generate has an underlying fact or not. Then, a word is either generated from the vocabulary or copied from the description of the predicted fact. We train and test the model on a new dataset, WikiFacts. In experiments, we show that the NKLM significantly improves the perplexity while generating a much smaller number of unknown words. In addition, we demonstrate that the sampled descriptions include named entities which used to be the unknown words in RNN language models.",
    "creator" : "TeX"
  }
}