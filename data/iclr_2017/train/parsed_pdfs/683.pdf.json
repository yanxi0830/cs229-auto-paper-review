{
  "name" : "683.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Residual Networks, a type of deep network recently introduced in He et al. (2015a), are characterized by the use of shortcut connections (sometimes also called skip connections), which connect the input of a layer of a deep network to the output of another layer positioned a number of levels “above” it. The result is that each one of these shortcuts shows that networks can be build in blocks, which rely on both the output of the previous layer and the previous block.\nResidual Networks have been developed with many more layers than traditional Deep Networks, in some cases with over 1000 blocks, such as the networks in He et al. (2016). A recent study in Veit et al. (2016) compares Residual Networks to an ensemble of smaller networks. This is done by unfolding the shortcut connections into the equivalent tree structure, which closely resembles an ensemble. An example of this can be shown in Figure 1.\nDense Convolutional Neural Networks Huang et al. (2016) are another type of network that makes use of shortcuts, with the difference that each layer is connected to all its ancestor layers directly by a shortcut. Similarly, these could be also unfolded into an equivalent ensemble.\nTrue ensemble methods are often left as an afterthought in Deep Learning models: it is generally considered sufficient to treat the Deep Learning method as a “black-box” and use a well-known generic Ensemble method to obtain marginal improvements on the original results. Whilst this is an effective way of improving on existing results without much additional effort, we find that it can amount to a waste of computations. Instead, it would be much better to apply an Ensemble method that is aware, and makes us of, the underlying Deep Learning algorithm’s architecture.\nWe define such methods as “white-box” Ensembles, which allow us to improve on the generalisation and training speed compared to traditional Ensembles, by making use of particular properties of the\nbase classifier’s learning algorithm and architecture. We propose a new such method, which we call Boosted Residual Networks, which makes use of developments in Deep Learning, previous other white-box Ensembles and combines several ideas to achieve improved results on benchmark datasets.\nUsing a white-box ensemble allows us to improve on the generalisation and training speed by making use of the knowledge of the base classifier’s structure and architecture. Experimental results show that Boosted Residual Networks achieves improved results on benchmark datasets.\nThe next section presents the background on Deep Incremental Boosting. Then the proposed Boosted Residual Networks method is described. Experiments and results are discussed next, and the paper ends with conlusions."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "Deep Incremental Boosting, introduced in Mosca & Magoulas (2016a), is an example of such whitebox ensemble method developed for building ensembles Convolutional Networks. The method makes use of principles from transfer of learning, like for example those used in Yosinski et al. (2014), applying them to conventional AdaBoost (Schapire (1990)). Deep Incremental Boosting increases the size of the network at each round by adding new layers at the end of the network, allowing subsequent rounds of boosting to run much faster. In the original paper on Deep Incremental Boosting Mosca & Magoulas (2016a), this has been shown to be an effective way to learn the corrections introduced by the emphatisation of learning mistakes of the boosting process. The argument as to why this works effectively is based on the fact that the datasets at rounds s and t+ 1 will be mostly similar, and therefore a classifier ht that performs better than randomly on the resampled dataset Xt will also perform better than randomly on the resampled dataset Xt+1. This is under the assumption that both datasets are sampled from a common ancestor set Xa. It is subsequently shown that such a classifier can be re-trained on the differences between Xt and Xt+1.\nThis practically enables the ensemble algorithm to train the subsequent rounds for a considerably smaller number of epochs, consequently reducing the overall training time by a large factor. The original paper also provides a conjecture-based justification for why it makes sense to extend the previously trained network to learn the “corrections” taught by the boosting algorithm. A high level description of the method is shown in Algorithm 1, and the structure of the network at each round is illustrated in Figure 3.\nAlgorithm 1 Deep Incremental Boosting\nD0(i) = 1/M for all i t = 0 W0 ← randomly initialised weights for first classifier while t < T do\nXt ← pick from original training set with distribution Dt ut ← create untrained classifier with additional layer of shape Lnew copy weights from Wt into the bottom layers of ut ht ← train ut classifier on current subset Wt+1 ← all weights from ht ǫt = 1 2 ∑ (i,y)∈B Dt(i)(1− ht(xi, yi) + ht(xi, y)) βt = ǫt/(1− ǫt) Dt+1(i) = Dt(i) Zt · β(1/2)(1+ht(xi,yi)−ht(xi,y)) where Zt is a normalisation factor such that Dt+1 is a distribution αt =\n1 βt\nt = t+ 1 end while H(x) = argmaxy∈Y ∑T t=1 logαtht(x, y)"
    }, {
      "heading" : "3 CREATING THE BOOSTED RESIDUAL NETWORK",
      "text" : "In this section we propose a method for generating Boosted Residual Networks. This works by increasing the size of an original residual network by one residual block at each round of boosting. The method achieves this by selecting an injection point index pi at which the new block is to be added, which is not necessarily the last block in the network, and by transferring the weights from the layers below pi in the network trained at the previous round of boosting.\nBecause the boosting method performs iterative re-weighting of the training set to skew the resample at each round to emphasize the training examples that are harder to train, it becomes necessary to utilise the entire ensemble at test time, rather than just use the network trained in the last round. This has the effect that the Boosted Residual Networks cannot be used as a way to train a single Residual Network incrementally. However, as we will discuss later, it is possible to alleviate this situation by deriving an approach that uses bagging instead of boosting; therefore removing the necessity to use the entire ensemble at test time. It is also possible to delete individual blocks from a Residual Network at training and/or testing time, as presented in He et al. (2015a), however this issue is considered out of the scope of this paper.\nThe iterative algorithm used in the paper is shown in Algorithm 2. At the first round, the entire training set is used to train a network of the original base architecture, for a number of epochs n0. After the first round, the following steps are taken at each subsequent round t:\n• The ensemble constructed so far is evaluated on the training set to obtain the set errors ǫ, so that a new training set can be sampled from the original training set. This is a step common to all boosting algorithms.\n• A new network is created, with the addition of a new block of layers Bnew immediately after position pt, which is determined as an initial pre-determined position p0 plus an offset i ∗ δp for all the blocks added at previous layers. This puts the new block of layers immediately after the block of layers added at the previous round, so that all new blocks are effectively added sequentially.\n• The weights from the layers below pt are copied from the network trained at round t − 1 to the new network. This step allows to considerably shorten the training thanks to the transfer of learning shown in Yosinski et al. (2014).\n• The newly created network is subsequently trained for a reduced number of epochs nt>0.\n• The new network is added to the ensemble following the traditional rules and weight αt used in AdaBoost.\nFigure 3 shows a diagram of how the Ensemble is constructed by deriving the next network at each round of boosting from the network used in the previous round.\nAlgorithm 2 Boosted Residual Networks\nD0(i) = 1/M for all i t = 0 W0 ← randomly initialised weights for first classifier set initial injection position p0 while t < T do\nXt ← pick from original training set with distribution Dt ut ← create untrained classifier with an additional block Bnew of pre-determined shapeNnew determine block injection position pt = pt−1 + |Bnew| connect the input of Bnew to the output of layer pt − 1 connect the output of Bnew and of layer pt − 1 to a merge layer mi connect the merge layer to the remainder of the network copy weights from Wt into the bottom layers l < pt of ut ht ← train ut classifier on current subset Wt+1 ← all weights from ht ǫt = 1 2 ∑ (i,y)∈B Dt(i)(1− ht(xi, yi) + ht(xi, y)) βt = ǫt/(1− ǫt) Dt+1(i) = Dt(i) Zt · β(1/2)(1+ht(xi,yi)−ht(xi,y)) where Zt is a normalisation factor such that Dt+1 is a distribution αt =\n1 βt\nt = t+ 1 end while H(x) = argmaxy∈Y ∑T t=1 logαtht(x, y)\nWe identified a number of optional variations to the algorithm that may be implemented in practice, which we have empirically established as not having an impact on the overall performance of the network. We report them here for completeness.\n• Freezing the layers that have been copied from the previous round.\n• Only utilising the weights distribution for the examples in the training set instead of resampling, as an input to the training algorithm.\n• Inserting the new block always at the same position, rather than after the previouslyinserted block (we found this to affect performance negatively)."
    }, {
      "heading" : "3.1 COMPARISON TO APPROXIMATE ENSEMBLES",
      "text" : "While both Residual Networks and Densely Connected Convolutional Networks may be unfolded into an equivalent ensemble, we note that there is a differentiation between an actual ensemble method and an ensemble “approximation”. During the creation of an ensemble, one of the principal factors is the creation of diversity: each base learner is trained independently, on variations (resamples in the case of boosting algorithms) of the training set, so that each classifier is guaranteed to learn a different function that represents an approximation of the training data. This is the enabling factor for the ensemble to perform better in aggregate.\nIn the case of Densely Connected Convolutional Networks (DCCN) specifically, one may argue that a partial unfolding of the network could be, from a schematic point of view, very similar to an ensemble of incrementally constructed Residual Networks. We make the observation that, although this would be correct, on top of the benefit of diversity, our method also provides a much faster training methodology: the only network that is trained for a full schedule is the network created at the first round, which is also the smallest one. All subsequent networks are trained for a much shorter schedule, saving a considerable amount of time. Additionally, while the schematic may seem identical, there is a subtle difference: each member network outputs a classification of its own, which is then aggregated by weighted averaging, whilst in a DCCN the input of the final aggregation layer is the output of each underlying set of layers. We conjecture that this aggressive dimensionality reduction before the aggregation will have a regularising effect on the ensemble."
    }, {
      "heading" : "4 EXPERIMENTS AND DISCUSSION",
      "text" : "In the experiments we used the MNIST, CIFAR-10 and CIFAR-100 datasets, and compared Boosted Residual Networks (BRN) with an equivalent Deep Incremental Boosting (DIB) without the skipconnections, AdaBoost with the equivalent Residual Network as its base classifier (AdaBoost), and the single Residual Network (Single Net) In order to reduce noise, we aligned the random initialisation of all networks across experiments, by fixing the seeds for the random number generators, and no dataset augmentation was used, both online and offline. Results are reported in Table 1, while Figure 4 shows a side-by-side comparison of accuracy levels at each round of boosting for both DIB and BRN on the MNIST and CIFAR-100 test sets. This figure illustrates how BRNs are able to consistently outperform DIB, regardless of ensemble size, and although such differences still fall within a Bernoulli confidence interval of 95%, we make the note that this does not take account of the fact that all the random initialisations were aligned, so both methods started with the exact same network.\nTable 2 shows that this is achieved without significant changes in the training time1. The main speed increase is due to the fact that the only network being trained with a full schedule is the first network, which is also the smallest, whilst all other derived networks are trained for a much shorter schedule (in this case only 10% of the original training schedule).\nThe initial network architectures for the first round of boosting are shown in Table 3a for MNIST, and Table 3b for CIFAR-10 and CIFAR-100. It is worth mentioning that we used relatively simple network architectures that were fast to train, which still perform well on the datasets at hand, with accuracy close to, but not comparable to, the state-of-the-art. This enabled us to test larger Ensembles within an acceptable training time.\nTraining used the WAME method (Mosca & Magoulas (2016b)), which has been shown to be faster than Adam and RMSprop, whilst still achieving comparable generalisation. This is thanks to a\n1In some cases BRN is actually faster than DIB, but we believe this to be just noise due to external factors such as system load.\nspecific weight-wise learning rate acceleration factor that is determined based only on the sign of the current and previous partial derivative ∂E(x) ∂wij . For the single Residual Network, and for the networks in AdaBoost, we trained each member for 100 epochs. For Deep Incremental Boosting and Boosted Residual Networks, we trained the first round for 50 epochs, and every subsequent round for 10 epochs, and ran all the algorithms for 10 rounds of boosting, except for the single network. The structure of each incremental block added to Deep Incremental Boosting and Boosted Residual Networks at each round is shown in Table 4a for MNIST, and in Table 4b for CIFAR-10 and CIFAR-100. All layers were initialised following the reccommendations in He et al. (2015b).\nDistilled Boosted Residual Network: DBRN In another set of experiments we tested the performance of a Distilled Boosted Residual Network (DBRN). Distillation has been shown to be an effective process for regularising large Ensembles of Convolutional Networks in Mosca & Magoulas (2016c), and we have applied the same methodology to the proposed Boosted Residual Network. For the distilled network structure we used the same architecture as that of the Residual Network from the final round of boosting. Accuracy results in testing are presented in Table 5, and for completeness of comparison we also report the results for the distillation of DIB, following the same procedure, as DDIB.\nBagged Residual Networks: BARN We experimented with substituting the boosting algorithm with a simpler bagging algorithm (Breiman (1996)) to evaluate whether it would be possible to only use the network from the final round of bagging as an approximation of the Ensemble. We called this the Bagged Approximate Residual Networks (BARN) method. We then also tested the performance of the Distilled version of the whole Bagging Ensemble for comparison. The results are reported as “DBARN”. The results are reported in Table 6. It is clear that trying to use the last round of bagging is not comparable to using the entire Bagging ensemble at test time, or deriving a new distilled network from it."
    }, {
      "heading" : "5 CONCLUSIONS AND FUTURE WORK",
      "text" : "In this paper we have derived a new ensemble algorithm specifically tailored to Convolutional Networks to generate Boosted Residual Networks. We have shown that this surpasses the performance of a single Residual Network equivalent to the one trained at the last round of boosting, of an ensemble of such networks trained with AdaBoost, and Deep Incremental Boosting on the MNIST and CIFAR datasets, without using augmentation techniques.\nWe then derived and looked at a distilled version of the method, and how this can serve as an effective way to reduce the test-time cost of running the Ensemble. We used Bagging as a proxy to test generating the approximate Residual Network, which, with the parameters tested, does not perform as well as the original Residual Network, BRN or DBRN.\nFurther experimentation of the Distilled methods presented in the paper, namely DBRN and DBARN, is necessary to fully investigate their behaviour. This is indeed part of our work in the near future. Additionally, the Residual Networks built in our experiments were comparatively smaller than those that achieve state-of-the-art performance. Reaching state-of-the-art on specific benchmark datasets was not our goal, instead we intended to show that we developed a methodology that makes it feasible to created ensembles of Residual Networks following a “white-box” approach to\nsignificantly improve the training times and accuracy levels. Nevertheless, it might be appealing in the future to evaluate the performance improvements obtained when creating ensembles of larger, state-of-the-art, networks. Additional further investigation could also be conducted on the creation of Boosted Densely Connected Convolutional Networks, by applying the same principle to DCCN instead of Residual Networks."
    } ],
    "references" : [ {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Kilian Q Weinberger" ],
      "venue" : "arXiv preprint arXiv:1608.06993,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep incremental boosting",
      "author" : [ "Alan Mosca", "George Magoulas" ],
      "venue" : "GCAI 2016. 2nd Global Conference on Artificial Intelligence,",
      "citeRegEx" : "Mosca and Magoulas.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mosca and Magoulas.",
      "year" : 2016
    }, {
      "title" : "Training convolutional networks with weight-wise adaptive learning rates",
      "author" : [ "Alan Mosca", "George D. Magoulas" ],
      "venue" : "In Under Review,",
      "citeRegEx" : "Mosca and Magoulas.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mosca and Magoulas.",
      "year" : 2016
    }, {
      "title" : "Regularizing deep learning ensembles by distillation",
      "author" : [ "Alan Mosca", "George D Magoulas" ],
      "venue" : "In 6th International Workshop on Combinations of Intelligent Methods and Applications (CIMA 2016),",
      "citeRegEx" : "Mosca and Magoulas.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mosca and Magoulas.",
      "year" : 2016
    }, {
      "title" : "The strength of weak learnability",
      "author" : [ "R.E. Schapire" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schapire.,? \\Q1990\\E",
      "shortCiteRegEx" : "Schapire.",
      "year" : 1990
    }, {
      "title" : "Residual Networks Behave Like Ensembles of Relatively Shallow Networks",
      "author" : [ "A. Veit", "M. Wilber", "S. Belongie" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Veit et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Veit et al\\.",
      "year" : 2016
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Residual Networks, a type of deep network recently introduced in He et al. (2015a), are characterized by the use of shortcut connections (sometimes also called skip connections), which connect the input of a layer of a deep network to the output of another layer positioned a number of levels “above” it.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "Residual Networks, a type of deep network recently introduced in He et al. (2015a), are characterized by the use of shortcut connections (sometimes also called skip connections), which connect the input of a layer of a deep network to the output of another layer positioned a number of levels “above” it. The result is that each one of these shortcuts shows that networks can be build in blocks, which rely on both the output of the previous layer and the previous block. Residual Networks have been developed with many more layers than traditional Deep Networks, in some cases with over 1000 blocks, such as the networks in He et al. (2016). A recent study in Veit et al.",
      "startOffset" : 65,
      "endOffset" : 642
    }, {
      "referenceID" : 0,
      "context" : "Residual Networks, a type of deep network recently introduced in He et al. (2015a), are characterized by the use of shortcut connections (sometimes also called skip connections), which connect the input of a layer of a deep network to the output of another layer positioned a number of levels “above” it. The result is that each one of these shortcuts shows that networks can be build in blocks, which rely on both the output of the previous layer and the previous block. Residual Networks have been developed with many more layers than traditional Deep Networks, in some cases with over 1000 blocks, such as the networks in He et al. (2016). A recent study in Veit et al. (2016) compares Residual Networks to an ensemble of smaller networks.",
      "startOffset" : 65,
      "endOffset" : 680
    }, {
      "referenceID" : 3,
      "context" : "Dense Convolutional Neural Networks Huang et al. (2016) are another type of network that makes use of shortcuts, with the difference that each layer is connected to all its ancestor layers directly by a shortcut.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "The method makes use of principles from transfer of learning, like for example those used in Yosinski et al. (2014), applying them to conventional AdaBoost (Schapire (1990)).",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "(2014), applying them to conventional AdaBoost (Schapire (1990)).",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "(2014), applying them to conventional AdaBoost (Schapire (1990)). Deep Incremental Boosting increases the size of the network at each round by adding new layers at the end of the network, allowing subsequent rounds of boosting to run much faster. In the original paper on Deep Incremental Boosting Mosca & Magoulas (2016a), this has been shown to be an effective way to learn the corrections introduced by the emphatisation of learning mistakes of the boosting process.",
      "startOffset" : 48,
      "endOffset" : 323
    }, {
      "referenceID" : 0,
      "context" : "It is also possible to delete individual blocks from a Residual Network at training and/or testing time, as presented in He et al. (2015a), however this issue is considered out of the scope of this paper.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "This step allows to considerably shorten the training thanks to the transfer of learning shown in Yosinski et al. (2014). • The newly created network is subsequently trained for a reduced number of epochs nt>0.",
      "startOffset" : 98,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "All layers were initialised following the reccommendations in He et al. (2015b).",
      "startOffset" : 62,
      "endOffset" : 80
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we present a new ensemble method, called Boosted Residual Networks, which builds an ensemble of Residual Networks by growing the member network at each round of boosting. The proposed approach combines recent developements in Residual Networks a method for creating very deep networks by including a shortcut layer between different groups of layers with the Deep Incremental Boosting, which has been proposed as a methodology to train fast ensembles of networks of increasing depth through the use of boosting. We demonstrate that the synergy of Residual Networks and Deep Incremental Boosting has better potential than simply boosting a Residual Network of fixed structure or using the equivalent Deep Incremental Boosting without the shortcut layers.",
    "creator" : "gnuplot 4.6 patchlevel 6"
  }
}