{
  "name" : "662.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UOUS AND DISCRETE ADDRESSING SCHEMES",
    "authors" : [ "Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio" ],
    "emails" : [ "name.lastname@umontreal.ca", "name.lastname@nyu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Designing general-purpose learning algorithms is one of the long-standing goals of artificial intelligence. Despite the success of deep learning in this area (see, e.g., (Goodfellow et al., 2016)) there are still a set of complex tasks that are not well addressed by conventional neural networks. Those tasks often require a neural network to be equipped with an explicit, external memory in which a larger, potentially unbounded, set of facts need to be stored. They include, but are not limited to, episodic question-answering (Weston et al., 2015b; Hermann et al., 2015; Hill et al., 2015), compact algorithms (Zaremba et al., 2015), dialogue (Serban et al., 2016; Vinyals & Le, 2015) and video caption generation (Yao et al., 2015).\nRecently two promising approaches based on neural networks to this type of tasks have been proposed. Memory networks (Weston et al., 2015b) explicitly store all the facts, or information, available for each episode in an external memory (as continuous vectors) and use the attention-based mechanism to index them when returning an output. On the other hand, neural Turing machines (NTM, (Graves et al., 2014)) read each fact in an episode and decides whether to read, write the fact or do both to the external, differentiable memory.\nA crucial difference between these two models is that the memory network does not have a mechanism to modify the content of the external memory, while the NTM does. In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks (Bordes et al., 2015; Dodge et al., 2015). On the contrary, the NTM has mainly been tested on a series of small-scale, carefully-crafted tasks such as copy and associative recall. The NTM, however is more expressive, precisely because it can store and modify the internal state of the network as it processes an episode.\nThe original NTM supports two modes of addressing (which can be used simultaneously.) They are content-based and location-based addressing. We notice that the location-based strategy is based on linear addressing. The distance between each pair of consecutive memory cells is fixed to a constant. We address this limitation, in this paper, by introducing a learnable address vector for each memory cell of the NTM with least recently used memory addressing mechanism, and we call this variant a dynamic neural Turing machine (D-NTM).\nWe evaluate the proposed D-NTM on the full set of Facebook bAbI task (Weston et al., 2015b) using either continuous, differentiable attention or discrete, non-differentiable attention (Zaremba & Sutskever, 2015) as an addressing strategy. Our experiments reveal that it is possible to use the discrete, non-differentiable attention mechanism, and in fact, the D-NTM with the discrete attention and GRU controller outperforms the one with the continuous attention. After we published our paper on arXiv, a new extension of NTM called DNC (Graves et al., 2016) has also provided results on bAbI task as well.\nWe also provide results on sequential-MNIST and algorithmic tasks proposed by (Graves et al., 2014) in order to investigate the ability of our model when dealing with long-term dependencies.\nOur Contributions\n1. We propose a generalization of Neural Turing Machine called a dynamic neural Turing machine (D-NTM) which employs a learnable and location-based addressing. 2. We demonstrate the application of neural Turing machines on a more natural and less toyish task: episodic question-answering besides the toy tasks. We provide detailed analysis of our model on this task. 3. We propose to use the discrete attention mechanism and empirically show that, it can outperform the continuous attention based addressing for episodic QA task. 4. We propose a curriculum strategy for our model with the feedforward controller and discrete attention that improves our results significantly."
    }, {
      "heading" : "2 DYNAMIC NEURAL TURING MACHINE",
      "text" : "The proposed dynamic neural Turing machine (D-NTM) extends the neural Turing machine (NTM, (Graves et al., 2014)) which has a modular design. The NTM consists of two main modules, a controller and, a memory. The controller, which is often implemented as a recurrent neural network, issues a command to the memory so as to read, write to and erase a subset of memory cells. Although the memory was originally envisioned as an integrated module, it is not necessary, and the memory may be an external, black box (Zaremba & Sutskever, 2015)."
    }, {
      "heading" : "2.1 CONTROLLER",
      "text" : "At each time step t, the controller (1) receives an input value xt, (2) addresses and reads the memory and creates the content vector φt, (3) erases/writes a portion of the memory, (4) updates its own hidden state ht, and (5) outputs a value yt (if needed.) In this paper, we use both a gated recurrent unit (GRU, (Cho et al., 2014)) and a feedforward-controller to implement the controller such that for a GRU controller\nht = GRU(xt,ht−1, φt) (1)\nor for a feedforward-controller\nht = σ(xt, φt). (2)"
    }, {
      "heading" : "2.2 MEMORY",
      "text" : "We use a rectangular matrix M ∈ RN×(dc+da) to denote N memory cells. Unlike the original NTM, we partition each memory cell vector into two parts:\nM = [A;C] .\nThe first part A ∈ RN×da is a learnable address matrix, and the second C ∈ RN×dc a content matrix. In other words, each memory cell mi is now\nmi = [ai; ci] .\nThe address part ai is considered a model parameter that is updated during training. During inference, the address part is not overwritten by the controller and remains constant. On the other hand, the content part ci is both read and written by the controller both during training and inference. At the beginning of each episode, the content part of the memory is refreshed to be an all-zero matrix, C0 = 0. This introduction of the learnable address portion for each memory cell allows the model to learn sophisticated location-based addressing strategies. A similar addressing mechanism is also explored in (Reed & de Freitas, 2015) in the context of learning program traces."
    }, {
      "heading" : "2.3 MEMORY ADDRESSING",
      "text" : "Memory addressing in the D-NTM is equivalent to computing an N -dimensional address vector. The DNTM computes three such vectors for respectively reading wt ∈ RN , erasing et ∈ Rdc and writing ut ∈ RN . Specifically for writing, the controller further computes a candidate memory content vector c̄t ∈\nRdc based on its current hidden state of the controller ht ∈ Rdh and the input of the controller scaled with a scalar gate αt which is a function of the hidden state and the input of the controller as well, see Eqn 4.\nαt = f(ht,xt), (3)\nc̄t = ReLU(Wmht + αtWxxt + bm). (4)\nReading With the read vector wt, the content vector read from the memory φt ∈ Rda+dc is retrieved by\nφt = (wt)>Mt−1, (5)\nwhere wt is a row vector.\nErasing and Writing Given the erase, write and candidate memory content vectors (et, utj , and c̄t respectively) generated by a simple MLP conditioned on the hidden state of the controller ht, the memory matrix is updated by,\nCt[j] = (1− etutj) Ct−1[j] + utj c̄t. (6)\nwhere the subscript j in Ct[j] denotes the j-th row of the content part Ct of the memory matrix Mt.\nNo Operation (NOP) As found in (Joulin & Mikolov, 2015), an additional NOP action might be beneficial for the controller not to access the memory once in a while. We model this situation by designating one memory cell as a NOP cell. Reading or writing from this memory cell is ignored."
    }, {
      "heading" : "2.4 LEARNING",
      "text" : "Once the proposed D-NTM is executed, it returns the output distribution p(y|x1, . . . ,xT ). As a result, we define a cost function as the negative log-likelihood:\nC(θ) = 1\nN N∑ n=1 − log p(yn|xn1 , . . . ,xnT ), (7)\nwhere θ is a set of all the parameters. As the proposed D-NTM, just like the original NTM, is fully end-to-end differentiable, we can compute the gradient of this cost function by using backpropagation and learn the parameters of the model with a gradient-based optimization algorithm, such as stochastic gradient descent, to train it end-to-end."
    }, {
      "heading" : "3 ADDRESSING MECHANISM",
      "text" : ""
    }, {
      "heading" : "3.1 ADDRESS VECTORS",
      "text" : "Each of the address vectors (both read and write) is computed in the same way. The way they are computed are very similar to the content based addressing in (Graves et al., 2014). First, the controller computes a key vector:\nkt = W>k h t + bk,\nwhere Wk ∈ RN×(da+dc) and bk ∈ Rda+dc if the read head is being computed, otherwise Wk ∈ RN×dc and bk ∈ Rdc if the write head weights are being computed. They can be the parameters for a specific head (either read or write.) Also, the sharpening factor βt ∈ R≥1 is computed as:\nsoftplus(x) = log(exp(x) + 1) (8)\nβt = softplus(u>β h t + bβ) + 1. (9)\nuβ and bβ are the parameters of the sharpening βt.\nThe address vector is then computed by, zti = β tS ( kt,mti ) (10)\nwti = exp(zti)∑ j exp(z t j) , (11)\nwhere the similarity function S ∈ R≥0 is defined as\nS (x,y) = x · y\n(||x||||y||+ ) ."
    }, {
      "heading" : "3.2 MULTI-STEP ADDRESSING",
      "text" : "At each time-step, controller may require more than one-step for accessing to the memory. The original NTM addresses this by implementing multiple sets of read, erase and write heads. In this paper, we explore an option of allowing each head to operate more than once at each time step, similar to the multi-hop mechanism from the end-to-end memory network (Sukhbaatar et al., 2015)."
    }, {
      "heading" : "3.3 DYNAMIC LEAST RECENTLY USED ADDRESSING",
      "text" : "We introduce a memory addressing schema that can learn to put more emphasis on the least recently used (LRU) memory locations. As observed in (Santoro et al., 2016; Rae et al., 2016), we find it easier to learn the write operations with the use of LRU addressing.\nTo learn a LRU based addressing, first we compute the exponentially moving averages of the logits (zt) as vt, vt = 0.1vt−1 + 0.9zt. We rescale the accumulated vt with γt, such that the controller adjusts the influence of how much previously written memory locations should effect the attention weights of a particular time-step. Next, we subtract vt from zt in order to reduce the weights of previously read or written memory locations. γt is a shallow MLP with a scalar output and it is conditioned on the hidden state of the controller. γt is parametrized with the parameters uγ and bγ ,\nγt = sigmoid(u>γ ht + bγ), (12)\nwt = softmax(zt − γtvt−1). (13)\nThis addressing method increases the weights of the least recently used rows of the memory. The magnitude of the influence of the least-recently used memory locations is being learned and adjusted with γt. Our LRU addressing is dynamic due to the model’s ability to switch between pure content-based addressing and LRU. During the training, we do not backpropagate through vt. Due to the dynamic nature of this addressing mechanism, it can be used for both read and write operations. If needed, the model will automatically learn to disable LRU while reading from the memory."
    }, {
      "heading" : "4 GENERATING DISCRETE ADDRESS VECTORS",
      "text" : "In this section, we describe the discrete attention based addressing strategy.\nDiscrete Addressing Let us use w to denote an address vector (either read, write or erase) at time t. By definition in Eq. (10), every element in this address vector is positive and sums up to one. In other words, we can treat this vector as the probabilities of a categorical distribution C(w) with dim(w) choices:\np(j) = wj ,\nwhere wj is the j-th element of w. We can readily sample from this categorical distribution and form an one-hot vector w̃ such that\nw̃k = I(k = j),\nwhere j ∼ C(w), and I is an indicator function.\nTraining We use this sampling-based strategy for all the heads during training. This clearly makes the use of backpropagation infeasible to compute the gradient, as the sampling procedure is not differentiable. Thus, we use REINFORCE (Williams, 1992) together with the three variance reduction techniques–global baseline, input-dependent baseline and variance normalization– suggested in (Mnih & Gregor, 2014).\nLet us define R(x) = log p(y|x1, . . . ,xT ) as a reward. We first center and re-scale the reward by\nR̃(x) = R(x)− b√ σ2 + ,\nwhere b and σ is running average and standard deviation of R. We can further center it for each input x separately, i.e.,\nR̃(x)← R̃(x)− b(x), where b(x) is computed by a baseline network which takes as input x and predicts its estimated reward. The baseline network is trained to minimize the Huber loss (Huber, 1964) between the true reward R̃(x)∗ and the predicted reward b(x). We use the Huber loss, which is defined by\nHδ(x) = { x2 for |x| ≤ δ, δ(2|x| − δ), otherwise,\ndue to its robustness. As a further measure to reduce the variance, we regularize the negative entropy of all those category distributions to facilitate a better exploration during training (Xu et al., 2015).\nThen, the cost function for each training example is approximated as Cn(θ) =− log p(y|x1:T , w̃1:J , ũ1:J , ẽ1:J)\n− J∑ j=1 R̃(xn)(log p(w̃j |x1:T ) + log p(ũj |x1:T ) + log p(ẽj |x1:T )) − λH J∑ j=1 (H(wj |x1:T ) +H(uj |x1:T ) +H(ej |x1:T )).\nwhere J is the number of addressing steps, λH is the entropy regularization coefficient, andH denotes the entropy.\nInference Once training is over, we switch to a deterministic strategy. We simply choose an element of w with the largest value to be the index of the target memory cell, such that\nw̃k = I(k = argmax(w)).\nCurriculum Learning for the Discrete Attention Training discrete attention with feed-forward controller and REINFORCE is challenging. We propose to use a curriculum strategy for training with the discrete attention in order to tackle this problem. For each minibatch, we sample π from a binomial distribution with the probability pt, πt ∼ Bin(pt). The model will either use the discrete or the continuous-attention based on the πt. We start the training procedure with p0 = 1 and during the training pt is annealed to 0 by setting pt = p 0\n√ 1+t .\nWe can rewrite the weights wt as in Equation 14, where it is expressed as the combination of continuous attention weights w̄t and discrete attention weights w̃t with πt being a binary variable that chooses to use one of them,\nwt ← πtw̄t + (1− πt)w̃t. (14)\nBy using this curriculum learning strategy, at the beginning of the training, the model learns to use the memory mainly with the continuous attention. As we anneal the pt, the model will rely more on the discrete attention."
    }, {
      "heading" : "5 REGULARIZING DYNAMIC NEURAL TURING MACHINES",
      "text" : "When the controller of D-NTM is a powerful recurrent neural network, it is important to regularize training of the D-NTM so as to avoid suboptimal solutions in which the D-NTM ignores the memory and works as a simple recurrent neural network.\nRead-Write Consistency Regularizer One such suboptimal solution we have observed in our preliminary experiments with the proposed D-NTM is that the D-NTM uses the address part A of the memory matrix simply as an additional weight matrix, rather than as a means to accessing the content part C. We found that this pathological case can be effectively avoided by encouraging the read head to point to a memory cell which has also been pointed by the write head. This can be implemented as the following regularization term:\nRrw(w,u) = λ T∑ t′=1 ||1− ( 1 t′ t′∑ t=1 ut) >wt′ ||22 (15)\nIn the equations above, ut is the write and wt is the read weights.\nNext Input Prediction as Regularization Temporal structure is a strong signal that should be exploited by the controller based on a recurrent neural network. We exploit this structure by letting the controller predict the input in the future. We maximize the predictability of the next input by the controller during training. This is equivalent to minimizing the following regularizer:\nRpred(W) = − log p(ft+1|ft,wt,ut,Mt;W))\nwhere ft is the current input and ft+1 is the input at next timestep. We found this regularizer to be effective in our preliminary experiments and use it for bAbI tasks."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "A recurrent neural network (RNN), which is used as a controller in the proposed D-NTM, has an implicit memory in the form of recurring hidden states. Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans (Bengio et al., 1994; Hochreiter, 1991). Long short-term memory (LSTM, (Hochreiter & Schmidhuber, 1997)) and gated recurrent units (GRU, (Cho et al., 2014)) have been found to address this issue. However all these models based solely on RNNs have been found to be limited when they are used to solve, e.g., algorithmic tasks and episodic question-answering.\nIn addition to the finite random access memory of the neural Turing machine, based on which the D-NTM is designed, other data structures have been proposed as external memory for neural networks. In (Sun et al., 1997; Grefenstette et al., 2015; Joulin & Mikolov, 2015), a continuous, differentiable stack was proposed. In (Zaremba et al., 2015; Zaremba & Sutskever, 2015), grid and tape storages are used. These approaches differ from the NTM in that their memory is unbounded and can grow indefinitely. On the other hand, they are often not randomly accessible.\nMemory networks (Weston et al., 2015b) form another family of neural networks with external memory. In this class of neural networks, information is stored explicitly as it is (in the form of its continuous representation) in the memory, without being erased or modified during an episode. Memory networks and their variants have been applied to various tasks successfully (Sukhbaatar et al., 2015; Bordes et al., 2015; Dodge et al., 2015; Xiong et al., 2016). Miller et al. (2016) have also independently proposed the idea of having separate key and value vectors for memory networks.\nAnother related family of models is the attention-based neural networks. Neural networks with continuous or discrete attention over an input have shown promising results on a variety of challenging tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), speech recognition (Chorowski et al., 2015), machine reading comprehension (Hermann et al., 2015) and image caption generation (Xu et al., 2015).\nThe latter two, the memory network and attention-based networks, are however clearly distinguishable from the D-NTM by the fact that they do not modify the content of the memory."
    }, {
      "heading" : "7 EXPERIMENTS",
      "text" : "We provide experimental results to demonstrate the abilities of our model, first on Facebook bAbI task (Weston et al., 2015a). We give detailed analysis and experimental results on this task. We also compare different variations of NTM on bAbI tasks. We have performed experiments on sequential permuted MNIST (Le et al., 2015) and on toy tasks to compare other published models on these tasks with a recurrent controller. The details of our experiments are provided in the supplementary material."
    }, {
      "heading" : "7.1 EPISODIC QUESTION-ANSWERING: BABI TASKS",
      "text" : "In this section, we evaluate the proposed D-NTM on the recently proposed episodic question-answering task called Facebook bAbI. We use the dataset with 10k training examples per sub-task provided by Facebook.1 For each episode, the D-NTM reads a sequence of factual sentences followed by a question, all of which are given as natural language sentences. The D-NTM is expected to store and retrieve relevant information in the memory in order to answer the question based on the presented facts. Exact implementation details and hyper-parameter settings are provided in the appendix."
    }, {
      "heading" : "7.1.1 GOALS",
      "text" : "The goal of this experiment is three-fold. First, we present for the first time the performance of a memory-based network that can both read and write dynamically on the Facebook bAbI tasks2. We aim to understand whether a model that has to learn to write an incoming fact to the memory, rather than storing it as it is, is able to work well, and to do so, we compare both the original NTM and proposed D-NTM against an LSTM-RNN.\nSecond, we investigate the effect of having to learn how to write. The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, (Sukhbaatar et al., 2015)) and dynamic memory network (DMN+, (Xiong et al., 2016)) both of which simply store the incoming facts as they are. We quantify this effect in this experiment. Lastly, we show the effect of the proposed learnable addressing scheme.\nWe further explore the effect of using a feedforward controller instead of the GRU controller. In addition to the explicit memory, the GRU controller can use its own internal hidden state as the memory. On the other hand, the feedforward controller must solely rely on the explicit memory, as it is the only memory available."
    }, {
      "heading" : "7.1.2 RESULTS AND ANALYSIS",
      "text" : "In Table 1, we first observe that the NTMs are indeed capable of solving this type of episodic question-answering better than the vanilla LSTM-RNN. Although the availability of explicit memory in the NTM has already suggested this result, we note that this is the first time neural Turing machines have been used in this specific task.\nAll the variants of NTM with the GRU controller outperform the vanilla LSTM-RNN. However, not all of them perform equally well. First, it is clear that the proposed dynamic NTM (D-NTM) using the GRU controller outperforms the original NTM with the GRU controller (NTM, CBA only NTM vs. continuous D-NTM, Discrete D-NTM). As discussed earlier, the learnable addressing scheme of the D-NTM allows the controller to access the memory slots by location in a potentially nonlinear way. We expect it to help with tasks that have non-trivial access patterns, and as anticipated, we see a large gain with the D-NTM over the original NTM in the tasks of, for instance, 12 - Conjunction and 17 - Positional Reasoning.\nAmong the recurrent variants of the proposed D-NTM, we notice significant improvements by using discrete addressing over using continuous addressing. We conjecture that this is due to certain types of tasks that require precise/sharp retrieval of a stored fact, in which case continuous addressing is in disadvantage over discrete addressing. This is evident from the observation that the D-NTM with discrete addressing significantly outperforms that with continuous addressing in the tasks of 8 -\n1 https://research.facebook.com/researchers/1543934539189348 2Similar experiments were done in the recently published (Graves et al., 2016), but D-NTM results for bAbI\ntasks were already available in arxiv by that time.\nLists/Sets and 11 - Basic Coreference. Furthermore, this is in line with an earlier observation in (Xu et al., 2015), where discrete addressing was found to generalize better in the task of image caption generation.\nIn Table 2, we also observe that the D-NTM with the feedforward controller and discrete attention performs worse than LSTM and D-NTM with continuous-attention. However, when the proposed curriculum strategy from Sec. 4 is used, the average test error drops from 68.30 to 37.79.\nWe empirically found training of the feedforward controller more difficult than that of the recurrent controller. We train our feedforward controller based models four times longer (in terms of the number of updates) than the recurrent controller based ones in order to ensure that they are converged for most of the tasks. On the other hand, the models trained with the GRU controller overfit on bAbI tasks very quickly. For example, on tasks 3 and 16 the feedforward controller based model underfits (i.e., high training loss) at the end of the training, whereas with the same number of units the model with the GRU controller can overfit on those tasks after 3,000 updates only.\nWhen our results are compared to the variants of the memory network Weston et al. (2015b) (MemN2N and DMN+), we notice a significant performance gap. We attribute this gap to the difficulty in learning to manipulate and store a complex input.\nWe also provide further experiments investigating different extensions on D-NTM in the appendix.\n7.2 SEQUENTIAL pMNIST\nIn sequential MNIST task, the pixels of the MNIST digits are provided to the model in scan line order, left to right and top to bottom (Le et al., 2015). At the end of sequence of pixels, the model predicts the label of the digit in the sequence of pixels. We experiment D-NTM on the variation of sequential MNIST where the order of the pixels is randomly shuffled, we call this task as permuted MNIST (pMNIST). An important contribution of this task to our paper, in particular, is to measure the model’s ability to perform well when dealing with long-term dependencies. We report our results in Table 33, we observe improvements over other models that we compare against. In Table 3, ”discrete addressing with MAB” refers to D-NTM model using REINFORCE with baseline computed from moving averages of the reward. Discrete addressing with IB refers to D-NTM using REINFORCE with input-based baseline."
    }, {
      "heading" : "7.3 NTM TOY TASKS",
      "text" : "We explore the possibility of using D-NTM to solve algorithmic tasks such as copy and associative recall tasks. We train our model on the same lengths of sequences that is experimented in (Graves et al., 2014). We report our results in Table 4. We find out that D-NTM using continuous-attention can successfully learn the ”Copy” and ”Associative Recall” tasks.\nIn Table 4, we train our model on sequences of the same length as the experiments in (Graves et al., 2014) and test the model on the sequences of the maximum length seen during the training. We consider model to be successful on copy or associative recall if its validation cost (binary cross-entropy) is lower than 0.02 over the sequences of maximum length seen during the training. We set the threshold to 0.02 to determine whether a model is successful on a task. Because empirically we observe that the models have higher validation costs perform badly in terms of generalization over the longer sequences. ”D-NTM discrete” model in this table is trained with REINFORCE using moving averages to estimate the baseline.\nTest Acc\nD-NTM discrete MAB 89.6 D-NTM discrete IB 92.3 Soft D-NTM 93.4 NTM 90.9\nI-RNN (Le et al., 2015) 82.0 Zoneout (Krueger et al., 2016) 93.1 LSTM (Krueger et al., 2016) 89.8 Unitary-RNN (Arjovsky et al., 2015) 91.4 Recurrent Dropout (Krueger et al., 2016) 92.5\nTable 3: Sequential pMNIST.\nCopy Tasks Associative Recall\nSoft D-NTM Success Success D-NTM discrete Success Failure NTM Success Success\nTable 4: NTM Toy Tasks."
    }, {
      "heading" : "8 CONCLUSION AND FUTURE WORK",
      "text" : "In this paper we extend neural Turing machines (NTM) by introducing a learnable addressing scheme which allows the NTM to be capable of performing highly nonlinear location-based addressing. This extension, to which we refer by dynamic NTM (D-NTM), is extensively tested with various configurations, including different addressing mechanisms (continuous vs. discrete) and different number of addressing steps, on the Facebook bAbI tasks. This is the first time an NTM-type model was tested on this task, and we observe that the NTM, especially the proposed D-NTM, performs better than vanilla LSTM-RNN. Furthermore, the experiments revealed that the discrete, discrete addressing works better than the continuous addressing with the GRU controller, and our analysis reveals that this is the case when the task requires precise retrieval of memory content.\nOur experiments show that the NTM-based models can be weaker than other variants of memory networks which do not learn but have an explicit mechanism of storing incoming facts as they are. We conjecture that this is due to the difficulty in learning how to write, manipulate and delete the content of memory. Despite this difficulty, we find the NTM-based approach, such as the proposed D-NTM,\n3Let us note that, the current state of art on this task is recurrent batch normalization with LSTM (Cooijmans et al., 2016) with 95.6% accuracy. It is possible to use recurrent batch normalization in our model and potentially improve our results on this task as well.\nto be a better, future-proof approach, because it can scale to a much longer horizon (where it becomes impossible to explicitly store all the experiences.)\nOn pMNIST task, we show that our model can outperform other similar type of approaches proposed to deal with the long-term dependencies. On copy and associative recall tasks, we show that our model can solve the algorithmic problems that are proposed to solve with NTM type of models.\nThe success of both the learnable address and the discrete addressing scheme suggests two future research directions. First, we should try both of these schemes in a wider array of memory-based models, as they are not specific to the neural Turing machines. Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization (Rush et al., 2015), visual questionanswering (Antol et al., 2015) and machine translation, in order to make a more concrete conclusion."
    }, {
      "heading" : "A EXPERIMENTAL DETAILS",
      "text" : "A.1 MODEL AND TRAINING DETAILS FOR BABI\nWe use the same hyperparameters for all the tasks for a given model.\nA.1.1 FACT REPRESENTATION\nWe use a recurrent neural network with GRU units to encode a variable-length fact into a fixed-size vector representation. This allows the D-NTM to exploit the word ordering in each fact, unlike when facts are encoded as bag-of-words vectors.\nA.1.2 CONTROLLER\nWe experiment with both a recurrent and feedforward neural network as the controller that generates the read and write weights. The controller has 180 units. We train our feed-forward controller using noisy-tanh activation function (Gulcehre et al., 2016) since we were experiencing training difficulties with sigmoid and tanh activation functions. We use both single-step and three-steps addressing with our GRU controller.\nA.1.3 MEMORY\nThe memory contains 120 memory cells. Each memory cell consists of a 16-dimensional address part and 28-dimensional content part.\nA.1.4 TRAINING DETAILS\nWe set aside a random 10% of the training examples as a validation set for each sub-task and use it for early-stopping and hyperparameter search. We train one D-NTM for each sub-task, using Adam (Kingma & Ba, 2014) with its learning rate set to 0.003 and 0.007 respectively for GRU and Feedforward controller. The size of each minibatch is 160, and each minibatch is constructed uniform-randomly from the training set.\nA.2 MODEL AND TRAINING DETAILS FOR SEQUENTIAL MNIST\nOn sequential MNIST task we try to keep the capacity of our model to be close to our baselines. We use 100 GRU units in the controller and each content vector of size 8 and with address vectors of size 8. We use a learning rate of 1e − 3 and trained the model with adam optimizer. We did not use the read and write consistency regularization in any of our models.\nA.3 MODEL AND TRAINING DETAILS FOR TOY TASKS\nOn both copy and associative recall tasks, we try to keep the capacity of our model to be close to our baselines. We use 100 GRU units in the controller and each content vector of has a size of 8 and using address vector of size 8. We use a learning rate of 1e− 3 and trained the model with adam optimizer. We did not use the read and write consistency regularization in any of our models. For the model with the discrete attention we use REINFORCE with baseline computed using moving averages.\nB VISUALIZATION OF DISCRETE ATTENTION\nWe visualize the attention of D-NTM with GRU controller with discrete attention in Figure 2. From this example, we can see that D-NTM has learned to find the correct supporting fact even without any supervision for the particular story in the visualization."
    }, {
      "heading" : "C LEARNING CURVES FOR THE RECURRENT CONTROLLER",
      "text" : "In Figure 3, we compare the learning curves of the continuous and discrete attention D-NTM model with recurrent controller on Task 1. Surprisingly, the discrete attention D-NTM converges faster than the continuous-attention model. The main difficulty of learning continuous-attention is due to the fact that learning to write with continuous-attention can be challenging."
    }, {
      "heading" : "D A COMPARISON BETWEEN THE LEARNING",
      "text" : "CURVES OF INPUT BASED BASELINE AND REGULAR BASELINE ON pMNIST\nIn Figure 4, we show the learning curves of input-based-baseline (ibb) and regular REINFORCE with moving averages baseline (mab) on the pMNIST task. We observe that input-based-baseline in general is much easier to optimize and converges faster as well. But it can quickly overfit to the task as well."
    }, {
      "heading" : "E TRAINING WITH CONTINUOUS-ATTENTION AND TESTING WITH DISCRETE-ATTENTION",
      "text" : "In Table 5, we provide results investigating the effects of using discrete attention model at the test-time for a model trained with feed-forward controller and continuous attention. Discrete∗ D-NTM model bootstraps the discrete attention with the continuous attention, using the curriculum method that we have\nintroduced in Section ”Curriculum Learning for the Discrete Attention”. Discrete† D-NTM model is the continuous-attention model which uses discrete-attention at the test time. We observe that the Discrete† D-NTM model which is trained with continuous-attention outperforms Discrete D-NTM model."
    }, {
      "heading" : "F D-NTM WITH BOW FACT REPRESENTATION",
      "text" : "In Table 6, we provide results for D-NTM using BoW with positional encoding (PE) Sukhbaatar et al. (2015) as the representation of the input facts. The facts representations are provided as an input to the GRU controller. In agreement to our results with the GRU fact representation, with the BoW fact representation we observe improvements with multi-step of addressing over single-step and discrete addressing over continuous addressing."
    } ],
    "references" : [ {
      "title" : "VQA: visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In 2015 IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Martin Arjovsky", "Amar Shah", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06464,",
      "citeRegEx" : "Arjovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proceedings Of The International Conference on Representation Learning (ICLR 2015),",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1506.02075,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1506.07503,",
      "citeRegEx" : "Chorowski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent batch normalization",
      "author" : [ "Tim Cooijmans", "Nicolas Ballas", "César Laurent", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1603.09025,",
      "citeRegEx" : "Cooijmans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cooijmans et al\\.",
      "year" : 2016
    }, {
      "title" : "Evaluating prerequisite qualities for learning end-to-end dialog systems",
      "author" : [ "Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston" ],
      "venue" : "CoRR, abs/1511.06931,",
      "citeRegEx" : "Dodge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning. Book in preparation for MIT Press, 2016",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : "URL http://www.deeplearningbook.org",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Hybrid computing using a neural network with dynamic external memory",
      "author" : [ "Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwińska", "Sergio Gómez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou" ],
      "venue" : null,
      "citeRegEx" : "Graves et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to transduce with unbounded memory",
      "author" : [ "Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2015
    }, {
      "title" : "Noisy activation functions",
      "author" : [ "Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1603.00391,",
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "arXiv preprint arXiv:1506.03340,",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1511.02301,",
      "citeRegEx" : "Hill et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen netzen",
      "author" : [ "Sepp Hochreiter" ],
      "venue" : "Diploma, Technische Universität München, pp",
      "citeRegEx" : "Hochreiter.,? \\Q1991\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Robust estimation of a location parameter",
      "author" : [ "Peter J. Huber" ],
      "venue" : "Ann. Math. Statist., 35(1):73–101,",
      "citeRegEx" : "Huber.,? \\Q1964\\E",
      "shortCiteRegEx" : "Huber.",
      "year" : 1964
    }, {
      "title" : "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "author" : [ "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Joulin and Mikolov.,? \\Q2015\\E",
      "shortCiteRegEx" : "Joulin and Mikolov.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Zoneout: Regularizing rnns by randomly preserving hidden activations",
      "author" : [ "David Krueger", "Tegan Maharaj", "János Kramár", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.01305,",
      "citeRegEx" : "Krueger et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning" ],
      "venue" : "In Proceedings Of The Conference on Empirical Methods for Natural Language Processing",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Key-value memory networks for directly reading",
      "author" : [ "Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston" ],
      "venue" : "documents. CoRR,",
      "citeRegEx" : "Miller et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural variational inference and learning in belief networks",
      "author" : [ "Andriy Mnih", "Karol Gregor" ],
      "venue" : "arXiv preprint arXiv:1402.0030,",
      "citeRegEx" : "Mnih and Gregor.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih and Gregor.",
      "year" : 2014
    }, {
      "title" : "Scaling memory-augmented neural networks with sparse reads and writes",
      "author" : [ "Jack W Rae", "Jonathan J Hunt", "Tim Harley", "Ivo Danihelka", "Andrew Senior", "Greg Wayne", "Alex Graves", "Timothy P Lillicrap" ],
      "venue" : "Advances in NIPS",
      "citeRegEx" : "Rae et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rae et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rush et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "One-shot learning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap" ],
      "venue" : "arXiv preprint arXiv:1605.06065,",
      "citeRegEx" : "Santoro et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau" ],
      "venue" : "In Proceedings of the 30th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1503.08895,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "The neural network pushdown automaton: Architecture, dynamics and training",
      "author" : [ "Guo-Zheng Sun", "C. Lee Giles", "Hsing-Hen Chen" ],
      "venue" : "In Adaptive Processing of Sequences and Data Structures, International Summer School on Neural Networks,",
      "citeRegEx" : "Sun et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 1997
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "arXiv preprint arXiv:1506.05869,",
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Towards ai-complete question answering: a set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov" ],
      "venue" : "arXiv preprint arXiv:1502.05698,",
      "citeRegEx" : "Weston et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Xiong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "In Proceedings Of The International Conference on Representation Learning (ICLR 2015),",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Describing videos by exploiting temporal structure",
      "author" : [ "Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "Yao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning neural turing machines",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever" ],
      "venue" : "CoRR, abs/1505.00521,",
      "citeRegEx" : "Zaremba and Sutskever.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zaremba and Sutskever.",
      "year" : 2015
    }, {
      "title" : "Learning simple algorithms from examples",
      "author" : [ "Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1511.07275,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    }, {
      "title" : "2014) with its learning rate set to 0.003 and 0.007 respectively for GRU",
      "author" : [ "Adam (Kingma", "Ba" ],
      "venue" : null,
      "citeRegEx" : ".Kingma and Ba,? \\Q2014\\E",
      "shortCiteRegEx" : ".Kingma and Ba",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : ", (Goodfellow et al., 2016)) there are still a set of complex tasks that are not well addressed by conventional neural networks.",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "They include, but are not limited to, episodic question-answering (Weston et al., 2015b; Hermann et al., 2015; Hill et al., 2015), compact algorithms (Zaremba et al.",
      "startOffset" : 66,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "They include, but are not limited to, episodic question-answering (Weston et al., 2015b; Hermann et al., 2015; Hill et al., 2015), compact algorithms (Zaremba et al.",
      "startOffset" : 66,
      "endOffset" : 129
    }, {
      "referenceID" : 39,
      "context" : ", 2015), compact algorithms (Zaremba et al., 2015), dialogue (Serban et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : ", 2015), dialogue (Serban et al., 2016; Vinyals & Le, 2015) and video caption generation (Yao et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 37,
      "context" : ", 2016; Vinyals & Le, 2015) and video caption generation (Yao et al., 2015).",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, neural Turing machines (NTM, (Graves et al., 2014)) read each fact in an episode and decides whether to read, write the fact or do both to the external, differentiable memory.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks (Bordes et al., 2015; Dodge et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks (Bordes et al., 2015; Dodge et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "After we published our paper on arXiv, a new extension of NTM called DNC (Graves et al., 2016) has also provided results on bAbI task as well.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "We also provide results on sequential-MNIST and algorithmic tasks proposed by (Graves et al., 2014) in order to investigate the ability of our model when dealing with long-term dependencies.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "The proposed dynamic neural Turing machine (D-NTM) extends the neural Turing machine (NTM, (Graves et al., 2014)) which has a modular design.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : ") In this paper, we use both a gated recurrent unit (GRU, (Cho et al., 2014)) and a feedforward-controller to implement the controller such that for a GRU controller h = GRU(xt,ht−1, φ) (1) or for a feedforward-controller h = σ(x, φ).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "The way they are computed are very similar to the content based addressing in (Graves et al., 2014).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "In this paper, we explore an option of allowing each head to operate more than once at each time step, similar to the multi-hop mechanism from the end-to-end memory network (Sukhbaatar et al., 2015).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 28,
      "context" : "As observed in (Santoro et al., 2016; Rae et al., 2016), we find it easier to learn the write operations with the use of LRU addressing.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "As observed in (Santoro et al., 2016; Rae et al., 2016), we find it easier to learn the write operations with the use of LRU addressing.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 34,
      "context" : "Thus, we use REINFORCE (Williams, 1992) together with the three variance reduction techniques–global baseline, input-dependent baseline and variance normalization– suggested in (Mnih & Gregor, 2014).",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "The baseline network is trained to minimize the Huber loss (Huber, 1964) between the true reward R̃(x)∗ and the predicted reward b(x).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 36,
      "context" : "As a further measure to reduce the variance, we regularize the negative entropy of all those category distributions to facilitate a better exploration during training (Xu et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans (Bengio et al., 1994; Hochreiter, 1991).",
      "startOffset" : 127,
      "endOffset" : 166
    }, {
      "referenceID" : 16,
      "context" : "Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans (Bengio et al., 1994; Hochreiter, 1991).",
      "startOffset" : 127,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "Long short-term memory (LSTM, (Hochreiter & Schmidhuber, 1997)) and gated recurrent units (GRU, (Cho et al., 2014)) have been found to address this issue.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 31,
      "context" : "In (Sun et al., 1997; Grefenstette et al., 2015; Joulin & Mikolov, 2015), a continuous, differentiable stack was proposed.",
      "startOffset" : 3,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "In (Sun et al., 1997; Grefenstette et al., 2015; Joulin & Mikolov, 2015), a continuous, differentiable stack was proposed.",
      "startOffset" : 3,
      "endOffset" : 72
    }, {
      "referenceID" : 39,
      "context" : "In (Zaremba et al., 2015; Zaremba & Sutskever, 2015), grid and tape storages are used.",
      "startOffset" : 3,
      "endOffset" : 52
    }, {
      "referenceID" : 30,
      "context" : "Memory networks and their variants have been applied to various tasks successfully (Sukhbaatar et al., 2015; Bordes et al., 2015; Dodge et al., 2015; Xiong et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "Memory networks and their variants have been applied to various tasks successfully (Sukhbaatar et al., 2015; Bordes et al., 2015; Dodge et al., 2015; Xiong et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "Memory networks and their variants have been applied to various tasks successfully (Sukhbaatar et al., 2015; Bordes et al., 2015; Dodge et al., 2015; Xiong et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 169
    }, {
      "referenceID" : 35,
      "context" : "Memory networks and their variants have been applied to various tasks successfully (Sukhbaatar et al., 2015; Bordes et al., 2015; Dodge et al., 2015; Xiong et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "Neural networks with continuous or discrete attention over an input have shown promising results on a variety of challenging tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), speech recognition (Chorowski et al.",
      "startOffset" : 162,
      "endOffset" : 205
    }, {
      "referenceID" : 23,
      "context" : "Neural networks with continuous or discrete attention over an input have shown promising results on a variety of challenging tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), speech recognition (Chorowski et al.",
      "startOffset" : 162,
      "endOffset" : 205
    }, {
      "referenceID" : 6,
      "context" : ", 2015), speech recognition (Chorowski et al., 2015), machine reading comprehension (Hermann et al.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : ", 2015), machine reading comprehension (Hermann et al., 2015) and image caption generation (Xu et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 36,
      "context" : ", 2015) and image caption generation (Xu et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans (Bengio et al., 1994; Hochreiter, 1991). Long short-term memory (LSTM, (Hochreiter & Schmidhuber, 1997)) and gated recurrent units (GRU, (Cho et al., 2014)) have been found to address this issue. However all these models based solely on RNNs have been found to be limited when they are used to solve, e.g., algorithmic tasks and episodic question-answering. In addition to the finite random access memory of the neural Turing machine, based on which the D-NTM is designed, other data structures have been proposed as external memory for neural networks. In (Sun et al., 1997; Grefenstette et al., 2015; Joulin & Mikolov, 2015), a continuous, differentiable stack was proposed. In (Zaremba et al., 2015; Zaremba & Sutskever, 2015), grid and tape storages are used. These approaches differ from the NTM in that their memory is unbounded and can grow indefinitely. On the other hand, they are often not randomly accessible. Memory networks (Weston et al., 2015b) form another family of neural networks with external memory. In this class of neural networks, information is stored explicitly as it is (in the form of its continuous representation) in the memory, without being erased or modified during an episode. Memory networks and their variants have been applied to various tasks successfully (Sukhbaatar et al., 2015; Bordes et al., 2015; Dodge et al., 2015; Xiong et al., 2016). Miller et al. (2016) have also independently proposed the idea of having separate key and value vectors for memory networks.",
      "startOffset" : 128,
      "endOffset" : 1530
    }, {
      "referenceID" : 22,
      "context" : "We have performed experiments on sequential permuted MNIST (Le et al., 2015) and on toy tasks to compare other published models on these tasks with a recurrent controller.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, (Sukhbaatar et al., 2015)) and dynamic memory network (DMN+, (Xiong et al.",
      "startOffset" : 168,
      "endOffset" : 193
    }, {
      "referenceID" : 35,
      "context" : ", 2015)) and dynamic memory network (DMN+, (Xiong et al., 2016)) both of which simply store the incoming facts as they are.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "com/researchers/1543934539189348 Similar experiments were done in the recently published (Graves et al., 2016), but D-NTM results for bAbI tasks were already available in arxiv by that time.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 36,
      "context" : "Furthermore, this is in line with an earlier observation in (Xu et al., 2015), where discrete addressing was found to generalize better in the task of image caption generation.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 33,
      "context" : "When our results are compared to the variants of the memory network Weston et al. (2015b) (MemN2N and DMN+), we notice a significant performance gap.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "In sequential MNIST task, the pixels of the MNIST digits are provided to the model in scan line order, left to right and top to bottom (Le et al., 2015).",
      "startOffset" : 135,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "We train our model on the same lengths of sequences that is experimented in (Graves et al., 2014).",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "In Table 4, we train our model on sequences of the same length as the experiments in (Graves et al., 2014) and test the model on the sequences of the maximum length seen during the training.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "9 I-RNN (Le et al., 2015) 82.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "0 Zoneout (Krueger et al., 2016) 93.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "1 LSTM (Krueger et al., 2016) 89.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "8 Unitary-RNN (Arjovsky et al., 2015) 91.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "4 Recurrent Dropout (Krueger et al., 2016) 92.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "Let us note that, the current state of art on this task is recurrent batch normalization with LSTM (Cooijmans et al., 2016) with 95.",
      "startOffset" : 99,
      "endOffset" : 123
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}