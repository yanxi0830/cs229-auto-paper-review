{
  "name" : "476.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Cybenko (1989) proved that a network with a large enough single hidden layer of sigmoid units can approximate any decision boundary. Empirical work, however, suggests that it can be difficult to train shallow nets to be as accurate as deep nets. Dauphin and Bengio (2013) trained shallow nets on SIFT features to classify a large-scale ImageNet dataset and found that it was difficult to train large, high-accuracy, shallow nets. A study of deep convolutional nets suggests that for vision tasks deeper models are preferred under a parameter budget (e.g. Eigen et al. (2014); He et al. (2015); Simonyan and Zisserman (2014); Srivastava et al. (2015)). Similarly, Seide et al. (2011) and Geras et al. (2015) show that deeper models are more accurate than shallow models in speech acoustic modeling. More recently, Romero et al. (2015) showed that it is possible to gain increases in accuracy in models with few parameters by training deeper, thinner nets (FitNets) to mimic much wider nets. Cohen and Shashua (2016); Liang and Srikant (2016) suggest that the representational efficiency of deep networks scales exponentially with depth, but it is unclear if this applies only to pathological problems, or is encountered in practice on data sets such as TIMIT and CIFAR.\nBa and Caruana (2014), however, demonstrated that shallow nets sometimes can learn the functions learned by deep nets, even when restricted to the same number of parameters as the deep nets. They did this by first training state-of-the-art deep models, and then training shallow models to mimic the deep models. Surprisingly, and for reasons that are not well understood, the shallow models learned more accurate functions when trained to mimic the deep models than when trained on the original data used to train the deep models. In some cases shallow models trained this way were as accurate as state-of-the-art deep models. But this demonstration was made on the TIMIT speech recognition benchmark. Although their deep teacher models used a convolutional layer, convolution is less important for TIMIT than it is for other domains such as image classification.\nBa and Caruana (2014) also presented results on CIFAR-10 which showed that a shallow model could learn functions almost as accurate as deep convolutional nets. Unfortunately, the results on CIFAR-10 are less convincing than those for TIMIT. To train accurate shallow models on CIFAR-10\nthey had to include at least one convolutional layer in the shallow model, and increased the number of parameters in the shallow model until it was 30 times larger than the deep teacher model. Despite this, the shallow convolutional student model was several points less accurate than a teacher model that was itself several points less accurate than state-of-the-art models on CIFAR-10.\nIn this paper we show that the methods Ba and Caruana used to train shallow students to mimic deep teacher models on TIMIT do not work as well on problems such as CIFAR-10 where multiple layers of convolution are required to train accurate teacher models. If the student models have a similar number of parameters as the deep teacher models, high accuracy can not be achieved without multiple layers of convolution even when the student models are trained via distillation.\nTo ensure that the shallow student models are trained as accurately as possible, we use Bayesian optimization to thoroughly explore the space of architectures and learning hyperparameters. Although this combination of distillation and hyperparameter optimization allows us to train the most accurate shallow models ever trained on CIFAR-10, the shallow models still are not as accurate as deep models. Our results clearly suggest that deep convolutional nets do, in fact, need to be both deep and convolutional, even when trained to mimic very accurate models via distillation (Hinton et al., 2015)."
    }, {
      "heading" : "2 TRAINING SHALLOW NETS TO MIMIC DEEPER CONVOLUTIONAL NETS",
      "text" : "In this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work, here we compare shallow models to state-of-the-art deep convolutional models, and restrict the number of parameters in the shallow student models to be comparable to the number of parameters in the deep convolutional teacher models. Because we anticipated that our results might be different, we follow their approach closely to eliminate the possibility that the results differ merely because of changes in methodology. Note that the goal of this paper is not to train models that are small or fast as in Bucila et al. (2006), Hinton et al. (2015), and Romero et al. (2015), but to examine if shallow models can be as accurate as deep convolutional models given the same parameter budget.\nThere are many steps required to train shallow student models to be as accurate as possible: train state-of-the-art deep convolutional teacher models, form an ensemble of the best deep models, collect and combine their predictions on a large transfer set, and then train carefully optimized shallow student models to mimic the teacher ensemble. For negative results to be informative, it is important that each of these steps be performed as well as possible. In this section we describe the experimental methodology in detail. Readers familiar with distillation (model compression), training deep models on CIFAR-10, data augmentation, and Bayesian hyperparameter optimization may wish to skip to the empirical results in Section 3."
    }, {
      "heading" : "2.1 MODEL COMPRESSION AND DISTILLATION",
      "text" : "The key idea behind model compression is to train a compact model to approximate the function learned by another larger, more complex model. Bucila et al. (2006) showed how a single neural net of modest size could be trained to mimic a much larger ensemble. Although the small neural nets contained 1000× fewer parameters, often they were as accurate as the large ensembles they were trained to mimic.\nModel compression works by passing unlabeled data through the large, accurate teacher model to collect the real-valued scores it predicts, and then training a student model to mimic these scores. Hinton et al. (2015) generalized the methods of Bucila et al. (2006) and Ba and Caruana (2014) by incorporating a parameter to control the relative importance of the soft targets provided by the teacher model to the hard targets in the original training data, as well as a temperature parameter that regularizes learning by pushing targets towards the uniform distribution. Hinton et al. (2015) also demonstrated that much of the knowledge passed from the teacher to the student is conveyed as dark knowledge contained in the relative scores (probabilities) of outputs corresponding to other classes, as opposed to the scores given to just the output for the one correct class.\nSurprisingly, distillation often allows smaller and/or shallower models to be trained that are nearly as accurate as the larger, deeper models they are trained to mimic, yet these same small models are not as accurate when trained on the 1-hot hard targets in the original training set. The reason for this is not yet well understood. Similar compression and distillation methods have also successfully\nbeen used in speech recognition (e.g. Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and reinforcement learning Parisotto et al. (2016); Rusu et al. (2016). Romero et al. (2015) showed that distillation methods can be used to train small students that are more accurate than the teacher models by making the student models deeper, but thinner, than the teacher model."
    }, {
      "heading" : "2.2 MIMIC LEARNING VIA L2 REGRESSION ON LOGITS",
      "text" : "We train shallow mimic nets using data labeled by an ensemble of deep teacher nets trained on the original 1-hot CIFAR-10 training data. The deep teacher models are trained in the usual way using softmax outputs and cross-entropy cost function. Following Ba and Caruana (2014), the student mimic models are not trained with cross-entropy on the ten p values where pk = ezk/ ∑ j e\nzj output by the softmax layer from the deep teacher model, but instead are trained on the un-normalized log probability values z (the logits) before the softmax activation. Training on the logarithms of predicted probabilities (logits) helps provide the dark knowledge that regularizes students by placing emphasis on the relationships learned by the teacher model across all of the outputs.\nAs in Ba and Caruana (2014), the student is trained as a regression problem given training data {(x(1), z(1)),...,(x(T ), z(T ))}:\nL(W ) = 1 T ∑ t ||g(x(t);W )− z(t)||22, (1)\nwhereW represents all of the weights in the network, and g(x(t);W ) is the model prediction on the tth training data sample."
    }, {
      "heading" : "2.3 USING A LINEAR BOTTLENECK TO SPEED UP TRAINING",
      "text" : "A shallow net has to have more hidden units in each layer to match the number of parameters in a deep net. Ba and Caruana (2014) found that training these wide, shallow mimic models with backpropagation was slow, and introduced a linear bottleneck layer between the input and non-linear layers to speed learning. The bottleneck layer speeds learning by reducing the number of parameters that must be learned, but does not make the model deeper because the linear terms can be absorbed back into the non-linear weight matrix after learning. See Ba and Caruana (2014) for details. To match their experiments we use linear bottlenecks when training student models with 0 or 1 convolutional layers, but did not find the linear bottlenecks necessary when training student models with more than 1 convolutional layer."
    }, {
      "heading" : "2.4 BAYESIAN HYPERPARAMETER OPTIMIZATION",
      "text" : "The goal of this work is to determine empirically if shallow nets can be trained to be as accurate as deep convolutional models using a similar number of parameters in the deep and shallow models. If we succeed in training a shallow model to be as accurate as a deep convolutional model, this provides an existence proof that shallow models can represent and learn the complex functions learned by deep convolutional models. If, however, we are unable to train shallow models to be as accurate as deep convolutional nets, we might fail only because we did not train the shallow nets well enough.\nIn all our experiments we employ Bayesian hyperparameter optimization using Gaussian process regression to ensure that we thoroughly and objectively explore the hyperparameters that govern learning. The implementation we use is Spearmint (Snoek et al., 2012). The hyperparameters we optimize with Bayesian optimization include the initial learning rate, momentum, scaling of the initial random weights, scaling of the inputs, and terms that determine the width of each of the network’s layers (i.e. number of convolutional filters and neurons). More details of the hyperparameter optimization can be found in Sections 2.5, 2.7, 2.8 and in the Appendix."
    }, {
      "heading" : "2.5 TRAINING DATA AND DATA AUGMENTATION",
      "text" : "The CIFAR-10 (Krizhevsky, 2009) data set consists of a set of natural images from 10 different object classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The dataset is a labeled subset of the 80 million tiny images dataset (Torralba et al., 2008) and is divided into 50,000 train and\n10,000 test images. Each image is 32×32 pixels in 3 color channels, yielding input vectors with 3072 dimensions. We prepared the data by subtracting the mean and dividing by the standard deviation of each image vector. We train all models on a subset of 40,000 images and use the remaining 10,000 images as the validation set for the Bayesian optimization. The final trained models only used 80% of the theoretically available training data (as opposed to retraining on all of the data after hyperparameter optimization).\nWe employ the HSV-data augmentation technique as described by Snoek et al. (2015). Thus we shift hue, saturation and value by uniform random values: ∆h ∼ U(−Dh, Dh), ∆s ∼ U(−Ds, Ds), ∆v ∼ U(−Dv, Dv). Saturation and value values are scaled globally: as ∼ U( 11+As , 1 + As), av ∼ U( 1 1+Av\n, 1 + Av). The five constants Dh, Ds, Dv, As, Av are treated as additional hyperparameters in the Bayesian hyperparameter optimization.\nAll training images are mirrored left-right randomly with a probability of 0.5. The input images are further scaled and jittered randomly by cropping windows of size 24×24 up to 32×32 at random locations and then scaling them back to 32×32. The procedure is as follows: we sample an integer value S ∼ U(24, 32) and then a pair of integers x, y ∼ U(0, 32 − S). The transformed resulting image is R = fspline,3(I[x : x + S, y : y + S]) with I denoting the original image and fspline,3 denoting the 3rd order spline interpolation function that maps the 2D array back to 32×32 (applied to the three color channels separately).\nAll data augmentations for the teacher models are computed on the fly using different random seeds. For student models trained to mimic the ensemble (see Section 2.7 for details of the ensemble teacher model), we pre-generated 160 epochs worth of randomly augmented training data, evaluated the ensemble’s predictions (logits) on these samples, and saved all data and predictions to disk. All student models thus see the same training data in the same order. The parameters for HSV-augmentation in this case had to be selected beforehand; we chose to use the settings found with the best single model (Dh = 0.06, Ds = 0.26, Dv = 0.20, As = 0.21, Av = 0.13). Pre-saving the logits and augmented data is important to reduce the computational cost at training time, and to ensure that all student models see the same training data\nBecause augmentation allows us to generate large training sets from the original 50,000 images, we use augmented data as the transfer set for model compression. No extra unlabeled data is required."
    }, {
      "heading" : "2.6 LEARNING-RATE SCHEDULE",
      "text" : "We train all models using SGD with Nesterov momentum. The initial learning rate and momentum are chosen by Bayesian optimization. The learning rate is reduced according to the evolution of the model’s validation error: it is halved if the validation error does not drop for ten epochs in a row. It is not reduced within the next eight epochs following a reduction step. Training ends if the error did not drop for 30 epochs in a row or if the learning rate was reduced by a factor of more than 2000 in total.\nThis schedule provides a way to train the highly varying models in a fair manner (it is not feasible to optimize all of the parameters that define the learning schedule). It also decreases the time spent to train each model compared to using a hand-selected overestimate of the number of epochs to train, thus allowing us to train more models in the hyperparameter search."
    }, {
      "heading" : "2.7 SUPER TEACHER: AN ENSEMBLE OF 16 DEEP CONVOLUTIONAL CIFAR-10 MODELS",
      "text" : "One limitation of the CIFAR-10 experiments performed in Ba and Caruana (2014) is that the teacher models were not state-of-the-art. The best deep models they trained on CIFAR-10 had only 88% accuracy, and the ensemble of deep models they used as a teacher had only 89% accuracy. The accuracies were not state-of-the-art because they did not use augmentation and because their deepest models had only three convolutional layers. Because our goal is to determine if shallow models can be as accurate as deep convolutional models, it is important that the deep models we compare to (and use as teachers) are as accurate as possible.\nWe train deep neural networks with eight convolutional layers, three intermittent max-pooling layers and two fully-connected hidden layers. We include the size of these layers in the hyperparameter optimization, by allowing the first two convolutional layers to contain from 32 to 96 filters each, the next two layers to contain from 64 to 192 filters, and the last four convolutional layers to contain\nfrom 128 to 384 filters. The two fully-connected hidden layers can contain from 512 to 1536 neurons. We parametrize these model-sizes by four scalars (the layers are grouped as 2-2-4) and include the scalars in the hyperparameter optimization. All models are trained using Theano (Bastien et al., 2012; Bergstra et al., 2010).\nWe optimize eighteen hyperparameters overall: initial learning rate on [0.01, 0.05], momentum on [0.80, 0.91], l2 weight decay on [5 · 10−5,4 · 10−4], initialization coefficient on [0.8, 1.35] which scales the initial weights of the CNN, four separate dropout rates, five constants controlling the HSV data augmentation, and the four scaling constants controlling the networks’ layer widths. The learning rate and momentum are optimized on a log-scale (as opposed to linear scale) by optimizing the exponent with appropriate bounds, e.g. LR = e−x optimized over x on [3.0, 4.6]. See the Appendix for more details about hyperparameter optimization.\nWe trained 129 deep CNN models with Spearmint. The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%. See Table 1 for the sizes and architectures of the three best models.\nWe are able to construct a more accurate model on CIFAR-10 by forming an ensemble of multiple deep convolutional neural nets, each trained with different hyperparameters, and each seeing slightly different training data (as the augmentation parameters vary). We experimented with a number of ensembles of the many deep convnets we trained, using accuracy on the validation set to select the best combination. The final ensemble contained 16 deep convnets and had an accuracy of 94.0% on the validation set, and 93.8% on the final test set. We believe this is among the top published results for deep learning on CIFAR-10. The ensemble averages the logits predicted by each model before the softmax layers.\nWe used this very accurate ensemble model as the teacher model to label the data used to train the shallower student nets. As described in Section 2.2, the logits (the scores just prior to the final softmax layer) from each of the CNN teachers in the ensemble model are averaged for each class, and the average logits are used as final regression targets to train the shallower student neural nets."
    }, {
      "heading" : "2.8 TRAINING SHALLOW STUDENT MODELS TO MIMIC AN ENSEMBLE OF DEEP CONVOLUTIONAL MODELS",
      "text" : "We trained student mimic nets with 1, 3.161, 10 and 31.6 million trainable parameters on the pre-computed augmented training data (Section 2.5) that was re-labeled by the teacher ensemble (Section 2.7). For each of the four student sizes we trained shallow fully-connected student MLPs containing 1, 2, 3, 4, or 5 layers of non-linear units (ReLU), and student CNNs with 1, 2, 3 or 4 convolutional layers. The convolutional student models also contain one fully-connected ReLU layer. Models with zero or only one convolutional layer contain an additional linear bottleneck layer to speed up learning (cf. Section 2.3). We did not need to use a bottleneck to speed up learning for the deeper models as the number of learnable parameters is naturally reduced by the max-pooling layers.\nThe student CNNs use max-pooling and Bayesian optimization controls the number of convolutional filters and hidden units in each layer. The hyperparameters we optimized in the student models are: initial learning rate, momentum, scaling of the initially randomly distributed learnable parameters, scaling of all pixel values of the input, and the scale factors that control the width of all hidden and convolutional layers in the model. Weights are initialized as in Glorot and Bengio (2010). We intentionally do not optimize and do not make use of weight decay and dropout when training student models because preliminary experiments showed that these consistently reduced the accuracy of student models by several percent. Please refer to the Appendix for more details on the individual architectures and hyperparameter ranges."
    }, {
      "heading" : "3 EMPIRICAL RESULTS",
      "text" : "Table 1 summarizes results after Bayesian hyperparameter optimization for models trained on the original 0/1 hard CIFAR-10 labels. All of these models use weight decay and are trained with the dropout hyperparameters included in the Bayesian optimization. The table shows the accuracy of the best three deep convolutional models we could train on CIFAR-10, as well as the accuracy of\n13.16 ≈ Sqrt(10) falls halfway between 1 and 10 on log scale.\nthe ensemble of 16 deep CNNs. For comparison, the accuracy of the ensemble trained by Ba and Caruana (2014)) is included at the bottom of the table.\nTable 2 summarizes the results after Bayesian hyperparameter optimization for student models of different depths and number of parameters trained on soft targets (average logits) to mimic the teacher ensemble of 16 deep CNNs. For comparison, the student model trained by Ba and Caruana (2014) also is shown.\nThe first four rows in Table 1 show the accuracy of convolutional models with 10 million parameters and 1, 2, 3, and 4 convolutional layers. The accuracies of these same architectures with 1M, 3.16M, 10M, and 31.6M parameters when trained as students on the soft targets predicted by the teacher ensemble are shown in Table 2. Comparing the accuracies of the models with 10 million parameters in both tables, we see that training student models to mimic the ensemble leads to significantly better accuracy in every case. The gains are more pronounced for shallower models, most likely because their learnable internal representations do not naturally lead to good generalization in this task when trained on the 0/1 hard targets: the difference in accuracy for models with one convolutional layer is 2.7% (87.3% vs. 84.6%) and only 0.8% (92.6% vs. 91.8%) for models with four convolutional layers.\nFigure 1 summarizes the results in Table 2 for student models of different depth, number of convolutional layers, and number of parameters when trained to mimic the ensemble teacher model. Student models trained on the ensemble logits are able to achieve accuracies previously unseen on CIFAR-10 for models with so few layers. Also, it is clear that there is a huge gap between the convolutional student models at the top of the figure, and the non-convolutional student models at the bottom of the figure: the most accurate student MLP has accuracy less than 75%, while the least accurate convolutional student model with the same number of parameters but only one convolutional layer has accuracy above 87%. And the accuracy of the convolutional student models increases further as more layers of convolution are added. Interestingly, the most accurate student MLPs with no convolutional layers have only 2 or 3 hidden layers; the student MLPs with 4 or 5 hidden layers are not as accurate.\nComparing the student MLP with only one hidden layer (bottom of the graph) to the student CNN with 1 convolutional layer clearly suggests that convolution is critical for this problem even when models are trained via distillation, and that it is very unlikely that a shallow non-convolutional model with 100 million parameters or less could ever achieve accuracy comparable to a convolutional model. It appears that if convolution is critical for teacher models trained on the original 0/1 hard targets, it\nis likely to be critical for student models trained to mimic these teacher models. Adding depth to the student MLPs without adding convolution does not significantly close this “convolutional gap”.\nFurthermore, comparing student CNNs with 1, 2, 3, and 4 convolutional layers, it is clear that CNN students benefit from multiple convolutional layers. Although the students do not need as many layers as teacher models trained on the original 0/1 hard targets, accuracy increases significantly as multiple convolutional layers are added to the model. For example, the best student with only one convolutional layer has 87.7% accuracy, while the student with the same number of parameters (31M) and 4 convolutional layers has 92.6% accuracy.\nFigure 1 includes short horizontal lines at 10M parameters indicating the accuracy of non-student models trained on the original 0/1 hard targets instead of on the soft targets. This “compression gap” is largest for shallower models, and as expected disappears as the student models become architecturally more similar to the teacher models with multiple layers of convolution. The benefits of distillation are most significant for shallow models, yielding an increase in accuracy of 3% or more.\nOne pattern that is clear in the graph is that all student models benefit when the number of parameters increases from 1 million to 31 million parameters. It is interesting to note, however, that the largest student (31M) with a one convolutional layer is less accurate than the smallest student (1M) with two convolutional layers, further demonstrating the value of depth in convolutional models.\nIn summary, depth-constrained student models trained to mimic a high-accuracy ensemble of deep convolutional models perform better than similar models trained on the original hard targets (the “compression” gaps in Figure 1), student models need at least 3-4 convolutional layers to have high accuracy on CIFAR-10, shallow students with no convolutional layers perform poorly on CIFAR-10, and student models need at least 3-10M parameters to perform well. We are not able to compress deep convolutional models to shallow student models without significant loss of accuracy.\nWe are currently running a reduced set of experiments on ImageNet, though the chances of shallow models performing well on a more challenging problem such as ImageNet appear to be slim."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "Although we are not able to train shallow models to be as accurate as deep models, the models trained via distillation are the most accurate models of their architecture ever trained on CIFAR-10. For example, the best single-layer fully-connected MLP (no convolution) we trained achieved an accuracy of 70.2%. We believe this to be the most accurate shallow MLP ever reported for CIFAR-10 (in comparison to 63.1% achieved by Le et al. (2013), 63.9% by Memisevic et al. (2015) and 64.3% by Geras and Sutton (2015)). Although this model cannot compete with convolutional models, clearly distillation helps when training models that are limited by architecture and/or number of parameters. Similarly, the student models we trained with 1, 2, 3, and 4 convolutional layers are, we believe, the most accurate convnets of those depths reported in the literature. For example, the ensemble teacher model in Ba and Caruana (2014) was an ensemble of four CNNs, each of which had 3 convolutional layers, but only achieved 89% accuracy, whereas the single student CNNs we train via distillation achieve accuracies above 90% with only 2 convolutional layers, and above 92% with 3 convolutional layers. The only other work we are aware of that achieves comparable high accuracy with non-convolutional MLPs is recent work by Lin et al. (2016). They train multi-layer Z-Lin networks, and use a powerful form of data augmentation based on deformations that we did not use.\nInterestingly, we noticed that mimic networks perform consistently worse when trained using dropout. This surprised us, and suggests that training student models on the soft-targets from a teacher provides significant regularization for the student models obviating the need for extra regularization methods such as dropout. This is consistent with the observation made by Ba and Caruana (2014) that student mimic models did not seem to overfit. Hinton et al. (2015) claim that soft targets convey more information per sample than Boolean hard targets. The also suggest that the dark knowledge in the soft targets for other classes further helped regularization, and that early stopping was unnecessary. Romero et al. (2015) extend distillation by using the intermediate representations learned by the teacher as hints to guide training deep students, and teacher confidences further help regularization by providing a measure of sample simplicity to the student, akin to curriculum learning. In other work, Pereyra et al. (2017) suggest that the soft targets provided by a teacher provide a form of confidence penalty that penalizes low entropy distributions and label smoothing, both of which improve regularization by maintaining a reasonable ratio between the logits of incorrect classes.\nZhang et al. (2016) question the traditional view of regularization in deep models. Although they do not discuss distillation, they suggest that in deep learning traditional function approximation appears to be deeply intertwined with massive memorization. The multiple soft targets used to train student models have a high information density (Hinton et al., 2015) and thus provide regularization by reducing the impact of brute-force memorization."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We train shallow nets with and without convolution to mimic state-of-the-art deep convolutional nets. If one controls for the number of learnable parameters, nets containing a single fully-connected non-linear layer and no convolutional layers are not able to learn functions as accurate as deeper convolutional models. This result is consistent with those reported in Ba and Caruana (2014). However, we also find that shallow nets that contain only 1-2 convolutional layers also are unable to achieve accuracy comparable to deeper models if the same number of parameters are used in the shallow and deep models. Deep convolutional nets are significantly more accurate than shallow convolutional models, given the same parameter budget. We do, however, see evidence that model compression allows accurate models to be trained that are shallower and have fewer convolutional layers than the deep convolutional architectures needed to learn high-accuracy models from the original 1-hot hard-target training data. The question remains why extra layers are required to train accurate models from the original training data."
    }, {
      "heading" : "6 APPENDIX",
      "text" : ""
    }, {
      "heading" : "6.1 DETAILS OF TRAINING THE TEACHER MODELS",
      "text" : "Weights of trained nets are initialized as in Glorot and Bengio (2010). The models trained in Section 2.7 contain eight convolutional layers organized into three groups (2-2-4) and two fully-connected hidden layers. The Bayesian hyperparameter optimization controls four constants C1, C2, C3, H1 all in the range [0, 1] that are then linearly transformed to the number of filters/neurons in each layer. The hyperparameters for which ranges were not shown in Section 2.7 are: the four separate dropout rates (DOc1,DOc2,DOc3,DOf) and the five constants Dh, Ds, Dv, As, Av controlling the HSV data augmentation. The ranges we selected are DOc1 ∈ [0.1, 0.3],DOc2 ∈ [0.25, 0.35],DOc3 ∈ [0.3, 0.44],DOf1 ∈ [0.2, 0.65],DOf2 ∈ [0.2, 0.65], Dh ∈ [0.03, 0.11], Ds ∈ [0.2, 0.3], Dv ∈ [0.0, 0.2], As ∈ [0.2, 0.3], Av ∈ [0.03, 0.2], partly guided by Snoek et al. (2015) and visual inspection of the resulting augmentations.\nThe number of filters and hidden units for the models have the following bounds: 1 conv. layer: 50 - 500 filters, 200 - 2000 hidden units, number of units in bottleneck is the dependent variable. 2 conv. layers: 50 - 500 filters, 100 - 400 filters, number of hidden units is the dependent variable. 3 conv. layers: 50 - 500 filters (layer 1), 100 - 300 filters (layers 2-3), # of hidden units is dependent the variable. 4 conv. layers: 50 - 300 filters (layers 1-2), 100 - 300 filters (layers 3-4), # of hidden units is the dependent variable.\nAll convolutional filters in the model are sized 3×3, max-pooling is applied over windows of 2×2 and we use ReLU units throughout all our models. We apply dropout after each max-pooling layer with the three rates DOc1,DOc2,DOc3 and after each of the two fully-connected layers with the same rate DOf ."
    }, {
      "heading" : "6.2 DETAILS OF TRAINING MODELS OF VARIOUS DEPTHS ON CIFAR-10 HARD 0/1 LABELS",
      "text" : "Models in the first four rows in Table 1 are trained similarly to those in Section 6.1, and are architecturally equivalent to the four convolutional student models shown in Table 2 with 10 million parameters. The following hyperparameters are optimized: initial learning rate [0.0015, 0.025] (optimized on a log scale), momentum [0.68, 0.97] (optimized on a log scale), constants C1, C2 ∈ [0, 1] that control the number of filters or neurons in different layers, and up to four different dropout rates DOc1 ∈ [0.05, 0.4],DOc2 ∈ [0.1, 0.6],DOc3 ∈ [0.1, 0.7],DOf1 ∈ [0.1, 0.7] for the different layers. Weight decay was set to 2 · 10−4 and we used the same data augmentation settings as for the student models. We use 5×5 convolutional filters, one nonlinear hidden layer in each model and each max-pooling operation is followed by dropout with a separately optimized rate. We use 2×2 max-pooling except in the model with only one convolutional layer where we apply 3×3 pooling as this seemed to boost performance and reduces the number of parameters."
    }, {
      "heading" : "6.3 DETAILS OF TRAINING STUDENT MODELS OF VARIOUS DEPTHS ON ENSEMBLE LABELS",
      "text" : "Our student models have the same architecture as models in Section 6.2. The model without convolutional layers consists of one linear layer that acts as a bottleneck followed by a hidden layer of ReLU units. The following hyperparameters are optimized: initial learning rate [0.0013, 0.016] (optimized on a log scale), momentum [0.68, 0.97] (optimized on a log scale), input-scale ∈ [0.8, 1.25], global initialization scale (after initialization) ∈ [0.4, 2.0], layer-width constants C1, C2 ∈ [0, 1] that control the number of filters or neurons. The exact ranges for the number of filters and implicitly resulting number of hidden units was chosen for all twenty optimization experiments independently, as architectures, number of units and number of parameters strongly interact.\nFor the non-convolutional models we chose a slightly different hyper-parameterization. Given that all layers (in models with “two layers” or more) are nonlinear and fully connected we treat all of them similarly from the hyperparameter-optimizer’s point of view. In order to smoothly enforce the parameter budgets without rejecting any samples from the Bayesian optimizer we instead optimize the ratios of hidden units in each layer (numbers between 0 and 1), and then re-normalize and scale them to the final number of neurons in each layer to match the target parameter budget.\nFigure 2 is similar to 1 but includes preliminary results from experiments for models with 100M parameters. We are also running experiments with 300M parameters. Unfortunately, Bayesian optimization on models with 100M and 300M parameters is even more expensive than for the other points in the graph.\nAs expected, adding capacity to the convolutional students (top of the figure) modestly increases their accuracy. Preliminary results for the MLPs however (too preliminary to include in the graph) may not show the same increase in accuracy with increasing model size. Models with two or three hidden layers may benefit from adding capacity to each layer, but we have yet to see any benefit from adding capacity to the MLPs with four or five hidden layers."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Jimmy Ba", "Rich Caruana" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ba and Caruana.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba and Caruana.",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "James Bergstra", "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "In SciPy,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Transferring knowledge from a RNN to a DNN",
      "author" : [ "William Chan", "Nan Rosemary Ke", "Ian Laner" ],
      "venue" : null,
      "citeRegEx" : "Chan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional rectifier networks as generalized tensor decompositions",
      "author" : [ "Nadav Cohen", "Amnon Shashua" ],
      "venue" : "arXiv preprint arXiv:1603.00162,",
      "citeRegEx" : "Cohen and Shashua.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cohen and Shashua.",
      "year" : 2016
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "George Cybenko" ],
      "venue" : "Mathematics of Control, Signals and Systems,",
      "citeRegEx" : "Cybenko.,? \\Q1989\\E",
      "shortCiteRegEx" : "Cybenko.",
      "year" : 1989
    }, {
      "title" : "Big neural networks waste capacity",
      "author" : [ "Yann N. Dauphin", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Dauphin and Bengio.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dauphin and Bengio.",
      "year" : 2013
    }, {
      "title" : "Understanding deep architectures using a recursive convolutional network",
      "author" : [ "David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun" ],
      "venue" : "In ICLR (workshop track),",
      "citeRegEx" : "Eigen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Eigen et al\\.",
      "year" : 2014
    }, {
      "title" : "Scheduled denoising autoencoders",
      "author" : [ "Krzysztof J. Geras", "Charles Sutton" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Geras and Sutton.,? \\Q2015\\E",
      "shortCiteRegEx" : "Geras and Sutton.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Fastfood-computing hilbert space expansions in loglinear time",
      "author" : [ "Quoc Le", "Tamás Sarlós", "Alexander Smola" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Le et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning small-size dnn with output-distribution-based criteria",
      "author" : [ "Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Why deep neural networks",
      "author" : [ "Shiyu Liang", "R Srikant" ],
      "venue" : "arXiv preprint arXiv:1610.04161,",
      "citeRegEx" : "Liang and Srikant.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liang and Srikant.",
      "year" : 2016
    }, {
      "title" : "How far can we go without convolution",
      "author" : [ "Zhouhan Lin", "Roland Memisevic", "Shaoqing Ren", "Kishore Konda" ],
      "venue" : "Improving fully-connected networks",
      "citeRegEx" : "Lin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-bias autoencoders and the benefits of co-adapting features",
      "author" : [ "Roland Memisevic", "Kishore Konda", "David Krueger" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Memisevic et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Memisevic et al\\.",
      "year" : 2015
    }, {
      "title" : "Actor-mimic: Deep multitask and transfer reinforcement learning",
      "author" : [ "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Parisotto et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Parisotto et al\\.",
      "year" : 2016
    }, {
      "title" : "Regularizing neural networks by penalizing output distributions",
      "author" : [ "Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Pereyra et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "FitNets: Hints for thin deep",
      "author" : [ "Adriana Romero", "Ballas Nicolas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "nets. ICLR,",
      "citeRegEx" : "Romero et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Policy distillation",
      "author" : [ "Andrei A. Rusu", "Sergio Gomez Colmenarejo", "Çaglar Gülçehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Rusu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rusu et al\\.",
      "year" : 2016
    }, {
      "title" : "Conversational speech transcription using context-dependent deep neural networks",
      "author" : [ "Frank Seide", "Gang Li", "Dong Yu" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Seide et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Seide et al\\.",
      "year" : 2011
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Practical bayesian optimization of machine learning",
      "author" : [ "Jasper Snoek", "Hugo Larochelle", "Ryan P Adams" ],
      "venue" : null,
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "Scalable bayesian optimization using deep neural networks",
      "author" : [ "Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md Patwary", "Mostofa Ali", "Ryan P Adams" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2015
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh K Srivastava", "Klaus Greff", "Juergen Schmidhuber" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "80 million tiny images: A large data set for nonparametric object and scene recognition",
      "author" : [ "Antonio Torralba", "Robert Fergus", "William T. Freeman" ],
      "venue" : null,
      "citeRegEx" : "Torralba et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Torralba et al\\.",
      "year" : 2008
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1611.03530,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Cohen and Shashua (2016); Liang and Srikant (2016) suggest that the representational efficiency of deep networks scales exponentially with depth, but it is unclear if this applies only to pathological problems, or is encountered in practice on data sets such as TIMIT and CIFAR.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Cohen and Shashua (2016); Liang and Srikant (2016) suggest that the representational efficiency of deep networks scales exponentially with depth, but it is unclear if this applies only to pathological problems, or is encountered in practice on data sets such as TIMIT and CIFAR.",
      "startOffset" : 0,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Ba and Caruana (2014), however, demonstrated that shallow nets sometimes can learn the functions learned by deep nets, even when restricted to the same number of parameters as the deep nets.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Ba and Caruana (2014), however, demonstrated that shallow nets sometimes can learn the functions learned by deep nets, even when restricted to the same number of parameters as the deep nets. They did this by first training state-of-the-art deep models, and then training shallow models to mimic the deep models. Surprisingly, and for reasons that are not well understood, the shallow models learned more accurate functions when trained to mimic the deep models than when trained on the original data used to train the deep models. In some cases shallow models trained this way were as accurate as state-of-the-art deep models. But this demonstration was made on the TIMIT speech recognition benchmark. Although their deep teacher models used a convolutional layer, convolution is less important for TIMIT than it is for other domains such as image classification. Ba and Caruana (2014) also presented results on CIFAR-10 which showed that a shallow model could learn functions almost as accurate as deep convolutional nets.",
      "startOffset" : 0,
      "endOffset" : 886
    }, {
      "referenceID" : 11,
      "context" : "Our results clearly suggest that deep convolutional nets do, in fact, need to be both deep and convolutional, even when trained to mimic very accurate models via distillation (Hinton et al., 2015).",
      "startOffset" : 175,
      "endOffset" : 196
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work, here we compare shallow models to state-of-the-art deep convolutional models, and restrict the number of parameters in the shallow student models to be comparable to the number of parameters in the deep convolutional teacher models.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work, here we compare shallow models to state-of-the-art deep convolutional models, and restrict the number of parameters in the shallow student models to be comparable to the number of parameters in the deep convolutional teacher models. Because we anticipated that our results might be different, we follow their approach closely to eliminate the possibility that the results differ merely because of changes in methodology. Note that the goal of this paper is not to train models that are small or fast as in Bucila et al. (2006), Hinton et al.",
      "startOffset" : 54,
      "endOffset" : 625
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work, here we compare shallow models to state-of-the-art deep convolutional models, and restrict the number of parameters in the shallow student models to be comparable to the number of parameters in the deep convolutional teacher models. Because we anticipated that our results might be different, we follow their approach closely to eliminate the possibility that the results differ merely because of changes in methodology. Note that the goal of this paper is not to train models that are small or fast as in Bucila et al. (2006), Hinton et al. (2015), and Romero et al.",
      "startOffset" : 54,
      "endOffset" : 647
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work, here we compare shallow models to state-of-the-art deep convolutional models, and restrict the number of parameters in the shallow student models to be comparable to the number of parameters in the deep convolutional teacher models. Because we anticipated that our results might be different, we follow their approach closely to eliminate the possibility that the results differ merely because of changes in methodology. Note that the goal of this paper is not to train models that are small or fast as in Bucila et al. (2006), Hinton et al. (2015), and Romero et al. (2015), but to examine if shallow models can be as accurate as deep convolutional models given the same parameter budget.",
      "startOffset" : 54,
      "endOffset" : 673
    }, {
      "referenceID" : 10,
      "context" : "Hinton et al. (2015) generalized the methods of Bucila et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "Hinton et al. (2015) generalized the methods of Bucila et al. (2006) and Ba and Caruana (2014) by incorporating a parameter to control the relative importance of the soft targets provided by the teacher model to the hard targets in the original training data, as well as a temperature parameter that regularizes learning by pushing targets towards the uniform distribution.",
      "startOffset" : 0,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "(2006) and Ba and Caruana (2014) by incorporating a parameter to control the relative importance of the soft targets provided by the teacher model to the hard targets in the original training data, as well as a temperature parameter that regularizes learning by pushing targets towards the uniform distribution.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "(2006) and Ba and Caruana (2014) by incorporating a parameter to control the relative importance of the soft targets provided by the teacher model to the hard targets in the original training data, as well as a temperature parameter that regularizes learning by pushing targets towards the uniform distribution. Hinton et al. (2015) also demonstrated that much of the knowledge passed from the teacher to the student is conveyed as dark knowledge contained in the relative scores (probabilities) of outputs corresponding to other classes, as opposed to the scores given to just the output for the one correct class.",
      "startOffset" : 11,
      "endOffset" : 333
    }, {
      "referenceID" : 3,
      "context" : "Chan et al. (2015); Geras et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Chan et al. (2015); Geras et al. (2015); Li et al.",
      "startOffset" : 0,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and reinforcement learning Parisotto et al.",
      "startOffset" : 0,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and reinforcement learning Parisotto et al. (2016); Rusu et al.",
      "startOffset" : 0,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and reinforcement learning Parisotto et al. (2016); Rusu et al. (2016). Romero et al.",
      "startOffset" : 0,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and reinforcement learning Parisotto et al. (2016); Rusu et al. (2016). Romero et al. (2015) showed that distillation methods can be used to train small students that are more accurate than the teacher models by making the student models deeper, but thinner, than the teacher model.",
      "startOffset" : 0,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "Following Ba and Caruana (2014), the student mimic models are not trained with cross-entropy on the ten p values where pk = ek/ ∑ j e zj output by the softmax layer from the deep teacher model, but instead are trained on the un-normalized log probability values z (the logits) before the softmax activation.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "As in Ba and Caruana (2014), the student is trained as a regression problem given training data {(x, z),.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Ba and Caruana (2014) found that training these wide, shallow mimic models with backpropagation was slow, and introduced a linear bottleneck layer between the input and non-linear layers to speed learning.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Ba and Caruana (2014) found that training these wide, shallow mimic models with backpropagation was slow, and introduced a linear bottleneck layer between the input and non-linear layers to speed learning. The bottleneck layer speeds learning by reducing the number of parameters that must be learned, but does not make the model deeper because the linear terms can be absorbed back into the non-linear weight matrix after learning. See Ba and Caruana (2014) for details.",
      "startOffset" : 0,
      "endOffset" : 459
    }, {
      "referenceID" : 24,
      "context" : "The implementation we use is Spearmint (Snoek et al., 2012).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "The CIFAR-10 (Krizhevsky, 2009) data set consists of a set of natural images from 10 different object classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 27,
      "context" : "The dataset is a labeled subset of the 80 million tiny images dataset (Torralba et al., 2008) and is divided into 50,000 train and",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "We employ the HSV-data augmentation technique as described by Snoek et al. (2015). Thus we shift hue, saturation and value by uniform random values: ∆h ∼ U(−Dh, Dh), ∆s ∼ U(−Ds, Ds), ∆v ∼ U(−Dv, Dv).",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "One limitation of the CIFAR-10 experiments performed in Ba and Caruana (2014) is that the teacher models were not state-of-the-art.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "All models are trained using Theano (Bastien et al., 2012; Bergstra et al., 2010).",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "All models are trained using Theano (Bastien et al., 2012; Bergstra et al., 2010).",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "Weights are initialized as in Glorot and Bengio (2010). We intentionally do not optimize and do not make use of weight decay and dropout when training student models because preliminary experiments showed that these consistently reduced the accuracy of student models by several percent.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "The last two models (*) are numbers reported by Ba and Caruana (2014). The models with 1-4 convolutional layers at the top of the table are included for comparison with student models of similar architecture in Table 2 .",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "The student model trained by Ba and Caruana (2014) is shown in the last line for comparison; it is less accurate and much larger than the student models trained here that also have 1 convolutional layer.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "8% trained on ensemble (Ba and Caruana, 2014)",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "For comparison, the accuracy of the ensemble trained by Ba and Caruana (2014)) is included at the bottom of the table.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "For comparison, the student model trained by Ba and Caruana (2014) also is shown.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "1% achieved by Le et al. (2013), 63.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 11,
      "context" : "1% achieved by Le et al. (2013), 63.9% by Memisevic et al. (2015) and 64.",
      "startOffset" : 15,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "3% by Geras and Sutton (2015)).",
      "startOffset" : 6,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "For example, the ensemble teacher model in Ba and Caruana (2014) was an ensemble of four CNNs, each of which had 3 convolutional layers, but only achieved 89% accuracy, whereas the single student CNNs we train via distillation achieve accuracies above 90% with only 2 convolutional layers, and above 92% with 3 convolutional layers.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "For example, the ensemble teacher model in Ba and Caruana (2014) was an ensemble of four CNNs, each of which had 3 convolutional layers, but only achieved 89% accuracy, whereas the single student CNNs we train via distillation achieve accuracies above 90% with only 2 convolutional layers, and above 92% with 3 convolutional layers. The only other work we are aware of that achieves comparable high accuracy with non-convolutional MLPs is recent work by Lin et al. (2016). They train multi-layer Z-Lin networks, and use a powerful form of data augmentation based on deformations that we did not use.",
      "startOffset" : 43,
      "endOffset" : 472
    }, {
      "referenceID" : 0,
      "context" : "This is consistent with the observation made by Ba and Caruana (2014) that student mimic models did not seem to overfit.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "This is consistent with the observation made by Ba and Caruana (2014) that student mimic models did not seem to overfit. Hinton et al. (2015) claim that soft targets convey more information per sample than Boolean hard targets.",
      "startOffset" : 48,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "This is consistent with the observation made by Ba and Caruana (2014) that student mimic models did not seem to overfit. Hinton et al. (2015) claim that soft targets convey more information per sample than Boolean hard targets. The also suggest that the dark knowledge in the soft targets for other classes further helped regularization, and that early stopping was unnecessary. Romero et al. (2015) extend distillation by using the intermediate representations learned by the teacher as hints to guide training deep students, and teacher confidences further help regularization by providing a measure of sample simplicity to the student, akin to curriculum learning.",
      "startOffset" : 48,
      "endOffset" : 400
    }, {
      "referenceID" : 0,
      "context" : "This is consistent with the observation made by Ba and Caruana (2014) that student mimic models did not seem to overfit. Hinton et al. (2015) claim that soft targets convey more information per sample than Boolean hard targets. The also suggest that the dark knowledge in the soft targets for other classes further helped regularization, and that early stopping was unnecessary. Romero et al. (2015) extend distillation by using the intermediate representations learned by the teacher as hints to guide training deep students, and teacher confidences further help regularization by providing a measure of sample simplicity to the student, akin to curriculum learning. In other work, Pereyra et al. (2017) suggest that the soft targets provided by a teacher provide a form of confidence penalty that penalizes low entropy distributions and label smoothing, both of which improve regularization by maintaining a reasonable ratio between the logits of incorrect classes.",
      "startOffset" : 48,
      "endOffset" : 705
    }, {
      "referenceID" : 11,
      "context" : "The multiple soft targets used to train student models have a high information density (Hinton et al., 2015) and thus provide regularization by reducing the impact of brute-force memorization.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "This result is consistent with those reported in Ba and Caruana (2014). However, we also find that shallow nets that contain only 1-2 convolutional layers also are unable to achieve accuracy comparable to deeper models if the same number of parameters are used in the shallow and deep models.",
      "startOffset" : 49,
      "endOffset" : 71
    } ],
    "year" : 2017,
    "abstractText" : "Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.",
    "creator" : "LaTeX with hyperref package"
  }
}