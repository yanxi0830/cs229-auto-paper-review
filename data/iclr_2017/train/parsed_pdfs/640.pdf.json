{
  "name" : "640.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BEYOND BILINGUAL: MULTI-SENSE WORD EMBED-",
    "authors" : [ "DINGS USING", "MULTILINGUAL CONTEXT" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Word embeddings (Turian, Ratinov, and Bengio, 2010; Mikolov, Yih, and Zweig, 2013, inter alia) represent a word as a point in a vector space. This space is able to capture semantic relationships: vectors of words with similar meanings have high cosine similarity. Use of embeddings as features has been shown to benefit several NLP tasks and serve as good initializations for deep architectures ranging from dependency parsing (Bansal, Gimpel, and Livescu, 2014) to named entity recognition (Guo et al., 2014b).\nAlthough these representations are now ubiquitous in NLP, most algorithms for learning wordembeddings do not allow a word to have different meanings in different contexts, a phenomenon known as polysemy. For example, the word bank assumes different meanings in financial (eg. “bank pays interest”) and geographical contexts (eg. “river bank”) and which cannot be represented adequately with a single embedding vector. Unfortunately, there are no large sense-tagged corpora available and such polysemy must be inferred from the data during the embedding process.\nSeveral attempts (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015) have been made to infer multi-sense word representations by modeling the sense as a latent variable in a Bayesian non-parametric framework. These approaches rely on the ”one-sense per collocation” heuristic (Yarowsky, 1995), which assumes that presence of nearby words correlate with the sense of the word of interest. This heuristic provides only a weak signal for sense identification, and such algorithms require large amount of training data to achieve competitive performance.\nRecently, several approaches (Guo et al., 2014a; Šuster, Titov, and van Noord, 2016) propose to learn multi-sense embeddings by exploiting the fact that different senses of the same word may be translated into different words in a foreign language (Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng, Wang, and Chan, 2003). For example, bank in English may be translated to banc or banque in French, depending on whether the sense is financial or geographical. Such bilingual distributional information allows the model to identify which sense of a word is being used during training.\nHowever, bilingual distributional signals often do not suffice. It is common that polysemy for a word survives translation. Fig. 1 shows an illustrative example – both senses of interest get translated to intérêt in French. However, this becomes much less likely as the number of languages under consideration grows. By looking at Chinese translation in Fig. 1, we can observe that the senses translate to different surface forms. Note that the opposite can also happen (i.e. same surface forms in Chinese, but different in French). Existing crosslingual approaches are inherently bilingual and cannot naturally extend to include additional languages due to several limitations (details in Section4). Furthermore, works like (Šuster, Titov, and van Noord, 2016) sets a fixed number of senses for each word, leading to inefficient use of parameters, and unnecessary model complexity.1\nThis paper addresses these limitations by proposing a multi-view Bayesian non-parametric word representation learning algorithm which leverages multilingual distributional information. Our representation learning framework is the first multilingual (not bilingual) approach, allowing us to utilize arbitrarily many languages to disambiguate words in English. To move to multilingual system, it is necessary to ensure that the embeddings of each foreign language are relatable to each other (i.e., they live in the same space). We solve this by proposing an algorithm in which word representations are learned jointly across languages, using English as a bridge. While large parallel corpora between two languages are scarce, using our approach we can concatenate multiple parallel corpora to obtain a large multilingual corpus. The parameters are estimated in a Bayesian nonparametric framework that allows our algorithm to only associate a word with a new sense vector when evidence (from either same or foreign language context) requires it. As a result, the model infers different number of senses for each word in a data-driven manner, avoiding wasting parameters.\nTogether, these two ideas – multilingual distributional information and nonparametric sense modeling – allow us to disambiguate multiple senses using far less data than is necessary for previous methods. We experimentally demonstrate that our algorithm can achieve competitive performance after training on a small multilingual corpus, comparable to a model trained monolingually on a much larger corpus. We present an analysis discussing the effect of various parameters – choice of language family for deriving the multilingual signal, crosslingual window size etc. and also show qualitative improvement in the embedding space."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Work on inducing multi-sense embeddings can be divided in two broad categories – two-staged approaches and joint learning approaches. Two-staged approaches (Reisinger and Mooney, 2010; Huang et al., 2012) induce multi-sense embeddings by first clustering the contexts and then using the clustering to obtain the sense vectors. The contexts can be topics induced using latent topic models(Liu, Qiu, and Huang, 2015; Liu et al., 2015), or Wikipedia (Wu and Giles, 2015) or coarse part-of-speech tags (Qiu et al., 2014). A more recent line of work in the two-staged category is that of retrofitting (Faruqui et al., 2015; Jauhar, Dyer, and Hovy, 2015), which aims to infuse semantic ontologies from resources like WordNet (Miller, 1995) and Framenet (Baker, Fillmore, and Lowe, 1998) into embeddings during a post-processing step. Such resources list (albeit not exhaustively) the senses of a word, and by retro-fitting it is possible to tease apart the different senses of a word. While some resources like WordNet (Miller, 1995) are available for many languages, they are not exhaustive in listing all possible senses. Indeed, the number senses of a word is highly dependent on the task and cannot be pre-determined using a lexicon (Kilgarriff, 1997). Ideally, the senses should be inferred in a data-driven manner, so that new senses not listed in such lexicons can be discovered. While recent work has attempted to remedy this by using parallel text for retrofitting sense-specific embeddings (Ettinger, Resnik, and Carpuat, 2016), their procedure requires creation of sense graphs, which introduces additional tuning parameters. On the other hand, our approach only requires two tuning parameters (prior α and maximum number of senses T ).\n1Most words in conventional English are monosemous, i.e. single sense (eg. the word monosemous)\nIn contrast, joint learning approaches (Neelakantan et al., 2014; Li and Jurafsky, 2015) jointly learn the sense clusters and embeddings by using non-parametrics. Our approach belongs to this category. The closest non-parametric approach to ours is that of (Bartunov et al., 2016), who proposed a multisense variant of the skip-gram model which learns the different number of sense vectors for all words from a large monolingual corpus (eg. English Wikipedia). Our work can be viewed as the multi-view extension of their model which leverages both monolingual and crosslingual distributional signals for learning the embeddings. In our experiments, we compare our model to monolingually trained version of their model.\nIncorporating crosslingual distributional information is a popular technique for learning word embeddings, and improves performance on several downstream tasks (Faruqui and Dyer, 2014; Guo et al., 2016; Upadhyay et al., 2016). However, there has been little work on learning multi-sense embeddings using crosslingual signals (Bansal, DeNero, and Lin, 2012; Guo et al., 2014a; Šuster, Titov, and van Noord, 2016) with only (Šuster, Titov, and van Noord, 2016) being a joint approach. (Kawakami and Dyer, 2015) also used bilingual distributional signals in a deep neural architecture to learn context dependent representations for words, though they do not learn separate sense vectors."
    }, {
      "heading" : "3 MODEL DESCRIPTION",
      "text" : "Let E = {xe1, .., xei , .., xeNe} denote the words of the English side and F = {x f 1 , .., x f i , .., x f Nf } denote the words of the foreign side of the parallel corpus. We assume that we have access to word alignments Ae→f and Af→e mapping words in English sentence to their translation in foreign sentence (and vice-versa), so that xe and xf are aligned if Ae→f (xe) = xf . We define Nbr(x, L, d) as the neighborhood in language L of size d (on either side) around word x in its sentence. The English and foreign neighboring words are denoted by ye and yf , respectively. Note that ye and yf need not be translations of each other. Each word xf in the foreign vocabulary is associated with a dense vector xf in Rm, and each word xe in English vocabulary admits at most T sense vectors, with the kth sense vector denoted as xek.\n2 As our main goal is to model multiple senses for words in English, we do not model polysemy in the foreign language and use a single vector to represent each word in the foreign vocabulary.\nWe model the joint conditional distribution of the context words ye, yf given an English word xe and its corresponding translation xf on the parallel corpus:\nP (ye, yf | xe, xf ;α, θ), (1) where θ are model parameters (i.e. all embeddings) and α governs the hyper-prior on latent senses.\nAssume xe has multiple senses, which are indexed by the random variable z, Eq. (1) can be rewritten, ∫\nβ\n∑ z P (ye, yfz, β | xe, xf , α; θ)dβ\nwhere β are the parameters determining the model probability on each sense for xe (i.e., the weight on each possible value for z). We place a Dirichlet process (Ferguson, 1973) prior on sense assignment for each word. Thus, adding the word-x subscript to emphasize that these are word-specific senses,\nP (zx = k | βx) = βxk ∏k−1\nr=1 (1− βxr), βxk | α\nind∼ Beta(βxk | 1, α), k = 1, . . . . (2)\nThat is, the potentially infinite number of senses for each word x have probability determined by the sequence of independent stick-breaking weights, βxk, in the constructive definition of the DP (Sethuraman, 1994). The hyper-prior concentration α provides information on the number of senses we expect to observe in our corpus.\nAfter conditioning upon word sense, we decompose the context probability P (ye, yf | z, xe, xf ; θ) into two terms, P (ye | xe, xf , z; θ)P (yf | xe, xf , z; θ). Both the first and the second terms are sense-dependent, and each factors as,\nP (y |xe, xf , z=k; θ)∝Ψ(xe, z=k, y)Ψ(xf , y) = exp(yTxek) exp(yTxf ) = exp(yT (xek+xf )), 2We also maintain a context vector for each word in the English and Foreign vocabularies. The context\nvector is used as the representation of the word when it appears as the context for another word.\nThe bank paid me [interest] on my savings.\nΨ(interest,2,savings)\nwhere xek is the embedding corresponding to the k th sense of the word xe, and y is either ye or yf . The factor Ψ(xe, z = k, y) use the corresponding sense vector in a skip-gram-like formulation. This results in total of 4 factors,\nP (ye, yf | z, xe, xf ; θ) ∝ Ψ(xe, z, ye)Ψ(xf , yf )Ψ(xe, z, yf )Ψ(xf , ye) (3) See Figure 2 for illustration of each factor. This modeling approach is reminiscent of (Luong, Pham, and Manning, 2015), who jointly learned embeddings for two languages l1 and l2 by optimizing a joint objective containing 4 skip-gram terms using the aligned pair (xe,xf )– two predicting monolingual contexts l1 → l1, l2 → l2 , and two predicting crosslingual contexts l1 → l2, l2 → l1.\nLearning. Learning involves maximizing the log-likelihood, P (ye, yf | xe, xf ;α, θ) = ∫ β ∑ z P (ye, yf , z, β | xe, xf , α; θ)dβ\nLet q(z, β) = q(z)q(β) where q(z) = ∏ i q(zi) and q(β) = ∏V w=1 ∏T k=1 βwk be the fully factorized variational approximation of the true posterior P (z, β | ye, yf , xe, xf , α), where V is the size of english vocabulary, and T is the maximum number of senses for any word. The optimization problem solves for θ,q(z) and q(β) using the stochastic variational inference technique (Hoffman et al., 2013) similar to (Bartunov et al., 2016) (refer for details).\nThe resulting learning algorithm is shown as Algorithm 1. The first for-loop (line 1) updates the English sense vectors using the crosslingual and monolingual contexts. First, the expected sense distribution for the current English word w is computed using the current estimate of q(β) (line 4). The sense distribution is updated (line 7) using the combined monolingual and crosslingual contexts (line 5) and re-normalized (line 8). Using the updated sense distribution q(β)’s sufficient statistics is re-computed (line 9) and the global parameter θ is updated (line 10) as follows,\nθ ← θ + ρt∇θ ∑\nk|zik> ∑ y∈yc zik log p(y|xi, k, θ) (4)\nNote that in the above sum, a sense participates in a update only if its probability exceeds a threshold (=0.001). The final model retains sense vectors whose sense probability exceeds the same threshold. The last for-loop (line 11) jointly optimizes the foreign embeddings using English context with the standard skip-gram updates.\nDisambiguation. Similar to (Bartunov et al., 2016), we can disambiguate the sense for the word xe given a monolingual context ye as follows,\nP (z | xe, ye) ∝ P (ye | xe, z; θ) ∑\nβ P (z | xe, β)q(β) (5)\nAlthough the model trains embeddings using both monolingual and crosslingual context, we only use monolingual context at test time. We found that so long as the model has been trained with multilingual context, it performs well in sense disambiguation on new data even if it contains only monolingual context. A similar observation was made by (Šuster, Titov, and van Noord, 2016)."
    }, {
      "heading" : "4 MULTILINGUAL EXTENSION",
      "text" : "Bilingual distributional signal alone may not be sufficient as polysemy may survive translation in the second language. Unlike existing approaches, we can easily incorporate multilingual distributional\nAlgorithm 1 Psuedocode of Learning Algorithm\nInput: parallel corpus E = {xe1, .., xei , .., xeNe} and F = {x f 1 , .., x f i , .., x f Nf } and alignments Ae→f and\nAf→e, Hyper-parameters α and T , window sizes d, d′ . Output: θ, q(β), q(z)\n1: for i = 1 to Ne do . update english vectors 2: w← xei 3: for k = 1 to T do 4: zik ← Eq(βw)[log p(zi = k|, x e i )] 5: yc← Nbr(xei ,E,d) ∪ Nbr(xfi ,F ,d ′) ∪ {xfi } where x f i = Ae→f (x e i ) 6: for y in yc do 7: SENSE-UPDATE(xei , y, zi) 8: Renormalize zi using softmax 9: Update suff. stats. for q(β) like (Bartunov et al., 2016) 10: Update θ using eq. (4) 11: for i = 1 to Nf do . jointly update foreign vectors 12: yc← Nbr(xfi ,F ,d) ∪ Nbr(x e i ,E,d\n′) ∪ {xei} where xei = Af→e(xfi ) 13: for y in yc do 14: SKIP-GRAM-UPDATE(xfi , y) 15: procedure SENSE-UPDATE(xi, y, zi) 16: zik ← zik + log p(y|xi, k, θ)\nsignals in our model. For using languages l1 and l2 to learn multi-sense embeddings for English, we train on a concatenation of En-l1 parallel corpus with an En-l2 parallel corpus. This technique can easily be generalized to more than two foreign languages to obtain a large multilingual corpus.\nValue of Ψ(ye, xf ). The factor modeling the dependence of the english context word ye on foreign word xf is crucial to performance when using multiple languages. Consider the case of using French and Spanish contexts to disambiguate the financial sense of the english word bank. In this case, the (financial) sense vector of bank will be used to predict vector of banco (Spanish context) and banque (French context). If vectors for banco and banque do not reside in the same space or are not close, the model will incorrectly assume they are different contexts to introduce a new sense for bank. This is precisely why the bilingual models, like that of (Šuster, Titov, and van Noord, 2016), cannot be extended to multilingual setting, as they pre-train the embeddings of second language before running the multi-sense embedding process. As a result of naive pre-training, the French and Spanish vectors of semantically similar pairs like (banco,banque) will lie in different spaces and need not be close. A similar reason holds for (Guo et al., 2014a), as they use a two step approach instead of joint learning.\nTo avoid this, the vector for pairs like banco and banque should lie in the same space and close to each other and the sense vector for bank. The Ψ(ye, xf ) term attempts to ensure this by using the vector for banco and banque to predict the vector of bank. This way, the model brings the embedding space for Spanish and French closer by using English as a bridge language during joint training. A similar idea of using English as a bridging language was used in the models proposed in (Hermann and Blunsom, 2014) and (Coulmance et al., 2015). Beside the benefit in the multilingual case, the Ψ(ye, xf ) term improves performance in the bilingual case as well, as it forces the English and second language embeddings to remain close in space.\nTo show the value of Ψ(ye, xf ) factor in our experiments, we ran a variant of Algorithm 1 without the Ψ(ye, xf ) factor, by only using monolingual neighborhood Nbr(xfi , F ) in line 12 of Algorithm 1. We call this variant ONE-SIDED model and the model in Algorithm 1 the FULL model."
    }, {
      "heading" : "5 EXPERIMENTAL SETUP",
      "text" : "Parallel Corpora. We use parallel corpora in English (En), French (Fr), Spanish (Es), Russian (Ru) and Chinese (Zh) in our experiments. Corpus statistics for all datasets used in our experiments are shown in Table 1. For En-Zh, we use the FBIS parallel corpus (LDC2003E14). For En-Fr, we use the first 10M lines from the Giga-EnFr corpus released as part of the WMT shared task (CallisonBurch et al., 2011). Note that the domain from which parallel corpus has been derived can affect the final result. To understand what choice of languages provide suitable disambiguation signal,\nit is necessary to control for domain in all parallel corpora. To this end, we also used the En-Fr, En-Es, En-Zh and En-Ru sections of the MultiUN parallel corpus (Eisele and Chen, 2010). Word alignments were generated using fast_align tool (Dyer, Chahuneau, and Smith, 2013) in the symmetric intersection mode. Tokenization and other preprocessing were performed using cdec 3 toolkit. Stanford Segmenter (Tseng et al., 2005) was used to preprocess the chinese corpora.\nWord Sense Induction (WSI). We evaluate our approach on word sense induction task. In this task, we are given several sentences showing usages of the same word, and are required to cluster all sentences which use the same sense (Nasiruddin, 2013). The predicted clustering is then compared against a provided gold clustering. Note that WSI is a harder task than Word Sense Disambiguation (WSD)(Navigli, 2009), as unlike WSD, this task does not involve any supervision or explicit human knowledge about senses of words. We use the disambiguation approach in eq. (5) to predict the sense given the word and four context words.\nTo allow for fair comparison with earlier work, we use the same benchmark datasets as (Bartunov et al., 2016) – Semeval-2007, 2010 and Wikipedia Word Sense Induction (WWSI). We report Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) in the experiments, as ARI is a more strict and precise metric than F-score and V-measure.\nParameter Tuning. For fairness, we used five context words on either side to update each English word-vectors in all the experiments. In the monolingual setting, all five words are English; in the multilingual settings, we used four neighboring English words plus the one foreign word aligned to the word being updated (d = 4, d′ = 0 in Algorithm 1). We also analyze effect of varying d′.\nWe tune the parameters α and T by maximizing the log-likelihood of a held out English text.4 The parameters were chosen from the following values α = {0.05, 0.1, .., 0.25}, T = {5, 10, .., 30}. All models were trained for 10 iteration with a decaying learning rate of 0.025, decayed to 0. Unless otherwise stated, all embeddings are 100 dimensional.\nUnder various choice of α and T , we identify only about 10-20% polysemous words in the vocabulary using monolingual training and 20-25% polysemous using multilingual training. It is evident using the non-parametric prior has led to substantially more efficient representation compared to previous methods with fixed number of senses per word."
    }, {
      "heading" : "6 EXPERIMENTAL RESULTS",
      "text" : "We performed extensive experiments to evaluate the benefit of leveraging bilingual and multilingual information during training. We also analyze how the different choices of language family (i.e. using more distant vs more similar languages) affect performance of the embeddings.\nWord Sense Induction Results. The results for WSI are shown in Table 2. MONO refers to the AdaGram model of (Bartunov et al., 2016) trained on the English side of the parallel corpus. In all cases, the MONO model is outperformed by ONE-SIDED and FULL models, showing the benefit of using crosslingual signal in training. Best performance is attained by the multilingual model (EnFrZh), showing value of multilingual signal. The value of Ψ(ye, xf ) term is also verified by the fact that the ONE-SIDED model performs worse than the FULL model.\nWe can also compare (unfairly to FULL model) to the best results described in (Bartunov et al., 2016), which achieved ARI scores of 0.069, 0.097 and 0.286 on the three datasets respectively after\n3github.com/redpony/cdec 4first 100k lines from the En-Fr Europarl (Koehn, 2005)\ntraining 300 dimensional embeddings on English Wikipedia (≈ 100M lines). Note that, as WWSI was derived from Wikipedia, training on Wikipedia gives AdaGram model an undue advantage, resulting in high ARI score on WWSI. Nevertheless, even in the unfair comparison, it noteworthy that on S-2007 and S-2010, we can achieve comparable performance (0.067 and 0.094) with multilingual training to a model trained on almost 5 times more data and higher (300) dimensional embeddings.\nContextual Word Similarity Results. For completeness, we report correlation scores on Stanford contextual word similarity dataset (SCWS) (Huang et al., 2012) in Table 2. The task requires computing similarity between two words given their contexts. While the bilingually trained model outperforms the monolingually trained model, surprisingly the multilingually trained model does not perform well on SCWS. We believe this may be due to our parameter tuning strategy.5\nEffect of Language Family Distance. Intuitively, choice of language can affect the result from crosslingual training as some languages may provide better disambiguation signals than others. We performed a systematic set of experiment to evaluate whether we should choose languages from a closer family (Indo-European languages) or farther family (Non-Indo European Languages) as training data alongside English.6 To control for domain here we use the MultiUN corpus. We use En paired with Fr and Es as Indo-European languages, and English paired with Ru and Zh for representing Non-Indo-European languages.\nFrom Table 3, we see that using Non-Indo European languages yield a slightly higher average improvement in WSI task than using Indo-European languages. This suggests that using languages from a distance family aids better disambiguation. Our findings echo those of (Resnik and Yarowsky, 1999), who found that the tendency to lexicalize different senses of an English word differently in a second language correlated with language distance.\nEffect of Window Size. Figure 3d shows the effect of increasing the crosslingual window (d′) on the average ARI on the WSI task for the En-Fr and En-Zh models. While increasing the window size improves the average score for En-Zh model, the score for the En-Fr model goes down. This suggests that it might be beneficial to have a separate window parameter per language. This also\n5Most works tune directly on the test dataset for Word Similarity tasks (Faruqui et al., 2016) 6 (Šuster, Titov, and van Noord, 2016) compared different languages but did not control for domain.\naligns with the observation earlier that different language families have different suitability (bigger crosslingual context from a distant family helped) and requirements for optimal performance.\nQualitative Illustration. As an illustration for the effects of multilingual training, Figure 3 shows PCA plots for 11 sense vectors for 9 words using monolingual, bilingual and multilingual models. From Fig 3a, we note that with monolingual training the senses are poorly separated. Although the model infers two senses for bank, the two senses of bank are close to financial terms, suggesting their distinction was not recognized. The same can be observed about apple. In Fig 3b, with bilingual training, the model infers two senses of bank correctly, and two sense of apple become more distant. The model can still improve eg. pulling interest towards the financial sense of bank, and pulling itunes towards apple 2. Finally, in Fig 3c, all senses of the words are more clearly clustered, improving over the clustering of Fig 3b. The senses of apple, interest, and bank are well separated, and are close to sense-specific words, showing the benefit of multilingual training."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We presented a multi-view, non-parametric word representation learning algorithm which can leverage multilingual distributional information. Our approach effectively combines the benefits of crosslingual training and Bayesian non-parametrics. Ours is the first multi-sense representation learning algorithm capable of using multilingual distributional information efficiently, by combining several parallel corpora to obtained a large multilingual corpus. Our experiments show how this multi-view approach learns high-quality embeddings using substantially less data and parameters than prior state-of-the-art. While we focused on improving the embedding of English words here, the same algorithm could learn better multi-sense embedding for Chinese, for instance. Exciting avenues for future research include extending our approach to model polysemy in foreign language. The sense vectors can then be aligned across languages (thanks to our joint training paradigm), to generate a multilingual Wordnet like resource, in a completely unsupervised manner."
    } ],
    "references" : [ {
      "title" : "The berkeley framenet project",
      "author" : [ "C.F. Baker", "C.J. Fillmore", "J.B. Lowe" ],
      "venue" : "ACL.",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Unsupervised translation sense clustering",
      "author" : [ "M. Bansal", "J. DeNero", "D. Lin" ],
      "venue" : "NAACL.",
      "citeRegEx" : "Bansal et al\\.,? 2012",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2012
    }, {
      "title" : "Tailoring continuous word representations for dependency parsing",
      "author" : [ "M. Bansal", "K. Gimpel", "K. Livescu" ],
      "venue" : "ACL.",
      "citeRegEx" : "Bansal et al\\.,? 2014",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2014
    }, {
      "title" : "Breaking sticks and ambiguities with adaptive skip-gram",
      "author" : [ "S. Bartunov", "D. Kondrashkin", "A. Osokin", "D. Vetrov" ],
      "venue" : "AISTATS.",
      "citeRegEx" : "Bartunov et al\\.,? 2016",
      "shortCiteRegEx" : "Bartunov et al\\.",
      "year" : 2016
    }, {
      "title" : "Findings of the 2011 workshop on statistical machine translation",
      "author" : [ "C. Callison-Burch", "P. Koehn", "C. Monz", "O.F. Zaidan" ],
      "venue" : "WMT Shared Task.",
      "citeRegEx" : "Callison.Burch et al\\.,? 2011",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2011
    }, {
      "title" : "Trans-gram, fast cross-lingual wordembeddings",
      "author" : [ "J. Coulmance", "J.-M. Marty", "G. Wenzek", "A. Benhalloum" ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Coulmance et al\\.,? 2015",
      "shortCiteRegEx" : "Coulmance et al\\.",
      "year" : 2015
    }, {
      "title" : "Word sense disambiguation using a second language monolingual corpus",
      "author" : [ "I. Dagan", "A. Itai" ],
      "venue" : "Computational linguistics.",
      "citeRegEx" : "Dagan and Itai,? 1994",
      "shortCiteRegEx" : "Dagan and Itai",
      "year" : 1994
    }, {
      "title" : "An unsupervised method for word sense tagging using parallel corpora",
      "author" : [ "M. Diab", "P. Resnik" ],
      "venue" : "ACL.",
      "citeRegEx" : "Diab and Resnik,? 2002",
      "shortCiteRegEx" : "Diab and Resnik",
      "year" : 2002
    }, {
      "title" : "A simple, fast, and effective reparameterization of ibm model",
      "author" : [ "C. Dyer", "V. Chahuneau", "N.A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Dyer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "MultiUN: A multilingual corpus from united nation documents",
      "author" : [ "A. Eisele", "Y. Chen" ],
      "venue" : "LREC.",
      "citeRegEx" : "Eisele and Chen,? 2010",
      "shortCiteRegEx" : "Eisele and Chen",
      "year" : 2010
    }, {
      "title" : "Retrofitting sense-specific word vectors using parallel text",
      "author" : [ "A. Ettinger", "P. Resnik", "M. Carpuat" ],
      "venue" : "NAACL.",
      "citeRegEx" : "Ettinger et al\\.,? 2016",
      "shortCiteRegEx" : "Ettinger et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving vector space word representations using multilingual correlation",
      "author" : [ "M. Faruqui", "C. Dyer" ],
      "venue" : "EACL.",
      "citeRegEx" : "Faruqui and Dyer,? 2014",
      "shortCiteRegEx" : "Faruqui and Dyer",
      "year" : 2014
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith" ],
      "venue" : "NAACL.",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Problems with evaluation of word embeddings using word similarity tasks",
      "author" : [ "M. Faruqui", "Y. Tsvetkov", "P. Rastogi", "C. Dyer" ],
      "venue" : "1st RepEval Workshop.",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "A bayesian analysis of some nonparametric problems",
      "author" : [ "T.S. Ferguson" ],
      "venue" : "The annals of statistics.",
      "citeRegEx" : "Ferguson,? 1973",
      "shortCiteRegEx" : "Ferguson",
      "year" : 1973
    }, {
      "title" : "Learning sense-specific word embeddings by exploiting bilingual resources",
      "author" : [ "J. Guo", "W. Che", "H. Wang", "T. Liu" ],
      "venue" : "COLING.",
      "citeRegEx" : "Guo et al\\.,? 2014a",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Revisiting embedding features for simple semi-supervised learning",
      "author" : [ "J. Guo", "W. Che", "H. Wang", "T. Liu" ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Guo et al\\.,? 2014b",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "A representation learning framework for multisource transfer parsing",
      "author" : [ "J. Guo", "W. Che", "D. Yarowsky", "H. Wang", "T. Liu" ],
      "venue" : "AAAI.",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual Distributed Representations without Word Alignment",
      "author" : [ "K.M. Hermann", "P. Blunsom" ],
      "venue" : "ICLR.",
      "citeRegEx" : "Hermann and Blunsom,? 2014",
      "shortCiteRegEx" : "Hermann and Blunsom",
      "year" : 2014
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "M.D. Hoffman", "D.M. Blei", "C. Wang", "J.W. Paisley" ],
      "venue" : "JMLR.",
      "citeRegEx" : "Hoffman et al\\.,? 2013",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "ACL.",
      "citeRegEx" : "Huang et al\\.,? 2012",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Comparing partitions",
      "author" : [ "L. Hubert", "P. Arabie" ],
      "venue" : "Journal of classification.",
      "citeRegEx" : "Hubert and Arabie,? 1985",
      "shortCiteRegEx" : "Hubert and Arabie",
      "year" : 1985
    }, {
      "title" : "Ontologically grounded multi-sense representation learning for semantic vector space models",
      "author" : [ "S.K. Jauhar", "C. Dyer", "E. Hovy" ],
      "venue" : "NAACL.",
      "citeRegEx" : "Jauhar et al\\.,? 2015",
      "shortCiteRegEx" : "Jauhar et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to represent words in context with multilingual supervision",
      "author" : [ "K. Kawakami", "C. Dyer" ],
      "venue" : "ICLR Workshop.",
      "citeRegEx" : "Kawakami and Dyer,? 2015",
      "shortCiteRegEx" : "Kawakami and Dyer",
      "year" : 2015
    }, {
      "title" : "I don’t believe in word senses",
      "author" : [ "A. Kilgarriff" ],
      "venue" : "Computers and the Humanities.",
      "citeRegEx" : "Kilgarriff,? 1997",
      "shortCiteRegEx" : "Kilgarriff",
      "year" : 1997
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "P. Koehn" ],
      "venue" : "MT summit, volume 5, 79–86.",
      "citeRegEx" : "Koehn,? 2005",
      "shortCiteRegEx" : "Koehn",
      "year" : 2005
    }, {
      "title" : "Do multi-sense embeddings improve natural language understanding",
      "author" : [ "J. Li", "D. Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "Li and Jurafsky,? \\Q2015\\E",
      "shortCiteRegEx" : "Li and Jurafsky",
      "year" : 2015
    }, {
      "title" : "Topical word embeddings",
      "author" : [ "Y. Liu", "Z. Liu", "T.-S. Chua", "M. Sun" ],
      "venue" : "AAAI.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning context-sensitive word embeddings with neural tensor skipgram model",
      "author" : [ "P. Liu", "X. Qiu", "X. Huang" ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Bilingual word representations with monolingual quality in mind",
      "author" : [ "T. Luong", "H. Pham", "C.D. Manning" ],
      "venue" : "Workshop on Vector Space Modeling for NLP.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "T. Mikolov", "W.-t. Yih", "G. Zweig" ],
      "venue" : "NAACL.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "G.A. Miller" ],
      "venue" : "Communications of the ACM.",
      "citeRegEx" : "Miller,? 1995",
      "shortCiteRegEx" : "Miller",
      "year" : 1995
    }, {
      "title" : "A state of the art of word sense induction: A way towards word sense disambiguation for under-resourced languages",
      "author" : [ "M. Nasiruddin" ],
      "venue" : "arXiv preprint arXiv:1310.1425.",
      "citeRegEx" : "Nasiruddin,? 2013",
      "shortCiteRegEx" : "Nasiruddin",
      "year" : 2013
    }, {
      "title" : "Word sense disambiguation: A survey",
      "author" : [ "R. Navigli" ],
      "venue" : "ACM Computing Surveys (CSUR).",
      "citeRegEx" : "Navigli,? 2009",
      "shortCiteRegEx" : "Navigli",
      "year" : 2009
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum" ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Neelakantan et al\\.,? 2014",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploiting parallel texts for word sense disambiguation: An empirical study",
      "author" : [ "H.T. Ng", "B. Wang", "Y.S. Chan" ],
      "venue" : "ACL.",
      "citeRegEx" : "Ng et al\\.,? 2003",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning word representation considering proximity and ambiguity",
      "author" : [ "L. Qiu", "Y. Cao", "Z. Nie", "Y. Yu", "Y. Rui" ],
      "venue" : "AAAI.",
      "citeRegEx" : "Qiu et al\\.,? 2014",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-prototype vector-space models of word meaning",
      "author" : [ "J. Reisinger", "R.J. Mooney" ],
      "venue" : "NAACL.",
      "citeRegEx" : "Reisinger and Mooney,? 2010",
      "shortCiteRegEx" : "Reisinger and Mooney",
      "year" : 2010
    }, {
      "title" : "Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation",
      "author" : [ "P. Resnik", "D. Yarowsky" ],
      "venue" : "NLE.",
      "citeRegEx" : "Resnik and Yarowsky,? 1999",
      "shortCiteRegEx" : "Resnik and Yarowsky",
      "year" : 1999
    }, {
      "title" : "A constructive definition of dirichlet priors",
      "author" : [ "J. Sethuraman" ],
      "venue" : "Statistica sinica.",
      "citeRegEx" : "Sethuraman,? 1994",
      "shortCiteRegEx" : "Sethuraman",
      "year" : 1994
    }, {
      "title" : "A conditional random field word segmenter for sighan bakeoff 2005",
      "author" : [ "H. Tseng", "P. Chang", "G. Andrew", "D. Jurafsky", "C. Manning" ],
      "venue" : "Proc. of SIGHAN.",
      "citeRegEx" : "Tseng et al\\.,? 2005",
      "shortCiteRegEx" : "Tseng et al\\.",
      "year" : 2005
    }, {
      "title" : "Word representations: a simple and general method for semisupervised learning",
      "author" : [ "J. Turian", "L. Ratinov", "Y. Bengio" ],
      "venue" : "ACL.",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Cross-lingual models of word embeddings: An empirical comparison",
      "author" : [ "S. Upadhyay", "M. Faruqui", "C. Dyer", "D. Roth" ],
      "venue" : "ACL.",
      "citeRegEx" : "Upadhyay et al\\.,? 2016",
      "shortCiteRegEx" : "Upadhyay et al\\.",
      "year" : 2016
    }, {
      "title" : "Bilingual learning of multi-sense embeddings with discrete autoencoders",
      "author" : [ "S. Šuster", "I. Titov", "G. van Noord" ],
      "venue" : null,
      "citeRegEx" : "Šuster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Šuster et al\\.",
      "year" : 2016
    }, {
      "title" : "Sense-aaware semantic analysis: A multi-prototype word representation model using wikipedia",
      "author" : [ "Z. Wu", "C.L. Giles" ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wu and Giles,? 2015",
      "shortCiteRegEx" : "Wu and Giles",
      "year" : 2015
    }, {
      "title" : "Unsupervised word sense disambiguation rivaling supervised methods",
      "author" : [ "D. Yarowsky" ],
      "venue" : "ACL.",
      "citeRegEx" : "Yarowsky,? 1995",
      "shortCiteRegEx" : "Yarowsky",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Use of embeddings as features has been shown to benefit several NLP tasks and serve as good initializations for deep architectures ranging from dependency parsing (Bansal, Gimpel, and Livescu, 2014) to named entity recognition (Guo et al., 2014b).",
      "startOffset" : 227,
      "endOffset" : 246
    }, {
      "referenceID" : 37,
      "context" : "Several attempts (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015) have been made to infer multi-sense word representations by modeling the sense as a latent variable in a Bayesian non-parametric framework.",
      "startOffset" : 17,
      "endOffset" : 94
    }, {
      "referenceID" : 34,
      "context" : "Several attempts (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015) have been made to infer multi-sense word representations by modeling the sense as a latent variable in a Bayesian non-parametric framework.",
      "startOffset" : 17,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "Several attempts (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015) have been made to infer multi-sense word representations by modeling the sense as a latent variable in a Bayesian non-parametric framework.",
      "startOffset" : 17,
      "endOffset" : 94
    }, {
      "referenceID" : 45,
      "context" : "These approaches rely on the ”one-sense per collocation” heuristic (Yarowsky, 1995), which assumes that presence of nearby words correlate with the sense of the word of interest.",
      "startOffset" : 67,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Recently, several approaches (Guo et al., 2014a; Šuster, Titov, and van Noord, 2016) propose to learn multi-sense embeddings by exploiting the fact that different senses of the same word may be translated into different words in a foreign language (Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng, Wang, and Chan, 2003).",
      "startOffset" : 29,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : ", 2014a; Šuster, Titov, and van Noord, 2016) propose to learn multi-sense embeddings by exploiting the fact that different senses of the same word may be translated into different words in a foreign language (Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng, Wang, and Chan, 2003).",
      "startOffset" : 208,
      "endOffset" : 306
    }, {
      "referenceID" : 38,
      "context" : ", 2014a; Šuster, Titov, and van Noord, 2016) propose to learn multi-sense embeddings by exploiting the fact that different senses of the same word may be translated into different words in a foreign language (Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng, Wang, and Chan, 2003).",
      "startOffset" : 208,
      "endOffset" : 306
    }, {
      "referenceID" : 7,
      "context" : ", 2014a; Šuster, Titov, and van Noord, 2016) propose to learn multi-sense embeddings by exploiting the fact that different senses of the same word may be translated into different words in a foreign language (Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng, Wang, and Chan, 2003).",
      "startOffset" : 208,
      "endOffset" : 306
    }, {
      "referenceID" : 37,
      "context" : "Two-staged approaches (Reisinger and Mooney, 2010; Huang et al., 2012) induce multi-sense embeddings by first clustering the contexts and then using the clustering to obtain the sense vectors.",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "Two-staged approaches (Reisinger and Mooney, 2010; Huang et al., 2012) induce multi-sense embeddings by first clustering the contexts and then using the clustering to obtain the sense vectors.",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "The contexts can be topics induced using latent topic models(Liu, Qiu, and Huang, 2015; Liu et al., 2015), or Wikipedia (Wu and Giles, 2015) or coarse part-of-speech tags (Qiu et al.",
      "startOffset" : 60,
      "endOffset" : 105
    }, {
      "referenceID" : 44,
      "context" : ", 2015), or Wikipedia (Wu and Giles, 2015) or coarse part-of-speech tags (Qiu et al.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : ", 2015), or Wikipedia (Wu and Giles, 2015) or coarse part-of-speech tags (Qiu et al., 2014).",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "A more recent line of work in the two-staged category is that of retrofitting (Faruqui et al., 2015; Jauhar, Dyer, and Hovy, 2015), which aims to infuse semantic ontologies from resources like WordNet (Miller, 1995) and Framenet (Baker, Fillmore, and Lowe, 1998) into embeddings during a post-processing step.",
      "startOffset" : 78,
      "endOffset" : 130
    }, {
      "referenceID" : 31,
      "context" : ", 2015; Jauhar, Dyer, and Hovy, 2015), which aims to infuse semantic ontologies from resources like WordNet (Miller, 1995) and Framenet (Baker, Fillmore, and Lowe, 1998) into embeddings during a post-processing step.",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "While some resources like WordNet (Miller, 1995) are available for many languages, they are not exhaustive in listing all possible senses.",
      "startOffset" : 34,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "Indeed, the number senses of a word is highly dependent on the task and cannot be pre-determined using a lexicon (Kilgarriff, 1997).",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 34,
      "context" : "In contrast, joint learning approaches (Neelakantan et al., 2014; Li and Jurafsky, 2015) jointly learn the sense clusters and embeddings by using non-parametrics.",
      "startOffset" : 39,
      "endOffset" : 88
    }, {
      "referenceID" : 26,
      "context" : "In contrast, joint learning approaches (Neelakantan et al., 2014; Li and Jurafsky, 2015) jointly learn the sense clusters and embeddings by using non-parametrics.",
      "startOffset" : 39,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "The closest non-parametric approach to ours is that of (Bartunov et al., 2016), who proposed a multisense variant of the skip-gram model which learns the different number of sense vectors for all words from a large monolingual corpus (eg.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "Incorporating crosslingual distributional information is a popular technique for learning word embeddings, and improves performance on several downstream tasks (Faruqui and Dyer, 2014; Guo et al., 2016; Upadhyay et al., 2016).",
      "startOffset" : 160,
      "endOffset" : 225
    }, {
      "referenceID" : 17,
      "context" : "Incorporating crosslingual distributional information is a popular technique for learning word embeddings, and improves performance on several downstream tasks (Faruqui and Dyer, 2014; Guo et al., 2016; Upadhyay et al., 2016).",
      "startOffset" : 160,
      "endOffset" : 225
    }, {
      "referenceID" : 42,
      "context" : "Incorporating crosslingual distributional information is a popular technique for learning word embeddings, and improves performance on several downstream tasks (Faruqui and Dyer, 2014; Guo et al., 2016; Upadhyay et al., 2016).",
      "startOffset" : 160,
      "endOffset" : 225
    }, {
      "referenceID" : 15,
      "context" : "However, there has been little work on learning multi-sense embeddings using crosslingual signals (Bansal, DeNero, and Lin, 2012; Guo et al., 2014a; Šuster, Titov, and van Noord, 2016) with only (Šuster, Titov, and van Noord, 2016) being a joint approach.",
      "startOffset" : 98,
      "endOffset" : 184
    }, {
      "referenceID" : 23,
      "context" : "(Kawakami and Dyer, 2015) also used bilingual distributional signals in a deep neural architecture to learn context dependent representations for words, though they do not learn separate sense vectors.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "We place a Dirichlet process (Ferguson, 1973) prior on sense assignment for each word.",
      "startOffset" : 29,
      "endOffset" : 45
    }, {
      "referenceID" : 39,
      "context" : "(2) That is, the potentially infinite number of senses for each word x have probability determined by the sequence of independent stick-breaking weights, βxk, in the constructive definition of the DP (Sethuraman, 1994).",
      "startOffset" : 200,
      "endOffset" : 218
    }, {
      "referenceID" : 19,
      "context" : "The optimization problem solves for θ,q(z) and q(β) using the stochastic variational inference technique (Hoffman et al., 2013) similar to (Bartunov et al.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : ", 2013) similar to (Bartunov et al., 2016) (refer for details).",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "Similar to (Bartunov et al., 2016), we can disambiguate the sense for the word x given a monolingual context y as follows, P (z | x, y) ∝ P (y | x, z; θ) ∑",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "for q(β) like (Bartunov et al., 2016) 10: Update θ using eq.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "A similar reason holds for (Guo et al., 2014a), as they use a two step approach instead of joint learning.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "A similar idea of using English as a bridging language was used in the models proposed in (Hermann and Blunsom, 2014) and (Coulmance et al.",
      "startOffset" : 90,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "A similar idea of using English as a bridging language was used in the models proposed in (Hermann and Blunsom, 2014) and (Coulmance et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "To this end, we also used the En-Fr, En-Es, En-Zh and En-Ru sections of the MultiUN parallel corpus (Eisele and Chen, 2010).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 40,
      "context" : "Stanford Segmenter (Tseng et al., 2005) was used to preprocess the chinese corpora.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "In this task, we are given several sentences showing usages of the same word, and are required to cluster all sentences which use the same sense (Nasiruddin, 2013).",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 33,
      "context" : "Note that WSI is a harder task than Word Sense Disambiguation (WSD)(Navigli, 2009), as unlike WSD, this task does not involve any supervision or explicit human knowledge about senses of words.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "To allow for fair comparison with earlier work, we use the same benchmark datasets as (Bartunov et al., 2016) – Semeval-2007, 2010 and Wikipedia Word Sense Induction (WWSI).",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "We report Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) in the experiments, as ARI is a more strict and precise metric than F-score and V-measure.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "MONO refers to the AdaGram model of (Bartunov et al., 2016) trained on the English side of the parallel corpus.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "We can also compare (unfairly to FULL model) to the best results described in (Bartunov et al., 2016), which achieved ARI scores of 0.",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 25,
      "context" : "com/redpony/cdec first 100k lines from the En-Fr Europarl (Koehn, 2005)",
      "startOffset" : 58,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "For completeness, we report correlation scores on Stanford contextual word similarity dataset (SCWS) (Huang et al., 2012) in Table 2.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 38,
      "context" : "Our findings echo those of (Resnik and Yarowsky, 1999), who found that the tendency to lexicalize different senses of an English word differently in a second language correlated with language distance.",
      "startOffset" : 27,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "This also Most works tune directly on the test dataset for Word Similarity tasks (Faruqui et al., 2016) 6 (Šuster, Titov, and van Noord, 2016) compared different languages but did not control for domain.",
      "startOffset" : 81,
      "endOffset" : 103
    } ],
    "year" : 2016,
    "abstractText" : "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multiview Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art monolingual model trained on five times more training data.",
    "creator" : "LaTeX with hyperref package"
  }
}