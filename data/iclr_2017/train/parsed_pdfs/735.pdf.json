{
  "name" : "735.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SOLVING INVERSE PROBLEMS",
    "authors" : [ "Ravi Garg", "Anders Eriksson" ],
    "emails" : [ "ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Dimensionality reduction techniques are widely used in data modeling, visualization and unsupervised learning. Principal component analysis (PCAJolliffe (2002)), Kernel PCA (KPCASchölkopf et al. (1998)) and Latent Variable Models (LVMsLawrence (2005)) are some of the well known techniques used to create low dimensional representations of the given data while preserving its significant information.\nOne key deployment of low-dimensional modeling occurs in solving ill-posed inference problems. Assuming the valid solutions to the problem lie near a low-dimensional manifold (i.e. can be parametrized with a reduced set of variables) allows for a tractable inference for otherwise underconstrained problems. After the seminal work of Candès & Recht (2009); Recht et al. (2010) on guaranteed rank minimization of the matrix via trace norm heuristics Fazel (2002), many ill-posed computer vision problems have been tackled by using the trace norm — a convex surrogate of the rank function — as a regularization term in an energy minimization frameworkCandès & Recht (2009); Zhou et al. (2014). The flexible and easy integration of low-rank priors is one of key factors for versatility and success of many algorithms. For example, pre-trained active appearance models Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature tracking Poling et al. (2014), dense registration Garg et al. (2013b) and vivid reconstructions of natural videos Garg et al. (2013a) with no a priori knowledge of the scene. Various bilinear factorization problems like background modeling, structure from motion or photometric stereo are also addressed with a variational formulation of the trace norm regularization Cabral et al. (2013).\nOn the other hand, although many non-linear dimensionality reduction techniques — in particular KPCA — have been shown to outperform their linear counterparts for many data modeling tasks, they are seldom used to solve inverse problems without using a training phase. A general (discriminative) framework for using non-linear dimensionality reduction is: (i) learn a low-dimensional representation for the data using training examples via the kernel trick (ii) project the test examples on the learned manifold and finally (iii) find a data point (pre-image) corresponding to each projection in the input space.\nThis setup has two major disadvantages. Firstly, many problems of interest come with corrupted observations — noise, missing data and outliers — which violate the low-dimensional modeling assumption.Secondly, computing the pre-image of any point in the low dimensional feature subspace is non-trivial: the pre-image for many points in the low dimensional space might not even exist because the non linear feature mapping function used for mapping the data from input space to the feature space is non-surjective.\nPreviously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009).\nGenerative models like LVMs Lawrence (2005) are often used for inference by searching the lowdimensional latent space for a location which maximizes the likelihood of the observations. Problems like segmentation, tracking and semantic 3D reconstruction Prisacariu & Reid (2011); Dame et al. (2013) greatly benefit from using LVM. However, the latent space is learned a priori with clean training data in all these approaches.\nAlmost all non-linear dimensionality reduction techniques are non-trivial to generalize for solving ill-posed problems (See section 4.2) without a pre-training stage. Badly under-constrained problems require the low-dimensional constraints even for finding an initial solution, eliminating applicability of the standard “projection + pre-image estimation” paradigm. This hinders the utility of nonlinear dimensionality reduction and a suitable regularization technique to penalize the non-linear dimensionality is desirable.\nSum and Substance: A closer look at most non-linear dimensionality reduction techniques reveals that they rely upon a non-linear mapping function which maps the data from input space to a (usually) higher dimensional feature space. In this feature space the data is assumed to lie on a low-dimensional hyperplane — thus, linear low-rank prior is apt in the feature space. Armed with this simple observation, our aim is to focus on incorporating the advances made in linear dimensionality reduction techniques to their non-linear counterparts, while addressing the problems described above. Figure 1 explains this central idea and proposed dimensionality regularizer in a nutshell with Non Rigid Structure from Motion (NRSfM) as the example application.\nOur Contribution: In this work we propose a unified for simultaneous robust KPCA and preimage estimation while solving an ill-posed inference problem without a pre-training stage.\nIn particular we propose a novel robust energy minimization algorithm which handles the implicitness of the feature space to directly penalize its rank by iteratively: (i) creating robust low-dimensional representation for the\ndata given the kernel matrix in closed form and (ii) reconstructing the noise-free version of the data (pre-image of the features space projections) using the estimated low-dimensional representations in a unified framework.\nThe proposed algorithm: (i) provides a novel closed form solution to robust KPCA; (ii) yields stateof-the-art results on missing data prediction for the well-known oil flow dataset; (iii) outperforms state-of-the-art linear dimensionality (rank) regularizers to solve NRSfM; and (iv) can be trivially generalized to incorporate other cost functions in an energy minimization framework to solve various ill-posed inference problems."
    }, {
      "heading" : "2 PROBLEM FORMULATION",
      "text" : "This paper focuses on solving a generic inverse problem of recovering causal factor S = [s1, s2, · · · sN ] ∈ X × N from N observations W = [w1, w2, · · · wN ] ∈ Y × N such that f(W,S) = 0. Here function f(observation,variable), is a generic loss function which aligns the observations W with the variable S (possibly via other causal factors. e.g. R or Z in Section 4.1 and 4.2).\nIf, f(W,S) = 0 is ill-conditioned (for example when Y X ), we want to recover matrix S under the assumption that the columns of it lie near a low-dimensional non-linear manifold. This can be done by solving a constrained optimization problem of the following form:\nmin S\nrank(Φ(S))\ns.t. f(W,S) ≤ (1)\nwhere Φ(S) = [φ(s1), φ(s2), · · · , φ(sN )] ∈ H ×N is the non-linear mapping of matrix S from the input space X to the feature space H (also commonly referred as Reproducing Kernel Hilbert Space), via a non-linear mapping function φ : X → H associated with a Mercer kernel K such that K(S)i,j = φ(si) Tφ(sj).\nIn this paper we present a novel energy minimization framework to solve problems of the general form (1).\nAs our first contribution, we relax the problem (1) by using the trace norm of Φ(S) — the convex surrogate of rank function — as a penalization function. The trace norm ‖M‖∗ =: ∑ i λi(M) of a matrix M is the sum of its eigenvalues λi(M) and was proposed as a tight convex relaxation1 of the rank(M) and is used in many vision problems as a rank regularizer Fazel (2002). Although the rank minimization via trace norm relaxation does not lead to a convex problem in presence of a non-linear kernel function, we show in 3.2 that it leads to a closed-form solution to denoising a kernel matrix via penalizing the rank of recovered data (S) directly in the feature space.\nWith these changes we can rewrite (1) as:\nmin S\nf(W,S) + τ‖Φ(S)‖∗ (2)\nwhere τ is a regularization strength.2\nIt is important to notice that although the rank of the kernel matrix K(S) is equal to the rank of Φ(S), ‖K(S)‖∗ is merely ‖Φ(S)‖2F . Thus, directly penalizing the sum of the singular values of K(S) will not encourage low-rank in the feature space.3\nAlthough we have relaxed the non-convex rank function, (2) is in general difficult to minimize due to the implicitness of the feature space. Most widely used kernel functions like RBF do not have a explicit definition of the function φ. Moreover, the feature space for many kernels is high(possibly infinite-) dimensional, leading to intractability. These issues are identified as the main\n1More precisely, ‖M‖∗ was shown to be the tight convex envelope of rank(M)/‖M‖s, where ‖M‖s represent spectral norm of M .\n21/τ can also be viewed as Lagrange multiplier to the constraints in (1). 3Although it is clear that relaxing the rank of kernel matrix to ‖K(S)‖∗ is suboptimal, works like Huang et al. (2012); Cabral et al. (2013) with a variational definition of nuclear norm, allude to the possibility of kernelization. Further investigation is required to compare this counterpart to our tighter relaxation.\nbarriers to robust KPCA and pre-image estimation Nguyen & De la Torre (2009). Thus, we have to reformulate (2) by applying kernel trick where the cost function (2) can be expressed in terms of the kernel function alone.\nThe key insight here is that under the assumption that kernel matrix K(S) is positive semidefinite, we can factorize it as: K(S) = CTC. Although, this factorization is non-unique, it is trivial to show the following: √\nλi(K(S)) = λi(C) = λi(Φ(S))\nThus: ‖C‖∗ = ‖Φ(S)‖∗ ∀ C : CTC = K(S) (3) where λi(.) is the function mapping the input matrix to its ith largest eigenvalue.\nThe row space of matrix C in (3) can be seen to span the eigenvectors associated with the kernel matrix K(S) — hence the principal components of the non-linear manifold we want to estimate.\nUsing (3), problem (2) can finally be written as:\nmin S,C\nf(W,S) + τ‖C‖∗\ns.t. K(S) = CTC (4)\nThe above minimization can be solved with a soft relaxation of the manifold constraint by assuming that the columns of S lie near the non-linear manifold.\nmin S,C\nf(W,S) + ρ\n2 ‖K(S)− CTC‖2F + τ‖C‖∗ (5)\nAs ρ → ∞, the optimum of (5) approaches the optimum of (4) . A local optimum of (4) can be achieved using the penalty method of Nocedal & Wright (2006) by optimizing (5) while iteratively increasing ρ as explained in Section 3.\nBefore moving on, we would like to discuss some alternative interpretations of (5) and its relationship to previous work – in particular LVMs. Intuitively, we can also interpret (5) from the probabilistic viewpoint as commonly used in latent variable model based approaches to define kernel function Lawrence (2005). For example a RBF kernel with additive Gaussian noise and inverse width γ can be defined as: K(S)i,j = e−γ‖si−sj‖ 2\n+ , where ∼ N (0, σ). In other words, with a finite ρ, our model allows the data points to lie near a non-linear low-rank manifold instead of on it. Its worth noting here that like LVMs, our energy formulation also attempts to maximize the likelihood of regenerating the training data W , (by choosing f(W,S) to be a simple least squares cost) while doing dimensionality reduction.\nNote that in closely related work Geiger et al. (2009), continuous rank penalization (with a logarithmic prior) has also been used for robust probabilistic non-linear dimensionality reduction and model selection in LVM framework. However, unlike Geiger et al. (2009); Lawrence (2005) where the non-linearities are modeled in latent space (of predefined dimensionality), our approach directly penalizes the non-linear dimensionality of data in a KPCA framework and is applicable to solve inverse problems without pre-training."
    }, {
      "heading" : "3 OPTIMIZATION",
      "text" : "We approach the optimization of (5) by solving the following two sub-problems in alternation:\nmin S\nf(W,S) + ρ\n2 ‖K(S)− CTC‖2F (6)\nmin C\nτ‖C‖∗ + ρ\n2 ‖K(S)− CTC‖2F (7)\nAlgorithm 1 outlines the approach and we give a detailed description and interpretations of both sub-problems (7) and (6) in next two sections of the paper."
    }, {
      "heading" : "3.1 PRE-IMAGE ESTIMATION TO SOLVE INVERSE PROBLEM.",
      "text" : "Subproblem (6) can be seen as a generalized pre-image estimation problem: we seek the factor si, which is the pre-image of the projection of φ(si) onto the principle subspace of the RKHS stored in\nAlgorithm 1: Inference with Proposed Regularizer. Input: Initial estimate S0 of S. Output: Low-dimensional S and kernel representation C. Parameters: Initial ρ0 and maximum ρmax penalty, with scale ρs. - S = S0, ρ = ρ0 ; while ρ ≤ ρmax do\nwhile not converged do - Fix S and estimate C via closed-form solution of (7) using Algorithm 2; - Fix C and minimize (6) to update S using LM algorithm;\n- ρ = ρρs ;\nCTC, which best explains the observation wi. Here (6) is generally a non-convex problem, unless the Mercer-kernel is linear, and must therefore be solved using non-linear optimization techniques. In this work, we use the Levenberg-Marquardt algorithm for optimizing (6).\nNotice that (6) only computes the pre-image for the feature space projections of the data points with which the non-linear manifold (matrix C) is learned. An extension to our formulation is desirable if one wants to use the learned non-linear manifold for denoising test data in a classic pre-image estimation framework. Although a valuable direction to pursue, it is out of scope of the present paper."
    }, {
      "heading" : "3.2 ROBUST DIMENSIONALITY REDUCTION",
      "text" : "Algorithm 2: Robust Dimensionality Reduction. Input: Current estimate of S. Output: Low-dimensional representation C. Parameters: Current ρ and regularization strength τ .\n- [U Λ UT ] = Singular Value Decomposition of K(S); // Λ is a diagonal matrix, storing N\nsingular values λi of K(S). for i = 1 to N do\n- Find three solutions (lr : r ∈ {1, 2, 3}) of: l3 − lλi + τ\n2ρ = 0 ;\n- set l4 = 0; - lr = max(lr, 0) ∀r ∈ {1, 2, 3, 4} ; - r = argmin\nr { ρ 2 ‖λi − l2r‖2 + τ lr } ;\n- λ̄i = lr ;\n- C = Λ̄UT ; // Λ̄ is diagonal matrix storing λ̄i.\nOne can interpret sub-problem (7) as a robust form of KPCA where the kernel matrix has been corrupted with Gaussian noise and we want to generate its low-rank approximation. Although (7) is non-convex we can solve it in closed-form via singular value decomposition. This closed-form solution is outlined in Algorithm 2 and is based on the following theorem: Theorem 1. With Sn 3 A 0 let A = UΣUT denote its singular value decomposition. Then\nmin L\nρ 2 ||A− LTL||2F + τ ||L||∗ (8)\n= n∑ i=1 (ρ 2 (σi − γ∗2i )2 + τγ∗i ) . (9)\nA minimizer L∗ of (8) is given by\nL∗ = Γ∗UT (10) with Γ∗ ∈ Dn+, γ∗i ∈ {α ∈ R+ | pσi,τ/2ρ(α) = 0} ⋃ {0}, where pa,b denotes the depressed cubic pa,b(x) = x 3 − ax+ b. Dn+ is the set of n-by-n diagonal matrices with non-negative entries.\nTheorem 1 shows that each eigenvalue of the minimizer C∗ of (7) can be obtained by solving a depressed cubic whose coefficients are determined by the corresponding eigenvalue of the kernel matrix and the regularization strength τ . The roots of each cubic, together with zero, comprise a set of candidates for the corresponding eigenvalue of C∗. The best one from this set is obtained by choosing the value which minimizes (9) (see Algorithm 2).\nAs elaborated in Section 2, problem (7) can be seen as regularizing sum of square root (L1/2 norm) of the eigenvalues of the matrix K(S). In a closely related work Zongben et al. (2012), authors advocateL1/2 norm as a better approximation for the cardinality of a vector then the more commonly used L1 norm. A closed form solution for L1/2 regularization similar to our work was outlined in Zongben et al. (2012) and was shown to outperform the L1 vector norm regularization for sparse coding. To that end, our Theorem 1 and the proposed closed form solution (Algo 2) for (7) can\nbe seen as generalization of Zongben et al. (2012) to include the L1/2 matrix norms for which a simplified proof is included in the Appendix A. It is important to note however, that the motivation and implication of using L1/2 regularization in the context of non-linear dimensionality reduction are significantly different to that of Zongben et al. (2012) and related work Du et al. (2013); Zhao et al. (2014) which are designed for linear modeling of the causal factors. The core insight of using L1 regularization in the feature space via the parametrization given in 3 facilitates a natural way for non-linear modeling of causal factors with low dimensionality while solving an inverse problem by making feature space tractable."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section we demonstrate the utility of the proposed algorithm. The aims of our experiments are twofold: (i) to compare our dimensionality reduction technique favorably with KPCA and its robust variants; and (ii) to demonstrate that the proposed non-linear dimensionality regularizer consistently outperforms its linear counterpart (a.k.a. nuclear norm) in solving inverse problems."
    }, {
      "heading" : "4.1 MATRIX COMPLETION",
      "text" : "The nuclear norm has been introduced as a low rank prior originally for solving the matrix completion problem. Thus, it is natural to evaluate its non-linear extensions on the same task. Assuming W ∈ Rm×n to be the input matrix and Z a binary matrix specifying the availability of the observations in W , Algorithm 1 can be used for recovering a complete matrix S with the following choice of f(W,Z, S): f(W,Z, S) = ‖Z ◦ (W − S)‖2F (11) where ◦ represents Hadamard product. To demonstrate the robustness of our algorithm for matrix completion problem, we choose 100 training samples from the oil flow dataset described in section 3.2 and randomly remove the elements from the data with varying range of probabilities to test the performance of the proposed algorithm against various baselines. Following the experimental setup as specified in Sanguinetti & Lawrence (2006), we repeat the experiments with 50 different samples of Z. We report the mean and standard deviation of the root mean square reconstruction error for our method with the choice of τ = 0.1, alongside five different methods in Table 1. Our method significantly improves the performance of missing data completion compared to other robust extensions of KPCA Tipping & Bishop (1999); Sanguinetti & Lawrence (2006); Nguyen & De la Torre (2009), for every probability of missing data.\nAlthough we restrict our experiments to least-squares cost functions, it is vital to restate here that our framework could trivially incorporate robust functions like the L1 norm instead of the Frobenius norm — as a robust data term f(W,Z, S) — to generalize algorithms like Robust PCA Wright et al. (2009) to their non-linear counterparts.\n4.2 KERNEL NON-RIGID STRUCTURE FROM MOTION\nNon-rigid structure from motion under orthography is an ill-posed problem where the goal is to estimate the camera locations and 3D structure of a deformable objects from a collection of 2D images which are labeled with landmark correspondences Bregler et al. (2000). Assuming si(xj) ∈ R3 to be the 3D location of point xj on the deformable object in the ith image, its orthographic projection wi(xj) ∈ R2 can be written as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix Bregler et al. (2000). Notice that as the object deforms, even with given camera poses, reconstructing the sequence by least-squares reprojection error minimization is an ill-posed problem. In their seminal work, Bregler et al. (2000) proposed to solve this problem with an additional assumption that the reconstructed shapes lie on a low-dimensional linear subspace and can be parameterized as linear combinations of a relatively low number of basis shapes. NRSfM was then cast as the low-rank factorization problem of estimating these basis shapes and corresponding coefficients.\nRecent work, like Dai et al. (2014); Garg et al. (2013a) have shown that the trace norm regularizer can be used as a convex envelope of the low-rank prior to robustly address ill-posed nature of the problem. A good solution to NRSfM can be achieved by optimizing:\nmin S,R τ‖S‖∗ + F∑ i=1 N∑ j=1 Zi(xj)‖wi(xj)−Risi(xj)‖2F (12)\nwhere S is the shape matrix whose columns are 3N dimensional vectors storing the 3D coordinates Si(xj) of the shapes and Zi(xj) is a binary variable indicating if projection of point xj is available in the image i.\nAssuming the projection matrices to be fixed, this problem is convex and can be exactly solved with standard convex optimization methods. Additionally, if the 2D projections wi(xj) are noise free, optimizing (12) with very small τ corresponds to selecting the the solution — out of the many solutions — with (almost) zero projection error, which has minimum trace norm Dai et al. (2014). Thus henceforth, optimization of (12) is referred as the trace norm heuristics (TNH). We solve this problem with a first order primal-dual variant of the algorithm given in Garg et al. (2013a), which can handle missing data. The algorithm is detailed and compared favorably with the state of the art NRSfM approaches (based on linear dimensionality regularization) Appendix C.\nA simple kernel extension of the above optimization problem is:\nmin S,R τ‖Φ(S)‖∗ + F∑ i=1 N∑ j=1\nZi(xj)‖wi(xj)−Risi(xj)‖2F︸ ︷︷ ︸ f(W,Z,R,S)\n(13)\nwhere Φ(S) is the non-linear mapping of S to the feature space using an RBF kernel.\nWith fixed projection matrices R, (13) is of the general form (2), for which the local optima can be found using Algorithm 1."
    }, {
      "heading" : "4.2.1 RESULTS ON THE CMU DATASET",
      "text" : "We use a sub-sampled version of CMU mocap dataset by selecting every 10th frame of the smoothly deforming human body consisting 41 mocap points used in Dai et al. (2014).4\nIn our experiments we use ground truth camera projection matrices to compare our algorithm against TNH. The advantage of this setup is that with ground-truth rotation and no noise, we can avoid the model selection (finding optimal regularization strength τ ) by setting it low enough. We run the TNH with τ = 10−7 and use this reconstruction as initialization for Algorithm 1. For the proposed method, we set τ = 10−4 and use following RBF kernel width selection approach:\n• Maximum distance criterion (dmax): we set the maximum distance in the feature space to be 3σ. Thus, the kernel matrix entry corresponding to the shape pairs obtained by TNH with maximum Euclidean distance becomes e−9/2.\n• Median distance criterion (dmed): the kernel matrix entry corresponding to the median euclidean distance is set to 0.5.\nFollowing the standard protocol in Dai et al. (2014); Akhter et al. (2009), we quantify the reconstruction results with normalized mean 3D errors e3D = 1σFN ∑ i ∑ j eij , where eij is the euclidean distance of a reconstructed point j in frame i from the ground truth, σ is the mean of standard deviation for 3 coordinates for the ground truth 3D structures, and F,N are number of input images and number of points reconstructed.\nTable 2 shows the results of the TNH and non-linear dimensionality regularization based methods using the experimental setup explained above, both without missing data and after randomly removing 50% of the image measurements. Our method consistently beats the TNH baseline and improves the mean reconstruction error by ∼ 40% with full data and by ∼ 25% when used with 50% missing data. Figure 2 shows qualitative comparison of the obtained 3D reconstruction using TNH and proposed non-lienar dimensionality regularization technique for some sample frames from various sequences. We refer readers to Appendix B for results with simultaneous reconstruction pose optimization."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper we have introduced a novel non-linear dimensionality regularizer which can be incorporated into an energy minimization framework, while solving an inverse problem. The proposed algorithm for penalizing the rank of the data in the feature space has been shown to be robust to noise and missing observations. We have picked NRSfM as an application to substantiate our arguments and have shown that despite missing data and model noise (such as erroneous camera poses) our algorithm significantly outperforms state-of-the-art linear counterparts.\nAlthough our algorithm currently uses slow solvers such as the penalty method and is not directly scalable to very large problems like dense non-rigid reconstruction, we are actively considering alternatives to overcome these limitations. An extension to estimate pre-images with a problem-\n4 Since our main goal is to validate the usefulness of the proposed non-linear dimensionality regularizer, we opt for a reduced size dataset for more rapid and flexible evaluation.\nspecific loss function is possible, and this will be useful for online inference with pre-learned lowdimensional manifolds.\nGiven the success of non-linear dimensionality reduction in modeling real data and overwhelming use of the linear dimensionality regularizers in solving real world problems, we expect that proposed non-linear dimensionality regularizer will be applicable to a wide variety of unsupervised inference problems: recommender systems; 3D reconstruction; denoising; shape prior based object segmentation; and tracking are all possible applications."
    }, {
      "heading" : "A PROOF OF THEOREM 3.1",
      "text" : "Proof. We will prove theorem 1 by first establishing a lower bound for (8) and subsequently showing that this lower bound is obtained at L∗ given by (10). The rotational invariance of the entering norms allows us to write (8) as:\nmin Γ∈Dn, WTW=I\nρ 2 ||Σ−WΓ2WT ||2F + τ ||Γ||∗. (14)\nExpanding (14) we obtain\nρ 2 min Γ,W\ntr ( Σ2 ) − 2 tr ( ΣWΓ2WT ) + tr ( Γ4 ) + 2τ\nρ n∑ i=1 γi (15)\n= ρ\n2 min Γ,W n∑ i=1 ( σ2i + γ 4 i + 2τ ρ γi ) − 2 n∑ i=1 n∑ j=1 w2ijγ 2 j σi (16)\n≥ ρ 2 min Γ n∑ i=1 ( σ2i − 2γ2i σi + γ4i + 2τ ρ γi ) (17)\n= ρ\n2 n∑ i=1 min γi≥0 ( σ2i − 2γ2i σi + γ4i + 2τ ρ γi ) (18)\nThe inequality in (17) follows directly by applying Hölder’s inequality to (16) and using the property that the column vectors wi are unitary.\nNext, with L = ΓUT in (8) we have ρ\n2 ||A− LTL||2F+τ ||L||∗ =\nρ 2 ||Σ− Γ2||2F + τ ||Γ||∗\n= n∑ i=1 (ρ 2 (σi − γ2i )2 + τγi ) . (19)\nFinally, since the subproblems in (18) are separable in γi, its minimizer must be KKT-points of the individual subproblems. As the constraints are simple non-negativity constraints, these KKT points are either (positive) stationary points of the objective functions or 0. It is simple to verify that the stationary points are given by the roots of the cubic function pσi,τ/2ρ. Hence it follows that there exists a γ ∗ i such that\nρ\n2\n( σ2i − 2γ2i σi + γ4i + 2τ\nρ γi\n) ≥ ρ\n2 (σi − γ∗2i )2 + τγ∗i , (20)\n∀γi ≥ 0, which completes the proof.\nA.1 VALIDATING THE CLOSED FORM SOLUTION\nGiven the relaxations proposed in Section 2, our assertion that the novel trace regularization based non-linear dimensionality reduction is robust need to be substantiated. To that end, we evaluate our closed-form solution of Algorithm 2 on the standard oil flow dataset introduced in Bishop & James (1993).\nThis dataset comprises 1000 training and 1000 testing data samples, each of which is of 12 dimensions and categorized into one of three different classes. We add zero mean Gaussian noise with variance σ to the training data5 and recover the low-dimensional manifold for this noisy training data Sσ with KPCA and contrast this with the results from Algorithm 2. An inverse width of the Gaussian kernel γ = 0.075 is used for all the experiments on the oil flow dataset.\nIt is important to note that in this experiment, we only estimate the principal components (and their variances) that explain the estimated non-linear manifold, i.e. matrix C by Algorithm 2, without reconstructing the denoised version of the corrupted data samples.\nBoth KPCA and our solution require model selection (choice of rank and τ respectively) which is beyond the scope of this paper. Here we resort to evaluate the performance of both methods under different parameters settings. To quantify the accuracy of the recovered manifold (C) we use following criteria:\n5Note that our formulation assumes Gaussian noise in K(S) where as for this evaluation we add noise to S directly."
    }, {
      "heading" : "B KERNEL NRSFM WITH CAMERA POSE ESTIMATION",
      "text" : "Extended from section 4.2\nTable 4 shows the reconstruction performance on a more realistic experimental setup, with the modification that the camera projection matrices are initialized with rigid factorization and were refined with the shapes by optimizing (2). To solve NRSfM problem with unknown projection matrices, we parameterize each Ri with quaternions and alternate between refining the 3D shapes S and projection matrices R using LM. The regularization strength τ was selected for the TNH method by golden section search and parabolic interpolation for every test case independently. This ensures the best possible performance for the baseline. For our proposed approach τ was kept to 10−4 for all sequences for both missing data and full data NRSfM. This experimental protocol somewhat disadvantages the non-linear method, since its performance can be further improved by a judicious choice of the regularization strength.\n6Errors from non-noisy kernel matrix can be replaced by cross validating the entries of the kernel matrix for model selection for more realistic experiment.\nHowever our purpose is primarily to show that the non-linear method adds value even without timeconsuming per-sequence tuning. To that end, note that despite large errors in the camera pose estimations by TNH and 50% missing measurements, the proposed method shows significant (∼ 10%) improvements in terms of reconstruction errors, proving our broader claims that non-linear representations are better suited for modeling real data, and that our robust dimensionality regularizer can improve inference for ill-posed problems.\nAs suggested by Dai et al. (2014), robust camera pose initialization is beneficial for the structure estimation. We have used rigid factorization for initializing camera poses here but this can be trivially changed. We hope that further improvements can be made by choosing better kernel functions, with cross validation based model selection (value of τ ) and with a more appropriate tuning of kernel width. Selecting a suitable kernel and its parameters is crucial for success of kernelized algorithms. It becomes more challenging when no training data is available. We hope to explore other kernel functions and parameter selection criteria in our future work.\nWe would also like to contrast our work with Gotardo & Martinez (2011a), which is the only work we are aware of where non-linear dimensionality reduction is attempted for NRSfM. While estimating the shapes lying on a two dimensional non-linear manifold, Gotardo & Martinez (2011a) additionally assumes smooth 3D trajectories (parametrized with a low frequency DCT basis) and a pre-defined hard linear rank constraint on 3D shapes. The method relies on sparse approximation of the kernel matrix as a proxy for dimensionality reduction. The reported results were hard to replicate under our experimental setup for a fair comparison due to non-smooth deformations. However, in contrast to Gotardo & Martinez (2011a), our algorithm is applicable in a more general setup, can be modified to incorporate smoothness priors and robust data terms but more importantly, is flexible to integrate with a wide range of energy minimization formulations leading to a larger applicability beyond NRSfM."
    }, {
      "heading" : "C TNH ALGORITHM FOR NRSFM",
      "text" : "In section 4.2, we have compared the proposed non-linear dimensionality reduction prior against a variant of Garg et al. (2013a) which handles missing data by optimizing:\nmin S,R τ‖S‖∗ + F∑ i=1 N∑ j=1 Zi(xj)‖wi(xj)−Risi(xj)‖2 (21)\nThis problem is convex in S given noise free projection matrix Ri’s but non-differentiable. To optimize (21), we first rewrite it in its primal-dual form by dualizing the trace norm 7 :\nmax Q min S,R τ < S,Q > + F∑ i=1 N∑ j=1 Zi(xj)‖wi(xj)−Risi(xj)‖2\ns.t.‖Q‖s ≤ 1 (22)\nwhere Q ∈ RX×N stores the dual variables to S and ‖.‖s represent spectral norm (highest eigenvalue) of a matrix.\n7For more details on primal dual formulation and dual norm of the trace norm see Rockafellar (1974); Recht et al. (2010); Chambolle & Pock (2011).\nAlgorithm 3: Trace norm Heuristics. Input: Initial estimates S0, R0 of S and R. Output: Low-dimensional S and camera poses R. Parameters: Regularization strength τ , measurements W and binary mask Z.\nDataset PTAAkhter et al. (2009) CSF2Gotardo & Martinez (2011b) BMMDai et al. (2014) TNH Drink 0.0229 0.0215 0.0238 0.0237 Pick-up 0.0992 0.0814 0.0497 0.0482 Yoga 0.0580 0.0371 0.0334 0.0333 Stretch 0.0822 0.0442 0.0456 0.0431\nWe choose quaternions to perametrize the 2 × 3 camera matrices Ri to satisfy orthonormality constraints as done in Garg et al. (2013a) and optimize the saddle point problem (22) using alternation. In particular, for a single iteration: (i) we optimize the camera poses Ri’s using LM, (ii) take a steepest descend step for updating S and (ii) a steepest ascend step for updating Q which is followed by projecting its spectral norm to unit ball. Given ground truth camera matrices ( without step (i)), alternation (ii-iii) can be shown to reach global minima of (22). Algorithm 3 outlines TNH algorithm.\nAs the main manuscript uses NRSfM only as a practical application of our non-linear dimensionality reduction prior, we have restricted our NRSfM experiments to only compare the proposed method against its linear counterpart. For the timely evaluation, the reported experiments we conducted on sub-sampled CMU mocap dataset. Here, we supplement the arguments presented in the main manuscript by favorably comparing the linear dimensionality reduction based NRSfM algorithm(TNH) to other NRSfM methods on full length CMU mocap sequences."
    } ],
    "references" : [ {
      "title" : "Input space regularization stabilizes pre-images for kernel pca de-noising",
      "author" : [ "Trine Julie Abrahamsen", "Lars Kai Hansen" ],
      "venue" : "In EEE International Workshop on Machine Learning for Signal Processing,",
      "citeRegEx" : "Abrahamsen and Hansen.,? \\Q2009\\E",
      "shortCiteRegEx" : "Abrahamsen and Hansen.",
      "year" : 2009
    }, {
      "title" : "Nonrigid structure from motion in trajectory space. In Advances in neural information processing",
      "author" : [ "Ijaz Akhter", "Yaser Sheikh", "Sohaib Khan", "Takeo Kanade" ],
      "venue" : null,
      "citeRegEx" : "Akhter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Akhter et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning to find pre-images",
      "author" : [ "Gokhan H Bakir", "Jason Weston", "Bernhard Schölkopf" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Bakir et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bakir et al\\.",
      "year" : 2004
    }, {
      "title" : "Analysis of multiphase flows using dual-energy gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers",
      "author" : [ "Christopher M Bishop", "Gwilym D James" ],
      "venue" : "Detectors and Associated Equipment,",
      "citeRegEx" : "Bishop and James.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bishop and James.",
      "year" : 1993
    }, {
      "title" : "A morphable model for the synthesis of 3d faces",
      "author" : [ "Volker Blanz", "Thomas Vetter" ],
      "venue" : "In 26th annual conference on Computer graphics and interactive techniques,",
      "citeRegEx" : "Blanz and Vetter.,? \\Q1999\\E",
      "shortCiteRegEx" : "Blanz and Vetter.",
      "year" : 1999
    }, {
      "title" : "Recovering non-rigid 3d shape from image streams",
      "author" : [ "Christoph Bregler", "Aaron Hertzmann", "Henning Biermann" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Bregler et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Bregler et al\\.",
      "year" : 2000
    }, {
      "title" : "Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition",
      "author" : [ "R. Cabral", "F. De la Torre", "J.P. Costeira", "A. Bernardino" ],
      "venue" : "In International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Cabral et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cabral et al\\.",
      "year" : 2013
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "Antonin Chambolle", "Thomas Pock" ],
      "venue" : "Journal of Mathematical Imaging and Vision,",
      "citeRegEx" : "Chambolle and Pock.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chambolle and Pock.",
      "year" : 2011
    }, {
      "title" : "Active appearance models",
      "author" : [ "Timothy F Cootes", "Gareth J Edwards", "Christopher J Taylor" ],
      "venue" : "IEEE Transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Cootes et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Cootes et al\\.",
      "year" : 2001
    }, {
      "title" : "A simple prior-free method for non-rigid structurefrom-motion factorization",
      "author" : [ "Yuchao Dai", "Hongdong Li", "Mingyi He" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Dai et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2014
    }, {
      "title" : "Dense reconstruction using 3d object shape priors",
      "author" : [ "Amaury Dame", "Victor Adrian Prisacariu", "Carl Yuheng Ren", "Ian Reid" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Dame et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dame et al\\.",
      "year" : 2013
    }, {
      "title" : "1/2-based iterative matrix completion for data transmission in lossy environment",
      "author" : [ "Rong Du", "Cailian Chen", "Zhiyi Zhou", "Xinping Guan. L" ],
      "venue" : "In Computer Communications Workshops (INFOCOM WKSHPS),",
      "citeRegEx" : "Du et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2013
    }, {
      "title" : "Matrix rank minimization with applications",
      "author" : [ "Maryam Fazel" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "Fazel.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fazel.",
      "year" : 2002
    }, {
      "title" : "Dense variational reconstruction of non-rigid surfaces from monocular video",
      "author" : [ "Ravi Garg", "Anastasios Roussos", "Lourdes Agapito" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Garg et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2013
    }, {
      "title" : "A variational approach to video registration with subspace constraints",
      "author" : [ "Ravi Garg", "Anastasios Roussos", "Lourdes Agapito" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "Garg et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2013
    }, {
      "title" : "Rank priors for continuous non-linear dimensionality reduction",
      "author" : [ "Andreas Geiger", "Raquel Urtasun", "Trevor Darrell" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Geiger et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2009
    }, {
      "title" : "Kernel non-rigid structure from motion",
      "author" : [ "Paulo FU Gotardo", "Aleix M Martinez" ],
      "venue" : "In IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Gotardo and Martinez.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gotardo and Martinez.",
      "year" : 2011
    }, {
      "title" : "Non-rigid structure from motion with complementary rank-3 spaces",
      "author" : [ "Paulo FU Gotardo", "Aleix M Martinez" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Gotardo and Martinez.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gotardo and Martinez.",
      "year" : 2011
    }, {
      "title" : "Robust regression",
      "author" : [ "Dong Huang", "Ricardo Silveira Cabral", "Fernando De la Torre" ],
      "venue" : "In European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Huang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Principal component analysis",
      "author" : [ "Ian Jolliffe" ],
      "venue" : "Wiley Online Library,",
      "citeRegEx" : "Jolliffe.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jolliffe.",
      "year" : 2002
    }, {
      "title" : "The pre-image problem in kernel methods",
      "author" : [ "JT-Y Kwok", "Ivor W Tsang" ],
      "venue" : "IEEE Transactions on Neural Networks,,",
      "citeRegEx" : "Kwok and Tsang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kwok and Tsang.",
      "year" : 2004
    }, {
      "title" : "Probabilistic non-linear principal component analysis with gaussian process latent variable models",
      "author" : [ "Neil D Lawrence" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Lawrence.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lawrence.",
      "year" : 2005
    }, {
      "title" : "Kernel pca and de-noising in feature spaces",
      "author" : [ "Sebastian Mika", "Bernhard Schölkopf", "Alex J Smola", "Klaus-Robert Müller", "Matthias Scholz", "Gunnar Rätsch" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Mika et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Mika et al\\.",
      "year" : 1998
    }, {
      "title" : "Robust kernel principal component analysis",
      "author" : [ "Minh Hoai Nguyen", "Fernando De la Torre" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Nguyen and Torre.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nguyen and Torre.",
      "year" : 2009
    }, {
      "title" : "Better feature tracking through subspace constraints",
      "author" : [ "Bryan Poling", "Gilad Lerman", "Arthur Szlam" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Poling et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Poling et al\\.",
      "year" : 2014
    }, {
      "title" : "Nonlinear shape manifolds as shape priors in level set segmentation and tracking",
      "author" : [ "Victor Adrian Prisacariu", "Ian Reid" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Prisacariu and Reid.,? \\Q2011\\E",
      "shortCiteRegEx" : "Prisacariu and Reid.",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "Recht et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2010
    }, {
      "title" : "Conjugate duality and optimization, volume",
      "author" : [ "Ralph Tyrrell Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1974\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1974
    }, {
      "title" : "Missing data in kernel pca",
      "author" : [ "Guido Sanguinetti", "Neil D Lawrence" ],
      "venue" : "In Machine Learning: ECML",
      "citeRegEx" : "Sanguinetti and Lawrence.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sanguinetti and Lawrence.",
      "year" : 2006
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "Bernhard Schölkopf", "Alexander Smola", "Klaus-Robert Müller" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1998
    }, {
      "title" : "Probabilistic principal component analysis",
      "author" : [ "Michael E Tipping", "Christopher M Bishop" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Tipping and Bishop.,? \\Q1999\\E",
      "shortCiteRegEx" : "Tipping and Bishop.",
      "year" : 1999
    }, {
      "title" : "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization",
      "author" : [ "John Wright", "Arvind Ganesh", "Shankar Rao", "Yigang Peng", "Yi Ma" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wright et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wright et al\\.",
      "year" : 2009
    }, {
      "title" : "Robust sparse principal component analysis",
      "author" : [ "Qian Zhao", "DeYu Meng", "ZongBen Xu" ],
      "venue" : "Science China Information Sciences,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    }, {
      "title" : "Low-rank modeling and its applications in image analysis",
      "author" : [ "Xiaowei Zhou", "Can Yang", "Hongyu Zhao", "Weichuan Yu" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "Zhou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2014
    }, {
      "title" : "L1/2 regularization: a thresholding representation theory and a fast solver",
      "author" : [ "Xu Zongben", "Chang Xiangyu", "Xu Fengmin", "Zhang Hai" ],
      "venue" : "IEEE Transactions on neural networks and learning systems,",
      "citeRegEx" : "Zongben et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zongben et al\\.",
      "year" : 2012
    }, {
      "title" : "3D reconstruction errors for different NRSfM approaches and our TNH Algorithm given ground truth camera projection matrices. Results for all the methods (except TNH",
      "author" : [ "Dai" ],
      "venue" : null,
      "citeRegEx" : "Dai,? \\Q2014\\E",
      "shortCiteRegEx" : "Dai",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Principal component analysis (PCAJolliffe (2002)), Kernel PCA (KPCASchölkopf et al.",
      "startOffset" : 33,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "Principal component analysis (PCAJolliffe (2002)), Kernel PCA (KPCASchölkopf et al. (1998)) and Latent Variable Models (LVMsLawrence (2005)) are some of the well known techniques used to create low dimensional representations of the given data while preserving its significant information.",
      "startOffset" : 33,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Principal component analysis (PCAJolliffe (2002)), Kernel PCA (KPCASchölkopf et al. (1998)) and Latent Variable Models (LVMsLawrence (2005)) are some of the well known techniques used to create low dimensional representations of the given data while preserving its significant information.",
      "startOffset" : 33,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Principal component analysis (PCAJolliffe (2002)), Kernel PCA (KPCASchölkopf et al. (1998)) and Latent Variable Models (LVMsLawrence (2005)) are some of the well known techniques used to create low dimensional representations of the given data while preserving its significant information. One key deployment of low-dimensional modeling occurs in solving ill-posed inference problems. Assuming the valid solutions to the problem lie near a low-dimensional manifold (i.e. can be parametrized with a reduced set of variables) allows for a tractable inference for otherwise underconstrained problems. After the seminal work of Candès & Recht (2009); Recht et al.",
      "startOffset" : 33,
      "endOffset" : 646
    }, {
      "referenceID" : 15,
      "context" : "Principal component analysis (PCAJolliffe (2002)), Kernel PCA (KPCASchölkopf et al. (1998)) and Latent Variable Models (LVMsLawrence (2005)) are some of the well known techniques used to create low dimensional representations of the given data while preserving its significant information. One key deployment of low-dimensional modeling occurs in solving ill-posed inference problems. Assuming the valid solutions to the problem lie near a low-dimensional manifold (i.e. can be parametrized with a reduced set of variables) allows for a tractable inference for otherwise underconstrained problems. After the seminal work of Candès & Recht (2009); Recht et al. (2010) on guaranteed rank minimization of the matrix via trace norm heuristics Fazel (2002), many ill-posed computer vision problems have been tackled by using the trace norm — a convex surrogate of the rank function — as a regularization term in an energy minimization frameworkCandès & Recht (2009); Zhou et al.",
      "startOffset" : 33,
      "endOffset" : 667
    }, {
      "referenceID" : 11,
      "context" : "(2010) on guaranteed rank minimization of the matrix via trace norm heuristics Fazel (2002), many ill-posed computer vision problems have been tackled by using the trace norm — a convex surrogate of the rank function — as a regularization term in an energy minimization frameworkCandès & Recht (2009); Zhou et al.",
      "startOffset" : 79,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "(2010) on guaranteed rank minimization of the matrix via trace norm heuristics Fazel (2002), many ill-posed computer vision problems have been tackled by using the trace norm — a convex surrogate of the rank function — as a regularization term in an energy minimization frameworkCandès & Recht (2009); Zhou et al.",
      "startOffset" : 79,
      "endOffset" : 301
    }, {
      "referenceID" : 11,
      "context" : "(2010) on guaranteed rank minimization of the matrix via trace norm heuristics Fazel (2002), many ill-posed computer vision problems have been tackled by using the trace norm — a convex surrogate of the rank function — as a regularization term in an energy minimization frameworkCandès & Recht (2009); Zhou et al. (2014). The flexible and easy integration of low-rank priors is one of key factors for versatility and success of many algorithms.",
      "startOffset" : 79,
      "endOffset" : 321
    }, {
      "referenceID" : 8,
      "context" : "For example, pre-trained active appearance models Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature tracking Poling et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "For example, pre-trained active appearance models Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature tracking Poling et al.",
      "startOffset" : 50,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "For example, pre-trained active appearance models Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature tracking Poling et al. (2014), dense registration Garg et al.",
      "startOffset" : 50,
      "endOffset" : 178
    }, {
      "referenceID" : 8,
      "context" : "For example, pre-trained active appearance models Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature tracking Poling et al. (2014), dense registration Garg et al. (2013b) and vivid reconstructions of natural videos Garg et al.",
      "startOffset" : 50,
      "endOffset" : 218
    }, {
      "referenceID" : 8,
      "context" : "For example, pre-trained active appearance models Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature tracking Poling et al. (2014), dense registration Garg et al. (2013b) and vivid reconstructions of natural videos Garg et al. (2013a) with no a priori knowledge of the scene.",
      "startOffset" : 50,
      "endOffset" : 282
    }, {
      "referenceID" : 6,
      "context" : "Various bilinear factorization problems like background modeling, structure from motion or photometric stereo are also addressed with a variational formulation of the trace norm regularization Cabral et al. (2013).",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 20,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al.",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al.",
      "startOffset" : 298,
      "endOffset" : 318
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009).",
      "startOffset" : 298,
      "endOffset" : 338
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009).",
      "startOffset" : 298,
      "endOffset" : 359
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009). Generative models like LVMs Lawrence (2005) are often used for inference by searching the lowdimensional latent space for a location which maximizes the likelihood of the observations.",
      "startOffset" : 298,
      "endOffset" : 387
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009). Generative models like LVMs Lawrence (2005) are often used for inference by searching the lowdimensional latent space for a location which maximizes the likelihood of the observations.",
      "startOffset" : 298,
      "endOffset" : 432
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009). Generative models like LVMs Lawrence (2005) are often used for inference by searching the lowdimensional latent space for a location which maximizes the likelihood of the observations. Problems like segmentation, tracking and semantic 3D reconstruction Prisacariu & Reid (2011); Dame et al.",
      "startOffset" : 298,
      "endOffset" : 666
    }, {
      "referenceID" : 2,
      "context" : "Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed to address the first concern, while various additional regularizers have been used to estimate the pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen & Hansen (2009). Generative models like LVMs Lawrence (2005) are often used for inference by searching the lowdimensional latent space for a location which maximizes the likelihood of the observations. Problems like segmentation, tracking and semantic 3D reconstruction Prisacariu & Reid (2011); Dame et al. (2013) greatly benefit from using LVM.",
      "startOffset" : 298,
      "endOffset" : 686
    }, {
      "referenceID" : 13,
      "context" : "The trace norm ‖M‖∗ =: ∑ i λi(M) of a matrix M is the sum of its eigenvalues λi(M) and was proposed as a tight convex relaxation1 of the rank(M) and is used in many vision problems as a rank regularizer Fazel (2002). Although the rank minimization via trace norm relaxation does not lead to a convex problem in presence of a non-linear kernel function, we show in 3.",
      "startOffset" : 203,
      "endOffset" : 216
    }, {
      "referenceID" : 18,
      "context" : "Although it is clear that relaxing the rank of kernel matrix to ‖K(S)‖∗ is suboptimal, works like Huang et al. (2012); Cabral et al.",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "(2012); Cabral et al. (2013) with a variational definition of nuclear norm, allude to the possibility of kernelization.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "Intuitively, we can also interpret (5) from the probabilistic viewpoint as commonly used in latent variable model based approaches to define kernel function Lawrence (2005). For example a RBF kernel with additive Gaussian noise and inverse width γ can be defined as: K(S)i,j = e−γ‖si−sj‖ 2 + , where ∼ N (0, σ).",
      "startOffset" : 157,
      "endOffset" : 173
    }, {
      "referenceID" : 16,
      "context" : "Note that in closely related work Geiger et al. (2009), continuous rank penalization (with a logarithmic prior) has also been used for robust probabilistic non-linear dimensionality reduction and model selection in LVM framework.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 16,
      "context" : "Note that in closely related work Geiger et al. (2009), continuous rank penalization (with a logarithmic prior) has also been used for robust probabilistic non-linear dimensionality reduction and model selection in LVM framework. However, unlike Geiger et al. (2009); Lawrence (2005) where the non-linearities are modeled in latent space (of predefined dimensionality), our approach directly penalizes the non-linear dimensionality of data in a KPCA framework and is applicable to solve inverse problems without pre-training.",
      "startOffset" : 34,
      "endOffset" : 267
    }, {
      "referenceID" : 16,
      "context" : "Note that in closely related work Geiger et al. (2009), continuous rank penalization (with a logarithmic prior) has also been used for robust probabilistic non-linear dimensionality reduction and model selection in LVM framework. However, unlike Geiger et al. (2009); Lawrence (2005) where the non-linearities are modeled in latent space (of predefined dimensionality), our approach directly penalizes the non-linear dimensionality of data in a KPCA framework and is applicable to solve inverse problems without pre-training.",
      "startOffset" : 34,
      "endOffset" : 284
    }, {
      "referenceID" : 35,
      "context" : "In a closely related work Zongben et al. (2012), authors advocateL1/2 norm as a better approximation for the cardinality of a vector then the more commonly used L1 norm.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 35,
      "context" : "In a closely related work Zongben et al. (2012), authors advocateL1/2 norm as a better approximation for the cardinality of a vector then the more commonly used L1 norm. A closed form solution for L1/2 regularization similar to our work was outlined in Zongben et al. (2012) and was shown to outperform the L1 vector norm regularization for sparse coding.",
      "startOffset" : 26,
      "endOffset" : 275
    }, {
      "referenceID" : 22,
      "context" : "Table 1: Performance comparison on missing data completion on Oil Flow Dataset: Row 1 shows the amount of missing data and subsequent rows show the mean and standard deviation of the error in recovered data matrix over 50 runs on 100 samples of oil flow dataset by: (1) The mean method (also the initialization of other methods) where the missing entries are replaced by the mean of the known values of the corresponding attributes, (2) 1-nearest neighbor method in which missing entries are filled by the values of the nearest point, (3) PPCA Tipping & Bishop (1999), (4) PKPCA of Sanguinetti & Lawrence (2006), (5)RKPCA Nguyen & De la Torre (2009) and our method.",
      "startOffset" : 596,
      "endOffset" : 612
    }, {
      "referenceID" : 22,
      "context" : "Table 1: Performance comparison on missing data completion on Oil Flow Dataset: Row 1 shows the amount of missing data and subsequent rows show the mean and standard deviation of the error in recovered data matrix over 50 runs on 100 samples of oil flow dataset by: (1) The mean method (also the initialization of other methods) where the missing entries are replaced by the mean of the known values of the corresponding attributes, (2) 1-nearest neighbor method in which missing entries are filled by the values of the nearest point, (3) PPCA Tipping & Bishop (1999), (4) PKPCA of Sanguinetti & Lawrence (2006), (5)RKPCA Nguyen & De la Torre (2009) and our method.",
      "startOffset" : 596,
      "endOffset" : 650
    }, {
      "referenceID" : 33,
      "context" : "be seen as generalization of Zongben et al. (2012) to include the L1/2 matrix norms for which a simplified proof is included in the Appendix A.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 33,
      "context" : "be seen as generalization of Zongben et al. (2012) to include the L1/2 matrix norms for which a simplified proof is included in the Appendix A. It is important to note however, that the motivation and implication of using L1/2 regularization in the context of non-linear dimensionality reduction are significantly different to that of Zongben et al. (2012) and related work Du et al.",
      "startOffset" : 29,
      "endOffset" : 357
    }, {
      "referenceID" : 12,
      "context" : "(2012) and related work Du et al. (2013); Zhao et al.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "(2012) and related work Du et al. (2013); Zhao et al. (2014) which are designed for linear modeling of the causal factors.",
      "startOffset" : 24,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "Following the experimental setup as specified in Sanguinetti & Lawrence (2006), we repeat the experiments with 50 different samples of Z.",
      "startOffset" : 63,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Following the experimental setup as specified in Sanguinetti & Lawrence (2006), we repeat the experiments with 50 different samples of Z. We report the mean and standard deviation of the root mean square reconstruction error for our method with the choice of τ = 0.1, alongside five different methods in Table 1. Our method significantly improves the performance of missing data completion compared to other robust extensions of KPCA Tipping & Bishop (1999); Sanguinetti & Lawrence (2006); Nguyen & De la Torre (2009), for every probability of missing data.",
      "startOffset" : 63,
      "endOffset" : 458
    }, {
      "referenceID" : 22,
      "context" : "Following the experimental setup as specified in Sanguinetti & Lawrence (2006), we repeat the experiments with 50 different samples of Z. We report the mean and standard deviation of the root mean square reconstruction error for our method with the choice of τ = 0.1, alongside five different methods in Table 1. Our method significantly improves the performance of missing data completion compared to other robust extensions of KPCA Tipping & Bishop (1999); Sanguinetti & Lawrence (2006); Nguyen & De la Torre (2009), for every probability of missing data.",
      "startOffset" : 63,
      "endOffset" : 489
    }, {
      "referenceID" : 22,
      "context" : "Following the experimental setup as specified in Sanguinetti & Lawrence (2006), we repeat the experiments with 50 different samples of Z. We report the mean and standard deviation of the root mean square reconstruction error for our method with the choice of τ = 0.1, alongside five different methods in Table 1. Our method significantly improves the performance of missing data completion compared to other robust extensions of KPCA Tipping & Bishop (1999); Sanguinetti & Lawrence (2006); Nguyen & De la Torre (2009), for every probability of missing data.",
      "startOffset" : 63,
      "endOffset" : 518
    }, {
      "referenceID" : 22,
      "context" : "Following the experimental setup as specified in Sanguinetti & Lawrence (2006), we repeat the experiments with 50 different samples of Z. We report the mean and standard deviation of the root mean square reconstruction error for our method with the choice of τ = 0.1, alongside five different methods in Table 1. Our method significantly improves the performance of missing data completion compared to other robust extensions of KPCA Tipping & Bishop (1999); Sanguinetti & Lawrence (2006); Nguyen & De la Torre (2009), for every probability of missing data. Although we restrict our experiments to least-squares cost functions, it is vital to restate here that our framework could trivially incorporate robust functions like the L1 norm instead of the Frobenius norm — as a robust data term f(W,Z, S) — to generalize algorithms like Robust PCA Wright et al. (2009) to their non-linear counterparts.",
      "startOffset" : 63,
      "endOffset" : 865
    }, {
      "referenceID" : 5,
      "context" : "Non-rigid structure from motion under orthography is an ill-posed problem where the goal is to estimate the camera locations and 3D structure of a deformable objects from a collection of 2D images which are labeled with landmark correspondences Bregler et al. (2000). Assuming si(xj) ∈ R to be the 3D location of point xj on the deformable object in the i image, its orthographic projection wi(xj) ∈ R can be written as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix Bregler et al.",
      "startOffset" : 245,
      "endOffset" : 267
    }, {
      "referenceID" : 5,
      "context" : "Non-rigid structure from motion under orthography is an ill-posed problem where the goal is to estimate the camera locations and 3D structure of a deformable objects from a collection of 2D images which are labeled with landmark correspondences Bregler et al. (2000). Assuming si(xj) ∈ R to be the 3D location of point xj on the deformable object in the i image, its orthographic projection wi(xj) ∈ R can be written as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix Bregler et al. (2000). Notice that as the object deforms, even with given camera poses, reconstructing the sequence by least-squares reprojection error minimization is an ill-posed problem.",
      "startOffset" : 245,
      "endOffset" : 512
    }, {
      "referenceID" : 5,
      "context" : "Non-rigid structure from motion under orthography is an ill-posed problem where the goal is to estimate the camera locations and 3D structure of a deformable objects from a collection of 2D images which are labeled with landmark correspondences Bregler et al. (2000). Assuming si(xj) ∈ R to be the 3D location of point xj on the deformable object in the i image, its orthographic projection wi(xj) ∈ R can be written as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix Bregler et al. (2000). Notice that as the object deforms, even with given camera poses, reconstructing the sequence by least-squares reprojection error minimization is an ill-posed problem. In their seminal work, Bregler et al. (2000) proposed to solve this problem with an additional assumption that the reconstructed shapes lie on a low-dimensional linear subspace and can be parameterized as linear combinations of a relatively low number of basis shapes.",
      "startOffset" : 245,
      "endOffset" : 725
    }, {
      "referenceID" : 5,
      "context" : "Non-rigid structure from motion under orthography is an ill-posed problem where the goal is to estimate the camera locations and 3D structure of a deformable objects from a collection of 2D images which are labeled with landmark correspondences Bregler et al. (2000). Assuming si(xj) ∈ R to be the 3D location of point xj on the deformable object in the i image, its orthographic projection wi(xj) ∈ R can be written as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix Bregler et al. (2000). Notice that as the object deforms, even with given camera poses, reconstructing the sequence by least-squares reprojection error minimization is an ill-posed problem. In their seminal work, Bregler et al. (2000) proposed to solve this problem with an additional assumption that the reconstructed shapes lie on a low-dimensional linear subspace and can be parameterized as linear combinations of a relatively low number of basis shapes. NRSfM was then cast as the low-rank factorization problem of estimating these basis shapes and corresponding coefficients. Recent work, like Dai et al. (2014); Garg et al.",
      "startOffset" : 245,
      "endOffset" : 1108
    }, {
      "referenceID" : 5,
      "context" : "Non-rigid structure from motion under orthography is an ill-posed problem where the goal is to estimate the camera locations and 3D structure of a deformable objects from a collection of 2D images which are labeled with landmark correspondences Bregler et al. (2000). Assuming si(xj) ∈ R to be the 3D location of point xj on the deformable object in the i image, its orthographic projection wi(xj) ∈ R can be written as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix Bregler et al. (2000). Notice that as the object deforms, even with given camera poses, reconstructing the sequence by least-squares reprojection error minimization is an ill-posed problem. In their seminal work, Bregler et al. (2000) proposed to solve this problem with an additional assumption that the reconstructed shapes lie on a low-dimensional linear subspace and can be parameterized as linear combinations of a relatively low number of basis shapes. NRSfM was then cast as the low-rank factorization problem of estimating these basis shapes and corresponding coefficients. Recent work, like Dai et al. (2014); Garg et al. (2013a) have shown that the trace norm regularizer can be used as a convex envelope of the low-rank prior to robustly address ill-posed nature of the problem.",
      "startOffset" : 245,
      "endOffset" : 1129
    }, {
      "referenceID" : 10,
      "context" : "Additionally, if the 2D projections wi(xj) are noise free, optimizing (12) with very small τ corresponds to selecting the the solution — out of the many solutions — with (almost) zero projection error, which has minimum trace norm Dai et al. (2014). Thus henceforth, optimization of (12) is referred as the trace norm heuristics (TNH).",
      "startOffset" : 231,
      "endOffset" : 249
    }, {
      "referenceID" : 10,
      "context" : "Additionally, if the 2D projections wi(xj) are noise free, optimizing (12) with very small τ corresponds to selecting the the solution — out of the many solutions — with (almost) zero projection error, which has minimum trace norm Dai et al. (2014). Thus henceforth, optimization of (12) is referred as the trace norm heuristics (TNH). We solve this problem with a first order primal-dual variant of the algorithm given in Garg et al. (2013a), which can handle missing data.",
      "startOffset" : 231,
      "endOffset" : 443
    }, {
      "referenceID" : 10,
      "context" : "We use a sub-sampled version of CMU mocap dataset by selecting every 10 frame of the smoothly deforming human body consisting 41 mocap points used in Dai et al. (2014).4 In our experiments we use ground truth camera projection matrices to compare our algorithm against TNH.",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : "Following the standard protocol in Dai et al. (2014); Akhter et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "(2014); Akhter et al. (2009), we quantify the reconstruction results with normalized mean 3D errors e3D = 1 σFN ∑",
      "startOffset" : 8,
      "endOffset" : 29
    } ],
    "year" : 2016,
    "abstractText" : "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, nonlinear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training. We re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy. To the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create their low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise. We demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.",
    "creator" : "LaTeX with hyperref package"
  }
}