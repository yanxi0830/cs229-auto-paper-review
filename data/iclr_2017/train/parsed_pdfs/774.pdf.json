{
  "name" : "774.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "tom.white@vuw.ac.nz" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Generative models are a popular approach to unsupervised machine learning. Generative neural network models are trained to produce data samples that resemble the training set. Because the number of model parameters is significantly smaller than the training data, the models are forced to discover efficient data representations. These models are sampled from a set of latent variables in a high dimensional space, here called a latent space. Latent space can be sampled to generate observable data values. Learned latent representations often also allow semantic operations with vector space arithmetic (Figure 1).\nGenerative models are often applied to datasets of images. Two popular generative models for image data are the Variational Autoencoder (VAE, Kingma & Welling, 2014) and the Generative Adversarial Network (GAN, Goodfellow et al., 2014). VAEs use the framework of probabilistic graphical models with an objective of maximizing a lower bound on the likelihood of the data. GANs instead formalize\nthe training process as a competition between a generative network and a separate discriminative network. Though these two frameworks are very different, both construct high dimensional latent spaces that can be sampled to generate images resembling training set data. Moreover, these latent spaces are generally highly structured and can enable complex operations on the generated images by simple vector space arithmetic in the latent space (Larsen et al., 2016).\nGenerative models are beginning to find their way out of academia and into creative applications. In this paper we present techniques for improving the visual quality of generative models that are generally independent of the model itself. These include spherical linear interpolation, visualizing analogies with J-diagrams, and generating local manifolds with MINE grids. These techniques can be combined generate low dimensional embeddings of images close to the trained manifold. These can be used for visualization and creating realistic interpolations across latent space. Also by standardizing these operations independent of model type, the latent space of different generative models are more directly comparable with each other, exposing the strengths and weaknesses of various approaches.\nAdditionally, two new techniques for building latent space attribute vectors are introduced. On labeled datasets with correlated labels, data replication can be used to create bias-corrected vectors. Synthetic attributes vectors also can be derived via data augmentation on unlabeled data. Quantitative analysis of attribute vectors can be performed by using them as the basis for attribute binary classifiers."
    }, {
      "heading" : "2 SAMPLING TECHNIQUES",
      "text" : "Generative models are often evaluated by examining samples from the latent space. Techniques frequently used are random sampling and linear interpolation. But often these can result in sampling the latent space from locations very far outside the manifold of probable locations.\nOur work has followed two useful principles when sampling the latent space of a generative model. The first is to avoid sampling from locations that are highly unlikely given the prior of the model. This technique is very well established - including being used in the original VAE paper which adjusted sampling through the inverse CDF of the Gaussian to accommodate the Gaussian prior (Kingma & Welling, 2014). The second principle is to recognize that the dimensionality of the latent space is often artificially high and may contains dead zones that are not on the manifold learned during training. This has been demonstrated for VAE models (Makhzani et al., 2016) and implies that simply matching the model’s prior will not always be sufficient to yield samples that appear to have been drawn from the training set."
    }, {
      "heading" : "2.1 INTERPOLATION",
      "text" : "Interpolation is used to traverse between two known locations in latent space. Research on generative models often uses interpolation as a way of demonstrating that a generative model has not simply memorized the training examples (eg: Radford et al., 2015, §6.1). In creative applications interpolations can be used to provide smooth transitions between two decoded images.\nFrequently linear interpolation is used, which is easily understood and implemented. But this is often inappropriate as the latent spaces of most generative models are high dimensional (> 50 dimensions) with a Gaussian or uniform prior. In such a space, linear interpolation traverses locations that are extremely unlikely given the prior. As a concrete example, consider a 100 dimensional space with the Gaussian prior µ=0, σ=1. Here all random vectors will generally a length very close to 10 (standard deviation < 1). However, linearly interpolating between any two will usually result in a \"tent-pole\" effect as the magnitude of the vector decreases from roughly 10 to 7 at the midpoint, which is over 4 standard deviations away from the expected length.\nOur proposed solution is to use spherical linear interpolation slerp instead of linear interpolation. We use a formula introduced by (Shoemake 85) in the context of great arc inbetweening for rotation animations:\nThis treats the interpolation as a great circle path on an n-dimensional hypersphere (with elevation changes). This technique has shown promising results on both VAE and GAN generative models and with both uniform and Gaussian priors (Figure 2)."
    }, {
      "heading" : "2.2 ANALOGY",
      "text" : "Analogy has been shown to capture regularities in continuous space models. In the latent space of some linguistic models “King – Man + Woman” results in a vector very close to “Queen” (Mikolov et al., 2013). This technique has also been used in the context of deep generative models to solve visual analogies (Reed et al., 2015).\nAnalogies are usually written in the form:\nA : B :: C :?\nSuch a formation answers the question “What is the result of applying the transformation A:B to C?” In a vector space the solution generally proposed is to solve the analogy using vector math:\n(B −A) = (?− C)\n? = C +B −A\nNote that an interesting property of this solution is an implied symmetry:\nA : C :: B :?\nBecause the same terms can be rearranged:\n(C −A) = (?−B)\n? = B + C −A\nGenerative models that include an encoder for computing latent vectors given new samples allow for visual analogies. We have devised a visual representation for depicting analogies of visual generative networks called a “J-Diagram”. The J- Diagram uses interpolation across two dimensions two expose the manifold of the analogy. It also makes the symmetric nature of the analogy clear (Figure 3).\nThe J-Diagram also serves as a reference visualization across different model settings because it is deterministically generated from images which can be held constant. This makes it a useful tool for comparing results across epochs during training, after adjusting hyperparameters, or even across completely different model types (Figure 4)."
    }, {
      "heading" : "2.3 MANIFOLD TRAVERSAL",
      "text" : "Generative models can produce a latent space that is not tightly packed, and the dimensionality of the latent space is often set artificially high. As a result, the manifold of trained examples is can be a subset of the latent space after training, resulting in dead zones in the expected prior.\nIf the model includes an encoder, one simple way to stay on the manifold is by only using out of sample encodings (ie: any data not used in training) in the latent space. This is a useful diagnostic,\nbut is overly restrictive in a creative application context since it prevents the model from suggesting new and novel samples from the model. However, we can recover this ability by also including the results of operations on these encoded samples that stay close to the manifold, such as interpolation, extrapolation, and analogy generation.\nIdeally, there would be a mechanism to discover this manifold within the latent space. In generative models with an encoder and ample out-of-sample data, we can instead precompute locations on the manifold with sufficient density, and later query for nearby points in the latent space from this known set. This offers a navigation mechanism based on hopping to nearest neighbors across a large database of encoded samples. When combined with interpolation, we call this visualization a Manifold Interpolated Neighbor Embedding (MINE). A MINE grid is useful to visualize local patches of the latent space (Figure 5)."
    }, {
      "heading" : "3 ATTRIBUTE VECTORS",
      "text" : "Many generative models result in a latent space that is highly structured, even on purely unsupervised datasets (Radford et al., 2015). When combined with labeled data, attribute vectors can be computed using simple arithmetic. For example, a vector can be computed which represents the smile attribute, which by shorthand we call a smile vector. Following (Larsen et al., 2016), the smile vector can be\ncomputed by simply subtracting the mean vector for images without the smile attribute from the mean vector for images with the smile attribute. This smile vector can then be applied to in a positive or negative direction to manipulate this visual attribute on samples taken from latent space (Figure 6)."
    }, {
      "heading" : "3.1 CORRELATED LABELS",
      "text" : "The approach of building attribute vectors from means of labeled data has been noted to suffer from correlated labels (Larsen et al., 2016). While many correlations would be expected from ground truths (eg: heavy makeup and wearing lipstick) we discovered others that appear to be from sampling bias. For example, male and smiling attributes have unexpected negative correlations because women in the CelebA dataset are much more likely to be smiling than men. (Table 1).\nIn an online service we setup to automatically add and remove smiles from images1, we discovered this gender bias was visually evident in the results. Our solution was to use replication on the training data such that the dataset was balanced across attributes. This was effective because ultimately the vectors are simply summed together when computing the attribute vector (Figure 7).\n1https://twitter.com/smilevector\nThis balancing technique can also be applied to attributes correlated due to ground truths. Decoupling attributes allows individual effects to be applied separately. As an example, the two attributes smiling and mouth open are highly correlated in the CelebA training set (Table 2). This is not surprising, as physically most people photographed smiling would also have their mouth open. However by forcing these attributes to be balanced, we can construct two decoupled attribute vectors. This allows for more flexibility in applying each attribute separately to varying degrees (Figure 8)."
    }, {
      "heading" : "3.2 SYNTHETIC ATTRIBUTES",
      "text" : "It has been noted that samples drawn from VAE based models is that they tend to be blurry (Goodfellow et al., 2014; Larsen et al., 2015). A possible solution to this would be to discover an attribute vector for “unblur”, and then apply this as a constant offset to latent vectors before decoding. CelebA includes a blur label for each image, and so a blur attribute vector was computed and then extrapolated in the negative direction. This was found to noticeably reduce blur, but also resulted in a number of unwanted artifacts such as increased image brightness.\nWe concluded this to be the result of human bias in labeling. Labelers appear more likely to label darker images as blurry, so this unblur vector was found to suffer from attribute correlation that also “lightened” the reconstruction. This bias could not be easily corrected because CelebA does not include a brightness label for rebalancing the data.\nFor the blurring attribute, an algorithmic solution is available. We take a large set of images from the training set and process them through a Gaussian blur filter (figure 9). Then we run both the original image set and the blurred image set through the encoder and subtract the means of each set to compute a new attribute vector for blur. We call this a synthetic attribute vector because this label is derived from algorithmic data augmentation of the training set. This technique removes the labeler bias, is straightforward to implement, and resulted in samples closely resembling the reconstructions with less noticeable blur (figure 10)."
    }, {
      "heading" : "3.3 QUANTITATIVE ANALYSIS WITH CLASSICICATION",
      "text" : "The results of applying attribute vectors visually to images are often quite striking. But it would be useful to be able to evaluate the relative efficacy of various attribute vectors to each other or across different models and hyperparamaters. Attribute vectors have potential to be widely applicable outside of images in domain specific latent spaces, and this provides additional challenges for quantifying their performance. For example, recent work has shown the ability to use latent space model as a continuous representation of molecules (Gómez 16); though it is straightforward to generate vectors for attributes such as solubility in these spaces, the evaluation of these operations to existing molecules would appear to require specific domain knowledge.\nWe have found that very effective classifiers can be built in latent space using the dot product of an encoded vector with a computed attribute vector (figure 11A, 11B). We have named this technique for building classifiers on latent spaces AtDot, and models trained on unsupervised data have been shown to produce strong results on CelebA across most attributes (table 3). Importantly, these binary\nclassifiers provide a quantitative basis to evaluate attribute vectors on a related surrogate task - their ability to be the basis for a simple linear classifier. This task is one of the most established paradigms in machine learning and provides a set of established tools such as ROC curves for offering new insights into the behavior of the attribute vectors across different hyperparameters or different models (figure 11C)."
    }, {
      "heading" : "4 FUTURE WORK",
      "text" : "Software to support most techniques presented in this paper is included in a python software library that can be used with various generative models2. We hope to continue to improve the library so that the techniques are applicable across a broad range of generative models. Attribute vector based classifiers also offer a promising way to evaluate the suitability of attribute vectors in domains outside of images.\nWe are investigating constructing a specially constructed prior on the latent space such that interpolations could be linear. This would simplify many of the latent space operations and might enable new types of operations.\nGiven sufficient test data, the extent to which an encoded dataset deviates from the expected prior should be quantifiable. Developing such a metric would be useful in understanding the structure of the different latent spaces including probability that random samples fall outside of the expected manifold of encoded data."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "I am thankful for the constructive feedback from readers including Ehud Ben-Reuven, Zachary Lipton, and Alex Champandard. I thank Victoria University of Wellington School of Design for supporting research on Creative Intelligence. I also thank the vibrant machine learning and creative coding communities on twitter for their support and encouragement."
    } ],
    "references" : [ {
      "title" : "Facial Attributes Classification Using Multi-Task Representation Learning",
      "author" : [ "Ehrlich", "Max", "Shields", "Timothy J", "Almaev", "Timur", "Amer", "Mohamed R" ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
      "citeRegEx" : "Ehrlich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ehrlich et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde- Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic chemical design using a data-driven continuous representation of molecules",
      "author" : [ "Gómez-Bombarelli", "Rafael", "Duvenaud", "David", "Miguel", "Hernández-Lobato", "José", "Aguilera-Iparraguirre", "Jorge", "Hirzel", "Timothy D", "Adams", "Ryan P", "Alán Aspuru-Guzik" ],
      "venue" : null,
      "citeRegEx" : "Gómez.Bombarelli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gómez.Bombarelli et al\\.",
      "year" : 2016
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Facetracer: A search engine for large collections of images with faces",
      "author" : [ "N. Kumar", "P. Belhumeur", "S. Nayar" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2008
    }, {
      "title" : "Discriminative Regularization for Generative Models",
      "author" : [ "Lamb", "Alex", "Dumoulin", "Vincent", "Courville", "Aaron" ],
      "venue" : null,
      "citeRegEx" : "Lamb et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lamb et al\\.",
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "Larsen", "Anders Boesen Lindbo", "Sønderby", "Søren Kaae", "Larochelle", "Hugo", "Winther", "Ole" ],
      "venue" : null,
      "citeRegEx" : "Larsen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Word-level training of a handwritten word recognizer based on convolutional neural networks",
      "author" : [ "Y. LeCun", "Y. Bengio" ],
      "venue" : "In ICPR,",
      "citeRegEx" : "LeCun and Bengio.,? \\Q1994\\E",
      "shortCiteRegEx" : "LeCun and Bengio.",
      "year" : 1994
    }, {
      "title" : "Learning surf cascade for fast and accurate object detection",
      "author" : [ "J. Li", "Y. Zhang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Li and Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Li and Zhang",
      "year" : 2013
    }, {
      "title" : "Linguistic Regularities in Continuous Space Word Representations",
      "author" : [ "Mikolov", "Tomas", "Yih", "Scott Wen-tau", "Zweig", "Geoffrey" ],
      "venue" : "NAACL-HLT,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "author" : [ "Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep visual analogy-making",
      "author" : [ "Reed", "Scott E", "Zhang", "Yi", "Yuting", "Lee", "Honglak" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Reed et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2015
    }, {
      "title" : "Animating rotation with quaternion curves",
      "author" : [ "Shoemake", "Ken" ],
      "venue" : "In ACM Siggraph,",
      "citeRegEx" : "Shoemake and Ken.,? \\Q1985\\E",
      "shortCiteRegEx" : "Shoemake and Ken.",
      "year" : 1985
    }, {
      "title" : "Panda: Pose aligned networks for deep attribute modeling",
      "author" : [ "N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L. Bourdev" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Moreover, these latent spaces are generally highly structured and can enable complex operations on the generated images by simple vector space arithmetic in the latent space (Larsen et al., 2016).",
      "startOffset" : 174,
      "endOffset" : 195
    }, {
      "referenceID" : 9,
      "context" : "In the latent space of some linguistic models “King – Man + Woman” results in a vector very close to “Queen” (Mikolov et al., 2013).",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "This technique has also been used in the context of deep generative models to solve visual analogies (Reed et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Many generative models result in a latent space that is highly structured, even on purely unsupervised datasets (Radford et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "Following (Larsen et al., 2016), the smile vector can be",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "1 CORRELATED LABELS The approach of building attribute vectors from means of labeled data has been noted to suffer from correlated labels (Larsen et al., 2016).",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "It has been noted that samples drawn from VAE based models is that they tend to be blurry (Goodfellow et al., 2014; Larsen et al., 2015).",
      "startOffset" : 90,
      "endOffset" : 136
    } ],
    "year" : 2016,
    "abstractText" : "We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model’s prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.",
    "creator" : "LaTeX with hyperref package"
  }
}