{
  "name" : "545.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "rcg@cs.washington.edu", "pedrod@cs.washington.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The depth of state-of-the-art convnets is a double-edged sword: it yields both nonlinearity for sophisticated discrimination and nonconvexity for frustrating optimization. The established training procedure for ILSVRC classification cycles through the million-image training set more than fifty times, requiring substantial stochasticity, data augmentation, and hand-tuned learning rates. On today’s consumer hardware, the process takes several days. However, performance depends heavily on hyperparameters, which include the number and connections of neurons as well as optimization details. Unfortunately, the space of hyperparameters is unbounded, and each configuration of hyperparameters requires the aforementioned training procedure. It is no surprise that large organizations with enough computational power to conduct this search dominate this task.\nYet mastery of object recognition on a static dataset is not enough to propel robotics and internetscale applications with ever-growing instances and categories. Each time the training set is modified, the convnet must be retrained (“fine-tuned”) for optimum performance. If the training set grows linearly with time, the total training computation grows quadratically.\nWe propose the Compositional Kernel Machine (CKM), a kernel-based visual classifier that has the symmetry and compositionality of convnets but with the training benefits of instance-based learning (IBL). CKMs branch from the original instance-based methods with virtual instances, an exponential set of plausible compositions of training instances. The first steps in this direction are promising compared to IBL and deep methods, and future work will benefit from over fifty years of research into nearest neighbor algorithms, kernel methods, and neural networks.\nIn this paper we first define CKMs, explore their formal and computational properties, and compare them to existing kernel methods. We then propose a key contribution of this work: a sum-product function (SPF) that efficiently sums over an exponential number of virtual instances. We then de-\nscribe how to train the CKM with and without parameter optimization. Finally, we present results on NORB and variants that show a CKM trained on a CPU can be competitive with convnets trained for much longer on a GPU and can outperform them on tests of composition and symmetry, as well as markedly improving over previous IBL methods."
    }, {
      "heading" : "2 COMPOSITIONAL KERNEL MACHINES",
      "text" : "The key issue in using an instance-based learner on large images is the curse of dimensionality. Even millions of training images are not enough to construct a meaningful neighborhood for a 256×256 pixel image. The compositional kernel machine (CKM) addresses this issue by constructing an exponential number of virtual instances. The core hypothesis is that a variation of the visual world can be understood as a rearrangement of low-dimensional pieces that have been seen before. For example, an image of a house could be recognized by matching many pieces from other images of houses from different viewpoints. The virtual instances represent this set of all possible transformations and recombinations of the training images. The arrangement of these pieces cannot be arbitrary, so CKMs learn how to compose virtual instances with weights on compositions. A major contribution of this work is the ability to efficiently sum over this set with a sum-product function.\nThe set of virtual instances is related to the nonlinear image manifolds described by Simard et al. (1992) but with key differences. Whereas the tangent distance accounts for transformations applied to the whole image, virtual instances can depict local transformations that are applied differently across an image. Secondly, the tangent plane approximation of the image manifold is only accurate near the training images. Virtual instances can easily represent distant transformations. Unlike the explicit augmentation of virtual support vectors in Schölkopf et al. (1996), the set of virtual instances in a CKM is implicit and exponentially larger. Platt & Allen (1996) demonstrated an early version of virtual instances to expand the set of negative examples for a linear classifier."
    }, {
      "heading" : "2.1 DEFINITION",
      "text" : "We define CKMs using notation common to other IBL techniques. The two prototypical instancebased learners are k-nearest neighbors and support vector machines. The foundation for both algorithms is a similarity or kernel function K(x, x′) between two instances. Given a training set of m labeled instances of the form 〈xi, yi〉 and query xq , the k-NN algorithm outputs the most common label of the k nearest instances:\nykNN(xq) = arg max c m∑ i=1 1 [ c = yi ∧K(xi, xq) ≥ K(xk, xq) ] where 1 [·] equals one if its argument is true and zero otherwise, and xk is the kth nearest training instance to query xq assuming unique distances. The multiclass support vector machine (Crammer & Singer, 2001) in its dual form can be seen as a weighted nearest neighbor that outputs the class with the highest weighted sum of kernel values with the query:\nySVM(xq) = arg max c m∑ i=1 αi,cK(xi, xq) (1)\nwhere αi,c is the weight on training instance xi that contributes to the score of class c.\nThe CKM performs the same classification as these instance-based methods but it sums over an exponentially larger set of virtual instances to mitigate the curse of dimensionality. Virtual instances are composed of rearranged elements from one or more training instances. Depending on the design of the CKM, elements can be subsets of instance variables (e.g., overlapping pixel patches) or features thereof (e.g., ORB features or a 2D grid of convnet feature vectors). We assume there is a deterministic procedure that processes each training or test instance xi into a fixed tuple of indexed elements Exi = (ei,1, . . . , ei,|Exi |), where instances may have different numbers of elements. The query instance xq (with tuple of elements Exq ) is the example that is being classified by the CKM; it is a training instance during training and a test instance during testing. A virtual instance z is represented by a tuple of elements from training instances, e.g. Ez = (e10,5, e71,2, . . . , e46,17). Given a query instance xq , the CKM represents a set of virtual instances each with the same number of elements asExq . We define a leaf kernelKL(ei,j , ei′,j′) that measures the similarity between any two elements. Using kernel composition (Aronszajn, 1950), we define the kernel between the query instance xq and a virtual instance z as the product of leaf kernels over their corresponding elements:\nK(z, xq) = ∏|Exq | j KL(ez,j , eq,j).\nWe combine leaf kernels with weighted sums and products to compactly represent a sum over kernels with an exponential number of virtual instances. Just as a sum-product network can compactly represent a mixture model that is a weighted sum over an exponential number of mixture components, the same algebraic decomposition can compactly encode a weighted sum over an exponential number of kernels. For example, if the query instance is represented by two elements Exq = (eq,1, eq,2) and the training set contains elements {e1, e2, e3, e4, e5, e6}, then\n[w1KL(eq,1, e1) + w2KL(eq,1, e2) + w3KL(eq,1, e3)]× [w4KL(eq,2, e4) + w5KL(eq,2, e5) + w6KL(eq,2, e6)]\nexpresses a weighted sum over nine virtual instances using eleven additions/multiplications instead of twenty-six for an expanded flat sum w1KL(eq,1, e1)KL(eq,2, e4) + . . . + w9KL(eq,1, e3) KL(eq,2, e6). If the query instance and training set contained 100 and 10000 elements, respectively, then a similar factorization would useO(106) operations compared to a naı̈ve sum over 10500 virtual instances. Leveraging the Sum-Product Theorem (Friesen & Domingos, 2016), we define CKMs to allow for more expressive architectures with this exponential computational savings. Definition 1. A compositional kernel machine (CKM) is defined recursively.\n1. A leaf kernel over a query element and a training set element is a CKM. 2. A product of CKMs with disjoint scopes is a CKM. 3. A weighted sum of CKMs with the same scope is a CKM.\nThe scope of an operator is the set of query elements it takes as inputs; it is analogous to the receptive field of a unit in a neural network, but with CKMs the query elements are not restricted to being pixels on the image grid (e.g., they may be defined as a set of extracted image features). A leaf kernel has singleton scope, internal nodes have scope over some subset of the query elements, and the root node of the CKM has full scope of all query elements Exq . This definition allows for rich CKM architectures with many layers to represent elaborate compositions. The value of each sum node child is multiplied by a weight wk,c and optionally a constant cost function φ(ei,j , ei′,j′) that rewards certain compositions of elements. Analogous to a multiclass SVM, the CKM has a separate set of weights for each class c in the dataset. The CKM classifies a query instance as yCKM(xq) = arg maxc Sc(xq), where Sc(xq) is the value of the root node of the CKM evaluating query instance xq using weights for class c. Definition 2 (Friesen & Domingos (2016)). A product node is decomposable iff the scopes of its children are disjoint. An SPF is decomposable iff all of its product nodes are decomposable. Theorem 1 (Sum-Product Theorem, Friesen & Domingos (2016)). Every decomposable SPF can be summed over its domain in time linear in its size. Corollary 1. Sc(xq) can sum over the set of virtual instances in time linear in the size of the SPF. Proof. For each query instance element eq,j we define a discrete variable Zj with a state for each training element ei′,j′ found in a leaf kernel KL(eq,j , ei′,j′) in the CKM. The Cartesian product of the domains of the variables Z defines the set of virtual instances represented by the CKM. Sc(xq) is a SPF over semiring (R,⊕,⊗, 0, 1), variables Z, constant functions w and φ, and univariate functions KL(eq,j , Zj). With the appropriate definition of leaf kernels, any semiring can be used. The definition above provides that the children of every product node have disjoint scopes. Constant functions have empty scope so there is no intersection with scopes of other children. With all product nodes decomposable, Sc(xq) is a decomposable SPF and can therefore sum over all states of Z, the virtual instances, in time linear to the size of the CKM.\nSpecial cases of CKMs include multiclass SVMs (flat sum-of-products) and naive Bayes nearest neighbor (Boiman et al., 2008) (flat product-of-sums). A CKM can be seen as a generalization of an image grammar (Fu, 1974) where terminal symbols corresponding to pieces of training images are scored with kernels and non-terminal symbols are sum nodes with a production for each child product node.\nThe weights and cost functions of the CKM control the weights on the virtual instances. Each virtual instance represented by the CKM defines a tree that connects the root to the leaf kernels over its unique composition of training set elements. If we were to expand the CKM into a flat sum (cf. Equation 1), the weight on a virtual instance would be the product of the weights and cost functions along the branches of its corresponding tree. These weights are important as they can prevent implausible virtual instances. For example, if we use image patches as the elements and allow all compositions, the set of virtual instances would largely contain nonsense noise patterns. If\nthe elements were pixels, the virtual instances could even contain arbitrary images from classes not present in the training set. There are many aspects of composition that can be encoded by the CKM. For example, we can penalize virtual instances that compose training set elements using different symmetry group transformations. We could also penalize compositions that juxtapose elements that disagree on the contents of their borders. Weights can be learned to establish clusters of elements and reward certain arrangements. In Section 3 we demonstrate one choice of weights and cost functions in a CKM architecture built from extracted image features."
    }, {
      "heading" : "2.2 LEARNING",
      "text" : "The training procedure for a CKM builds an SPF that encodes the virtual instances. There are then two options for how to set weights in the model. As with k-NN, the weights in the CKM could be set to uniform. Alternatively, as with SVMs, the weights could be optimized to improve generalization and reduce model size.\nFor weight learning, we use block-coordinate gradient descent to optimize leave-one-out loss over the training set. The leave-one-out loss on a training instance xi is the loss on that instance made by the learner trained on all data except xi. Though it is an almost unbiased estimate of generalization error (Luntz & Brailovsky, 1969), it is typically too expensive to compute or optimize with non-IBL methods (Chapelle et al., 2002). With CKMs, caching the SPFs and efficient data structures make it feasible to compute exact partial derivatives of the leave-one-out loss over the whole training set. We use a multiclass squared-hinge loss\nL(xi, yi) = max 1 + Sy′(xi)︸ ︷︷ ︸ Best incorrect class −Syi(xi)︸ ︷︷ ︸ True class , 0  2\nfor the loss on training instance xi with true label yi and highest-scoring incorrect class y′. We use the squared version of the hinge loss as it performs better empirically and prioritizes updates to element weights that led to larger margin violations. In general, this objective is not convex as it involves the difference of the two discriminant functions which are strictly convex (due to the choice of semiring and the product of weights on each virtual instance). In the special case of the sum-product semiring and unique weights on virtual instances the objective is convex as is true for L2-SVMs. Convnets also have a non-convex objective, but they require lengthy optimization to perform well. As we show in Section 3, CKMs can achieve high accuracy with uniform weights, which further serves as good initialization for gradient descent.\nFor each epoch, we iterate through the training set, for each training instance xi optimizing the block of weights on those branches with Exi as descendants. We take gradient steps to lower the leaveone-out loss over the rest of the training set ∑ i′∈([1,m]\\i) L(xi′ , yi′). We iterate until convergence or an early stopping condition. A component of the gradient of the squared-hinge loss on an instance takes the form\n∂\n∂wk,c L(xi, yi) =  2∆(xi, yi) ∂Sy′ (xi) ∂wk,c if ∆(xi, yi) > 0 ∧ c = y′\n−2∆(xi, yi)∂Syi (xi)∂wk,c if ∆(xi, yi) > 0 ∧ c = yi 0 otherwise\nwhere ∆(xi, yi) = 1 + Sy′(xi) − Syi(xi). We compute partial derivatives ∂Sc(xi)∂wk,c with backpropagation through the SPF. For efficiency, terms of the gradient can be set to zero and excluded from backpropagation if the values of corresponding leaf kernels are small enough. This is either exact (e.g., if ⊕ is maximization) or an approximation (e.g., if ⊕ is normal addition)."
    }, {
      "heading" : "2.3 SCALABILITY",
      "text" : "CKMs have several scalability advantages over convnets. As mentioned previously, they do not require a lengthy training procedure. This makes it much easier to add new instances and categories. Whereas most of the computation to evaluate a single setting of convnet hyperparameters is sunk in training, CKMs can efficiently race hyperparameters on hold-out data (Lee & Moore, 1994).\nThe evaluation of the CKM depends on the structure of the SPF, the size of the training set, and the computer architecture. A basic building block of these SPFs is a sum node with a number of children on the order of magnitude of the training set elements |E|. On a sufficiently parallel\ncomputer, assuming the size of the training set elements greatly exceeds the dimensionality of the leaf kernel, this sum node will require O(log(|E|)) time (the depth of a parallel ⊕ reduction circuit) and O(|E|) space. Duda et al. (2000) describe a constant time nearest neighbor circuit that relies on precomputed Voronoi partitions, but this has impractical space requirements in high dimensions. As with SVMs, optimization of sparse element weights can greatly reduce model size.\nOn a modest multicore computer, we must resort to using specialized data structures. Hash codes can be used to index raw features or to measure Hamming distance as a proxy to more expensive distance functions. While they are perhaps the fastest method to accelerate a nearest neighbor search, the most accurate hashing methods involve a training period yet do not necessarily result in high recall (Torralba et al., 2008; Heo et al., 2012). There are many space-partitioning data structure trees in the literature, however in practice none are able to offer exact search of nearest neighbors in high dimensions in logarithmic time. In our experiments we use hierarchical k-means trees (Muja & Lowe, 2009), which are a good compromise between speed and accuracy."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "We test CKMs on three image classification scenarios that feature images from either the small NORB dataset or the NORB jittered-cluttered dataset (LeCun et al., 2004). Both NORB datasets contain greyscale images of five categories of plastic toys photographed with varied altitudes, azimuths, and lighting conditions. Table 1 summarizes the datasets. We first describe the SPN architecture and then detail each of the three scenarios."
    }, {
      "heading" : "3.1 EXPERIMENTAL ARCHITECTURE",
      "text" : "In our experiments the architecture of the SPF Sc(xq) for each query image is based on its unique set of extracted ORB features. Like SIFT features, ORB features are rotation-invariant and produce a descriptor from intensity differences, but ORB is much faster to compute and thus suitable for real time applications (Rublee et al., 2011). The elements Exi = (ei,1, . . . , ei,|Ei|) of each image xi are its extracted keypoints, where an element’s feature vector and image position are denoted by ~f(ei,j) and ~p(ei,j) respectively. We use the max-sum semiring (⊕ = max, ⊗ = +) because it is more robust to noisy virtual instances, yields sparser gradients, is more efficient to compute, and performs better empirically compared with the sum-product semiring.\nThe SPF Sc(xq) maximizes over variables Z = (Z1, . . . , Z|Exq |) corresponding to query elements Exq with states for all possible virtual instances. The SPF contains a unary scope max node for every variable {Zj} that maximizes over the weighted kernels of all possible training elements E : ⊕(Zj) = ⊕ zj∈E wzj ,c ⊗KL(zj , eq,j). The SPF contains a binary scope max node for all pairs of variables {Zj , Zj′} for which at least one corresponding query element is within the k-nearest spatial neighbors of the other. These nodes maximize over the weighted kernels of all possible combinations of training set elements.\n⊕(Zj , Zj′) = ⊕ zj∈E ⊕ zj′∈E wzj ,c ⊗ wzj′ ,c ⊗ φ(zj , zj′)⊗KL(zj , eq,j)⊗KL(zj′ , eq,j′) (2)\nThis maximizes over all possible pairs of training set elements, weighting the two leaf kernels by two corresponding element weights and a cost function. We use a leaf kernel for image elements that incorporates both the Hamming distance between their features and the Euclidean distance between their image positions: KL(ei,j , ei′,j′) = max(β0 − β1dHam(~f(ei,j), ~f(ei′,j′)), 0) + max(β2||(~p(ei,j), ~p(ei′,j′)||, β3). This rewards training set elements that look like a query instance element and appear in a similar location, with thresholds for efficiency. This can represent, for example, the photographic bias to center foreground objects or a discriminative cue from seeing sky at the top of the image. We use the pairwise cost function φ(ei,j , ei′,j′) = 1[i = i′]β4 that rewards combinations of elements from the same source training image. This captures the intuition that\ncompositions sourced from more images are less coherent and more likely to contain nonsense than those using fewer. The image is represented as a sum of these unary and binary max nodes. The scopes of children of the sum are restricted to be disjoint, so the children {⊕(Z1, Z2),⊕(Z2, Z3)} would be disallowed, for example. This restriction is what allows the SPF to be tractable, and with multiple sums the SPF has high-treewidth. By comparison, a Markov random field expressing these dependencies would be intractable. The root max node of the SPF has P sums as children, each of which has its random set of unary and binary scope max node children that cover full scope Z. We illustrate a simplified version of the SPF architecture in Figure 1. Though this SPF models limited image structure, the definition of CKMs allows for more expressive architectures as with SPNs.\nIn the following sections, we refer to two variants CKM and CKMW . The CKM version uses uniform weights wk,c, similar to the basic k-nearest neighbor algorithm. The CKMW method optimizes weights wk,c as described in Section 2.2. Both versions restrict weights for class c to be −∞ (⊕ identity) for those training elements not in class c. This constraint ensures that method CKM is discriminative (as is true with k-NN) and reduces the number of parameters optimized by CKMW . The hyperparameters of ORB feature extraction, leaf kernels, cost function, and optimization were chosen using grid search on a validation set.\nWith our CPU implementation, CKM trains in a single pass of feature extraction and storage at ∼5ms/image, CKMW trains in under ten epochs at ∼90ms/image, and both versions test at ∼80ms/image. The GPU-optimized convnets train at ∼2ms/image for many epochs and test at ∼1ms/image. Remarkably, CKM on a CPU trains faster than the convnet on a GPU."
    }, {
      "heading" : "3.2 SMALL NORB",
      "text" : "We use the original train-test separation which measures generalization to new instances of a category (i.e. tested on toy truck that is different from the toys it was trained on). We show promising results in Table 2 comparing CKMs to deep and IBL methods. With improvement over k-NN and SVM, the CKM and CKMW results show the benefit of using virtual instances to combat the curse of dimensionality. We note that the CKM variant that does not optimize weights performs nearly as well as the CKMW version that does. Since the test set uses a different set of toys, the use of untrained ORB features hurts the performance of the CKM. Convnets have an advantage here because they discriminatively train their lowest level of features and represent richer image structure in their architecture. To become competitive, future work should improve upon this preliminary CKM\narchitecture. We demonstrate the advantage of CKMs for representing composition and symmetry in the following experiments."
    }, {
      "heading" : "3.3 NORB COMPOSITIONS",
      "text" : "A general goal of representation learning is to disentangle the factors of variation of a signal without having to see those factors in all combinations. To evaluate progress towards this, we created images containing three toys each, sourced from the small NORB training set. Small NORB contains ten types of each toy category (e.g., ten different airplanes), which we divided into two collections. Each image is generated by choosing one of the collections uniformly and for each of three categories (person, airplane, animal) randomly sampling a toy from that collection with higher probability (P = 56 ) than from the other collection (i.e., there are two children with disjoint toy collections but they sometimes borrow). The task is to determine which of the two collections generated the image. This dataset measures whether a method can distinguish different compositions without having seen all possible permutations of those objects through symmetries and noisy intra-class variation. Analogous tasks include identifying people by their clothing, recognizing social groups by their members, and classifying cuisines by their ingredients.\nWe compare CKMs to other methods in Table 3. Convnets and their features are computed using the TensorFlow library (Abadi et al., 2015). Training convnets from few images is very difficult without resorting to other datasets; we augment the training set with random crops, which still yields test accuracy near chance. In such situations it is common to train an SVM with features extracted by a convnet trained on a different, larger dataset. We use 2048-dimensional features extracted from the penultimate layer of the pre-trained Inception network (Szegedy et al., 2015) and a linear kernel SVM with squared-hinge loss (Pedregosa et al., 2011). Notably, the CKM is much more accurate than the deep methods, and it is about as fast as the SVM despite not taking advantage of the GPU."
    }, {
      "heading" : "3.4 NORB SYMMETRIES",
      "text" : "Composition is a useful tool for modeling the symmetries of objects. When we see an image of an object in a new pose, parts of the image may look similar to parts of images of the object in poses we have seen before. In this experiment, we partition the training set of NORB jittered-cluttered into a\nnew dataset with 10% withheld for each of validation and testing. Training and testing on the same group of toy instances, this measures the ability to generalize to new angles, lighting conditions, backgrounds, and distortions.\nWe vary the amount of training data to plot learning curves in Figure 3. We observe that CKMs are better able to generalize to these distortions than other methods, especially with less data. Importantly, the performance of CKM improves with more data, without requiring costly optimization as data is added. We note that the benefit of CKMW using weight learning becomes apparent with 200 training instances. This learning curve suggests that CKMs would be well suited for applications in cluttered environments with many 3D transformations (e.g., loop closure)."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "This paper proposed compositional kernel machines, an instance-based method for object recognition that addresses some of the weaknesses of deep architectures and other kernel methods. We showed how using a sum-product function to represent a discriminant function leads to tractable summation over the weighted kernels to an exponential set of virtual instances, which can mitigate the curse of dimensionality and improve sample complexity. We proposed a method to discriminatively learn weights on individual instance elements and showed that this improves upon uniform weighting. Finally, we presented results in several scenarios showing that CKMs are a significant improvement for IBL and show promise compared with deep methods.\nFuture research directions include developing other architectures and learning procedures for CKMs, integrating symmetry transformations into the architecture through kernels and cost functions, and applying CKMs to structured prediction, regression, and reinforcement learning problems. CKMs exhibit a reversed trade-off of fast learning speed and large model size compared to neural networks. Given that animals can benefit from both trade-offs, these results may inspire computational theories of different brain structures, especially the neocortex versus the cerebellum (Ito, 2012)."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors are grateful to John Platt for helpful discussions and feedback. This research was partly supported by ONR grant N00014-16-1-2697, AFRL contract FA8750-13-2-0019, a Google PhD Fellowship, an AWS in Education Grant, and an NVIDIA academic hardware grant. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ONR, AFRL, or the United States Government."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "cent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Scaling learning algorithms towards AI",
      "author" : [ "Yoshua Bengio", "Yann LeCun" ],
      "venue" : "Large-Scale Kernel Machines,",
      "citeRegEx" : "Bengio and LeCun.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bengio and LeCun.",
      "year" : 2007
    }, {
      "title" : "In defense of nearest-neighbor based image classification",
      "author" : [ "Oren Boiman", "Eli Shechtman", "Michal Irani" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,",
      "citeRegEx" : "Boiman et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Boiman et al\\.",
      "year" : 1992
    }, {
      "title" : "Choosing multiple parameters for support vector machines",
      "author" : [ "Olivier Chapelle", "Vladimir Vapnik", "Olivier Bousquet", "Sayan Mukherjee" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2002
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "Koby Crammer", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Crammer and Singer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Crammer and Singer.",
      "year" : 2001
    }, {
      "title" : "Pattern Classification",
      "author" : [ "Richard O Duda", "Peter E Hart", "David G Stork" ],
      "venue" : null,
      "citeRegEx" : "Duda et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Duda et al\\.",
      "year" : 2000
    }, {
      "title" : "The sum-product theorem: A foundation for learning tractable models",
      "author" : [ "Abram L Friesen", "Pedro Domingos" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Friesen and Domingos.,? \\Q2016\\E",
      "shortCiteRegEx" : "Friesen and Domingos.",
      "year" : 2016
    }, {
      "title" : "Syntactic Methods in Pattern Recognition, volume 112",
      "author" : [ "King Sun Fu" ],
      "venue" : null,
      "citeRegEx" : "Fu.,? \\Q1974\\E",
      "shortCiteRegEx" : "Fu.",
      "year" : 1974
    }, {
      "title" : "Spherical hashing",
      "author" : [ "Jae-Pil Heo", "Youngwoon Lee", "Junfeng He", "Shih-Fu Chang", "Sung-Eui Yoon" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,",
      "citeRegEx" : "Heo et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Heo et al\\.",
      "year" : 2012
    }, {
      "title" : "The Cerebellum: Brain for an Implicit Self",
      "author" : [ "Masao Ito" ],
      "venue" : "FT press,",
      "citeRegEx" : "Ito.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ito.",
      "year" : 2012
    }, {
      "title" : "Learning methods for generic object recognition with invariance to pose and lighting",
      "author" : [ "Yann LeCun", "Fu Jie Huang", "Léon Bottou" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2004
    }, {
      "title" : "Efficient algorithms for minimizing cross validation error",
      "author" : [ "Mary S Lee", "AW Moore" ],
      "venue" : "In Proceedings of the 8th International Conference on Machine Learning,",
      "citeRegEx" : "Lee and Moore.,? \\Q1994\\E",
      "shortCiteRegEx" : "Lee and Moore.",
      "year" : 1994
    }, {
      "title" : "On estimation of characters obtained in statistical procedure of recognition",
      "author" : [ "Aleksandr Luntz", "Viktor Brailovsky" ],
      "venue" : "Technicheskaya Kibernetica,",
      "citeRegEx" : "Luntz and Brailovsky.,? \\Q1969\\E",
      "shortCiteRegEx" : "Luntz and Brailovsky.",
      "year" : 1969
    }, {
      "title" : "Fast approximate nearest neighbors with automatic algorithm configuration",
      "author" : [ "Marius Muja", "David G Lowe" ],
      "venue" : "In International Conference on Computer Vision Theory and Application (VISSAPP),",
      "citeRegEx" : "Muja and Lowe.,? \\Q2009\\E",
      "shortCiteRegEx" : "Muja and Lowe.",
      "year" : 2009
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincnet Dubourg", "Jake Vanderplas", "Alexandre Passos", "David Cournapeau", "Matthieu Brucher", "Matthieu Perrot", "Édouard Duchesnay" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "A neural network classifier for the I1000",
      "author" : [ "John C Platt", "Timothy P Allen" ],
      "venue" : "OCR chip. In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Platt and Allen.,? \\Q1996\\E",
      "shortCiteRegEx" : "Platt and Allen.",
      "year" : 1996
    }, {
      "title" : "ORB: An efficient alternative to SIFT or SURF",
      "author" : [ "Ethan Rublee", "Vincent Rabaud", "Kurt Konolige", "Gary Bradski" ],
      "venue" : "In 2011 International Conference on Computer Vision,",
      "citeRegEx" : "Rublee et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rublee et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep Boltzmann machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 12th Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Salakhutdinov and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2009
    }, {
      "title" : "Incorporating invariances in support vector learning machines",
      "author" : [ "Bernhard Schölkopf", "Chris Burges", "Vladimir Vapnik" ],
      "venue" : "In Artificial Neural Networks (ICANN),",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1996
    }, {
      "title" : "Efficient pattern recognition using a new transformation distance",
      "author" : [ "Patrice Simard", "Yann LeCun", "John S Denker" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Simard et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Simard et al\\.",
      "year" : 1992
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "arXiv preprint arXiv:1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Small codes and large image databases for recognition",
      "author" : [ "Antonio Torralba", "Rob Fergus", "Yair Weiss" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,",
      "citeRegEx" : "Torralba et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Torralba et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "The set of virtual instances is related to the nonlinear image manifolds described by Simard et al. (1992) but with key differences.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "Unlike the explicit augmentation of virtual support vectors in Schölkopf et al. (1996), the set of virtual instances in a CKM is implicit and exponentially larger.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "Unlike the explicit augmentation of virtual support vectors in Schölkopf et al. (1996), the set of virtual instances in a CKM is implicit and exponentially larger. Platt & Allen (1996) demonstrated an early version of virtual instances to expand the set of negative examples for a linear classifier.",
      "startOffset" : 63,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "A CKM can be seen as a generalization of an image grammar (Fu, 1974) where terminal symbols corresponding to pieces of training images are scored with kernels and non-terminal symbols are sum nodes with a production for each child product node.",
      "startOffset" : 58,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Though it is an almost unbiased estimate of generalization error (Luntz & Brailovsky, 1969), it is typically too expensive to compute or optimize with non-IBL methods (Chapelle et al., 2002).",
      "startOffset" : 167,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "Duda et al. (2000) describe a constant time nearest neighbor circuit that relies on precomputed Voronoi partitions, but this has impractical space requirements in high dimensions.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "While they are perhaps the fastest method to accelerate a nearest neighbor search, the most accurate hashing methods involve a training period yet do not necessarily result in high recall (Torralba et al., 2008; Heo et al., 2012).",
      "startOffset" : 188,
      "endOffset" : 229
    }, {
      "referenceID" : 8,
      "context" : "While they are perhaps the fastest method to accelerate a nearest neighbor search, the most accurate hashing methods involve a training period yet do not necessarily result in high recall (Torralba et al., 2008; Heo et al., 2012).",
      "startOffset" : 188,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "3 EXPERIMENTS We test CKMs on three image classification scenarios that feature images from either the small NORB dataset or the NORB jittered-cluttered dataset (LeCun et al., 2004).",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "Like SIFT features, ORB features are rotation-invariant and produce a descriptor from intensity differences, but ORB is much faster to compute and thus suitable for real time applications (Rublee et al., 2011).",
      "startOffset" : 188,
      "endOffset" : 209
    }, {
      "referenceID" : 10,
      "context" : "3% k-NN (LeCun et al., 2004) 81.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "6% Logistic regression (LeCun et al., 2004) 77.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "We use 2048-dimensional features extracted from the penultimate layer of the pre-trained Inception network (Szegedy et al., 2015) and a linear kernel SVM with squared-hinge loss (Pedregosa et al.",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : ", 2015) and a linear kernel SVM with squared-hinge loss (Pedregosa et al., 2011).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Given that animals can benefit from both trade-offs, these results may inspire computational theories of different brain structures, especially the neocortex versus the cerebellum (Ito, 2012).",
      "startOffset" : 180,
      "endOffset" : 191
    } ],
    "year" : 2017,
    "abstractText" : "Convolutional neural networks (convnets) have achieved impressive results on recent computer vision benchmarks. While they benefit from multiple layers that encode nonlinear decision boundaries and a degree of translation invariance, training convnets is a lengthy procedure fraught with local optima. Alternatively, a kernel method that incorporates the compositionality and symmetry of convnets could learn similar nonlinear concepts yet with easier training and architecture selection. We propose compositional kernel machines (CKMs), which effectively create an exponential number of virtual training instances by composing transformed sub-regions of the original ones. Despite this, CKM discriminant functions can be computed efficiently using ideas from sum-product networks. The ability to compose virtual instances in this way gives CKMs invariance to translations and other symmetries, and combats the curse of dimensionality. Just as support vector machines (SVMs) provided a compelling alternative to multilayer perceptrons when they were introduced, CKMs could become an attractive approach for object recognition and other vision problems. In this paper we define CKMs, explore their properties, and present promising results on NORB datasets. Experiments show that CKMs can outperform SVMs and be competitive with convnets in a number of dimensions, by learning symmetries and compositional concepts from fewer samples without data augmentation.",
    "creator" : "LaTeX with hyperref package"
  }
}