{
  "name" : "449.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "FRACTALNET: ULTRA-DEEP NEURAL NETWORKS WITHOUT RESIDUALS",
    "authors" : [ "Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich" ],
    "emails" : [ "larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Residual networks (He et al., 2016a), or ResNets, lead a recent and dramatic increase in both depth and accuracy of convolutional neural networks, facilitated by constraining the network to learn residuals. ResNet variants (He et al., 2016a;b; Huang et al., 2016b) and related architectures (Srivastava et al., 2015) employ the common technique of initializing and anchoring, via a pass-through channel, a network to the identity function. Training now differs in two respects. First, the objective changes to learning residual outputs, rather than unreferenced absolute mappings. Second, these networks exhibit a type of deep supervision (Lee et al., 2014), as near-identity layers effectively reduce distance to the loss. He et al. (2016a) speculate that the former, the residual formulation itself, is crucial.\nWe show otherwise, by constructing a competitive extremely deep architecture that does not rely on residuals. Our design principle is pure enough to communicate in a single word, fractal, and a simple diagram (Figure 1). Yet, fractal networks implicitly recapitulate many properties hard-wired into previous successful architectures. Deep supervision not only arises automatically, but also drives a type of student-teacher learning (Ba & Caruana, 2014; Urban et al., 2017) internal to the network. Modular building blocks of other designs (Szegedy et al., 2015; Liao & Carneiro, 2015) resemble special cases of a fractal network’s nested substructure.\nFor fractal networks, simplicity of training mirrors simplicity of design. A single loss, attached to the final layer, suffices to drive internal behavior mimicking deep supervision. Parameters are randomly initialized. As they contain subnetworks of many depths, fractal networks are robust to choice of overall depth; make them deep enough and training will carve out a useful assembly of subnetworks.\nThe entirety of emergent behavior resulting from a fractal design may erode the need for recent engineering tricks intended to achieve similar effects. These tricks include residual functional forms with identity initialization, manual deep supervision, hand-crafted architectural modules, and studentteacher training regimes. Section 2 reviews this large body of related techniques. Hybrid designs could certainly integrate any of them with a fractal architecture; we leave open the question of the degree to which such hybrids are synergistic.\nOur main contribution is twofold:\n• We introduce FractalNet, the first simple alternative to ResNet. FractalNet shows that explicit residual learning is not a requirement for building ultra-deep neural networks.\n• Through analysis and experiments, we elucidate connections between FractalNet and an array of phenomena engineered into previous deep network designs.\nAs an additional contribution, we develop drop-path, a novel regularization protocol for ultradeep fractal networks. Without data augmentation, fractal networks, trained with drop-path and dropout (Hinton et al., 2012), exceed the performance of residual networks regularized via stochastic depth (Huang et al., 2016b). Though, like stochastic depth, it randomly removes macro-scale components, drop-path further exploits our fractal structure in choosing which components to disable.\nDrop-path constitutes not only a regularization strategy, but also provides means of optionally imparting fractal networks with anytime behavior. A particular schedule of dropped paths during learning prevents subnetworks of different depths from co-adapting. As a consequence, both shallow and deep subnetworks must individually produce correct output. Querying a shallow subnetwork thus yields a quick and moderately accurate result in advance of completion of the full network.\nSection 3 elaborates the technical details of fractal networks and drop-path. Section 4 provides experimental comparisons to residual networks across the CIFAR-10, CIFAR-100 (Krizhevsky, 2009), SVHN (Netzer et al., 2011), and ImageNet (Deng et al., 2009) datasets. We also evaluate regularization and data augmentation strategies, investigate subnetwork student-teacher behavior during training, and benchmark anytime networks obtained using drop-path. Section 5 provides synthesis. By virtue of encapsulating many known, yet seemingly distinct, design principles, selfsimilar structure may materialize as a fundamental component of neural architectures."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Deepening feed-forward neural networks has generally returned dividends in performance. A striking example within the computer vision community is the improvement on the ImageNet (Deng et al., 2009) classification task when transitioning from AlexNet (Krizhevsky et al., 2012) to VGG (Simonyan & Zisserman, 2015) to GoogLeNet (Szegedy et al., 2015) to ResNet (He et al., 2016a). Unfortunately, greater depth also makes training more challenging, at least when employing a firstorder optimization method with randomly initialized layers. As the network grows deeper and more non-linear, the linear approximation of a gradient step becomes increasingly inappropriate. Desire to overcome these difficulties drives research on both optimization techniques and network architectures.\nOn the optimization side, much recent work yields improvements. To prevent vanishing gradients, ReLU activation functions now widely replace sigmoid and tanh units (Nair & Hinton, 2010). This subject remains an area of active inquiry, with various tweaks on ReLUs, e.g. PReLUs (He et al., 2015), and ELUs (Clevert et al., 2016). Even with ReLUs, employing batch normalization (Ioffe & Szegedy, 2015) speeds training by reducing internal covariate shift. Good initialization can also ameliorate this problem (Glorot & Bengio, 2010; Mishkin & Matas, 2016). Path-SGD (Neyshabur et al., 2015) offers an alternative normalization scheme. Progress in optimization is somewhat orthogonal to our architectural focus, with the expectation that advances in either are ripe for combination.\nNotable ideas in architecture reach back to skip connections, the earliest example of a nontrivial routing pattern within a neural network. Recent work further elaborates upon them (Maire et al., 2014; Hariharan et al., 2015). Highway networks (Srivastava et al., 2015) and ResNet (He et al., 2016a;b) offer additional twists in the form of parameterized pass-through and gating. In work subsequent to our own, Huang et al. (2016a) investigate a ResNet variant with explicit skip connections. These methods share distinction as the only other designs demonstrated to scale to hundreds of layers and beyond. ResNet’s building block uses the identity map as an anchor point and explicitly parameterizes an additive correction term (the residual). Identity initialization also appears in the context of recurrent networks (Le et al., 2015). A tendency of ResNet and highway networks to fall-back to the identity map may make their effective depth much smaller than their nominal depth.\nSome prior results hint at what we experimentally demonstrate in Section 4. Namely, reduction of effective depth is key to training extremely deep networks; residuals are incidental. Huang et al. (2016b) provide one clue in their work on stochastic depth: randomly dropping layers from ResNet during training, thereby shrinking network depth by a constant factor, provides additional performance benefit. We build upon this intuition through drop-path, which shrinks depth much more drastically.\nThe success of deep supervision (Lee et al., 2014) provides another clue that effective depth is crucial. Here, an auxiliary loss, forked off mid-level layers, introduces a shorter path during backpropagation. The layer at the fork receives two gradients, originating from the main loss and the auxiliary loss, that are added together. Deep supervision is now common, being adopted, for example, by GoogLeNet (Szegedy et al., 2015). However, irrelevance of the auxiliary loss at test time introduces the drawback of having a discrepancy between the actual objective and that used for training.\nExploration of the student-teacher paradigm (Ba & Caruana, 2014) illuminates the potential for interplay between networks of different depth. In the model compression scenario, a deeper network (previously trained) guides and improves the learning of a shallower and faster student network (Ba & Caruana, 2014; Urban et al., 2017). This is accomplished by feeding unlabeled data through the teacher and having the student mimic the teacher’s soft output predictions. FitNets (Romero et al., 2015) explicitly couple students and teachers, forcing mimic behavior across several intermediate points in the network. Our fractal networks capture yet another alternative, in the form of implicit coupling, with the potential for bidirectional information flow between shallow and deep subnetworks.\nWidening networks, by using larger modules in place of individual layers, has also produced performance gains. For example, an Inception module (Szegedy et al., 2015) concatenates results of convolutional layers of different receptive field size. Stacking these modules forms the GoogLeNet architecture. Liao & Carneiro (2015) employ a variant with maxout in place of concatenation. Figure 1 makes apparent our connection with such work. As a fractal network deepens, it also widens. Moreover, note that stacking two 2D convolutional layers with the same spatial receptive field (e.g. 3ˆ 3) achieves a larger (5ˆ 5) receptive field. A horizontal cross-section of a fractal network is reminiscent of an Inception module, except with additional joins due to recursive structure."
    }, {
      "heading" : "3 FRACTAL NETWORKS",
      "text" : "We begin with a more formal presentation of the ideas sketched in Figure 1. Convolutional neural networks serve as our running example and, in the subsequent section, our experimental platform. However, it is worth emphasizing that our framework is more general. In principle, convolutional layers in Figure 1 could be replaced by a different layer type, or even a custom-designed module or subnetwork, in order to generate other fractal architectures.\nLet C denote the index of the truncated fractal fCp¨q. Our network’s structure, connections and layer types, is defined by fCp¨q. A network consisting of a single convolutional layer is the base case:\nf1pzq “ convpzq (1)\nWe define successive fractals recursively:\nfC`1pzq “ rpfC ˝ fCqpzqs ‘ rconvpzqs (2)\nwhere ˝ denotes composition and ‘ a join operation. When drawn in the style of Figure 1, C corresponds to the number of columns, or width, of network fCp¨q. Depth, defined to be the number of conv layers on the longest path between input and output, scales as 2C´1. Convolutional networks for classification typically intersperse pooling layers. We achieve the same by using fCp¨q as a building block and stacking it with subsequent pooling layers B times, yielding total depth B ¨ 2C´1. The join operation ‘ merges two feature blobs into one. Here, a blob is the result of a conv layer: a tensor holding activations for a fixed number of channels over a spatial domain. The channel count corresponds to the size of the filter set in the preceding conv layer. As the fractal is expanded, we collapse neighboring joins into a single join layer which spans multiple columns, as shown on the right side of Figure 1. The join layer merges all of its input feature blobs into a single output blob.\nSeveral choices seem reasonable for the action of a join layer, including concatenation and addition. We instantiate each join to compute the element-wise mean of its inputs. This is appropriate for convolutional networks in which channel count is set the same for all conv layers within a fractal block. Averaging might appear similar to ResNet’s addition operation, but there are critical differences:\n• ResNet makes clear distinction between pass-through and residual signals. In FractalNet, no signal is privileged. Every input to a join layer is the output of an immediately preceding conv layer. The network structure alone cannot identify any as being primary.\n• Drop-path regularization, as described next in Section 3.1, forces each input to a join to be individually reliable. This reduces the reward for even implicitly learning to allocate part of one signal to act as a residual for another.\n• Experiments show that we can extract high-performance subnetworks consisting of a single column (Section 4.2). Such a subnetwork is effectively devoid of joins, as only a single path is active throughout. They produce no signal to which a residual could be added.\nTogether, these properties ensure that join layers are not an alternative method of residual learning."
    }, {
      "heading" : "3.1 REGULARIZATION VIA DROP-PATH",
      "text" : "Dropout (Hinton et al., 2012) and drop-connect (Wan et al., 2013) modify interactions between sequential network layers in order to discourage co-adaptation. Since fractal networks contain additional macro-scale structure, we propose to complement these techniques with an analogous coarse-scale regularization scheme.\nFigure 2 illustrates drop-path. Just as dropout prevents co-adaptation of activations, drop-path prevents co-adaptation of parallel paths by randomly dropping operands of the join layers. This discourages the network from using one input path as an anchor and another as a corrective term (a configuration that, if not prevented, is prone to overfitting). We consider two sampling strategies:\n• Local: a join drops each input with fixed probability, but we make sure at least one survives. • Global: a single path is selected for the entire network. We restrict this path to be a single\ncolumn, thereby promoting individual columns as independently strong predictors.\nAs with dropout, signals may need appropriate rescaling. With element-wise means, this is trivial; each join computes the mean of only its active inputs.\nIn experiments, we train with dropout and a mixture model of 50% local and 50% global sampling for drop-path. We sample a new subnetwork each mini-batch. With sufficient memory, we can simultaneously evaluate one local sample and all global samples for each mini-batch by keeping separate networks and tying them together via weight sharing.\nWhile fractal connectivity permits the use of paths of any length, global drop-path forces the use of many paths whose lengths differ by orders of magnitude (powers of 2). The subnetworks sampled by drop-path thus exhibit large structural diversity. This property stands in contrast to stochastic depth regularization of ResNet, which, by virtue of using a fixed drop probability for each layer in a chain, samples subnetworks with a concentrated depth distribution (Huang et al., 2016b).\nGlobal drop-path serves not only as a regularizer, but also as a diagnostic tool. Monitoring performance of individual columns provides insight into both the network and training mechanisms, as Section 4.3 discusses in more detail. Individually strong columns of various depths also give users choices in the trade-off between speed (shallow) and accuracy (deep)."
    }, {
      "heading" : "3.2 DATA AUGMENTATION",
      "text" : "Data augmentation can reduce the need for regularization. ResNet demonstrates this, achieving 27.22% error rate on CIFAR-100 with augmentation compared to 44.76% without (Huang et al., 2016b). While augmentation benefits fractal networks, we show that drop-path provides highly effective regularization, allowing them to achieve competitive results even without data augmentation."
    }, {
      "heading" : "3.3 IMPLEMENTATION DETAILS",
      "text" : "We implement FractalNet using Caffe (Jia et al., 2014). Purely for convenience, we flip the order of pool and join layers at the end of a block in Figure 1. We pool individual columns immediately before the joins spanning all columns, rather than pooling once immediately after them.\nWe train fractal networks using stochastic gradient descent with momentum. As now standard, we employ batch normalization together with each conv layer (convolution, batch norm, then ReLU)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "The CIFAR, SVHN, and ImageNet datasets serve as testbeds for comparison to prior work and analysis of FractalNet’s internal behavior. We evaluate performance on the standard classification task associated with each dataset. For CIFAR and SVHN, which consist of 32ˆ 32 images, we set our fractal network to have 5 blocks (B “ 5) with 2ˆ 2 non-overlapping max-pooling and subsampling applied after each. This reduces the input 32ˆ 32 spatial resolution to 1ˆ 1 over the course of the entire network. A softmax prediction layer attaches at the end of the network. Unless otherwise noted, we set the number of filter channels within blocks 1 through 5 as p64, 128, 256, 512, 512q, mostly matching the convention of doubling the number of channels after halving spatial resolution.\nFor ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a). We use the same first and last layer as ResNet-34, but change the middle of the network to consist of 4 blocks (B “ 4), each of 8 layers (C “ 4 columns). We use a filter channel progression of p128, 256, 512, 1024q in blocks 1 through 4."
    }, {
      "heading" : "4.1 TRAINING",
      "text" : "For experiments using dropout, we fix drop rate per block at p0%, 10%, 20%, 30%, 40%q, similar to Clevert et al. (2016). Local drop-path uses 15% drop rate across the entire network.\n1Densely connected networks (DenseNets) are concurrent work, appearing subsequent to our original arXiv paper on FractalNet. A variant of residual networks, they swap addition for concatenation in the residual functional form. We report performance of their 250-layer DenseNet-BC network with growth rate k “ 24.\n2This deeper (4 column) FractalNet has fewer parameters. We vary column width: p128, 64, 32, 16q channels across columns initially, doubling each block except the last. A linear projection temporarily widens thinner columns before joins. As in Iandola et al. (2016), we switch to a mix of 1ˆ 1 and 3ˆ 3 convolutional filters.\nMethod Top-1 (%) Top-5 (%) VGG-16 28.07 9.33 ResNet-34 C 24.19 7.40 FractalNet-34 24.12 7.39\nTable 2: ImageNet (validation set, 10-crop).\nCols. Depth Params. Error (%) 1 5 0.3M 37.32 2 10 0.8M 30.71 3 20 2.1M 27.69 4 40 4.8M 27.38 5 80 10.2M 26.46 6 160 21.1M 27.38\nTable 3: Ultra-deep fractal networks (CIFAR-100++). Increasing depth greatly improves accuracy until eventual diminishing returns. Contrast with plain networks, which are not trainable if made too deep (Table 4).\nModel Depth Train Loss Error (%) Plain 5 0.786 36.62 Plain 10 0.159 32.47 Plain 20 0.037 31.31 Plain 40 0.580 38.84 Fractal Col #1 5 0.677 37.23 Fractal Col #2 10 0.141 32.85 Fractal Col #3 20 0.029 31.31 Fractal Col #4 40 0.016 31.75 Fractal Full 40 0.015 27.40\nTable 4: Fractal structure as a training apparatus (CIFAR-100++). Plain networks perform well if moderately deep, but exhibit worse convergence during training if instantiated with great depth. However, as a column trained within, and then extracted from, a fractal network with mixed drop-path, we recover a plain network that overcomes such depth limitation (possibly due to a student-teacher effect).\nWe run for 400 epochs on CIFAR, 20 epochs on SVHN, and 70 epochs on ImageNet. Our learning rate starts at 0.02 (for ImageNet, 0.001) and we train using stochastic gradient descent with batch size 100 (for ImageNet, 32) and momentum 0.9. For CIFAR/SVHN, we drop the learning rate by a factor of 10 whenever the number of remaining epochs halves. For ImageNet, we drop by a factor of 10 at epochs 50 and 65. We use Xavier initialization (Glorot & Bengio, 2010).\nA widely employed (Lin et al., 2013; Clevert et al., 2016; Srivastava et al., 2015; He et al., 2016a;b; Huang et al., 2016b; Targ et al., 2016) scheme for data augmentation on CIFAR consists of only horizontal mirroring and translation (uniform offsets in r´4, 4s), with images zero-padded where needed after mean subtraction. We denote results achieved using no more than this degree of augmentation by appending a “+” to the dataset name (e.g. CIFAR-100+). A “++” marks results reliant on more data augmentation; here exact schemes may vary. Our entry in this category is modest and simply changes the zero-padding to reflect-padding."
    }, {
      "heading" : "4.2 RESULTS",
      "text" : "Table 1 compares performance of FractalNet on CIFAR and SVHN with competing methods. FractalNet (depth 20) outperforms the original ResNet across the board. With data augmentation, our CIFAR-100 accuracy is close to that of the best ResNet variants. With neither augmentation nor regularization, FractalNet’s performance on CIFAR is superior to both ResNet and ResNet with stochastic depth, suggesting that FractalNet may be less prone to overfitting. Most methods perform similarly on SVHN. Increasing depth to 40, while borrowing some parameter reduction tricks (Iandola et al., 2016), reveals FractalNet’s performance to be consistent across a range of configuration choices.\nExperiments without data augmentation highlight the power of drop-path regularization. On CIFAR100, drop-path reduces FractalNet’s error rate from 35.34% to 28.20%. Unregularized ResNet is far behind (44.76%) and ResNet with stochastic depth (37.80%) does not catch up to our unregularized starting point of 35.34%. CIFAR-10 mirrors this story. With data augmentation, drop-path provides a boost (CIFAR-10), or does not significantly influence FractalNet’s performance (CIFAR-100).\nNote that the performance of the deepest column of the fractal network is close to that of the full network (statistically equivalent on CIFAR-10). This suggests that the fractal structure may be more important as a learning framework than as a final model architecture.\nTable 2 shows that FractalNet scales to ImageNet, matching ResNet (He et al., 2016a) at equal depth. Note that, concurrent with our work, refinements to the residual network paradigm further improve the state-of-the-art on ImageNet. Wide residual networks (Zagoruyko & Komodakis, 2016) of 34-layers reduce single-crop Top-1 and Top-5 validation error by approximately 2% and 1%, respectively, over\nResNet-34 by doubling feature channels in each layer. DenseNets (Huang et al., 2016a) substantially improve performance by building residual blocks that concatenate rather than add feature channels.\nTable 3 demonstrates that FractalNet resists performance degradation as we increase C to obtain extremely deep networks (160 layers for C “ 6). Scores in this table are not comparable to those in Table 1. For time and memory efficiency, we reduced block-wise feature channels to p16, 32, 64, 128, 128q and the batch size to 50 for the supporting experiments in Tables 3 and 4. Table 4 provides a baseline showing that training of plain deep networks begins to degrade by the time their depth reaches 40 layers. In our experience, a plain 160-layer completely fails to converge. This table also highlights the ability to use FractalNet and drop-path as an engine for extracting trained networks (columns) with the same topology as plain networks, but much higher test performance."
    }, {
      "heading" : "4.3 INTROSPECTION",
      "text" : "With Figure 3, we examine the evolution of a 40-layer FractalNet during training. Tracking columns individually (recording their losses when run as stand-alone networks), we observe that the 40-layer column initially improves slowly, but picks up once the loss of the rest of the network begins to stabilize. Contrast with a plain 40-layer network trained alone (dashed blue line), which never makes fast progress. The column has the same initial plateau, but subsequently improves after 25 epochs, producing a loss curve uncharacteristic of plain networks.\nWe hypothesize that the fractal structure triggers effects akin to deep supervision and lateral studentteacher information flow. Column #4 joins with column #3 every other layer, and in every fourth layer this join involves no other columns. Once the fractal network partially relies on the signal going through column #3, drop-path puts pressure on column #4 to produce a replacement signal when column #3 is dropped. This task has constrained scope. A particular drop only requires two consecutive layers in column #4 to substitute for one in column #3 (a mini student-teacher problem).\nThis explanation of FractalNet dynamics parallels what, in concurrent work, Greff et al. (2017) claim for ResNet. Specifically, Greff et al. (2017) suggest residual networks learn unrolled iterative estimation, with each layer performing a gradual refinement on its input representation. The deepest FractalNet column could behave in the same manner, with the remainder of the network acting as a scaffold for building smaller refinement steps by doubling layers from one column to the next.\nThese interpretations appear not to mesh with the conclusions of Veit et al. (2016), who claim that ensemble-like behavior underlies the success of ResNet. This is certainly untrue of some very deep networks, as FractalNet provides a counterexample: we can extract a single column (plain network topology) and it alone (no ensembling) performs nearly as well as the entire network. Moreover, the gradual refinement view may offer an alternative explanation for the experiments of Veit et al. (2016). If each layer makes only a small modification, removing one may look, to the subsequent portion of the network, like injecting a small amount of input noise. Perhaps noise tolerance explains the gradual performance degradation that Veit et al. (2016) observe when removing ResNet layers."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "Our experiments with fractal networks provide strong evidence that path length is fundamental for training ultra-deep neural networks; residuals are incidental. Key is the shared characteristic of FractalNet and ResNet: large nominal network depth, but effectively shorter paths for gradient propagation during training. Fractal architectures are arguably the simplest means of satisfying this requirement, and match residual networks in experimental performance. Fractal networks are resistant to being too deep; extra depth may slow training, but does not impair accuracy.\nWith drop-path, regularization of extremely deep fractal networks is intuitive and effective. Drop-path doubles as a method of enforcing speed (latency) vs. accuracy tradeoffs. For applications where fast responses have utility, we can obtain fractal networks whose partial evaluation yields good answers.\nOur analysis connects the internal behavior of fractal networks with phenomena engineered into other networks. Their substructure resembles hand-crafted modules used as components in prior work. Their training evolution may emulate deep supervision and student-teacher learning."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Jimmy Ba", "Rich Caruana" ],
      "venue" : null,
      "citeRegEx" : "Ba and Caruana.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba and Caruana.",
      "year" : 2014
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (ELUs)",
      "author" : [ "Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter" ],
      "venue" : null,
      "citeRegEx" : "Clevert et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2016
    }, {
      "title" : "ImageNet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Highway and residual networks learn unrolled iterative estimation",
      "author" : [ "Klaus Greff", "Rupesh Kumar Srivastava", "Jürgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "Greff et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2017
    }, {
      "title" : "Hypercolumns for object segmentation and fine-grained localization",
      "author" : [ "Bharath Hariharan", "Pablo Arbelaez", "Ross Girshick", "Jitendra Malik" ],
      "venue" : null,
      "citeRegEx" : "Hariharan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hariharan et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Identity mappings in deep residual networks. ECCV, 2016b",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and ă1MB model size",
      "author" : [ "Forrest N. Iandola", "Matthew W. Moskewicz", "Khalid Ashraf", "Song Han", "William J. Dally", "Kurt Keutzer" ],
      "venue" : null,
      "citeRegEx" : "Iandola et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Iandola et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : null,
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "Generalizing pooling functions in convolutional neural networks: Mixed",
      "author" : [ "Chen-Yu Lee", "Patrick W Gallagher", "Zhuowen Tu" ],
      "venue" : "gated, and tree. AISTATS,",
      "citeRegEx" : "Lee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent convolutional neural network for object recognition",
      "author" : [ "Ming Liang", "Xiaolin Hu" ],
      "venue" : null,
      "citeRegEx" : "Liang and Hu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liang and Hu.",
      "year" : 2015
    }, {
      "title" : "Reconstructive sparse code transfer for contour detection and semantic labeling",
      "author" : [ "Michael Maire", "Stella X. Yu", "Pietro Perona" ],
      "venue" : null,
      "citeRegEx" : "Maire et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Maire et al\\.",
      "year" : 2014
    }, {
      "title" : "All you need is a good init",
      "author" : [ "Dmytro Mishkin", "Jiri Matas" ],
      "venue" : null,
      "citeRegEx" : "Mishkin and Matas.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mishkin and Matas.",
      "year" : 2016
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y. Ng" ],
      "venue" : "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Path-SGD: Path-normalized optimization in deep neural networks",
      "author" : [ "Behnam Neyshabur", "Ruslan Salakhutdinov", "Nathan Srebro" ],
      "venue" : null,
      "citeRegEx" : "Neyshabur et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neyshabur et al\\.",
      "year" : 2015
    }, {
      "title" : "Fitnets: Hints for thin deep",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "nets. ICLR,",
      "citeRegEx" : "Romero et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "ICLR,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Scalable bayesian optimization using deep neural networks",
      "author" : [ "Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md Patwary", "Mostofa Ali", "Ryan P Adams" ],
      "venue" : null,
      "citeRegEx" : "Snoek et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2015
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "ICLR (workshop track),",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Resnet in resnet: Generalizing residual architectures",
      "author" : [ "Sasha Targ", "Diogo Almeida", "Kevin Lyman" ],
      "venue" : null,
      "citeRegEx" : "Targ et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Targ et al\\.",
      "year" : 2016
    }, {
      "title" : "Do deep convolutional nets really need to be deep and convolutional? ICLR, 2017",
      "author" : [ "Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana" ],
      "venue" : null,
      "citeRegEx" : "Urban et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Urban et al\\.",
      "year" : 2017
    }, {
      "title" : "Residual networks behave like ensembles of relatively shallow networks",
      "author" : [ "Andreas Veit", "Michael Wilber", "Serge Belongie" ],
      "venue" : null,
      "citeRegEx" : "Veit et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Veit et al\\.",
      "year" : 2016
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "Wan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Deep supervision not only arises automatically, but also drives a type of student-teacher learning (Ba & Caruana, 2014; Urban et al., 2017) internal to the network.",
      "startOffset" : 99,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Residual networks (He et al., 2016a), or ResNets, lead a recent and dramatic increase in both depth and accuracy of convolutional neural networks, facilitated by constraining the network to learn residuals. ResNet variants (He et al., 2016a;b; Huang et al., 2016b) and related architectures (Srivastava et al., 2015) employ the common technique of initializing and anchoring, via a pass-through channel, a network to the identity function. Training now differs in two respects. First, the objective changes to learning residual outputs, rather than unreferenced absolute mappings. Second, these networks exhibit a type of deep supervision (Lee et al., 2014), as near-identity layers effectively reduce distance to the loss. He et al. (2016a) speculate that the former, the residual formulation itself, is crucial.",
      "startOffset" : 19,
      "endOffset" : 742
    }, {
      "referenceID" : 9,
      "context" : "Without data augmentation, fractal networks, trained with drop-path and dropout (Hinton et al., 2012), exceed the performance of residual networks regularized via stochastic depth (Huang et al.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "Section 4 provides experimental comparisons to residual networks across the CIFAR-10, CIFAR-100 (Krizhevsky, 2009), SVHN (Netzer et al.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "Section 4 provides experimental comparisons to residual networks across the CIFAR-10, CIFAR-100 (Krizhevsky, 2009), SVHN (Netzer et al., 2011), and ImageNet (Deng et al.",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : ", 2011), and ImageNet (Deng et al., 2009) datasets.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "A striking example within the computer vision community is the improvement on the ImageNet (Deng et al., 2009) classification task when transitioning from AlexNet (Krizhevsky et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : ", 2009) classification task when transitioning from AlexNet (Krizhevsky et al., 2012) to VGG (Simonyan & Zisserman, 2015) to GoogLeNet (Szegedy et al.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "PReLUs (He et al., 2015), and ELUs (Clevert et al.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : ", 2015), and ELUs (Clevert et al., 2016).",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "Path-SGD (Neyshabur et al., 2015) offers an alternative normalization scheme.",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "Recent work further elaborates upon them (Maire et al., 2014; Hariharan et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "Recent work further elaborates upon them (Maire et al., 2014; Hariharan et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "Identity initialization also appears in the context of recurrent networks (Le et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 30,
      "context" : "In the model compression scenario, a deeper network (previously trained) guides and improves the learning of a shallower and faster student network (Ba & Caruana, 2014; Urban et al., 2017).",
      "startOffset" : 148,
      "endOffset" : 188
    }, {
      "referenceID" : 25,
      "context" : "FitNets (Romero et al., 2015) explicitly couple students and teachers, forcing mimic behavior across several intermediate points in the network.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : ", 2015), and ELUs (Clevert et al., 2016). Even with ReLUs, employing batch normalization (Ioffe & Szegedy, 2015) speeds training by reducing internal covariate shift. Good initialization can also ameliorate this problem (Glorot & Bengio, 2010; Mishkin & Matas, 2016). Path-SGD (Neyshabur et al., 2015) offers an alternative normalization scheme. Progress in optimization is somewhat orthogonal to our architectural focus, with the expectation that advances in either are ripe for combination. Notable ideas in architecture reach back to skip connections, the earliest example of a nontrivial routing pattern within a neural network. Recent work further elaborates upon them (Maire et al., 2014; Hariharan et al., 2015). Highway networks (Srivastava et al., 2015) and ResNet (He et al., 2016a;b) offer additional twists in the form of parameterized pass-through and gating. In work subsequent to our own, Huang et al. (2016a) investigate a ResNet variant with explicit skip connections.",
      "startOffset" : 19,
      "endOffset" : 925
    }, {
      "referenceID" : 1,
      "context" : ", 2015), and ELUs (Clevert et al., 2016). Even with ReLUs, employing batch normalization (Ioffe & Szegedy, 2015) speeds training by reducing internal covariate shift. Good initialization can also ameliorate this problem (Glorot & Bengio, 2010; Mishkin & Matas, 2016). Path-SGD (Neyshabur et al., 2015) offers an alternative normalization scheme. Progress in optimization is somewhat orthogonal to our architectural focus, with the expectation that advances in either are ripe for combination. Notable ideas in architecture reach back to skip connections, the earliest example of a nontrivial routing pattern within a neural network. Recent work further elaborates upon them (Maire et al., 2014; Hariharan et al., 2015). Highway networks (Srivastava et al., 2015) and ResNet (He et al., 2016a;b) offer additional twists in the form of parameterized pass-through and gating. In work subsequent to our own, Huang et al. (2016a) investigate a ResNet variant with explicit skip connections. These methods share distinction as the only other designs demonstrated to scale to hundreds of layers and beyond. ResNet’s building block uses the identity map as an anchor point and explicitly parameterizes an additive correction term (the residual). Identity initialization also appears in the context of recurrent networks (Le et al., 2015). A tendency of ResNet and highway networks to fall-back to the identity map may make their effective depth much smaller than their nominal depth. Some prior results hint at what we experimentally demonstrate in Section 4. Namely, reduction of effective depth is key to training extremely deep networks; residuals are incidental. Huang et al. (2016b) provide one clue in their work on stochastic depth: randomly dropping layers from ResNet during training, thereby shrinking network depth by a constant factor, provides additional performance benefit.",
      "startOffset" : 19,
      "endOffset" : 1680
    }, {
      "referenceID" : 1,
      "context" : ", 2015), and ELUs (Clevert et al., 2016). Even with ReLUs, employing batch normalization (Ioffe & Szegedy, 2015) speeds training by reducing internal covariate shift. Good initialization can also ameliorate this problem (Glorot & Bengio, 2010; Mishkin & Matas, 2016). Path-SGD (Neyshabur et al., 2015) offers an alternative normalization scheme. Progress in optimization is somewhat orthogonal to our architectural focus, with the expectation that advances in either are ripe for combination. Notable ideas in architecture reach back to skip connections, the earliest example of a nontrivial routing pattern within a neural network. Recent work further elaborates upon them (Maire et al., 2014; Hariharan et al., 2015). Highway networks (Srivastava et al., 2015) and ResNet (He et al., 2016a;b) offer additional twists in the form of parameterized pass-through and gating. In work subsequent to our own, Huang et al. (2016a) investigate a ResNet variant with explicit skip connections. These methods share distinction as the only other designs demonstrated to scale to hundreds of layers and beyond. ResNet’s building block uses the identity map as an anchor point and explicitly parameterizes an additive correction term (the residual). Identity initialization also appears in the context of recurrent networks (Le et al., 2015). A tendency of ResNet and highway networks to fall-back to the identity map may make their effective depth much smaller than their nominal depth. Some prior results hint at what we experimentally demonstrate in Section 4. Namely, reduction of effective depth is key to training extremely deep networks; residuals are incidental. Huang et al. (2016b) provide one clue in their work on stochastic depth: randomly dropping layers from ResNet during training, thereby shrinking network depth by a constant factor, provides additional performance benefit. We build upon this intuition through drop-path, which shrinks depth much more drastically. The success of deep supervision (Lee et al., 2014) provides another clue that effective depth is crucial. Here, an auxiliary loss, forked off mid-level layers, introduces a shorter path during backpropagation. The layer at the fork receives two gradients, originating from the main loss and the auxiliary loss, that are added together. Deep supervision is now common, being adopted, for example, by GoogLeNet (Szegedy et al., 2015). However, irrelevance of the auxiliary loss at test time introduces the drawback of having a discrepancy between the actual objective and that used for training. Exploration of the student-teacher paradigm (Ba & Caruana, 2014) illuminates the potential for interplay between networks of different depth. In the model compression scenario, a deeper network (previously trained) guides and improves the learning of a shallower and faster student network (Ba & Caruana, 2014; Urban et al., 2017). This is accomplished by feeding unlabeled data through the teacher and having the student mimic the teacher’s soft output predictions. FitNets (Romero et al., 2015) explicitly couple students and teachers, forcing mimic behavior across several intermediate points in the network. Our fractal networks capture yet another alternative, in the form of implicit coupling, with the potential for bidirectional information flow between shallow and deep subnetworks. Widening networks, by using larger modules in place of individual layers, has also produced performance gains. For example, an Inception module (Szegedy et al., 2015) concatenates results of convolutional layers of different receptive field size. Stacking these modules forms the GoogLeNet architecture. Liao & Carneiro (2015) employ a variant with maxout in place of concatenation.",
      "startOffset" : 19,
      "endOffset" : 3685
    }, {
      "referenceID" : 9,
      "context" : "Dropout (Hinton et al., 2012) and drop-connect (Wan et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 32,
      "context" : ", 2012) and drop-connect (Wan et al., 2013) modify interactions between sequential network layers in order to discourage co-adaptation.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "We implement FractalNet using Caffe (Jia et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "35 Generalized Pooling (Lee et al., 2016) 32.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 28,
      "context" : "92 All-CNN (Springenberg et al., 2014) - 33.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "72 ELU (Clevert et al., 2016) - 24.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "55 Scalable BO (Snoek et al., 2015) - 27.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "69 ResNet in ResNet (Targ et al., 2016) - 22.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "35 Generalized Pooling (Lee et al., 2016) 32.37 - 7.62 6.05 - 1.69 Recurrent CNN (Liang & Hu, 2015) 31.75 - 8.69 7.09 - 1.77 Multi-scale (Liao & Carneiro, 2015) 27.56 - 6.87 - 1.76 FitNet Romero et al. (2015) - 35.",
      "startOffset" : 24,
      "endOffset" : 209
    }, {
      "referenceID" : 6,
      "context" : "For ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a). We use the same first and last layer as ResNet-34, but change the middle of the network to consist of 4 blocks (B “ 4), each of 8 layers (C “ 4 columns).",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "For experiments using dropout, we fix drop rate per block at p0%, 10%, 20%, 30%, 40%q, similar to Clevert et al. (2016). Local drop-path uses 15% drop rate across the entire network.",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "As in Iandola et al. (2016), we switch to a mix of 1ˆ 1 and 3ˆ 3 convolutional filters.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "A widely employed (Lin et al., 2013; Clevert et al., 2016; Srivastava et al., 2015; He et al., 2016a;b; Huang et al., 2016b; Targ et al., 2016) scheme for data augmentation on CIFAR consists of only horizontal mirroring and translation (uniform offsets in r ́4, 4s), with images zero-padded where needed after mean subtraction.",
      "startOffset" : 18,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "A widely employed (Lin et al., 2013; Clevert et al., 2016; Srivastava et al., 2015; He et al., 2016a;b; Huang et al., 2016b; Targ et al., 2016) scheme for data augmentation on CIFAR consists of only horizontal mirroring and translation (uniform offsets in r ́4, 4s), with images zero-padded where needed after mean subtraction.",
      "startOffset" : 18,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "Increasing depth to 40, while borrowing some parameter reduction tricks (Iandola et al., 2016), reveals FractalNet’s performance to be consistent across a range of configuration choices.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "This explanation of FractalNet dynamics parallels what, in concurrent work, Greff et al. (2017) claim for ResNet.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "This explanation of FractalNet dynamics parallels what, in concurrent work, Greff et al. (2017) claim for ResNet. Specifically, Greff et al. (2017) suggest residual networks learn unrolled iterative estimation, with each layer performing a gradual refinement on its input representation.",
      "startOffset" : 76,
      "endOffset" : 148
    }, {
      "referenceID" : 31,
      "context" : "These interpretations appear not to mesh with the conclusions of Veit et al. (2016), who claim that ensemble-like behavior underlies the success of ResNet.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 31,
      "context" : "These interpretations appear not to mesh with the conclusions of Veit et al. (2016), who claim that ensemble-like behavior underlies the success of ResNet. This is certainly untrue of some very deep networks, as FractalNet provides a counterexample: we can extract a single column (plain network topology) and it alone (no ensembling) performs nearly as well as the entire network. Moreover, the gradual refinement view may offer an alternative explanation for the experiments of Veit et al. (2016). If each layer makes only a small modification, removing one may look, to the subsequent portion of the network, like injecting a small amount of input noise.",
      "startOffset" : 65,
      "endOffset" : 499
    }, {
      "referenceID" : 31,
      "context" : "These interpretations appear not to mesh with the conclusions of Veit et al. (2016), who claim that ensemble-like behavior underlies the success of ResNet. This is certainly untrue of some very deep networks, as FractalNet provides a counterexample: we can extract a single column (plain network topology) and it alone (no ensembling) performs nearly as well as the entire network. Moreover, the gradual refinement view may offer an alternative explanation for the experiments of Veit et al. (2016). If each layer makes only a small modification, removing one may look, to the subsequent portion of the network, like injecting a small amount of input noise. Perhaps noise tolerance explains the gradual performance degradation that Veit et al. (2016) observe when removing ResNet layers.",
      "startOffset" : 65,
      "endOffset" : 751
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a design strategy for neural network macro-architecture based on selfsimilarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of highperformance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.",
    "creator" : "LaTeX with hyperref package"
  }
}