{
  "name" : "441.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Yacine Jernite" ],
    "emails" : [ "jernite@cs.nyu.edu", "egrave@fb.com", "ajoulin@fb.com", "tmikolov@fb.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The class of Recurrent Neural Network models (RNNs) is particularly well suited to dealing with sequential data, and has been successfully applied to a diverse array of tasks, such as language modeling and speech recognition (Mikolov, 2012), machine translation (Mikolov, 2012; Cho et al., 2014a), or acoustic modeling (Robinson et al., 1993; Graves & Jaitly, 2014) among others. Two factors have been instrumental in allowing this paradigm to be so widely adopted and give rise to the aforementioned successes. On the one hand, recent advances in both hardware and software have had a significant role in bringing the training of recurrent models to tractable time periods. On the other hand, novel units and architectures have allowed recurrent networks to model certain features of sequential data better than Elman’s simple RNN architecture (Elman, 1990). These include such developments as the LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014a) units, which can more easily learn to model long range interactions (Chung et al., 2014), or attention mechanisms that allow the model to focus on a specific part of its history when making a prediction (Bahdanau et al., 2014). In this work, we focus on another feature of recurrent networks: the ability to efficiently model processes happening at different and possibly varying time scales.\nMost existing recurrent models take one of two approaches regarding the amount of computation they require. Either the computational load is constant over time, or it follows a fixed (or deterministic) schedule (Koutnı́k et al., 2014), (Mikolov et al., 2014). The latter approach has proven especially useful when dealing with sequences which reflect processes taking place at different lev-\n∗Work done at Facebook AI Research\nels (and time scales) (Bojanowski et al., 2015). However, we believe that taking a more flexible approach could prove useful.\nConsider sequential data such as video feeds, audio signal, or language. In video data, there are time periods where the frames differ very slightly, and where the underlying model should probably do much less computation than when the scene completely changes. When modeling speech from an audio signal, it is also reasonable to expect that the model should be able do little to no computation during silences. Finally, in the case of character level language modeling, having more computational power at word boundaries can certainly help: after reading the left context The prime. . . , the model should be able to put a higher likelihood on the sequence of characters that make up the word minister. However, we can take this idea one step further: after reading The prime min. . . , the next few characters are almost deterministic, and the model should require little computation to predict the sequence i-s-t-e-r.\nIn this work, we show how to modify two commonly used recurrent unit architectures, namely the Elman and Gated Recurrent Unit, to obtain their variable computation counterparts. This gives rise to two new architecture, the Variable Computation RNN and Variable Computation GRU (VCRNN and VCGRU), which take advantage of these phenomena by deciding at each time step how much computation is required based on the current hidden state and input. We show that the models learn time patterns of interest, can perform fewer operations, and may even take advantage of these time structures to produce better predictions than the constant computation versions.\nWe start by giving an overview of related work in Section 2, provide background on the class of Recurrent Neural Networks in Section 3, describe our model and learning procedure in Section 4, and present experimental results on music as well as bit and character level language modeling in section 5. Finally, Section 6 concludes and lays out possible directions for future work."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "How to properly handle sequences which reflect processes happening at different time scales has been a widely explored question. Among the proposed approaches, a variety of notable systems based on Hidden Markov Models (HMMs) have been put forward in the last two decades. The Factorial HMM model of (Ghahramani & Jordan, 1997) (and its infinite extension in (Gael et al., 2008)) use parallel interacting hidden states to model concurrent processes. While there is no explicit handling of different time scales, the model achieves good held-out likelihood on Bach chorales, which exhibit multi-scale behaviors. The hierarchical HMM model of (Fine et al., 1998) and (Murphy & Paskin, 2001) takes a more direct approach to representing multiple scales of processes. In these works, the higher level HMM can recursively call sub-HMMs to generate short sequences without changing its state, and the authors show a successful application to modeling cursive writing. Finally, the Switching State-Space Model of (Ghahramani & Hinton, 2000) combines HMMs and Linear Dynamical Systems: in this model, the HMM is used to switch between LDS parameters, and the experiments show that the HMM learns higher-level, slower dynamics than the LDS.\nOn the side of Recurrent Neural Networks, the idea that the models should have mechanisms that allow them to handle processes happening at different time scales is not a new one either. On the one hand, such early works as (Schmidhuber, 1991) and (Schmidhuber, 1992) already presented a two level architecture, with an “automatizer” acting on every time step and a “chunker” which should only be called when the automatizer fails to predict the next item, and which the author hypothesizes learns to model slower scale processes. On the other hand, the model proposed in (Mozer, 1993) has slow-moving units as well as regular ones, where the slowness is defined by a parameter τ ∈ [0, 1] deciding how fast the representation changes by taking a convex combination of the previous and predicted hidden state.\nBoth these notions, along with different approaches to multi-scale sequence modeling, have been developed in more recent work. (Mikolov et al., 2014) expand upon the idea of having slow moving units in an RNN by proposing an extension of the Elman unit which forces parts of the transition matrix to be close to the identity. The idea of having recurrent layers called at different time steps has also recently regained popularity. The Clockwork RNN of (Koutnı́k et al., 2014), for example, has RNN layers called every 1, 2, 4, 8, etc. . . time steps. The conditional RNN of (Bojanowski et al., 2015) takes another approach by using known temporal structure in the data: in the character level\nlevel language modeling application, the first layer is called for every character, while the second is only called once per word. It should also be noted that state-of-the art results for language models have been obtained using multi-layer RNNs (Józefowicz et al., 2016), where the higher layers can in theory model slower processes. However, introspection in these models is more challenging, and it is difficult to determine whether they are actually exhibiting significant temporal behaviors.\nFinally, even more recent efforts have considered using dynamic time schedules. (Chung et al., 2016) presents a multi-layer LSTM, where each layer decides whether or not to activate the next one at every time step. They show that the model is able to learn sensible time behaviors and achieve good perplexity on their chosen tasks. Another implementation of the general concept of adaptive timedependent computation is presented in (Graves, 2016). In that work, the amount of computation performed at each time step is varied not by calling units in several layers, but rather by having a unique RNN perform more than one update of the hidden state on a single time step. There too, the model can be shown to learn an intuitive time schedule.\nIn this paper, we present an alternative view of adaptive computation, where a single Variable Computation Unit (VCU) decides dynamically how much of its hidden state needs to change, leading to both savings in the number of operations per time step and the possibility for the higher dimensions of the hidden state to keep longer term memory."
    }, {
      "heading" : "3 RECURRENT NEURAL NETWORKS",
      "text" : "Let us start by formally defining the class of Recurrent Neural Networks (RNNs). For tasks such as language modeling, we are interested in defining a probability distribution over sequences w = (w1, . . . , wT ). Using the chain rule, the negative log likelihood of a sequence can be written:\nL(w) = − T∑ t=1 log ( p ( wt|F (w1, . . . , wt−1) )) . (1)\nwhere F is a filtration, a function which summarizes all the relevant information from the past. RNNs are a class of models that can read sequences of arbitrary length to provide such a summary in the form of a hidden state ht ≈ F(w1, . . . , wt−1), by applying the same operation (recurrent unit) at each time step. More specifically, the recurrent unit is defined by a recurrence function g which takes as input the previous hidden state ht−1 at each time step t, as well as a representation of the input xt (where ht−1 and xt are D-dimensional vectors), and (with the convention h0 = 0,) outputs the new hidden state:\nht = g(ht−1, xt) (2)\nElman Unit. The unit described in (Elman, 1990) is often considered to be the standard unit. It is parametrized by U and V , which are square, D-dimensional transition matrices, and uses a sigmoid non-linearity to obtain the new hidden state:\nht = tanh(Uht−1 + V xt). (3)\nIn the Elman unit, the bulk of the computation comes from the matrix multiplications, and the cost per time step is O(D2). In the following section, we show a simple modification of the unit which allows it to reduce this cost significantly.\nGated Recurrent Unit. The Gated Recurrent Unit (GRU) was introduced in (Cho et al., 2014b). The main difference between the GRU and Elman unit consists in the model’s ability to interpolate between a proposed new hidden state and the current one, which makes it easier to model longer range dependencies. More specifically, at each time step t, the model computes a reset gate rt, an update gate zt, a proposed new hidden state h̃t and a final new hidden state ht as follows:\nrt = σ(Urht−1 + Vrxt), zt = σ(Uzht−1 + Vzxt) (4)\nh̃t = tanh(U(rt ht−1) + V xt) (5) And: ht = zt h̃t + (1− zt) ht−1 (6) Where denotes the element-wise product."
    }, {
      "heading" : "4 VARIABLE COMPUTATION RNN",
      "text" : "As noted in the previous section, the bulk of the computation in the aforementioned settings comes from the linear layers; a natural option to reduce the number of operations would then be to only apply the linear transformations to a sub-set of the hidden dimensions. These could in theory correspond to any sub-set indices in {1, . . . , D}; however, we want a setting where the computational cost of the choice is much less than the cost of computing the new hidden state. Thus, we only consider the sets of first d dimensions of RD, so that there is a single parameter d to compute.\nOur Variable Computation Units (VCUs) implement this idea using two modules: a scheduler decides how many dimensions need to be updated at the current time step, and the VCU performs a partial update of its hidden state accordingly, as illustrated in Figure 1. Section 4.1 formally describes the scheduler and partial update operations, and Section 4.2 outlines the procedure to jointly learn both modules."
    }, {
      "heading" : "4.1 MODEL DESCRIPTION",
      "text" : "Scheduler. The model first needs to decide how much computation is required at the current time step. To make that decision, the recurrent unit has access to the current hidden state and input; this way, the model can learn to ignore an uninformative input, or to decide on more computation when an it is unexpected given the current hidden state. The scheduler is then defined as a function m : R2D → [0, 1] which decides what portion of the hidden state to change based on the current hidden and input vectors. In this work, we decide to implement it as a simple log-linear function with parameter vectors u and v, and bias b, and at each time step t, we have:\nmt = σ(u · ht−1 + v · xt + b). (7)\nPartial update. Once the scheduler has decided on a computation budget mt, the VCU needs to perform a partial update of the first dmtDe dimensions of its hidden state. Recall the hidden state ht−1 is a D-dimensional vector. Given a smaller dimension d ∈ {1, . . . , D}, a partial update of the hidden state would take the following form. Let gd be the d-dimensional version of the model’s recurrence function g as defined in Equation 2, which uses the upper left d by d square sub-matrices of the linear transformations (Ud, Vd, . . .), and hdt−1 and x d t denote the first d elements of ht−1 and xt. We apply gd to hdt−1 and x d t , and carry dimensions d + 1 to D from the previous hidden state, so the new hidden state ht is defined by:\nhdt = gd(h d t−1, x d t ) and ∀i > d, ht,i = ht−1,i.\nSoft mask. In practice, the transition function we just defined would require making a hard choice at each time step of the number of dimensions to be updated, which makes the model non-differentiable and can significantly complicate optimization. Instead, we approximate the hard choice by using a gate function to apply a soft mask. Given mt ∈ [0, 1] and a sharpness parameter λ, we use the gating vector et ∈ RD defined by:\n∀i ∈ 1, . . . , D, (et)i = Thres ( σ ( λ(mtD − i) )) , (8)\nwhere Thres maps all values greater than 1− and smaller than to 1 and 0 respectively. That way, the model performs an update using the first (mt ×D + η) dimensions of the hidden state, where η goes to 0 as λ increases, and leaves its last ((1−mt)×D − η) dimensions unchanged. Thus, if g is the recurrence function defined in Equation 2, we have:\nh̄t−1 = et ht−1, īt = et xt, and ht = et g(h̄t−1, x̄t) + (1− et) ht. (9) The computational cost of this model at each step t, defined as the number of multiplications involving possibly non-zero elements is then O(m2tD\n2). The construction of the VCRNN and VCGRU architectures using this method is described in the appendix."
    }, {
      "heading" : "4.2 LEARNING",
      "text" : "Since the soft mask et is a continuous function of the model parameters, the scheduler can be learned through back-propagation. However, we have found that the naive approach of using a fixed sharpness parameter and simply minimizing the negative log-likelihood defined in Equation 1 led to the model being stuck in a local optimum which updates all dimensions at every step. We found that the following two modifications allowed the model to learn better parametrizations.\nFirst, we can encourage mt to be either close or no greater than a target m̄ at all time by adding a penalty term Ω to the objective. For example, we can apply a `1 or `2 penalty to values of mt that are greater than the target, or that simply diverge from it (in which case we also discourage the model from using too few dimensions). The cost function defined in Equation 1 then becomes:\nO(w, U, V,O, u, v, b) = L(w, U, V,O, u, v, b) + Ω(m, m̄). (10)\nSecondly, for the model to be able to explore the effect of using fewer or more dimensions, we need to start training with a smooth mask (small λ parameter), since for small values of λ, the model actually uses the whole hidden state. We can then gradually increase the sharpness parameter until the model truly does a partial update."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We ran experiments with the Variable Computation variants of the Elman and Gated Recurrent Units (VCRNN and VCGRU respectively) on several sequence modeling tasks. All experiments were run using a symmetrical `1 penalty on the scheduler m, that is, penalizing mt when it is greater or smaller than target m̄, with m̄ taking various values in the range [0.2, 0.5]. In all experiments, we start with a sharpness parameter λ = 0.1, and increase it by 0.1 per epoch to a maximum value of 1.\nIn each of our experiments, we are interested in investigating two specific aspects of our model. On the one hand, do the time patterns that emerge agree with our intuition of the time dynamics expressed in the data? On the other hand, does the Variable Computation Unit (VCU) yield a good predictive model? More specifically, does it lead to lower perplexity than a constant computation counterpart which performs as many or more operations? In order to be able to properly assess the efficiency of the model, and since we do not know a priori how much computation the VCU uses, we always report the “equivalent RNN” dimension (noted as RNN-d in Table 3) along with the performance on test data, i.e. the dimension of an Elman RNN that would have performed the same amount of computation. Note that the computational complexity gains we refer to are exclusively in terms of lowering the number of operations, which does not necessarily correlate with a speed up of training when using general purpose GPU kernels; it is however a prerequisite to achieving such a speed up with the proper implementation, motivating our effort.\nWe answer both of these questions on the tasks of music modeling, bit and character level language modeling on the Penn Treebank text, and character level language modeling on the Text8 data set as well as two languages from the Europarl corpus."
    }, {
      "heading" : "5.1 MUSIC MODELING",
      "text" : "We downloaded a corpus of Irish traditional tunes from https://thesession.org and split them into a training validation and test of 16,000 (2.4M tokens), 1,511 (227,000 tokens) and 2,000 (288,000 tokens) melodies respectively. Each sub-set includes variations of melodies, but no melody has variations across subsets. We consider each (pitch, length) pair to be a different symbol; with rests and bar symbols, this comes to a total vocabulary of 730 symbols.\nTable 1 compares the perplexity on the test set to Elman RNNs with equivalent computational costs: an VCRNN with hidden dimension 500 achieves better perplexity with fewer operations than an RNN with dimension 250.\nLooking at the output of the scheduler on the validation set also reveals some interesting patterns. First, bar symbols are mostly ignored: the average value of mt on bar symbols is 0.14, as opposed to 0.46 on all others. This is not surprising: our pre-processing does not handle polyphony or time signatures, so bars en up having different lengths. The best thing for the model to do is then just to ignore them and focus on the melody. Similarly, the model spends lest computation on rests (0.34 average mt), and pays less attention to repeated notes (0.51 average for mt on the first note of a repetition, 0.45 on the second).\nWe also notice that the model needs to do more computation on fast passages, which often have richer ornamentation, as illustrated in Table 2. While it is difficult to think a priori of all the sorts of behaviors that could be of interest, these initial results certainly show a sensible behavior of the scheduler on the music modeling task."
    }, {
      "heading" : "5.2 BIT AND CHARACTER LEVEL LANGUAGE MODELING",
      "text" : "We also chose to apply our model to the tasks of bit level and character level language modeling. Those appeared as good applications since we know a priori what kind of temporal structure to look for: ASCII encoding means that we expect a significant change (change of character) every 8 bits in bit level modeling, and we believe the structure of word units to be useful when modeling text at the character level."
    }, {
      "heading" : "5.2.1 PENN TREEBANK AND TEXT8",
      "text" : "We first ran experiments on two English language modeling tasks, using the Penn TreeBank and Text8 data sets. We chose the former as it is a well studied corpus, and one of the few corpora for which people have reported bit-level language modeling results. It is however quite small for our purposes, with under 6M characters, which motivated us to apply our models to the larger Text8 data set (100M characters). Table 3 shows bit per bit and bit per character results for bit and character level language modeling. We compare our results with those obtained with standard Elman RNN, GRU, and LSTM networks, as well as with the Conditional RNN of (Bojanowski et al., 2015).\nQuantitative Results. We first compare the VCRNN to the regular Elman RNN, as well as to the Conditional RNN of (Bojanowski et al., 2015), which combines two layers running at bit and character level for bit level modeling, or character and word level for character level modeling. For bit level language modeling, the VCRNN not only performs fewer operations than the standard unit, it also achieves better performance. For character level modeling, the Elman model using a hidden dimension of 1024 achieved 1.47 bits per character, while our best performing VCRNN does slightly better while only requiring as much computation as a dimension 760 Elman unit. While we do slightly more computation than the Conditional RNN, it should be noted that our model is not explicitly given word-level information: it learns how to summarize it from character-level input.\nThe comparison between the constant computation and Variable Computation GRU (VCGRU) follows the same pattern, both on the PTB and Text8 corpora. On PTB, the VCGRU with the best validation perplexity performs as well as a GRU (and LSTM) of the same dimension with less than half the number of operations. On Text8, the VCGRU models with various values of the target m̄ always achieve better perplexity than other models performing similar or greater numbers of operations. It should be noted that none of the models we ran on Text8 overfits significantly (the training and validation perplexities are the same), which would indicate that the gain is not solely a matter of regularization.\nBit Level Scheduler. The scheduler in the bit level language model manages to learn the structure of ASCII encoding: Figure 2 shows that the higher dimensions are modified roughly every 8 bits. We also created some artificial data by taking the PTB text and adding 8 or 24 0 bits between each character. Figure 2, shows that the model learns to mostly ignore these “buffers”, doing most of its computation on actual characters.\nCharacter Level Scheduler. On character level language modeling, the scheduler learns to make use of word boundaries and some language structures. Figure 3 shows that the higher dimensions are used about once per words, and in some cases, we even observe a spike at the end of each morpheme (long-stand-ing, as shown in Figure 5). While we provide results for the VCRNN specifically in this Section, the VCGRU scheduler follows the same patterns."
    }, {
      "heading" : "5.2.2 EUROPARL CZECH AND GERMAN",
      "text" : "We also ran our model on two languages form the Europarl corpus. We chose Czech, which has a larger alphabet than other languages in the corpus, and German , which is a language that features long composite words without white spaces to indicate a new unit. Both are made up of about 20M characters. We tried two settings. In the “guide” setting, we use the penalty on mt to encourage the model to use more dimensions on white spaces. The “learn” setting is fully unsupervised, and encourages lower values of mt across the board.\nFigure 4 shows that both perform similarly on the Czech dataset, achieving better performance more efficiently than the standard RNN. On German, the guided settings remains slightly more efficient than the fully learned one, but both are more efficient than the RNN and achieve the same performance when using more dimensions. Both learn to use more dimensions at word boundaries as shown in Figure 3. The German model also appears to be learning interesting morphology (Luftver-kehrs, eben-falls in Figure 3, An-satz, Um-welt-freund-lich in Figure 5), and grammar (focusing on case markers at the end of articles, Figure 5)."
    }, {
      "heading" : "6 CONCLUSION AND FUTURE WORK",
      "text" : "In this work, we have presented two kinds of Variable Computation recurrent units: the VCRNN and VCGRU, which modify the Elman and Gated Recurrent Unit respectively to allow the models to achieve better performance with fewer operations, and can be shown to find time patterns of interest in sequential data. We hope that these encouraging results will open up paths for further exploration of adaptive computation paradigms in neural networks in general, which could lead to more computation-efficient models, better able to deal with varying information flow or multi-scale processes. We also see a few immediate possibilities for extensions of this specific model. For example, the same idea of adaptive computation can similarly be applied to yet other commonly used recurrent units, such as LSTMs, or to work within the different layers of a stacked architecture, and we are working on adapting our implementation to those settings. We also hope to investigate the benefits of using stronger supervision signals to train the scheduler, such as the entropy of the prediction, to hopefully push our current results even further."
    }, {
      "heading" : "A APPENDIX",
      "text" : "A.1 NEW UNITS: VCRNN AND VCGRU\nWe apply the method outlined in the previous paragraph to two commonly used architecture. Recall that, given a proportion of dimensions to use mt ∈ [0, 1] and a sharpness parameter λ, we the gating vector et ∈ RD is defined as:\n∀i ∈ 1, . . . , D, (et)i = Thres ( σ ( λ(mtD − i) )) . (11)\nThe masked versions of the previous hidden and current input vectors respectively are then:\nh̄t−1 = et ht−1 and īt = et xt. (12)\nFirst, we derive a variable computation version of the Elman RNN to get the Variable Computation Recurrent Neural Network (VCRNN) by transforming Equation 3 as follows:\nht = et g(h̄t−1, x̄t) + (1− et) ht. (13)\nSecondly, we obtain the Variable Computation Gated Recurrent Unit (VCGRU) by deriving the variable computation of the GRU architecture. This is achieved by modifying Equations 4 to 6 as follows:\nrt = σ(Urh̄t−1 + Vrx̄t), zt = et σ(Uzh̄t−1 + Vzx̄t) (14)\nh̃t = tanh(U(rt h̄t−1) + V x̄t) (15) And:\nht = zt h̃t + (1− zt) ht−1 (16)"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Alternative structures for character-level rnns",
      "author" : [ "Piotr Bojanowski", "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "CoRR, abs/1511.06303,",
      "citeRegEx" : "Bojanowski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "In Proceedings of SSST@EMNLP",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1412.3555,",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Chung et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L. Elman" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "Elman.,? \\Q1990\\E",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "The hierarchical hidden markov model: Analysis and applications",
      "author" : [ "Shai Fine", "Yoram Singer", "Naftali Tishby" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Fine et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fine et al\\.",
      "year" : 1998
    }, {
      "title" : "The infinite factorial hidden markov model",
      "author" : [ "Jurgen Van Gael", "Yee Whye Teh", "Zoubin Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems 21,",
      "citeRegEx" : "Gael et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gael et al\\.",
      "year" : 2008
    }, {
      "title" : "Variational learning for switching state-space models",
      "author" : [ "Zoubin Ghahramani", "Geoffrey E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Ghahramani and Hinton.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ghahramani and Hinton.",
      "year" : 2000
    }, {
      "title" : "Factorial hidden markov models",
      "author" : [ "Zoubin Ghahramani", "Michael I. Jordan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Ghahramani and Jordan.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ghahramani and Jordan.",
      "year" : 1997
    }, {
      "title" : "Adaptive computation time for recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1603.08983,",
      "citeRegEx" : "Graves.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2016
    }, {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Graves and Jaitly.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves and Jaitly.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Józefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "Józefowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Józefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "A clockwork RNN",
      "author" : [ "Jan Koutnı́k", "Klaus Greff", "Faustino J. Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Koutnı́k et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Koutnı́k et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical language models based on neural networks",
      "author" : [ "Tomáš Mikolov" ],
      "venue" : "Brno University of Technology,",
      "citeRegEx" : "Mikolov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov.",
      "year" : 2012
    }, {
      "title" : "Learning longer memory in recurrent neural networks",
      "author" : [ "Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michaël Mathieu", "Marc’Aurelio Ranzato" ],
      "venue" : "CoRR, abs/1412.7753,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2014
    }, {
      "title" : "Induction of multiscale temporal structure",
      "author" : [ "Michael C Mozer" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Mozer.,? \\Q1993\\E",
      "shortCiteRegEx" : "Mozer.",
      "year" : 1993
    }, {
      "title" : "Linear-time inference in hierarchical hmms",
      "author" : [ "Kevin P. Murphy", "Mark A. Paskin" ],
      "venue" : "NIPS 2001, December",
      "citeRegEx" : "Murphy and Paskin.,? \\Q2001\\E",
      "shortCiteRegEx" : "Murphy and Paskin.",
      "year" : 2001
    }, {
      "title" : "A neural network based, speaker independent, large vocabulary, continuous speech recognition system: the WERNICKE project",
      "author" : [ "Tony Robinson", "Luı́s B. Almeida", "Jean-Marc Boite", "Hervé Bourlard", "Frank Fallside", "Mike Hochberg", "Dan J. Kershaw", "Phil Kohn", "Yochai Konig", "Nelson Morgan", "João Paulo Neto", "Steve Renals", "Marco Saerens", "Chuck Wooters" ],
      "venue" : "In Third European Conference on Speech Communication and Technology,",
      "citeRegEx" : "Robinson et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Robinson et al\\.",
      "year" : 1993
    }, {
      "title" : "Neural sequence chunkers",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "Schmidhuber.,? \\Q1991\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1991
    }, {
      "title" : "Learning complex, extended sequences using the principle of history compression",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmidhuber.,? \\Q1992\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1992
    }, {
      "title" : "Architectural complexity measures of recurrent neural networks",
      "author" : [ "Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systemsn,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "The class of Recurrent Neural Network models (RNNs) is particularly well suited to dealing with sequential data, and has been successfully applied to a diverse array of tasks, such as language modeling and speech recognition (Mikolov, 2012), machine translation (Mikolov, 2012; Cho et al.",
      "startOffset" : 225,
      "endOffset" : 240
    }, {
      "referenceID" : 16,
      "context" : "The class of Recurrent Neural Network models (RNNs) is particularly well suited to dealing with sequential data, and has been successfully applied to a diverse array of tasks, such as language modeling and speech recognition (Mikolov, 2012), machine translation (Mikolov, 2012; Cho et al., 2014a), or acoustic modeling (Robinson et al.",
      "startOffset" : 262,
      "endOffset" : 296
    }, {
      "referenceID" : 20,
      "context" : ", 2014a), or acoustic modeling (Robinson et al., 1993; Graves & Jaitly, 2014) among others.",
      "startOffset" : 31,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "On the other hand, novel units and architectures have allowed recurrent networks to model certain features of sequential data better than Elman’s simple RNN architecture (Elman, 1990).",
      "startOffset" : 170,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : ", 2014a) units, which can more easily learn to model long range interactions (Chung et al., 2014), or attention mechanisms that allow the model to focus on a specific part of its history when making a prediction (Bahdanau et al.",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : ", 2014), or attention mechanisms that allow the model to focus on a specific part of its history when making a prediction (Bahdanau et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "Either the computational load is constant over time, or it follows a fixed (or deterministic) schedule (Koutnı́k et al., 2014), (Mikolov et al.",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : ", 2014), (Mikolov et al., 2014).",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "els (and time scales) (Bojanowski et al., 2015).",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "The Factorial HMM model of (Ghahramani & Jordan, 1997) (and its infinite extension in (Gael et al., 2008)) use parallel interacting hidden states to model concurrent processes.",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "The hierarchical HMM model of (Fine et al., 1998) and (Murphy & Paskin, 2001) takes a more direct approach to representing multiple scales of processes.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "On the one hand, such early works as (Schmidhuber, 1991) and (Schmidhuber, 1992) already presented a two level architecture, with an “automatizer” acting on every time step and a “chunker” which should only be called when the automatizer fails to predict the next item, and which the author hypothesizes learns to model slower scale processes.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "On the one hand, such early works as (Schmidhuber, 1991) and (Schmidhuber, 1992) already presented a two level architecture, with an “automatizer” acting on every time step and a “chunker” which should only be called when the automatizer fails to predict the next item, and which the author hypothesizes learns to model slower scale processes.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, the model proposed in (Mozer, 1993) has slow-moving units as well as regular ones, where the slowness is defined by a parameter τ ∈ [0, 1] deciding how fast the representation changes by taking a convex combination of the previous and predicted hidden state.",
      "startOffset" : 41,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "(Mikolov et al., 2014) expand upon the idea of having slow moving units in an RNN by proposing an extension of the Elman unit which forces parts of the transition matrix to be close to the identity.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "The Clockwork RNN of (Koutnı́k et al., 2014), for example, has RNN layers called every 1, 2, 4, 8, etc.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "The conditional RNN of (Bojanowski et al., 2015) takes another approach by using known temporal structure in the data: in the character level",
      "startOffset" : 23,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "It should also be noted that state-of-the art results for language models have been obtained using multi-layer RNNs (Józefowicz et al., 2016), where the higher layers can in theory model slower processes.",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "(Chung et al., 2016) presents a multi-layer LSTM, where each layer decides whether or not to activate the next one at every time step.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Another implementation of the general concept of adaptive timedependent computation is presented in (Graves, 2016).",
      "startOffset" : 100,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "The unit described in (Elman, 1990) is often considered to be the standard unit.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "We compare our results with those obtained with standard Elman RNN, GRU, and LSTM networks, as well as with the Conditional RNN of (Bojanowski et al., 2015).",
      "startOffset" : 131,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "CRNN refers to the Conditional RNN from (Bojanowski et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "∗From (Zhang et al., 2016)",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "We first compare the VCRNN to the regular Elman RNN, as well as to the Conditional RNN of (Bojanowski et al., 2015), which combines two layers running at bit and character level for bit level modeling, or character and word level for character level modeling.",
      "startOffset" : 90,
      "endOffset" : 115
    } ],
    "year" : 2017,
    "abstractText" : "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence’s time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}