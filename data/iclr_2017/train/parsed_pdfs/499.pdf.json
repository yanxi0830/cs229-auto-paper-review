{
  "name" : "499.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "David Ha", "Andrew M. Dai" ],
    "emails" : [ "hadavid@google.com", "adai@google.com", "qvl@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In this work, we consider an approach of using a small network (called a “hypernetwork\") to generate the weights for a larger network (called a main network). The behavior of the main network is the same as with any usual neural network: it learns to map some raw inputs to their desired targets; whereas the hypernetwork takes a set of inputs that contain information about the structure of the weights and generates the weights for that layer.\nThe focus of this work is to use hypernetworks to generate weights for recurrent networks (RNN). In this case, the weights Wt for the main RNN at step t is a function of the input to the hidden state of the main RNN at the previous step ht−1 and the input at the current time step xt. This weight-generation scheme allows approximate weight-sharing across layers of the main RNN.\nWe perform experiments to investigate the behaviors of hypernetworks in a range of contexts and find that hypernetworks mix well with other techniques such as batch normalization and layer normalization. Our main result is that hypernetworks can generate non-shared weights for LSTM that work better than the standard version of LSTM (Hochreiter & Schmidhuber, 1997). On language modelling tasks with character Penn Treebank, Hutter Prize Wikipedia datasets, hypernetworks for LSTM achieve near state-of-the-art results. On a handwriting generation task with IAM handwriting dataset, hypernetworks for LSTM achieves good quantitative and qualitative results. On machine translation, hypernetworks for LSTM also obtain state-of-the-art performance on the WMT’14 en→fr benchmark."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Our approach is inspired by methods in evolutionary computing, where it is difficult to directly operate in large search spaces consisting of millions of weight parameters. A more efficient method is to evolve a smaller network to generate the structure of weights for a larger network, so that the search is constrained within the much smaller weight space. An instance of this approach is the work on the HyperNEAT framework (Stanley et al., 2009). In the HyperNEAT framework, Compositional Pattern-Producing Networks (CPPNs) are evolved to define the weight structure of the much larger main network. Closely related to our approach is a simplified variation of HyperNEAT, where the structure is fixed and the weights are evolved through Discrete Cosine Transform (DCT), called Compressed Weight Search (Koutnik et al., 2010). Even more closely related to our\n∗Work done as a member of the Google Brain Residency program (g.co/brainresidency).\napproach are Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016), and ACDC-Networks (Moczulski et al., 2015), where linear layers are compressed with DCT and the parameters are learned. Most reported results using these methods, however, are in small scales, perhaps because they are both slow to train and require heuristics to be efficient. The main difference between our approach and HyperNEAT is that hypernetworks in our approach are trained end-to-end with gradient descent together with the main network, and therefore are more efficient.\nAnother closely related idea to hypernetworks is the concept of fast weights Schmidhuber (1992; 1993) in which one network can produce context-dependent weight changes for a second network. Small scale experiments were conducted to demonstrate fast weights for feed forward networks at the time, but perhaps due to the lack of modern computational tools, the recurrent network version was mentioned mainly as a thought experiment (Schmidhuber, 1993). A subsequent work demonstrated practical applications of fast weights (Gomez & Schmidhuber, 2005), where a generator network is learnt through evolution to solve an artificial control problem.\nThe focus of this work is to apply our method to recurrent networks. In this context, our method has a connection to second-order or multiplicative networks (Goudreau et al., 1994; Sutskever et al., 2011; Wu et al., 2016), where the hidden state of the last step and the input vector of the current time step interact in a multiplicative fashion. The key difference between our approach and second-order networks is that our approach is more memory efficient because we only learn the scaling factors in the interaction matrix. Furthermore, in second-order or multiplicative networks, the weights of the RNN are not fixed, but a linear function of the previous hidden state. In our work, we explore the use of a smaller RNN, rather than a linear function, to produce the weights of the main RNN.\nThe concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network. These studies however did not explore the use of this approach to recurrent networks, which is a main contribution of our work."
    }, {
      "heading" : "3 METHODS",
      "text" : "Though it is possible to use hypernetworks to generate weights for feedforward or convolutional networks, in this paper, our focus is on using hypernetworks with recurrent networks. As can be seen below, when they are applied to recurrent networks, hypernetworks can be seen as a form of relaxed weight-sharing in the time dimension."
    }, {
      "heading" : "3.1 HYPERRNN",
      "text" : "Our hypernetworks can be used to generate weights for the RNN and LSTM. When a hypernetwork is used to generate the weights for an RNN, we refer to it as the HyperRNN. At every time step t, a HyperRNN takes as input the concatenated vector of input xt and the hidden states of the main RNN ht−1, it then generates as output the vector ĥt. This output vector is then used to generate the weights for the main RNN at the same timestep. Both the HyperRNN and the main RNN are trained jointly with backpropagation and gradient descent. In the following, we will give a more formal description of the model.\nThe standard formulation of a Basic RNN is given by:\nht = φ(Whht−1 +Wxxt + b) (1)\nwhere ht is the hidden state, φ is a non-linear operation such as tanh or relu, and the weight matrices and bias Wh ∈ RNh×Nh ,Wx ∈ RNh×Nx , b ∈ RNh is fixed each timestep for an input sequence X = (x1, x2, . . . , xT ).\nIn HyperRNN, we allow Wh and Wx to float over time by using a smaller hypernetwork to generate these parameters of the main RNN at each step (see Figure 1). More concretely, the parameters Wh,Wx, b of the main RNN are different at different time steps, so that ht can now be computed as:\nht = φ ( Wh(zh)ht−1 +Wx(zx)xt + b(zb) ) , where\nWh(zh) = 〈Whz, zh〉 Wx(zx) = 〈Wxz, zx〉 b(zb) =Wbzzb + b0\n(2)\nWhere Whz ∈ RNh×Nh×Nz ,Wxz ∈ RNh×Nx×Nz ,Wbz ∈ RNh×Nz , b0 ∈ RNh and zh, zx, zz ∈ RNz . We use a recurrent hypernetwork to compute zh, zx and zb as a function of xt and ht−1:\nx̂t = ( ht−1 xt ) ĥt = φ(Wĥĥt−1 +Wx̂x̂t + b̂)\nzh =Wĥhĥt−1 + bĥh\nzx =Wĥxĥt−1 + bĥx\nzb =Wĥbĥt−1\n(3)\nWhere Wĥ ∈ R Nĥ×Nĥ ,Wx̂ ∈ RNĥ×(Nh+Nz), b ∈ RNĥ , and Wĥh,Wĥx,Wĥb ∈ R Nz×Nĥ and bĥh, bĥx ∈ R Nz . This HyperRNN cell has Nĥ hidden units. Typically Nĥ is much smaller than Nh.\nAs the embeddings zh, zx and zb are of dimensions Nz , which is typically smaller than the hidden state size Nĥ of the HyperRNN cell, a linear network is used to project the output of the HyperRNN cell into the embeddings in Equation 3. After the embeddings are computed, they will be used to generate the full weight matrix of the main RNN.\nThe above is a general formulation of HyperRNN. However, Equation 2 is not practical because the memory usage becomes too large for real problems. We modify the HyperRNN described in Equation 2 so that it can be more memory efficient. We will use an intermediate hidden vector d(z) ∈ RNh to parametrize each weight matrix, where d(z) will be a linear function of z. To dynamically modify a weight matrix W , we will allow each row of this weight matrix to be scaled linearly by an element in vector d. We refer d as a weight scaling vector. Below is the modification to W (z):\nW (z) =W ( d(z) ) =  d0(z)W0d1(z)W1... dNh(z)WNh  (4) While we sacrifice the ability to construct an entire weight matrix from a linear combination of Nz matrices of the same size, we are able to linearly scale the rows of a single matrix withNz degrees of\nfreedom. We find this change to have a good trade-off, as this formulation of converting W (z) into W (d(z)) decreases the amount of memory required by the HyperRNN. Rather than requiring Nz times the memory of a Basic RNN, we will only be using memory in the order Nz times the number of hidden units, which is an acceptable amount of extra memory usage that is often available in many applications. In addition, the row-level operation in Equation 4 can be shown to be equivalent to an element-wise multiplication operator and hence computationally much more efficient in practice. Below is the more memory efficient version of the setup of Equation 2:\nht = φ ( dh(zh) Whht−1 + dx(zx) Wxxt + b(zb) ) , where\ndh(zh) =Whzzh\ndx(zx) =Wxzzx\nb(zb) =Wbzzb + b0\n(5)\nIn our experiments, we focus on the use of hypernetworks with the Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) because LSTM often works better than the Basic RNN. In such case, an LSTM will have more weight matrices and biases, and thus our main change is to have many more d’s, each d is being associated with each weight matrix or bias."
    }, {
      "heading" : "3.2 RELATED APPROACHES",
      "text" : "The formulation of the HyperRNN in Equation 5 has similarities to Recurrent Batch Normalization (Cooijmans et al., 2016) and Layer Normalization (Ba et al., 2016). The central idea for the normalization techniques is to calculate the first two statistical moments of the inputs to the activation function, and to linearly scale the inputs to have zero mean and unit variance. After the normalization, an additional set of fixed parameters are learned to unscale the inputs if required.\nSince the HyperRNN cell can indirectly modify the rows of each weight matrix and also the bias of the main RNN, it is implicitly also performing a linear scaling to the inputs of the activation function. The difference here is that the linear scaling parameters will be learned by the HyperRNN cell, and not based on statistical-moments. We note that the existing normalization approaches can work together with the HyperRNN approach, where the HyperRNN cell will be tasked with discovering a better dynamical scaling policy to complement normalization. We also explore this combination in our experiments.\nThe element-wise operation also has similarities to the Multiplicative RNN and its extensions (mRNN, mLSTM) (Sutskever et al., 2011; Krause et al., 2016) and Multiplicative Integration RNN (MI-RNN) (Wu et al., 2016). In the case of the mRNN, the hidden-to-hidden weight matrix is replaced with a factorized matrix, to allow the weights to be input dependent. The factorization is described below in Equation 6 (Krause et al., 2016).\nht = φ(Whm(Wmxxt) (Wmhht−1) +Wxxt + b) (6)\nFor the MI-RNN approach, a second order term is added to the Basic RNN formulation, along with scaling vectors for each term, as described in Equation 7. The addition of the scaling vectors allow parameters to be shared more efficiently.\nht = φ(α Wxxt Whht−1 + β1 Whht−1 + β2 Wxxt + b) (7)\nIn the HyperRNN approach, the weights are also input dependent. However, unlike mRNN, both weight matrices and also the bias term will be dependent to not only to the inputs, but also to the hidden states. In the MI-RNN approach, the weights are also be augmented by both the input and hidden states, via the second order term in Equation 7. In both mRNN and MI-RNN approaches, the weight augmentation terms are produced by a linear operation, while in the HyperRNN approach, the weight scaling vectors d are dynamically produced by another RNN with its own hidden states and non-linearities."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In the following experiments, we will benchmark the performance of HyperLSTM on language modelling with Penn Treebank, and Hutter Prize Wikipedia. We will also benchmark the method on the tasks of handwriting generation with IAM On-Line Handwriting Database, and machine translation with WMT’14 en→fr."
    }, {
      "heading" : "4.1 CHARACTER-LEVEL PENN TREEBANK LANGUAGE MODELLING",
      "text" : "We first evaluation the HyperLSTM model on a character level prediction task with the Penn Treebank corpus (Marcus et al., 1993) using the train/validation/test split outlined in (Mikolov et al., 2012). As the dataset is quite small, we apply dropout on both input and output layers with a keep probability of 90%. Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability.\nWe compare our model to the basic LSTM cell, stacked LSTM cells (Graves, 2013), and LSTM with layer normalization applied. In addition, we also experimented with applying layer normalization to HyperLSTM. Using the setup in (Graves, 2013), we use networks with 1000 units and train the network to predict the next character. In this task, the HyperLSTM cell has 128 units and an embedding size of 4. As the HyperLSTM cell has more trainable parameters compared to the basic LSTM cell, we also experimented with an LSTM cell with 1250 units.\nFor character-level Penn Treebank, we use mini-batches of size 128, to train on sequences of length 100. We train the model using Adam (Kingma & Ba, 2015) with a learning rate of 0.001 and gradient clipping of 1.0. During evaluation, we generate the entire sequence, and do not use information about previous test errors for prediction, e.g., dynamic evaluation (Graves, 2013; Rocki, 2016b). As mentioned earlier, we apply dropout to the input and output layers, and also apply recurrent dropout with a keep probability of 90%. For baseline models, orthogonal initialization (Henaff et al., 2016) is used for all weights.\nWe also experiment with a version of the model using a larger embedding size of 16, and also with a lower dropout keep probability of 85%, and report results with this “Large Embedding\" model in Table 1. Lastly, we stack two layers of this “Large Embedding\" model together to measure the benefits of a multi-layer version of HyperLSTM, with a dropout keep probability of 80%.\nOur results are presented in Table 1. The key observation here is that 1) HyperLSTM outperforms standard LSTM and 2) HyperLSTM also achieves similar improvements compared to Layer Normalization. The combination of Layer Normalization and Hyper LSTM achieves the best test perplexity so far on this dataset."
    }, {
      "heading" : "4.2 HUTTER PRIZE WIKIPEDIA LANGUAGE MODELLING",
      "text" : "We train our model on the larger and more challenging Hutter Prize Wikipedia dataset, also known as enwik8 (Hutter, 2012) consisting of a sequence of 100M characters composed of 205 unique characters. Unlike Penn Treebank, enwik8 contains some foreign words (Latin, Arabic, Chinese), indented XML, metadata, and internet addresses, making it a more realistic and practical dataset to test character language models.\nOur setup is similar in the previous experiment, using the same mini-batch size, learning rate, weight initialization, gradient clipping parameters and optimizer. We do not use dropout for the input and output layers, but still apply recurrent dropout with a keep probability of 90%. Similar to (Chung et al., 2015), we train on the first 90M characters of the dataset, use the next 5M as a validation set for early stopping, and the last 5M characters as the test set.\nAs enwik8 is a bigger dataset compared to Penn Treebank, we will use 1800 units for our networks. We also perform training on sequences of length 250. Our normal HyperLSTM cell consists of 256 units, and we use an embedding size of 64. To improve results, we also experiment with a larger model where both HyperLSTM and main network both have 2048 hidden units. The HyperLSTM cell consists of 512 units with an embedding size of 64. We also apply recurrent dropout to this larger model, with dropout keep probability of 85%, and train on a longer sequence length of 300.\nThe results are summarized in Table 2. As can be seen from the table, HyperLSTM is once again competitive to Layer Norm LSTM, and if we combine both techniques, the Layer Norm HyperLSTM achieves respectable results. The large version of HyperLSTM with normalization that uses 2048 hidden units achieve near state-of-the-art performance for this task. In addition, HyperLSTM converges more quickly compared to LSTM and Layer Norm LSTM (see Figure 2).\nWe perform additional analysis to understand the behavior of HyperLSTM by visualizing how the weight scaling vectors of the main LSTM change during the character sampling process. In Figure 3, we examine a sample text passage generated by HyperLSTM after training on enwik8 along with the weight differences below the text. We see that in regions of low intensity, where the weights of the main LSTM are relatively static, the types of phrases generated seem more deterministic. For example, the weights do not change much during the words Europeans, possessions and reservation. The regions of high intensity is when the HyperLSTM cell is making relatively large changes to the weights of the main LSTM.\n1We do not compare against methods that use dynamic evaluation. 2Our implementation. 3Based on results of version 2 at the time of writing. http://arxiv.org/abs/1609.01704v2 4This method uses information about test errors during inference for predicting the next characters, hence\nit is not directly comparable to other methods that do not use this information.\n In 1955-37 most American and Europeans signed into the sea. An absence of [[Japan (Korea city)|Japan]], the Mayotte like Constantino\n ple (in its first week, in [[880]]) that served as the mother of emperors, as the Corinthians, Bernard on his continued sequel toget\n her ordered [[Operation Moabili]]. The Gallup churches in the army promulgated the possessions sitting at the reservation, and [[Mel\n ito de la Vegeta Provine|Felix]] had broken Diocletian desperate from the full victory of Augustus, cited by Stephen I. Alexander Se\n nate became Princess Cartara, an annual ruler of war (777-184) and founded numerous extremiti of justice practitioners.\nFigure 3: Example text generated from HyperLSTM model. We visualize how four of the main RNN’s weight matrices (W ih, W g h , W f h , W o h ) effectively change over time by plotting the norm of the changes below each generated character. High intensity represent large changes being made to weights of main RNN."
    }, {
      "heading" : "4.3 HANDWRITING SEQUENCE GENERATION",
      "text" : "In addition to modelling discrete sequential data, we want to see how the model performs when modelling sequences of real valued data. We will train our model on the IAM online handwriting database (Liwicki & Bunke, 2005) and have our model predict pen strokes as per Section 4.2 of (Graves, 2013). The dataset has contains 12179 handwritten lines from 221 writers, digitally recorded from a tablet. We will model the (x, y) coordinate of the pen location at each recorded time step, along with a binary indicator of pen-up/pen-down. The average sequence length is around 700 steps and the longest around 1900 steps, making the training task particularly challenging as the network needs to retain information about both the stroke history and also the handwriting style in order to predict plausible future handwriting strokes.\nWe will use the same model architecture described in (Graves, 2013) and use a Mixture Density Network layer (Bishop, 1994) to generate a mixture of bi-variate Gaussian distributions to model at each time step to model the pen location. We normalize the data and use the same train/validation split as per (Graves, 2013) in this experiment. We remove samples less than length 300 as we found these samples contain a lot of recording errors and noise. After the pre-processing, as the dataset is small, we introduce data augmentation of chosen uniformly from +/- 10% and apply a this random scaling a the samples used for training.\nFor model training, will apply recurrent dropout and also dropout to the output layer with a keep probability of 0.95. The model is trained on mini-batches of size 32 containing sequences of variable length. We trained the model using Adam (Kingma & Ba, 2015) with a learning rate of 0.0001 and gradient clipping of 5.0. Our HyperLSTM cell consists of 128 units and a signal size of 4. For baseline models, orthogonal initialization (Henaff et al., 2016) is performed for all weights.\n1Our implementation, to replicate setup of (Graves, 2013). 2Our implementation, with data augmentation, dropout and recurrent dropout.\nThe results are summarized in Table 3. Our main result is that HyperLSTM with 900 units, without Layer Norm, achieves best log loss on the validation set across all models in published work and in our experiment. HyperLSTM also converges more quickly compared to other models (see Figure 2).\nSimilar to the earlier character generation experiment, we show a generated handwriting sample from the HyperLSTM model in Figure 4, along with a plot of how the weight scaling vectors of the main RNN is changing over time below the sample. For a more detailed interactive demonstration of handwriting generation using HyperLSTM, visit http://blog.otoro.net/2016/09/28/ hyper-networks/.\nWe observe that the regions of high intensity is concentrated at many discrete instances, rather than slowly varying over time. This implies that the weights experience regime changes rather than gradual slow adjustments. We can see that many of these weight changes occur at the boundaries between words, and between characters. While the LSTM model alone already does a decent job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperLSTM over a normal LSTM."
    }, {
      "heading" : "4.4 NEURAL MACHINE TRANSLATION",
      "text" : "Finally, we experiment with the Neural Machine Translation task using the same experimental setup outlined in (Wu et al., 2016). Our model is the same wordpiece model architecture with a vocabulary size of 32k, but we replace the LSTM cells with HyperLSTM cells. We benchmark both models on WMT’14 En→Fr using the same test/validation set split described in the GNMT paper (Wu et al., 2016). The GNMT network has 8 layers in the encoder, 8 layers in the decoder. The first layer of the encoder has bidirectional connections. The attention module is a neural network with 1 hidden layer. When a LSTM cell is used, the number of hidden units in each layer is 1024. The model is trained in a distributed setting with a parameter sever and 12 workers. Additionally, each worker uses 8 GPUs and a minibatch of 128.\nOur experimental setup is similar to that in the GNMT paper (Wu et al., 2016), with two simplifications. First, we use only Adam without SGD at the end. Adam was used with the same same hyperparameters described in the GNMT paper: learning rate of 0.0002 for 1M training steps.\nWe apply the HyperLSTM cell with Layer Norm to the GNMT architecture that uses a vocabulary of 32K wordpieces. We keep the same number of hidden units, which means that our model will have 16% more parameters.\nThe results are reported in Table 4, which shows that the HyperLSTM cell improves the performance of the existing GNMT model, achieving state-of-the-art single model results for this dataset. In addition, we demonstrate the applicability of hypernetworks to large-scale models used in production systems."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we presented a method to use one network to generate weights for another neural network. Our hypernetworks are trained end-to-end with backpropagation and therefore are efficient and scalable. We focused on applying hypernetworks to generate weights for recurrent networks. On language modelling and handwriting generation, hypernetworks are competitive to or sometimes better than state-of-the-art models. On machine translation, hypernetworks achieve a significant gain on top of a state-of-the-art production-level model."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Jeff Dean, Geoffrey Hinton, Mike Schuster and the Google Brain team for their help with the project."
    }, {
      "heading" : "A APPENDIX",
      "text" : "A.1 EXAMPLES OF GENERATED WIKIPEDIA TEXT\nA.2 EXAMPLES OF RANDOMLY CHOSEN GENERATED HANDWRITING SAMPLES\nA.3 EXAMPLES OF RANDOMLY CHOSEN MACHINE TRANSLATION SAMPLES\nWe randomly selected translation samples generated from both LSTM baseline and HyperLSTM models from the WMT’14 En→Fr Test Set. Given an English phrase, we can compare between the correct French translation, the LSTM translation, and the HyperLSTM translation.\nEnglish Input\nI was expecting to see gnashing of teeth and a fight breaking out at the gate .\nFrench (Ground Truth)\nJe m’ attendais à voir des grincements de dents et une bagarre éclater à la porte .\nLSTM Translation\nJe m’ attendais à voir des larmes de dents et un combat à la porte .\nHyperLSTM Translation\nJe m’ attendais à voir des dents grincer des dents et une bataille éclater à la porte .\nEnglish Input\nProsecuting , Anne Whyte said : \" If anyone should know not to the break the law , it is a criminal solicitor . \"\nFrench (Ground Truth)\nLe procureur Anne Whyte a déclaré : « Si quelqu’ un doit savoir qu’ il ne faut pas violer la loi , c’ est bien un avocat pénaliste . »\nLSTM Translation\nProsecuting , Anne Whyte a dit : « Si quelqu’ un doit savoir qu’ il ne faut pas enfreindre la loi , c’ est un solicitor criminel .\nHyperLSTM Translation\nEn poursuivant , Anne Whyte a dit : « Si quelqu’ un doit savoir ne pas enfreindre la loi , c’ est un avocat criminel .\nEnglish Input\nAccording to her , the CSRS was invited to a mediation and she asked for an additional period for consideration .\nFrench (Ground Truth)\nSelon elle , la CSRS a été invitée à une médiation et elle a demandé un délai supplémentaire pour y réfléchir .\nLSTM Translation\nSelon elle , le SCRS a été invité à une médiation et elle a demandé un délai supplémentaire .\nHyperLSTM Translation\nSelon elle , le SCRS a été invité à une médiation et elle a demandé une période de réflexion supplémentaire .\nEnglish Input\nRelations between the US and Germany have come under strain following claims that the NSA bugged Chancellor Angela ’s Merkel ’s phone .\nFrench (Ground Truth)\nLes relations entre les États-Unis et l’ Allemagne ont été mises à rude épreuve à la suite de plaintes selon lesquelles la NSA avait mis sur écoute le téléphone portable de la chancelière allemande Angela Merkel .\nLSTM Translation\nLes relations entre les Etats-Unis et l’ Allemagne ont été mises à rude épreuve suite aux affirmations selon lesquelles la NSA aurait pris le téléphone de Merkel de la chancelière Angela .\nHyperLSTM Translation\nLes relations entre les États-Unis et l’ Allemagne ont été mises à rude épreuve après que la NSA a attaqué le téléphone de la chancelière Angela Angela .\nEnglish Input\nGermany ’s BfV advises executives to consider using simple prepaid mobiles when on foreign trips because of the risk that smart phones are compromised .\nFrench (Ground Truth)\nLe BfV d’ Allemagne conseille à ses dirigeants d’ envisager d’ utiliser de simples téléphones portables prépayés lors de leurs voyages à l’ étranger en raison du risque d’ atteinte à l’ intégrité des smartphones .\nLSTM Translation\nLe BfV allemand conseille aux dirigeants d’ envisager l’ utilisation de mobiles prépayés simples lors de voyages à l’ étranger en raison du risque de compromission des téléphones intelligents .\nHyperLSTM Translation\nLe BfV allemand conseille aux dirigeants d’ envisager l’ utilisation de téléphones mobiles prépayés simples lors de voyages à l’ étranger en raison du risque que les téléphones intelligents soient compromis .\nEnglish Input\nI was on the mid-evening news that same evening , and on TV the following day as well .\nFrench (Ground Truth)\nLe soir-même , je suis au 20h , le lendemain aussi je suis à la télé .\nLSTM Translation\nJ’ étais au milieu de l’ actualité le soir même , et à la télévision le lendemain également .\nHyperLSTM Translation\nJ’ étais au milieu de la soirée ce soir-là et à la télévision le lendemain ."
    } ],
    "references" : [ {
      "title" : "Learning to learn by gradient descent by gradient descent",
      "author" : [ "M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul", "N. de Freitas" ],
      "venue" : "arXiv preprint arXiv:1606.04474,",
      "citeRegEx" : "Andrychowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andrychowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning feed-forward one-shot learners",
      "author" : [ "Luca Bertinetto", "João F. Henriques", "Jack Valmadre", "Philip H.S. Torr", "Andrea Vedaldi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bertinetto et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bertinetto et al\\.",
      "year" : 2016
    }, {
      "title" : "Mixture density networks",
      "author" : [ "Christopher M. Bishop" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Bishop.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1994
    }, {
      "title" : "Gated feedback recurrent neural networks",
      "author" : [ "Junyoung Chung", "Caglar Gülçehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.02367,",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1609.01704,",
      "citeRegEx" : "Chung et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic filter networks",
      "author" : [ "Bert De Brabandere", "Xu Jia", "Tinne Tuytelaars", "Luc Van Gool" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Brabandere et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Brabandere et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting Parameters in Deep Learning",
      "author" : [ "Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc’Aurelio Ranzato", "Nando de Freitas" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Denil et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2013
    }, {
      "title" : "Convolution by evolution: Differentiable pattern producing networks",
      "author" : [ "Chrisantha Fernando", "Dylan Banarse", "Malcolm Reynolds", "Frederic Besse", "David Pfau", "Max Jaderberg", "Marc Lanctot", "Daan Wierstra" ],
      "venue" : "In GECCO,",
      "citeRegEx" : "Fernando et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fernando et al\\.",
      "year" : 2016
    }, {
      "title" : "Evolving modular fast-weight networks for control",
      "author" : [ "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In ICANN,",
      "citeRegEx" : "Gomez and Schmidhuber.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gomez and Schmidhuber.",
      "year" : 2005
    }, {
      "title" : "First-order versus second-order single-layer recurrent neural networks",
      "author" : [ "Mark W Goudreau", "C Lee Giles", "Srimat T Chakradhar", "D Chen" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Goudreau et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Goudreau et al\\.",
      "year" : 1994
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Orthogonal RNNs and long-memory tasks",
      "author" : [ "Mikael Henaff", "Arthur Szlam", "Yann LeCun" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Henaff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Juergen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "URL http://prize",
      "author" : [ "Marcus Hutter. The human knowledge compression contest." ],
      "venue" : "hutter1.net/.",
      "citeRegEx" : "contest.,? 2012",
      "shortCiteRegEx" : "contest.",
      "year" : 2012
    }, {
      "title" : "Decoupled Neural Interfaces using Synthetic Gradients",
      "author" : [ "Max Jaderberg", "Wojciech Marian Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1608.05343,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Evolving neural networks in compressed weight space",
      "author" : [ "Jan Koutnik", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In GECCO,",
      "citeRegEx" : "Koutnik et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koutnik et al\\.",
      "year" : 2010
    }, {
      "title" : "Multiplicative LSTM for sequence modelling",
      "author" : [ "B. Krause", "L. Lu", "I. Murray", "S. Renals" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Krause et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2016
    }, {
      "title" : "Zoneout: Regularizing RNNs by randomly preserving hidden activations",
      "author" : [ "David Krueger", "Tegan Maharaj", "János Kramár", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.01305,",
      "citeRegEx" : "Krueger et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2016
    }, {
      "title" : "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard",
      "author" : [ "Marcus Liwicki", "Horst Bunke" ],
      "venue" : "In ICDAR,",
      "citeRegEx" : "Liwicki and Bunke.,? \\Q2005\\E",
      "shortCiteRegEx" : "Liwicki and Bunke.",
      "year" : 2005
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Tomáš Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "Jan Cernocky" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "ACDC: A Structured Efficient Linear Layer",
      "author" : [ "Marcin Moczulski", "Misha Denil", "Jeremy Appleyard", "Nando de Freitas" ],
      "venue" : "arXiv preprint arXiv:1511.05946,",
      "citeRegEx" : "Moczulski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Moczulski et al\\.",
      "year" : 2015
    }, {
      "title" : "Regularizing recurrent networks-on injected noise and normbased methods",
      "author" : [ "Saahil Ognawala", "Justin Bayer" ],
      "venue" : "arXiv preprint arXiv:1410.5684,",
      "citeRegEx" : "Ognawala and Bayer.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ognawala and Bayer.",
      "year" : 2014
    }, {
      "title" : "Recurrent memory array structures",
      "author" : [ "Kamil Rocki" ],
      "venue" : "arXiv preprint arXiv:1607.03085,",
      "citeRegEx" : "Rocki.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rocki.",
      "year" : 2016
    }, {
      "title" : "Surprisal-driven feedback in recurrent networks",
      "author" : [ "Kamil Rocki" ],
      "venue" : "arXiv preprint arXiv:1608.06027,",
      "citeRegEx" : "Rocki.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rocki.",
      "year" : 2016
    }, {
      "title" : "Learning to control fast-weight memories: An alternative to dynamic recurrent networks",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmidhuber.,? \\Q1992\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1992
    }, {
      "title" : "A ‘self-referential’ weight matrix",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "In ICANN,",
      "citeRegEx" : "Schmidhuber.,? \\Q1993\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1993
    }, {
      "title" : "Recurrent dropout without memory loss",
      "author" : [ "Stanislaw Semeniuta", "Aliases Severyn", "Erhardt Barth" ],
      "venue" : null,
      "citeRegEx" : "Semeniuta et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Semeniuta et al\\.",
      "year" : 2016
    }, {
      "title" : "A hypercube-based encoding for evolving large-scale neural networks",
      "author" : [ "Kenneth O. Stanley", "David B. D’Ambrosio", "Jason Gauci" ],
      "venue" : "Artificial Life,",
      "citeRegEx" : "Stanley et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Stanley et al\\.",
      "year" : 2009
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "On multiplicative integration with recurrent neural networks",
      "author" : [ "Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Fried Convnets",
      "author" : [ "Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep recurrent models with fastforward connections for neural machine translation",
      "author" : [ "Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu" ],
      "venue" : "CoRR, abs/1606.04199,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent highway networks",
      "author" : [ "Julian Zilly", "Rupesh Srivastava", "Jan Koutník", "Jürgen Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1607.03474,",
      "citeRegEx" : "Zilly et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zilly et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "An instance of this approach is the work on the HyperNEAT framework (Stanley et al., 2009).",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "Closely related to our approach is a simplified variation of HyperNEAT, where the structure is fixed and the weights are evolved through Discrete Cosine Transform (DCT), called Compressed Weight Search (Koutnik et al., 2010).",
      "startOffset" : 202,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : "approach are Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016), and ACDC-Networks (Moczulski et al.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : ", 2016), and ACDC-Networks (Moczulski et al., 2015), where linear layers are compressed with DCT and the parameters are learned.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 28,
      "context" : "Small scale experiments were conducted to demonstrate fast weights for feed forward networks at the time, but perhaps due to the lack of modern computational tools, the recurrent network version was mentioned mainly as a thought experiment (Schmidhuber, 1993).",
      "startOffset" : 240,
      "endOffset" : 259
    }, {
      "referenceID" : 9,
      "context" : "In this context, our method has a connection to second-order or multiplicative networks (Goudreau et al., 1994; Sutskever et al., 2011; Wu et al., 2016), where the hidden state of the last step and the input vector of the current time step interact in a multiplicative fashion.",
      "startOffset" : 88,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : "In this context, our method has a connection to second-order or multiplicative networks (Goudreau et al., 1994; Sutskever et al., 2011; Wu et al., 2016), where the hidden state of the last step and the input vector of the current time step interact in a multiplicative fashion.",
      "startOffset" : 88,
      "endOffset" : 152
    }, {
      "referenceID" : 32,
      "context" : "In this context, our method has a connection to second-order or multiplicative networks (Goudreau et al., 1994; Sutskever et al., 2011; Wu et al., 2016), where the hidden state of the last step and the input vector of the current time step interact in a multiplicative fashion.",
      "startOffset" : 88,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "The concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al.",
      "startOffset" : 84,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "The concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al.",
      "startOffset" : 84,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : ", 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network.",
      "startOffset" : 24,
      "endOffset" : 116
    }, {
      "referenceID" : 33,
      "context" : ", 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network.",
      "startOffset" : 24,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : ", 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network.",
      "startOffset" : 24,
      "endOffset" : 116
    }, {
      "referenceID" : 31,
      "context" : "The element-wise operation also has similarities to the Multiplicative RNN and its extensions (mRNN, mLSTM) (Sutskever et al., 2011; Krause et al., 2016) and Multiplicative Integration RNN (MI-RNN) (Wu et al.",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "The element-wise operation also has similarities to the Multiplicative RNN and its extensions (mRNN, mLSTM) (Sutskever et al., 2011; Krause et al., 2016) and Multiplicative Integration RNN (MI-RNN) (Wu et al.",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 32,
      "context" : ", 2016) and Multiplicative Integration RNN (MI-RNN) (Wu et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "The factorization is described below in Equation 6 (Krause et al., 2016).",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "We first evaluation the HyperLSTM model on a character level prediction task with the Penn Treebank corpus (Marcus et al., 1993) using the train/validation/test split outlined in (Mikolov et al.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : ", 1993) using the train/validation/test split outlined in (Mikolov et al., 2012).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability.",
      "startOffset" : 161,
      "endOffset" : 182
    }, {
      "referenceID" : 10,
      "context" : "We compare our model to the basic LSTM cell, stacked LSTM cells (Graves, 2013), and LSTM with layer normalization applied.",
      "startOffset" : 64,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "Using the setup in (Graves, 2013), we use networks with 1000 units and train the network to predict the next character.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : ", dynamic evaluation (Graves, 2013; Rocki, 2016b).",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "For baseline models, orthogonal initialization (Henaff et al., 2016) is used for all weights.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "Model Test Validation Param Count ME n-gram (Mikolov et al., 2012) 1.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : "32 Recurrent Dropout LSTM (Semeniuta et al., 2016) 1.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "338 Zoneout RNN (Krueger et al., 2016) 1.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "27 HM-LSTM (Chung et al., 2016) 1.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "Similar to (Chung et al., 2015), we train on the first 90M characters of the dataset, use the next 5M as a validation set for early stopping, and the last 5M characters as the test set.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "Model enwik8 Param Count Stacked LSTM (Graves, 2013) 1.",
      "startOffset" : 38,
      "endOffset" : 52
    }, {
      "referenceID" : 31,
      "context" : "0 M MRNN (Sutskever et al., 2011) 1.",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "60 Grid-LSTM (Kalchbrenner et al., 2016) 1.",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 32,
      "context" : "45 MI-LSTM (Wu et al., 2016) 1.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "44 MLSTM (Krause et al., 2016) 1.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "42 Recurrent Highway Networks (Zilly et al., 2016) 1.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "40 HM-LSTM (Chung et al., 2016) 1.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "2 of (Graves, 2013).",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "We will use the same model architecture described in (Graves, 2013) and use a Mixture Density Network layer (Bishop, 1994) to generate a mixture of bi-variate Gaussian distributions to model at each time step to model the pen location.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "We will use the same model architecture described in (Graves, 2013) and use a Mixture Density Network layer (Bishop, 1994) to generate a mixture of bi-variate Gaussian distributions to model at each time step to model the pen location.",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "We normalize the data and use the same train/validation split as per (Graves, 2013) in this experiment.",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "For baseline models, orthogonal initialization (Henaff et al., 2016) is performed for all weights.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "Our implementation, to replicate setup of (Graves, 2013).",
      "startOffset" : 42,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.",
      "startOffset" : 43,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.",
      "startOffset" : 158,
      "endOffset" : 172
    }, {
      "referenceID" : 32,
      "context" : "Finally, we experiment with the Neural Machine Translation task using the same experimental setup outlined in (Wu et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "We benchmark both models on WMT’14 En→Fr using the same test/validation set split described in the GNMT paper (Wu et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "Our experimental setup is similar to that in the GNMT paper (Wu et al., 2016), with two simplifications.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "Model Test BLEU Log Perplexity Param Count Deep-Att + PosUnk (Zhou et al., 2016) 39.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : "2 GNMT WPM-32K, LSTM (Wu et al., 2016) 38.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 32,
      "context" : "7 M GNMT WPM-32K, ensemble of 8 LSTMs (Wu et al., 2016) 40.",
      "startOffset" : 38,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network. We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.",
    "creator" : "LaTeX with hyperref package"
  }
}