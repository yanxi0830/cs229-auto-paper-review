{
  "name" : "459.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEEP MULTI-TASK REPRESENTATION LEARNING: A TENSOR FACTORISATION APPROACH",
    "authors" : [ "Yongxin Yang", "Timothy M. Hospedales" ],
    "emails" : [ "t.hospedales}@qmul.ac.uk" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The paradigm of multi-task learning is to learn multiple related tasks simultaneously so that knowledge obtained from each task can be re-used by the others. Early work in this area focused on neural network models (Caruana, 1997), while more recent methods have shifted focus to kernel methods, sparsity and low-dimensional task representations of linear models (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012). Nevertheless given the impressive practical efficacy of contemporary deep neural networks (DNN)s in many important applications, we are motivated to revisit MTL from a deep learning perspective.\nWhile the machine learning community has focused on MTL for shallow linear models recently, applications have continued to exploit neural network MTL (Zhang et al., 2014; Liu et al., 2015). The typical design pattern dates back at least 20 years (Caruana, 1997): define a DNN with shared lower representation layers, which then forks into separate layers and losses for each task. The sharing structure is defined manually: full-sharing up to the fork, and full separation after the fork. However this complicates DNN architecture design because the user must specify the sharing structure: How many task specific layers? How many task independent layers? How to structure sharing if there are many tasks of varying relatedness?\nIn this paper we present a method for end-to-end multi-task learning in DNNs. This contribution can be seen as generalising shallow MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012) to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL (Caruana, 1997; Zhang et al., 2014; Spieckermann et al., 2014; Liu et al., 2015) which currently must be defined manually on a problem-by-problem basis.\nBefore proceeding it is worth explicitly distinguishing some different problem settings, which have all been loosely referred to as MTL in the literature. Homogeneous MTL: Each task corresponds to a single output. For example, MNIST digit recognition is commonly used to evaluate MTL algorithms by casting it as 10 binary classification tasks (Kumar & Daumé III, 2012). Heterogeneous MTL: Each task corresponds to a unique set of output(s) (Zhang et al., 2014). For example, one may want simultaneously predict a person’s age (task one: multi-class classification or regression) as well as identify their gender (task two: binary classification) from a face image.\nIn this paper, we propose a multi-task learning method that works on all these settings. The key idea is to use tensor factorisation to divide each set of model parameters (i.e., both FC weight matrices,\nand convolutional kernel tensors) into shared and task-specific parts. It is a natural generalisation of shallow MTL methods that explicitly or implicitly are based on matrix factorisation (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Daumé III, 2007). As linear methods, these typically require pre-engineered features. In contrast, as a deep network, our generalisation can learn directly from raw image data, determining sharing structure in a layer-wise fashion. For the simplest NN architecture – no hidden layer, single output – our method reduces to matrix-based ones, therefore matrix-based methods including (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Daumé III, 2007) are special cases of ours."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Multi-Task Learning Most contemporary MTL algorithms assume that the input and model are both D-dimensional vectors. The models of T tasks can then be stacked into a D × T sized matrix W . Despite different motivations and implementations, many matrix-based MTL methods work by placing constrains on W . For example, posing an `2,1 norm on W to encourage low-rank W (Argyriou et al., 2008). Similarly, (Kumar & Daumé III, 2012) factorises W as W = LS, i.e., it assigns a lower rank as a hyper-parameter. An earlier work (Evgeniou & Pontil, 2004) proposes that the linear model for each task t can be written as wt = ŵt + ŵ0. This is the factorisation L = [ŵ0, ŵ1, . . . , ŵT ] and S = [11×T ; IT ]. In fact, such matrix factorisation encompasses many MTL methods. E.g., (Xue et al., 2007) assumes S·,i (the ith column of S) is a unit vector generated by a Dirichlet Process and (Passos et al., 2012) models W using linear factor analysis with Indian Buffet Process (Griffiths & Ghahramani, 2011) prior on S.\nTensor Factorisation In deep learning, tensor factorisation has been used to exploit factorised tensors’ fewer parameters than the original (e.g., 4-way convolutional kernel) tensor, and thus compress and/or speed up the model, e.g., (Lebedev et al., 2015; Novikov et al., 2015). For shallow linear MTL, tensor factorisation has been used to address problems where tasks are described by multiple independent factors rather than merely indexed by a single factor (Yang & Hospedales, 2015). Here the D-dimensional linear models for all unique tasks stack into a tensor W , of e.g. D × T1 × T2 in the case of two task factors. Knowledge sharing is then achieved by imposing tensor norms on W (Romera-paredes et al., 2013; Wimalawarne et al., 2014). Our framework factors tensors for the different reason that for DNN models, parameters include convolutional kernels (N -way tensors) or D1 × D2 FC layer weight matrices (2-way tensors). Stacking up these parameters for many tasks results in D1 × · · · ×DN × T tensors within which we share knowledge through factorisation. Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs (Caruana, 1997). This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output. Heterogeneous MTL typically uses neural networks with multiple sets of outputs and losses. E.g., Huang et al. (2013) proposes a shared-hidden-layer DNN model for multilingual speech processing, where each task corresponds to an individual language. Zhang et al. (2014) uses a DNN to find facial landmarks (regression) as well as recognise facial attributes (classification); while Liu et al. (2015) proposes a DNN for query classification and information retrieval (ranking for web search). A key commonality of these studies is that they all require a user-defined parameter sharing strategy. A typical design pattern is to use shared layers (same parameters) for lower layers of the DNN and then split (independent parameters) for the top layers. However, there is no systematic way to make such design choices, so researchers usually rely on trial-and-error, further complicating the already somewhat dark art of DNN design. In contrast, our method learns where and how much to share representation parameters across the tasks, hence significantly reducing the space of DNN design choices.\nParametrised DNNs Our MTL approach is a parameterised DNN (Sigaud et al., 2015), in that DNN weights are dynamically generated given some side information – in the case of MTL, given the task identity. In a related example of speaker-adaptive speech recognition (Tan et al., 2016) there may be several clusters in the data (e.g., gender, acoustic conditions), and each speaker’s model could be a linear combination of these latent task/clusters’ models. They model each speaker i’s weight matrix W (i) as a sum of K base models W̃ , i.e., W (i) = ∑K k=1 λ (i) p W̃ (p). The difference between speakers/tasks comes from λ and the base models are shared. An advantage of this is that,\nwhen new data come, one can choose to re-train λ parameters only, and keep W̃ fixed. This will significantly reduce the number of parameters to learn, and consequently the required training data. Beyond this, Yang & Hospedales (2015) show that it is possible to train another neural network to predict those λ values from some abstract metadata. Thus a model for an unseen task can be generated on-the-fly with no training instances given an abstract description of the task. The techniques developed here are compatible with both these ideas of generating models with minimal or no effort."
    }, {
      "heading" : "3 METHODOLOGY",
      "text" : ""
    }, {
      "heading" : "3.1 PRELIMINARIES",
      "text" : "We first recap some tensor factorisation basics before explaining how to factorise DNN weight tensors for multi-task representation learning. An N -way tensorW with shape D1 ×D2 × · · ·DN is an N -dimensional array containing ∏N n=1Dn elements. Scalars, vectors, and matrices can be seen as 0, 1, and 2-way tensors respectively, although the term tensor is usually used for 3-way or higher. A mode-n fibre ofW is a Dn-dimensional vector obtained by fixing all but the nth index. The mode-n flatteningW(n) ofW is the matrix of sizeDn× ∏ i¬nDi constructed by concatenating\nall of the ∏\ni¬nDi mode-n fibres along columns.\nThe dot product of two tensors is a natural extension of matrix dot product, e.g., if we have a tensor A of sizeM1×M2×· · ·P and a tensor B of size P ×N1×N2 . . . , the tensor dot productA•B will be a tensor of size M1 ×M2 × · · ·N1 × N2 · · · by matrix dot product AT(−1)B(1) and reshaping\n1. More generally, tensor dot product can be performed along specified axes, A • B(i,j) = AT(i)B(j) and reshaping. Here the subscripts indicate the axes of A and B at which dot product is performed. E.g., when A is of size M1 × P ×M3 × · · ·MI and B is of size N1 × N2 × P × · · ·NJ , then A • B(2,3) is a tensor of size M1 ×M3 × · · ·MI ×N1 ×N2 × · · ·NJ .\nMatrix-based Knowledge Sharing Assume we have T linear models (tasks) parametrised by Ddimensional weight vectors, so the collection of all models forms a size D × T matrix W . One commonly used MTL approach (Kumar & Daumé III, 2012) is to place a structure constraint on W , e.g., W = LS, where L is a D ×K matrix and S is a K × T matrix. This factorisation recovers a shared factor L and a task-specific factor S. One can see the columns of L as latent basis tasks, and the model w(i) for the ith task is the linear combination of those latent basis tasks with task-specific information S·,i.\nw(i) := W·,i = LS·,i = K∑ k=1 L·,kSk,i (1)\nFrom Single to Multiple Outputs Consider extending this matrix factorisation approach to the case of multiple outputs. The model for each task is then a D1 × D2 matrix, for D1 input and D2 output dimensions. The collection of all those matrices constructs a D1 × D2 × T tensor. A straightforward extension of Eq. 1 to this case is\nW (i) :=W·,·,i = K∑\nk=1\nL·,·,kSk,i (2)\nThis is equivalent to imposing the same structural constraint on WT(3) (transposed mode-3 flattening ofW). It is important to note that this allows knowledge sharing across the tasks only. I.e., knowledge sharing is only across-tasks not across dimensions within a task. However it may be that the knowledge learned in the mapping to one output dimension may be useful to the others within one task. E.g., consider recognising photos of handwritten and print digits – it may be useful to share across handwritten-print; as well as across different digits within each. In order to support general knowledge sharing across both tasks and outputs within tasks, we propose to use more general tensor factorisation techniques. Unlike for matrices, there are multiple definitions of tensor factorisation, and we use Tucker (Tucker, 1966) and Tensor Train (TT) (Oseledets, 2011) decompositions.\n1We slightly abuse ‘-1’ referring to the last axis of the tensor."
    }, {
      "heading" : "3.2 TENSOR FACTORISATION FOR KNOWLEDGE SHARING",
      "text" : "Tucker Decomposition Given anN -way tensor of sizeD1×D2 · · ·×DN , Tucker decomposition outputs a core tensor S of size K1 × K2 · · · × KN , and N matrices U (n) of size Dn × Kn, such that,\nWd1,d2,...,dN = K1∑\nk1=1 K2∑ k2=1 · · · KN∑ kN=1 Sk1,k2,...,kNU (1) d1,k1 U (2) d2,k2 · · ·U (N)dN ,kN (3)\nW = S • U(1)(1,2) • U (2) (1,2) · · · • U (N) (1,2) (4)\nTucker decomposition is usually implemented by an alternating least squares (ALS) method (Kolda & Bader, 2009). However (Lathauwer et al., 2000) treat it as a higher-order singular value decomposition (HOSVD), which is more efficient to solve: U (n) is exactly the U matrix from the SVD of mode-n flattening W(n) ofW , and the core tensor S is obtained by,\nS =W • U(1)(1,1) • U (2) (1,1) · · · • U (N) (1,1) (5)\nTensor Train Decomposition Tensor Train (TT) Decomposition outputs 2 matrices U (1) and U (N) of size D1 × K1 and KN−1 × DN respectively, and (N − 2) 3-way tensors U (n) of size Kn−1 ×Dn ×Kn. The elements ofW can be computed by,\nWd1,d2,...,dN = K1∑\nk1=1 K2∑ k2=1 · · · KN−1∑ kN−1=1 U (1) d1,k1 U (2)k1,d2,k2U (3) k2,d3,k3 · · ·U (N)kN−1,dN (6)\n= U (1) d1,·U (2) ·,d2,·U (3) ·,d3,· · · ·U (d) ·,dN (7)\nW = U (1) • U (2) · · · • U (N) (8)\nwhere U (n)·,dn,· is a matrix of size Kn−1 ×Kn sliced from U (n) with the second axis fixed at dn. The TT decomposition is typically realised with a recursive SVD-based solution (Oseledets, 2011).\nKnowledge Sharing If the final axis of the input tensor above indexes tasks, i.e. if DN = T then the last factor U (N) in both decompositions encodes a matrix of task specific knowledge, and the other factors encode shared knowledge."
    }, {
      "heading" : "3.3 DEEP MULTI-TASK REPRESENTATION LEARNING",
      "text" : "To realise deep multi-task representation learning (DMTRL), we learn one DNN per-task each with the same architecture2. However each corresponding layer’s weights are generated with one of the knowledge sharing structures in Eq. 2, Eq. 4 or Eq. 8. It is important to note that we apply these ‘right-to-left’ in order to generate weight tensors with the specified sharing structure, rather than actually applying Tucker or TT to decompose an input tensor. In the forward pass, we synthesise weight tensorsW and perform inference as usual, so the method can be thought of as tensor composition rather than decomposition.\nOur weight generation (construct tensors from smaller pieces) does not introduce non-differentiable terms, so our deep multi-task representation learner is trainable via standard backpropagation. Specifically, in the backward pass over FC layers, rather than directly learning the 3-way tensor W , our methods learn either {S, U1, U2, U3} (DMTRL-Tucker, Eq. 4), {U1,U2, U3} (DMTRL-TT, Eq. 8), or in the simplest case {L, S} (DMTRL-LAF3, Eq. 2). Besides FC layers, contemporary\n2Except heterogeneous MTL, where the output layer is necessarily unshared due to different dimensionality. 3LAF refers to Last Axis Flattening.\n   \nDNN designs often exploit convolutional layers. Those layers usually contain kernel filter parameters that are 3-way tensors of sizeH×W ×C, (whereH is height, W is width, and C is the number of input channels) or 4-way tensors of sizeH×W ×C×M , whereM is the number of filters in this layer (i.e., the number of output channels). The proposed methods naturally extend to convolution layers as convolution just adds more axes on the left-hand side. E.g., the collection of parameters from a given convolutional layer of T neural networks forms a tensor of shapeH×W×C×M×T . These knowledge sharing strategies provide a way to softly share parameters across the corresponding layers of each task’s DNN: where, what, and how much to share are learned from data. This is in contrast to the conventional Deep-MTL approach of manually selecting a set of layers to undergo hard parameter sharing: by tying weights so each task uses exactly the same weight matrix/tensor for the corresponding layer (Zhang et al., 2014; Liu et al., 2015); and a set of layers to be completely separate: by using independent weight matrices/tensors. In contrast our approach benefits from: (i) automatically learning this sharing structure from data rather than requiring user trial and error, and (ii) smoothly interpolating between fully shared and fully segregated layers, rather than a hard switching between these states. An illustration of the proposed framework for different problem settings can be found in Fig. 1."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "Implementation Details Our method is implemented with TensorFlow (Abadi et al., 2015). The code is released on GitHub4. For DMTRL-Tucker, DMTRL-TT, and DMTRL-LAF, we need to assign the rank of each weight tensor. The DNN architecture itself may be complicated and so can benefit from different ranks at different layers, but grid-search is impractical. However, since\n4https://github.com/wOOL/DMTRL\nboth Tucker and TT decomposition methods have SVD-based solutions, and vanilla SVD is directly applicable to DMTRL-LAF, we can initialise the model and set the ranks as follows: First train the DNNs independently in single task learning mode. Then pack the layer-wise parameters as the input for tensor decomposition. When SVD is applied, set a threshold for relative error so SVD will pick the appropriate rank. Thus our method needs only a single hyper parameter of max reconstruction error (we set to = 10% throughout) that indirectly specifies the ranks of every layer. Note that training from random initialisation also works, but the STL-based initialisation makes rank selection easy and transparent. Nevertheless, like (Kumar & Daumé III, 2012) the framework is not sensitive to rank choice so long as they are big enough. If random initialisation is desired to eliminate the pre-training requirement, good practice is to initialise parameter tensors by a suitable random weight distribution first, then do decomposition, and use the decomposed values for initialising the factors (the real learnable parameters in our framework). In this way, the resulting re-composed tensors will have approximately the intended distribution. Our sharing is applied to weight parameters only, bias terms are not shared. Apart from initialisation, decomposition is not used anywhere."
    }, {
      "heading" : "4.1 HOMOGENEOUS MTL",
      "text" : "Dataset, Settings and Baselines We use MNIST handwritten digits. The task is to recognise digit images zero to nine. When this dataset is used for the evaluation of MTL methods, ten 1-vs-all binary classification problems usually define ten tasks (Kumar & Daumé III, 2012). The dataset has a given train (60,000 images) and test (10,000 images) split. Each instance is a monochrome image of size 28× 28× 1. We use a modified LeNet (LeCun et al., 1998) as the CNN architecture. The first convolutional layer has 32 filters of size 5 × 5, followed by 2 × 2 max pooling. The second convolutional layer has 64 filters of size 4× 4, and again a 2× 2 max pooling. After these two convolutional layers, two fully connected layers with 512 and 1 output(s) are placed sequentially. The convolutional and first FC layer use RELU f(x) = max(x, 0) activation function. We use hinge loss, `(y) = max(0, 1− ŷ ·y), where y ∈ ±1 is the true label and ŷ is the output of each task’s neural network. Conventional matrix-based MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) are linear models taking vector input only, so they need a preprocessing that flattens the image into a vector, and typically reduce dimension by PCA. As per our motivation for studying Deep MTL, our methods decisively outperform such shallow linear baselines. Thus to find a stronger MTL competitor, we instead search user defined architectures for Deep-MTL parameter sharing (cf (Zhang et al., 2014; Liu et al., 2015; Caruana, 1997)). In all of the four parametrised layers (pooling has no parameters), we set the first N (1 ≤ N ≤ 3) to be hard shared5. We then use cross-validation to select among the three userdefined MTL architectures and the best option is N = 3, i.e., the first three layers are fully shared (we denote this model UD-MTL). For our methods, all four parametrised layers are softly shared with the different factorisation approaches. To evaluate different MTL methods and a baseline of single task learning (STL), we take ten different fractions of the given 60K training split, train the model, and test on the 10K testing split. For each fraction, we repeat the experiment 5 times with randomly sampled training data. We report two performance metrics: (1) the mean error rate of the ten binary classification problems and (2) the error rate of recognising a digit by ranking each task’s 1-vs-all output (multi-class classification error).\nResults As we can see in Fig. 2, all MTL approaches outperform STL, and the advantage is more significant when the training data is small. The proposed methods, DMTRL-TT and DMTRLTucker outperform the best user-defined MTL when the training data is very small, and their performance is comparable when the training data is large.\nFurther Discussion For a slightly unfair comparison, in the case of binary classification with 1000 training data, shallow matrix-based MTL methods with PCA feature (Kang et al., 2011; Kumar & Daumé III, 2012) reported 14.0% / 13.4% error rate. With the same amount of data, our methods\n5This is not strictly all possible user-defined sharing options. For example, another possibility is the first convolutional layer and the first FC layer could be fully shared, with the second convolutional layer being independent (task specific). However, this is against the intuition that lower/earlier layers are more task agnostic, and later layers more task specific. Note that sharing the last layer is technically possible but not intuitive, and in any case not meaningful unless at least one early layer is unshared, as the tasks are different.\nhave error rate below 6%. This shows the importance of our deep end-to-end multi-task representation learning contribution versus conventional shallow MTL. Since the error rates in (Kang et al., 2011; Kumar & Daumé III, 2012) were produced on a private subset of MNIST dataset with PCA representations only, to ensure a direct comparison, we implement several classic MTL methods and compare them in Appendix A.\nFor readers interested in the connection to model capacity (number of parameters), we present further analysis in Appendix B."
    }, {
      "heading" : "4.2 HETEROGENEOUS MTL: FACE ANALYSIS",
      "text" : "Dataset, Settings and Baselines The AdienceFaces (Eidinger et al., 2014) is a large-scale face images dataset with the labels of each person’s gender and age group. We use this dataset for the evaluation of heterogeneous MTL with two tasks: (i) gender classification (two classes) and (ii) age group classification (eight classes). Two independent CNN models for this benchmark are introduced in (Levi & Hassncer, 2015). The two CNNs have the same architecture except for the last fully-connected layer, since the heterogeneous tasks have different number of outputs (two / eight). We take these CNNs from (Levi & Hassncer, 2015) as the STL baseline. We again search for the best possible user-defined MTL architecture as a strong competitor: the proposed CNN has six layers – three convolutional and three fully-connected layers. The last fully-connected layer has non-shareable parameters because they are of different size. To search the MTL design-space, we try setting the first N (1 ≤ N ≤ 5) layers to be hard shared between the tasks. Running 5-fold cross-validation on the train set to evaluate the architectures, we find the best choice is N = 5 (i.e., all layers fully shared before the final heterogeneous outputs). For our proposed methods, all the layers before the last heterogeneous dimensionality FC layers are softly shared.\nWe select increasing fractions of the AdienceFaces train split randomly, train the model, and evaluate on the same test set. For reference, there are 12245 images with gender labelled for training, 4007 ones for testing, and 11823 images with age group labelled for training, and 4316 ones for testing.\nResults Fig. 3 shows the error rate for each task. For the gender recognition task, we find that: (i) User-defined MTL is not consistently better than STL, but (ii) our methods, esp., DMTRLTucker, consistently outperform both STL and the best user-defined MTL. For the harder age group classification task, our methods generally improve on STL. However UD-MTL does not consistently improve on STL, and even reduces performance when the training set is bigger. This is the negative transfer phenomenon (Rosenstein et al., 2005), where using a transfer learning algorithm is worse than not using it. This difference in outcomes is attributed to sufficient data eventually providing some effective task-specific representation. Our methods can discover and exploit this, but UDMTL’s hard switch between sharing and not sharing can not represent or exploit such increasing task-specificity of representation."
    }, {
      "heading" : "4.3 HETEROGENEOUS MTL: MULTI-ALPHABET RECOGNITION",
      "text" : "Dataset, Settings and Baselines We next consider the task of learning to recognise handwritten letters in multiple languages using the Omniglot (Lake et al., 2015) dataset. Omniglot contains handwritten characters in 50 different alphabets (e.g., Cyrillic, Korean, Tengwar), each with its own number of unique characters (14 ∼ 55). In total, there are 1623 unique characters, and each has exactly 20 instances. Here each task corresponds to an alphabet, and the goal is to recognise its characters. MTL has a clear motivation here, as cross-alphabet knowledge sharing is likely to be useful as one is unlikely to have extensive training data for a wide variety of less common alphabets.\nThe images are monochrome of size 105 × 105. We design a CNN with 3 convolutional and 2 FC layers. The first conv layer has 8 filters of size 5 × 5; the second conv layer has 12 filters of size 3 × 3, and the third convolutional layer has 16 filters of size 3 × 3. Each convolutional layer is followed by a 2 × 2 max-pooling. The first FC layer has 64 neurons, and the second FC layer has size corresponding to the number of unique classes in the alphabet. The activation function is tanh.\nWe use a similar strategy to find the best user-defined MTL model: the CNN has 5 parametrised layers, of which 4 layers are potentially shareable. So we tried hard-sharing the firstN (1 ≤ N ≤ 4) layers. Evaluating these options by 5-fold cross-validation, the best option turned out to be N = 3, i.e., the first three layers are hard shared. For our methods, all four shareable layers are softly shared.\nSince there is no standard train/test split for this dataset, we use the following setting: We repeatedly pick at random 5, . . . 90% of images per class for training. Note that 5% is the minimum, corresponding to one-shot learning. The remaining data are used for evaluation.\nResults Fig. 4 reports the average error rate across all 50 tasks (alphabets). Our proposed MTL methods surpass the STL baseline in all cases. User-defined MTL does not work well when the training data is very small, but does help when training fraction is larger than 50%.\nMeasuring the Learned Sharing Compared to the conventional user-defined sharing architectures, our method learns how to share from data. We next try to quantify the amount of sharing estimated by our model on the Omniglot data. Returning to the key factorisation W = LS, we can find that S-like matrix appears in all variants of proposed method. It is S in DMTRL-LAF, the transposed U (N) in DMTRL-Tucker, and U (N) in DMTRL-TT (N is the last axis of W). S is a K × T size matrix, where T is the number of tasks, and K is the number of latent tasks (Kumar & Daumé III, 2012) or the dimension of task coding (Yang & Hospedales, 2015). Each column of S is a set of coefficients that produce the final weight matrix/tensor by linear combination. If we put STL and user-defined MTL (for a certain shared layer) in this framework, we see that STL is to assign (rather than learn) S to be an identity matrix IT . Similarly, user-defined MTL (for a certain shared layer) is to assign S to be a matrix with all zeros but one particular row is all ones, e.g., S = [11×T ;0]. Between these two extremes, our method learns the sharing structure in S. We propose the following equation to measure the learned sharing strength:\nρ = 1( T 2 ) ∑ i<j Ω(S·,i, S·,j) = 2 T (T − 1) ∑ i<j Ω(S·,i, S·,j) (9)\nHere Ω(a, b) is a similarity measure for two vectors a and b and we use cosine similarity. ρ is the average on all combinations of column-wise similarity. So ρmeasures how much sharing is encoded by S between ρ = 0 for STL (nothing to share) and ρ = 1 for user-defined MTL (completely shared). Since S is a real-valued matrix in our scenario, we normalise it before applying Eq. 9: First we take absolute values, because large either positive or negative value suggests a significant coefficient. Second we normalise each column of S by applying a softmax function, so the sum of every column is 1. The motivation behind the second step is to make a matched range of our S with S = IT or S = [11×T ;0], as for those two cases, the sum of each column is 1 and the range is [0, 1].\nFor the Omniglot experiment, we plot the measured sharing amount for training fraction 10%. Fig. 4 reveals that three proposed methods tend to share more for bottom layers (‘Conv1’, ‘Conv2’, and ‘Conv3’) and share less for top layer (‘FC1’). This is qualitatively similar to the best user-defined MTL, where the first three layers are fully shared (ρ = 1) and the 4th layer is completely not shared (ρ = 0). However, our methods: (i) learn this structure in a purely data-driven way and (ii) benefits from the ability to smoothly interpolate between high and low degrees of sharing as depth increases. As an illustration, Fig. 4 also shows example text from the most and least similar language pairs as estimated at our multilingual character recogniser’s FC1 layer (the result can vary across layers)."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we propose a novel framework for end-to-end multi-task representation learning in contemporary deep neural networks. The key idea is to generalise matrix factorisation-based multitask ideas to tensor factorisation, in order to flexibly share knowledge in fully connected and convolutional DNN layers. Our method provides consistently better performance than single task learning and comparable or better performance than the best results from exhaustive search of user-defined MTL architectures. It reduces the design choices and architectural search space that must be explored in the workflow of Deep MTL architecture design (Caruana, 1997; Zhang et al., 2014; Liu et al., 2015), relieving researchers of the need to decide how to structure layer sharing/segregation. Instead sharing structure is determined in a data-driven way on a layer-by-layer basis that moreover allows a smooth interpolation between sharing and not sharing in progressively deeper layers.\nAcknowledgements This work was supported by EPSRC (EP/L023385/1), and the European Union’s Horizon 2020 research and innovation program under grant agreement No 640891."
    }, {
      "heading" : "A COMPARISON WITH CLASSIC (SHALLOW) MTL METHODS",
      "text" : "We provide a comparison with classic (shallow, matrix-based) MTL methods for the first experiment (MNIST, binary one-vs-rest classification, 1% training data, mean of error rates for 10-fold CV). A subtlety in making this comparison is what feature should the classic methods use? Conventionally they use a PCA feature (obtained by flattening the image, then dimension reduction by PCA). However for visual recognition tasks, performance is better with deep features – a key motivation for our focus on deep approaches to MTL. We therefore also compare the classic methods when using a feature extracted from the penultimate layer of the CNN network used in our experiment.\nAs expected, the classic methods improve on STL, and they perform significantly better with CNN than PCA features. However, our DMTRL methods still outperform the best classic methods, even when they are enhanced by CNN features. This is due to soft (cf hard) sharing of the feature extraction layers and the ability of end-to-end training of both the classifier and feature extractor. Finally, we note that more fundamentally, the classic methods are restricted to binary problems (due to their matrix-based nature) and so, unlike our tensor-based approach, they are unsuitable for multi-class problems like omniglot and age-group classification."
    }, {
      "heading" : "B MODEL CAPACITY AND PERFORMANCE",
      "text" : "We list the number of parameters for each model in the first experiment (MNIST, binary one-vs-rest classification) and the performance (1% training data, mean of error rate for 10-fold CV).\nThe conventional hard-sharing method (UD-MTL) design is to share all layers except the top layer. Its number of parameter is roughly 10% of the single task learning method (STL), as most parameters are shared across the 10 tasks corresponding to 10 digits. Our soft-sharing methods also significantly reduce the number of parameters compared to STL, but are larger than UD-MTL’s hard sharing.\nTo compare our method to UD-MTL, while controlling for network capacity, we expanded UDMDL by adding more hidden neurons so its number of parameter is close to our methods (denoted UD-MTL-Large). However UD-MDL performance does not increase. This is evidence that our model’s good performance is not simply due to greater capacity than UD-MTL."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "cent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-task gaussian process prediction",
      "author" : [ "Edwin V Bonilla", "Kian M Chai", "Christopher Williams" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Bonilla et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bonilla et al\\.",
      "year" : 2007
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Caruana.,? \\Q1997\\E",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "Hal Daumé III" ],
      "venue" : "In ACL,",
      "citeRegEx" : "III.,? \\Q2007\\E",
      "shortCiteRegEx" : "III.",
      "year" : 2007
    }, {
      "title" : "Age and gender estimation of unfiltered faces",
      "author" : [ "Eran Eidinger", "Roee Enbar", "Tal Hassner" ],
      "venue" : "IEEE Transactions on Information Forensics and Security,",
      "citeRegEx" : "Eidinger et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Eidinger et al\\.",
      "year" : 2014
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "In Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Evgeniou and Pontil.,? \\Q2004\\E",
      "shortCiteRegEx" : "Evgeniou and Pontil.",
      "year" : 2004
    }, {
      "title" : "The indian buffet process: An introduction and review",
      "author" : [ "Thomas L. Griffiths", "Zoubin Ghahramani" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Griffiths and Ghahramani.,? \\Q2011\\E",
      "shortCiteRegEx" : "Griffiths and Ghahramani.",
      "year" : 2011
    }, {
      "title" : "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
      "author" : [ "Jui-Ting Huang", "Jinyu Li", "Dong Yu", "Li Deng", "Yifan Gong" ],
      "venue" : "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),",
      "citeRegEx" : "Huang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2013
    }, {
      "title" : "Clustered multi-task learning: A convex formulation",
      "author" : [ "Laurent Jacob", "Jean-philippe Vert", "Francis R Bach" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Jacob et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jacob et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning with whom to share in multi-task feature learning",
      "author" : [ "Zhuoliang Kang", "Kristen Grauman", "Fei Sha" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2011
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Tamara G. Kolda", "Brett W. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Kolda and Bader.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolda and Bader.",
      "year" : 2009
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "Abhishek Kumar", "Hal Daumé III" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kumar and III.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kumar and III.",
      "year" : 2012
    }, {
      "title" : "Human-level concept learning through probabilistic program induction",
      "author" : [ "Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum" ],
      "venue" : null,
      "citeRegEx" : "Lake et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2015
    }, {
      "title" : "A multilinear singular value decomposition",
      "author" : [ "Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Lathauwer et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lathauwer et al\\.",
      "year" : 2000
    }, {
      "title" : "Speeding-up convolutional neural networks using fine-tuned cp-decomposition",
      "author" : [ "Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan V. Oseledets", "Victor S. Lempitsky" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Lebedev et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lebedev et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Age and gender classification using convolutional neural networks",
      "author" : [ "G. Levi", "T. Hassncer" ],
      "venue" : "In Computer Vision and Pattern Recognition Workshops (CVPRW),",
      "citeRegEx" : "Levi and Hassncer.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levi and Hassncer.",
      "year" : 2015
    }, {
      "title" : "Representation learning using multi-task deep neural networks for semantic classification and information",
      "author" : [ "Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Tensorizing neural networks",
      "author" : [ "Alexander Novikov", "Dmitry Podoprikhin", "Anton Osokin", "Dmitry Vetrov" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Novikov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Novikov et al\\.",
      "year" : 2015
    }, {
      "title" : "Tensor-train decomposition",
      "author" : [ "I.V. Oseledets" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Oseledets.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oseledets.",
      "year" : 2011
    }, {
      "title" : "Flexible modeling of latent task structures in multitask learning",
      "author" : [ "Alexandre Passos", "Piyush Rai", "Jacques Wainer", "Hal Daumé III" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Passos et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Passos et al\\.",
      "year" : 2012
    }, {
      "title" : "Multilinear multitask learning",
      "author" : [ "Bernardino Romera-paredes", "Hane Aung", "Nadia Bianchi-berthouze", "Massimiliano Pontil" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Romera.paredes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Romera.paredes et al\\.",
      "year" : 2013
    }, {
      "title" : "To transfer or not to transfer",
      "author" : [ "Michael T. Rosenstein", "Zvika Marx", "Leslie Pack Kaelbling", "Thomas G. Dietterich" ],
      "venue" : "NIPS Workshop, Inductive Transfer: 10 Years Later,",
      "citeRegEx" : "Rosenstein et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rosenstein et al\\.",
      "year" : 2005
    }, {
      "title" : "Data-effiicient temporal regression with multitask recurrent neural networks",
      "author" : [ "Sigurd Spieckermann", "Steffen Udluft", "Thomas Runkler" ],
      "venue" : "In NIPS Workshop on Transfer and Multi-Task Learning,",
      "citeRegEx" : "Spieckermann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Spieckermann et al\\.",
      "year" : 2014
    }, {
      "title" : "Cluster adaptive training for deep neural network based acoustic model",
      "author" : [ "Tian Tan", "Yanmin Qian", "Kai Yu" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech & Language Processing,",
      "citeRegEx" : "Tan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2016
    }, {
      "title" : "Some mathematical notes on three-mode factor analysis",
      "author" : [ "L.R. Tucker" ],
      "venue" : null,
      "citeRegEx" : "Tucker.,? \\Q1966\\E",
      "shortCiteRegEx" : "Tucker.",
      "year" : 1966
    }, {
      "title" : "Multitask learning meets tensor factorization: task imputation via convex optimization",
      "author" : [ "Kishan Wimalawarne", "Masashi Sugiyama", "Ryota Tomioka" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Wimalawarne et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wimalawarne et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-task learning for classification with dirichlet process priors",
      "author" : [ "Ya Xue", "Xuejun Liao", "Lawrence Carin", "Balaji Krishnapuram" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Xue et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2007
    }, {
      "title" : "A unified perspective on multi-domain and multi-task learning",
      "author" : [ "Yongxin Yang", "Timothy M. Hospedales" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Yang and Hospedales.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang and Hospedales.",
      "year" : 2015
    }, {
      "title" : "Facial landmark detection by deep multi-task learning",
      "author" : [ "Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang" ],
      "venue" : "In European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Early work in this area focused on neural network models (Caruana, 1997), while more recent methods have shifted focus to kernel methods, sparsity and low-dimensional task representations of linear models (Evgeniou & Pontil, 2004; Argyriou et al.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Early work in this area focused on neural network models (Caruana, 1997), while more recent methods have shifted focus to kernel methods, sparsity and low-dimensional task representations of linear models (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012).",
      "startOffset" : 205,
      "endOffset" : 278
    }, {
      "referenceID" : 30,
      "context" : "While the machine learning community has focused on MTL for shallow linear models recently, applications have continued to exploit neural network MTL (Zhang et al., 2014; Liu et al., 2015).",
      "startOffset" : 150,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "While the machine learning community has focused on MTL for shallow linear models recently, applications have continued to exploit neural network MTL (Zhang et al., 2014; Liu et al., 2015).",
      "startOffset" : 150,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "The typical design pattern dates back at least 20 years (Caruana, 1997): define a DNN with shared lower representation layers, which then forks into separate layers and losses for each task.",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "This contribution can be seen as generalising shallow MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012) to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL (Caruana, 1997; Zhang et al.",
      "startOffset" : 66,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : ", 2008; Kumar & Daumé III, 2012) to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL (Caruana, 1997; Zhang et al., 2014; Spieckermann et al., 2014; Liu et al., 2015) which currently must be defined manually on a problem-by-problem basis.",
      "startOffset" : 142,
      "endOffset" : 222
    }, {
      "referenceID" : 30,
      "context" : ", 2008; Kumar & Daumé III, 2012) to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL (Caruana, 1997; Zhang et al., 2014; Spieckermann et al., 2014; Liu et al., 2015) which currently must be defined manually on a problem-by-problem basis.",
      "startOffset" : 142,
      "endOffset" : 222
    }, {
      "referenceID" : 24,
      "context" : ", 2008; Kumar & Daumé III, 2012) to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL (Caruana, 1997; Zhang et al., 2014; Spieckermann et al., 2014; Liu et al., 2015) which currently must be defined manually on a problem-by-problem basis.",
      "startOffset" : 142,
      "endOffset" : 222
    }, {
      "referenceID" : 18,
      "context" : ", 2008; Kumar & Daumé III, 2012) to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL (Caruana, 1997; Zhang et al., 2014; Spieckermann et al., 2014; Liu et al., 2015) which currently must be defined manually on a problem-by-problem basis.",
      "startOffset" : 142,
      "endOffset" : 222
    }, {
      "referenceID" : 30,
      "context" : "Heterogeneous MTL: Each task corresponds to a unique set of output(s) (Zhang et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "It is a natural generalisation of shallow MTL methods that explicitly or implicitly are based on matrix factorisation (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Daumé III, 2007).",
      "startOffset" : 118,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "For the simplest NN architecture – no hidden layer, single output – our method reduces to matrix-based ones, therefore matrix-based methods including (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Daumé III, 2007) are special cases of ours.",
      "startOffset" : 150,
      "endOffset" : 240
    }, {
      "referenceID" : 1,
      "context" : "For example, posing an `2,1 norm on W to encourage low-rank W (Argyriou et al., 2008).",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 28,
      "context" : ", (Xue et al., 2007) assumes S·,i (the ith column of S) is a unit vector generated by a Dirichlet Process and (Passos et al.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 21,
      "context" : ", 2007) assumes S·,i (the ith column of S) is a unit vector generated by a Dirichlet Process and (Passos et al., 2012) models W using linear factor analysis with Indian Buffet Process (Griffiths & Ghahramani, 2011) prior on S.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : ", (Lebedev et al., 2015; Novikov et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : ", (Lebedev et al., 2015; Novikov et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "Knowledge sharing is then achieved by imposing tensor norms on W (Romera-paredes et al., 2013; Wimalawarne et al., 2014).",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : "Knowledge sharing is then achieved by imposing tensor norms on W (Romera-paredes et al., 2013; Wimalawarne et al., 2014).",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs (Caruana, 1997).",
      "startOffset" : 118,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output.",
      "startOffset" : 51,
      "endOffset" : 221
    }, {
      "referenceID" : 2,
      "context" : "This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output.",
      "startOffset" : 51,
      "endOffset" : 221
    }, {
      "referenceID" : 9,
      "context" : "This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output.",
      "startOffset" : 51,
      "endOffset" : 221
    }, {
      "referenceID" : 22,
      "context" : "This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output.",
      "startOffset" : 51,
      "endOffset" : 221
    }, {
      "referenceID" : 27,
      "context" : "This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output.",
      "startOffset" : 51,
      "endOffset" : 221
    }, {
      "referenceID" : 25,
      "context" : "In a related example of speaker-adaptive speech recognition (Tan et al., 2016) there may be several clusters in the data (e.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "For example, posing an `2,1 norm on W to encourage low-rank W (Argyriou et al., 2008). Similarly, (Kumar & Daumé III, 2012) factorises W as W = LS, i.e., it assigns a lower rank as a hyper-parameter. An earlier work (Evgeniou & Pontil, 2004) proposes that the linear model for each task t can be written as wt = ŵt + ŵ0. This is the factorisation L = [ŵ0, ŵ1, . . . , ŵT ] and S = [11×T ; IT ]. In fact, such matrix factorisation encompasses many MTL methods. E.g., (Xue et al., 2007) assumes S·,i (the ith column of S) is a unit vector generated by a Dirichlet Process and (Passos et al., 2012) models W using linear factor analysis with Indian Buffet Process (Griffiths & Ghahramani, 2011) prior on S. Tensor Factorisation In deep learning, tensor factorisation has been used to exploit factorised tensors’ fewer parameters than the original (e.g., 4-way convolutional kernel) tensor, and thus compress and/or speed up the model, e.g., (Lebedev et al., 2015; Novikov et al., 2015). For shallow linear MTL, tensor factorisation has been used to address problems where tasks are described by multiple independent factors rather than merely indexed by a single factor (Yang & Hospedales, 2015). Here the D-dimensional linear models for all unique tasks stack into a tensor W , of e.g. D × T1 × T2 in the case of two task factors. Knowledge sharing is then achieved by imposing tensor norms on W (Romera-paredes et al., 2013; Wimalawarne et al., 2014). Our framework factors tensors for the different reason that for DNN models, parameters include convolutional kernels (N -way tensors) or D1 × D2 FC layer weight matrices (2-way tensors). Stacking up these parameters for many tasks results in D1 × · · · ×DN × T tensors within which we share knowledge through factorisation. Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs (Caruana, 1997). This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output. Heterogeneous MTL typically uses neural networks with multiple sets of outputs and losses. E.g., Huang et al. (2013) proposes a shared-hidden-layer DNN model for multilingual speech processing, where each task corresponds to an individual language.",
      "startOffset" : 63,
      "endOffset" : 2309
    }, {
      "referenceID" : 1,
      "context" : "For example, posing an `2,1 norm on W to encourage low-rank W (Argyriou et al., 2008). Similarly, (Kumar & Daumé III, 2012) factorises W as W = LS, i.e., it assigns a lower rank as a hyper-parameter. An earlier work (Evgeniou & Pontil, 2004) proposes that the linear model for each task t can be written as wt = ŵt + ŵ0. This is the factorisation L = [ŵ0, ŵ1, . . . , ŵT ] and S = [11×T ; IT ]. In fact, such matrix factorisation encompasses many MTL methods. E.g., (Xue et al., 2007) assumes S·,i (the ith column of S) is a unit vector generated by a Dirichlet Process and (Passos et al., 2012) models W using linear factor analysis with Indian Buffet Process (Griffiths & Ghahramani, 2011) prior on S. Tensor Factorisation In deep learning, tensor factorisation has been used to exploit factorised tensors’ fewer parameters than the original (e.g., 4-way convolutional kernel) tensor, and thus compress and/or speed up the model, e.g., (Lebedev et al., 2015; Novikov et al., 2015). For shallow linear MTL, tensor factorisation has been used to address problems where tasks are described by multiple independent factors rather than merely indexed by a single factor (Yang & Hospedales, 2015). Here the D-dimensional linear models for all unique tasks stack into a tensor W , of e.g. D × T1 × T2 in the case of two task factors. Knowledge sharing is then achieved by imposing tensor norms on W (Romera-paredes et al., 2013; Wimalawarne et al., 2014). Our framework factors tensors for the different reason that for DNN models, parameters include convolutional kernels (N -way tensors) or D1 × D2 FC layer weight matrices (2-way tensors). Stacking up these parameters for many tasks results in D1 × · · · ×DN × T tensors within which we share knowledge through factorisation. Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs (Caruana, 1997). This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output. Heterogeneous MTL typically uses neural networks with multiple sets of outputs and losses. E.g., Huang et al. (2013) proposes a shared-hidden-layer DNN model for multilingual speech processing, where each task corresponds to an individual language. Zhang et al. (2014) uses a DNN to find facial landmarks (regression) as well as recognise facial attributes (classification); while Liu et al.",
      "startOffset" : 63,
      "endOffset" : 2461
    }, {
      "referenceID" : 1,
      "context" : "For example, posing an `2,1 norm on W to encourage low-rank W (Argyriou et al., 2008). Similarly, (Kumar & Daumé III, 2012) factorises W as W = LS, i.e., it assigns a lower rank as a hyper-parameter. An earlier work (Evgeniou & Pontil, 2004) proposes that the linear model for each task t can be written as wt = ŵt + ŵ0. This is the factorisation L = [ŵ0, ŵ1, . . . , ŵT ] and S = [11×T ; IT ]. In fact, such matrix factorisation encompasses many MTL methods. E.g., (Xue et al., 2007) assumes S·,i (the ith column of S) is a unit vector generated by a Dirichlet Process and (Passos et al., 2012) models W using linear factor analysis with Indian Buffet Process (Griffiths & Ghahramani, 2011) prior on S. Tensor Factorisation In deep learning, tensor factorisation has been used to exploit factorised tensors’ fewer parameters than the original (e.g., 4-way convolutional kernel) tensor, and thus compress and/or speed up the model, e.g., (Lebedev et al., 2015; Novikov et al., 2015). For shallow linear MTL, tensor factorisation has been used to address problems where tasks are described by multiple independent factors rather than merely indexed by a single factor (Yang & Hospedales, 2015). Here the D-dimensional linear models for all unique tasks stack into a tensor W , of e.g. D × T1 × T2 in the case of two task factors. Knowledge sharing is then achieved by imposing tensor norms on W (Romera-paredes et al., 2013; Wimalawarne et al., 2014). Our framework factors tensors for the different reason that for DNN models, parameters include convolutional kernels (N -way tensors) or D1 × D2 FC layer weight matrices (2-way tensors). Stacking up these parameters for many tasks results in D1 × · · · ×DN × T tensors within which we share knowledge through factorisation. Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs (Caruana, 1997). This differs from the previously discussed studies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly assume that each task has a single output. Heterogeneous MTL typically uses neural networks with multiple sets of outputs and losses. E.g., Huang et al. (2013) proposes a shared-hidden-layer DNN model for multilingual speech processing, where each task corresponds to an individual language. Zhang et al. (2014) uses a DNN to find facial landmarks (regression) as well as recognise facial attributes (classification); while Liu et al. (2015) proposes a DNN for query classification and information retrieval (ranking for web search).",
      "startOffset" : 63,
      "endOffset" : 2591
    }, {
      "referenceID" : 26,
      "context" : "Unlike for matrices, there are multiple definitions of tensor factorisation, and we use Tucker (Tucker, 1966) and Tensor Train (TT) (Oseledets, 2011) decompositions.",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Unlike for matrices, there are multiple definitions of tensor factorisation, and we use Tucker (Tucker, 1966) and Tensor Train (TT) (Oseledets, 2011) decompositions.",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "However (Lathauwer et al., 2000) treat it as a higher-order singular value decomposition (HOSVD), which is more efficient to solve: U (n) is exactly the U matrix from the SVD of mode-n flattening W(n) ofW , and the core tensor S is obtained by,",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "The TT decomposition is typically realised with a recursive SVD-based solution (Oseledets, 2011).",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "This is in contrast to the conventional Deep-MTL approach of manually selecting a set of layers to undergo hard parameter sharing: by tying weights so each task uses exactly the same weight matrix/tensor for the corresponding layer (Zhang et al., 2014; Liu et al., 2015); and a set of layers to be completely separate: by using independent weight matrices/tensors.",
      "startOffset" : 232,
      "endOffset" : 270
    }, {
      "referenceID" : 18,
      "context" : "This is in contrast to the conventional Deep-MTL approach of manually selecting a set of layers to undergo hard parameter sharing: by tying weights so each task uses exactly the same weight matrix/tensor for the corresponding layer (Zhang et al., 2014; Liu et al., 2015); and a set of layers to be completely separate: by using independent weight matrices/tensors.",
      "startOffset" : 232,
      "endOffset" : 270
    }, {
      "referenceID" : 16,
      "context" : "We use a modified LeNet (LeCun et al., 1998) as the CNN architecture.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Conventional matrix-based MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) are linear models taking vector input only, so they need a preprocessing that flattens the image into a vector, and typically reduce dimension by PCA.",
      "startOffset" : 38,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Conventional matrix-based MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) are linear models taking vector input only, so they need a preprocessing that flattens the image into a vector, and typically reduce dimension by PCA.",
      "startOffset" : 38,
      "endOffset" : 166
    }, {
      "referenceID" : 27,
      "context" : "Conventional matrix-based MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar & Daumé III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) are linear models taking vector input only, so they need a preprocessing that flattens the image into a vector, and typically reduce dimension by PCA.",
      "startOffset" : 38,
      "endOffset" : 166
    }, {
      "referenceID" : 30,
      "context" : "Thus to find a stronger MTL competitor, we instead search user defined architectures for Deep-MTL parameter sharing (cf (Zhang et al., 2014; Liu et al., 2015; Caruana, 1997)).",
      "startOffset" : 120,
      "endOffset" : 173
    }, {
      "referenceID" : 18,
      "context" : "Thus to find a stronger MTL competitor, we instead search user defined architectures for Deep-MTL parameter sharing (cf (Zhang et al., 2014; Liu et al., 2015; Caruana, 1997)).",
      "startOffset" : 120,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : "Thus to find a stronger MTL competitor, we instead search user defined architectures for Deep-MTL parameter sharing (cf (Zhang et al., 2014; Liu et al., 2015; Caruana, 1997)).",
      "startOffset" : 120,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "Further Discussion For a slightly unfair comparison, in the case of binary classification with 1000 training data, shallow matrix-based MTL methods with PCA feature (Kang et al., 2011; Kumar & Daumé III, 2012) reported 14.",
      "startOffset" : 165,
      "endOffset" : 209
    }, {
      "referenceID" : 10,
      "context" : "Since the error rates in (Kang et al., 2011; Kumar & Daumé III, 2012) were produced on a private subset of MNIST dataset with PCA representations only, to ensure a direct comparison, we implement several classic MTL methods and compare them in Appendix A.",
      "startOffset" : 25,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Dataset, Settings and Baselines The AdienceFaces (Eidinger et al., 2014) is a large-scale face images dataset with the labels of each person’s gender and age group.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "This is the negative transfer phenomenon (Rosenstein et al., 2005), where using a transfer learning algorithm is worse than not using it.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "Dataset, Settings and Baselines We next consider the task of learning to recognise handwritten letters in multiple languages using the Omniglot (Lake et al., 2015) dataset.",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "It reduces the design choices and architectural search space that must be explored in the workflow of Deep MTL architecture design (Caruana, 1997; Zhang et al., 2014; Liu et al., 2015), relieving researchers of the need to decide how to structure layer sharing/segregation.",
      "startOffset" : 131,
      "endOffset" : 184
    }, {
      "referenceID" : 30,
      "context" : "It reduces the design choices and architectural search space that must be explored in the workflow of Deep MTL architecture design (Caruana, 1997; Zhang et al., 2014; Liu et al., 2015), relieving researchers of the need to decide how to structure layer sharing/segregation.",
      "startOffset" : 131,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : "It reduces the design choices and architectural search space that must be explored in the workflow of Deep MTL architecture design (Caruana, 1997; Zhang et al., 2014; Liu et al., 2015), relieving researchers of the need to decide how to structure layer sharing/segregation.",
      "startOffset" : 131,
      "endOffset" : 184
    } ],
    "year" : 2017,
    "abstractText" : "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.",
    "creator" : "LaTeX with hyperref package"
  }
}