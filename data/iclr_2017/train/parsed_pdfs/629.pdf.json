{
  "name" : "629.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HUMAN PERCEPTION IN COMPUTER VISION / CONFERENCE SUBMISSIONS",
    "authors" : [ "Ron Dekel" ],
    "emails" : [ "ron.dekel@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : "1 QUICK EXPERT SUMMARY",
      "text" : "Considering the learned computation of ImageNet-trained DNNs, we find:\n• Large computation changes for perceptually salient image changes (Figure 1). • Gestalt: segmentation, crowding, and shape interactions in computation (Figure 2). • Contrast constancy: bandpass transduction in first layers is later corrected (Figure 3).\nThese properties are reminiscent of human perception, perhaps because learned general-purpose classifiers (human and DNN) tend to converge."
    }, {
      "heading" : "2 INTRODUCTION",
      "text" : "Deep neural networks (DNNs) are a class of computer learning algorithms that have become widely used in recent years (LeCun et al., 2015). By training with millions of examples, such models achieve unparalleled degrees of task-trained accuracy (Krizhevsky et al., 2012). This is not unprecedented on its own - steady progress has been made in computer vision for decades, and to some degree current designs are just scaled versions of long-known principles (Lecun et al., 1998). In previous models, however, only the design is general-purpose, while learning is mostly specific to the context of a trained task. Interestingly, for current DNNs trained to solve a large-scale image recognition problem (Russakovsky et al., 2014), the learned computation is useful as a building block for drastically different and untrained visual problems (Huh et al., 2016; Yosinski et al., 2014).\nFor example, orientation- and frequency-selective features (Gabor patches) can be considered general-purpose visual computations. Such features are routinely discovered by DNNs (Krizhevsky et al., 2012; Zeiler & Fergus, 2013), by other learning algorithms (Hinton & Salakhutdinov, 2006;\n∗https://sites.google.com/site/rondekelhomepage/\nLee et al., 2008; 2009; Olshausen & Field, 1997), and are extensively hard-coded in computer vision (Jain & Farrokhnia, 1991). Furthermore, a similar computation is believed to underlie the spatial response properties of visual neurons of diverse animal phyla (Carandini et al., 2005; DeAngelis et al., 1995; Hubel & Wiesel, 1968; Seelig & Jayaraman, 2013), and is evident in human visual perception (Campbell & Robson, 1968; Fogel & Sagi, 1989; Neri et al., 1999). This diversity culminates in satisfying theoretical arguments as to why Gabor-like features are so useful in general-purpose vision (Olshausen, 1996; Olshausen & Field, 1997).\nAs an extension, general-purpose computations are perhaps of universal use. For example, a dimensionality reduction transformation that optimally preserves recognition-relevant information may constitute an ideal computation for both DNN and animal. More formally, different learning algorithms with different physical implementations may converge to the same computation when similar (or sufficiently general) problems are solved near-optimally. Following this line of reasoning, DNN models with good general-purpose computations may be computationally similar to biological visual systems, even more so than less accurate and less general biologically plausible simulations (Kriegeskorte, 2015; Yamins & DiCarlo, 2016).\nRelated work seems to be consistent with computation convergence. First, different DNN training regimes seem to converge to a similar learned computation (Li et al., 2015; Zhou et al., 2014). Second, image representation may be similar in trained DNN and in biological visual systems. That is, when the same images are processed by DNN and by humans or monkeys, the final DNN computation stages are strong predictors of human fMRI and monkey electrophysiology data collected from visual areas V4 and IT (Cadieu et al., 2014; Khaligh-Razavi & Kriegeskorte, 2014; Yamins et al., 2014). Furthermore, more accurate DNN models exhibit stronger predictive power (Cadieu et al., 2014; Dubey & Agarwal, 2016; Yamins et al., 2014), and the final DNN computation stage is even a strong predictor of human-perceived shape discrimination (Kubilius et al., 2016). However, some caution is perhaps unavoidable, since measured similarity may be confounded with categorization consistency, view-invariance resilience, or similarity in the inherent difficulty of the tasks undergoing comparison. A complementary approach is to consider images that were produced by optimizing trained DNN-based perceptual metrics (Gatys et al., 2015a;b; Johnson et al., 2016; Ledig et al., 2016), which perhaps yields undeniable evidence of non-trivial computational similarity, although a more objective approach may be warranted.\nHere, we quantify the similarity between human visual perception, as measured by psychophysical experiments, and individual computational stages (layers) in feed-forward DNNs trained on a large-scale image recognition problem (ImageNet LSVRC). Comparison is achieved by feeding the experimental image stimuli to the trained DNN and comparing a DNN metric (mean mutual information or mean absolute change) to perceptual data. The use of reduced (simplified and typically non-natural) stimuli ensures identical inherent task difficulty across compared categories and prevents confounding of categorization consistency with measured similarity. Perception, a systemlevel computation, may be influenced less by the architectural discrepancy (biology vs. DNN) than are neural recordings."
    }, {
      "heading" : "3 CORRELATE FOR IMAGE CHANGE SENSITIVITY",
      "text" : "From a perceptual perspective, an image change of fixed size has different saliency depending on image context (Polat & Sagi, 1993). To investigate whether the computation in trained DNNs exhibits similar contextual modulation, we used the Local Image Masking Database (Alam et al., 2014), in which 1080 partially-overlapping images were subjected to different levels of the same random additive noise perturbation, and for each image, a psychophysical experiment determined the threshold noise level at which the added-noise image is discriminated from two noiseless copies at 75% (Figure 1a). Threshold is the objective function that is compared with an L1-distance correlate in the DNN representation. The scale of measured threshold was:\n20 · log10 (\nstd (noise) T\n) , (1)\nwhere std (noise) is the standard deviation of the additive noise, and T is the mean image pixel value calculated over the region where the noise is added (i.e. image center).\nThe DNN correlate of perceptual threshold we used was the average L1 change in DNN computation between added-noise images and the original, noiseless image. Formally,\nLi,n1 (I) = ∣∣∣ai (I + noise (n))− ai (I)∣∣∣ , (2)\nwhere ai (X) is the activation value of neuron i during the DNN feedforward pass for input image X , and the inner average (denoted by bar) is taken over repetitions with random n-sized noise (noise is introduced at random phase spectra in a fixed image location, an augmentation that follows the between-image randomization described by Alam et al., 2014; the number of repetitions was 10 or more). Unless otherwise specified, the final L1 prediction is L i,n 1 averaged across noise levels (−40 to 25 dB with 5-dB intervals) and computational neurons (first within and then across computational stages). Using L1 averaged across noise levels as a correlate for the noise level of perceptual threshold is a simple approximation with minimal assumptions.\nResults show that the L1 metric is correlated with the perceptual threshold for all tested DNN architectures (Figure 1b, 4a-c). In other words, higher values of the L1 metric (indicating larger changes in DNN computation due to image perturbation, consistent with higher perturbation saliency) are\ncorrelated with lower values of measured perceptual threshold (indicating that weaker noise levels are detectable, i.e. higher saliency once more).\nTo quantify and compare predictive power, we considered the percent of linearly explained variability (R2). For all tested DNN architectures, the prediction explains about 60% of the perceptual variability (Tables 1, 2; baselines at Tables 3-5), where inter-person similarity representing theoretical maximum is 84% (Alam et al., 2014). The DNN prediction is far more accurate than a prediction based on simple image statistical properties (e.g. RMS contrast), and is on par with a detailed perceptual model that relies on dozens of psychophysically collected parameters (Alam et al., 2014). The Spearmann correlation coefficient is much higher compared with the perceptual model (with an absolute SROCC value of about 0.79 compared with 0.70, Table 1), suggesting that the L1 metric gets the order right but not the scale. We did not compare these results with models that fit the experimental data (e.g. Alam et al., 2015; Liu & Allebach, 2016), since the L1 metric has no explicit parameters. Also, different DNN architectures exhibited high similarity in their predictions (R2 of about 0.9, e.g. Figure 4d).\nPrediction can also be made from isolated computational stages, instead of across all stages as before. This analysis shows that the predictive power peaks mid-computation across all tested image scales (Figure 1c). This peak is consistent with use of middle DNN layers to optimize perceptual metrics (Gatys et al., 2015a;b; Ledig et al., 2016), and is reminiscent of cases in which low- to mid-level vision is the performance limiting computation in the detection of at-threshold stimuli (Campbell & Robson, 1968; Del Cul et al., 2007).\nFinally, considering the images for which the L1-based prediction has a high error suggests a factor which causes a systematic inconsistency with perception (Figures 1d, 6). This factor may be related to the mean image luminance: by introducing noise perturbations according to the scale of Equation 1, a fixed noise size (in dB) corresponds to smaller pixel changes in dark compared with bright images. (Using this scales reflects an assumption of multiplicative rather than additive conservation; this assumption may be justified for the representation at the final but perhaps not the intermediate computational stages considering the log-linear contrast response discussed in Section 5). Another factor may the degree to which image content is identifiable."
    }, {
      "heading" : "4 CORRELATE FOR MODULATION OF SENSITIVITY BY CONTEXT",
      "text" : "The previous analysis suggested gross computational similarity between human perception and trained DNNs. Next, we aimed to extend the comparison to more interpretable properties of perception by considering more highly controlled designs. To this end, we considered cases in which a static background context modulates the difficulty of discriminating a foreground shape, despite no spatial overlap of foreground and background. This permits interpretation by considering the cause of the modulation.\nWe first consider segmentation, in which arrangement is better discriminated for arrays of consistently oriented lines compared with inconsistently oriented lines (Figure 2a) (Pinchuk-Yacobi et al., 2016). Crowding is considered next, where surround clutter that is similar to the discriminated target leads to deteriorated discrimination performance (Figure 2b) (Livne & Sagi, 2007). Last to be addressed is object superiority, in which a target line location is better discriminated when it is in a shape-forming layout (Figure 2c) (Weisstein & Harris, 1974). In this case, clutter is controlled by having the same fixed number of lines in context. To measure perceptual discrimination, these works introduced performance-limiting manipulations such as location jittering, brief presentation, and temporal masking. While different manipulations showed different measured values, order-of-difficulty was typically preserved. Here we changed all the original performance-limiting manipulations to location jittering (whole-shape or element-wise, see Section 8.4).\nTo quantify discrimination difficulty in DNNs, we measured the target-discriminative information of isolated neurons (where performance is limited by location jittering noise), then averaged across all neurons (first within and then across computational layer stages). Specifically, for each neuron, we measured the reduction in categorization uncertainty due to observation, termed mutual information (MI): MI (Ai;C) = H (C)−H (C|Ai) , (3) where H stands for entropy, and Ai is a random variable for the value of neuron i when the DNN processes a random image from a category defined by the random variable C. For example, if a neuron gives a value in the range of 100.0 to 200.0 when the DNN processes images from category A, and 300.0 to 400.0 for category B, then the category is always known by observing the value, and so mutual information is high (MI=1 bits). On the other extreme, if the neuron has no discriminative task information, then MI=0 bits. To measure MI, we quantized activations into eight equal-amount bins, and used 500 samples (repetitions having different location jittering noise) across categories. The motivation for this correlate is the assumption that the perceptual order-of-difficulty reflects the quantity of task-discriminative information in the representation.\nResults show that, across hundreds of configurations (varying pattern element size, target location, jitter magnitude, and DNN architecture; see Section 8.4), the qualitative order of difficulty in terms of the DNN MI metric is consistent with the order of difficulty measured in human psychophysical experiments, for the conditions addressing segmentation and crowding (Figures 2d, 7; for baseline models see Figure 8). It is interesting to note that the increase in similarity develops gradually along different layer types in the DNN computation (i.e. not just pooling layers), and is accompanied by a gradual increase in the quantity of task-relevant information (Figure 2e-g). This indicates a link between task relevance and computational similarity for the tested conditions. Note that unlike the evident increase in isolated unit task information, the task information from all units combined decreases by definition along any computational hierarchy. An intuition for this result is that the total hidden information decreases, while more accessible per-unit information increases.\nFor shape formation, four out of six shapes consistently show order of difficulty like perception, and two shapes consistently do no (caricature at Figure 2h; actual data at Figure 9)."
    }, {
      "heading" : "5 CORRELATE FOR CONTRAST SENSITIVITY",
      "text" : "A cornerstone of biological vision research is the use of sine gratings at different frequencies, orientations, and contrasts (Campbell & Robson, 1968). Notable are results showing that the lowest perceivable contrast in human perception depends on frequency. Specifically, high spatial frequencies are attenuated by the optics of the eye, and low spatial frequencies are believed to be attenuated due to processing inefficiencies (Watson & Ahumada, 2008), so that the lowest perceivable contrast is found at intermediate frequencies. (To appreciate this yourself, examine Figure 3a). Thus, for low-contrast gratings, the physical quantity of contrast is not perceived correctly: it is not preserved across spatial frequencies. Interestingly, this is corrected for gratings of higher contrasts, for which perceived contrast is more constant across spatial frequencies (Georgeson & Sullivan, 1975).\nThe DNN correlate we considered is the mean absolute change in DNN representation between a gray image and sinusoidal gratings, at all combinations of spatial frequency and contrast. Formally, for neurons in a given layer, we measured:\nL1(contrast, frequency) = 1\nNneurons Nneurons∑ i=1 ∣∣∣ai (contrast, frequency)− ai (0, 0)∣∣∣ , (4)\nwhere ai (contrast, frequency) is the average activation value of neuron i to 250 sine images (random orientation, random phase), ai (0, 0) is the response to a blank (gray) image, and Nneurons is the number of neurons in the layer. This measure reflects the overall change in response vs. the gray image.\nResults show a bandpass response for low-contrast gratings (blue lines strongly modulated by frequency, Figures 3, 10), and what appears to be a mostly constant response at high contrast for end-computation layers (red lines appear more invariant to frequency), in accordance with perception.\nWe next aimed to compare these results with perception. Data from human experiments is generally iso-output (i.e. for a pre-set output, such as 75% detection accuracy, the input is varied to find the value which produce the preset output). However, the DNN measurements here are iso-input (i.e. for a fixed input contrast the L1 is measured). As such, human data should be compared to the interpoalted inverse of DNN measurements. Specifically, for a set output value, the interpolated contrast value which produce the output is found for every frequency (Figure 11). This analysis permits quantifying the similarity of iso-output curves for human and DNN, measured here as the percent of log-Contrast variability in human measurements which is explained by the DNN predictions. This showed a high explained variability at the end computation stage (prob layer, R2 = 94%), but importantly, a similarly high value at the first computational stage (conv1 1 layer, R2 = 96%). Intiutively, while the ”internal representation” variability in terms of L1 is small, the iso-output number-of-input-contrast-cahnges variability is still high. For example. for the prob layer, about the same L1 is measured for (Contrast=1,freq=75) and for (Contrast=0.18,freq=12).\nAn interesting, unexpected observation is that the logarithmically spaced contrast inputs are linearly spaced at the end-computation layers. That is, the average change in DNN representation scales logarithmically with the size of input change. This can be quantified by the correlation of output L1 with log Contrast input, which showed R2 = 98% (averaged across spatial frequencies) for prob, while much lower values were observed for early and middle layers (up to layer fc7). The same computation when scrambling the learned parameters of the model showed R2 = 60%. Because the degree of log-linearity observed was extremely high, it may be an important emergent property of the learned DNN computation, which may deserve further investigation. However, this property is only reminiscent and not immediately consistent with the perceptual power-law scaling (Gottesman et al., 1981)."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : ""
    }, {
      "heading" : "6.1 HUMAN PERCEPTION IN COMPUTER VISION",
      "text" : "It may be tempting to believe that what we see is the result of a simple transformation of visual input. Centuries of psychophysics have, however, revealed complex properties in perception, by crafting stimuli that isolate different perceptual properties. In our study, we used the same stimuli to investigate the learned properties of deep neural networks (DNNs), which are the leading computer vision algorithms to date (LeCun et al., 2015).\nThe DNNs we used were trained in a supervised fashion to assign labels to input images. To some degree, this task resembles the simple verbal explanations given to children by their parents. Since human perception is obviously much richer than the simple external supervision provided, we were not surprised to find that the best correlate for perceptual saliency of image changes is a part of the DNN computation that is only supervised indirectly (i.e. the mid-computation stage). This similarity is so strong, that even with no fine-tuning to human perception, the DNN metric is competitively accurate, even compared with a direct model of perception.\nThis strong, quantifiable similarity to a gross aspect of perception may, however, reflect a mix of similarities and discrepancies in different perceptual properties. To address isolated perceptual effects, we considered experiments that manipulate a spatial interaction, where the difficulty of discriminating a foreground target is modulated by a background context. Results showed modulation of DNN target diagnostic, isolated unit information, consistent with the modulation found in perceptual discrimination. This was shown for contextual interactions reflecting grouping/segmentation (Harris et al., 2015), crowding/clutter (Livne & Sagi, 2007; Pelli et al., 2004), and shape superiority (Weisstein & Harris, 1974). DNN similarity to these groupings/gestalt phenomena appeared at the end-computation stages.\nNo less interesting, are the cases in which there is no similarity. For example, perceptual effects related to 3D (Erdogan & Jacobs, 2016) and symmetry (Pramod & Arun, 2016) do not appear to have a strong correlate in the DNN computation. Indeed, it may be interesting to investigate the influence of visual experience in these cases. And, equally important, similarity should be considered in terms of specific perceptual properties rather than as a general statement."
    }, {
      "heading" : "6.2 RECURRENT VS. FEEDFORWARD CONNECTIVITY",
      "text" : "In the human hierarchy of visual processing areas, information is believed to be processed in a feedforward sweep, followed by recurrent processing loops (top-down and lateral) (Lamme & Roelfsema, 2000). Thus, for example, the early visual areas can perform deep computations. Since mapping from visual areas to DNN computational layers is not simple, it will not be considered here. (Note that ResNet connectivity is perhaps reminiscent of unrolled recurrent processing).\nInterestingly, debate is ongoing about the degree to which visual perception is dependent on recurrent connectivity (Fabre-Thorpe et al., 1998; Hung et al., 2005): recurrent representations are obviously richer, but feedforward computations converge much faster. An implicit question here regarding the extent of feasible feed-forward representations is, perhaps: Can contour segmentation, contextual influences, and complex shapes be learned? Based on the results reported here for feedforward DNNs, a feedforward representation may seem sufficient. However, the extent to which this is true may be very limited. In this study we used small images with a small number of lines, while effects such as contour integration seem to take place even in very large configurations (Field et al., 1993). Such scaling seems more likely in a recurrent implementation. As such, a reasonable hypothesis may be that the full extent of contextual influence is only realizable with recurrence, while feedforward DNNs learn a limited version by converging towards a useful computation."
    }, {
      "heading" : "6.3 IMPLICATIONS AND FUTURE WORK",
      "text" : ""
    }, {
      "heading" : "6.3.1 USE IN BRAIN MODELING",
      "text" : "The use of DNNs in modeling of visual perception (or of biological visual systems in general) is subject to a tradeoff between accuracy and biological plausibility. In terms of architecture, other deep models better approximate our current understanding of the visual system (Riesenhuber &\nPoggio, 1999; Serre, 2014). However, the computation in trained DNN models is quite generalpurpose (Huh et al., 2016; Yosinski et al., 2014) and offers unparalleled accuracy in recognition tasks (LeCun et al., 2015). Since visual computations are, to some degree, task- rather than architecturedependent, an accurate and general-purpose DNN model may better resemble biological processing than less accurate biologically plausible ones (Kriegeskorte, 2015; Yamins & DiCarlo, 2016). We support this view by considering a controlled condition in which similarity is not confounded with task difficulty or categorization consistency."
    }, {
      "heading" : "6.3.2 USE IN PSYCHOPHYSICS",
      "text" : "Our results imply that trained DNN models have good predictive value for outcomes of psychophysical experiments, permitting a zero-cost first-order approximation. Note, however, that the scope of such simulations may be limited, since learning (Sagi, 2011) and adaptation (Webster, 2011) were not considered here.\nAnother fascinating option is the formation of hypotheses in terms of mathematically differentiable trained-DNN constraints, whereby it is possible to efficiently solve for the visual stimuli that optimally dissociate the hypotheses (see Gatys et al. 2015a;b; Mordvintsev et al. 2015 and note Goodfellow et al. 2014; Szegedy et al. 2013). The conclusions drawn from such stimuli can be independent of the theoretical assumptions about the generating process (for example, creating new visual illusions that can be seen regardless of how they were created)."
    }, {
      "heading" : "6.3.3 USE IN ENGINEERING (A PERCEPTUAL LOSS METRIC)",
      "text" : "As proposed previously (Dosovitskiy & Brox, 2016; Johnson et al., 2016; Ledig et al., 2016), the saliency of small image changes can be estimated as the representational distance in trained DNNs. Here, we quantified this approach by relying on data from a controlled psychophysical experiment (Alam et al., 2014). We found the metric to be far superior to simple image statistical properties, and on par with a detailed perceptual model (Alam et al., 2014). This metric can be useful in image compression, whereby optimizing degradation across image sub-patches by comparing perceptual loss may minimize visual artifacts and content loss."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Yoram Bonneh for his valuable questions which led to much of this work."
    }, {
      "heading" : "7 APPENDIX: FIGURES AND TABLES",
      "text" : ""
    }, {
      "heading" : "8 APPENDIX: EXPERIMENTAL SETUP",
      "text" : ""
    }, {
      "heading" : "8.1 DNN MODELS",
      "text" : "To collect DNN computation snapshots, we used MATLAB with MatConvNet version 1.0-beta20 (Vedaldi & Lenc, 2015). All MATLAB code will be made available upon acceptance of this manuscript. The pre-trained DNN models we have used are: CaffeNet (which is a variant of AlexNet provided in Caffe, Jia et al., 2014), GoogLeNet (Szegedy et al., 2014), VGG-19 (Simonyan & Zisserman, 2014), and ResNet-152 (He et al., 2015). The models were trained on the same ImageNet LSVRC. The CaffeNet model was trained using Caffe with the default ImageNet training parameters (stopping at iteration 310, 000) and imported into MatConvNet. For the GoogLeNet model, we used the imported pre-trained reference-Caffe implementation. For VGG-19 and ResNet-152, we used the imported pre-trained original versions. In all experiments input image size was 224× 224 or 227× 227."
    }, {
      "heading" : "8.2 BASELINE MODELS",
      "text" : "As baselines to compare with pre-trained DNN models, we consider: (a) a multiscale linear filter bank of Gabor functions, (b) a steerable-pyramid linear filter bank (Simoncelli & Freeman, 1995), (c) the VGG-19 model for which the learned parameters (weights) were randomly scrambled within layer, and (d) the CaffeNet model at multiple time points during training. For the Gabor decomposition, the following Gabor filters were used: all compositions of σ = {1, 2, 4, 8, 16, 32, 64}px, λ = {1, 2} · σ, orientation= {0, π/3, 2π/3, π, 4π/3, 5π/3}, and phase= {0, π/2}."
    }, {
      "heading" : "8.3 IMAGE PERTURBATION EXPERIMENT",
      "text" : "The noiseless images were obtained from Alam et al. (2014). In main text, ”image scale” refers to percent coverage of DNN input. Since size of original images (149 × 149) is smaller than DNN input of (224× 224) or (227× 227), the images were resized by a factor of 1.5 so that 100% image scale covers approximately the entire DNN input area.\nHuman psychophysics and DNN experiments were done for nearly identical images. A slight discrepancy relates to how the image is blended with the background in the special case where the region where noise is added has no image surround at one or two side. In these sides (which depend on the technical procedure with which images were obtained, see Alam et al., 2014), the surround blending here was hard, while the original was smooth."
    }, {
      "heading" : "8.4 BACKGROUND CONTEXT EXPERIMENT",
      "text" : ""
    }, {
      "heading" : "8.4.1 SEGMENTATION",
      "text" : "The images used are based on the Texture Discrimination Task (Karni & Sagi, 1991). In the variant considered here (Pinchuk-Yacobi et al., 2015), subjects were presented with a grid of lines, all of which were horizontal, except two or three that were diagonal. Subjects discriminated whether the arrangement of diagonal lines is horizontal or vertical, and this discrimination was found to be more difficult when the central line is horizontal rather than diagonal (”Hard” vs. ”Easy” in Figure 2a). To limit human performance in this task, two manipulations were applied: (a) the location of each line in the pattern was jittered, and (b) a noise mask was presented briefly after the pattern. Here we only retained (a).\nA total of 90 configurations were tested, obtained by combinations of the following alternatives:\n• Three scales: line length of 9, 12.3, or 19.4 px (number of lines co-varied with line length, see Figure 12).\n• Three levels of location jittering, defined as a multiple of line length: {1, 2, 3} · 0.0625 · l px, where l is the length of a line in the pattern. Jittering was applied separately to each line in the pattern.\n• Ten locations of diagonal lines: center, random, four locations of half-distance from center to corners, four locations of half-distance from center to image borders.\nFor each configuration, the discriminated arrangement of diagonal lines was either horizontal or vertical, and the central line was either horizontal or diagonal (i.e. hard or easy)."
    }, {
      "heading" : "8.4.2 CROWDING",
      "text" : "The images used are motivated by the crowding effect (Livne & Sagi, 2007; Pelli et al., 2004).\nA total of 90 configurations were tested, obtained by combinations of the following alternatives:\n• Three scales: font size of 15.1, 20.6, or 32.4 px (see Figure 13). • Three levels of discriminated-letter location jittering, defined as a multiple of font size: {1, 2, 3} · 0.0625 · l px, where l is font size. The jitter of surround letters (M, N, S, and T) was fixed (i.e. the background was static).\n• Ten locations: center, random, four locations of half-distance from center to corners, four locations of half-distance from center to image borders.\nFor each configuration, the discriminated letter was either A, B, C, D, E, or F, and the background was either blank (easy) or composed of the letters M, N, S, and T (hard)."
    }, {
      "heading" : "8.4.3 SHAPE",
      "text" : "The images used are based on the object superiority effect by Weisstein & Harris (1974), where discriminating a line location is easier when combined with surrounding lines a shape is formed.\nA total of 90 configurations were tested, obtained by combinations of the following alternatives:\n• Three scales: discriminated-line length of 9, 15.1, or 22.7 px (see Figure 14). • Five levels of whole-pattern location jittering, defined as a multiple of discriminated-line\nlength: {1, 2, 5, 10, 15} · 0.0625 · l px, where l is the length of the discriminated line.\n• Six ”hard” background line layouts (patterns b-f of their Figure 2 and the additional pattern f of their Figure 3 in Weisstein & Harris, 1974). The ”easy” layout was always the same (pattern a).\nFor each configuration, the line whose location is discriminated had four possible locations (two locations are shown in Figure 2c), and the surrounding background line layout could compose a shape (easy) or not (hard)."
    }, {
      "heading" : "8.5 CONTRAST SENSITIVITY EXPERIMENT",
      "text" : "Used images depicted sine gratings at different contrast, spatial frequency, sine phase, and sine orientation combinations."
    } ],
    "references" : [ {
      "title" : "Local masking in natural images: A database and analysis",
      "author" : [ "Md Mushfiqul Alam", "Kedarnath P Vilankar", "David J Field", "Damon M Chandler" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "Alam et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alam et al\\.",
      "year" : 2014
    }, {
      "title" : "A computational model for predicting local distortion visibility via convolutional neural network trainedon natural scenes",
      "author" : [ "Md Mushfiqul Alam", "Pranita Patil", "Martin T Hagan", "Damon M Chandler" ],
      "venue" : "In Image Processing (ICIP),",
      "citeRegEx" : "Alam et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Alam et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep neural networks rival the representation of primate IT cortex for core visual object recognition",
      "author" : [ "Charles F Cadieu", "Ha Hong", "Daniel L K Yamins", "Nicolas Pinto", "Diego Ardila", "Ethan A Solomon", "Najib J Majaj", "James J DiCarlo" ],
      "venue" : "PLoS computational biology,",
      "citeRegEx" : "Cadieu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cadieu et al\\.",
      "year" : 2014
    }, {
      "title" : "Application of Fourier analysis to the visibility of gratings",
      "author" : [ "Fergus W Campbell", "J G Robson" ],
      "venue" : "The Journal of physiology,",
      "citeRegEx" : "Campbell and Robson.,? \\Q1968\\E",
      "shortCiteRegEx" : "Campbell and Robson.",
      "year" : 1968
    }, {
      "title" : "Do we know what the early visual system does",
      "author" : [ "Matteo Carandini", "Jonathan B Demb", "Valerio Mante", "David J Tolhurst", "Yang Dan", "Bruno A Olshausen", "Jack L Gallant", "Nicole C Rust" ],
      "venue" : "The Journal of Neuroscience,",
      "citeRegEx" : "Carandini et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carandini et al\\.",
      "year" : 2005
    }, {
      "title" : "Receptive-field dynamics in the central visual pathways",
      "author" : [ "Gregory C DeAngelis", "Izumi Ohzawa", "Ralph D Freeman" ],
      "venue" : "Trends in neurosciences,",
      "citeRegEx" : "DeAngelis et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "DeAngelis et al\\.",
      "year" : 1995
    }, {
      "title" : "Brain dynamics underlying the nonlinear threshold for access to consciousness",
      "author" : [ "Antoine Del Cul", "Sylvain Baillet", "Stanislas Dehaene" ],
      "venue" : "PLoS Biol,",
      "citeRegEx" : "Cul et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cul et al\\.",
      "year" : 2007
    }, {
      "title" : "Generating images with perceptual similarity metrics based on deep networks",
      "author" : [ "Alexey Dosovitskiy", "Thomas Brox" ],
      "venue" : "arXiv preprint arXiv:1602.02644,",
      "citeRegEx" : "Dosovitskiy and Brox.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy and Brox.",
      "year" : 2016
    }, {
      "title" : "Examining Representational Similarity in ConvNets and the Primate Visual Cortex",
      "author" : [ "Abhimanyu Dubey", "Sumeet Agarwal" ],
      "venue" : "arXiv preprint arXiv:1609.03529,",
      "citeRegEx" : "Dubey and Agarwal.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dubey and Agarwal.",
      "year" : 2016
    }, {
      "title" : "A 3D shape inference model matches human visual object similarity judgments better than deep convolutional neural networks",
      "author" : [ "Goker Erdogan", "Robert A Jacobs" ],
      "venue" : "In Proceedings of the 38th Annual Conference of the Cognitive Science Society. Cognitive Science Society Austin,",
      "citeRegEx" : "Erdogan and Jacobs.,? \\Q2016\\E",
      "shortCiteRegEx" : "Erdogan and Jacobs.",
      "year" : 2016
    }, {
      "title" : "Rapid categorization of natural images by rhesus monkeys",
      "author" : [ "Michèle Fabre-Thorpe", "Ghislaine Richard", "Simon J Thorpe" ],
      "venue" : "Neuroreport,",
      "citeRegEx" : "Fabre.Thorpe et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fabre.Thorpe et al\\.",
      "year" : 1998
    }, {
      "title" : "Contour integration by the human visual system: evidence for a local association field",
      "author" : [ "David J Field", "Anthony Hayes", "Robert F Hess" ],
      "venue" : "Vision research,",
      "citeRegEx" : "Field et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Field et al\\.",
      "year" : 1993
    }, {
      "title" : "Gabor filters as texture discriminator",
      "author" : [ "Itzhak Fogel", "Dov Sagi" ],
      "venue" : "Biological cybernetics,",
      "citeRegEx" : "Fogel and Sagi.,? \\Q1989\\E",
      "shortCiteRegEx" : "Fogel and Sagi.",
      "year" : 1989
    }, {
      "title" : "A Neural Algorithm of Artistic Style. aug 2015a",
      "author" : [ "Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge" ],
      "venue" : null,
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks. may 2015b",
      "author" : [ "Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge" ],
      "venue" : null,
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "Contrast constancy: deblurring in human vision by spatial frequency channels",
      "author" : [ "M A Georgeson", "G D Sullivan" ],
      "venue" : "The Journal of Physiology,",
      "citeRegEx" : "Georgeson and Sullivan.,? \\Q1975\\E",
      "shortCiteRegEx" : "Georgeson and Sullivan.",
      "year" : 1975
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "A power law for perceived contrast in human vision",
      "author" : [ "Jon Gottesman", "Gary S Rubin", "Gordon E Legge" ],
      "venue" : "Vision research,",
      "citeRegEx" : "Gottesman et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Gottesman et al\\.",
      "year" : 1981
    }, {
      "title" : "Target selective tilt-after effect during texture learning",
      "author" : [ "Hila Harris", "Noga Pinchuk-Yacobi", "Dov Sagi" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "Harris et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Harris et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Reducing the dimensionality of data with neural networks. Science (New York, N.Y.)",
      "author" : [ "Hinton", "Salakhutdinov" ],
      "venue" : "jul",
      "citeRegEx" : "Hinton and Salakhutdinov.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton and Salakhutdinov.",
      "year" : 2006
    }, {
      "title" : "Receptive fields and functional architecture of monkey striate cortex",
      "author" : [ "D.H. Hubel", "T.N. Wiesel" ],
      "venue" : "The Journal of Physiology,",
      "citeRegEx" : "Hubel and Wiesel.,? \\Q1968\\E",
      "shortCiteRegEx" : "Hubel and Wiesel.",
      "year" : 1968
    }, {
      "title" : "What makes ImageNet good for transfer learning? aug 2016",
      "author" : [ "Minyoung Huh", "Pulkit Agrawal", "Alexei A. Efros" ],
      "venue" : null,
      "citeRegEx" : "Huh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huh et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast readout of object identity from macaque inferior temporal cortex",
      "author" : [ "Chou P Hung", "Gabriel Kreiman", "Tomaso Poggio", "James J DiCarlo" ],
      "venue" : "Science, 310(5749):863–866,",
      "citeRegEx" : "Hung et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hung et al\\.",
      "year" : 2005
    }, {
      "title" : "Unsupervised texture segmentation using Gabor filters",
      "author" : [ "Anil K Jain", "Farshid Farrokhnia" ],
      "venue" : "Pattern recognition,",
      "citeRegEx" : "Jain and Farrokhnia.,? \\Q1991\\E",
      "shortCiteRegEx" : "Jain and Farrokhnia.",
      "year" : 1991
    }, {
      "title" : "Perceptual losses for real-time style transfer and super-resolution",
      "author" : [ "Justin Johnson", "Alexandre Alahi", "Li Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1603.08155,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Where practice makes perfect in texture discrimination: evidence for primary visual cortex plasticity",
      "author" : [ "A. Karni", "D. Sagi" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Karni and Sagi.,? \\Q1991\\E",
      "shortCiteRegEx" : "Karni and Sagi.",
      "year" : 1991
    }, {
      "title" : "Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation",
      "author" : [ "Seyed-Mahdi Khaligh-Razavi", "Nikolaus Kriegeskorte" ],
      "venue" : "PLoS Computational Biology,",
      "citeRegEx" : "Khaligh.Razavi and Kriegeskorte.,? \\Q2014\\E",
      "shortCiteRegEx" : "Khaligh.Razavi and Kriegeskorte.",
      "year" : 2014
    }, {
      "title" : "Deep neural networks: A new framework for modeling biological vision and brain information processing",
      "author" : [ "Nikolaus Kriegeskorte" ],
      "venue" : "Annual Review of Vision Science,",
      "citeRegEx" : "Kriegeskorte.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kriegeskorte.",
      "year" : 2015
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep Neural Networks as a Computational Model for Human Shape Sensitivity",
      "author" : [ "Jonas Kubilius", "Stefania Bracci", "Hans P Op de Beeck" ],
      "venue" : "PLoS Comput Biol,",
      "citeRegEx" : "Kubilius et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kubilius et al\\.",
      "year" : 2016
    }, {
      "title" : "The distinct modes of vision offered by feedforward and recurrent processing",
      "author" : [ "Victor A.F. Lamme", "Pieter R. Roelfsema" ],
      "venue" : "Trends in Neurosciences,",
      "citeRegEx" : "Lamme and Roelfsema.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lamme and Roelfsema.",
      "year" : 2000
    }, {
      "title" : "Most apparent distortion: full-reference image quality assessment and the role of strategy",
      "author" : [ "Eric C Larson", "Damon M Chandler" ],
      "venue" : "Journal of Electronic Imaging,",
      "citeRegEx" : "Larson and Chandler.,? \\Q2010\\E",
      "shortCiteRegEx" : "Larson and Chandler.",
      "year" : 2010
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Lecun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lecun et al\\.",
      "year" : 1998
    }, {
      "title" : "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. sep 2016",
      "author" : [ "Christian Ledig", "Lucas Theis", "Ferenc Huszar", "Jose Caballero", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi" ],
      "venue" : null,
      "citeRegEx" : "Ledig et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ledig et al\\.",
      "year" : 2016
    }, {
      "title" : "Sparse deep belief net model for visual area V2",
      "author" : [ "Honglak Lee", "Chaitanya Ekanadham", "Andrew Y. Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lee et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2008
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning - ICML",
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Convergent Learning: Do different neural networks learn the same representations",
      "author" : [ "Yixuan Li", "Jason Yosinski", "Jeff Clune", "Hod Lipson", "John Hopcroft" ],
      "venue" : "arXiv preprint arXiv:1511.07543,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Near-threshold perceptual distortion prediction based on optimal structure classification",
      "author" : [ "Yucheng Liu", "Jan P. Allebach" ],
      "venue" : "IEEE International Conference on Image Processing (ICIP),",
      "citeRegEx" : "Liu and Allebach.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu and Allebach.",
      "year" : 2016
    }, {
      "title" : "Configuration influence on crowding",
      "author" : [ "Tomer Livne", "Dov Sagi" ],
      "venue" : "Journal of Vision,",
      "citeRegEx" : "Livne and Sagi.,? \\Q2007\\E",
      "shortCiteRegEx" : "Livne and Sagi.",
      "year" : 2007
    }, {
      "title" : "Inceptionism: Going deeper into neural networks",
      "author" : [ "Alexander Mordvintsev", "Christopher Olah", "Mike Tyka" ],
      "venue" : "Google Research Blog. Retrieved June,",
      "citeRegEx" : "Mordvintsev et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mordvintsev et al\\.",
      "year" : 2015
    }, {
      "title" : "Probing the human stereoscopic system with reverse correlation",
      "author" : [ "Peter Neri", "Andrew J Parker", "Colin Blakemore" ],
      "venue" : "Nature, 401(6754):695–698,",
      "citeRegEx" : "Neri et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Neri et al\\.",
      "year" : 1999
    }, {
      "title" : "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "author" : [ "Bruno A Olshausen" ],
      "venue" : "Nature, 381(6583):607–609,",
      "citeRegEx" : "Olshausen.,? \\Q1996\\E",
      "shortCiteRegEx" : "Olshausen.",
      "year" : 1996
    }, {
      "title" : "Sparse coding with an overcomplete basis set: A strategy employed by V1",
      "author" : [ "Bruno A. Olshausen", "David J. Field" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "Olshausen and Field.,? \\Q1997\\E",
      "shortCiteRegEx" : "Olshausen and Field.",
      "year" : 1997
    }, {
      "title" : "Crowding is unlike ordinary masking: Distinguishing feature integration from detection",
      "author" : [ "Denis G Pelli", "Melanie Palomares", "Najib J Majaj" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "Pelli et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Pelli et al\\.",
      "year" : 2004
    }, {
      "title" : "Expectation and the tilt aftereffect",
      "author" : [ "Noga Pinchuk-Yacobi", "Ron Dekel", "Dov Sagi" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "Pinchuk.Yacobi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pinchuk.Yacobi et al\\.",
      "year" : 2015
    }, {
      "title" : "Target-selective tilt aftereffect during texture learning",
      "author" : [ "Noga Pinchuk-Yacobi", "Hila Harris", "Dov Sagi" ],
      "venue" : "Vision research,",
      "citeRegEx" : "Pinchuk.Yacobi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pinchuk.Yacobi et al\\.",
      "year" : 2016
    }, {
      "title" : "Lateral interactions between spatial channels: suppression and facilitation revealed by lateral masking experiments",
      "author" : [ "U Polat", "D Sagi" ],
      "venue" : "Vision research,",
      "citeRegEx" : "Polat and Sagi.,? \\Q1993\\E",
      "shortCiteRegEx" : "Polat and Sagi.",
      "year" : 1993
    }, {
      "title" : "Do computational models differ systematically from human object perception",
      "author" : [ "R T Pramod", "S P Arun" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Pramod and Arun.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pramod and Arun.",
      "year" : 2016
    }, {
      "title" : "Hierarchical models of object recognition in cortex",
      "author" : [ "Maximilian Riesenhuber", "Tomaso Poggio" ],
      "venue" : "Nature neuroscience,",
      "citeRegEx" : "Riesenhuber and Poggio.,? \\Q1999\\E",
      "shortCiteRegEx" : "Riesenhuber and Poggio.",
      "year" : 1999
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2014
    }, {
      "title" : "Perceptual learning in vision research",
      "author" : [ "Dov Sagi" ],
      "venue" : "Vision research,",
      "citeRegEx" : "Sagi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sagi.",
      "year" : 2011
    }, {
      "title" : "Feature detection and orientation tuning in the Drosophila central complex",
      "author" : [ "Johannes D Seelig", "Vivek Jayaraman" ],
      "venue" : "Nature, 503(7475):262–266,",
      "citeRegEx" : "Seelig and Jayaraman.,? \\Q2013\\E",
      "shortCiteRegEx" : "Seelig and Jayaraman.",
      "year" : 2013
    }, {
      "title" : "Hierarchical Models of the Visual System",
      "author" : [ "Thomas Serre" ],
      "venue" : "In Encyclopedia of Computational Neuroscience,",
      "citeRegEx" : "Serre.,? \\Q2014\\E",
      "shortCiteRegEx" : "Serre.",
      "year" : 2014
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : null,
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "Going Deeper with Convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : null,
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Matconvnet: Convolutional neural networks for matlab",
      "author" : [ "Andrea Vedaldi", "Karel Lenc" ],
      "venue" : "In Proceedings of the 23rd ACM international conference on Multimedia,",
      "citeRegEx" : "Vedaldi and Lenc.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vedaldi and Lenc.",
      "year" : 2015
    }, {
      "title" : "Predicting visual acuity from wavefront aberrations",
      "author" : [ "Andrew B Watson", "Albert J Ahumada" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "Watson and Ahumada.,? \\Q2008\\E",
      "shortCiteRegEx" : "Watson and Ahumada.",
      "year" : 2008
    }, {
      "title" : "Adaptation and visual coding",
      "author" : [ "Michael A Webster" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "Webster.,? \\Q2011\\E",
      "shortCiteRegEx" : "Webster.",
      "year" : 2011
    }, {
      "title" : "Visual Detection of Line Segments: An Object-Superiority Effect",
      "author" : [ "N. Weisstein", "C.S. Harris" ],
      "venue" : "Science, 186(4165):752–755,",
      "citeRegEx" : "Weisstein and Harris.,? \\Q1974\\E",
      "shortCiteRegEx" : "Weisstein and Harris.",
      "year" : 1974
    }, {
      "title" : "Using goal-driven deep learning models to understand sensory cortex",
      "author" : [ "Daniel L K Yamins", "James J DiCarlo" ],
      "venue" : "Nature neuroscience,",
      "citeRegEx" : "Yamins and DiCarlo.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yamins and DiCarlo.",
      "year" : 2016
    }, {
      "title" : "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
      "author" : [ "Daniel L K Yamins", "Ha Hong", "Charles F Cadieu", "Ethan A Solomon", "Darren Seibert", "James J DiCarlo" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Yamins et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yamins et al\\.",
      "year" : 2014
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2014
    }, {
      "title" : "Visualizing and Understanding Convolutional Networks. nov",
      "author" : [ "Matthew D Zeiler", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "Zeiler and Fergus.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zeiler and Fergus.",
      "year" : 2013
    }, {
      "title" : "Object Detectors Emerge in Deep Scene CNNs",
      "author" : [ "Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2014
    }, {
      "title" : "The noiseless images",
      "author" : [ "Alam" ],
      "venue" : null,
      "citeRegEx" : "Alam,? \\Q2014\\E",
      "shortCiteRegEx" : "Alam",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "By training with millions of examples, such models achieve unparalleled degrees of task-trained accuracy (Krizhevsky et al., 2012).",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "This is not unprecedented on its own - steady progress has been made in computer vision for decades, and to some degree current designs are just scaled versions of long-known principles (Lecun et al., 1998).",
      "startOffset" : 186,
      "endOffset" : 206
    }, {
      "referenceID" : 50,
      "context" : "Interestingly, for current DNNs trained to solve a large-scale image recognition problem (Russakovsky et al., 2014), the learned computation is useful as a building block for drastically different and untrained visual problems (Huh et al.",
      "startOffset" : 89,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : ", 2014), the learned computation is useful as a building block for drastically different and untrained visual problems (Huh et al., 2016; Yosinski et al., 2014).",
      "startOffset" : 119,
      "endOffset" : 160
    }, {
      "referenceID" : 63,
      "context" : ", 2014), the learned computation is useful as a building block for drastically different and untrained visual problems (Huh et al., 2016; Yosinski et al., 2014).",
      "startOffset" : 119,
      "endOffset" : 160
    }, {
      "referenceID" : 29,
      "context" : "Such features are routinely discovered by DNNs (Krizhevsky et al., 2012; Zeiler & Fergus, 2013), by other learning algorithms (Hinton & Salakhutdinov, 2006; ∗https://sites.",
      "startOffset" : 47,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, a similar computation is believed to underlie the spatial response properties of visual neurons of diverse animal phyla (Carandini et al., 2005; DeAngelis et al., 1995; Hubel & Wiesel, 1968; Seelig & Jayaraman, 2013), and is evident in human visual perception (Campbell & Robson, 1968; Fogel & Sagi, 1989; Neri et al.",
      "startOffset" : 133,
      "endOffset" : 229
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, a similar computation is believed to underlie the spatial response properties of visual neurons of diverse animal phyla (Carandini et al., 2005; DeAngelis et al., 1995; Hubel & Wiesel, 1968; Seelig & Jayaraman, 2013), and is evident in human visual perception (Campbell & Robson, 1968; Fogel & Sagi, 1989; Neri et al.",
      "startOffset" : 133,
      "endOffset" : 229
    }, {
      "referenceID" : 41,
      "context" : ", 1995; Hubel & Wiesel, 1968; Seelig & Jayaraman, 2013), and is evident in human visual perception (Campbell & Robson, 1968; Fogel & Sagi, 1989; Neri et al., 1999).",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 42,
      "context" : "This diversity culminates in satisfying theoretical arguments as to why Gabor-like features are so useful in general-purpose vision (Olshausen, 1996; Olshausen & Field, 1997).",
      "startOffset" : 132,
      "endOffset" : 174
    }, {
      "referenceID" : 28,
      "context" : "Following this line of reasoning, DNN models with good general-purpose computations may be computationally similar to biological visual systems, even more so than less accurate and less general biologically plausible simulations (Kriegeskorte, 2015; Yamins & DiCarlo, 2016).",
      "startOffset" : 229,
      "endOffset" : 273
    }, {
      "referenceID" : 37,
      "context" : "First, different DNN training regimes seem to converge to a similar learned computation (Li et al., 2015; Zhou et al., 2014).",
      "startOffset" : 88,
      "endOffset" : 124
    }, {
      "referenceID" : 65,
      "context" : "First, different DNN training regimes seem to converge to a similar learned computation (Li et al., 2015; Zhou et al., 2014).",
      "startOffset" : 88,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "That is, when the same images are processed by DNN and by humans or monkeys, the final DNN computation stages are strong predictors of human fMRI and monkey electrophysiology data collected from visual areas V4 and IT (Cadieu et al., 2014; Khaligh-Razavi & Kriegeskorte, 2014; Yamins et al., 2014).",
      "startOffset" : 218,
      "endOffset" : 297
    }, {
      "referenceID" : 62,
      "context" : "That is, when the same images are processed by DNN and by humans or monkeys, the final DNN computation stages are strong predictors of human fMRI and monkey electrophysiology data collected from visual areas V4 and IT (Cadieu et al., 2014; Khaligh-Razavi & Kriegeskorte, 2014; Yamins et al., 2014).",
      "startOffset" : 218,
      "endOffset" : 297
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, more accurate DNN models exhibit stronger predictive power (Cadieu et al., 2014; Dubey & Agarwal, 2016; Yamins et al., 2014), and the final DNN computation stage is even a strong predictor of human-perceived shape discrimination (Kubilius et al.",
      "startOffset" : 72,
      "endOffset" : 137
    }, {
      "referenceID" : 62,
      "context" : "Furthermore, more accurate DNN models exhibit stronger predictive power (Cadieu et al., 2014; Dubey & Agarwal, 2016; Yamins et al., 2014), and the final DNN computation stage is even a strong predictor of human-perceived shape discrimination (Kubilius et al.",
      "startOffset" : 72,
      "endOffset" : 137
    }, {
      "referenceID" : 30,
      "context" : ", 2014), and the final DNN computation stage is even a strong predictor of human-perceived shape discrimination (Kubilius et al., 2016).",
      "startOffset" : 112,
      "endOffset" : 135
    }, {
      "referenceID" : 25,
      "context" : "A complementary approach is to consider images that were produced by optimizing trained DNN-based perceptual metrics (Gatys et al., 2015a;b; Johnson et al., 2016; Ledig et al., 2016), which perhaps yields undeniable evidence of non-trivial computational similarity, although a more objective approach may be warranted.",
      "startOffset" : 117,
      "endOffset" : 182
    }, {
      "referenceID" : 34,
      "context" : "A complementary approach is to consider images that were produced by optimizing trained DNN-based perceptual metrics (Gatys et al., 2015a;b; Johnson et al., 2016; Ledig et al., 2016), which perhaps yields undeniable evidence of non-trivial computational similarity, although a more objective approach may be warranted.",
      "startOffset" : 117,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "To investigate whether the computation in trained DNNs exhibits similar contextual modulation, we used the Local Image Masking Database (Alam et al., 2014), in which 1080 partially-overlapping images were subjected to different levels of the same random additive noise perturbation, and for each image, a psychophysical experiment determined the threshold noise level at which the added-noise image is discriminated from two noiseless copies at 75% (Figure 1a).",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "42 RMS contrast (Alam et al., 2014) .",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "40 Perceptual model** (Alam et al., 2014) .",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "73 Inter-person (Alam et al., 2014) .",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "For all tested DNN architectures, the prediction explains about 60% of the perceptual variability (Tables 1, 2; baselines at Tables 3-5), where inter-person similarity representing theoretical maximum is 84% (Alam et al., 2014).",
      "startOffset" : 208,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "RMS contrast), and is on par with a detailed perceptual model that relies on dozens of psychophysically collected parameters (Alam et al., 2014).",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 34,
      "context" : "This peak is consistent with use of middle DNN layers to optimize perceptual metrics (Gatys et al., 2015a;b; Ledig et al., 2016), and is reminiscent of cases in which low- to mid-level vision is the performance limiting computation in the detection of at-threshold stimuli (Campbell & Robson, 1968; Del Cul et al.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 46,
      "context" : "We first consider segmentation, in which arrangement is better discriminated for arrays of consistently oriented lines compared with inconsistently oriented lines (Figure 2a) (Pinchuk-Yacobi et al., 2016).",
      "startOffset" : 175,
      "endOffset" : 204
    }, {
      "referenceID" : 17,
      "context" : "However, this property is only reminiscent and not immediately consistent with the perceptual power-law scaling (Gottesman et al., 1981).",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "This was shown for contextual interactions reflecting grouping/segmentation (Harris et al., 2015), crowding/clutter (Livne & Sagi, 2007; Pelli et al.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 44,
      "context" : ", 2015), crowding/clutter (Livne & Sagi, 2007; Pelli et al., 2004), and shape superiority (Weisstein & Harris, 1974).",
      "startOffset" : 26,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "Interestingly, debate is ongoing about the degree to which visual perception is dependent on recurrent connectivity (Fabre-Thorpe et al., 1998; Hung et al., 2005): recurrent representations are obviously richer, but feedforward computations converge much faster.",
      "startOffset" : 116,
      "endOffset" : 162
    }, {
      "referenceID" : 23,
      "context" : "Interestingly, debate is ongoing about the degree to which visual perception is dependent on recurrent connectivity (Fabre-Thorpe et al., 1998; Hung et al., 2005): recurrent representations are obviously richer, but feedforward computations converge much faster.",
      "startOffset" : 116,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "In this study we used small images with a small number of lines, while effects such as contour integration seem to take place even in very large configurations (Field et al., 1993).",
      "startOffset" : 160,
      "endOffset" : 180
    }, {
      "referenceID" : 22,
      "context" : "However, the computation in trained DNN models is quite generalpurpose (Huh et al., 2016; Yosinski et al., 2014) and offers unparalleled accuracy in recognition tasks (LeCun et al.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 63,
      "context" : "However, the computation in trained DNN models is quite generalpurpose (Huh et al., 2016; Yosinski et al., 2014) and offers unparalleled accuracy in recognition tasks (LeCun et al.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 28,
      "context" : "Since visual computations are, to some degree, task- rather than architecturedependent, an accurate and general-purpose DNN model may better resemble biological processing than less accurate biologically plausible ones (Kriegeskorte, 2015; Yamins & DiCarlo, 2016).",
      "startOffset" : 219,
      "endOffset" : 263
    }, {
      "referenceID" : 51,
      "context" : "Note, however, that the scope of such simulations may be limited, since learning (Sagi, 2011) and adaptation (Webster, 2011) were not considered here.",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 59,
      "context" : "Note, however, that the scope of such simulations may be limited, since learning (Sagi, 2011) and adaptation (Webster, 2011) were not considered here.",
      "startOffset" : 109,
      "endOffset" : 124
    }, {
      "referenceID" : 55,
      "context" : "Another fascinating option is the formation of hypotheses in terms of mathematically differentiable trained-DNN constraints, whereby it is possible to efficiently solve for the visual stimuli that optimally dissociate the hypotheses (see Gatys et al. 2015a;b; Mordvintsev et al. 2015 and note Goodfellow et al. 2014; Szegedy et al. 2013).",
      "startOffset" : 233,
      "endOffset" : 337
    }, {
      "referenceID" : 25,
      "context" : "3 USE IN ENGINEERING (A PERCEPTUAL LOSS METRIC) As proposed previously (Dosovitskiy & Brox, 2016; Johnson et al., 2016; Ledig et al., 2016), the saliency of small image changes can be estimated as the representational distance in trained DNNs.",
      "startOffset" : 71,
      "endOffset" : 139
    }, {
      "referenceID" : 34,
      "context" : "3 USE IN ENGINEERING (A PERCEPTUAL LOSS METRIC) As proposed previously (Dosovitskiy & Brox, 2016; Johnson et al., 2016; Ledig et al., 2016), the saliency of small image changes can be estimated as the representational distance in trained DNNs.",
      "startOffset" : 71,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "Here, we quantified this approach by relying on data from a controlled psychophysical experiment (Alam et al., 2014).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "We found the metric to be far superior to simple image statistical properties, and on par with a detailed perceptual model (Alam et al., 2014).",
      "startOffset" : 123,
      "endOffset" : 142
    } ],
    "year" : 2017,
    "abstractText" : "Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and generalpurpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases. We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding and shape has DNN end-computation correlates. Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning. 1 QUICK EXPERT SUMMARY Considering the learned computation of ImageNet-trained DNNs, we find: • Large computation changes for perceptually salient image changes (Figure 1). • Gestalt: segmentation, crowding, and shape interactions in computation (Figure 2). • Contrast constancy: bandpass transduction in first layers is later corrected (Figure 3). These properties are reminiscent of human perception, perhaps because learned general-purpose classifiers (human and DNN) tend to converge.",
    "creator" : "LaTeX with hyperref package"
  }
}