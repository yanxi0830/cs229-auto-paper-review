{
  "name" : "309.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "AUXILIARY TASKS", "Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu" ],
    "emails" : [ "jaderberg@google.com", "vmnih@google.com", "lejlot@google.com", "schaul@google.com", "jzl@google.com", "davidsilver@google.com", "korayk@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations ot and executes actions at. These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.\nThe classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent’s long-term goals, potentially leading to more useful representations.\nConsider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase “redness” by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.\nOur architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates\n∗Joint first authors. Ordered alphabetically by first name.\n⇡V ⇡V⇡V⇡V\n0 0 0 +1\n(c) Reward Prediction\nrt } Replay Buffer\nVVV\n+RRR\n(d) Value Function Replayt⌧ t⌧+1 t⌧+2 t⌧+3\nt⌧ 3 t⌧ 2 t⌧ 1\nr⌧ Skewed sampling\n(b) Pixel Control\nQaux\nV\n…\nEnvironment\n… …\not\nAgent LSTM\nAgent ConvNet\nAux DeConvNet\nAux FC net\n(a) Base A3C Agent\nFigure 1: Overview of the UNREAL agent. (a) The base agent is a CNN-LSTM agent trained on-policy with the A3C loss (Mnih et al., 2016). Observations, rewards, and actions are stored in a small replay buffer which encapsulates a short history of agent experience. This experience is used by auxiliary learning tasks. (b) Pixel Control – auxiliary policies Qaux are trained to maximise change in pixel intensity of different regions of the input. The agent CNN and LSTM are used for this task along with an auxiliary deconvolution network. This auxiliary control task requires the agent to learn how to control the environment. (c) Reward Prediction – given three recent frames, the network must predict the reward that will be obtained in the next unobserved timestep. This task network uses instances of the agent CNN, and is trained on reward biased sequences to remove the perceptual sparsity of rewards. (d) Value Function Replay – further training of the value function using the agent network is performed to promote faster value iteration. Further visualisation of the agent can be found in https://youtu.be/Uz-zGYrYEjA\nto the critics. Just as animals dream about positively or negatively rewarding events more frequently (Olafsdottir et al., 2015; Schacter et al., 2012), our agents preferentially replay sequences containing rewarding events.\nImportantly, both the auxiliary control and auxiliary prediction tasks share the convolutional neural network and LSTM that the base agent uses to act. By using this jointly learned representation, the base agent learns to optimise extrinsic reward much faster and, in many cases, achieves better policies at the end of training.\nThis paper brings together the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) framework (Mnih et al., 2016), outlined in Section 2, with auxiliary control tasks and auxiliary reward tasks, defined in sections Section 3.1 and Section 3.2 respectively. These auxiliary tasks do not require any extra supervision or signals from the environment than the vanilla A3C agent. The result is our UNsupervised REinforcement and Auxiliary Learning (UNREAL) agent (Section 3.4)\nIn Section 4 we apply our UNREAL agent to a challenging set of 3D-vision based domains known as the Labyrinth (Mnih et al., 2016), learning solely from the raw RGB pixels of a first-person view. Our agent significantly outperforms the baseline agent using vanilla A3C, even when the baseline was augmented with an unsupervised reconstruction loss, in terms of speed of learning, robustness to hyperparameters, and final performance. The result is an agent which on average achieves 87% of expert human-normalised score, compared to 54% with A3C, and on average 10× faster than A3C. Our UNREAL agent also significantly outperforms the previous state-of-the-art in the Atari domain."
    }, {
      "heading" : "1 RELATED WORK",
      "text" : "A variety of reinforcement learning architectures have focused on learning temporal abstractions, such as options (Sutton et al., 1999b), with policies that may maximise pseudo-rewards (Konidaris & Barreto, 2009; Silver & Ciosek, 2012). The emphasis here has typically been on the development of temporal abstractions that facilitate high-level learning and planning. In contrast, our agents do\nnot make any direct use of the pseudo-reward maximising policies that they learn (although this is an interesting direction for future research). Instead, they are used solely as auxiliary objectives for developing a more effective representation.\nThe Horde architecture (Sutton et al., 2011) also applied reinforcement learning to identify value functions for a multitude of distinct pseudo-rewards. However, this architecture was not used for representation learning; instead each value function was trained separately using distinct weights.\nThe UVFA architecture (Schaul et al., 2015a) is a factored representation of a continuous set of optimal value functions, combining features of the state with an embedding of the pseudo-reward function. Initial work on UVFAs focused primarily on architectural choices and learning rules for these continuous embeddings. A pre-trained UVFA representation was successfully transferred to novel pseudo-rewards in a simple task.\nSimilarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016).\nAnother, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments.\nMore recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments. Lample & Chaplot (2016) showed that predicting internal features of the emulator, such as the presence of an enemy on the screen, is beneficial. Mirowski et al. (2016) study auxiliary prediction of depth in the context of navigation."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "We assume the standard reinforcement learning setting where an agent interacts with an environment over a number of discrete time steps. At time t the agent receives an observation ot along with a reward rt and produces an action at. The agent’s state st is a function of its experience up until time t, st = f(o1, r1, a1, ..., ot, rt). The n-step return Rt:t+n at time t is defined as the discounted sum of rewards, Rt:t+n = ∑n i=1 γ\ni−1rt+i. The value function is the expected return from state s, V π(s) = E [Rt:∞|st = s, π], when actions are selected accorded to a policy π(a|s). The actionvalue functionQπ(s, a) = E [Rt:∞|st = s, at = a, π] is the expected return following action a from state s.\nValue-based reinforcement learning algorithms, such as Q-learning (Watkins, 1989), or its deep learning instantiations DQN (Mnih et al., 2015) and asynchronous Q-learning (Mnih et al., 2016), approximate the action-value function Q(s, a; θ) using parameters θ, and then update parameters to minimise the mean-squared error, for example by optimising an n-step lookahead loss (Peng & Williams, 1996), LQ = E [ (Rt:t+n + γ n maxa′ Q(s ′, a′; θ−)−Q(s, a; θ))2 ] ; where θ− are\nprevious parameters and the optimisation is with respect to θ.\nPolicy gradient algorithms adjust the policy to maximise the expected reward, Es∼π [R1:∞], using the gradient ∂Es∼π [R1:∞]∂θ = E [ ∂ ∂θ log π(a|s)(Qπ(s, a)− V π(s)) ] (Watkins, 1989; Sutton et al., 1999a); in practice the true value functions Qπ and V π are substituted with approximations. The Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) constructs an approximation to both the policy π(a|s, θ) and the value function V (s, θ) using parameters θ. Both policy and value are adjusted towards an n-step lookahead value, Rt:t+n + γnV (st+n+1, θ), using an entropy regularisation penalty, LA3C ≈ LVR + Lπ − Es∼π [αH(π(s, ·, θ)], where LVR = Es∼π [ (Rt:t+n + γ nV (st+n+1, θ −)− V (st, θ))2 ] .\nIn A3C many instances of the agent interact in parallel with many instances of the environment, which both accelerates and stabilises learning. The A3C agent architecture we build on uses an LSTM to jointly approximate both policy π and value function V , given the entire history of experience as inputs (see Figure 1 (a))."
    }, {
      "heading" : "3 AUXILIARY TASKS FOR REINFORCEMENT LEARNING",
      "text" : "In this section we incorporate auxiliary tasks into the reinforcement learning framework in order to promote faster training, more robust learning, and ultimately higher performance for our agents. Section 3.1 introduces the use of auxiliary control tasks, Section 3.2 describes the addition of reward focussed auxiliary tasks, and Section 3.4 describes the complete UNREAL agent combining these auxiliary tasks."
    }, {
      "heading" : "3.1 AUXILIARY CONTROL TASKS",
      "text" : "The auxiliary control tasks we consider are defined as additional pseudo-reward functions in the environment the agent is interacting with. We formally define an auxiliary control task c by a reward function r(c) : S × A → R, where S is the space of possible states and A is the space of available actions. The underlying state space S includes both the history of observations and rewards as well as the state of the agent itself, i.e. the activations of the hidden units of the network.\nGiven a set of auxiliary control tasks C, let π(c) be the agent’s policy for each auxiliary task c ∈ C and let π be the agent’s policy on the base task. The overall objective is to maximise total performance across all these auxiliary tasks,\nargmax θ Eπ[R1:∞] + λC ∑ c∈C Eπc [R (c) 1:∞], (1)\nwhere, R(c)t:t+n = ∑n k=1 γ k−1r(c)t+k is the discounted return for auxiliary reward r (c), and θ is the set of parameters of π and all π(c)’s. By sharing some of the parameters of π and all π(c) the agent must balance improving its performance with respect to the global reward rt with improving performance on the auxiliary tasks.\nIn principle, any reinforcement learning method could be applied to maximise these objectives. However, to efficiently learn to maximise many different pseudo-rewards simultaneously in parallel from a single stream of experience, it is necessary to use off-policy reinforcement learning. We focus on value-based RL methods that approximate the optimal action-values by Qlearning. Specifically, for each control task c we optimise an n-step Q-learning loss L(c)Q = E [( Rt:t+n + γ n maxa′ Q (c)(s′, a′, θ−)−Q(c)(s, a, θ) )2] , as described in Mnih et al. (2016).\nWhile many types of auxiliary reward functions can be defined from these quantities we focus on two specific types:\n• Pixel changes - Changes in the perceptual stream often correspond to important events in an environment. We train agents that learn a separate policy for maximally changing the pixels in each cell of an n× n non-overlapping grid placed over the input image. We refer to these auxiliary tasks as pixel control. See Section 4 for a complete description. • Network features - Since the policy or value networks of an agent learn to extract task-\nrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control. Hence, the activation of any hidden unit of the agent’s neural network can itself be an auxiliary reward. We train agents that learn a separate policy for maximally activating each of the units in a specific hidden layer. We refer to these tasks as feature control.\nThe Figure 1 (b) shows an A3C agent architecture augmented with a set of auxiliary pixel control tasks. In this case, the base policy π shares both the convolutional visual stream and the LSTM with the auxiliary policies. The output of the auxiliary network head is an Nact × n × n tensor Qaux where Qaux(a, i, j) represents the network’s current estimate of the optimal discounted expected change in cell (i, j) of the input after taking action a. We exploit the spatial nature of the auxiliary tasks by using a deconvolutional neural network to produce the auxiliary values Qaux."
    }, {
      "heading" : "3.2 AUXILIARY REWARD TASKS",
      "text" : "In addition to learning generally about the dynamics of the environment, an agent must learn to maximise the global reward stream. To learn a policy to maximise rewards, an agent requires features\nthat recognise states that lead to high reward and value. An agent with a good representation of rewarding states, will allow the learning of good value functions, and in turn should allow the easy learning of a policy.\nHowever, in many interesting environments reward is encountered very sparsely, meaning that it can take a long time to train feature extractors adept at recognising states which signify the onset of reward. We want to remove the perceptual sparsity of rewards and rewarding states to aid the training of an agent, but to do so in a way which does not introduce bias to the agent’s policy.\nTo do this, we introduce the auxiliary task of reward prediction – that of predicting the onset of immediate reward given some historical context. This task consists of processing a sequence of consecutive observations, and requiring the agent to predict the reward picked up in the subsequent unseen frame. This is similar to value learning focused on immediate reward (γ = 0).\nUnlike learning a value function, which is used to estimate returns and as a baseline while learning a policy, the reward predictor is not used for anything other than shaping the features of the agent. This keeps us free to bias the data distribution, therefore biasing the reward predictor and feature shaping, without biasing the value function or policy.\nWe train the reward prediction task on sequences Sτ = (sτ−k, sτ−k+1, . . . , sτ−1) to predict the reward rτ , and sample Sτ from the experience of our policy π in a skewed manner so as to overrepresent rewarding events (presuming rewards are sparse within the environment). Specifically, we sample such that zero rewards and non-zero rewards are equally represented, i.e. the predicted probability of a non-zero reward is P (rτ 6= 0) = 0.5. The reward prediction is trained to minimise a loss LRP. In our experiments we use a multiclass cross-entropy classification loss across three classes (zero, positive, or negative reward), although a mean-squared error loss is also feasible.\nThe auxiliary reward predictions may use a different architecture to the agent’s main policy. Rather than simply “hanging” the auxiliary predictions off the LSTM, we use a simpler feedforward network that concatenates a stack of states Sτ after being encoded by the agent’s CNN, see Figure 1 (c). The idea is to simplify the temporal aspects of the prediction task in both the future direction (focusing only on immediate reward prediction rather than long-term returns) and past direction (focusing only on immediate predecessor states rather than the complete history); the features discovered in this manner are shared with the primary LSTM (via shared weights in the convolutional encoder) to enable the policy to be learned more efficiently."
    }, {
      "heading" : "3.3 EXPERIENCE REPLAY",
      "text" : "Experience replay has proven to be an effective mechanism for improving both the data efficiency and stability of deep reinforcement learning algorithms (Mnih et al., 2015). The main idea is to store transitions in a replay buffer, and then apply learning updates to sampled transitions from this buffer.\nExperience replay provides a natural mechanism for skewing the distribution of reward prediction samples towards rewarding events: we simply split the replay buffer into rewarding and nonrewarding subsets, and replay equally from both subsets. The skewed sampling of transitions from\na replay buffer means that rare rewarding states will be oversampled, and learnt from far more frequently than if we sampled sequences directly from the behaviour policy. This approach can be viewed as a simple form of prioritised replay (Schaul et al., 2015b).\nIn addition to reward prediction, we also use the replay buffer to perform value function replay (see Figure 1). This amounts to resampling recent historical sequences from the behaviour policy distribution and performing extra value function regression in addition to the on-policy value function regression in A3C. By resampling previous experience, and randomly varying the temporal position of the truncation window over which the n-step return is computed, value function replay performs value iteration and exploits newly discovered features shaped by reward prediction. We do not skew the distribution for this case.\nExperience replay is also used to increase the efficiency and stability of the auxiliary control tasks. Q-learning updates are applied to sampled experiences that are drawn from the replay buffer, allowing features to be developed extremely efficiently.\n3.4 UNREAL AGENT\nThe UNREAL algorithm combines the benefits of two separate, state-of-the-art approaches to deep reinforcement learning. The primary policy is trained with A3C (Mnih et al., 2016): it learns from parallel streams of experience to gain efficiency and stability; it is updated online using policy gradient methods; and it uses a recurrent neural network to encode the complete history of experience. This allows the agent to learn effectively in partially observed environments.\nThe auxiliary tasks are trained on very recent sequences of experience that are stored and randomly sampled; these sequences may be prioritised (in our case according to immediate rewards) (Schaul et al., 2015b); these targets are trained off-policy by Q-learning; and they may use simpler feedforward architectures. This allows the representation to be trained with maximum efficiency.\nThe UNREAL algorithm optimises a single combined loss function with respect to the joint parameters of the agent, θ, that combines the A3C loss LA3C together with an auxiliary control loss LPC = ∑ c L (c) Q , auxiliary reward prediction loss LRP and replayed value loss LVR,\nLUNREAL(θ) = LA3C + λVRLVR + λPC ∑ c L(c)Q + λRPLRP (2)\nwhere λVR, λPC, λRP are weighting terms on the individual loss components.\nIn practice, the loss is broken down into separate components that are computed either on-policy, directly from experience; or off-policy, on replayed transitions. Specifically, the A3C loss LA3C is minimised on-policy; while the value function loss LVR is optimised from replayed data, in addition to the A3C loss (of which it is one component, see Section 2). The auxiliary control loss LPC is optimised off-policy from replayed data, by n-step Q-learning. Finally, the reward loss LRP is optimised from rebalanced replay data."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section we give the results of experiments performed on the 3D environment Labyrinth in Section 4.1 and Atari in Section 4.2.\nIn all our experiments we used an A3C CNN-LSTM agent as our baseline and the UNREAL agent along with its ablated variants added auxiliary outputs and losses to this base agent. The agent is trained on-policy with 20-step returns and the auxiliary tasks are performed every 20 environment steps, corresponding to every update of the base A3C agent. The replay buffer stores the most recent 2k observations, actions, and rewards taken by the base agent. In Labyrinth we use the same set of 17 discrete actions for all games and on Atari the action set is game dependent (between 3 and 18 discrete actions). The full implementation details can be found in Section B."
    }, {
      "heading" : "4.1 LABYRINTH RESULTS",
      "text" : "Labyrinth (see Figure 2) is a first-person 3D game platform extended from OpenArena (contributors, 2005), which is itself based on Quake3 (id software, 1999). Labyrinth is comparable to other first-\nperson 3D game platforms for AI research like VizDoom (Kempka et al., 2016) or Minecraft (Tessler et al., 2016). However, in comparison, Labyrinth has considerably richer visuals and more realistic physics. Textures in Labyrinth are often dynamic (animated) so as to convey a game world where walls and floors shimmer and pulse, adding significant complexity to the perceptual task. The action space allows for fine-grained pointing in a fully 3D world. Labyrinth also supports continuous motion unlike the Minecraft platform of (Oh et al., 2016), which is a 3D grid world.\nWe evaluated agent performance on 13 Labyrinth levels that tested a range of different agent abilities. A top-down visualization showing the layout of each level can be found in Figure 9 of the Appendix. A gallery of example images from the first-person perspective of the agent are in Figure 10 of the Appendix. The levels can be divided into four categories:\n1. Simple fruit gathering levels with a static map (seekavoid arena 01 and stairway to melon 01). The goal of these levels is to collect apples (small positive reward) and melons (large positive reward) while avoiding lemons (small negative reward).\n2. Navigation levels with a static map layout (nav maze static 0{1, 2, 3} and nav maze random goal 0{1, 2, 3}). These levels test the agent’s ability to find their way to a goal in a fixed maze that remains the same across episodes. The starting location is random. In this case, agents could encode the structure of the maze in network weights. In the random goal variant, the location of the goal changes in every episode. The optimal policy is to find the goal’s location at the start of each episode and then use long-term knowledge of the maze layout to return to it as quickly as possible from any location. The static variant is simpler in that the goal location is always fixed for all episodes and only the agent’s starting location changes so the optimal policy does not require the first step of exploring to find the current goal location.\n3. Procedurally-generated navigation levels requiring effective exploration of a new maze generated on-the-fly at the start of each episode (nav maze all random 0{1, 2, 3}). These levels test the agent’s ability to effectively explore a totally new environment. The optimal\npolicy would begin by exploring the maze to rapidly learn its layout and then exploit that knowledge to repeatedly return to the goal as many times as possible before the end of the episode (between 60 and 300 seconds).\n4. Laser-tag levels requiring agents to wield laser-like science fiction gadgets to tag bots controlled by the game’s in-built AI (lt horse shoe color and lt hallway slope). A reward of 1 is delivered whenever the agent tags a bot by reducing its shield to 0. These levels approximate the default OpenArena/Quake3 gameplay mode. In lt hallway slope there is a sloped arena, requiring the agent to look up and down. In lt horse shoe color, the colors and textures of the bots are randomly generated at the start of each episode. This prevents agents from relying on color for bot detection. These levels test aspects of fine-control (for aiming), planning (to anticipate where bots are likely to move), strategy (to control key areas of the map such as gadget spawn points), and robustness to the substantial visual complexity arising from the large numbers of independently moving objects (gadget projectiles and bots)."
    }, {
      "heading" : "4.1.1 RESULTS",
      "text" : "We compared the full UNREAL agent to a basic A3C LSTM agent along with several ablated versions of UNREAL with different components turned off. A video of the final agent performance, as well as visualisations of the activations and auxiliary task outputs can be viewed at https://youtu.be/Uz-zGYrYEjA.\nFigure 3 (top left) shows curves of mean human-normalised scores over the 13 Labyrinth levels. Adding each of our proposed auxiliary tasks to an A3C agent substantially improves the performance. Combining different auxiliary tasks leads to further improvements over the individual auxiliary tasks. The UNREAL agent, which combines all three auxiliary tasks, achieves more than twice the final human-normalised mean performance of A3C, increasing from 54% to 87% (45% to 92% for median performance). This includes a human-normalised score of 116% on lt hallway slope and 100% on nav maze random goal 02.\nPerhaps of equal importance, aside from final performance on the games, UNREAL is significantly faster at learning and therefore more data efficient, achieving a mean speedup of the number of steps to reach A3C best performance of 10× (median 11×) across all levels and up to 18× on nav maze random goal 02. This translates in a drastic improvement in the data efficiency of UNREAL over A3C, requiring less than 10% of the data to reach the final performance of A3C. We can also measure the robustness of our learning algorithms to hyperparameters by measuring the performance over all hyperparameters (namely learning rate and entropy cost). This is shown in Figure 3 Top Right: every auxiliary task in our agent improves robustness. A breakdown of the performance of A3C, UNREAL and UNREAL without pixel control on the individual Labyrinth levels is shown in Figure 4.\nUnsupervised Reinforcement Learning In order to better understand the benefits of auxiliary control tasks we compared it to two simple baselines on three Labyrinth levels. The first baseline was A3C augmented with a pixel reconstruction loss, which has been shown to improve performance on 3D environments (Kulkarni et al., 2016). The second baseline was A3C augmented with an input change prediction loss, which can be seen as simply predicting the immediate auxiliary reward instead of learning to control. Finally, we include preliminary results for A3C augmented with the feature control auxiliary task on one of the levels. We retuned the hyperparameters of all methods (including learning rate and the weight placed on the auxiliary loss) for each of the three Labyrinth levels. Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels. The results show that learning to control pixel changes is indeed better than simply predicting immediate pixel changes, which in turn is better than simply learning to reconstruct the input. In fact, learning to reconstruct only led to faster initial learning and actually made the final scores worse when compared to vanilla A3C. Our hypothesis is that input reconstruction hurts final performance because it puts too much focus on reconstructing irrelevant parts of the visual input instead of visual cues for rewards, which rewarding objects are rarely visible. We saw a substantial improvement from including the feature control auxiliary task, which was only slightly worse than for pixel control. Combining feature control with other auxiliary tasks is a promising future direction."
    }, {
      "heading" : "4.2 ATARI",
      "text" : "We applied the UNREAL agent as well as UNREAL without pixel control to 57 Atari games from the Arcade Learning Environment (Bellemare et al., 2012) domain. We use the same evaluation protocol as for our Labyrinth experiments where we evaluate 50 different random hyper parameter settings (learning rate and entropy cost) on each game. The results are shown in the bottom row of Figure 3. The left side shows the average performance curves of the top 3 agents for all three methods the right half shows sorted average human-normalised scores for each hyperparameter setting. More detailed learning curves for individual levels can be found in Figure 6. We see that UNREAL surpasses the current state-of-the-art agents, i.e. A3C and Prioritized Dueling DQN (Wang et al., 2016), across all levels attaining 880% mean and 250% median performance. Notably, UNREAL is also substantially more robust to hyper parameter settings than A3C."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We have shown how augmenting a deep reinforcement learning agent with auxiliary control and reward prediction tasks can drastically improve both data efficiency and robustness to hyperparameter settings. Most notably, our proposed UNREAL architecture more than doubled the previous stateof-the-art results on the challenging set of 3D Labyrinth levels, bringing the average scores to over 87% of human scores. The same UNREAL architecture also significantly improved both the learning speed and the robustness of A3C over 57 Atari games."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We thank Charles Beattie, Julian Schrittwieser, Marcus Wainwright, and Stig Petersen for environment design and development, and Amir Sadik and Sarah York for expert human game testing. We also thank Joseph Modayil, Andrea Banino, Hubert Soyer, Razvan Pascanu, and Raia Hadsell for many helpful discussions."
    }, {
      "heading" : "A ATARI GAMES",
      "text" : "B IMPLEMENTATION DETAILS\nThe input to the agent at each timestep was an 84× 84 RGB image. All agents processed the input with the convolutional neural network (CNN) originally used for Atari by Mnih et al. (2013). The network consists of two convolutional layers. The first one has 16 8× 8 filters applied with stride 4, while the second one has 32 4 × 4 filters with stride 2. This is followed by a fully connected layer with 256 units. All three layers are followed by a ReLU non-linearity. All agents used an LSTM with forget gates (Gers et al., 2000) with 256 cells which take in the CNN-encoded observation concatenated with the previous action taken and current reward. The policy and value function are linear projections of the LSTM output. The agent is trained with 20-step unrolls. The action space of the agent in the environment is game dependent for Atari (between 3 and 18 discrete actions), and 17 discrete actions for Labyrinth. Labyrinth runs at 60 frames-per-second. We use an action repeat of four, meaning that each action is repeated four times, with the agent receiving the final fourth frame as input to the next processing step.\nFor the pixel control auxiliary tasks we trained policies to control the central 80 × 80 crop of the inputs. The cropped region was subdivided into a 20× 20 grid of non-overlapping 4× 4 cells. The instantaneous reward in each cell was defined as the average absolute difference from the previous frame, where the average is taken over both pixels and channels in the cell. The output tensor of auxiliary values, Qaux, is produced from the LSTM outputs by a deconvolutional network. The LSTM outputs are first mapped to a 32 × 7 × 7 spatial feature map with a linear layer followed by a ReLU. This is followed by a doconvolutional layer of 32 3 × 3 filters and a ReLU, resulting in a 32 × 9 × 9 feature map. Deconvolution layers with 1 and Nact filters of size 4 × 4 and stride 2 map the 32 × 9 × 9 into a value tensor and an advantage tensor respectively. The spatial map is then decoded into Q-values using the dueling parametrization (Wang et al., 2016) producing the Nact × 20× 20 output Qaux. There is a final ReLU nonlinearity on the Qaux output. The architecture for feature control was similar. We learned to control the second hidden layer, which is a spatial feature map with size 32× 9× 9. Similarly to pixel control, we exploit the spatial structure in the data and used a deconvolutional network to produce Qaux from the LSTM outputs.\nThe reward prediction task is performed on a sequence of three observations, which are fed through three instances of the agent’s CNN. The three encoded CNN outputs are concatenated and fed through a fully connected layer of 128 units with ReLU activations, followed by a final linear threeclass classifier and softmax. The reward is predicted as one of three classes: positive, negative, or zero and trained with a task weight λRP = 1. The value function replay is performed on a sequence of length 20 with a task weight λVR = 1.\nThe auxiliary tasks are performed every 20 environment steps, corresponding to every update of the base A3C agent, once the replay buffer has filled with agent experience. The replay buffer stores the most recent 2k observations, actions, and rewards taken by the base agent.\nThe agents are optimised over 32 asynchronous threads with shared RMSprop (Mnih et al., 2016). The learning rates are sampled from a log-uniform distribution between 0.0001 and 0.005. The entropy costs are sampled from the log-uniform distribution between 0.0005 and 0.01. Task weight λPC is sampled from log-uniform distribution between 0.01 and 0.1 for Labyrinth and 0.0001 and 0.01 for Atari (since Atari games are not homogeneous in terms of pixel intensities changes, thus we need to fit this normalization factor)."
    }, {
      "heading" : "C RANDOMNESS ROBUSTNESS",
      "text" : "Each agent was trained with 45 randomly sampled values of hyperparameters. Each of them also starts with a different random seed (however, due to asynchronous nature of A3C this does not determinise the learning procedure).\nPrevious sections showed that the UNREAL agent is more robust to the choice of hyperparameters than A3C. To present an even clearer picture of this effect, we show learning curves averaged over all hyperparamters/seeds used in the experiments in Figure 7. It is worth noting, that standard error for such curves is not increased despite adding our auxiliary tasks.\nWe also include scatter plots of averaged final human normalised performance with respect to the two main hyperparameters (learning rate and entropy cost) in Figure 8. The final performance across all levels varies rather smoothly across similar hyperparameters, showing that learning is not significantly affected by random seeds. The only significant inconsistency, which can be spotted around (−3,−3) point in UNREAL plot is an effect of the third hyperparamer - λPC, which differs a lot between these runs."
    }, {
      "heading" : "D RAW ATARI SCORES",
      "text" : ""
    }, {
      "heading" : "E LABYRINTH LEVELS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Successor features for transfer in reinforcement learning",
      "author" : [ "André Barreto", "Rémi Munos", "Tom Schaul", "David Silver" ],
      "venue" : "arXiv preprint arXiv:1606.05312,",
      "citeRegEx" : "Barreto et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barreto et al\\.",
      "year" : 2016
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2012
    }, {
      "title" : "Improving generalization for temporal difference learning: The successor representation",
      "author" : [ "Peter Dayan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Dayan.,? \\Q1993\\E",
      "shortCiteRegEx" : "Dayan.",
      "year" : 1993
    }, {
      "title" : "Learning to forget: Continual prediction with lstm",
      "author" : [ "Felix A Gers", "Jürgen Schmidhuber", "Fred Cummins" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Gers et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Gers et al\\.",
      "year" : 2000
    }, {
      "title" : "Vizdoom: A doom-based ai research platform for visual reinforcement learning",
      "author" : [ "Michał Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Jaśkowski" ],
      "venue" : null,
      "citeRegEx" : "Kempka et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kempka et al\\.",
      "year" : 2016
    }, {
      "title" : "Skill discovery in continuous reinforcement learning domains using skill chaining",
      "author" : [ "George Konidaris", "Andre S Barreto" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Konidaris and Barreto.,? \\Q2009\\E",
      "shortCiteRegEx" : "Konidaris and Barreto.",
      "year" : 2009
    }, {
      "title" : "Deep successor reinforcement learning",
      "author" : [ "Tejas D Kulkarni", "Ardavan Saeedi", "Simanta Gautam", "Samuel J Gershman" ],
      "venue" : "arXiv preprint arXiv:1606.02396,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Playing FPS games with deep reinforcement learning",
      "author" : [ "Guillaume Lample", "Devendra Singh Chaplot" ],
      "venue" : null,
      "citeRegEx" : "Lample and Chaplot.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lample and Chaplot.",
      "year" : 2016
    }, {
      "title" : "Recurrent reinforcement learning: A hybrid approach",
      "author" : [ "Xiujun Li", "Lihong Li", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Li Deng", "Ji He" ],
      "venue" : "arXiv preprint arXiv:1509.03044,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Memory approaches to reinforcement learning in non-markovian domains",
      "author" : [ "Long-Ji Lin", "Tom M Mitchell" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Lin and Mitchell.,? \\Q1992\\E",
      "shortCiteRegEx" : "Lin and Mitchell.",
      "year" : 1992
    }, {
      "title" : "Learning to navigate in complex environments",
      "author" : [ "Piotr Mirowski", "Razvan Pascanu", "Fabio Viola", "Andrea Banino", "Hubert Soyer", "Andy Ballard", "Misha Denil", "Ross Goroshin", "Laurent Sifre", "Koray Kavukcuoglu", "Dharshan Kumaran", "Raia Hadsell" ],
      "venue" : null,
      "citeRegEx" : "Mirowski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mirowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "In NIPS Deep Learning Workshop",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adrià Puigdomènech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Action-conditional video prediction using deep networks in atari games",
      "author" : [ "Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L Lewis", "Satinder Singh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Control of memory, active perception, and action in minecraft",
      "author" : [ "Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee" ],
      "venue" : "arXiv preprint arXiv:1605.09128,",
      "citeRegEx" : "Oh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2016
    }, {
      "title" : "Hippocampal place cells construct reward related sequences through unexplored",
      "author" : [ "H Freyja Olafsdottir", "Caswell Barry", "Aman B Saleem", "Demis Hassabis", "Hugo J Spiers" ],
      "venue" : "space. Elife,",
      "citeRegEx" : "Olafsdottir et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Olafsdottir et al\\.",
      "year" : 2015
    }, {
      "title" : "Incremental multi-step q-learning",
      "author" : [ "Jing Peng", "Ronald J Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Peng and Williams.,? \\Q1996\\E",
      "shortCiteRegEx" : "Peng and Williams.",
      "year" : 1996
    }, {
      "title" : "The future of memory: remembering",
      "author" : [ "Daniel L Schacter", "Donna Rose Addis", "Demis Hassabis", "Victoria C Martin", "R Nathan Spreng", "Karl K Szpunar" ],
      "venue" : "imagining, and the brain. Neuron,",
      "citeRegEx" : "Schacter et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schacter et al\\.",
      "year" : 2012
    }, {
      "title" : "Universal value function approximators",
      "author" : [ "Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver" ],
      "venue" : "arXiv preprint arXiv:1511.05952,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "Formal theory of creativity, fun, and intrinsic motivation (1990–2010)",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "IEEE Transactions on Autonomous Mental Development,",
      "citeRegEx" : "Schmidhuber.,? \\Q2010\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2010
    }, {
      "title" : "Compositional planning using optimal option models",
      "author" : [ "David Silver", "Kamil Ciosek" ],
      "venue" : "arXiv preprint arXiv:1206.6473,",
      "citeRegEx" : "Silver and Ciosek.,? \\Q2012\\E",
      "shortCiteRegEx" : "Silver and Ciosek.",
      "year" : 2012
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Richard S Sutton", "Doina Precup", "Satinder Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "Richard S Sutton", "Joseph Modayil", "Michael Delp", "Thomas Degris", "Patrick M Pilarski", "Adam White", "Doina Precup" ],
      "venue" : "In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume",
      "citeRegEx" : "Sutton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "A deep hierarchical approach to lifelong learning in minecraft",
      "author" : [ "Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor" ],
      "venue" : "arXiv preprint arXiv:1604.07255,",
      "citeRegEx" : "Tessler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tessler et al\\.",
      "year" : 2016
    }, {
      "title" : "Dueling Network Architectures for Deep Reinforcement Learning",
      "author" : [ "Z. Wang", "N. de Freitas", "M. Lanctot" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Watkins.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins.",
      "year" : 1989
    }, {
      "title" : "Modelbased reinforcement learning with parametrized physical models and optimism-driven exploration",
      "author" : [ "Christopher Xie", "Sachin Patil", "Teodor Mihai Moldovan", "Sergey Levine", "Pieter Abbeel" ],
      "venue" : "CoRR, abs/1509.06824,",
      "citeRegEx" : "Xie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2015
    }, {
      "title" : "Graying the black box: Understanding dqns",
      "author" : [ "Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Zahavy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zahavy et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "(a) The base agent is a CNN-LSTM agent trained on-policy with the A3C loss (Mnih et al., 2016).",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "Just as animals dream about positively or negatively rewarding events more frequently (Olafsdottir et al., 2015; Schacter et al., 2012), our agents preferentially replay sequences containing rewarding events.",
      "startOffset" : 86,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "Just as animals dream about positively or negatively rewarding events more frequently (Olafsdottir et al., 2015; Schacter et al., 2012), our agents preferentially replay sequences containing rewarding events.",
      "startOffset" : 86,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "This paper brings together the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) framework (Mnih et al., 2016), outlined in Section 2, with auxiliary control tasks and auxiliary reward tasks, defined in sections Section 3.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "4) In Section 4 we apply our UNREAL agent to a challenging set of 3D-vision based domains known as the Labyrinth (Mnih et al., 2016), learning solely from the raw RGB pixels of a first-person view.",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "The Horde architecture (Sutton et al., 2011) also applied reinforcement learning to identify value functions for a multitude of distinct pseudo-rewards.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.",
      "startOffset" : 40,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.",
      "startOffset" : 40,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.",
      "startOffset" : 40,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al.",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : ", 2016) or to different scales of reward (Kulkarni et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 128
    }, {
      "referenceID" : 30,
      "context" : "Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al.",
      "startOffset" : 54,
      "endOffset" : 698
    }, {
      "referenceID" : 0,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments.",
      "startOffset" : 54,
      "endOffset" : 716
    }, {
      "referenceID" : 0,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments. More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments. Lample & Chaplot (2016) showed that predicting internal features of the emulator, such as the presence of an enemy on the screen, is beneficial.",
      "startOffset" : 54,
      "endOffset" : 911
    }, {
      "referenceID" : 0,
      "context" : "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments. More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments. Lample & Chaplot (2016) showed that predicting internal features of the emulator, such as the presence of an enemy on the screen, is beneficial. Mirowski et al. (2016) study auxiliary prediction of depth in the context of navigation.",
      "startOffset" : 54,
      "endOffset" : 1055
    }, {
      "referenceID" : 29,
      "context" : "Value-based reinforcement learning algorithms, such as Q-learning (Watkins, 1989), or its deep learning instantiations DQN (Mnih et al.",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Value-based reinforcement learning algorithms, such as Q-learning (Watkins, 1989), or its deep learning instantiations DQN (Mnih et al., 2015) and asynchronous Q-learning (Mnih et al.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : ", 2015) and asynchronous Q-learning (Mnih et al., 2016), approximate the action-value function Q(s, a; θ) using parameters θ, and then update parameters to minimise the mean-squared error, for example by optimising an n-step lookahead loss (Peng & Williams, 1996), LQ = E [ (Rt:t+n + γ n maxa′ Q(s ′, a′; θ−)−Q(s, a; θ)) ] ; where θ− are previous parameters and the optimisation is with respect to θ.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : "Policy gradient algorithms adjust the policy to maximise the expected reward, Es∼π [R1:∞], using the gradient ∂Es∼π [R1:∞] ∂θ = E [ ∂ ∂θ log π(a|s)(Qπ(s, a)− V (s)) ] (Watkins, 1989; Sutton et al., 1999a); in practice the true value functions Q and V π are substituted with approximations.",
      "startOffset" : 167,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "The Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) constructs an approximation to both the policy π(a|s, θ) and the value function V (s, θ) using parameters θ.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Specifically, for each control task c we optimise an n-step Q-learning loss L Q = E [( Rt:t+n + γ n maxa′ Q (c)(s′, a′, θ−)−Q(c)(s, a, θ) )2] , as described in Mnih et al. (2016).",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : "• Network features - Since the policy or value networks of an agent learn to extract taskrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control.",
      "startOffset" : 137,
      "endOffset" : 198
    }, {
      "referenceID" : 31,
      "context" : "• Network features - Since the policy or value networks of an agent learn to extract taskrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control.",
      "startOffset" : 137,
      "endOffset" : 198
    }, {
      "referenceID" : 23,
      "context" : "• Network features - Since the policy or value networks of an agent learn to extract taskrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control.",
      "startOffset" : 137,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "Experience replay has proven to be an effective mechanism for improving both the data efficiency and stability of deep reinforcement learning algorithms (Mnih et al., 2015).",
      "startOffset" : 153,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "The primary policy is trained with A3C (Mnih et al., 2016): it learns from parallel streams of experience to gain efficiency and stability; it is updated online using policy gradient methods; and it uses a recurrent neural network to encode the complete history of experience.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "person 3D game platforms for AI research like VizDoom (Kempka et al., 2016) or Minecraft (Tessler et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : ", 2016) or Minecraft (Tessler et al., 2016).",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "Labyrinth also supports continuous motion unlike the Minecraft platform of (Oh et al., 2016), which is a 3D grid world.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "Duel Clip and Duel Clip are Dueling Networks with gradient clipped to 10 as reported in Wang et al. (2016) Right: The final human-normalised score of every job in our hyperparameter sweep, sorted by score.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "The first baseline was A3C augmented with a pixel reconstruction loss, which has been shown to improve performance on 3D environments (Kulkarni et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "We applied the UNREAL agent as well as UNREAL without pixel control to 57 Atari games from the Arcade Learning Environment (Bellemare et al., 2012) domain.",
      "startOffset" : 123,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : "A3C and Prioritized Dueling DQN (Wang et al., 2016), across all levels attaining 880% mean and 250% median performance.",
      "startOffset" : 32,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also learns separate policies for maximising many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth. Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations ot and executes actions at. These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards. The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent’s long-term goals, potentially leading to more useful representations. Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase “redness” by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images. Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates ∗Joint first authors. Ordered alphabetically by first name.",
    "creator" : "LaTeX with hyperref package"
  }
}