{
  "name" : "435.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "WARM RESTARTS", "Ilya Loshchilov", "Frank Hutter" ],
    "emails" : [ "ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep neural networks (DNNs) are currently the best-performing method for many classification problems, such as object recognition from images (Krizhevsky et al., 2012a; Donahue et al., 2014) or speech recognition from audio data (Deng et al., 2013). Their training on large datasets (where DNNs perform particularly well) is the main computational bottleneck: it often requires several days, even on high-performance GPUs, and any speedups would be of substantial value.\nThe training of a DNN with n free parameters can be formulated as the problem of minimizing a function f : IRn → IR. The commonly used procedure to optimize f is to iteratively adjust xt ∈ IRn (the parameter vector at time step t) using gradient information ∇ft(xt) obtained on a relatively small t-th batch of b datapoints. The Stochastic Gradient Descent (SGD) procedure then becomes an extension of the Gradient Descent (GD) to stochastic optimization of f as follows:\nxt+1 = xt − ηt∇ft(xt), (1)\nwhere ηt is a learning rate. One would like to consider second-order information\nxt+1 = xt − ηtH−1t ∇ft(xt), (2)\nbut this is often infeasible since the computation and storage of the inverse Hessian H−1t is intractable for large n. The usual way to deal with this problem by using limited-memory quasiNewton methods such as L-BFGS (Liu & Nocedal, 1989) is not currently in favor in deep learning, not the least due to (i) the stochasticity of ∇ft(xt), (ii) ill-conditioning of f and (iii) the presence of saddle points as a result of the hierarchical geometric structure of the parameter space (Fukumizu & Amari, 2000). Despite some recent progress in understanding and addressing the latter problems (Bordes et al., 2009; Dauphin et al., 2014; Choromanska et al., 2014; Dauphin et al., 2015), state-ofthe-art optimization techniques attempt to approximate the inverse Hessian in a reduced way, e.g., by considering only its diagonal to achieve adaptive learning rates. AdaDelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014) are notable examples of such methods.\nIntriguingly enough, the current state-of-the-art results on CIFAR-10, CIFAR-100, SVHN, ImageNet, PASCAL VOC and MS COCO datasets were obtained by Residual Neural Networks (He et al., 2015; Huang et al., 2016c; He et al., 2016; Zagoruyko & Komodakis, 2016) trained without the use of advanced methods such as AdaDelta and Adam. Instead, they simply use SGD with momentum 1:\nvt+1 = µtvt − ηt∇ft(xt), (3) xt+1 = xt + vt+1, (4)\nwhere vt is a velocity vector initially set to 0, ηt is a decreasing learning rate and µt is a momentum rate which defines the trade-off between the current and past observations of ∇ft(xt). The main difficulty in training a DNN is then associated with the scheduling of the learning rate and the amount of L2 weight decay regularization employed. A common learning rate schedule is to use a constant learning rate and divide it by a fixed constant in (approximately) regular intervals. The blue line in Figure 1 shows an example of such a schedule, as used by Zagoruyko & Komodakis (2016) to obtain the state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN datasets.\nIn this paper, we propose to periodically simulate warm restarts of SGD, where in each restart the learning rate is initialized to some value and is scheduled to decrease. Four different instantiations of this new learning rate schedule are visualized in Figure 1. Our empirical results suggest that SGD with warm restarts requires 2× to 4× fewer epochs than the currently-used learning rate schedule schemes to achieve comparable or even better results. Furthermore, combining the networks obtained right before restarts in an ensemble following the approach proposed by Huang et al. (2016a) improves our results further to 3.14% for CIFAR-10 and 16.21% for CIFAR-100. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.\n1More specifically, they employ Nesterov’s momentum (Nesterov, 1983; 2013)"
    }, {
      "heading" : "2 RELATED WORK",
      "text" : ""
    }, {
      "heading" : "2.1 RESTARTS IN GRADIENT-FREE OPTIMIZATION",
      "text" : "When optimizing multimodal functions one may want to find all global and local optima. The tractability of this task depends on the landscape of the function at hand and the budget of function evaluations. Gradient-free optimization approaches based on niching methods (Preuss, 2015) usually can deal with this task by covering the search space with dynamically allocated niches of local optimizers. However, these methods usually work only for relatively small search spaces, e.g., n < 10, and do not scale up due to the curse of dimensionality (Preuss, 2010). Instead, the current state-of-the-art gradient-free optimizers employ various restart mechanisms (Hansen, 2009; Loshchilov et al., 2012). One way to deal with multimodal functions is to iteratively sample a large number λ of candidate solutions, make a step towards better solutions and slowly shape the sampling distribution to maximize the likelihood of successful steps to appear again (Hansen & Kern, 2004). The larger the λ, the more global search is performed requiring more function evaluations. In order to achieve good anytime performance, it is common to start with a small λ and increase it (e.g., by doubling) after each restart. This approach works best on multimodal functions with a global funnel structure and also improves the results on ill-conditioned problems where numerical issues might lead to premature convergence when λ is small (Hansen, 2009)."
    }, {
      "heading" : "2.2 RESTARTS IN GRADIENT-BASED OPTIMIZATION",
      "text" : "Gradient-based optimization algorithms such as BFGS can also perform restarts to deal with multimodal functions (Ros, 2009). In large-scale settings when the usual number of variables n is on the order of 103 − 109, the availability of gradient information provides a speedup of a factor of n w.r.t. gradient-free approaches. Warm restarts are usually employed to improve the convergence rate rather than to deal with multimodality: often it is sufficient to approach any local optimum to a given precision and in many cases the problem at hand is unimodal. Fletcher & Reeves (1964) proposed to flesh the history of conjugate gradient method every n or (n + 1) iterations. Powell (1977) proposed to check whether enough orthogonality between ∇f(xt−1) and ∇f(xt) has been lost to warrant another warm restart. Recently, O’Donoghue & Candes (2012) noted that the iterates of accelerated gradient schemes proposed by Nesterov (1983; 2013) exhibit a periodic behavior if momentum is overused. The period of the oscillations is proportional to the square root of the local condition number of the (smooth convex) objective function. The authors showed that fixed warm restarts of the algorithm with a period proportional to the conditional number achieves the optimal linear convergence rate of the original accelerated gradient scheme. Since the condition number is an unknown parameter and its value may vary during the search, they proposed two adaptive warm restart techniques (O’Donoghue & Candes, 2012):\n• The function scheme restarts whenever the objective function increases.\n• The gradient scheme restarts whenever the angle between the momentum term and the negative gradient is obtuse, i.e, when the momentum seems to be taking us in a bad direction, as measured by the negative gradient at that point. This scheme resembles the one of Powell (1977) for the conjugate gradient method.\nO’Donoghue & Candes (2012) showed (and it was confirmed in a set of follow-up works) that these simple schemes provide an acceleration on smooth functions and can be adjusted to accelerate stateof-the-art methods such as FISTA on nonsmooth functions.\nSmith (2015; 2016) recently introduced cyclical learning rates for deep learning, his approach is closely-related to our approach in its spirit and formulation but does not focus on restarts.\nYang & Lin (2015) showed that Stochastic subGradient Descent with restarts can achieve a linear convergence rate for a class of non-smooth and non-strongly convex optimization problems where the epigraph of the objective function is a polyhedron. In contrast to our work, they never increase the learning rate to perform restarts but decrease it geometrically at each epoch. To perform restarts, they periodically reset the current solution to the averaged solution from the previous epoch."
    }, {
      "heading" : "3 STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS (SGDR)",
      "text" : "The existing restart techniques can also be used for stochastic gradient descent if the stochasticity is taken into account. Since gradients and loss values can vary widely from one batch of the data to another, one should denoise the incoming information: by considering averaged gradients and losses, e.g., once per epoch, the above-mentioned restart techniques can be used again.\nIn this work, we consider one of the simplest warm restart approaches. We simulate a new warmstarted run / restart of SGD once Ti epochs are performed, where i is the index of the run. Importantly, the restarts are not performed from scratch but emulated by increasing the learning rate ηt while the old value of xt is used as an initial solution. The amount of this increase controls to which extent the previously acquired information (e.g., momentum) is used.\nWithin the i-th run, we decay the learning rate with a cosine annealing for each batch as follows:\nηt = η i min +\n1 2 (ηimax − ηimin)(1 + cos( Tcur Ti π)), (5)\nwhere ηimin and η i max are ranges for the learning rate, and Tcur accounts for how many epochs have been performed since the last restart. Since Tcur is updated at each batch iteration t, it can take discredited values such as 0.1, 0.2, etc. Thus, ηt = ηimax when t = 0 and Tcur = 0. Once Tcur = Ti, the cos function will output −1 and thus ηt = ηimin. The decrease of the learning rate is shown in Figure 1 for fixed Ti = 50, Ti = 100 and Ti = 200; note that the logarithmic axis obfuscates the typical shape of the cosine function.\nIn order to improve anytime performance, we suggest an option to start with an initially small Ti and increase it by a factor of Tmult at every restart (see, e.g., Figure 1 for T0 = 1, Tmult = 2 and T0 = 10, Tmult = 2). It might be of great interest to decrease ηimax and η i min at every new restart. However, for the sake of simplicity, here, we keep ηimax and η i min the same for every i to reduce the number of hyperparameters involved.\nSince our simulated warm restarts (the increase of the learning rate) often temporarily worsen performance, we do not always use the last xt as our recommendation for the best solution (also called the incumbent solution). While our recommendation during the first run (before the first restart) is indeed the last xt, our recommendation after this is a solution obtained at the end of the last performed run at ηt = ηimin. We emphasize that with the help of this strategy, our method does not require a separate validation data set to determine a recommendation."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS",
      "text" : ""
    }, {
      "heading" : "4.1 EXPERIMENTAL SETTINGS",
      "text" : "We consider the problem of training Wide Residual Neural Networks (WRNs; see Zagoruyko & Komodakis (2016) for details) on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). We will use the abbreviation WRN-d-k to denote a WRN with depth d and width k. Zagoruyko & Komodakis (2016) obtained the best results with a WRN-28-10 architecture, i.e., a Residual Neural Network with d = 28 layers and k = 10 times more filters per layer than used in the original Residual Neural Networks (He et al., 2015; 2016).\nThe CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 32×32 color images drawn from 10 and 100 classes, respectively, split into 50,000 train and 10,000 test images. For image preprocessing Zagoruyko & Komodakis (2016) performed global contrast normalization and ZCA whitening. For data augmentation they performed horizontal flips and random crops from the image padded by 4 pixels on each side, filling missing pixels with reflections of the original image.\nFor training, Zagoruyko & Komodakis (2016) used SGD with Nesterov’s momentum with initial learning rate set to η0 = 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9 and minibatch size to 128. The learning rate is dropped by a factor of 0.2 at 60, 120 and 160 epochs, with a total budget of 200 epochs. We reproduce the results of Zagoruyko & Komodakis (2016) with the same settings except that i) we subtract per-pixel mean only and do not use ZCA whitening; ii) we use SGD with momentum as described by eq. (3-4) and not Nesterov’s momentum.\nThe schedule of ηt used by Zagoruyko & Komodakis (2016) is depicted by the blue line in Figure 1. The same schedule but with η0 = 0.05 is depicted by the red line. The schedule of ηt used in SGDR is also shown in Figure 1, with two initial learning rates T0 and two restart doubling periods."
    }, {
      "heading" : "4.2 SINGLE-MODEL RESULTS",
      "text" : "Table 1 shows that our experiments reproduce the results given by Zagoruyko & Komodakis (2016) for WRN-28-10 both on CIFAR-10 and CIFAR-100. These “default” experiments with η0 = 0.1 and η0 = 0.05 correspond to the blue and red lines in Figure 2. The results for η0 = 0.05 show better performance, and therefore we use η0 = 0.05 in our later experiments.\nSGDR with T0 = 50, T0 = 100 and T0 = 200 for Tmult = 1 perform warm restarts every 50, 100 and 200 epochs, respectively. A single run of SGD with the schedule given by eq. (5) for T0 = 200 shows the best results suggesting that the original schedule of WRNs might be suboptimal w.r.t. the test error in these settings. However, the same setting with T0 = 200 leads to the worst anytime performance except for the very last epochs.\nSGDR with T0 = 1, Tmult = 2 and T0 = 10, Tmult = 2 performs its first restart after 1 and 10 epochs, respectively. Then, it doubles the maximum number of epochs for every new restart. The main purpose of this doubling is to reach good test error as soon as possible, i.e., achieve good anytime performance. Figure 2 shows that this is achieved and test errors around 4% on CIFAR-10 and around 20% on CIFAR-100 can be obtained about 2-4 times faster than with the default schedule used by Zagoruyko & Komodakis (2016).\nSince SGDR achieves good performance faster, it may allow us to train larger networks. We therefore investigated whether results on CIFAR-10 and CIFAR-100 can be further improved by making WRNs two times wider, i.e., by training WRN-28-20 instead of WRN-28-10. Table 1 shows that the results indeed improved, by about 0.25% on CIFAR-10 and by about 0.5-1.0% on CIFAR-100. While network architecture WRN-28-20 requires roughly three-four times more computation than WRN-28-10, the aggressive learning rate reduction of SGDR nevertheless allowed us to achieve a better error rate in the same time on WRN-28-20 as we spent on 200 epochs of training on WRN28-10. Specifically, Figure 2 (right middle and right bottom) show that after only 50 epochs, SGDR (even without restarts, using T0 = 50, Tmult = 1) achieved an error rate below 19% (whereas none of the other learning methods performed better than 19.5% on WRN-28-10). We therefore have hope that – by enabling researchers to test new architectures faster – SGDR’s good anytime performance may also lead to improvements of the state of the art.\nIn a final experiment for SGDR by itself, Figure 7 in the appendix compares SGDR and the default schedule with respect to training and test performance. As the figure shows, SGDR optimizes training loss faster than the standard default schedule until about epoch 120. After this, the default schedule overfits, as can be seen by an increase of the test error both on CIFAR-10 and CIFAR-100 (see, e.g., the right middle plot of Figure 7). In contrast, we only witnessed very mild overfitting for SGDR."
    }, {
      "heading" : "4.3 ENSEMBLE RESULTS",
      "text" : "Our initial arXiv report on SGDR (Loshchilov & Hutter, 2016) inspired a follow-up study by Huang et al. (2016a) in which the authors suggest to takeM snapshots of the models obtained by SGDR (in their paper referred to as cyclical learning rate schedule and cosine annealing cycles) right before M last restarts and to use those to build an ensemble, thereby obtaining ensembles “for free” (in contrast to having to perform multiple independent runs). The authors demonstrated new state-of-\nthe-art results on CIFAR datasets by making ensembles of DenseNet models (Huang et al., 2016b). Here, we investigate whether their conclusions hold for WRNs used in our study. We used WRN28-10 trained by SGDR with T0 = 10, Tmult = 2 as our baseline model.\nFigure 3 and Table 2 aggregate the results of our study. The original test error of 4.03% on CIFAR-10 and 19.57% on CIFAR-100 (median of 16 runs) can be improved to 3.51% on CIFAR-10 and 17.75% on CIFAR-100 when M = 3 snapshots are taken at epochs 30, 70 and 150: when the learning rate of SGDR with T0 = 10, Tmult = 2 is scheduled to achieve 0 (see Figure 1) and the models are used with uniform weights to build an ensemble. To achieve the same result, one would have to aggregate N = 3 models obtained at epoch 150 of N = 3 independent runs (see N = 3,M = 1 in Figure 3). Thus, the aggregation from snapshots provides a 3-fold speedup in these settings because additional (M > 1-th) snapshots from a single SGDR run are computationally free. Interestingly, aggregation of models from independent runs (when N > 1 and M = 1) does not scale up as well as from M > 1 snapshots of independent runs when the same number of models is considered: the case of N = 3 and M = 3 provides better performance than the cases of M = 1 with N = 18 and N = 21. Not only the number of snapshots M per run but also their origin is crucial. Thus, naively building ensembles from models obtained at last epochs only (i.e., M = 3 snapshots at epochs 148, 149, 150) did not improve the results (i.e., the baseline of M = 1 snapshot at 150) thereby confirming the conclusion of Huang et al. (2016a) that snapshots of SGDR provide a useful diversity of predictions for ensembles.\nThree runs (N = 3) of SGDR with M = 3 snapshots per run are sufficient to greatly improve the results to 3.25% on CIFAR-10 and 16.64% on CIFAR-100 outperforming the results of Huang et al. (2016a). By increasing N to 16 one can achieve 3.14% on CIFAR-10 and 16.21% on CIFAR-100. We believe that these results could be further improved by considering better baseline models than WRN-28-10 (e.g., WRN-28-20)."
    }, {
      "heading" : "4.4 EXPERIMENTS ON A DATASET OF EEG RECORDINGS",
      "text" : "To demonstrate the generality of SGDR, we also considered a very different domain: a dataset of electroencephalographic (EEG) recordings of brain activity for classification of actual right and left hand and foot movements of 14 subjects with roughly 1000 trials per subject. The best classification results obtained with the original pipeline based on convolutional neural networks [R. Schirrmeister et al. Convolutional neural networks for EEG analysis: Design choices, training strategies, and feature visualization., under review at Neuroimage] were used as our reference. First, we compared the baseline learning rate schedule with different settings of the total number of epochs and initial learning rates (see Figure 4). When 30 epochs were considered, we dropped the learning rate by a factor of 10 at epoch indexes 10, 15 and 20. As expected, with more epochs used and a similar (budget proportional) schedule better results can be achieved. Alternatively, one can consider SGDR and get a similar final performance while having a better anytime performance without defining the total budget of epochs beforehand.\nSimilarly to our results on the CIFAR datasets, our experiments with the EEG data confirm that snapshots are useful and the median reference error (about 9%) can be improved i) by 1-2% when model snapshots of a single run are considered, and ii) by 2-3% when model snapshots from both hyperparameter settings are considered. The latter would correspond to N = 2 in Section (4.3)."
    }, {
      "heading" : "4.5 PRELIMINARY EXPERIMENTS ON A DOWNSAMPLED IMAGENET DATASET",
      "text" : "In order to additionally validate our SGDR on a larger dataset, we constructed a downsampled version of the ImageNet dataset [P. Chrabaszcz, I. Loshchilov and F. Hutter. A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets., in preparation]. In contrast to earlier attempts (Pouransari & Ghili, 2015), our downsampled ImageNet contains exactly the same images from 1000 classes as the original ImageNet but resized with box downsampling to 32 × 32 pixels. Thus, this dataset is substantially harder than the original ImageNet dataset because the average number of pixels per image is now two orders of magnitude smaller. The new dataset is also more difficult than the CIFAR datasets because more classes are used and the relevant objects to be classified often cover only a tiny subspace of the image and not most of the image as in the CIFAR datasets.\nWe benchmarked SGD with momentum with the default learning rate schedule, SGDR with T0 = 1, Tmult = 2 and SGDR with T0 = 10, Tmult = 2 on WRN-28-10, all trained with 4 settings of the initial learning rate ηimax: 0.050, 0.025, 0.01 and 0.005. We used the same data augmentation procedure as for the CIFAR datasets. Similarly to the results on the CIFAR datasets, Figure 5 shows that SGDR demonstrates better anytime performance. SGDR with T0 = 10, Tmult = 2, ηimax = 0.01 achieves top-1 error of 39.24% and top-5 error of 17.17% matching the original results by AlexNets (40.7% and 18.2%, respectively) obtained on the original ImageNet with full-size images of ca. 50 times more pixels per image (Krizhevsky et al., 2012b). Interestingly, when the dataset is permuted only within 10 subgroups each formed from 100 classes, SGDR also demonstrates better results (see Figure 8 in the Supplementary Material). An interpretation of this might be that while the initial learning rate seems to be very important, SGDR reduces the problem of improper selection of the latter by scanning / annealing from the initial learning rate to 0.\nClearly, longer runs (more than 40 epochs considered in this preliminary experiment) and hyperparameter tuning of learning rates, regularization and other hyperparameters shall further improve the results."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "Our results suggest that even without any restarts the proposed aggressive learning rate schedule given by eq. (5) is competitive w.r.t. the default schedule when training WRNs on the CIFAR10 (e.g., for T0 = 200, Tmult = 1) and CIFAR-100 datasets. In practice, the proposed schedule requires only two hyper-parameters to be defined: the initial learning rate and the total number of epochs.\nWe found that the anytime performance of SGDR remain similar when shorter epochs are considered (see section 8.1 in the Supplemenary Material).\nOne should not suppose that the parameter values used in this study and many other works with (Residual) Neural Networks are selected to demonstrate the fastest decrease of the training error. Instead, the best validation or / and test errors are in focus. Notably, the validation error is rarely used when training Residual Neural Networks because the recommendation is defined by the final solution (in our approach, the final solution of each run). One could use the validation error to determine the optimal initial learning rate and then run on the whole dataset; this could further improve results.\nThe main purpose of our proposed warm restart scheme for SGD is to improve its anytime performance. While we mentioned that restarts can be useful to deal with multi-modal functions, we do not claim that we observe any effect related to multi-modality.\nAs we noted earlier, one could decrease ηimax and η i min at every new warm restart to control the amount of divergence. If new restarts are worse than the old ones w.r.t. validation error, then one might also consider going back to the last best solution and perform a new restart with adjusted hyperparameters.\nOur results reproduce the finding by Huang et al. (2016a) that intermediate models generated by SGDR can be used to build efficient ensembles at no cost. This finding makes SGDR especially attractive for scenarios when ensemble building is considered."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we investigated a simple warm restart mechanism for SGD to accelerate the training of DNNs. Our SGDR simulates warm restarts by scheduling the learning rate to achieve competitive results on CIFAR-10 and CIFAR-100 roughly two to four times faster. We also achieved new stateof-the-art results with SGDR, mainly by using even wider WRNs and ensembles of snapshots from\nSGDR’s trajectory. Future empirical studies should also consider the SVHN, ImageNet and MS COCO datasets, for which Residual Neural Networks showed the best results so far. Our preliminary results on a dataset of EEG recordings suggest that SGDR delivers better and better results as we carry out more restarts and use more model snapshots. The results on our downsampled ImageNet dataset suggest that SGDR might also reduce the problem of learning rate selection because the annealing and restarts of SGDR scan / consider a range of learning rate values. Future work should consider warm restarts for other popular training algorithms such as AdaDelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014).\nAlternative network structures should be also considered; e.g., soon after our initial arXiv report (Loshchilov & Hutter, 2016), Zhang et al. (2016); Huang et al. (2016b); Han et al. (2016) reported that WRNs models can be replaced by more memory-efficient models. Thus, it should be tested whether our results for individual models and ensembles can be further improved by using their networks instead of WRNs. Deep compression methods (Han et al., 2015) can be used to reduce the time and memory costs of DNNs and their ensembles."
    }, {
      "heading" : "7 ACKNOWLEDGMENTS",
      "text" : "This work was supported by the German Research Foundation (DFG), under the BrainLinksBrainTools Cluster of Excellence (grant number EXC 1086). We thank Gao Huang, Kilian Quirin Weinberger, Jost Tobias Springenberg, Mark Schmidt and three anonymous reviewers for their helpful comments and suggestions."
    }, {
      "heading" : "8 SUPPLEMENTARY MATERIAL",
      "text" : ""
    }, {
      "heading" : "8.1 50K VS 100K EXAMPLES PER EPOCH",
      "text" : "Our data augmentation procedure code is inherited from the Lasagne Recipe code for ResNets where flipped images are added to the training set. This doubles the number of training examples per epoch and thus might impact the results because hyperparameter values defined as a function of epoch index have a different meaning. While our experimental results given in Table 1 reproduced the results obtained by Zagoruyko & Komodakis (2016), here we test whether SGDR still makes sense for WRN-28-1 (i.e., ResNet with 28 layers) where one epoch corresponds to 50k training examples. We investigate different learning rate values for the default learning rate schedule (4 values out of [0.01, 0.025, 0.05, 0.1]) and SGDR (3 values out of [0.025, 0.05, 0.1]). In line with the results given in the main paper, Figure 6 suggests that SGDR is competitive in terms of anytime performance."
    } ],
    "references" : [ {
      "title" : "Sgd-qn: Careful quasi-newton stochastic gradient descent",
      "author" : [ "Antoine Bordes", "Léon Bottou", "Patrick Gallinari" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2009
    }, {
      "title" : "The loss surface of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1412.0233,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2014
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Rmsprop and equilibrated adaptive learning rates for non-convex optimization",
      "author" : [ "Yann N Dauphin", "Harm de Vries", "Junyoung Chung", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.04390,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2015
    }, {
      "title" : "New types of deep neural network learning for speech recognition and related applications: An overview",
      "author" : [ "L. Deng", "G. Hinton", "B. Kingsbury" ],
      "venue" : "In Proc. of ICASSP’13,",
      "citeRegEx" : "Deng et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2013
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "In Proc. of ICML’14,",
      "citeRegEx" : "Donahue et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2014
    }, {
      "title" : "Function minimization by conjugate gradients",
      "author" : [ "Reeves Fletcher", "Colin M Reeves" ],
      "venue" : "The computer journal,",
      "citeRegEx" : "Fletcher and Reeves.,? \\Q1964\\E",
      "shortCiteRegEx" : "Fletcher and Reeves.",
      "year" : 1964
    }, {
      "title" : "Local minima and plateaus in hierarchical structures of multilayer perceptrons",
      "author" : [ "Kenji Fukumizu", "Shun-ichi Amari" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Fukumizu and Amari.,? \\Q2000\\E",
      "shortCiteRegEx" : "Fukumizu and Amari.",
      "year" : 2000
    }, {
      "title" : "Deep pyramidal residual networks",
      "author" : [ "Dongyoon Han", "Jiwhan Kim", "Junmo Kim" ],
      "venue" : "arXiv preprint arXiv:1610.02915,",
      "citeRegEx" : "Han et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally" ],
      "venue" : "arXiv preprint arXiv:1510.00149,",
      "citeRegEx" : "Han et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Benchmarking a BI-population CMA-ES on the BBOB-2009 function testbed",
      "author" : [ "Nikolaus Hansen" ],
      "venue" : "In Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers,",
      "citeRegEx" : "Hansen.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hansen.",
      "year" : 2009
    }, {
      "title" : "Evaluating the cma evolution strategy on multimodal test functions",
      "author" : [ "Nikolaus Hansen", "Stefan Kern" ],
      "venue" : "In International Conference on Parallel Problem Solving from Nature,",
      "citeRegEx" : "Hansen and Kern.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hansen and Kern.",
      "year" : 2004
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Snapshot ensembles: Train 1, get m for free",
      "author" : [ "Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger" ],
      "venue" : "ICLR 2017 submission,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Kilian Q Weinberger" ],
      "venue" : "arXiv preprint arXiv:1608.06993,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "In Proc. of NIPS’12,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "Dong C Liu", "Jorge Nocedal" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Liu and Nocedal.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu and Nocedal.",
      "year" : 1989
    }, {
      "title" : "SGDR: Stochastic Gradient Descent with Restarts",
      "author" : [ "Ilya Loshchilov", "Frank Hutter" ],
      "venue" : "arXiv preprint arXiv:1608.03983,",
      "citeRegEx" : "Loshchilov and Hutter.,? \\Q2016\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2016
    }, {
      "title" : "Alternative restart strategies for CMA-ES",
      "author" : [ "Ilya Loshchilov", "Marc Schoenauer", "Michele Sebag" ],
      "venue" : "In International Conference on Parallel Problem Solving from Nature,",
      "citeRegEx" : "Loshchilov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Loshchilov et al\\.",
      "year" : 2012
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate o (1/k2)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "In Soviet Mathematics Doklady,",
      "citeRegEx" : "Nesterov.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 1983
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course, volume 87",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Nesterov.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2013
    }, {
      "title" : "Adaptive restart for accelerated gradient schemes",
      "author" : [ "Brendan O’Donoghue", "Emmanuel Candes" ],
      "venue" : "arXiv preprint arXiv:1204.3982,",
      "citeRegEx" : "O.Donoghue and Candes.,? \\Q2012\\E",
      "shortCiteRegEx" : "O.Donoghue and Candes.",
      "year" : 2012
    }, {
      "title" : "Tiny imagenet visual recognition challenge",
      "author" : [ "Hadi Pouransari", "Saman Ghili" ],
      "venue" : "CS231 course at STANFORD,",
      "citeRegEx" : "Pouransari and Ghili.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pouransari and Ghili.",
      "year" : 2015
    }, {
      "title" : "Restart procedures for the conjugate gradient method",
      "author" : [ "Michael James David Powell" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Powell.,? \\Q1977\\E",
      "shortCiteRegEx" : "Powell.",
      "year" : 1977
    }, {
      "title" : "Niching the CMA-ES via nearest-better clustering",
      "author" : [ "Mike Preuss" ],
      "venue" : "In Proceedings of the 12th annual conference companion on Genetic and evolutionary computation,",
      "citeRegEx" : "Preuss.,? \\Q2010\\E",
      "shortCiteRegEx" : "Preuss.",
      "year" : 2010
    }, {
      "title" : "Niching methods and multimodal optimization performance",
      "author" : [ "Mike Preuss" ],
      "venue" : null,
      "citeRegEx" : "Preuss.,? \\Q2015\\E",
      "shortCiteRegEx" : "Preuss.",
      "year" : 2015
    }, {
      "title" : "Benchmarking the bfgs algorithm on the bbob-2009 function testbed",
      "author" : [ "Raymond Ros" ],
      "venue" : "In Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers,",
      "citeRegEx" : "Ros.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ros.",
      "year" : 2009
    }, {
      "title" : "No more pesky learning rate guessing games",
      "author" : [ "Leslie N Smith" ],
      "venue" : "arXiv preprint arXiv:1506.01186,",
      "citeRegEx" : "Smith.,? \\Q2015\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 2015
    }, {
      "title" : "Cyclical learning rates for training neural networks",
      "author" : [ "Leslie N Smith" ],
      "venue" : "arXiv preprint arXiv:1506.01186v3,",
      "citeRegEx" : "Smith.,? \\Q2016\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 2016
    }, {
      "title" : "Stochastic subgradient methods with linear convergence for polyhedral convex optimization",
      "author" : [ "Tianbao Yang", "Qihang Lin" ],
      "venue" : "arXiv preprint arXiv:1510.01444,",
      "citeRegEx" : "Yang and Lin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang and Lin.",
      "year" : 2015
    }, {
      "title" : "Wide residual networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "arXiv preprint arXiv:1605.07146,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2016
    }, {
      "title" : "Adadelta: An adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Residual Networks of Residual Networks: Multilevel Residual Networks",
      "author" : [ "K. Zhang", "M. Sun", "T.X. Han", "X. Yuan", "L. Guo", "T. Liu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Deep neural networks (DNNs) are currently the best-performing method for many classification problems, such as object recognition from images (Krizhevsky et al., 2012a; Donahue et al., 2014) or speech recognition from audio data (Deng et al.",
      "startOffset" : 142,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : ", 2014) or speech recognition from audio data (Deng et al., 2013).",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Despite some recent progress in understanding and addressing the latter problems (Bordes et al., 2009; Dauphin et al., 2014; Choromanska et al., 2014; Dauphin et al., 2015), state-ofthe-art optimization techniques attempt to approximate the inverse Hessian in a reduced way, e.",
      "startOffset" : 81,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "Despite some recent progress in understanding and addressing the latter problems (Bordes et al., 2009; Dauphin et al., 2014; Choromanska et al., 2014; Dauphin et al., 2015), state-ofthe-art optimization techniques attempt to approximate the inverse Hessian in a reduced way, e.",
      "startOffset" : 81,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "Despite some recent progress in understanding and addressing the latter problems (Bordes et al., 2009; Dauphin et al., 2014; Choromanska et al., 2014; Dauphin et al., 2015), state-ofthe-art optimization techniques attempt to approximate the inverse Hessian in a reduced way, e.",
      "startOffset" : 81,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "Despite some recent progress in understanding and addressing the latter problems (Bordes et al., 2009; Dauphin et al., 2014; Choromanska et al., 2014; Dauphin et al., 2015), state-ofthe-art optimization techniques attempt to approximate the inverse Hessian in a reduced way, e.",
      "startOffset" : 81,
      "endOffset" : 172
    }, {
      "referenceID" : 36,
      "context" : "AdaDelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014) are notable examples of such methods.",
      "startOffset" : 9,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "Intriguingly enough, the current state-of-the-art results on CIFAR-10, CIFAR-100, SVHN, ImageNet, PASCAL VOC and MS COCO datasets were obtained by Residual Neural Networks (He et al., 2015; Huang et al., 2016c; He et al., 2016; Zagoruyko & Komodakis, 2016) trained without the use of advanced methods such as AdaDelta and Adam.",
      "startOffset" : 172,
      "endOffset" : 256
    }, {
      "referenceID" : 13,
      "context" : "Intriguingly enough, the current state-of-the-art results on CIFAR-10, CIFAR-100, SVHN, ImageNet, PASCAL VOC and MS COCO datasets were obtained by Residual Neural Networks (He et al., 2015; Huang et al., 2016c; He et al., 2016; Zagoruyko & Komodakis, 2016) trained without the use of advanced methods such as AdaDelta and Adam.",
      "startOffset" : 172,
      "endOffset" : 256
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, combining the networks obtained right before restarts in an ensemble following the approach proposed by Huang et al. (2016a) improves our results further to 3.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 24,
      "context" : "More specifically, they employ Nesterov’s momentum (Nesterov, 1983; 2013)",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 30,
      "context" : "Gradient-free optimization approaches based on niching methods (Preuss, 2015) usually can deal with this task by covering the search space with dynamically allocated niches of local optimizers.",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : ", n < 10, and do not scale up due to the curse of dimensionality (Preuss, 2010).",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "Instead, the current state-of-the-art gradient-free optimizers employ various restart mechanisms (Hansen, 2009; Loshchilov et al., 2012).",
      "startOffset" : 97,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "Instead, the current state-of-the-art gradient-free optimizers employ various restart mechanisms (Hansen, 2009; Loshchilov et al., 2012).",
      "startOffset" : 97,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "This approach works best on multimodal functions with a global funnel structure and also improves the results on ill-conditioned problems where numerical issues might lead to premature convergence when λ is small (Hansen, 2009).",
      "startOffset" : 213,
      "endOffset" : 227
    }, {
      "referenceID" : 31,
      "context" : "Gradient-based optimization algorithms such as BFGS can also perform restarts to deal with multimodal functions (Ros, 2009).",
      "startOffset" : 112,
      "endOffset" : 123
    }, {
      "referenceID" : 28,
      "context" : "Gradient-based optimization algorithms such as BFGS can also perform restarts to deal with multimodal functions (Ros, 2009). In large-scale settings when the usual number of variables n is on the order of 10 − 10, the availability of gradient information provides a speedup of a factor of n w.r.t. gradient-free approaches. Warm restarts are usually employed to improve the convergence rate rather than to deal with multimodality: often it is sufficient to approach any local optimum to a given precision and in many cases the problem at hand is unimodal. Fletcher & Reeves (1964) proposed to flesh the history of conjugate gradient method every n or (n + 1) iterations.",
      "startOffset" : 113,
      "endOffset" : 581
    }, {
      "referenceID" : 26,
      "context" : "Powell (1977) proposed to check whether enough orthogonality between ∇f(xt−1) and ∇f(xt) has been lost to warrant another warm restart.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 26,
      "context" : "Powell (1977) proposed to check whether enough orthogonality between ∇f(xt−1) and ∇f(xt) has been lost to warrant another warm restart. Recently, O’Donoghue & Candes (2012) noted that the iterates of accelerated gradient schemes proposed by Nesterov (1983; 2013) exhibit a periodic behavior if momentum is overused.",
      "startOffset" : 0,
      "endOffset" : 173
    }, {
      "referenceID" : 28,
      "context" : "This scheme resembles the one of Powell (1977) for the conjugate gradient method.",
      "startOffset" : 33,
      "endOffset" : 47
    }, {
      "referenceID" : 32,
      "context" : "Smith (2015; 2016) recently introduced cyclical learning rates for deep learning, his approach is closely-related to our approach in its spirit and formulation but does not focus on restarts. Yang & Lin (2015) showed that Stochastic subGradient Descent with restarts can achieve a linear convergence rate for a class of non-smooth and non-strongly convex optimization problems where the epigraph of the objective function is a polyhedron.",
      "startOffset" : 0,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "1 EXPERIMENTAL SETTINGS We consider the problem of training Wide Residual Neural Networks (WRNs; see Zagoruyko & Komodakis (2016) for details) on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009).",
      "startOffset" : 182,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : ", a Residual Neural Network with d = 28 layers and k = 10 times more filters per layer than used in the original Residual Neural Networks (He et al., 2015; 2016).",
      "startOffset" : 138,
      "endOffset" : 161
    }, {
      "referenceID" : 19,
      "context" : "The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 32×32 color images drawn from 10 and 100 classes, respectively, split into 50,000 train and 10,000 test images.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "1 EXPERIMENTAL SETTINGS We consider the problem of training Wide Residual Neural Networks (WRNs; see Zagoruyko & Komodakis (2016) for details) on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). We will use the abbreviation WRN-d-k to denote a WRN with depth d and width k. Zagoruyko & Komodakis (2016) obtained the best results with a WRN-28-10 architecture, i.",
      "startOffset" : 183,
      "endOffset" : 310
    }, {
      "referenceID" : 12,
      "context" : ", a Residual Neural Network with d = 28 layers and k = 10 times more filters per layer than used in the original Residual Neural Networks (He et al., 2015; 2016). The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 32×32 color images drawn from 10 and 100 classes, respectively, split into 50,000 train and 10,000 test images. For image preprocessing Zagoruyko & Komodakis (2016) performed global contrast normalization and ZCA whitening.",
      "startOffset" : 139,
      "endOffset" : 394
    }, {
      "referenceID" : 12,
      "context" : ", a Residual Neural Network with d = 28 layers and k = 10 times more filters per layer than used in the original Residual Neural Networks (He et al., 2015; 2016). The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 32×32 color images drawn from 10 and 100 classes, respectively, split into 50,000 train and 10,000 test images. For image preprocessing Zagoruyko & Komodakis (2016) performed global contrast normalization and ZCA whitening. For data augmentation they performed horizontal flips and random crops from the image padded by 4 pixels on each side, filling missing pixels with reflections of the original image. For training, Zagoruyko & Komodakis (2016) used SGD with Nesterov’s momentum with initial learning rate set to η0 = 0.",
      "startOffset" : 139,
      "endOffset" : 678
    }, {
      "referenceID" : 12,
      "context" : ", a Residual Neural Network with d = 28 layers and k = 10 times more filters per layer than used in the original Residual Neural Networks (He et al., 2015; 2016). The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 32×32 color images drawn from 10 and 100 classes, respectively, split into 50,000 train and 10,000 test images. For image preprocessing Zagoruyko & Komodakis (2016) performed global contrast normalization and ZCA whitening. For data augmentation they performed horizontal flips and random crops from the image padded by 4 pixels on each side, filling missing pixels with reflections of the original image. For training, Zagoruyko & Komodakis (2016) used SGD with Nesterov’s momentum with initial learning rate set to η0 = 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9 and minibatch size to 128. The learning rate is dropped by a factor of 0.2 at 60, 120 and 160 epochs, with a total budget of 200 epochs. We reproduce the results of Zagoruyko & Komodakis (2016) with the same settings except that i) we subtract per-pixel mean only and do not use ZCA whitening; ii) we use SGD with momentum as described by eq.",
      "startOffset" : 139,
      "endOffset" : 1006
    }, {
      "referenceID" : 12,
      "context" : "depth-k # params # runs CIFAR-10 CIFAR-100 original-ResNet (He et al., 2015) 110 1.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "91 n/a pre-act-ResNet (He et al., 2016) 110 1.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "Figure 3: Test errors of ensemble models built from N runs of SGDR on WRN-28-10 with M model snapshots per run made at epochs 150, 70 and 30 (right before warm restarts of SGDR as suggested by Huang et al. (2016a)).",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "Our initial arXiv report on SGDR (Loshchilov & Hutter, 2016) inspired a follow-up study by Huang et al. (2016a) in which the authors suggest to takeM snapshots of the models obtained by SGDR (in their paper referred to as cyclical learning rate schedule and cosine annealing cycles) right before M last restarts and to use those to build an ensemble, thereby obtaining ensembles “for free” (in contrast to having to perform multiple independent runs).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "the-art results on CIFAR datasets by making ensembles of DenseNet models (Huang et al., 2016b). Here, we investigate whether their conclusions hold for WRNs used in our study. We used WRN28-10 trained by SGDR with T0 = 10, Tmult = 2 as our baseline model. Figure 3 and Table 2 aggregate the results of our study. The original test error of 4.03% on CIFAR-10 and 19.57% on CIFAR-100 (median of 16 runs) can be improved to 3.51% on CIFAR-10 and 17.75% on CIFAR-100 when M = 3 snapshots are taken at epochs 30, 70 and 150: when the learning rate of SGDR with T0 = 10, Tmult = 2 is scheduled to achieve 0 (see Figure 1) and the models are used with uniform weights to build an ensemble. To achieve the same result, one would have to aggregate N = 3 models obtained at epoch 150 of N = 3 independent runs (see N = 3,M = 1 in Figure 3). Thus, the aggregation from snapshots provides a 3-fold speedup in these settings because additional (M > 1-th) snapshots from a single SGDR run are computationally free. Interestingly, aggregation of models from independent runs (when N > 1 and M = 1) does not scale up as well as from M > 1 snapshots of independent runs when the same number of models is considered: the case of N = 3 and M = 3 provides better performance than the cases of M = 1 with N = 18 and N = 21. Not only the number of snapshots M per run but also their origin is crucial. Thus, naively building ensembles from models obtained at last epochs only (i.e., M = 3 snapshots at epochs 148, 149, 150) did not improve the results (i.e., the baseline of M = 1 snapshot at 150) thereby confirming the conclusion of Huang et al. (2016a) that snapshots of SGDR provide a useful diversity of predictions for ensembles.",
      "startOffset" : 74,
      "endOffset" : 1634
    }, {
      "referenceID" : 14,
      "context" : "the-art results on CIFAR datasets by making ensembles of DenseNet models (Huang et al., 2016b). Here, we investigate whether their conclusions hold for WRNs used in our study. We used WRN28-10 trained by SGDR with T0 = 10, Tmult = 2 as our baseline model. Figure 3 and Table 2 aggregate the results of our study. The original test error of 4.03% on CIFAR-10 and 19.57% on CIFAR-100 (median of 16 runs) can be improved to 3.51% on CIFAR-10 and 17.75% on CIFAR-100 when M = 3 snapshots are taken at epochs 30, 70 and 150: when the learning rate of SGDR with T0 = 10, Tmult = 2 is scheduled to achieve 0 (see Figure 1) and the models are used with uniform weights to build an ensemble. To achieve the same result, one would have to aggregate N = 3 models obtained at epoch 150 of N = 3 independent runs (see N = 3,M = 1 in Figure 3). Thus, the aggregation from snapshots provides a 3-fold speedup in these settings because additional (M > 1-th) snapshots from a single SGDR run are computationally free. Interestingly, aggregation of models from independent runs (when N > 1 and M = 1) does not scale up as well as from M > 1 snapshots of independent runs when the same number of models is considered: the case of N = 3 and M = 3 provides better performance than the cases of M = 1 with N = 18 and N = 21. Not only the number of snapshots M per run but also their origin is crucial. Thus, naively building ensembles from models obtained at last epochs only (i.e., M = 3 snapshots at epochs 148, 149, 150) did not improve the results (i.e., the baseline of M = 1 snapshot at 150) thereby confirming the conclusion of Huang et al. (2016a) that snapshots of SGDR provide a useful diversity of predictions for ensembles. Three runs (N = 3) of SGDR with M = 3 snapshots per run are sufficient to greatly improve the results to 3.25% on CIFAR-10 and 16.64% on CIFAR-100 outperforming the results of Huang et al. (2016a). By increasing N to 16 one can achieve 3.",
      "startOffset" : 74,
      "endOffset" : 1911
    }, {
      "referenceID" : 14,
      "context" : "Our results reproduce the finding by Huang et al. (2016a) that intermediate models generated by SGDR can be used to build efficient ensembles at no cost.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 36,
      "context" : "Future work should consider warm restarts for other popular training algorithms such as AdaDelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014).",
      "startOffset" : 97,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "Deep compression methods (Han et al., 2015) can be used to reduce the time and memory costs of DNNs and their ensembles.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : "Future work should consider warm restarts for other popular training algorithms such as AdaDelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014). Alternative network structures should be also considered; e.g., soon after our initial arXiv report (Loshchilov & Hutter, 2016), Zhang et al. (2016); Huang et al.",
      "startOffset" : 98,
      "endOffset" : 291
    }, {
      "referenceID" : 12,
      "context" : "(2016); Huang et al. (2016b); Han et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "(2016b); Han et al. (2016) reported that WRNs models can be replaced by more memory-efficient models.",
      "startOffset" : 9,
      "endOffset" : 27
    } ],
    "year" : 2017,
    "abstractText" : "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradientbased optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR",
    "creator" : "LaTeX with hyperref package"
  }
}