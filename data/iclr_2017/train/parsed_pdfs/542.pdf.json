{
  "name" : "542.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "POLICY SKETCHES", "Jacob Andreas", "Dan Klein", "Sergey Levine" ],
    "emails" : [ "jda@eecs.berkeley.edu", "klein@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nThis paper describes a framework for learning composable deep subpolicies in a multitask setting, guided only by abstract policy sketches. We are interested in problems like the ones shown in Figure 1, with collections of tasks that involve sparse rewards and long-term planning, but which share structure in the form of common subgoals or reusable high-level actions. Our work aims to develop models that can learn efficiently from these sparse rewards and rapidly adapt to new tasks, by exploiting this shared structure and translating success on one task into progress on others. Our approach ultimately induces a library of high-level actions directly from symbolic annotations like the ones marked K1 and K2 in the figure.\nThis approach builds on a significant body of research in reinforcement learning that focuses on hierarchical representations of behavior. In these approaches, a high-level controller learns a policy over high-level actions—known variously as options (Sutton et al., 1999), skills\n(Konidaris & Barto, 2007), or primitives (Hauser et al., 2008)—which are themselves implemented as policies over low-level actions in the environment. While one line of research (e.g. Daniel et al. (2012)) investigates learning hierarchical policies without any supervision, such hierarchies are empirically difficult to learn directly from unconstrained interaction (Hengst, 2002). The bulk of existing work instead relies on additional information (in the form of intermediate rewards, subtask completion signals, or intrinsic motivations) that guide the learner toward useful high-level actions. While effective, these approaches depend on state representations simple or structured enough that suitable reward signals can be effectively engineered by hand.\nHere we focus on multitask learning of hierarchical policies from a weaker form of supervision: at training time, each task (τ1 and τ2 in Figure 1) is annotated with a sketch (K1 andK2) consisting of a sequence of high-level action symbols (b1, b2 and b3)—with no information about how these actions should be implemented. Our approach associates each such high-level action with its own lowlevel subpolicy, and jointly optimizes over concatenated task-specific policies by tying parameters across shared subpolicies. Our thesis is that even the minimal information about high-level policy structure contained in a sketch provides enough of a learning signal to induce general, reusable subpolicies. Crucially, sketches are totally ungrounded in the representation of the world—they require no intervention in a simulator or environment model.\nThe present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016; Reed & de Freitas, 2015). Here we focus on learning in interactive environments with reinforcement training signals. This extension presents a variety of technical challenges. Concretely, our contributions are:\n• A general paradigm for multitask, hierarchical, deep reinforcement learning guided by abstract sketches of task-specific policies.\n• A concrete agent architecture for learning in this paradigm, featuring a modular model structure and multitask actor–critic training objective.\nWe evaluate our approach on two families of tasks: a maze navigation game (Figure 3a), in which the agent must navigate through a sequence of locked doors to reach a target room; and a 2-D Minecraft-inspired crafting game (Figure 3b), in which the agent must acquire particular resources by finding raw ingredients, combining them together in the proper order, and in some cases building intermediate tools that enable the agent to alter the environment itself. In both games, the agent receives a reward only after the final goal is accomplished. For the most challenging tasks, involving sequences of four or five high-level actions, a task-specific agent initially following a random policy essentially never discovers the reward signal.\nWe evaluate a modular agent architecture trained with guidance from policy sketches under several different data conditions: (1) when learning the full collection of tasks jointly via reinforcement, (2) in a zero-shot setting where a policy sketch is available for the held-out task, and (3) in a adaptation setting, where sketches are hidden and the agent must learn a policy over high-level actions. In all cases, our approach substantially outperforms standard policy optimization baselines."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "The agent representation we describe in this paper belongs to the broader family of hierarchical reinforcement learners described in the literature. As detailed in Section 3, our subpolicies may be viewed as a relaxation of the options framework first described by Sutton et al. (1999). A large body of work describes techniques for learning options and related abstract actions, in both single- and multitask settings. For learning the implementation of options, most techniques rely on intermediate supervisory signals, e.g. to encourage exploration (Kearns & Singh, 2002) or completion of predefined subtasks (Kulkarni et al., 2016). An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011). Techniques for learning options with less guidance than the present work include Bacon & Precup (2015) and Vezhnevets et al. (2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al. (2002).\nOnce a library of high-level actions exists, agents are faced with the problem of learning high-level (typically semi-Markov) policies that invoke appropriate high-level actions in sequence (Precup,\n2000). The learning problem we describe in this paper is in some sense the direct dual to the problem of learning these high-level policies. There, the agent begins with an inventory of complex primitives and must learn to model their behavior and select among them; here we begin knowing the names of appropriate high-level actions but nothing about how they are implemented, and must infer implementations (but not, initially, high-level plans) from context. We expect that our approach could be coupled with a generic learner of options policies to provide a general mechanism for hierarchical RL; we leave this for future work.\nOur approach is also inspired by a number of recent efforts toward compositional reasoning and interaction with structured deep models. Such models have been previously used for tasks involving question answering (Iyyer et al., 2014; Andreas et al., 2016) and relational reasoning (Socher et al., 2012), and more recently for multi-task, multi-robot transfer problems (Devin et al., 2016). In this work—as in existing approaches employing dynamically assembled modular networks—taskspecific training signals are propagated through a collection of composed discrete structures with tied weights. Here the composed structures specify time-varying policies rather than feedforward computations, and their parameters must be learned via interaction rather than direct supervision. Another closely related family of models includes neural programmers (Neelakantan et al., 2015) and programmer–interpreters (Reed & de Freitas, 2015), which generate discrete computational structures but require supervision in the form of output actions or full execution traces.\nA closely related line of work is the Hierarchical Abstract Machines (HAM) framework introduced by Parr & Russell (1998). Like our approach, HAMs begin with a representation of a high-level policy as an automaton (or a more general computer program; Andre & Russell, 2001) and use reinforcement learning to fill in low-level details. Variations on this architecture have considered a number of control constructs beyond the scope of the current paper (e.g. concurrency and recursion; Marthi et al., 2004). However, because these approaches attempt to learn a single representation of the Q function for all subtasks and contexts, they require extremely strong formal assumptions about the form of the reward function and state representation (Andre & Russell, 2002) that the present work avoids by decoupling the policy representation from the value function.\nOur approach also bears some resemblance to the instruction following literature in natural language processing. Existing work on instruction following falls into two broad categories: approaches that require a highly structured (typically logical) action and world representations (Chen & Mooney, 2011; Artzi & Zettlemoyer, 2013; Andreas & Klein, 2015; Tellex et al., 2011), and approaches that require detailed supervision of action sequences or dense reward signals essentially equivalent to full action traces (Branavan et al., 2009; Vogel & Jurafsky, 2010; Mei et al., 2016). By contrast, the framework we describe here involves no formal or logical language for describing plans, and no supervised action sequences. Additionally, the modular model described in this paper natrually supports adaptation to tasks where no sketches are available, while all existing instruction following models learn a joint policy over instructions and actions, and are unable to function in the absence of instructions."
    }, {
      "heading" : "3 LEARNING MODULAR POLICIES",
      "text" : "We consider a multitask reinforcement learning problem arising from a family of infinite-horizon discounted Markov decision processes in a shared environment. This environment is specified by a tuple (S,A, P, γ), with S a set of states, A a set of low-level actions, P : S × A × S → R a transition probability distribution, and γ a discount factor. Each task τ ∈ T is then specified by a pair (Rτ , ρτ ), with Rτ : S → R a task-specific reward function and ρτ : S → R an initial distribution over states. For a fixed sequence {(si, ai)} of states and actions obtained from a rollout of a given policy, we will denote the empirical return starting in state si as qi := ∑∞ j=i γ\njR(sj). In addition to the components of a standard multitask RL problem, we assume that tasks are annotated with sketches Kτ , each consisting of a sequence (bτ1, bτ2, . . .) of high-level symbolic labels drawn from a fixed vocabulary B. Our model associates each of these symbols with a randomly initialized modular subpolicy. By sharing each subpolicy across all tasks annotated with the corresponding symbol, our approach naturally learns the shared abstraction for the corresponding subtask, without requiring any information about the grounding of that task to be explicitly specified by annotation."
    }, {
      "heading" : "3.1 MODEL",
      "text" : "We exploit the structural information provided by sketches by constructing for each symbol b a corresponding subpolicy πb. At each timestep, a subpolicy may select either a low-level action a ∈ A or a special STOP action. We denote the augmented state space A+ := A ∪ {STOP}. While this framework is agnostic to the implementation of subpolicies, we are especially interested in the case where subpolicies are specified by deep networks. As shown in Figure 2, the experiments in this paper represent each πb as a neural network whose input is a representation of the current state, and whose output is a distribution over A+. While all action spaces in our experiments are discrete, it is straightforward to instead allow this last layer to parameterize a mixed distribution over an underlying continuous action space and the STOP action. These subpolicies may be viewed as options of the kind described by Sutton et al. (1999), with the key distinction that they have no initiation semantics, but are instead invokable everywhere, and have no explicit representation as a function from an initial state to a distribution over final states (instead implicitly using the STOP action to terminate).\nGiven a sketch, a task-specific policy Πτ is formed by concatenating its associated subpolicies in sequence. In particular, the high-level policy maintains a subpolicy index i (initially 0), and executes actions from πbi until the STOP symbol is emitted, at which point control is passed to πbi+1 . We may thus think of Πτ as inducing a Markov chain over the state space S × B, with transitions given by:\n(s, bi) → (s′, bi) with probability ∑ a∈A πbi(a|s) · P (s′|s, a)\n→ (s, bi+1) with probability πbi(STOP|s) Note that Πτ is semi-Markov with respect to projection of the augmented state space S ×B onto the underlying state space S. We denote the complete family of task-specific policies Π := ⋃ τ{Πτ}, and let each πb be an arbitrary function of the current environment state parameterized by some weight vector θb. The learning problem is to optimize over all θb to maximize the sum of expected discounted rewards J(Π) := ∑ τ J(Πτ ) := ∑ τ Esi∼Πτ [∑ i γ iRτ (si) ] across all tasks τ ∈ T .\n3.2 POLICY OPTIMIZATION\nHere that optimization is accomplished via a simple decoupled actor–critic method. In a standard policy gradient approach, with a single policy π with parameters θ, we compute gradient steps of the form (Williams, 1992):\n∇θJ(π) = ∑ i ( ∇θ log π(ai|si) )( qi − c(s) ) , (1)\nwhere the baseline or “critic” c can be chosen independently of the future without introducing bias into the gradient. Recalling our previous definition of qi as the empirical return starting from si, this form of the gradient corresponds to a generalized advantage estimator (Schulman et al., 2015) with λ = 1. Here c achieves close to the optimal variance (Greensmith et al., 2004) when it is set exactly equal to the state-value function Vπ(si) = Eπqi for the target policy π starting in state si.\nThe situation becomes slightly more complicated when generalizing to modular policies built by sequencing subpolicies. In this case, we will have one subpolicy per symbol but one critic per task. This is because subpolicies πb might participate in a number of composed policies Πτ , each associated with its own reward function Rτ . Thus individual subpolicies are not uniquely identified with value functions, and the aforementioned subpolicy-specific state-value estimator is no longer well-defined. We extend the actor–critic method to incorporate the decoupling of policies from value functions by allowing the critic to vary per-sample (that is, per-task-and-timestep) depending on the reward function with which the sample is associated. Noting that∇θbJ(Π) = ∑ t:b∈Kτ ∇θbJ(Πτ ), i.e. the expected reward across all tasks in which πb participates, we have:\n∇θJ(Π) = ∑ τ ∇θJ(Πτ ) = ∑ τ ∑ i ( ∇θb log πb(aτi|sτi) )( qi − cτ (sτi) ) , (2)\nwhere each state-action pair (sτi, aτi) was selected by the subpolicy πb in the context of the task τ .\nNow minimization of the gradient variance requires that each cτ actually depend on the task identity. (This follows immediately by applying the corresponding argument in Greensmith et al. (2004) individually to each term in the sum over τ in Equation 2.) Because the value function is itself unknown, an approximation must be estimated from data. Here we allow these cτ to be implemented with an arbitrary function approximator parameterized by a vector ητ . This is trained to minimize a squared error criterion, with gradients given by\n∇ητ [ 1\n2 ∑ i (qi − cτ (si))2 ] = ∑ i ( ∇ητ cτ (si) )( qi − cτ (si) ) . (3)\nAlternative forms of the advantage estimator (e.g. the TD residual Rτ (si) + Vτ (si+1) − γVτ (si) or any other member of the GAE family) can be easily substituted by simply maintaining one such estimator per task. Experiments (Section 4.3) show that conditioning on both the state and the task identity results in noticeable performance improvements, suggesting that the variance reduction provided by this objective is important for efficient joint learning of modular policies.\nAlgorithm 1 DO-STEP(Π, curriculum) 1: D ← ∅ 2: while |D| < D do 3: τ ∼ curriculum(·) . sample task τ from curriculum (Section 3.3) 4: d = {(si, ai, bi = Kτ,i, qi, τ), . . .} ∼ Πτ . do rollout 5: D ← D ∪ d\n6: for b ∈ B, τ ∈ T do 7: d = {(si, ai, b′, qi, τ ′) ∈ D : b′ = b, τ ′ = τ} 8: θb ← θb − αD ∑ d ( ∇ log πb(ai|si) )( qi − cτ (si) ) . update policy\n9: ητ ← ητ − βD ∑ d ( ∇cτ (si) )( qi − cτ (si) ) . update critic\nThe complete procedure for computing a single gradient step is given in Algorithm 1. (The outer training loop over these steps, which is driven by a curriculum learning procedure, is described in the following section and specified in Algorithm 2.) This is an on-policy algorithm. In each step, the agent samples tasks from a task distribution provided by a curriculum (described in the following subsection). The current family of policies Π is used to perform rollouts in each sampled task, accumulating the resulting tuples of (states, low-level actions, high-level symbols, rewards, and task identities) into a dataset D. Once D reaches a maximum size D, it is used to compute gradients w.r.t. both policy and critic parameters, and the parameter vectors are updated accordingly. The step sizes α and β in Algorithm 1 can be chosen adaptively using any first-order method."
    }, {
      "heading" : "3.3 CURRICULUM LEARNING",
      "text" : "For complex tasks, like the one depicted in Figure 3b, it is difficult for the agent to discover any states with positive reward until many subpolicy behaviors have already been learned. It is thus a better use of the learner’s time to focus on “easy” tasks, where many rollouts will result in high reward from which appropriate subpolicy behavior can be inferred. But there is a fundamental tradeoff involved here: if the learner spends too much time on easy tasks before being made aware of the existence of harder ones, it may overfit and learn subpolicies that no longer generalize or exhibit the desired structural properties.\nTo avoid both of these problems, we use a curriculum learning scheme (Bengio et al., 2009) that allows the model to smoothly scale up from easy tasks to more difficult ones while avoiding overfitting. Initially the model is presented with tasks associated with short sketches. Once average reward on all these tasks reaches a certain threshold, the length limit is incremented. We assume that rewards across tasks are normalized with maximum achievable reward 0 < qi < 1. Let Êrτ denote the empirical estimate of the expected reward for the current policy on task t. Then at each timestep, tasks are sampled in proportion to 1 − Êrτ , which by assumption must be positive. Experiments show that both components of this curriculum learning scheme improve the rate at which the model converges to a good policy (Section 4.3).\nThe complete curriculum-based training procedure is specified in Algorithm 2. Initially, the maximum sketch length `max is set to one, and the curriculum initialized to sample length-1 tasks uniformly. (Neither of the environments we consider in this paper feature any length-1 tasks; in this case, observe that Algorithm 2 will simply advance to length-2 tasks without any parameter updates.) For each setting of `max, the algorithm uses the current collection of task policies Π to compute and apply the gradient step described in Algorithm 1. The rollouts obtained from the call to DO-STEP can also be used to compute reward estimates Êrτ ; these estimates determine a new task distribution for the curriculum. The inner loop is repeated until the reward threshold rmin is exceeded, at which point `max is incremented and the process repeated over a (now-expanded) collection of tasks."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "As described in the introduction, we evaluate the performance of our approach in two environments: a maze navigation game and a crafting game. Both games involve nontrivial low-level control: agents must learn to avoid obstacles and interact with various kinds of objects. But the environments also feature hierarchical structure: rewards are accessible only after the agent has completed two to five high-level actions in the appropriate sequence.\nIn all our experiments, we implement each subpolicy as a multilayer perceptron with ReLU nonlinearities and a hidden layer with 128 hidden units, and each critic as a linear function of the current state. Each subpolicy network receives as input a set of features describing the current state of the environment, and outputs a distribution over actions. The agent acts at every timestep by sampling from this distribution. The gradient steps given in lines 8 and 9 of Algorithm 1 are implemented using RMSPROP (Tieleman, 2012) with a step size of 0.001 and gradient clipping to a unit norm. We take the batch size parameter D in Algorithm 1 to be 2000, and set γ = 0.9 in both environments. For curriculum learning, the improvement threshold rgood is set to 0.8."
    }, {
      "heading" : "4.1 ENVIRONMENTS",
      "text" : "The maze environment (Figure 3a) corresponds closely to the the “light world” described by Konidaris & Barto (2007). The agent is placed in a discrete world consisting of a series of rooms, some of which are connected by doors. Some doors require that the agent first pick up a key to open them. For our experiments, each task corresponds to a goal room (always at the same position relative to the agent’s starting position) that the agent must reach by navigating through a sequence of intermediate rooms. The agent has one sensor on each side of its body, which reports the distance to keys, closed doors, and open doors in the corresponding direction. Sketches specify a particular sequence of directions for the agent to traverse between rooms to reach the goal. Mazes are sampled with random sizes and random decisions about whether to connect rooms with open doors, locked doors, or no doors. The sketch always corresponds to a viable traversal from the start to the goal position, but other (possibly shorter) traversals may also exist.\nThe crafting environment (Figure 3b) is inspired by the popular game Minecraft, but is implemented in a 2-D grid world. The agent may interact with some objects in the world by facing them\nAlgorithm 2 TRAIN-POLICIES() 1: Π = INIT() . initialize subpolicies randomly 2: `max ← 1 3: loop 4: rmin ←∞ 5: curriculum(·) = Unif(T ′) . initialize `max-step curriculum uniformly 6: T ′ = {τ ∈ T : |Kτ | ≤ `max} 7: while rmin < rgood do 8: DO-STEP(Π, curriculum) . update parameters (Algorithm 1) 9: Z = ∑ t∈T ′ [1− Êrτ ] 10: curriculum(t) = 1[τ ∈ T ′](1− Êrτ )/Z ∀τ ∈ T 11: rmin ← minτ Êrτ 12: `max ← `max + 1\nand executing a special INTERACT action. Interacting with raw materials initially scattered around the environment causes them to be added to an inventory. Interacting with different crafting stations causes objects in the agent’s inventory to be combined or transformed into other objects. Each task in this game corresponds to some crafted object the agent must produce; the most complicated goals require the agent to also craft intermediate ingredients, and in some cases build tools (like a pickaxe and a bridge) to reach ingredients located in initially inaccessible regions of the environment.\nA complete listing of tasks and sketches is given in Appendix A."
    }, {
      "heading" : "4.2 MULTITASK LEARNING",
      "text" : "The primary experimental question in this paper is whether the extra structure provided by policy sketches alone is enough to enable fast learning of coupled policies across tasks. To evaluate this, we compare our modular approach to two policy gradient baselines—one that learns an independent policy for each task and one that learns a joint policy across all tasks—as well as a critic-only Q reader baseline. For the independent model, task-specific policies are represented by networks with the same structure as the modular subpolicies. The joint model conditions both on these environment features, as well as a feature vector encoding the complete sketch. The Q reader forms the same joint state and action space described in Section 3.1, and learns a single feedforward network to map from both environment states and representations of action symbols onto Q values. This baseline can be viewed either as a chain-structured hierarchical abstract machine with a learned state abstractor (Andre & Russell, 2002), or as a standard instruction following baseline from the natural language processing literature (Vogel & Jurafsky, 2010).\nLearning curves for baselines and the modular model are shown in Figure 4. It can be seen that in both the maze domain and the crafting domain, our approach substantially outperforms the baselines: it induces policies with substantially higher average reward and converges more quickly than the policy gradient baselines. It can further be seen in Figure 4c that after policies have been learned on simple tasks, the model is able to rapidly adapt to more complex ones, even when the longer tasks involve high-level actions not required for any of the short tasks (Appendix A).\nHaving demonstrated the overall effectiveness of our approach, our remaining experiments explore (1) the importance of various components of the training procedure, and (2) the learned models’ ability to generalize or adapt to held-out tasks. For compactness, we restrict our consideration on the crafting domain, which features a larger and more diverse range of tasks and high-level actions."
    }, {
      "heading" : "4.3 ABLATIONS",
      "text" : "In addition to the overall modular parameter-tying structure induced by our sketches, the key components of our training procedure are the decoupled critic and the curriculum. Our next experiments investigate the extent to which these are necessary for good performance.\nTo evaluate the the critic, we consider three ablations: (1) removing the dependence of the model on the environment state, in which case the baseline is a single scalar per task; (2) removing the dependence of the model on the task, in which case the baseline is a conventional generalized advantage estimator; and (3) removing both, in which case the baseline is a single scalar, as in a vanilla policy gradient approach. Results are shown in Figure 5a. Introducing both state and task dependence into the baseline leads to faster convergence of the model: the approach with a constant baseline achieves less than half the overall performance of the full critic after 3 million episodes. Introducing task and state dependence independently improve this performance; combining them gives the best result.\nWe also investigate two aspects of our curriculum learning scheme: starting with short examples and moving to long ones, and sampling tasks in inverse proportion to their accumulated reward. Experiments are shown in Figure 5b. We again see that both components are essential for good performance. Sampling uniformly across all tasks of the target length results in slow convergence."
    }, {
      "heading" : "4.4 ZERO-SHOT AND ADAPTATION LEARNING",
      "text" : "In our final experiments, we consider the model’s ability to generalize to new tasks unseen at training time. We consider two evaluation conditions: a zero-shot setting, in which the model is provided a sketch for the new task and must immediately achieve good performance, and a adaptation setting, in which no sketch is provided and the model must learn the form of a suitable sketch by interacting with the new task.\nModel MT 0-S Ad.\nIndependent .44 – <.1 Joint .49 <.1 – Modular .89 .77 .76\nTable 1: Model performance under various evaluation conditions. MT is the multitask training condition described in Section 4.2, while 0-S and Ad. are respectively the zero-shot and adaptation experiments described in Section 4.4.\nWe hold out two length-four tasks from the full inventory used in Section 4.2, and train on the remaining tasks. For zeroshot experiments, we simply form the concatenated policy described by the sketches of the held-out tasks, and repeatedly execute this policy (without learning) in order to obtain an estimate of its effectiveness. For adaptation experiments, we consider ordinary reinforcement learning over B rather than A, implementing the high-level learner with the same agent architecture as described in Section 3.1. Note that the Independent baseline cannot be applied to the zero-shot evaluation, while the joint baseline cannot be applied to the adaptation baseline (because it depends on pre-specified sketch fea-\ntures). Results are shown in Table 1. The held-out tasks are sufficiently challenging that the baselines are unable to obtain more than negligible reward, while the modular model does comparatively well."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We have described an approach for multitask learning of neural network policies guided by symbolic policy sketches. By associating each symbol appearing in a sketch with a modular neural subpolicy, we have shown that it is possible to build agents that share behavior across tasks in order to achieve success in tasks with sparse and delayed rewards. This process induces an inventory of reusable and interpretable subpolicies which can be employed for zero-shot generalization when further sketches are available, and hierarchical reinforcement learning when they are not. Our work suggests that these sketches, which are easy to produce and require no grounding in the environment, provide an effective scaffold for learning hierarchical policies from minimal supervision. We have released our code at http://github.com/jacobandreas/psketch."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "JA is supported by a Facebook Graduate Fellowship and a Huawei / Berkeley AI fellowship."
    }, {
      "heading" : "A TASKS AND SKETCHES",
      "text" : "The complete list of tasks, sketches, and symbols is given below. Tasks marked with an asterisk∗ are held out for the generalization experiments described in Section 4.4, but included in the multitask training experiments in Sections 4.2 and 4.3.\nGoal Sketch\nMaze environment goal1 left left goal2 left down goal3 right down goal4 up left goal5 up right goal6 up right up goal7 down right up goal8 left left down goal9 right down down goal10 left up right\nCrafting environment make plank get wood use toolshed make stick get wood use workbench make cloth get grass use factory make rope get grass use toolshed make bridge get iron get wood use factory make bed∗ get wood use toolshed get grass use workbench make axe∗ get wood use workbench get iron use toolshed make shears get wood use workbench get iron use workbench get gold get iron get wood use factory use bridge get gem get wood use workbench get iron use toolshed use axe"
    } ],
    "references" : [ {
      "title" : "Programmable reinforcement learning agents",
      "author" : [ "David Andre", "Stuart Russell" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Andre and Russell.,? \\Q2001\\E",
      "shortCiteRegEx" : "Andre and Russell.",
      "year" : 2001
    }, {
      "title" : "State abstraction for programmable reinforcement learning agents",
      "author" : [ "David Andre", "Stuart Russell" ],
      "venue" : "In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Andre and Russell.,? \\Q2002\\E",
      "shortCiteRegEx" : "Andre and Russell.",
      "year" : 2002
    }, {
      "title" : "Alignment-based compositional semantics for instruction following",
      "author" : [ "Jacob Andreas", "Dan Klein" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Andreas and Klein.,? \\Q2015\\E",
      "shortCiteRegEx" : "Andreas and Klein.",
      "year" : 2015
    }, {
      "title" : "Learning to compose neural networks for question answering",
      "author" : [ "Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein" ],
      "venue" : "In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Andreas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics",
      "author" : [ "Yoav Artzi", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Artzi and Zettlemoyer.,? \\Q2013\\E",
      "shortCiteRegEx" : "Artzi and Zettlemoyer.",
      "year" : 2013
    }, {
      "title" : "The option-critic architecture",
      "author" : [ "Pierre-Luc Bacon", "Doina Precup" ],
      "venue" : "In NIPS Deep Reinforcement Learning Workshop,",
      "citeRegEx" : "Bacon and Precup.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bacon and Precup.",
      "year" : 2015
    }, {
      "title" : "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization",
      "author" : [ "Bram Bakker", "Jürgen Schmidhuber" ],
      "venue" : "In Proc. of the 8-th Conf. on Intelligent Autonomous Systems,",
      "citeRegEx" : "Bakker and Schmidhuber.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bakker and Schmidhuber.",
      "year" : 2004
    }, {
      "title" : "Reinforcement learning for mapping instructions to actions",
      "author" : [ "S.R.K. Branavan", "Harr Chen", "Luke S. Zettlemoyer", "Regina Barzilay" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Branavan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Branavan et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning to interpret natural language navigation instructions from observations",
      "author" : [ "David L. Chen", "Raymond J. Mooney" ],
      "venue" : "In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Chen and Mooney.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen and Mooney.",
      "year" : 2011
    }, {
      "title" : "Hierarchical relative entropy policy search",
      "author" : [ "Christian Daniel", "Gerhard Neumann", "Jan Peters" ],
      "venue" : "In Proceedings of the International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Daniel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Daniel et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning modular neural network policies for multi-task and multi-robot transfer",
      "author" : [ "Coline Devin", "Abhishek Gupta", "Trevor Darrell", "Pieter Abbeel", "Sergey Levine" ],
      "venue" : "arXiv preprint arXiv:1609.07088,",
      "citeRegEx" : "Devin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Devin et al\\.",
      "year" : 2016
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Greensmith et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Greensmith et al\\.",
      "year" : 2004
    }, {
      "title" : "Using motion primitives in probabilistic sample-based planning for humanoid robots",
      "author" : [ "Kris Hauser", "Timothy Bretl", "Kensuke Harada", "Jean-Claude Latombe" ],
      "venue" : "In Algorithmic foundation of robotics,",
      "citeRegEx" : "Hauser et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hauser et al\\.",
      "year" : 2008
    }, {
      "title" : "Discovering hierarchy in reinforcement learning with HEXQ",
      "author" : [ "Bernhard Hengst" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Hengst.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hengst.",
      "year" : 2002
    }, {
      "title" : "A neural network for factoid question answering over paragraphs",
      "author" : [ "Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daumé III" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2014
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns and Singh.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns and Singh.",
      "year" : 2002
    }, {
      "title" : "Building portable options: Skill transfer in reinforcement learning",
      "author" : [ "George Konidaris", "Andrew G Barto" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Konidaris and Barto.,? \\Q2007\\E",
      "shortCiteRegEx" : "Konidaris and Barto.",
      "year" : 2007
    }, {
      "title" : "Robot learning from demonstration by constructing skill trees",
      "author" : [ "George Konidaris", "Scott Kuindersma", "Roderic Grupen", "Andrew Barto" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "Konidaris et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Konidaris et al\\.",
      "year" : 2011
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum" ],
      "venue" : "arXiv preprint arXiv:1604.06057,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Concurrent hierarchical reinforcement learning",
      "author" : [ "Bhaskara Marthi", "David Lantham", "Carlos Guestrin", "Stuart Russell" ],
      "venue" : "In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Marthi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Marthi et al\\.",
      "year" : 2004
    }, {
      "title" : "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences",
      "author" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew Walter" ],
      "venue" : "In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Mei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "Q-cutdynamic discovery of sub-goals in reinforcement learning",
      "author" : [ "Ishai Menache", "Shie Mannor", "Nahum Shimkin" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "Menache et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Menache et al\\.",
      "year" : 2002
    }, {
      "title" : "Neural programmer: Inducing latent programs with gradient descent",
      "author" : [ "Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever" ],
      "venue" : "arXiv preprint arXiv:1511.04834,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with hierarchies of machines",
      "author" : [ "Ron Parr", "Stuart Russell" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Parr and Russell.,? \\Q1998\\E",
      "shortCiteRegEx" : "Parr and Russell.",
      "year" : 1998
    }, {
      "title" : "Temporal abstraction in reinforcement learning",
      "author" : [ "Doina Precup" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Precup.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup.",
      "year" : 2000
    }, {
      "title" : "Neural programmer-interpreters",
      "author" : [ "Scott Reed", "Nando de Freitas" ],
      "venue" : "Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Reed and Freitas.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reed and Freitas.",
      "year" : 2015
    }, {
      "title" : "Highdimensional continuous control using generalized advantage estimation",
      "author" : [ "John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1506.02438,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher Manning", "Andrew Ng" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning options in reinforcement learning",
      "author" : [ "Martin Stolle", "Doina Precup" ],
      "venue" : "In International Symposium on Abstraction, Reformulation, and Approximation,",
      "citeRegEx" : "Stolle and Precup.,? \\Q2002\\E",
      "shortCiteRegEx" : "Stolle and Precup.",
      "year" : 2002
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Richard S Sutton", "Doina Precup", "Satinder Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Understanding natural language commands for robotic navigation and mobile manipulation",
      "author" : [ "Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R. Walter", "Ashis Gopal Banerjee", "Seth Teller", "Nicholas Roy" ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "Tellex et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tellex et al\\.",
      "year" : 2011
    }, {
      "title" : "Strategic attentive writer for learning macro-actions",
      "author" : [ "Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1606.04695,",
      "citeRegEx" : "Vezhnevets et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vezhnevets et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to follow navigational directions",
      "author" : [ "Adam Vogel", "Dan Jurafsky" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Vogel and Jurafsky.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vogel and Jurafsky.",
      "year" : 2010
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "In these approaches, a high-level controller learns a policy over high-level actions—known variously as options (Sutton et al., 1999), skills",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "(Konidaris & Barto, 2007), or primitives (Hauser et al., 2008)—which are themselves implemented as policies over low-level actions in the environment.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "(2012)) investigates learning hierarchical policies without any supervision, such hierarchies are empirically difficult to learn directly from unconstrained interaction (Hengst, 2002).",
      "startOffset" : 169,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "Daniel et al. (2012)) investigates learning hierarchical policies without any supervision, such hierarchies are empirically difficult to learn directly from unconstrained interaction (Hengst, 2002).",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "The present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016; Reed & de Freitas, 2015).",
      "startOffset" : 150,
      "endOffset" : 197
    }, {
      "referenceID" : 18,
      "context" : "to encourage exploration (Kearns & Singh, 2002) or completion of predefined subtasks (Kulkarni et al., 2016).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011).",
      "startOffset" : 131,
      "endOffset" : 178
    }, {
      "referenceID" : 24,
      "context" : "As detailed in Section 3, our subpolicies may be viewed as a relaxation of the options framework first described by Sutton et al. (1999). A large body of work describes techniques for learning options and related abstract actions, in both single- and multitask settings.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011). Techniques for learning options with less guidance than the present work include Bacon & Precup (2015) and Vezhnevets et al.",
      "startOffset" : 155,
      "endOffset" : 283
    }, {
      "referenceID" : 16,
      "context" : "An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011). Techniques for learning options with less guidance than the present work include Bacon & Precup (2015) and Vezhnevets et al. (2016), and other general hierarchical policy learners include Daniel et al.",
      "startOffset" : 155,
      "endOffset" : 312
    }, {
      "referenceID" : 9,
      "context" : "(2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "(2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al.",
      "startOffset" : 63,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "(2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al. (2002).",
      "startOffset" : 63,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "Such models have been previously used for tasks involving question answering (Iyyer et al., 2014; Andreas et al., 2016) and relational reasoning (Socher et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Such models have been previously used for tasks involving question answering (Iyyer et al., 2014; Andreas et al., 2016) and relational reasoning (Socher et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : ", 2016) and relational reasoning (Socher et al., 2012), and more recently for multi-task, multi-robot transfer problems (Devin et al.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : ", 2012), and more recently for multi-task, multi-robot transfer problems (Devin et al., 2016).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 22,
      "context" : "Another closely related family of models includes neural programmers (Neelakantan et al., 2015) and programmer–interpreters (Reed & de Freitas, 2015), which generate discrete computational structures but require supervision in the form of output actions or full execution traces.",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Variations on this architecture have considered a number of control constructs beyond the scope of the current paper (e.g. concurrency and recursion; Marthi et al., 2004).",
      "startOffset" : 117,
      "endOffset" : 170
    }, {
      "referenceID" : 30,
      "context" : "Existing work on instruction following falls into two broad categories: approaches that require a highly structured (typically logical) action and world representations (Chen & Mooney, 2011; Artzi & Zettlemoyer, 2013; Andreas & Klein, 2015; Tellex et al., 2011), and approaches that require detailed supervision of action sequences or dense reward signals essentially equivalent to full action traces (Branavan et al.",
      "startOffset" : 169,
      "endOffset" : 261
    }, {
      "referenceID" : 7,
      "context" : ", 2011), and approaches that require detailed supervision of action sequences or dense reward signals essentially equivalent to full action traces (Branavan et al., 2009; Vogel & Jurafsky, 2010; Mei et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 212
    }, {
      "referenceID" : 20,
      "context" : ", 2011), and approaches that require detailed supervision of action sequences or dense reward signals essentially equivalent to full action traces (Branavan et al., 2009; Vogel & Jurafsky, 2010; Mei et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 212
    }, {
      "referenceID" : 29,
      "context" : "These subpolicies may be viewed as options of the kind described by Sutton et al. (1999), with the key distinction that they have no initiation semantics, but are instead invokable everywhere, and have no explicit representation as a function from an initial state to a distribution over final states (instead implicitly using the STOP action to terminate).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 33,
      "context" : "In a standard policy gradient approach, with a single policy π with parameters θ, we compute gradient steps of the form (Williams, 1992):",
      "startOffset" : 120,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "Recalling our previous definition of qi as the empirical return starting from si, this form of the gradient corresponds to a generalized advantage estimator (Schulman et al., 2015) with λ = 1.",
      "startOffset" : 157,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "Here c achieves close to the optimal variance (Greensmith et al., 2004) when it is set exactly equal to the state-value function Vπ(si) = Eπqi for the target policy π starting in state si.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "(This follows immediately by applying the corresponding argument in Greensmith et al. (2004) individually to each term in the sum over τ in Equation 2.",
      "startOffset" : 68,
      "endOffset" : 93
    } ],
    "year" : 2016,
    "abstractText" : "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate each task with a sequence of named subtasks, providing high-level structural relationships among tasks, but not providing the detailed guidance required by previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). Our approach associates every subtask with its own modular subpolicy, and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. This optimization is accomplished via a simple decoupled actor–critic training objective that facilitates learning common behaviors from dissimilar reward functions. We evaluate the effectiveness of our approach on a maze navigation game and a 2-D Minecraft-inspired crafting game. Both games feature extremely sparse rewards that can be obtained only after completing a number of high-level subgoals (e.g. escaping from a sequence of locked rooms or collecting and combining various ingredients in the proper order). Experiments illustrate two main advantages of our approach. First, we outperform standard baselines that learn task-specific or shared monolithic policies. Second, our method naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}