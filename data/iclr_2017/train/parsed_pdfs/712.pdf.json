{
  "name" : "712.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A NEURAL STOCHASTIC VOLATILITY MODEL",
    "authors" : [ "Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang" ],
    "emails" : [ "r.luo@cs.ucl.ac.uk,", "j.wang@cs.ucl.ac.uk,", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The volatility of the price movements reflects the ubiquitous uncertainty within financial markets. It is critical that the level of risk, indicated by volatility, is taken into consideration before investment decisions are made and portfolio are optimised (Hull, 2006); volatility is substantially a key variable in the pricing of derivative securities. Hence, estimating and forecasting volatility is of great importance in branches of financial studies, including investment, risk management, security valuation and monetary policy making (Poon & Granger, 2003).\nVolatility is measured typically by using the standard deviation of price change in a fixed time interval, such as a day, a month or a year. The higher the volatility, the riskier the asset. One of the primary challenges in designing volatility models is to identify the existence of latent (stochastic) variables or processes and to characterise the underlying dependences or interactions between variables within a certain time span. A classic approach has been to handcraft the characteristic features of volatility models by imposing assumptions and constraints, given prior knowledge and observations. Notable examples include autoregressive conditional heteroskedasticity (ARCH) model (Engle, 1982) and its generalisation GARCH (Bollerslev, 1986), which makes use of autoregression to capture the properties of time-variant volatility within many time series. Heston (1993) assumed that the volatility follows a Cox-Ingersoll-Ross (CIR) process (Cox et al., 1985) and derived a closed-form solution for options pricing. While theoretically sound, those approaches require strong assumptions which might involve complex probability distributions and non-linear dynamics that drive the process, and in practice, one may have to impose less prior knowledge and rectify a solution under the worst-case volatility case (Avellaneda & Paras, 1996).\nIn this paper, we take a fully data driven approach and determine the configurations with as few exogenous input as possible, or even purely from the historical data. We propose a neural network re-formulation of stochastic volatility by leveraging stochastic models and recurrent neural networks (RNNs). We are inspired by the recent development on variational approaches of stochastic (deep) neural networks (Kingma & Welling, 2013; Rezende et al., 2014) to a recurrent case (Chung et al., 2015; Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014), and our formulation shows that existing volatility models such as the GARCH (Bollerslev, 1986) and the Heston model (Heston, 1993) are the special cases of our neural stochastic volatility formulation. With the hidden latent\nvariables in the neural networks we naturally uncover the underlying stochastic process formulated from the models.\nExperiments with synthetic data and real-world financial data are performed, showing that the proposed model outperforms the widely-used GARCH model on several metrics of the fitness and the accuracy of time series modelling and prediction: it verifies our model’s high flexibility and rich expressive power."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "A notable volatility method is autoregressive conditional heteroskedasticity (ARCH) model (Engle, 1982): it can accurately capture the properties of time-variant volatility within many types of time series. Inspired by ARCH model, a large body of diverse work based on stochastic process for volatility modelling has emerged. Bollerslev (1986) generalised ARCH model to the generalised autoregressive conditional heteroskedasticity (GARCH) model in a manner analogous to the extension from autoregressive (AR) model to autoregressive moving average (ARMA) model by introducing the past conditional variances in the current conditional variance estimation. Engle & Kroner (1995) presented theoretical results on the formulation and estimation of multivariate GARCH model within simultaneous equations systems. The extension to multivariate model allows the covariances to present and depend on the historical information, which are particularly useful in multivariate financial models. Heston (1993) derived a closed-form solution for option pricing with stochastic volatility where the volatility process is a CIR process driven by a latent Wiener process such that the current volatility is no longer a deterministic function even if the historical information is provided. Notably, empirical evidences have confirmed that volatility models provide accurate forecasts (Andersen & Bollerslev, 1998) and models such as ARCH and its descendants/variants have become indispensable tools in asset pricing and risk evaluation.\nOn the other hand, deep learning (LeCun et al., 2015; Schmidhuber, 2015) that utilises nonlinear structures known as deep neural networks, powers various applications. It has triumph over pattern recognition challenges, such as image recognition (Krizhevsky et al., 2012; He et al., 2015; van den Oord et al., 2016), speech recognition (Hinton et al., 2012; Graves et al., 2013; Chorowski et al., 2015), machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few.\nTime-dependent neural networks models include RNNs with advanced neuron structure such as long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), gated recurrent unit (GRU) (Cho et al., 2014), and bidirectional RNN (BRNN) (Schuster & Paliwal, 1997). Recent results show that RNNs excel for sequence modelling and generation in various applications (Graves, 2013; Gregor et al., 2015). However, despite its capability as non-linear universal approximator, one of the drawbacks of neural networks is its deterministic nature. Adding latent variables and their processes into neural networks would easily make the posterori computationally intractable. Recent work shows that efficient inference can be found by variational inference when hidden continuous variables are embedded into the neural networks structure (Kingma & Welling, 2013; Rezende et al., 2014). Some early work has started to explore the use of variational inference to make RNNs stochastic (Chung et al., 2015; Bayer & Osendorfer, 2014; Fabius & van Amersfoort, 2014). Bayer & Osendorfer (2014) and Fabius & van Amersfoort (2014) considered the hidden variables are independent between times, whereas (Fraccaro et al., 2016) utilised a backward propagating inference network according to its Markovian properties. Our work in this paper extends the work (Chung et al., 2015) with a focus on volatility modelling for time series. We assume that the hidden stochastic variables follow a Gaussian autoregression process, which is then used to model both the variance and the mean. We show that the neural network formulation is a general one, which covers two major financial stochastic volatility models as the special cases by defining the specific hidden variables and non-linear transforms."
    }, {
      "heading" : "3 PRELIMINARY: VOLATILITY MODELS",
      "text" : "Stochastic processes are often defined by stochastic differential equations (SDEs), e.g. a (univariate) generalised Wiener process is dxt = µd t + σ dwt, where µ and σ denote the time-invariant rates of drift and standard deviation (square root of variance) while dwt ∼ N (0,d t) is the increment of\nstandard Wiener process at time t. In a small time interval between t and t+∆t, the change in the variable is ∆xt = µ∆t+ σ∆wt. Let ∆t = 1, we obtain the discrete-time version of basic volatility model:\nxt = xt−1 + µ+ σ t, (1)\nwhere t ∼ N (0, 1) is a sample drawn from standard normal distribution. In the multivariate case,Σ represents the covariance matrix in place of σ2. As presumed that the variables are multidimensional, we will useΣ to represent variance in general case except explicitly noted."
    }, {
      "heading" : "3.1 DETERMINISTIC VOLATILITY",
      "text" : "The time-invariant variance Σ can be extended to be a function Σt = Σ(x<t) relying on history of the (observable) underlying stochastic process {x<t}. The current variance Σt is therefore determined given the history {x<t} up to time t. An example of such extensions is the univariate GARCH(1,1) model (Bollerslev, 1986):\nσ2t = α0 + α1(xt−1 − µt−1)2 + β1σ2t−1, (2)\nwhere xt−1 is the observation from N (µt−1, σ2t−1) at time t − 1. Note that the determinism is in a conditional sense, which means that it only holds under the condition that the complete history {x<t} is presented, such as the case of 1-step-ahead forecast. otherwise the current volatility would still be stochastic as it is built on stochastic process {xt}. However, for multi-step-ahead forecast, we usually exploit the relation Et−1[(xt−µt)2] = σ2t to substitute the corresponding terms and calculate the forecasts with longer horizon in a recursive fashion, for example, σ2t+1 = α0 + α1Et−1[(xt − µt) 2] + β1σ 2 t = α0 + (α1 + β1)σ 2 t . For n-step-ahead forecast, there will be n iterations and the procedure is hence also deterministic."
    }, {
      "heading" : "3.2 STOCHASTIC VOLATILITY",
      "text" : "Another extension is applicable for Σt from being conditionally deterministic (i.e. deterministic given the complete history {x<t}) to fully stochastic: Σt = Σ(z≤t) is driven by another latent stochastic process {zt} instead of the observable process {xt}. Heston (1993) model instantiates a continuous-time stochastic volatility model for univariate processes:\ndxt = (µ− 0.5σ2t ) d t+ σt dw 〈1〉 t , (3) dσt = aσt d t+ bdw 〈2〉 t , (4)\nwhere the correlation between dw〈1〉t and dw 〈2〉 t applies: E[dw 〈1〉 t ·dw 〈2〉 t ] = ρd t. We apply Euler’s scheme of quantisation (Stoer & Bulirsch, 2013) to obtain the discrete analogue to the continuoustime Heston model (Eqs. (3) and (4)):\nxt = (xt−1 + µ− 0.5σ2t ) + σ t σt = (1 + a)σt−1 + bzt where [ t zt ] = N (0, [ 1 ρ ρ 1 ] ). (5)"
    }, {
      "heading" : "3.3 VOLATILITY MODEL IN GENERAL",
      "text" : "As discussed above, the observable variable xt follows Gaussian distribution of which the mean and variance depend on the history of observable process {xt} and latent {zt}. We presume in addition that the latent process {zt} is an autoregressive model such that zt is (conditionally) Gaussian distributed. Therefore, we formulate the volatility model in general as:\nzt ∼ N (µz(z<t),Σz(z<t)), (6) xt ∼ N (µx(x<t, z≤t),Σx(x<t, z≤t)), (7)\nwhere µz(x<t, z≤t) and Σz(x<t, z≤t) denote the autoregressive time-varying mean and variance of the latent variable zt while µx(x<t, z≤t) and Σx(x<t, z≤t) represent the mean and variance of observable variable xt, which depend on not only history of the observable process {x<t} but that of the latent process {z≤t}. These two formulas (Eqs. (6) and (7)) abstract the generalised formulation of volatility models. Together, they represents a broad family of volatility models with latent variables, where the Heston\nmodel for stochastic volatility is merely a special case of the family. Furthermore, it will degenerate to deterministic volatility models such as the well-studied GARCH model if we disable the latent process."
    }, {
      "heading" : "4 NEURAL STOCHASTIC VOLATILITY MODELS",
      "text" : "In this section, we establish the neural stochastic volatility model (NSVM) for stochastic volatility estimation and forecast."
    }, {
      "heading" : "4.1 GENERATING OBSERVABLE SEQUENCE",
      "text" : "Recall that the latent variable zt (Eq. (6)) and the observable xt (Eq. (7)) are described by autoregressive models (xt has the exogenous input {z≤t}.) For the distributions of {zt} and {xt}, the following factorisation applies:\npΦ(Z) = ∏ t pΦ(zt|z<t) = ∏ t N (zt;µzΦ(z<t),ΣzΦ(z<t)), (8)\npΦ(X|Z) = ∏ t pΦ(xt|x<t, z≤t) = ∏ t N (xt;µxΦ(x<t, z≤t),ΣxΦ(x<t, z≤t)), (9)\nwhere X = {xt} and Z = {zt} are the sequences of observable and latent variables, respectively, while Φ represents the parameter set of the model. The full generative model is defined as the joint distribution:\npΦ(X,Z) = ∏ t pΦ(xt|x<t, z≤t)pΦ(zt|z<t)\n= ∏ t N (zt;µzΦ(z<t),ΣzΦ(z<t))N (xt;µxΦ(x<t, z≤t),ΣxΦ(x<t, z≤t)). (10)\nIt is observed that the means and variances are conditionally deterministic: given the historical information {z<t}, the current mean µzt = µzΦ(z<t) and varianceΣzt = ΣzΦ(z<t) of zt is obtained and hence the distribution N (zt;µzt ,Σzt ) of zt is specified; after sampling zt from the specified distribution, we incorporate {x<t} and calculate the current meanµxt = µxΦ(x<t, z≤t) and variance Σxt = Σ x Φ(x<t, z≤t) of xt and determine its distribution N (xt;µxt ,Σxt ) of xt. It is natural and convenient to present such a procedure in a recurrent fashion because of its autoregressive nature. As is known that RNNs can essentially approximate arbitrary function of recurrent form (Hammer, 2000), the means and variances, which may be driven by complex non-linear dynamics, can be efficiently computed using RNNs.\nIt is always a good practice to reparameterise the random variables before we go into RNN architecture. As the covariance matrix Σ is symmetric and positive definite, it can be factorised as Σ = UΛU>, where Λ is a full-rank diagonal matrix with positive diagonal elements. Let A = UΛ 1 2 , we have Σ = AA>. Hence we can reparameterise the latent variable zt (Eq. (6)) and observable xt (Eq. (7)):\nzt = µ z t +A z t z t , (11) xt = µ x t +A x t x t , (12)\nwhere Azt (A z t ) > = Σzt ,A x t (A x t ) > = Σxt and x t ∼ N (0, Ix), zt ∼ N (0, Iz) are auxiliary variables. Note that the randomness within the variables of interest (e.g. zt) is extracted by the auxiliary variables (e.g. t) which follow the standard distributions. Hence, the reparameterisation guarantees that gradient-based methods can be applied in learning phase (Kingma & Welling, 2013).\nIn this paper, the joint generative model is comprised of two sets of RNN and multilayer perceptron (MLP): RNNzg/MLP z g for the latent variable, while RNN x g /MLP z g for the observables. We stack these two RNN/MLP together according to the causal dependency between those variables. The\njoint generative model is implemented as the generative network:\n{µzt ,Azt } = MLP z g(h z t ;Φ), (13)\nhzt = RNN z g(h z t−1, zt−1;Φ), (14)\nzt = µ z t +A z t z t , (15)\n{µxt ,Axt } = MLP x g(h x t ;Φ), (16)\nhxt = RNN x g(h x t−1,xt−1, zt;Φ), (17) xt = µ x t +A x t x t , (18)\nwhere hzt and h x t denote the hidden states of the corresponding RNNs. The MLPs map the hidden states of RNNs into the means and deviations of variables of interest. The parameter set Φ is comprised of the weights of RNNs and MLPs.\nOne should notice that when the latent variable z is obtained, e.g. by inference (details in the next subsection), the conditional distribution pΦ(X|Z) (Eq. (9)) will involve in generating the observable xt instead of the joint distribution pΦ(X,Z) (Eq. (10)). This is essentially the scenario of predicting future values of the observable variable given its history. We will use the term “generative model” and will not discriminate the joint generative model or the conditional one as it can be inferred in context."
    }, {
      "heading" : "4.2 INFERENCING THE LATENT PROCESS",
      "text" : "As the generative model involves latent variable zt, of which the true valus are unaccessible even we have observed xt. Hence, the marginal likelihood pΦ(X) becomes the key that bridges the model and the data. The calculation of marginal likelihood involves the posterior distribution pΦ(Z|X), which is often intractable as complex integrals are involved. We are unable to learn the paramters or to infer the latent variables. Therefore, we consider instead a restricted family of tractable distributions qΨ(Z|X), referred to as the approximate posterior family, as approximations to the true posterior pΦ(Z|X) such that the family is sufficiently rich and flexible to provide good approximations (Bishop, 2006; Kingma & Welling, 2013; Rezende et al., 2014).\nWe define the inference model in accordance with the approximate posterior family we have presumed, in a similar fashion as (Chung et al., 2015), where the factorised distribution is formulated as follows:\nqΨ(Z|X) = ∏ t qΨ(zt|z<t,x<t) = ∏ t N (zt; µ̃zΨ(z<t,x<t), Σ̃zΨ(z<t,x<t)), (19)\nwhere µ̃zΨ(z<t,x<t) and Σ̃ z Ψ(z<t,x<t) are functions of the historical information {z<t}, {x<t}, representing the approximated mean and variance of the latent variable zt, respectively. Note that Ψ represents the parameter set of inference model.\nThe inference model essentially describes an autoregressive model on zt with exogenous input xt. Hence, in a similar fashion as the generative model, we implement the inference model as the inference network using RNN/MLP:\n{µ̃zt , Ãzt } = MLP z i (h̃ z t ), (20)\nh̃zt = RNN z i (h̃ z t−1, zt−1,xt−1), (21)\nzt = µ̃ z t + Ã z t ̃ z t , (22)\nwhere Ãzt (Ã z t ) > = Σ̃zt = Σ̃ z Ψ(z<t,x<t) while h̃ z t represents the hidden state of RNN and ̃ z t ∼ N (0, Iz) is an auxiliary variable to extract randomness. The inference mean µ̃zt and deviation Ãzt is computed by an MLP from the hidden state h̃zt . We use the subscript i instead of g to distinguish the architecture used in inference model in contrast to generative model."
    }, {
      "heading" : "4.3 FORECASTING OBSERVATIONS IN FUTURE",
      "text" : "In the realm of time series analysis, we usually pay more attention on forecasting over generating (Box et al., 2015). It means that we are essentially more interested in the generation procedure\nconditioning on the historical information rather than generation purely based on a priori belief since the observations in the past of x<t influences our belief of the latent variable zt. Therefore, we apply the approximate posterior distribution of the latent variable zt (Eq. (19)) as discussed in previous subsection, in place of the prior distribution (Eq. (8)) to build our predictive model.\nGiven the historical observations x<t, the predictive model infers the current value of latent variable zt using inference network and then generates the prediction of the current observation xt using generative network. The procedure of forecasting is shown in Fig. 1.\nNSVM is learned using Stochastic Gradient Variational Bayes following (Kingma & Welling, 2013; Rezende et al., 2014). For readability, we provide the detailed derivation in Appendix A."
    }, {
      "heading" : "4.4 LINKS TO GARCH(1,1) AND HESTON MODEL",
      "text" : "Although we refer to GARCH and Heston as volatility models, the purposes of them are quite different: GARCH is a predictive model used for volatility forecasting whereas Heston is more of a generative model of the underlying dynamics which facilitate closed-form solutions to SDEs in option pricing. The proposed NSVM has close relations to GARCH(1,1) and Heston model: both of them can be regarded as a special case of the neural network formulation. Recall Eq. (2), GARCH(1,1) is formulated as σ2t = α0 + α1(xt−1 − µt−1)2 + β1σ2t−1, where µt−1 is the trend estimate of {xt} at time step t calculated by some mean models. A common practice is to assume that µt follows the ARMA family (Box et al., 2015), or even simpler, as a constant that µt ≡ µ. We adopt the constant trend for simplicity as our focus is on volatility estimation.\nWe define the hidden state as hxt = [µ, σt] >, and disable the latent variable zt ≡ 0 as the volatility modelled by GARCH(1,1) is conditionally deterministic. Hence, we instantiate the generative network (Eqs. (16), (17) and (18)) as follows:\n{µ, σt} = MLPxg(hxt ;Φ) = {[1, 0]hxt , [0, 1]hxt }, (23) hxt = RNN x g(h x t−1, xt−1;Φ)\n= √[ 0 α0 ] + [ 0 α1 ] (xt−1 − [1, 0]hxt−1)2 + [ 1 0 0 β1 ] (hxt−1) 2, (24)\nxt = µ+ σt t where t ∼ N (0, 1). (25)\nThe set of generative parameters is Φ = {µ, α0, α1, β1}. Next, we show the link between NSVM and (discrete-time) Heston model (Eq. (5)). Let hxt = [xt−1, µ, σt] > be the hidden state and zt be i.i.d. standard Gaussian instead of autoregressive vari-\nable, we represent the Heston model in the framework of NSVM as:[ t zt ] = N (0, [ 1 ρ ρ 1 ] ), (26)\n{µt, σt} = MLPxg(hxt ;Φ) = {[1, 1, 0]hxt − [0, 0, 0.5](hxt )2, [0, 0, 1]hxt }, (27) hxt = RNN x g(h x t−1, xt−1, zt;Φ)\n= [ 0 0 0 0 1 0 0 0 1 + a ] hxt−1 + [ 1 0 0 ] xt−1 + [ 0 0 b ] zt, (28)\nxt = µt + σt t. (29)\nThe set of generative parameters is Φ = {µ, a, b}. One should notice that, in practice, the formulation may change in accordance with the specific architecture of neural networks involved in building the model, and hence a closed-form representation may be absent."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In this section, we present our experiments1 both on the synthetic and real-world datasets to validate the effectiveness of NSVM."
    }, {
      "heading" : "5.1 BASELINES AND EVALUATION METRICS",
      "text" : "To evaluate the performance of volatility modelling, we adopt the standard econometric model GARCH(1,1) Bollerslev (1986) as well as its variants EGARCH(1,1) Nelson (1991), GJR-GARCH(1,1,1) Glosten et al. (1993), ARCH(5), TARCH(1,1,1), APARCH(1,1,1), AGARCH(1,1,1), NAGARCH(1,1,1), IGARCH(1,1), IAVGARCH(1,1), FIGARCH(1,d,1) as baselines, which incorporate with the corresponding mean model AR(20). We would also compare our NSVM against a MCMC-based model “stochvol” and the recent Gaussian-processes-based model “GPVOL” Wu et al. (2014), which is a non-parametric model jointly learning the dynamics and hidden states via online inference algorithm. In addition, we setup a naive forecasting model as an alternative baseline referred to as NAIVE, which maintains a sliding window of size 20 on the most recent historical observations and forecasts the current values of mean and volatility by the average mean and variance of the window.\nFor synthetic data experiments, we take four metrics into consideration for performance evaluation: 1) the negative log-likelihood (NLL) of observing the test sequence with respect to the generative model parameters; 2) the mean-squared error (MSE) between the predicted mean and the ground truth (µ-MSE), 3) MSE of the predicted variance against the true variance (σ-MSE); 4) smoothness of fit, which is the standard deviation of the differences of succesive variance estimates. As for the real-world scenarios, the trend and volatility are implicit such that no ground truth is accessible to compare with, we consider only NLL and smoothness as the metrics for evaluation on real-world data experiment."
    }, {
      "heading" : "5.2 MODEL IMPLEMENTATION",
      "text" : "The implementation of NSVM in experiments is in accordance with the architecture illustrated in Fig. 1: it consists of two neural networks, namely inference network and generative network. Each network comprises a set of RNN/MLP as we have discussed above: the RNN is instantiated by stacked LSTM layers whereas the MLP is essentially a 1-layer fully-connected feedforward network which splits into two equal-sized sublayers with different activation functions – one sublayer applies exponential function to impose the non-negativity and prevents overshooting of variance estimates while the other uses linear function to calculate mean estimates. During experiment, the model is structured by cascading the inference network and generative network as depicted in Fig. 1. The input layer is of size 20, which is the same as the embedding dimension DE ; the layer on the\n1Repeatable experiment code: https://github.com/xxj96/nsvm\ninterface of inference network and generative network – we call it latent variable layer – represents the latent variable z, where its dimension is 2. The output layer has the same structure as the input one, therefore the latent variable layer acts as a bottleneck of the entire architecture which helps to extract the key factor. The stacked layers between input layer, latent variable layer and output layer are the hidden layers of either inference network or generative network, it consists of 1 or 2 LSTM layers with size 10, which contains recurrent connection for temporal dependencies modelling.\nState-of-the-art learning techniques have been applied: we introduce Dropout (Zaremba et al., 2014) into each LSTM recurrent layer and impose L2-norm on the weights of each fully-connected feedforward layer as regularistion; NADAM optimiser (Dozat, 2015) is exploited for fast convergence, which is a variant of ADAM optimiser (Kingma & Ba, 2014) incorporated with Nesterov momentum; stepwise exponential learning rate decay is adopted to anneal the variations of convergence as time goes.\nFor econometric models, we utilise several widely-used packages for time series analysis: statsmodels (http://statsmodels.sourceforge.net/), arch (https://pypi.python. org/pypi/arch/3.2), Oxford-MFE-toolbox (https://www.kevinsheppard. com/MFE_Toolbox), stochvol (https://cran.r-project.org/web/packages/ stochvol) and fGarch (https://cran.r-project.org/web/packages/fGarch). The implementation of GPVOL is retrived from http://jmhl.org and we adopt the same hyperparameter setting as in Wu et al. (2014)."
    }, {
      "heading" : "5.3 SYNTHETIC DATA EXPERIMENT",
      "text" : "We build up the synthetic dataset by generating 256 heteroskedastic univariate time series, each with 2000 data points i.e. 2000 time steps. At each time step, the observation is drawn from a Gaussian distribution with pre-determined mean and variance, where the tendency of mean and variance is synthesised as linear combinations of sine functions. Specifically, for the trend and variance, we synthesis each using 3 sine functions with randomly chosen amplitudes and frequencies; then the value of the synthesised signal at each timestep is drawn from a Gaussian distribution with the corresponding value of trend and variance at that timestep. A sampled sequence is shown in Fig. 2a. We expect that this limited dataset could well simulate the real-world scenarios: one usually has very limited chances to observe and collect a large amount of data from time-invariant distributions. In addition, it seems that every observable or latent quantity within time series varies from time to time and seldom repeats the old patterns. Hence, we presume that the tendency shows long-term patterns and the period of tendency is longer than observation. In the experiment, we take the former 1500 time steps as the training set whereas the latter 500 as the test set.\nFor the synthetic data experiment, we simplify the recurrent layers in both inference net and generative net as single LSTM layer of size 10. The actual input {~xt} fed to NSVM is DEdimensional time-delay embedding (Kennel et al., 1992) of raw univariate observation {xt} such that ~xt = [xt+1−DE , . . . , xt]. 2-dimensional latent variable zt is adopted to capture the latent process, and enforces an orthogonal representation of the process by using diagonal covariance matrix. At each time step, 30 samples of latent variable zt are generated via reparameterisation (Eq. (22))."
    }, {
      "heading" : "5.4 REAL-WORLD DATA EXPERIMENT",
      "text" : "We select 162 out of more than 1500 stocks from Chinese stock market and collect the time series of their daily closing prices from 3 institutions in China. We favour those with earlier listing date of trading (from 2006 or earlier) and fewer suspension days (at most 50 suspension days in total during the period of observation) so as to reduce the noise introduced by insufficient observation or missing values, which has significant influences on the performance but is essentially irrelevant to the purpose of volatility forecasting. More specifically, the dataset obtained contains 162 time series, each with 2552 data points (7 years). A sampled sequence is shown in Fig. 2b. We divide the whole dataset into two subsets: the training subset consists of the first 2000 data points while the test subset contains the rest 552 data points.\nSimilar model configuration is applied to the real-world data experiment: time-delay embedding of dimension DE on the raw univariate time series; 2-dimensional latent variable with diagonal\n0 500 1000 1500 2000 timestep\n3\n2\n1\n0\n1\n2\n3\nx\ndata\nµ\nµ+ σ µ− σ\n0 500 1000 1500 2000 timestep\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nv a ri\na n ce\nground truth variance garch's prediction nsvm's prediction\n(a) Synthetic time series prediction. (up) The data and the predicted µx and bounds µx ± σx. (down) The groundtruth data variance and the corresponding prediction from GARCH(1,1) and NSVM.\n0 500 1000 1500 2000 2500 timestep\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nx\ndata\nµ\nµ+ σ µ− σ\n0 500 1000 1500 2000 2500 timestep\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nv a ri\na n ce\ngarch's prediction nsvm's prediction\n(b) Real-world stock price prediction. (up) The data and the predicted µx and bounds µx ± σx. (down) The variance prediction from GARCH(1,1) and NSVM. The prediction of NSVM is more smooth and stable than that of GARCH(1,1), also yielding smaller NLL.\nFigure 2: A case study of time series prediction.\ncovariance matrix; 30 sampling for the latent variable at each time step. Instead of single LSTM layers, here we adopt stacked LSTM layers composed of 2× 10 LSTM cells."
    }, {
      "heading" : "5.5 RESULT AND DISCUSSION",
      "text" : "The overall performance of NSVM and baselines is listed in details in Table 1 and case studies on synthetic data and real-world financial data are illustrated in Fig. 2. The results show that NSVM has higher accuracies for modelling heteroskedastic time series on various metrics: NLL shows the fitness of the model under likelihood measure; the smoothness indicates that NSVM obtains more robust representation of the latent volatility; µ-MSE and σ-MSE in synthetic data experiment imply the ability of recognising the underlying patterns of both trend and volatility, which in fact verifies our claim of NSVM’s high flexibility and rich expressive power for volatility (as well as trend) modelling and forecasting compared with the baselines. Although the improvement comes at the cost of longer training time before convergence, it can be mitigated by applying parallel computing techniques as well as more advanced network architecture or training procedure.\nThe newly proposed NSVM outperforms standard econometric models GARCH(1,1), EGARCH(1,1), GJR-GARCH(1,1,1) and some other variants as well as the MCMC-based model “stochvol” and the recent GP-based model “GPVOL”. Apart from the higher accuracy NSVM obtained, it provides us with the ability to simply generalise univariate time series analysis to multivariate cases by extending network dimensions and manipulating the covariance matrices. Furthermore, it allows us to implement and deploy a similar framework on other applications, for example signal processing and denoising. The shortcoming of NSVM comparing to GPVOL is that the training procedure is offline: for short-term prediction, the experiments have shown the accuracy, but for long-term forecasting, the parameters need retraining, which will be rather time consuming. The online algorithm for inference will be one of the work in the future.\nSpecifically, our NSVM outperforms GARCH(1,1) on 142 out of 162 stocks on the metric of NLL. In particular, NSVM obtains −2.111, −2.044, −2.609 and −1.939 on the stocks corresponding to Fig2(b), Fig 4(a), (b) and (c) respectively, each of which is better than the that of GARCH (0.3433, 0.589, 0.109 and 0.207 lower on NLL)."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, a novel volatility model NSVM has been proposed for stochastic volatility estimation and forecast. We integrated statistical models and RNNs, leveraged the characteristics of each model, organised the dependences between random variables in the form of graphical models, implemented the mappings among variables and parameters through RNNs, and finally established a powerful stochastic recurrent model with universal approximation capability. The proposed architecture comprises a pair of complementary stochastic neural networks: the generative network and inference network. The former models the joint distribution of the stochastic volatility process with both observable and latent variables of interest; the latter provides with the approximate posterior i.e. an analytical approximation to the (intractable) conditional distribution of the latent variables given the observable ones. The parameters (and consequently the underlying distributions) are learned (and inferred) via variational inference, which maximises the lower bound for the marginal log-likelihood of the observable variables. Our NSVM has presented higher accuracy compared to GARCH(1,1), EGARCH(1,1) and GJR-GARCH(1,1,1) as well as GPVOL for volatility modelling and forecasting on synthetic data and real-world financial data. Future work on NSVM would be to incorporate well-established models such as ARMA/ARIMA and to investigate the modelling of seasonal time series and correlated sequences.\nAs we have known, for models that evolve explicitly in terms of the squares of the residuals (e2t = (xt − µt)2), e.g. GARCH, the multi-step-ahead forecasts have closed-form solutions, which means\nthat those forecasts can be efficiently computed in a recursive fashion due to the linear formulation of the model and the exploitation of relation Et−1[e2t ] = σ 2 t .\nOn the other hand, for models that are not linear or do not explicitly evolve in terms of e2, e.g. EGARCH (linear but not evolve in terms of e2), our NSVM (nonlinear and not evolve in terms of e2), the closed-form solutions are absent and thus the analytical forecast is not available. We will instead use simulation-based forecast, which uses random number generator to simulate draws from the predicted distribution and build up a pre-specified number of paths of the variances at 1 step ahead. The draws are then averaged to produce the forecast of the next step. For n-step-ahead forecast, it requires n iterations of 1-step-ahead forecast to get there.\nNSVM is designed as an end-to-end model for volatility estimation and forecast. It takes the price of stocks as input and outputs the distribution of the price at next step. It learns the dynamics using RNN, leading to an implicit, highly nonlinear formulation, where only simulation-based forecast is available. In order to obtain reasonably accurate forecasts, the number of draws should be relatively large, which will be very expensive for computation. Moreover, the number of draws will increase exponentially as the forecast horizon grows, so it will be infeasible to forecast several time steps ahead. We have planned to investigate the characteristics of NSVM’s long-horizontal forecasts and try to design a model specific sampling method for efficient evaluation in the future."
    }, {
      "heading" : "A COMPLEMENTARY DISCUSSIONS OF NSVM",
      "text" : "In this appendix section we present detailed derivations of NSVM, specifically, the parameters learning and calibration, and covariance reparameterisation.\nA.1 LEARNING PARAMETERS / CALIBRATION\nGiven the observationsX , the objective of learning is to maximise the marginal log-likelihood ofX given Φ, where the posterior is involved. However, as we have discussed in the previous subsection, the true posterior is usually intractable, which means exact inference is difficult. Hence, approximate inference is applied instead of rather than exact inference by following (Kingma & Welling, 2013; Rezende et al., 2014). We represent the marginal log-likelihood ofX in the following form:\nln pΦ(X) = EqΨ(Z|X) [ ln pΦ(X,Z)\npΦ(Z|X)\n] = EqΨ(Z|X) [ ln pΦ(X,Z)\nqΨ(Z|X) qΨ(Z|X) pΦ(Z|X) ] = EqΨ(Z|X)[ln pΦ(X,Z)− ln qΨ(Z|X)] +KL[qΨ(Z|X)‖pΦ(Z|X)] ≥ EqΨ(Z|X)[ln pΦ(X,Z)− ln qΨ(Z|X)] (as KL ≥ 0), (30)\nwhere the expectation term EqΨ(Z|X)[ln pΦ(X,Z) − ln qΨ(Z|X)] is referred to as the variational lower bound L[q;X,Φ,Ψ] of the approximate posterior qΨ(Z|X,Ψ). The lower bound is essentially a functional with respect to distribution q and parameterised by observationsX and parameter sets Φ,Ψ of both generative and inference model. In theory, the marginal log-likelihood is maximised by optimisation on the lower bound L[q;X,Φ,Ψ] with respect to Φ and Ψ. We apply the factorisations in Eqs. (10) and (19) to the integrand within expectation of Eq. (30):\nln pΦ(X,Z)− ln qΨ(Z|X) = ∑ t [ lnN (xt;µxΦ(x<t, z≤t),ΣxΦ(x<t, z≤t))\n+ lnN (zt;µzΦ(z<t),ΣzΦ(z<t))− lnN (zt; µ̃zΨ(z<t,x<t), Σ̃zΨ(z<t,x<t)) ] . (31)\nAs there is usually no closed-form solution for the expecation (Eq. (30)), we have to estimate the expectation by applying sampling methods to latent variable zt through time in accordance with the causal dependences. We utilise the reparameterisation of zt as shown in Eq. (22) such that we sample the corresponding auxiliary standard variable ̃t rather than zt itself and compute the value of zt on the fly. This ensures that the gradient-based optimisation techniques are applicable as the reparameterisation isolates the model parameters of interest from the sampling procedure. By sampling N sample paths, the estimator of the lower bound is defined as the average of paths:\nL̂ =− 1 2N ∑ t [ ln detΣzt + (µ̃ z t + Ã z t ̃ z t − µzt )>(Σzt )−1(µ̃zt + Ãzt ̃zt − µzt )\n+ ln detΣxt + (xt − µxt )>(Σxt )−1(xt − µxt )− ln det Σ̃t ] + const, (32)\nwhere Ãzt (Ã z t ) > = Σ̃zt and ̃ z t ∼ N (0, Iz) is parameter-independent and considered as constant when calculating derivatives.\nA.2 COVARIANCE PARAMETERISATION\nAs is known, it entails a computational complexity of O(M3) to maintain and update the full-size covariance Σ with M dimensions (Rezende et al., 2014). In the case of very high dimensions, the full-size covariance matrix would be too computationally expensive to afford. Hence, we use instead the covariance matrices with much fewer parameters for efficiency. The simplest setting is to use diagonal precision matrix (i.e. the inverse of covariance matrix) Σ−1 = D. However, it draws very strong restrictions on representation of the random variable of interest as the diagonal precision matrix (and thus diagonal covariance matrix) indicates independence among the dimensions. Therefore, the tradeoff becomes low-rank perturbation on diagonal matrix: Σ−1 = D + V V >, where V = {v1, . . . ,vK} denotes the perturbation while each vk is a M -dimensional column vector.\nThe corresponding covariance matrix and its determinant is obtained using Woodbury identity and matrix determinant lemma:\nΣ =D−1 −D−1V (I + V >D−1V )−1V >D−1 (33) ln detΣ = − ln det (D + V V >) = − ln detD − ln det (I + V >D−1V ) (34)\nTo calculate the deviation A for the factorisation of covariance matrix Σ = AA>, we first consider the rank-1 perturbation where K = 1. It follows that V = v is a column vector, and I + V >D−1V = 1 + v>D−1v is a real number. A particular solution ofA is obtain:\nA =D− 1 2 − [γ−1(1−√η)]D−1vv>D− 12 (35)\nwhere γ = v>D−1v, η = (1 + γ)−1. The computational complexity involved here is merely O(M).\nObserve that V V > = ∑K\nk=1 vkv > k , the perturbation of rank K is essentially the superposition of\nK perturbations of rank 1. Therefore, we can calculate the deviation A iteratively, an algorithm is provided to demonstrate the procedure of calculation. The computational complexity for rank-K perturbation remains to be O(M) given K M . Algorithm 1 gives the detailed calculation scheme.\nAlgorithm 1 Calculation of rank-K perturbation of precision matrices Input: The original diagonal matrixD; The rank-K perturbation V = {v1, . . . ,vK} Output: A such that the factorisationAA> = Σ = (D + V V >)−1 holds\n1: A(0) =D − 12 2: i = 0 3: while i < K do 4: γ(i) = v > (i)A(i)A > (i)v(i) 5: η(i) = (1 + γ(i)) −1 6: A(i+1) = A(i) − [γ−1(i) (1− √ η(i))]A(i)A > (i)v(i)v > (i)A(i)\n7: A = A(K)"
    }, {
      "heading" : "B MORE CASE STUDIES",
      "text" : "In this appendix section we add more case studies of NVSM performance on both synthetic data and real-world stock data.\nNSVM obtains −2.044, −2.609 and −1.939 on the stocks corresponding to Fig 4(a), (b) and (c) respectively, each of which is better than the that of GARCH (0.589, 0.109 and 0.207 lower on NLL).\nThe reason of the drops in Fig 4(b) and (c) seems to be that NSVM has captured the jumps and drops of the stock price using its nonlinear dynamics and modelled the sudden changes as part of the trend: the estimated trend “mu” goes very close to the real observed price even around the jumps and drops (see the upper figure of Fig 4(b) and (c) around step 1300 and 1600). The residual (i.e. difference between the real value of observation and the trend of prediction) therefore becomes quite small, which lead to a lower volatility estimation.\nOn the other hand, for the baselines, we adopt AR as the trend model, which is a relatively simple linear model compared with the nonlinear NSVM. AR would not capture the sudden changes and leave those spikes in the residual; GARCH then took the residuals as input for volatility modelling, resulting in the spikes in volatility estimation.\n0 500 1000 1500 2000 2500 timestep\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nx\ndata\nµ\nµ+ σ µ− σ\n0 500 1000 1500 2000 2500 timestep\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nv a ri\na n ce\ngarch's prediction nsvm's prediction\n(a) Real-world stock price prediction II. (up) The data and the predicted µx and bounds µx ± σx. (down) The variance prediction from GARCH(1,1) and NSVM. The prediction of NSVM is more smooth and stable than that of GARCH(1,1), also yielding smaller NLL.\n0 500 1000 1500 2000 2500 timestep\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nx\ndata\nµ\nµ+ σ µ− σ\n0 500 1000 1500 2000 2500 timestep\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nv a ri\na n ce\ngarch's prediction nsvm's prediction\n(b) Real-world stock price prediction III. (up) The data and the predicted µx and bounds µx ± σx. (down) The variance prediction from GARCH(1,1) and NSVM. The prediction of NSVM is more smooth and stable than that of GARCH(1,1), also yielding smaller NLL.\n0 500 1000 1500 2000 2500 timestep\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nx\ndata\nµ\nµ+ σ µ− σ\n0 500 1000 1500 2000 2500 timestep\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nv a ri\na n ce\ngarch's prediction nsvm's prediction\n(c) Real-world stock price prediction IV. (up) The data and the predicted µx and bounds µx ± σx. (down) The variance prediction from GARCH(1,1) and NSVM. The prediction of NSVM is more smooth and stable than that of GARCH(1,1), also yielding smaller NLL.\nFigure 4: A case study of real-world stock time series prediction."
    } ],
    "references" : [ {
      "title" : "Answering the skeptics: Yes, standard volatility models do provide accurate forecasts",
      "author" : [ "Torben G Andersen", "Tim Bollerslev" ],
      "venue" : "International economic review,",
      "citeRegEx" : "Andersen and Bollerslev.,? \\Q1998\\E",
      "shortCiteRegEx" : "Andersen and Bollerslev.",
      "year" : 1998
    }, {
      "title" : "Managing the volatility risk of portfolios of derivative securities: the lagrangian uncertain volatility model",
      "author" : [ "Marco Avellaneda", "Antonio Paras" ],
      "venue" : "Applied Mathematical Finance,",
      "citeRegEx" : "Avellaneda and Paras.,? \\Q1996\\E",
      "shortCiteRegEx" : "Avellaneda and Paras.",
      "year" : 1996
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning stochastic recurrent networks",
      "author" : [ "Justin Bayer", "Christian Osendorfer" ],
      "venue" : "arXiv preprint arXiv:1411.7610,",
      "citeRegEx" : "Bayer and Osendorfer.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bayer and Osendorfer.",
      "year" : 2014
    }, {
      "title" : "Pattern recognition",
      "author" : [ "Christopher M Bishop" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bishop.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 2006
    }, {
      "title" : "Generalized autoregressive conditional heteroskedasticity",
      "author" : [ "Tim Bollerslev" ],
      "venue" : "Journal of econometrics,",
      "citeRegEx" : "Bollerslev.,? \\Q1986\\E",
      "shortCiteRegEx" : "Bollerslev.",
      "year" : 1986
    }, {
      "title" : "Time series analysis: forecasting and control",
      "author" : [ "George EP Box", "Gwilym M Jenkins", "Gregory C Reinsel", "Greta M Ljung" ],
      "venue" : null,
      "citeRegEx" : "Box et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Box et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chorowski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "A recurrent latent variable model for sequential data",
      "author" : [ "Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "A theory of the term structure of interest rates",
      "author" : [ "John C Cox", "Jonathan E Ingersoll Jr.", "Stephen A Ross" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Cox et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Cox et al\\.",
      "year" : 1985
    }, {
      "title" : "Incorporating nesterov momentum into adam",
      "author" : [ "Timothy Dozat" ],
      "venue" : null,
      "citeRegEx" : "Dozat.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dozat.",
      "year" : 2015
    }, {
      "title" : "Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation",
      "author" : [ "Robert F Engle" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Engle.,? \\Q1982\\E",
      "shortCiteRegEx" : "Engle.",
      "year" : 1982
    }, {
      "title" : "Multivariate simultaneous generalized arch",
      "author" : [ "Robert F Engle", "Kenneth F Kroner" ],
      "venue" : "Econometric theory,",
      "citeRegEx" : "Engle and Kroner.,? \\Q1995\\E",
      "shortCiteRegEx" : "Engle and Kroner.",
      "year" : 1995
    }, {
      "title" : "Variational recurrent auto-encoders",
      "author" : [ "Otto Fabius", "Joost R van Amersfoort" ],
      "venue" : "arXiv preprint arXiv:1412.6581,",
      "citeRegEx" : "Fabius and Amersfoort.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fabius and Amersfoort.",
      "year" : 2014
    }, {
      "title" : "Sequential neural models with stochastic layers",
      "author" : [ "Marco Fraccaro", "Søren Kaae Sønderby", "Ulrich Paquet", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1605.07571,",
      "citeRegEx" : "Fraccaro et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fraccaro et al\\.",
      "year" : 2016
    }, {
      "title" : "On the relation between the expected value and the volatility of the nominal excess return on stocks",
      "author" : [ "Lawrence R Glosten", "Ravi Jagannathan", "David E Runkle" ],
      "venue" : "The journal of finance,",
      "citeRegEx" : "Glosten et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Glosten et al\\.",
      "year" : 1993
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton" ],
      "venue" : "IEEE international conference on acoustics, speech and signal processing,",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1502.04623,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "On the approximation capability of recurrent neural",
      "author" : [ "Barbara Hammer" ],
      "venue" : "networks. Neurocomputing,",
      "citeRegEx" : "Hammer.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hammer.",
      "year" : 2000
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "A closed-form solution for options with stochastic volatility with applications to bond and currency options",
      "author" : [ "Steven L Heston" ],
      "venue" : "Review of financial studies,",
      "citeRegEx" : "Heston.,? \\Q1993\\E",
      "shortCiteRegEx" : "Heston.",
      "year" : 1993
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Options, futures, and other derivatives",
      "author" : [ "John C Hull" ],
      "venue" : "Pearson Education India,",
      "citeRegEx" : "Hull.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hull.",
      "year" : 2006
    }, {
      "title" : "Determining embedding dimension for phase-space reconstruction using a geometrical construction",
      "author" : [ "Matthew B Kennel", "Reggie Brown", "Henry DI Abarbanel" ],
      "venue" : "Physical review A,",
      "citeRegEx" : "Kennel et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Kennel et al\\.",
      "year" : 1992
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1508.04025,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional heteroskedasticity in asset returns: A new approach",
      "author" : [ "Daniel B Nelson" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Nelson.,? \\Q1991\\E",
      "shortCiteRegEx" : "Nelson.",
      "year" : 1991
    }, {
      "title" : "Forecasting volatility in financial markets: A review",
      "author" : [ "Ser-Huang Poon", "Clive WJ Granger" ],
      "venue" : "Journal of economic literature,",
      "citeRegEx" : "Poon and Granger.,? \\Q2003\\E",
      "shortCiteRegEx" : "Poon and Granger.",
      "year" : 2003
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1401.4082,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Schmidhuber.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2015
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Schuster and Paliwal.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Introduction to numerical analysis, volume 12",
      "author" : [ "Josef Stoer", "Roland Bulirsch" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Stoer and Bulirsch.,? \\Q2013\\E",
      "shortCiteRegEx" : "Stoer and Bulirsch.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1601.06759,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Gaussian process volatility model",
      "author" : [ "Yue Wu", "José Miguel Hernández-Lobato", "Zoubin Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1409.2329,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "It is critical that the level of risk, indicated by volatility, is taken into consideration before investment decisions are made and portfolio are optimised (Hull, 2006); volatility is substantially a key variable in the pricing of derivative securities.",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "Notable examples include autoregressive conditional heteroskedasticity (ARCH) model (Engle, 1982) and its generalisation GARCH (Bollerslev, 1986), which makes use of autoregression to capture the properties of time-variant volatility within many time series.",
      "startOffset" : 84,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "Notable examples include autoregressive conditional heteroskedasticity (ARCH) model (Engle, 1982) and its generalisation GARCH (Bollerslev, 1986), which makes use of autoregression to capture the properties of time-variant volatility within many time series.",
      "startOffset" : 127,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "Heston (1993) assumed that the volatility follows a Cox-Ingersoll-Ross (CIR) process (Cox et al., 1985) and derived a closed-form solution for options pricing.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 33,
      "context" : "We are inspired by the recent development on variational approaches of stochastic (deep) neural networks (Kingma & Welling, 2013; Rezende et al., 2014) to a recurrent case (Chung et al.",
      "startOffset" : 105,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : ", 2014) to a recurrent case (Chung et al., 2015; Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014), and our formulation shows that existing volatility models such as the GARCH (Bollerslev, 1986) and the Heston model (Heston, 1993) are the special cases of our neural stochastic volatility formulation.",
      "startOffset" : 28,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : ", 2015; Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014), and our formulation shows that existing volatility models such as the GARCH (Bollerslev, 1986) and the Heston model (Heston, 1993) are the special cases of our neural stochastic volatility formulation.",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 22,
      "context" : ", 2015; Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014), and our formulation shows that existing volatility models such as the GARCH (Bollerslev, 1986) and the Heston model (Heston, 1993) are the special cases of our neural stochastic volatility formulation.",
      "startOffset" : 182,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "Notable examples include autoregressive conditional heteroskedasticity (ARCH) model (Engle, 1982) and its generalisation GARCH (Bollerslev, 1986), which makes use of autoregression to capture the properties of time-variant volatility within many time series. Heston (1993) assumed that the volatility follows a Cox-Ingersoll-Ross (CIR) process (Cox et al.",
      "startOffset" : 128,
      "endOffset" : 273
    }, {
      "referenceID" : 12,
      "context" : "A notable volatility method is autoregressive conditional heteroskedasticity (ARCH) model (Engle, 1982): it can accurately capture the properties of time-variant volatility within many types of time series.",
      "startOffset" : 90,
      "endOffset" : 103
    }, {
      "referenceID" : 34,
      "context" : "On the other hand, deep learning (LeCun et al., 2015; Schmidhuber, 2015) that utilises nonlinear structures known as deep neural networks, powers various applications.",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : "It has triumph over pattern recognition challenges, such as image recognition (Krizhevsky et al., 2012; He et al., 2015; van den Oord et al., 2016), speech recognition (Hinton et al.",
      "startOffset" : 78,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "It has triumph over pattern recognition challenges, such as image recognition (Krizhevsky et al., 2012; He et al., 2015; van den Oord et al., 2016), speech recognition (Hinton et al.",
      "startOffset" : 78,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : ", 2016), speech recognition (Hinton et al., 2012; Graves et al., 2013; Chorowski et al., 2015), machine translation (Sutskever et al.",
      "startOffset" : 28,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : ", 2016), speech recognition (Hinton et al., 2012; Graves et al., 2013; Chorowski et al., 2015), machine translation (Sutskever et al.",
      "startOffset" : 28,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : ", 2016), speech recognition (Hinton et al., 2012; Graves et al., 2013; Chorowski et al., 2015), machine translation (Sutskever et al.",
      "startOffset" : 28,
      "endOffset" : 94
    }, {
      "referenceID" : 37,
      "context" : ", 2015), machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few.",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : ", 2015), machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few.",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : ", 2015), machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few.",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 30,
      "context" : ", 2015), machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few.",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "Time-dependent neural networks models include RNNs with advanced neuron structure such as long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), gated recurrent unit (GRU) (Cho et al., 2014), and bidirectional RNN (BRNN) (Schuster & Paliwal, 1997).",
      "startOffset" : 181,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "Recent results show that RNNs excel for sequence modelling and generation in various applications (Graves, 2013; Gregor et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Recent results show that RNNs excel for sequence modelling and generation in various applications (Graves, 2013; Gregor et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 133
    }, {
      "referenceID" : 33,
      "context" : "Recent work shows that efficient inference can be found by variational inference when hidden continuous variables are embedded into the neural networks structure (Kingma & Welling, 2013; Rezende et al., 2014).",
      "startOffset" : 162,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "Some early work has started to explore the use of variational inference to make RNNs stochastic (Chung et al., 2015; Bayer & Osendorfer, 2014; Fabius & van Amersfoort, 2014).",
      "startOffset" : 96,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "Bayer & Osendorfer (2014) and Fabius & van Amersfoort (2014) considered the hidden variables are independent between times, whereas (Fraccaro et al., 2016) utilised a backward propagating inference network according to its Markovian properties.",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "Our work in this paper extends the work (Chung et al., 2015) with a focus on volatility modelling for time series.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "Bollerslev (1986) generalised ARCH model to the generalised autoregressive conditional heteroskedasticity (GARCH) model in a manner analogous to the extension from autoregressive (AR) model to autoregressive moving average (ARMA) model by introducing the past conditional variances in the current conditional variance estimation.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "Bollerslev (1986) generalised ARCH model to the generalised autoregressive conditional heteroskedasticity (GARCH) model in a manner analogous to the extension from autoregressive (AR) model to autoregressive moving average (ARMA) model by introducing the past conditional variances in the current conditional variance estimation. Engle & Kroner (1995) presented theoretical results on the formulation and estimation of multivariate GARCH model within simultaneous equations systems.",
      "startOffset" : 0,
      "endOffset" : 352
    }, {
      "referenceID" : 4,
      "context" : "Bollerslev (1986) generalised ARCH model to the generalised autoregressive conditional heteroskedasticity (GARCH) model in a manner analogous to the extension from autoregressive (AR) model to autoregressive moving average (ARMA) model by introducing the past conditional variances in the current conditional variance estimation. Engle & Kroner (1995) presented theoretical results on the formulation and estimation of multivariate GARCH model within simultaneous equations systems. The extension to multivariate model allows the covariances to present and depend on the historical information, which are particularly useful in multivariate financial models. Heston (1993) derived a closed-form solution for option pricing with stochastic volatility where the volatility process is a CIR process driven by a latent Wiener process such that the current volatility is no longer a deterministic function even if the historical information is provided.",
      "startOffset" : 0,
      "endOffset" : 673
    }, {
      "referenceID" : 2,
      "context" : ", 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few. Time-dependent neural networks models include RNNs with advanced neuron structure such as long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), gated recurrent unit (GRU) (Cho et al., 2014), and bidirectional RNN (BRNN) (Schuster & Paliwal, 1997). Recent results show that RNNs excel for sequence modelling and generation in various applications (Graves, 2013; Gregor et al., 2015). However, despite its capability as non-linear universal approximator, one of the drawbacks of neural networks is its deterministic nature. Adding latent variables and their processes into neural networks would easily make the posterori computationally intractable. Recent work shows that efficient inference can be found by variational inference when hidden continuous variables are embedded into the neural networks structure (Kingma & Welling, 2013; Rezende et al., 2014). Some early work has started to explore the use of variational inference to make RNNs stochastic (Chung et al., 2015; Bayer & Osendorfer, 2014; Fabius & van Amersfoort, 2014). Bayer & Osendorfer (2014) and Fabius & van Amersfoort (2014) considered the hidden variables are independent between times, whereas (Fraccaro et al.",
      "startOffset" : 8,
      "endOffset" : 1135
    }, {
      "referenceID" : 2,
      "context" : ", 2014; Bahdanau et al., 2014; Luong et al., 2015) to name a few. Time-dependent neural networks models include RNNs with advanced neuron structure such as long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), gated recurrent unit (GRU) (Cho et al., 2014), and bidirectional RNN (BRNN) (Schuster & Paliwal, 1997). Recent results show that RNNs excel for sequence modelling and generation in various applications (Graves, 2013; Gregor et al., 2015). However, despite its capability as non-linear universal approximator, one of the drawbacks of neural networks is its deterministic nature. Adding latent variables and their processes into neural networks would easily make the posterori computationally intractable. Recent work shows that efficient inference can be found by variational inference when hidden continuous variables are embedded into the neural networks structure (Kingma & Welling, 2013; Rezende et al., 2014). Some early work has started to explore the use of variational inference to make RNNs stochastic (Chung et al., 2015; Bayer & Osendorfer, 2014; Fabius & van Amersfoort, 2014). Bayer & Osendorfer (2014) and Fabius & van Amersfoort (2014) considered the hidden variables are independent between times, whereas (Fraccaro et al.",
      "startOffset" : 8,
      "endOffset" : 1170
    }, {
      "referenceID" : 5,
      "context" : "An example of such extensions is the univariate GARCH(1,1) model (Bollerslev, 1986): σ t = α0 + α1(xt−1 − μt−1) + β1σ t−1, (2) where xt−1 is the observation from N (μt−1, σ t−1) at time t − 1.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "Heston (1993) model instantiates a continuous-time stochastic volatility model for univariate processes: dxt = (μ− 0.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 20,
      "context" : "As is known that RNNs can essentially approximate arbitrary function of recurrent form (Hammer, 2000), the means and variances, which may be driven by complex non-linear dynamics, can be efficiently computed using RNNs.",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Therefore, we consider instead a restricted family of tractable distributions qΨ(Z|X), referred to as the approximate posterior family, as approximations to the true posterior pΦ(Z|X) such that the family is sufficiently rich and flexible to provide good approximations (Bishop, 2006; Kingma & Welling, 2013; Rezende et al., 2014).",
      "startOffset" : 270,
      "endOffset" : 330
    }, {
      "referenceID" : 33,
      "context" : "Therefore, we consider instead a restricted family of tractable distributions qΨ(Z|X), referred to as the approximate posterior family, as approximations to the true posterior pΦ(Z|X) such that the family is sufficiently rich and flexible to provide good approximations (Bishop, 2006; Kingma & Welling, 2013; Rezende et al., 2014).",
      "startOffset" : 270,
      "endOffset" : 330
    }, {
      "referenceID" : 9,
      "context" : "We define the inference model in accordance with the approximate posterior family we have presumed, in a similar fashion as (Chung et al., 2015), where the factorised distribution is formulated as follows: qΨ(Z|X) = ∏",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "In the realm of time series analysis, we usually pay more attention on forecasting over generating (Box et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 33,
      "context" : "NSVM is learned using Stochastic Gradient Variational Bayes following (Kingma & Welling, 2013; Rezende et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "A common practice is to assume that μt follows the ARMA family (Box et al., 2015), or even simpler, as a constant that μt ≡ μ.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "To evaluate the performance of volatility modelling, we adopt the standard econometric model GARCH(1,1) Bollerslev (1986) as well as its variants EGARCH(1,1) Nelson (1991), GJR-GARCH(1,1,1) Glosten et al.",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "To evaluate the performance of volatility modelling, we adopt the standard econometric model GARCH(1,1) Bollerslev (1986) as well as its variants EGARCH(1,1) Nelson (1991), GJR-GARCH(1,1,1) Glosten et al.",
      "startOffset" : 104,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "To evaluate the performance of volatility modelling, we adopt the standard econometric model GARCH(1,1) Bollerslev (1986) as well as its variants EGARCH(1,1) Nelson (1991), GJR-GARCH(1,1,1) Glosten et al. (1993), ARCH(5), TARCH(1,1,1), APARCH(1,1,1), AGARCH(1,1,1), NAGARCH(1,1,1), IGARCH(1,1), IAVGARCH(1,1), FIGARCH(1,d,1) as baselines, which incorporate with the corresponding mean model AR(20).",
      "startOffset" : 104,
      "endOffset" : 212
    }, {
      "referenceID" : 5,
      "context" : "To evaluate the performance of volatility modelling, we adopt the standard econometric model GARCH(1,1) Bollerslev (1986) as well as its variants EGARCH(1,1) Nelson (1991), GJR-GARCH(1,1,1) Glosten et al. (1993), ARCH(5), TARCH(1,1,1), APARCH(1,1,1), AGARCH(1,1,1), NAGARCH(1,1,1), IGARCH(1,1), IAVGARCH(1,1), FIGARCH(1,d,1) as baselines, which incorporate with the corresponding mean model AR(20). We would also compare our NSVM against a MCMC-based model “stochvol” and the recent Gaussian-processes-based model “GPVOL” Wu et al. (2014), which is a non-parametric model jointly learning the dynamics and hidden states via online inference algorithm.",
      "startOffset" : 104,
      "endOffset" : 539
    }, {
      "referenceID" : 40,
      "context" : "State-of-the-art learning techniques have been applied: we introduce Dropout (Zaremba et al., 2014) into each LSTM recurrent layer and impose L2-norm on the weights of each fully-connected feedforward layer as regularistion; NADAM optimiser (Dozat, 2015) is exploited for fast convergence, which is a variant of ADAM optimiser (Kingma & Ba, 2014) incorporated with Nesterov momentum; stepwise exponential learning rate decay is adopted to anneal the variations of convergence as time goes.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : ", 2014) into each LSTM recurrent layer and impose L2-norm on the weights of each fully-connected feedforward layer as regularistion; NADAM optimiser (Dozat, 2015) is exploited for fast convergence, which is a variant of ADAM optimiser (Kingma & Ba, 2014) incorporated with Nesterov momentum; stepwise exponential learning rate decay is adopted to anneal the variations of convergence as time goes.",
      "startOffset" : 149,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : ", 2014) into each LSTM recurrent layer and impose L2-norm on the weights of each fully-connected feedforward layer as regularistion; NADAM optimiser (Dozat, 2015) is exploited for fast convergence, which is a variant of ADAM optimiser (Kingma & Ba, 2014) incorporated with Nesterov momentum; stepwise exponential learning rate decay is adopted to anneal the variations of convergence as time goes. For econometric models, we utilise several widely-used packages for time series analysis: statsmodels (http://statsmodels.sourceforge.net/), arch (https://pypi.python. org/pypi/arch/3.2), Oxford-MFE-toolbox (https://www.kevinsheppard. com/MFE_Toolbox), stochvol (https://cran.r-project.org/web/packages/ stochvol) and fGarch (https://cran.r-project.org/web/packages/fGarch). The implementation of GPVOL is retrived from http://jmhl.org and we adopt the same hyperparameter setting as in Wu et al. (2014).",
      "startOffset" : 150,
      "endOffset" : 902
    }, {
      "referenceID" : 26,
      "context" : "The actual input {~ xt} fed to NSVM is DEdimensional time-delay embedding (Kennel et al., 1992) of raw univariate observation {xt} such that ~ xt = [xt+1−DE , .",
      "startOffset" : 74,
      "endOffset" : 95
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones. Our focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and outperforms standard econometric models GARCH, EGARCH, GJR-GARCH and some other GARCH variants as well as MCMC-based model stochvol and a recent Gaussian processes based volatility model GPVOL on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.",
    "creator" : "LaTeX with hyperref package"
  }
}