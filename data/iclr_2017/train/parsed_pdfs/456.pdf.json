{
  "name" : "456.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DOMAIN-INVARIANT REPRESENTATION LEARNING",
    "authors" : [ "Werner Zellinger", "Edwin Lughofer", "Susanne Saminger-Platz", "Thomas Natschläger" ],
    "emails" : [ "susanne.saminger-platz}@jku.at", "thomas.natschlaeger}@scch.at" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The collection and preprocessing of large amounts of data for new domains is often time consuming and expensive. This in turn limits the application of state-of-the-art methods like deep neural network architectures, that require large amounts of data. However, often data from related domains can be used to improve the prediction model in the new domain. This paper addresses the particularly important and challenging domain-invariant representation learning task of unsupervised domain adaptation (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016). In unsupervised domain adaptation, the training data consists of labeled data from the source domain(s) and unlabeled data from the target domain. In practice, this setting is quite common, as in many applications\n∗http://www.flll.jku.at †http://www.scch.at 1https://github.com/wzell/cmd\nthe collection of input data is cheap, but the collection of labels is expensive. Typical examples include image analysis tasks and sentiment analysis, where labels have to be collected manually.\nRecent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016). The most successful methods have in common that they encourage similarity between the latent network representations w. r. t. the different domains. This similarity is often enforced by minimizing a certain distance between the networks’ domainspecific hidden activations. Three outstanding approaches for the choice of the distance function are the Proxy A-distance (Ben-David et al., 2010), the Kullback-Leibler (KL) divergence Kullback & Leibler (1951), applied to the mean of the activations (Zhuang et al., 2015), and the Maximum Mean Discrepancy (Gretton et al., 2006, MMD).\nTwo of them, the MMD and the KL-divergence approach, can be viewed as the matching of statistical moments. The KL-divergence approach is based on mean (first raw moment) matching. Using the Taylor expansion of the Gaussian kernel, most MMD-based approaches can be viewed as minimizing a certain distance between weighted sums of all raw moments (Li et al., 2015).\nThe interpretation of the KL-divergence approaches and MMD-based approaches as moment matching procedures motivate us to match the higher order moments of the domain-specific activation distributions directly in the hidden activation space. The matching of the higher order moments is performed explicitly for each moment order and each hidden coordinate. Compared to KL-divergencebased approaches, which only match the first moment, our approach also matches higher order moments. In comparison to MMD-based approaches, our method explicitly matches the moments for each order, and it does not require any computationally expensive distance- and kernel matrix computations.\nThe proposed distribution matching method induces a metric between probability distributions. This is possible since distributions on compact intervals have an equivalent representation by means of their moment sequences. We utilize central moments due to their translation invariance and natural geometric interpretation. We call the new metric Central Moment Discrepancy (CMD).\nThe contributions of this paper are as follows:\n• We propose to match the domain-specific hidden representations by explicitly minimizing differences of higher order central moments for each moment order. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, which we call Central Moment Discrepancy (CMD).\n• Probability theoretic analysis is used to prove that CMD is a metric on the set of probability distributions on a compact interval.\n• We additionally prove that convergence of probability distributions on compact intervals w. r. t. to the new metric implies convergence in distribution of the respective random variables. This means that minimizing the CMD metric between probability distributions leads to convergence of the cumulative distribution functions of the random variables.\n• In contrast to MMD-based approaches our method does not require computationally expensive kernel matrix computations.\n• We achieve a new state-of-the-art performance on most domain adaptation tasks of Office and outperform networks trained with MMD, variational fair autoencoders and domain adversarial neural networks on Amazon reviews.\n• A parameter sensitivity analysis shows that CMD is insensitive to parameter changes within a certain interval. Consequently, no additional hyper-parameter search has to be performed."
    }, {
      "heading" : "2 HIDDEN ACTIVATION MATCHING",
      "text" : "We consider the unsupervised domain adaptation setting (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016) with an input space X and a label space Y . Two distributions over X × Y are given: the labeled source domain DS and the unlabeled target domain DT . Two corresponding samples are given: the source sample S = (XS , YS) = {(xi, yi)}ni=1 i.i.d.∼ (DS)n and\nthe target sample T = XT = {xi}mi=1 i.i.d.∼ (DT )m. The goal of the unsupervised domain adaptation setting is to build a classifier f : X → Y with a low target risk RT (f) = Pr (x,y)∼DT (f(x) 6= y), while no information about the labels in DT is given.\nWe focus our studies on neural network classifiers fθ : X → Y with parameters θ ∈ Θ, the input space X = RI with input dimension I , and the label space Y = [0, 1]|C| with the cardinality |C| of the set of classes C. We further assume a network output fθ(x) ∈ [0, 1]|C| of an example x ∈ RI to be normalized by the softmax-function σ : R|C| → [0, 1]|C| with σ(z)j = e\nzj∑|C| k=1 e zk for\nz = {z1, . . . , z|C|}. We focus on bounded activation functions gH : R→ [a, b]N for the hidden layer H with N hidden nodes, e.g. the hyperbolic tangent or the sigmoid function. Unbounded activation functions, e.g. rectified linear units or exponential linear units, can be used if the output is clipped or normalized to be bounded. Using the loss function l : Θ × X × Y → R, e.g. cross-entropy l(θ, x, y) = − ∑ i∈C yi log(fθ(x)i), and the sample set (X,Y ) ⊂ RI × [0, 1]|C|, we define the objective function as min θ∈Θ E(l(θ,X, Y )) (1)\nwhere E denotes the empirical expectation, i.e. E(l(θ,X, Y )) = 1|(X,Y )| ∑\n(x,y)∈(X,Y ) l(θ, x, y). Let us denote the source hidden activations by AH(θ,XS) = gH(θTHAH′(θ,XS)) ⊂ [a, b]N and the target hidden activations by AH(θ,XT ) = gH(θTHAH′(θ,XT )) ⊂ [a, b]N for the hidden layer H with N hidden nodes and parameter θH , and the hidden layer H ′ before H .\nOne fundamental assumption of most unsupervised domain adaptation networks is that the source risk RS(f) is a good indicator for the target risk RT (f), when the domain-specific latent space representations are similar (Ganin et al., 2016). This similarity can be enforced by matching the distributions of the hidden activations AH(θ,XS) and AH(θ,XT ) of higher layers H . Recent stateof-the-art approaches define a domain regularizer d : ([a, b]N )n × ([a, b]N )m → [0,∞), which gives a measure for the domain discrepancy in the activation space [a, b]N . The domain regularizer is added to the objective by means of an additional weighting parameter λ.\nmin θ∈Θ\nE(l(θ,XS , YS)) + λ · d(AH(θ,XS), AH(θ,XT )) (2)\nFig. 1 shows a sketch of the described architecture and fig. 2 shows the hidden activations of a simple neural network optimized by eq. (1) (left) and eq. (2) (right). It can be seen that similar activation distributions are obtained when being optimized on the basis of the domain regularized objective."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "Recently, several measures d for objective (2) have been proposed. One approach is the Proxy Adistance, given by d̂A = 2(1 − 2 ), where is the generalization error on the problem of discriminating between source and target samples (Ben-David et al., 2010). Ganin et al. (2016) compute the\nvalue with a neural network classifier that is simultaneously trained with the original network by means of a gradient reversal layer. They call their approach domain-adversarial neural networks. Unfortunately, a new classifier has to be trained in this approach including the need of new parameters, additional computation times and validation procedures.\nAnother approach is to make use of the MMD (Gretton et al., 2006) as domain regularizer. MMD(X,Y )2 = E(K(X,X))− 2E(K(X,Y )) + E(K(Y, Y )) (3)\nwhere E(K(X,Y )) = 1|X|·|Y | ∑ k∈K(X,Y ) k is the empirical expectation of the kernel products k between all examples in X and Y stored by the kernel matrix K(X,Y ). A suitable choice of the kernel seems to be the Gaussian kernel e−β‖x−y‖ 2\n(Louizos et al., 2016; Li et al., 2015; Tzeng et al., 2014). This approach has two major drawbacks: (a) the need of tuning an additional kernel parameter β, and (b) the need of the kernel matrix computation K(X,Y ) (computational complexity O(n2 + nm+m2)), which becomes inefficient (resource-intensive) in case of large data sets. Concerning (a), the tuning of β is sophisticated since no target samples are available in the domain adaptation setting. Suitable tuning procedures are transfer learning specific cross-validation methods (Zhong et al., 2010). More general methods that don’t utilize source labels include heuristics that are based on kernel space properties (Sriperumbudur et al., 2009; Gretton et al., 2012), combinations of multiple kernels (Li et al., 2015), and kernel choices that maximize the MMD test power (Sutherland et al., 2016). The drawback (b) of the kernel matrix computation can be handled by approximating the MMD (Zhao & Meng, 2015), or by using linear time estimators (Gretton et al., 2012). In this work we focus on the quadratic-time MMD with the Gaussian kernel (Gretton et al., 2012; Tzeng et al., 2014) and transfer learning specific cross-validation for parameter tuning (Zhong et al., 2010; Ganin et al., 2016).\nThe two approaches MMD and the Proxy A-distance have in common that they do not minimize the domain discrepancy explicitly in the hidden activation space. In contrast, the authors in Zhuang et al. (2015) do so by minimizing a modified version of the Kullback-Leibler divergence of the mean activations (MKL). That is, for samples X,Y ⊂ RN ,\nMKL(X,Y ) = N∑ i=1 E(X)i log E(X)i E(Y )i + E(Y )i log E(Y )i E(X)i\n(4)\nwith E(X)i being the ith coordinate of the empirical expectation E(X) = 1|X| ∑ x∈X x. This approach is fast to compute and has an explicit interpretation in the activation space. Our empirical observations (section Experiments) show that minimizing the distance between only the first moment (mean) of the activation distributions can be improved by also minimizing the distance between higher order moments.\nAs noted in the introduction, our approach is motivated by the fact that the MMD and the KLdivergence approach can be seen as the matching of statistical moments of the hidden activations AH(θ,XS) and AH(θ,XT ). In particular, MMD-based approaches that use the Gaussian kernel are equivalent to minimizing a certain distance between weighted sums of all moments of the hidden activation distributions (Li et al., 2015).\nWe propose to minimize differences of higher order central moments of the activations AH(θ,XS) and AH(θ,XT ). The difference minimization is performed explicitly for each moment order. Our\napproach utilizes the equivalent representation of probability distributions in terms of its moment series. We further utilize central moments due to their translation invariance and natural geometric interpretation. Our approach contrasts with other moment-based approaches, as they either match only the first moment (MKL) or they don’t explicitly match the moments for each order (MMD). As a result, our approach improves over MMD-based approaches in terms of computational complexity with O (N(n+m)) for CMD and O ( N(n2 + nm+m2) ) for MMD. In contrast to MKL-based approaches more accurate distribution matching characteristics are obtained. In addition, CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, variational fair autoencoders and domain adversarial neural networks on Amazon reviews."
    }, {
      "heading" : "4 CENTRAL MOMENT DISCREPANCY (CMD)",
      "text" : "In this section we first propose a new distance function CMD on probability distributions on compact intervals. The definition is extended by two theorems that identify CMD as a metric and analyze a convergence property. The final domain regularizer is then defined as an empirical estimate of CMD. The proofs of the theorems are given in the appendix.\nDefinition 1 (CMD metric). Let X = (X1, . . . , Xn) and Y = (Y1, . . . , Yn) be bounded random vectors independent and identically distributed from two probability distributions p and q on the compact interval [a, b]N . The central moment discrepancy metric (CMD) is defined by\nCMD(p, q) = 1\n|b− a| ‖E(X)− E(Y )‖2 + ∞∑ k=2\n1\n|b− a|k ‖ck(X)− ck(Y )‖2 (5)\nwhere E(X) is the expectation of X , and\nck(X) = ( E ( N∏ i=1 (Xi − E(Xi))ri ))\nr1+...+rN=k r1,...,rn≥0\nis the central moment vector of order k.\nThe first order central moments are zero, the second order central moments are related to variance, and the third and fourth order central moments are related to the skewness and the kurtosis of probability distributions. It is easy to see that CMD(p, q) ≥ 0, CMD(p, q) = CMD(q, p), CMD(p, q) ≤ CMD(p, r) + CMD(r, q) and p = q ⇒ CMD(p, q) = 0. The following theorem shows the remaining property for CMD to be a metric on the set of probability distributions on a compact interval.\nTheorem 1. Let p and q be two probability distributions on a compact interval and let CMD be defined as in (5), then\nCMD(p, q) = 0 ⇒ p = q\nOur approach is to minimize the discrepancy between the domain-specific hidden activation distributions by minimizing the CMD. Thus, in the optimization procedure, we increasingly expect to see the domain-specific cumulative distribution functions approach each other. This characteristic can be expressed by the concept of convergence in distribution and it is shown in the following theorem.\nTheorem 2. Let pn and p be probability distributions on a compact interval and let CMD be defined as in (5), then\nCMD(pn, p)→ 0 ⇒ pn d−→ p\nwhere d−→ denotes convergence in distribution.\nWe define the final central moment discrepancy regularizer as an empirical estimate of the CMD metric. Only the central moments that correspond to the marginal distributions are computed. The number of central moments is limited by a new parameter K and the expectation is sampled by the empirical expectation.\nDefinition 2 (CMD regularizer). Let X and Y be bounded random samples with respective probability distributions p and q on the interval [a, b]N . The central moment discrepancy regularizer CMDK is defined as an empirical estimate of the CMD metric, by\nCMDK(X,Y ) = 1\n|b− a| ‖E(X)−E(Y )‖2 + K∑ k=2\n1\n|b− a|k ‖Ck(X)− Ck(Y )‖2 (6)\nwhere E(X) = 1|X| ∑ x∈X x is the empirical expectation vector computed on the sample X and Ck(X) = E((x−E(X))k) is the vector of all kth order sample central moments of the coordinates of X .\nThis definition includes three approximation steps: (a) the computation of only marginal central moments, (b) the bound on the order of central moment terms via parameter K, and (c) the sampling of the probability distributions by the replacement of the expected value with the empirical expectation.\nApplying approximation (a) and assuming independent marginal distributions, a zero CMD distance value still implies equal joint distributions (thm. 1) but convergence in distribution (thm. 2) applies only to the marginals. In the case of dependent marginal distributions, zero CMD distance implies equal marginals and convergence in CMD implies convergence in distribution of the marginals. However, the matching properties for the joint distributions are not obtained with dependent marginals and approximation (a). The computational complexity is reduced to be linear w. r. t. the number of samples.\nConcerning (b), proposition 1 shows that the marginal distribution specific CMD terms have an upper bound that is strictly decreasing with increasing moment order. This bound is convergent to zero. That is, higher CMD terms can contribute less to the overall distance value. This observation is experimentally strengthened in subsection Parameter Sensitivity. Proposition 1. Let X and Y be bounded random vectors with respective probability distributions p and q on the compact interval [a, b]N . Then\n1\n|b− a|k ‖ck(X)− ck(Y )‖2 ≤ 2\n√ N\n( 1\nk + 1\n( k\nk + 1\n)k + 1\n21+k\n) (7)\nwhere ck(X) = E((X − E(X))k) is the vector of all kth order sample central moments of the marginal distributions of p.\nConcerning approximation (c), the joint application of the weak law of large numbers (Billingsley, 2008) with the continuous mapping theorem (Billingsley, 2013) proves that this approximation creates a consistent estimate.\nWe would like to underline that the training of neural networks with eq. (2) and the CMD regularizer in eq. (6) can be easily realized by gradient descent algorithms. The gradients of the CMD regularizer are simple aggregations of derivatives of the standard functions gH , xk and ‖.‖2."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "Our experimental evaluations are based on two benchmark datasets for domain adaptation, Amazon reviews and Office, described in subsection Datasets. The experimental setup is discussed in subsection Experimental Setup and our classification accuracy results are discussed in subsection Results. Subsection Parameter Sensitivity analysis the accuracy sensitivity w. r. t. parameter changes of K for CMD and β for MMD."
    }, {
      "heading" : "5.1 DATASETS",
      "text" : "Amazon reviews: For our first experiment we use the Amazon reviews data set with the same preprocessing as used by Chen et al. (2012); Ganin et al. (2016); Louizos et al. (2016). The data set contains product reviews of four different product categories: books, DVDs, kitchen appliances and electronics. Reviews are encoded in 5000 dimensional feature vectors of bag-of-words unigrams and bigrams with binary labels: 0 if the product is ranked by 1 − 3 stars and 1 if the product is ranked\nby 4 or 5 stars. From the four categories we obtain twelve domain adaptation tasks (each category serves once as source category and once as target category).\nOffice: The second experiment is based on the computer vision classification data set from Saenko et al. (2010) with images from three distinct domains: amazon (A), webcam (W) and dslr (D). This data set is a de facto standard for domain adaptation algorithms in computer vision. Amazon, the largest domain, is a composition of 2817 images and its corresponding 31 classes. Following previous works we assess the performance of our method across all six possible transfer tasks."
    }, {
      "heading" : "5.2 EXPERIMENTAL SETUP",
      "text" : "Amazon Reviews:\nFor the Amazon reviews experiment, we use the same data splits as previous works for every task. Thus we have 2000 labeled source examples and 2000 unlabeled target examples for training, and between 3000 and 6000 examples for testing.\nWe use a similar architecture as Ganin et al. (2016) with one dense hidden layer with 50 hidden nodes, sigmoid activation functions and softmax output function. Three neural networks are trained by means of eq. (2): (a) a base model without domain regularization (λ = 0), (b) with the MMD as domain regularizer and (c) with CMD as domain regularizer. These models are additionally compared with the state-of-the-art models VFAE (Louizos et al., 2016) and DANN (Ganin et al., 2016). The models (a),(b) and (c) are trained with similar setup as in Louizos et al. (2016) and Ganin et al. (2016).\nFor the CMD regularizer, the λ parameter of eq. (2) is set to 1, i.e. the weighting parameter λ is neglected. The parameter K is heuristically set to five, as the first five moments capture rich geometric information about the shape of a distribution and K = 5 is small enough to be computationally efficient. However, the experiments in subsection Parameter Sensitivity show that similar results are obtained for K ≥ 3. For the MMD regularizer we use the Gaussian kernel with parameter β. We performed a hyperparameter search for β and λ, which has to be performed in an unsupervised way (no labels in the target domain). We use a variant of the reverse cross-validation approach proposed by Zhong et al. (2010), in which we initialize the model weights of the reverse classifier by the weights of the first learned classifier (see Ganin et al. (2016) for details). Thereby, the parameter λ is tuned on 10 values between 0.1 and 500 on a logarithmic scale. The parameter β is tuned on 10 values between 0.01 and 10 on a logarithmic scale. Without this parameter search, no competitive prediction accuracy results could be obtained.\nSince we have to deal with sparse data, we rely on the Adagrad optimizer (Duchi et al., 2011). For all evaluations, the default parametrization is used as implemented in Keras (Chollet, 2015). All evaluations are repeated 10 times based on different shuffles of the data, and the mean accuracies and standard deviations are analyzed.\nOffice: Since the office dataset is rather small with only 2817 images in its largest domain, we use the latent representations of the convolution neural network VGG16 of Simonyan & Zisserman (2014). In particular we train a classifier with one hidden layer, 256 hidden nodes and sigmoid activation function on top of the output of the first dense layer in the network. We again train one base model without domain regularization and a CMD regularized version with K = 5 and λ = 1.\nWe follow the standard training protocol for this data set and use all available source and target examples during training. Using this ”fully-transductive” protocol, we compare our method with other state-of-the-art approaches including DLID (Chopra et al., 2013), DDC (Tzeng et al., 2014), DAN (Long et al., 2015), Deep CORAL (Sun & Saenko, 2016), and DANN (Ganin et al., 2016), based on fine-tuning of the baseline model AlexNet (Krizhevsky et al., 2012). We further compare our method to LSSA (Aljundi et al., 2015), CORAL (Sun et al., 2016), and AdaBN (Li et al., 2016), based on the fine-tuning of InceptionBN (Ioffe & Szegedy, 2015).\nAs an alternative to Adagrad for non-sparse data, we use the Adadelta optimizer from Zeiler (2012). Again, the default parametrization from Keras is used. We handle unbalances between source and target sample by randomly down-sampling (up-sampling) the source sample. In addition, we ensure a sub-sampled source batch that is balanced w. r. t. the class labels.\nSince all hyper-parameters are set a-priori, no hyper-parameter search has to be performed.\nAll experiments are repeated 10 times with randomly shuffled data sets and random initializations."
    }, {
      "heading" : "5.3 RESULTS",
      "text" : "Amazon Reviews: Table 1 shows the classification accuracies of four models: The Source Only model is the non domain regularized neural network trained with objective (1), and serves as a base model for the domain adaptation improvements. The models MMD and CMD are trained with the same architecture and objective (2) with d as the domain regularizer MMD and CMD, respectively. VFAE refers to the Variational Fair Autoencoder of Louizos et al. (2016), including a slightly modified version of the MMD regularizer for faster computations, and DANN refers to the domainadversarial neural networks model of Ganin et al. (2016). The last two columns are taken directly from these publications.\nAs one can observe in table 1, our accuracy of the CMD-based model is the highest in 9 out of 12 domain adaptation tasks, whereas on the remaining 3 it is the second best method. However, the difference in accuracy compared to the best method is smaller than the standard deviation over all data shuffles.\nOffice: Table 2 shows the classification accuracy of different models trained on the Office dataset. Note that some of the methods (LSSA, CORAL and AdaBN) are evaluated based on the InceptionBN model, which shows higher accuracy than the base model (VGG16) of our method in most tasks. However, our method outperforms related state-of-the-art methods on all except two tasks, on which it performs similar. We improve the previous state-of-the-art method AdaBN (Li et al., 2016) by more than 3.2% in average accuracy."
    }, {
      "heading" : "5.4 PARAMETER SENSITIVITY",
      "text" : "The first sensitivity experiment aims at providing evidence regarding the accuracy sensitivity of the CMD regularizer w. r. t. parameter changes of K. That is, the contribution of higher terms in the CMD regularizer are analyzed. The claim is that the accuracy of CMD-based networks does not depend strongly on the choice of K in a range around its default value 5.\nIn fig. 3 on the upper left we analyze the classification accuracy of a CMD-based network trained on all tasks of the Amazon reviews experiment. We perform a grid search for the two regularization hyper-parameters λ andK. We empirically choose a representative stable region for each parameter, [0.3, 3] for λ and {1, . . . , 7} for K. Since we want to analyze the sensitivity w. r. t. K, we averaged over the λ-dimension, resulting in one accuracy value per K for each of the 12 tasks. Each accuracy is transformed into an accuracy ratio value by dividing it with the accuracy of K = 5. Thus, for each K and task we get one value representing the ratio between the obtained accuracy (for this K and task) and the accuracy of K = 5. The results are shown in fig. 3 (upper left). The accuracy\nratios between K = 5 and K ∈ {3, 4, 6, 7} are lower than 0.5%, which underpins the claim that the accuracy of CMD-based networks does not depend strongly on the choice of K in a range around its default value 5. For K = 1 and K = 2 higher ratio values are obtained. In addition, for these two values many tasks show worse accuracy than obtained by K ∈ {3, 4, 5, 6, 7}. From this we additionally conclude that higher values of K are preferable to K = 1 and K = 2.\nThe same experimental procedure is performed with MMD regularization wighted by λ ∈ [5, 45] and Gaussian kernel parameter β ∈ [0.3, 1.7]. We calculate the ratio values w. r. t. the accuracy of β = 1.2, since this value of β shows the highest mean accuracy of all tasks. Fig. 3 (upper right) shows the results. It can be seen that the accuracy of the MMD network is more sensitive to parameter changes than the CMD regularized version. Note that the problem of finding the best settings for the parameter β of the Gaussian kernel is a well known problem (Hsu et al., 2003).\nThe default number of hidden nodes in all our experiments is 256 because of the high classification accuracy of the networks without domain regularization (Source Only) on the source domains. The question arises if the accuracy of the CMD is lower for higher numbers of hidden nodes. That is, if the accuracy ratio between the accuracy, of the CMD regularized networks compared to the accuracy of the Source Only models, decreases with increasing hidden activation dimension. In order to answer this question we calculate these ratio values for each task of the Amazon reviews data set for different number of hidden nodes (128, 256, 384, . . . , 1664). For higher numbers of hidden nodes our Source Only models don’t converge with the optimization settings under consideration. For the parameters λ and K we use our default setting λ = 1 and K = 5. Fig. 3 on the lower left shows the ratio values (vertical axis) for every number of hidden nodes (horizontal axis) and every task (colored lines). It can be seen that the accuracy improvement of the CMD domain regularizer varies between 4% and 6%. However, no accuracy ratio decrease can be observed.\nPlease note that we use a default setting for K and λ. Thus, fig. 3 shows that our default setting (λ = 1,K = 5) can be used independently of the number of hidden nodes. This is an additional result.\nThe same procedure is performed with the MMD weighted by parameter λ = 9 and β = 1.2 as these values show the highest classification accuracy for 256 hidden nodes. Fig. 3 on the lower right shows that the accuracy improvement using the MMD decreases with increasing number of hidden nodes for this parameter setting. That is, for accurate performance of the MMD, additional parameter tuning procedures for λ and β need to be performed. Note that the problem of finding the best setting for the parameter β of the Gaussian kernel is a well known problem (Hsu et al., 2003)."
    }, {
      "heading" : "6 CONCLUSION AND OUTLOOK",
      "text" : "In this paper we proposed the central moment discrepancy (CMD) for domain-invariant representation learning, a distance function between probability distributions. Similar to other state-of-the-art approaches (MMD, KL-divergence, Proxy A-distance), the CMD function can be used to minimize the domain discrepancy of latent feature representations. This is achieved by order-wise differences\nof central moments. By using probability theoretic analysis, we proved that CMD is a metric and that convergence in CMD implies convergence in distribution for probability distributions on compact intervals. Our method yields state-of-the-art performance on most tasks of the Office benchmark data set and outperforms Gaussian kernel based MMD, VFAE and DANN on most tasks of the Amazon reviews benchmark data set. These results are achieved with the default parameter setting of K = 5. In addition, we experimentally underpinned the claim that the classification accuracy is not sensitive to the particular choice of K for K ≥ 3. Therefore, no computationally expensive hyper-parameter selection is required.\nIn our experimental analysis we compared our approach to different other state-of-the-art distribution matching methods like the Maximum Mean Discrepancy (MMD) based on the Gaussian kernel using a quadratic time estimate. In the future we want to extend our experimental analysis to other MMD approaches including other kernels, parameter selection procedures and linear time estimators. In addition, we plan to use the CMD for training generative models and to further investigate the approximation quality of the proposed empirical estimate."
    }, {
      "heading" : "A THEOREM PROOFS",
      "text" : "Theorem 1. Let p and q be two probability distributions on a compact interval and let CMD be defined as in (5), then\nCMD(p, q) = 0 ⇒ p = q\nProof. Let X and Y be two random vectors that have probability distributions p and q, respectively. Let X̂ = X − E(X) and Ŷ = Y − E(Y ) be the mean centered random variables. From CMD(p, q) = 0 it follows that all moments of the bounded random variables X̂ and Ŷ are equal. Therefore, the joint moment generating functions of X̂ and Ŷ are equal. Using the property that p\nand q have compact support, we obtain the equality of the joint distribution functions of X̂ and Ŷ . Since E(X) = E(Y ), it follows that X = Y .\nTheorem 2. Let pn and p be probability distributions on a compact interval and let CMD be defined as in (5), then\nCMD(pn, p)→ 0 ⇒ pn d−→ p\nwhere d−→ denotes convergence in distribution.\nProof. Let Xn and X be random vectors that have probability distributions pn and p respectively. Let X̂ = X − E(X) and X̂n = Xn − E(Xn) be the mean centered random variables. From CMD(Xn, X) → 0 it follows that the moments of X̂n converge to the moments of X̂ . Therefore, the joint moment generating functions of X̂n converge to the joint moment generating function of X̂ , which implies convergence in distribution of the mean centered random variables. Using E(Xn)→ E(X) we obtain pn d−→ p.\nProposition 1. Let X and Y be bounded random vectors with respective probability distributions p and q on the compact interval [a, b]N . Then\n1\n|b− a|k ‖ck(X)− ck(Y )‖2 ≤ 2\n√ N\n( 1\nk + 1\n( k\nk + 1\n)k + 1\n21+k\n) (8)\nwhere ck(X) = E((X − E(X))k) is the vector of all kth order sample central moments of the marginal distributions of p.\nProof. Let X ([a, b]) be the set of all random variables with values in [a, b]. Then it follows that 1\n|b− a|k ‖ck(X)− ck(Y )‖2 = ∥∥∥∥ ck(X)|b− a|k − ck(Y )|b− a|k ∥∥∥∥\n2 ≤ ∥∥∥∥ ck(X)|b− a|k ∥∥∥∥ 2 + ∥∥∥∥ ck(Y )|b− a|k ∥∥∥∥ 2\n= ∥∥∥∥∥E (( X − E(X) |b− a| )k)∥∥∥∥∥ 2 + ∥∥∥∥∥E (( Y − E(Y ) |b− a| )k)∥∥∥∥∥ 2\n≤ ∥∥∥∥∥E (∣∣∣∣X − E(X)b− a ∣∣∣∣k )∥∥∥∥∥\n2\n+ ∥∥∥∥∥E (∣∣∣∣Y − E(Y )b− a ∣∣∣∣k )∥∥∥∥∥\n2\n≤ 2 √ N sup\nX∈X ([a,b]) E (∣∣∣∣X − E(X)b− a ∣∣∣∣k )\nThe latter term refers to the absolute central moment of order k, for which the smallest upper bound is known (Egozcue et al., 2012):\n1\n|b− a|k ‖ck(X)− ck(Y )‖2 ≤ 2\n√ N sup\nx∈[0,1] x(1− x)k + (1− x)xk\nEgozcue et al. (2012) also give a more explicit bound:\n1\n|b− a|k ‖ck(X)− ck(Y )‖2 ≤ 2\n√ N\n( 1\nk + 1\n( k\nk + 1\n)k + 1\n21+k\n)"
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "The research reported in this paper has been supported by the Austrian Ministry for Transport, Innovation and Technology, the Federal Ministry of Science, Research and Economy, and the Province of Upper Austria in the frame of the COMET center SCCH.\nWe would like to thank Bernhard Moser and Florian Sobieczky for fruitful discussions on metric spaces."
    } ],
    "references" : [ {
      "title" : "Landmarks-based kernelized subspace alignment for unsupervised domain adaptation",
      "author" : [ "Rahaf Aljundi", "Rémi Emonet", "Damien Muselet", "Marc Sebban" ],
      "venue" : "In International Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Aljundi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Aljundi et al\\.",
      "year" : 2015
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2010
    }, {
      "title" : "Probability and measure",
      "author" : [ "Patrick Billingsley" ],
      "venue" : null,
      "citeRegEx" : "Billingsley.,? \\Q2008\\E",
      "shortCiteRegEx" : "Billingsley.",
      "year" : 2008
    }, {
      "title" : "Convergence of probability measures",
      "author" : [ "Patrick Billingsley" ],
      "venue" : null,
      "citeRegEx" : "Billingsley.,? \\Q2013\\E",
      "shortCiteRegEx" : "Billingsley.",
      "year" : 2013
    }, {
      "title" : "Marginalized denoising autoencoders for domain adaptation",
      "author" : [ "Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Keras: Deep learning library for theano and tensorflow",
      "author" : [ "François Chollet" ],
      "venue" : null,
      "citeRegEx" : "Chollet.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2015
    }, {
      "title" : "Dlid: Deep learning for domain adaptation by interpolating between domains",
      "author" : [ "Sumit Chopra", "Suhrid Balakrishnan", "Raghuraman Gopalan" ],
      "venue" : "International Conference on Machine Learning Workshop on Challenges in Representation Learning,",
      "citeRegEx" : "Chopra et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "The smallest upper bound for the pth absolute central moment of a class of random variables",
      "author" : [ "Martin Egozcue", "Luis Fuentes Garcı́a", "Wing Keung Wong", "Ricardas Zitikis" ],
      "venue" : "The Mathematical Scientist,",
      "citeRegEx" : "Egozcue et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Egozcue et al\\.",
      "year" : 2012
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ganin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "A kernel method for the two-sample-problem",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte Rasch", "Bernhard Schölkopf", "Alex J Smola" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2006
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "A practical guide to support vector classification",
      "author" : [ "Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : null,
      "citeRegEx" : "Hsu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2003
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "Solomon Kullback", "Richard A Leibler" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "Kullback and Leibler.,? \\Q1951\\E",
      "shortCiteRegEx" : "Kullback and Leibler.",
      "year" : 1951
    }, {
      "title" : "Revisiting batch normalization for practical domain adaptation",
      "author" : [ "Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou" ],
      "venue" : "arXiv preprint arXiv:1603.04779,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised domain adaptation by domain invariant projection",
      "author" : [ "Yujia Li", "Kevin Swersky", "Richard Zemel" ],
      "venue" : "In Neural Information Processing Systems Workshop on Transfer and Multitask Learning,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Generative moment matching networks",
      "author" : [ "Yujia Li", "Kevin Swersky", "Richard Zemel" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning transferable features with deep adaptation networks",
      "author" : [ "Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael Jordan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "The variational fair auto encoder",
      "author" : [ "Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "Louizos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Louizos et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain adaptation via transfer component analysis",
      "author" : [ "Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Pan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2011
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Saenko et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Saenko et al\\.",
      "year" : 2010
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Kernel choice and classifiability for rkhs embeddings of probability distributions",
      "author" : [ "Bharath K Sriperumbudur", "Kenji Fukumizu", "Arthur Gretton", "Gert RG Lanckriet", "Bernhard Schölkopf" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sriperumbudur et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sriperumbudur et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep coral: Correlation alignment for deep domain adaptation",
      "author" : [ "Baochen Sun", "Kate Saenko" ],
      "venue" : "arXiv preprint arXiv:1607.01719,",
      "citeRegEx" : "Sun and Saenko.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sun and Saenko.",
      "year" : 2016
    }, {
      "title" : "Return of frustratingly easy domain adaptation",
      "author" : [ "Baochen Sun", "Jiashi Feng", "Kate Saenko" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Sun et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative models and model criticism via optimized maximum mean discrepancy",
      "author" : [ "Dougal J Sutherland", "Hsiao-Yu Tung", "Heiko Strathmann", "Soumyajit De", "Aaditya Ramdas", "Alex Smola", "Arthur Gretton" ],
      "venue" : "arXiv preprint arXiv:1611.04488,",
      "citeRegEx" : "Sutherland et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sutherland et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep domain confusion: Maximizing for domain invariance",
      "author" : [ "Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell" ],
      "venue" : "arXiv preprint arXiv:1412.3474,",
      "citeRegEx" : "Tzeng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tzeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Fastmmd: Ensemble of circular discrepancy for efficient two-sample test",
      "author" : [ "Ji Zhao", "Deyu Meng" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Zhao and Meng.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao and Meng.",
      "year" : 2015
    }, {
      "title" : "Cross validation framework to choose amongst models and datasets for transfer learning",
      "author" : [ "Erheng Zhong", "Wei Fan", "Qiang Yang", "Olivier Verscheure", "Jiangtao Ren" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Zhong et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2010
    }, {
      "title" : "Supervised representation learning: Transfer learning with deep autoencoders",
      "author" : [ "Fuzhen Zhuang", "Xiaohu Cheng", "Ping Luo", "Sinno Jialin Pan", "Qing He" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Zhuang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "This paper addresses the particularly important and challenging domain-invariant representation learning task of unsupervised domain adaptation (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 220
    }, {
      "referenceID" : 18,
      "context" : "This paper addresses the particularly important and challenging domain-invariant representation learning task of unsupervised domain adaptation (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 220
    }, {
      "referenceID" : 22,
      "context" : "This paper addresses the particularly important and challenging domain-invariant representation learning task of unsupervised domain adaptation (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 220
    }, {
      "referenceID" : 9,
      "context" : "This paper addresses the particularly important and challenging domain-invariant representation learning task of unsupervised domain adaptation (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 220
    }, {
      "referenceID" : 9,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 17,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 0,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 20,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 19,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 33,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 21,
      "context" : "Recent research shows that domain adaptation approaches work particularly well with (deep) neural networks, which produce outstanding results on some domain adaptation data sets (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016).",
      "startOffset" : 178,
      "endOffset" : 336
    }, {
      "referenceID" : 1,
      "context" : "Three outstanding approaches for the choice of the distance function are the Proxy A-distance (Ben-David et al., 2010), the Kullback-Leibler (KL) divergence Kullback & Leibler (1951), applied to the mean of the activations (Zhuang et al.",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : ", 2010), the Kullback-Leibler (KL) divergence Kullback & Leibler (1951), applied to the mean of the activations (Zhuang et al., 2015), and the Maximum Mean Discrepancy (Gretton et al.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Using the Taylor expansion of the Gaussian kernel, most MMD-based approaches can be viewed as minimizing a certain distance between weighted sums of all raw moments (Li et al., 2015).",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : ", 2016; Aljundi et al., 2015; Long et al., 2015; Li et al., 2015; Zhuang et al., 2015; Louizos et al., 2016). The most successful methods have in common that they encourage similarity between the latent network representations w. r. t. the different domains. This similarity is often enforced by minimizing a certain distance between the networks’ domainspecific hidden activations. Three outstanding approaches for the choice of the distance function are the Proxy A-distance (Ben-David et al., 2010), the Kullback-Leibler (KL) divergence Kullback & Leibler (1951), applied to the mean of the activations (Zhuang et al.",
      "startOffset" : 8,
      "endOffset" : 566
    }, {
      "referenceID" : 10,
      "context" : "We consider the unsupervised domain adaptation setting (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016) with an input space X and a label space Y .",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "We consider the unsupervised domain adaptation setting (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016) with an input space X and a label space Y .",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "We consider the unsupervised domain adaptation setting (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016) with an input space X and a label space Y .",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "We consider the unsupervised domain adaptation setting (Glorot et al., 2011; Li et al., 2014; Pan et al., 2011; Ganin et al., 2016) with an input space X and a label space Y .",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "One fundamental assumption of most unsupervised domain adaptation networks is that the source risk RS(f) is a good indicator for the target risk RT (f), when the domain-specific latent space representations are similar (Ganin et al., 2016).",
      "startOffset" : 219,
      "endOffset" : 239
    }, {
      "referenceID" : 1,
      "context" : "One approach is the Proxy Adistance, given by d̂A = 2(1 − 2 ), where is the generalization error on the problem of discriminating between source and target samples (Ben-David et al., 2010).",
      "startOffset" : 164,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "One approach is the Proxy Adistance, given by d̂A = 2(1 − 2 ), where is the generalization error on the problem of discriminating between source and target samples (Ben-David et al., 2010). Ganin et al. (2016) compute the",
      "startOffset" : 165,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "Another approach is to make use of the MMD (Gretton et al., 2006) as domain regularizer.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "A suitable choice of the kernel seems to be the Gaussian kernel e−β‖x−y‖ 2 (Louizos et al., 2016; Li et al., 2015; Tzeng et al., 2014).",
      "startOffset" : 75,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "A suitable choice of the kernel seems to be the Gaussian kernel e−β‖x−y‖ 2 (Louizos et al., 2016; Li et al., 2015; Tzeng et al., 2014).",
      "startOffset" : 75,
      "endOffset" : 134
    }, {
      "referenceID" : 29,
      "context" : "A suitable choice of the kernel seems to be the Gaussian kernel e−β‖x−y‖ 2 (Louizos et al., 2016; Li et al., 2015; Tzeng et al., 2014).",
      "startOffset" : 75,
      "endOffset" : 134
    }, {
      "referenceID" : 32,
      "context" : "Suitable tuning procedures are transfer learning specific cross-validation methods (Zhong et al., 2010).",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "More general methods that don’t utilize source labels include heuristics that are based on kernel space properties (Sriperumbudur et al., 2009; Gretton et al., 2012), combinations of multiple kernels (Li et al.",
      "startOffset" : 115,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "More general methods that don’t utilize source labels include heuristics that are based on kernel space properties (Sriperumbudur et al., 2009; Gretton et al., 2012), combinations of multiple kernels (Li et al.",
      "startOffset" : 115,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : ", 2012), combinations of multiple kernels (Li et al., 2015), and kernel choices that maximize the MMD test power (Sutherland et al.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : ", 2015), and kernel choices that maximize the MMD test power (Sutherland et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "The drawback (b) of the kernel matrix computation can be handled by approximating the MMD (Zhao & Meng, 2015), or by using linear time estimators (Gretton et al., 2012).",
      "startOffset" : 146,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "In this work we focus on the quadratic-time MMD with the Gaussian kernel (Gretton et al., 2012; Tzeng et al., 2014) and transfer learning specific cross-validation for parameter tuning (Zhong et al.",
      "startOffset" : 73,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "In this work we focus on the quadratic-time MMD with the Gaussian kernel (Gretton et al., 2012; Tzeng et al., 2014) and transfer learning specific cross-validation for parameter tuning (Zhong et al.",
      "startOffset" : 73,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : ", 2014) and transfer learning specific cross-validation for parameter tuning (Zhong et al., 2010; Ganin et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : ", 2014) and transfer learning specific cross-validation for parameter tuning (Zhong et al., 2010; Ganin et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : ", 2010; Ganin et al., 2016). The two approaches MMD and the Proxy A-distance have in common that they do not minimize the domain discrepancy explicitly in the hidden activation space. In contrast, the authors in Zhuang et al. (2015) do so by minimizing a modified version of the Kullback-Leibler divergence of the mean activations (MKL).",
      "startOffset" : 8,
      "endOffset" : 233
    }, {
      "referenceID" : 19,
      "context" : "In particular, MMD-based approaches that use the Gaussian kernel are equivalent to minimizing a certain distance between weighted sums of all moments of the hidden activation distributions (Li et al., 2015).",
      "startOffset" : 189,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "Concerning approximation (c), the joint application of the weak law of large numbers (Billingsley, 2008) with the continuous mapping theorem (Billingsley, 2013) proves that this approximation creates a consistent estimate.",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Concerning approximation (c), the joint application of the weak law of large numbers (Billingsley, 2008) with the continuous mapping theorem (Billingsley, 2013) proves that this approximation creates a consistent estimate.",
      "startOffset" : 141,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "Amazon reviews: For our first experiment we use the Amazon reviews data set with the same preprocessing as used by Chen et al. (2012); Ganin et al.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "Amazon reviews: For our first experiment we use the Amazon reviews data set with the same preprocessing as used by Chen et al. (2012); Ganin et al. (2016); Louizos et al.",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "Amazon reviews: For our first experiment we use the Amazon reviews data set with the same preprocessing as used by Chen et al. (2012); Ganin et al. (2016); Louizos et al. (2016). The data set contains product reviews of four different product categories: books, DVDs, kitchen appliances and electronics.",
      "startOffset" : 115,
      "endOffset" : 178
    }, {
      "referenceID" : 23,
      "context" : "Office: The second experiment is based on the computer vision classification data set from Saenko et al. (2010) with images from three distinct domains: amazon (A), webcam (W) and dslr (D).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "These models are additionally compared with the state-of-the-art models VFAE (Louizos et al., 2016) and DANN (Ganin et al.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : ", 2016) and DANN (Ganin et al., 2016).",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : "Since we have to deal with sparse data, we rely on the Adagrad optimizer (Duchi et al., 2011).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "For all evaluations, the default parametrization is used as implemented in Keras (Chollet, 2015).",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Using this ”fully-transductive” protocol, we compare our method with other state-of-the-art approaches including DLID (Chopra et al., 2013), DDC (Tzeng et al.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 29,
      "context" : ", 2013), DDC (Tzeng et al., 2014), DAN (Long et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : ", 2014), DAN (Long et al., 2015), Deep CORAL (Sun & Saenko, 2016), and DANN (Ganin et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : ", 2015), Deep CORAL (Sun & Saenko, 2016), and DANN (Ganin et al., 2016), based on fine-tuning of the baseline model AlexNet (Krizhevsky et al.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : ", 2016), based on fine-tuning of the baseline model AlexNet (Krizhevsky et al., 2012).",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "We further compare our method to LSSA (Aljundi et al., 2015), CORAL (Sun et al.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : ", 2015), CORAL (Sun et al., 2016), and AdaBN (Li et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : ", 2016), and AdaBN (Li et al., 2016), based on the fine-tuning of InceptionBN (Ioffe & Szegedy, 2015).",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "We use a similar architecture as Ganin et al. (2016) with one dense hidden layer with 50 hidden nodes, sigmoid activation functions and softmax output function.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "We use a similar architecture as Ganin et al. (2016) with one dense hidden layer with 50 hidden nodes, sigmoid activation functions and softmax output function. Three neural networks are trained by means of eq. (2): (a) a base model without domain regularization (λ = 0), (b) with the MMD as domain regularizer and (c) with CMD as domain regularizer. These models are additionally compared with the state-of-the-art models VFAE (Louizos et al., 2016) and DANN (Ganin et al., 2016). The models (a),(b) and (c) are trained with similar setup as in Louizos et al. (2016) and Ganin et al.",
      "startOffset" : 33,
      "endOffset" : 568
    }, {
      "referenceID" : 5,
      "context" : "We use a similar architecture as Ganin et al. (2016) with one dense hidden layer with 50 hidden nodes, sigmoid activation functions and softmax output function. Three neural networks are trained by means of eq. (2): (a) a base model without domain regularization (λ = 0), (b) with the MMD as domain regularizer and (c) with CMD as domain regularizer. These models are additionally compared with the state-of-the-art models VFAE (Louizos et al., 2016) and DANN (Ganin et al., 2016). The models (a),(b) and (c) are trained with similar setup as in Louizos et al. (2016) and Ganin et al. (2016). For the CMD regularizer, the λ parameter of eq.",
      "startOffset" : 33,
      "endOffset" : 592
    }, {
      "referenceID" : 5,
      "context" : "We use a similar architecture as Ganin et al. (2016) with one dense hidden layer with 50 hidden nodes, sigmoid activation functions and softmax output function. Three neural networks are trained by means of eq. (2): (a) a base model without domain regularization (λ = 0), (b) with the MMD as domain regularizer and (c) with CMD as domain regularizer. These models are additionally compared with the state-of-the-art models VFAE (Louizos et al., 2016) and DANN (Ganin et al., 2016). The models (a),(b) and (c) are trained with similar setup as in Louizos et al. (2016) and Ganin et al. (2016). For the CMD regularizer, the λ parameter of eq. (2) is set to 1, i.e. the weighting parameter λ is neglected. The parameter K is heuristically set to five, as the first five moments capture rich geometric information about the shape of a distribution and K = 5 is small enough to be computationally efficient. However, the experiments in subsection Parameter Sensitivity show that similar results are obtained for K ≥ 3. For the MMD regularizer we use the Gaussian kernel with parameter β. We performed a hyperparameter search for β and λ, which has to be performed in an unsupervised way (no labels in the target domain). We use a variant of the reverse cross-validation approach proposed by Zhong et al. (2010), in which we initialize the model weights of the reverse classifier by the weights of the first learned classifier (see Ganin et al.",
      "startOffset" : 33,
      "endOffset" : 1306
    }, {
      "referenceID" : 5,
      "context" : "We use a similar architecture as Ganin et al. (2016) with one dense hidden layer with 50 hidden nodes, sigmoid activation functions and softmax output function. Three neural networks are trained by means of eq. (2): (a) a base model without domain regularization (λ = 0), (b) with the MMD as domain regularizer and (c) with CMD as domain regularizer. These models are additionally compared with the state-of-the-art models VFAE (Louizos et al., 2016) and DANN (Ganin et al., 2016). The models (a),(b) and (c) are trained with similar setup as in Louizos et al. (2016) and Ganin et al. (2016). For the CMD regularizer, the λ parameter of eq. (2) is set to 1, i.e. the weighting parameter λ is neglected. The parameter K is heuristically set to five, as the first five moments capture rich geometric information about the shape of a distribution and K = 5 is small enough to be computationally efficient. However, the experiments in subsection Parameter Sensitivity show that similar results are obtained for K ≥ 3. For the MMD regularizer we use the Gaussian kernel with parameter β. We performed a hyperparameter search for β and λ, which has to be performed in an unsupervised way (no labels in the target domain). We use a variant of the reverse cross-validation approach proposed by Zhong et al. (2010), in which we initialize the model weights of the reverse classifier by the weights of the first learned classifier (see Ganin et al. (2016) for details).",
      "startOffset" : 33,
      "endOffset" : 1446
    }, {
      "referenceID" : 4,
      "context" : "For all evaluations, the default parametrization is used as implemented in Keras (Chollet, 2015). All evaluations are repeated 10 times based on different shuffles of the data, and the mean accuracies and standard deviations are analyzed. Office: Since the office dataset is rather small with only 2817 images in its largest domain, we use the latent representations of the convolution neural network VGG16 of Simonyan & Zisserman (2014). In particular we train a classifier with one hidden layer, 256 hidden nodes and sigmoid activation function on top of the output of the first dense layer in the network.",
      "startOffset" : 82,
      "endOffset" : 438
    }, {
      "referenceID" : 0,
      "context" : "We further compare our method to LSSA (Aljundi et al., 2015), CORAL (Sun et al., 2016), and AdaBN (Li et al., 2016), based on the fine-tuning of InceptionBN (Ioffe & Szegedy, 2015). As an alternative to Adagrad for non-sparse data, we use the Adadelta optimizer from Zeiler (2012). Again, the default parametrization from Keras is used.",
      "startOffset" : 39,
      "endOffset" : 281
    }, {
      "referenceID" : 20,
      "context" : "VFAE refers to the Variational Fair Autoencoder of Louizos et al. (2016), including a slightly modified version of the MMD regularizer for faster computations, and DANN refers to the domainadversarial neural networks model of Ganin et al.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "(2016), including a slightly modified version of the MMD regularizer for faster computations, and DANN refers to the domainadversarial neural networks model of Ganin et al. (2016). The last two columns are taken directly from these publications.",
      "startOffset" : 160,
      "endOffset" : 180
    }, {
      "referenceID" : 20,
      "context" : "The last two columns are taken directly from Louizos et al. (2016) and Ganin et al.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "(2016) and Ganin et al. (2016).",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "We improve the previous state-of-the-art method AdaBN (Li et al., 2016) by more than 3.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "The first 10 rows are taken directly from the papers of Ganin et al. (2016) and Li et al.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "The first 10 rows are taken directly from the papers of Ganin et al. (2016) and Li et al. (2016). The models DLID –DANN are based on the AlexNet model, LSSA –AdaBN are based on the InceptionBN model, and our method (CMD) is based on the VGG16 model.",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Note that the problem of finding the best settings for the parameter β of the Gaussian kernel is a well known problem (Hsu et al., 2003).",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "Note that the problem of finding the best setting for the parameter β of the Gaussian kernel is a well known problem (Hsu et al., 2003).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "≤ 2 √ N sup X∈X ([a,b]) E ∣∣∣∣X − E(X) b− a ∣∣∣∣k ) The latter term refers to the absolute central moment of order k, for which the smallest upper bound is known (Egozcue et al., 2012): 1 |b− a|k ‖ck(X)− ck(Y )‖2 ≤ 2 √ N sup x∈[0,1] x(1− x) + (1− x)x",
      "startOffset" : 162,
      "endOffset" : 184
    } ],
    "year" : 2017,
    "abstractText" : "The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy, an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w. r. t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with Maximum Mean Discrepancy, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w. r. t. parameter changes in a certain interval. The source code of the experiments is publicly available1.",
    "creator" : "LaTeX with hyperref package"
  }
}