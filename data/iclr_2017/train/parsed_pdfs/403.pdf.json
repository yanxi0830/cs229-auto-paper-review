{
  "name" : "403.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LIE-ACCESS NEURAL TURING MACHINES",
    "authors" : [ "Greg Yang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recent work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has repopularized the use of explicit external memory in neural networks and demonstrated that these networks can be effectively trained in an end-to-end fashion. These methods have been successfully applied to question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and other tasks. This methodology has the potential to extend deep networks in a general-purpose way beyond the limitations of fixed-length encodings such as standard recurrent neural networks (RNNs).\nA shared theme in many of these works (and earlier exploration of neural memory) is to re-frame traditional memory access paradigms to be continuous and possibly differentiable to allow for backpropagation. In MemNNs, traditional random-access memory is replaced with a ranking approach that finds the most likely memory. In the work of Grefenstette et al. (2015), classical stack-, queue-, and deque-based memories are replaced by soft-differentiable stack, queue, and deque datastructures. In NTMs, sequential local-access memory is simulated by an explicit tape data structure.\nThis work questions the assumption that neural memory should mimic the structure of traditional discrete memory. We argue that a neural memory should provide the following: (A) differentiability for end-to-end training and (B) robust relative indexing (perhaps in addition to random-access). Surprisingly many neural memory systems fail one of these conditions, either lacking Criterion B, discussed below, or employing extensions like REINFORCE to work around lack of differentiability (Zaremba & Sutskever, 2015).\nWe propose instead a class of memory access techniques based around Lie groups, i.e. groups with differentiable operations, which provide a natural structure for neural memory access. By definition, their differentiability satisfies the concerns of Criterion A. Additionally the group axioms provide identity, invertibility, and associativity, all of which are desirable properties for a relative indexing scheme (Criterion B), and all of which are satisfied by standard Turing machines. Notably though,\nsimple group properties like invertibility are not satisfied by neural Turing machines, differentiable neural computers, or even by simple soft-tape machines. In short, in our method, we construct memory systems with keys placed on a manifold, and where relative access operations are provided by Lie groups.\nTo experiment with this approach, we implement a neural Turing machine with an LSTM controller and several versions of Lie-access memory, which we call Lie-access neural Turing machines (LANTM). The details of these models are exhibited in Section 4.1 Our main experimental results are presented in Section 5. The LANTM model is able to learn non-trivial algorithmic tasks such as copying and permutating sequences with higher accuracy than more traditional memory-based approaches, and significantly better than fixed memory LSTM models. The memory structures and key transformation learned by the model resemble interesting continuous space representations of traditional discrete memory data structures."
    }, {
      "heading" : "2 BACKGROUND: RECURRENT NEURAL NETWORKS WITH MEMORY",
      "text" : "This work focuses particularly on recurrent neural network (RNN) controllers of abstract neural memories. Formally, an RNN is a differentiable function RNN : X × H → H, where X is an arbitrary input space and H is the hidden state space. On input (x(1), . . . , x(T )) ∈ X T and with initial state h(0) ∈ H, the RNN produces states h(1), . . . , h(T ) based on the recurrence,\nh(t) := RNN(x(t), h(t−1)).\nThese states can be used for downstream tasks, for example sequence prediction which produces outputs (y(1), . . . , y(T )) based on an additional transformation and prediction layer y(t) = F (h(t)) such as a linear-layer followed by a softmax. RNNs can be trained end-to-end by backpropagationthrough-time (BPTT) (Werbos, 1990). In practice, we use long short-term memory (LSTM) RNNs (Hochreiter & Schmidhuber, 1997). LSTM’s hidden state consists of two variables (c(t), h(t)), where h(t) is also the output to the external world; we however use the above notation for simplicity.\nAn RNN can also serve as the controller for an external memory system (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015), which enables: (1) the entire system to carry state over time from both the RNN and the external memory, and (2) the RNN controller to collect readings from and compute additional instructions to the external memory. Formally, we extend the recurrence to,\nh(t) := RNN([x(t); ρ(t−1)], h(t−1)),\nΣ(t), ρ(t) := RW(Σ(t−1), h(t)),\nwhere Σ is the abstract memory state, and ρ(t) is the value read from memory, and h is used as an abstract controller command to a read/write function RW. Writing occurs in the mutation of Σ at each time step. Throughout this work, Σ will take the form of an ordered set {(ki, vi, si)}i where ki ∈ K is an arbitrary key, vi ∈ Rm is a memory value, and si ∈ R+ is a memory strength. In order for the model to be trainable with backpropagation, the memory function RW must also be differentiable. Several forms of differentiable memory have been proposed in the literature. We begin by describing two simple forms: (neural) random-access memory and (neural) tape-based memory. For this section, we focus on the read step and assume Σ is fixed.\nRandom-Access Memory Random-access memory consists of using a now standard attentionmechanism or MemNN to read a memory (our description follows Miller et al. (2016)). The controller hidden state is used to output a random-access pointer, q′(h) that determines a weighting of memory vectors via dot products with the corresponding keys. This weighting in turn determines the read values via linear smoothing based on a function w,\nwi(q,Σ) := si exp 〈q, ki〉∑\nj sj exp 〈q, kj〉 ρ := ∑ i wi(q ′(h),Σ)vi.\nThe final read memory is based on how “close” the read pointer was to each of the keys, where closeness in key space is determined by w.\n1Our implementations are available at https://github.com/harvardnlp/lie-access-memory\nTape-Based Memory Neural memories can also be extended to support relative access by maintaining read state. Following notation from Turing machines, we call this state the head, q. In the simplest case the recurrence now has the form,\nΣ′, q′, ρ = RW(Σ, q, h),\nand this can be extended to support multiple heads.\nIn the simplest case of soft tape-based memory (a naive version of the much more complicated neural Turing machine), the keys ki indicate one-hot positions along a tape with ki = δi. The head q is a probability distribution over tape positions. It determines the read value by directly specifying the weights. The controller can only “shift” the head by outputting a kernel K(h) = (K−1,K0,K+1) in the probability simplex ∆2 and applying convolution.\nq′(q, h) := q ∗K(h), i.e. q′j = qj−1K+1 + qjK0 + qj+1K−1\nWe can view this as the soft version of a single-step discrete Turing machine where the kernel can softly shift the “head” of the machine one to the left, one to the right, or remain in the same location. The value returned can then be computed with linear smoothing as above,\nwi(q,Σ) := si〈q, ki〉∑\nj sj〈q, kj〉 ρ := ∑ i wi(q ′(q, h),Σ)vi."
    }, {
      "heading" : "3 LIE GROUPS FOR MEMORY",
      "text" : "Let us now take a brief digression and consider the standard (non-neural) Turing machine (TM) and the movement of its head over a tape. A TM has a head q ∈ Z indicating the position on a tape. Between reads, the head can move any number of steps left or right. Moving a + b steps and then c steps eventually puts the head at the same location as moving a steps and then b + c steps — i.e. the head movement is associative. In addition, the machine should be able to reverse a head shift, for example, in a stack simulation algorithm, going from push to pop — i.e. each head movement should also have a corresponding inverse. Finally, the head should also be allowed to stay put, for example, to read a single data item and use it for multiple time points, an identity.\nThese movements correspond directly to group actions: the possible head movements should be associative, and contain inverse and identity elements. This group acts on the set of possible head locations. In a TM, the set of Z-valued head movement acts on the set of locations on the Z-indexed infinite tape. By our reasoning above, if a Turing machine is to store data contents at points in a general space K (instead of an infinite Z-indexed tape), then its head movements should form a group and act on K via group actions. For a neural memory system, we desire the network to be (almost everywhere) differentiable. The notion of “differentiable” groups is well-studied in mathematics, where they are known as Lie groups, and “differentiable group actions” are correspondingly called Lie group actions. In our case, using Lie group actions as generalized head movements on a general key space (more accurately, manifolds) would most importantly mean that we can take derivatives of these movements and perform the usual backpropagation algorithm."
    }, {
      "heading" : "4 LIE-ACCESS NEURAL TURING MACHINES",
      "text" : "These properties motivate us to propose Lie access as an alternative formalism to popular neural memory systems, such as probabilistic tapes, which surprisingly do not satisfy invertibility and often do not provide an identity.2 Our Lie-access memory will consist of a set of points in a manifold K.\n2The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and sketched in Section 2 does not in general have inverses. Indeed, the authors reported problems with the soft head losing “sharpness” over time, which they dealt with by sharpening coefficients. In the followup work, Graves et al. (2016) utilize a temporal memory link matrix for actions. They note, “the operation Lw smoothly shifts the focus forwards to the locations written ... whereas L>w shifts the focus backwards” but do not enforce this as a true inverse. They also explicitly do not include an identity, noting “Self-links are excluded (the diagonal of the link matrix is always 0)”; however, they could ignore the link matrix with an interpolation gate, which in effect acts as the identity.\nWe replace the discrete head with a continuous head q ∈ K. The head moves based on a set of Lie group actions a ∈ A generated by the controller. To read memories, we will rely on a distance measure in this space, d : K × K → R≥0.3 Together these properties describe a general class of possible neural memory architectures.\nFormally a Lie-access neural Turing machine (LANTM) computes the following function,\nΣ′, q′, q′(w), ρ := RW(Σ, q, q(w), h)\nwhere q, q(w) ∈ K are resp. read and write heads, and Σ is the memory itself. We implement Σ, as above, as a weighted dictionary Σ = {(ki, vi, si)}i."
    }, {
      "heading" : "4.1 ADDRESSING PROCEDURE",
      "text" : "The LANTM maintains a read head q which at every step is first updated to q′ and then used to read from the memory table. This update occurs by selecting a Lie group action from A which then acts smoothly on the key space K. We parametrize the action transformation, a : H 7→ A by the hidden state to produce the Lie action, a(h) ∈ A. In the simplest case, the head is then updated based on this action (here · denotes group action): q′ := a(h) · q. For instance, consider two possible Lie groups:\n(1) A shift group R2 acting additively on R2. This means that A = R2 so that a(h) = (α, β) acts upon a head q = (x, y) by,\na(h) · q = (α, β) + (x, y) = (x+ α, y + β).\n(2) A rotation group SO(3) acting on the sphere S2 = {v ∈ R3 : ‖v‖ = 1}. Each rotation can be described by its axis ξ (a unit vector) and angle θ. An action (ξ, θ) · q is just the appropriate rotation of the point q, and is given by Rodrigues’ rotation formula,\na(h) · q = (ξ, θ) · q = q cos θ + (ξ × q) sin θ + ξ〈ξ, q〉(1− cos θ). Here × denotes cross product."
    }, {
      "heading" : "4.2 READING AND WRITING MEMORIES",
      "text" : "Recall that memories are stored in Σ, each with a key, ki, memory vector, vi, and strength, si, and that memories are read using linear smoothing over vectors based on a key weighting function w, ρ := ∑ i wi(q\n′,Σ)vi . While there are many possible weighting schemes, we use one based on the distance of each memory address from the head in key-space assuming a metric d on K. We consider two different weighting functions (1) inverse-square and (2) softmax. There first uses the polynomial law and the second an annealed softmax of the squared distances:\nw (1) i (q,Σ) :=\nsid(q, ki) −2∑\nj sjd(q, kj) −2 w\n(2) i (q,Σ, T ) := si exp(−d(q, ki)2/T )∑ j sj exp(−d(q, kj)2/T ) ,\nwhere we use the convention that it takes the limit value when q → ki and T is a temperature that represents the certainty of its reading, i.e. higher T creates more uniform w.\nThe writing procedure is similar to reading. The LANTM maintains a separate write head q(w) that moves analogously to the read head, i.e. with action function a(w)(h) and updated value q′(w) . At each call to RW, a new memory is automatically appended to Σ with k = q′(w). The corresponding\n3This metric should satisfy a compatibility relation with the Lie group action. When points x, y ∈ X are simultaneously moved by the same Lie group action v, their distance should stay the same (One possible mathematical formalization is thatX should be a Riemannian manifold and the Lie group should be a subgroup of X’s isometry group.): d(vx, vy) = d(x, y). This condition ensures that if the machine writes a sequence of data along a “straight line” at points x, vx, v2x, . . . , vkx, then it can read the same sequence by emitting a read location y close to x and then follow the “v-trail” y, vy, v2y, . . . , vky.\nmemory v and strength s are created by MLP’s v(h) ∈ Rm and s(h) ∈ [0, 1] taking h as input. After writing, the new memory set is,\nΣ′ := Σ ∪ {(q′(w), v(h), s(h))}.\nNo explicit erase mechanism is provided, but to erase a memory (k, v, s), the controller may in theory write (k,−v, s)."
    }, {
      "heading" : "4.3 COMBINING WITH RANDOM ACCESS",
      "text" : "Finally we combine this relative addressing procedure with direct random-access to give the model the ability for absolute address access. We do this by outputting an absolute address each step and simply interpolating with our current head. Write t(h) ∈ [0, 1] for the interpolation gate and q̃(h) ∈ K for our proposed random-access layer. For key space manifolds K like Rn, 4 there’s a well defined straight-line interpolation between two points, so we can set\nq′ := a · (tq + (1− t)q̃)\nwhere we have omitted the implied dependence on h. For other manifolds like the spheres Sn that have well-behaved projection functions π : Rn → Sn, we can just project the straight-line interpolation to the sphere:\nq′ := a · π(tq + (1− t)q̃).\nIn the case of a sphere Sn, π is just L2-normalization.5"
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We experiment with Lie-access memory on a variety of algorithmic learning tasks. We are particularly interested in: (a) how Lie-access memory can be trained, (b) whether it can be effectively utilized for algorithmic learning, and (c) what internal structures the model learns compared to systems based directly on soft discrete memory. In particular Lie access is not equipped with an explicit stack or tape, so it would need to learn continuous patterns that capture these properties.\nSetup. Our experiments utilize an LSTM controller in a version of the encoder-decoder setup (Sutskever et al., 2014), i.e. an encoding input pass followed by a decoding output pass. The encoder reads and writes memories at each step; the decoder only reads memories. The encoder is given 〈s〉,\n4Or in general, manifolds with convex embeddings in Rn. 5Technically, in the sphere case, domπ = Rd − {0}. But in practice one almost never gets 0 from a\nstraight-line interpolation, so computationally this makes little difference.\nfollowed by an the input sequence, and then 〈/s〉 to terminate input. The decoder is not re-fed its output or the correct symbol, i.e. we do not use teacher forcing, so x(t) is a fixed placeholder input symbol. The decoder must correctly emit an end-of-output symbol 〈/e〉 to terminate. Models and Baselines. We implement three main baseline models including: (a) a standard LSTM encoder-decoder, without explicit external memory, (b) a random access memory network, RAM using the key-value formulation as described in the background, roughly analogous to an attentionbased encoder-decoder, and (c) an interpolation of a RAM/Tape-based memory network as described in the background, i.e. a highly simplified version of a true NTM (Graves et al., 2014) with a sharpening parameter. Our models include four versions of Lie-access memory. The main model, LANTM, has an LSTM controller, with a shift group A = R2 acting additively on key space K = R2. We also consider a model SLANTM with spherical memory, utilizing a rotation group A = SO(3) acting on keys in the sphere K = S2. For both of the models, the distance function d is the Euclidean (L2) distance, and we experiment with smoothing using inverse-square (default) and with an annealed softmax.6\nModel Setup. For all tasks, the LSTM baseline has 1 to 4 layers, each with 256 cells. Each of the other models has a single-layer, 50-cell LSTM controller, with memory width (i.e. the size of each memory vector) 20. Other parameters such as learning rate, decay, and intialization are found through grid search. Further hyperparameter details are give in the appendix.\nTasks. Our experiments are on a series of algorithmic tasks shown in Table 1a. The COPY, REVERSE, and BIGRAM FLIP tasks are based on Grefenstette et al. (2015); the DOUBLE and INTERLEAVED ADD tasks are designed in a similar vein. Additionally we also include three harder tasks: ODD FIRST, REPEAT COPY, and PRIORITY SORT. In ODD FIRST, the model must output the oddindexed elements first, followed by the even-indexed elements. In REPEAT COPY, each model must repeat a sequence of length 20, N times. In PRIORITY SORT, each item of the input sequence is given a priority, and the model must output them in priority order.\nWe train each model in two regimes, one with a small number of samples (16K) and one with a large number of samples (320K). In the former case, the samples are iterated through 20 times, while in the latter, the samples are iterated through only once. Thus in both regimes, the total training times are the same. Training is done by minimizing negative log likelihood with RMSProp.\nPrediction is performed via argmax/greedy prediction at each step. To evaluate the performance of the models, we compute the fraction of tokens correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine and coarse scores. We assess the models on 3.2K randomly generated out-of-sample 2x length examples, i.e. with sequence lengths 2k (or repeat number 2N in the case of REPEAT COPY) to test the generalization of the system. More precisely, for all tasks other than repeat copy, during training, the length k is varied in the interval [lk, uk] (as shown in table 1ba). During test time, the length k is varied in the range [uk + 1, 2uk]. For repeat copy, the repetition number N is varied similarly, instead of k.\nResults. Main results comparing the different memory systems and read computations on a series of tasks are shown in Table 1b. Consistent with previous work the fixed-memory LSTM system fails consistently when required to generalize to the 2x samples, unable to solve any 2x problem correctly, and only able to predict at most ∼ 50% of the symbols for all tasks except interleaved addition, regardless of training regime. The RAM (attention-based) and the RAM/tape hybrid are much stronger baselines, answering more than 50% of the characters correctly for all but the 6-ODD FIRST task. Perhaps surprisingly, RAM and RAM/tape learned the 7-REPEAT COPY task with almost perfect generalization scores when trained in the large sample regime. In general, it does not seem that the simple tape memory confers much advantage to the RAM model, as the generalization performances of both models are similar for the most part, which motivates more advanced NTM enhancements beyond sharpening.\nThe last four columns illustrate the performance of the LANTM models. We found the inversesquare LANTM and SLANTM models to be the most effective, achieving > 90% generalization\n6Note that the read weight calculation of a SLANTM with softmax is essentially the same as the RAM model: For head q, exp(−d(q, ki)2/T ) = exp(−‖q − ki‖2/T ) = exp(−(2 − 2〈q, ki〉)/T ), where the last equality comes from ‖q‖ = ‖ki‖ = 1 (key-space is on the sphere). Therefore the weights wi =\nsi exp(−d(q,ki)2/T )∑ j sj exp(−d(q,kj)2/T ) = si exp(−2〈q,ki〉/T )∑ j sj exp(−2〈q,kj〉/T ) , which is the RAM weighting scheme.\nTask Input Output Size k |V| 1 - COPY a1a2a3 · · · ak a1a2a3 · · · ak [2, 64] 128 2 - REVERSE a1a2a3 · · · ak akak−1ak−2 · · · a1 [2, 64] 128 3 - BIGRAM FLIP a1a2a3a4 · · · a2k−1a2k a2a1a4a3 · · · a2ka2k−1 [1, 16] 128 4 - DOUBLE a1a2 · · · ak 2× |ak · · · a1| [2, 40] 10 5 - INTERLEAVED ADD a1a2a3a4 · · · a2k−1a2k |a2ka2k−2 · · · a2|+ |a2k−1 · · · a1| [2, 16] 10 6 - ODD FIRST a1a2a3a4 · · · a2k−1a2k a1a3 · · · a2k−1a2a4 · · · a2k [1, 16] 128 7 - REPEAT COPY Na1 · · · a20 a1 · · · a20 · · · a1 · · · a20 (N times) N ∈ [1, 5] 128 8 - PRIORITY SORT 5a52a29a9 · · · a1a2a3 · · · ak [2, 10] 128\n(a) Task descriptions and parameters. |ak · · · a1| means the decimal number repesented by decimal digits ak · · · a1. Arithmetic tasks have all numbers formatted with the least significant digits on the left and with zero padding. The DOUBLE task takes an integer x ∈ [0, 10k) padded to k digits and outputs 2x in k + 1 digits, zero padded to k+ 1 digits. The INTERLEAVED ADD task takes two integers x, y ∈ [0, 10k) padded to k digits and interleaved, forming a length 2k input sequence and outputs x + y zero padded to k + 1 digits. The last two tasks use numbers in unary format: N is the shorthand for a length N sequence of a special symbol @, encoding N in unary, e.g. 3 = @@@.\nBase Memory Lie LSTM RAM RAM/Tape LANTM LANTM-s SLANTM SLANTM-s\nS L S L S L S L S L S L S L\n1 16/0 21/0 61/0 61/1 70/2 70/1 ? ? ? ? ? ? ? ? 2 26/0 32/0 58/2 54/2 24/1 43/2 ? ? 97/44 98/88 99/96 ? ? ? 3 30/0 39/0 56/5 54/9 64/8 69/9 ? ? ? 99/94 99/99 97/67 93/60 90/43 4 44/0 47/0 72/8 74/15 70/12 71/6 ? ? ? ? ? ? ? ? 5 60/0 61/0 74/13 76/17 77/23 67/19 99/93 99/93 90/38 94/57 99/91 99/97 98/78 ? 6 29/0 42/0 31/5 46/4 43/8 62/8 99/91 99/95 90/29 50/0 49/7 56/8 74/15 76/16 7 24/0 37/0 98/56 99/98 71/18 99/93 67/0 70/0 17/0 48/0 99/91 99/78 96/41 99/51 8 46/0 53/0 60/5 80/22 78/15 66/9 87/35 98/72 99/95 99/99 ? 99/99 98/79 ?\n(b) Main results. Numbers represent the accuracy percentages on the fine/coarse evaluations on the out-ofsample 2× tasks. The S and L columns resp. indicate small and large sample training regimes. Symbol ? indicates exact 100% accuracy (Fine scores above 99.5 are not rounded up). Baselines are described in the body. LANTM and SLANTM use inverse-square while LANTM-s and SLANTM-s use softmax weighting scheme. The best scores, if not 100% (denoted by stars), are bolded for each of the small and large sample regimes.\naccuracy on most tasks, and together they solve all of the tasks here with > 90% coarse score. In particular, LANTM is able to solve the 6-ODD FIRST problem when no other model can correctly solve 20% of the 2x instances; SLANTM on the other hand is the only Lie access model able to solve the 7-REPEAT COPY problem.\nThe best Lie access model trained with the small sample regime beats or is competitive with any of the baseline trained under the large sample regime. In all tasks other than 7-REPEAT COPY, the gap in the coarse score between the best Lie access model in small sample regime and the best baseline in any sample regime is ≥ 70%. However, in most cases, training under the large sample regime does not improve much. For a few tasks, small sample regime actually produces a model with better generalization than large sample regime. We observed in these instances, the generalization error curve under a large sample regime reaches an optimum at around 2/3 to 3/4 of training time, and then increases almost monotonically from there. Thus, the model likely has found an algorithm that works only for the training sizes; in particular, this phenomenon does not seem to be due to lack of training time."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "Qualitative Analysis. We did further visual analysis of the different Lie-access techniques to see how the models were learning the underlying tasks, and to verify that they were using the relative addressing scheme. Figure 2 shows two diagrams of the LANTM model of the tasks of priority sort and repeat copy. Figure 3 shows two diagrams of the SLANTM model for the same two tasks. Fig-\nure 4 shows the memory access pattern of LANTM on 6-ODD FIRST task. Additionally, animations tracing the evolution of the memory access pattern of models over training time can be found at http://nlp.seas.harvard.edu/lantm. They demonstrate that the models indeed learn relative addressing and internally are constructing geometric data structures to solve these algorithmic tasks.\nUnbounded storage One possible criticism of the LANTM framework could be that the amount of information stored increases linearly with time, which limits the usefulness of this framework for long timescale tasks. This is indeed the case with our implementations, but need not be the case in general. There can be many ways of limiting physical memory usage. For example, a simple way is to discard the least recently used memory, as in the work of Graves et al. (2016) and Gulcehre et al. (2016). Another way is to approximate with fixed number of bits the read function that takes a head position and returns the read value. For example, noting that this function is a rational function on the head position, keys, and memory vectors, we can approximate the numerators and denominators with a fixed degree polynomial.\nContent address Our Lie-access framework is not mutually exclusive from content addressing methods. For example, in each of our implementations, we could have the controllers output both a position in the key space and a content addresser of the same size as memory vectors, and interpolated the read values from Lie-access and the read values from content addressing."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "This paper introduces Lie-access memory as an alternative neural memory access paradigm, and explored several different implementations of this approach. LANTMs follow similar axioms as discrete Turing machines while providing differentiability. Experiments show that simple models can learn algorithmic tasks. Internally these models naturally learn equivalence of standard data structures like stack and cyclic lists. In future work we hope to experiment with more groups and to scale these methods to more difficult reasoning tasks. For instance, we hope to build a general purpose encoder-decoder model for tasks like question answering and machine translation that makes use of differentiable relative-addressing schemes to replace RAM-style attention."
    }, {
      "heading" : "A EXPERIMENTAL DETAILS",
      "text" : "We obtain our results by performing a grid search over the hyperparameters specified in Table A.1 and also over seeds 1 to 3, and take the best scores. We bound the norm of the LANTM head shifts by 1, whereas we try both bounding and not bounding the angle of rotation in our grid for SLANTM. We initialize the Lie access models to favor Lie access over random access through the interpolation mechanism discussed in section 4.3.\nThe RAM model read mechanism is as discussed in section 2, and writing is done by appending new (k, v, s) tuples to the memory Σ. The only additions to this model in RAM/tape is that left and right keys are now computed using shifted convolution with the read weights:\nkL := ∑ i wi+1ki\nkR := ∑ i wi−1ki\nand these keys kL and kR are available (along with the random access key output by the controller) to the controller on the next turn to select from via interpolation. We also considered weight sharpening in the RAM/Tape model according to Graves et al. (2014): the controller can output a sharpening coefficient γ ≥ 1 each turn, so that the final weights are w̃i =\nwγi∑ j w γ j\n. We included this as a feature to grid search over.\nWe found that weight sharpening only confers small advantage over vanilla on the COPY, BIGRAM FLIP, and DOUBLE tasks, but deteriorates performance on all other tasks."
    }, {
      "heading" : "B ACTION INTERPOLATION",
      "text" : "We also experimented with adding an interpolation between the last action a(t−1) with a candidate action a(h) via a gate r(h) ∈ [0, 1] to produce the final action a(t). Then the final equation of the new head is q′ := a(t) · π(tq + (1− t)q̃). This allows the controller to easily move in “a straight line” by just saturating both t and r.\nFor example, for the translation group we have straight-line interpolation, a(t) := ra+(1−r)a(t−1). For the rotation group SO(3), each rotation is represented by its axis ξ ∈ S2 and angle θ ∈ (−π, π],\nand we just interpolate each separately ξ(t) := π(rξ+(1−r)ξ(t−1)) and θ(t) := rθ+(1−r)θ(t−1). where π is L2-normalization.7\nWe perform the same experiments, with the same grid as specified in the last section, and with the initial action interpolation gates biased toward the previous action. The results are given in table B.2. Figure B.1 shows action interpolation’s impact on performance. Most notably, interpolation seems to improve performance of most models in the 5-INTERLEAVED ADD task and of the spherical memory models in the 6-ODD FIRST task, but causes failure to learn in many situations, most significantly, the failure of LANTM to learn 6-ODD FIRST.\n7There is, in fact, a canonical way to interpolate the most common Lie groups, including all of the groups mentioned above, based on the exponential map and the Baker-Campbell-Hausdorff formula (Lee, 2012), but the details are outside the scope of this paper and the computational cost, while acceptable in control theory settings, is too hefty for us. Interested readers are referred to Shingel (2009) and Marthinsen (1999)."
    } ],
    "references" : [ {
      "title" : "Neural Turing Machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : "[cs],",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Hybrid computing using a neural network with dynamic external",
      "author" : [ "Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwińska", "Sergio Gómez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou" ],
      "venue" : "memory. Nature,",
      "citeRegEx" : "Graves et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to Transduce with Unbounded Memory",
      "author" : [ "Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "[cs],",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes",
      "author" : [ "Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "[cs],",
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2016
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jrgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets",
      "author" : [ "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "[cs],",
      "citeRegEx" : "Joulin and Mikolov.,? \\Q2015\\E",
      "shortCiteRegEx" : "Joulin and Mikolov.",
      "year" : 2015
    }, {
      "title" : "Neural GPUs Learn Algorithms",
      "author" : [ "ukasz Kaiser", "Ilya Sutskever" ],
      "venue" : "[cs],",
      "citeRegEx" : "Kaiser and Sutskever.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kaiser and Sutskever.",
      "year" : 2015
    }, {
      "title" : "Grid Long Short-Term Memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves" ],
      "venue" : "[cs],",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2015
    }, {
      "title" : "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Richard Socher" ],
      "venue" : "[cs],",
      "citeRegEx" : "Kumar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Random-Access Machines",
      "author" : [ "Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever" ],
      "venue" : "[cs],",
      "citeRegEx" : "Kurach et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kurach et al\\.",
      "year" : 2015
    }, {
      "title" : "Introduction to Smooth Manifolds",
      "author" : [ "John Lee" ],
      "venue" : "Number 218 in Graduate Texts in Mathematics. Springer, 2 edition,",
      "citeRegEx" : "Lee.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lee.",
      "year" : 2012
    }, {
      "title" : "Interpolation in Lie Groups",
      "author" : [ "A. Marthinsen" ],
      "venue" : "SIAM Journal on Numerical Analysis,",
      "citeRegEx" : "Marthinsen.,? \\Q1999\\E",
      "shortCiteRegEx" : "Marthinsen.",
      "year" : 1999
    }, {
      "title" : "Key-value memory networks for directly reading",
      "author" : [ "Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston" ],
      "venue" : "documents. CoRR,",
      "citeRegEx" : "Miller et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2016
    }, {
      "title" : "Interpolation in special orthogonal groups",
      "author" : [ "Tatiana Shingel" ],
      "venue" : "IMA Journal of Numerical Analysis,",
      "citeRegEx" : "Shingel.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shingel.",
      "year" : 2009
    }, {
      "title" : "End-To-End Memory Networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "[cs],",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to Sequence Learning with Neural Networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "[cs],",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "Paul J. Werbos" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Werbos.,? \\Q1990\\E",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "Memory Networks. arXiv:1410.3916 [cs, stat",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes" ],
      "venue" : "URL http://arxiv.org/abs/1410.3916",
      "citeRegEx" : "Weston et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning Neural Turing Machines - Revised",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever" ],
      "venue" : "[cs],",
      "citeRegEx" : "Zaremba and Sutskever.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zaremba and Sutskever.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al., 2014; Weston et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al., 2014; Weston et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "Recent work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al.",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : ", 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has repopularized the use of explicit external memory in neural networks and demonstrated that these networks can be effectively trained in an end-to-end fashion.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "These methods have been successfully applied to question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.",
      "startOffset" : 67,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "These methods have been successfully applied to question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.",
      "startOffset" : 67,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "These methods have been successfully applied to question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.",
      "startOffset" : 67,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.",
      "startOffset" : 28,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.",
      "startOffset" : 28,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.",
      "startOffset" : 28,
      "endOffset" : 201
    }, {
      "referenceID" : 2,
      "context" : ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.",
      "startOffset" : 28,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : ", 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and other tasks.",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Recent work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has repopularized the use of explicit external memory in neural networks and demonstrated that these networks can be effectively trained in an end-to-end fashion. These methods have been successfully applied to question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and other tasks. This methodology has the potential to extend deep networks in a general-purpose way beyond the limitations of fixed-length encodings such as standard recurrent neural networks (RNNs). A shared theme in many of these works (and earlier exploration of neural memory) is to re-frame traditional memory access paradigms to be continuous and possibly differentiable to allow for backpropagation. In MemNNs, traditional random-access memory is replaced with a ranking approach that finds the most likely memory. In the work of Grefenstette et al. (2015), classical stack-, queue-, and deque-based memories are replaced by soft-differentiable stack, queue, and deque datastructures.",
      "startOffset" : 46,
      "endOffset" : 1230
    }, {
      "referenceID" : 16,
      "context" : "RNNs can be trained end-to-end by backpropagationthrough-time (BPTT) (Werbos, 1990).",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "An RNN can also serve as the controller for an external memory system (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015), which enables: (1) the entire system to carry state over time from both the RNN and the external memory, and (2) the RNN controller to collect readings from and compute additional instructions to the external memory.",
      "startOffset" : 70,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "An RNN can also serve as the controller for an external memory system (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015), which enables: (1) the entire system to carry state over time from both the RNN and the external memory, and (2) the RNN controller to collect readings from and compute additional instructions to the external memory.",
      "startOffset" : 70,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "Random-Access Memory Random-access memory consists of using a now standard attentionmechanism or MemNN to read a memory (our description follows Miller et al. (2016)).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and sketched in Section 2 does not in general have inverses.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and sketched in Section 2 does not in general have inverses. Indeed, the authors reported problems with the soft head losing “sharpness” over time, which they dealt with by sharpening coefficients. In the followup work, Graves et al. (2016) utilize a temporal memory link matrix for actions.",
      "startOffset" : 70,
      "endOffset" : 332
    }, {
      "referenceID" : 15,
      "context" : "Our experiments utilize an LSTM controller in a version of the encoder-decoder setup (Sutskever et al., 2014), i.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "a highly simplified version of a true NTM (Graves et al., 2014) with a sharpening parameter.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "a highly simplified version of a true NTM (Graves et al., 2014) with a sharpening parameter. Our models include four versions of Lie-access memory. The main model, LANTM, has an LSTM controller, with a shift group A = R acting additively on key space K = R. We also consider a model SLANTM with spherical memory, utilizing a rotation group A = SO(3) acting on keys in the sphere K = S. For both of the models, the distance function d is the Euclidean (L2) distance, and we experiment with smoothing using inverse-square (default) and with an annealed softmax.6 Model Setup. For all tasks, the LSTM baseline has 1 to 4 layers, each with 256 cells. Each of the other models has a single-layer, 50-cell LSTM controller, with memory width (i.e. the size of each memory vector) 20. Other parameters such as learning rate, decay, and intialization are found through grid search. Further hyperparameter details are give in the appendix. Tasks. Our experiments are on a series of algorithmic tasks shown in Table 1a. The COPY, REVERSE, and BIGRAM FLIP tasks are based on Grefenstette et al. (2015); the DOUBLE and INTERLEAVED ADD tasks are designed in a similar vein.",
      "startOffset" : 43,
      "endOffset" : 1090
    }, {
      "referenceID" : 0,
      "context" : "For example, a simple way is to discard the least recently used memory, as in the work of Graves et al. (2016) and Gulcehre et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "For example, a simple way is to discard the least recently used memory, as in the work of Graves et al. (2016) and Gulcehre et al. (2016). Another way is to approximate with fixed number of bits the read function that takes a head position and returns the read value.",
      "startOffset" : 90,
      "endOffset" : 138
    } ],
    "year" : 2017,
    "abstractText" : "External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al., 2014; Weston et al., 2014). These models generally utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the storage necessary for computational tasks. In this work, we argue that these neural memory systems lack specific structure important for relative indexing, and propose an alternative model, Lieaccess memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintaining differentiability. To experiment with this approach, we implement a simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.",
    "creator" : "TeX"
  }
}