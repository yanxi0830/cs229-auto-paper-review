{
  "name" : "503.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "jearevaloo@unal.edu.co", "solorio@cs.uh.edu", "smmontesg@inaoep.mx", "fagonzalezo@unal.edu.co" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Representation learning methods have received a lot of attention by researchers and practitioners because of its successful application to complex problems in areas such as computer vision, speech recognition and text processing (LeCun et al., 2015). Most of these efforts have concentrated on data involving one type of information (images, text, speech, etc.), despite data being naturally multimodal. Multimodality refers to the fact that the same real-world concept can be described by different views or data types. Collaborative encyclopedias (such as Wikipedia) describe a famous person through a mixture of text, images and, in some cases, audio. Users from social networks comment events like concerts or sport games with small phrases and multimedia attachments (images/videos/audios). Medical records are represented by a collection of images, sound, text and signals, among others. The increasing availability of multimodal databases from different sources has motivated the development of automatic analysis techniques to exploit the potential of these data as a source of knowledge in the form of patterns and structures that reveal complex relationships (Bhatt & Kankanhalli, 2011; Atrey et al., 2010). In recent years, multimodal tasks have acquired attention by the representation learning community. Strategies for visual question answering (Antol et al., 2015), or image captioning (Vinyals et al., 2015; Xu et al., 2015; Johnson et al., 2015) have developed interesting ways of combining different representation learning architectures.\nMost of these models are focused on mapping from one modality to another or solving an auxiliary task to create a common representation with the information of all modalities. In this work, we design a novel module that combines multiple sources of information, which is optimized with respect to the end goal objective function. Our proposed module is based on the idea of gates for selecting which parts of the input are more likely to contribute for correctly generating the desired output. We\nuse multiplicative gates that assign importance to various features simultaneously, creating a rich multimodal representation that does not require manual tuning, but instead it learns directly from the training data. Our gated model can be reused in different network architectures for solving different tasks, and can be optimized end-to-end with other modules in the architecture using standard gradient-based optimization algorithms.\nAs an application use case, we explore the task of identifying a movie genre based on its plot and its poster. Genre classification has several application areas like document categorization (Kanaris & Stamatatos, 2009), recommendation systems (Makita & Lenskiy, 2016a), and information retrieval systems, among others. Figure 1 depicts the challenging task of assigning genres to a particular movie based solely on the usage of one modality. Such predictions were done with MaxoutMLP w2v and VGG transfer approaches (See Section 3), both of them are models based on representation learning. It can be seen that even a human might be confused if both modalities are not available. The main hypothesis of this work is that a model using gating units, in contrast to a hand-coded multimodal fusion architecture, will be able to learn an input-dependent gate-activation pattern that determines how each modality contribute to the output of hidden units.\nThe rest of the paper is organized as follows: Section 2 presents a literature review and some considerations of the previous work. Section 3 describes the methods used as baseline as well as our representation-leaning-based model proposed. Section 4 presents the experimental evaluation setup along with the details of the MM-IMDb dataset. Section 5 shows and discusses the results for movie genre classification. Finally, Section 6 draws the conclusions and future work."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : ""
    }, {
      "heading" : "2.1 MULTIMODAL FUSION",
      "text" : "Different reviews (Atrey et al., 2010; Bhatt & Kankanhalli, 2011; Li Deng, 2014; Deng, 2014) have summarized strategies that addressed multimodal analysis. Most of the collected works claimed the superiority of multimodal over unimodal approaches for automatic analysis tasks. A conventional multimodal analysis system receives as input two or more modalities that describe a particular concept. The most common multimodal sources are video, audio, images and text. In recent years there has been a consensus with respect to the use of representation learning models to characterize the information of this kind of sources (LeCun et al., 2015). However, the way that such extracted features are combined is still in exploration.\nMultimodal combination seeks to generate a single representation that makes easier automatic analysis tasks when building classifiers or other predictors. A simple approach is to concatenate features to get a final representation (Kiela & Bottou, 2014; Pei et al., 2013; Suk & Shen, 2013). Although it is a straightforward strategy, it ignores inherent correlations between different modalities.\nMore complex fusion strategies include Restricted Boltzmann Machines (RBMs) and autoencoders. Ngiam et al. (2011) concatenated higher level representations and train two RBMs to reconstruct the original audio and video representations respectively. Additionally, they trained a model to recon-\nstruct both modalities given only one of them as input. In an interesting result, Ngiam et al. (2011) were able to mimic a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception known as McGurk effect. A similar approach was proposed by Srivastava & Salakhutdinov (2012). They modified feature learning and reconstruction phases with Deep Boltzmann Machines. Authors claimed that such strategy is able to exploit large amounts of unlabeled data by improving the performance in retrieval and annotation tasks. Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.\nAn alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014). These strategies usually assume that there exists a common latent space where modalities can express the same semantic concept through a set of transformations of the raw data. The semantic embedding representations are such that two concepts are similar if and only if their semantic embeddings are close (Norouzi et al., 2014). In (Socher et al., 2013) a multimodal strategy to perform zero-shot classification was proposed. They trained a word-based neural network model (Huang et al., 2012) to represent textual information, whilst use unsupervised feature learning models proposed in (Coates & Ng, 2011) to get image representation. The fusion was done by learning an image linear mapping to project images into the semantic word space learned in the neural network model. Additionally a Bayesian framework was included to decide whether an image is of a seen or unseen class. Frome et al. (2013) learn the image representation using a CNN trained with the Imagenet dataset and a word-based neural language model (Mikolov et al., 2013b) to represent the textual modality. To perform the fusion they re-train CNN using text representation as targets. This work outperforms scalability with respect to (Socher et al., 2013) from 2 to 20,000 unknown classes in the zero-shot learning task. A modified strategy of Frome et al. (2013) was presented by Norouzi et al. (2014). Instead of re-train the CNN network, they built a convex combination with probabilities estimated by the classifier and semantic embedding vector of the unseen label. This simple strategy outperforms state-of-the-art results. Because the cost function involves both multimodal combination and supervision, these family of models are tied to the task of interest. Thus, if the domain or task conditions changes, adaptations are required.\nThe proposed model is closely related to the mixture of experts (MoE) approach (Jacobs et al., 1991). However, the common usage of MoE is focused on performing decision fusion, i.e. combining predictors to address a supervised learning problem (Yuksel et al., 2012). Our model is devised as a new component in the representation learning scheme, making it independent from the final task (e.g. classification, regression, unsupervised learning, etc) provided that the defined cost function be differentiable."
    }, {
      "heading" : "2.2 MOVIE GENRE CLASSIFICATION",
      "text" : "With respect to movie genre classification, several strategies also have been proposed. These strategies have used different modalities to characterize each movie, such as textual features, image features and multimedia features (audio and/or video). Huang et al. (2007) were one of the first teams exploring this task. They classified movie previews into 3 genres by extracting handcrafted features from the video and training a decision tree classifier. They evaluated the model using 44 films. Using only textual modality, Shah et al. (2013) performed single-label genre classification of movie scripts using clustering algorithms with 260 movies. Later, combining two modalities, Pais et al. (2012) classified movies between drama and non-drama using visual and textual features with 107 samples. Hong & Hwang (2015) explored different PLSA models to combine 3 modalities: audio, image and text to predict genre of movie previews. It was single label classification with 4 genres for 140 movies taken from IMDb.\nRecently, Fu et al. (2015) used a set of handcrafted visual features for poster characterization and bag-of-words for synopsis. Then, they trained one SVM per each modality to combine their predictions. The dataset contained 2, 400 movies with one genre (out of 4) each.\nThe previous mentioned works present this problem in a single label setup. However, a more realistic scenario would be multilabel, since most of the movies belong to more than one genre, (e.g.\nMatrix(2000) is a Sci-fi/Action movie). In this setup, Anand (2014) explores the efficiency of using keywords and users’ tags to perform multilabeling using the movies from MovieLens 1M dataset which contains 1, 700 movies. Also Ivasic-Kos et al. (2014; 2015) performed multilabel classification using handcrafted features from posters, with 1, 500 samples for 6 genres. Makita & Lenskiy (2016a;b) use movie ratings matrix and genre correlation matrix to predict the genre. It used a smaller version of the Movielens dataset with 18 movie genres.\nMost of the above works have used the publicly available MovieLens datasets. However, there is not a single experimental setup defined so that all methods can be systematically compared. Also, to the best of our knowledge, none of the previous works contain more than 10, 000 samples. With this work we will release a dataset created with the movies of the MovieLens 20M dataset. We include not only genre, poster and plot information used in this work, but also the poster of the movie as well as more than 50 characteristics taken from the IMDb website. We will also release the source code to automatically add more movies and genres."
    }, {
      "heading" : "3 METHODS",
      "text" : "This paper presents a neural-network-based strategy for multilabel classification of multimodal data. The key component of the strategy is a novel type of hidden unit, the Gated Multimodal Unit (GMU), which learns to decide how modalities influence the activation of the unit using gates. The details of the GMU are presented in Subsection 3.1.\nStatistical properties usually are not shared across modalities (Srivastava & Salakhutdinov, 2012). And thus, they require different representation strategies according to the nature of data. This work explored several strategies to address text and visual representation. For text information we evaluated word2vec models, n-grams models and RNN models. The details are discussed in Subsection 3.2. On the other hand, two different convolutional neural networks were evaluated for processing visual data and are presented in Subsection 3.3."
    }, {
      "heading" : "3.1 GATED MULTIMODAL UNIT FOR MULTIMODAL FUSION",
      "text" : "Multimodal learning is closely related to data fusion. Data fusion looks for optimal ways of combining different information sources into an integrated representation that provides more information than the individual sources (Bhatt & Kankanhalli, 2011). This fusion can be performed at different levels, that can be categorized into two broad categories: feature fusion and decision fusion. Feature fusion, also called early fusion, looks for a subset of features from different modalities, or combinations of them, that better represent the information needed to solve a particular problem. On the other hand, decision fusion, or late fusion, combines decisions from different systems, e.g. classifiers, to produce consensus. This consensus may be reached by a simple average, a voting system or a more complex Bayesian framework.\nIn this work we present a model, based on gated neural networks, for data fusion that combines ideas from both feature and decision fusion. The model, called Gated Multimodal Unit (GMU), is inspired by the flow control in recurrent architectures like GRU or LSTM. A GMU is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. Figure 2.a depicts the structure of a GMU. Each xi corresponds to a feature vector associated with modality i. Each feature vector feeds a neuron with a tanh activation function, which is intended to encode an internal representation feature based on the particular modality. For each input modality, xi, there is a gate neuron (represented by σ nodes in the diagram), which controls the contribution of the feature calculated from xi to the overall output of the unit. When a new sample is fed to the network, a gate neuron associated to modality i receives as input the feature vectors from all the modalities and uses them to decide whether the modality i may contribute, or not, to the internal encoding of the particular input sample.\nFigure 2.b shows a simplified version of the GMU for two input modalities, xv (visual modality) and xt (textual modality), that will be used in the remaining of the paper. It should be noted that both models are not completely equivalent, since in the bimodal case the gates are tied. Such weight tying constraints the model, so that the units trade off between both modalities while they use less\nparameters than the multimodal case. The equations governing this GMU are as follows:\nhv = tanh (Wv · xv) ht = tanh (Wt · xt) z = σ (Wz · [xv, xt]) h = z ∗ hv + (1− z) ∗ ht Θ = {Wv,Wt,Wz}\nwith Θ the parameters to be learned and [·, ·] the concatenation operator. Since all are differentiable operations, this model can be easily coupled with other neural network architectures and trained with stochastic gradient descent."
    }, {
      "heading" : "3.2 TEXT REPRESENTATION",
      "text" : "Text representation is a critical step when classification tasks are addressed using machine learning methods. Traditional approaches are based on counting frequencies of n-gram occurrences such as words or sequences of characters (e.g. bag-of-words models). The main drawback of such approaches is the difficulty to model relationships between words and their context. An alternative approach was initially proposed by Bengio et al. (2003), by building a language model based on a neural network architecture (NNLM). The NNLM was able to learn distributed representations of words that capture contextual information. Later, this model was simplified to deal with large corpora by removing hidden layers in the neural network architecture (word2vec) (Mikolov et al., 2013a). This is a fully unsupervised model that takes advantage of large sets of unlabeled documents. Herein, three text representations were evaluated:\nn-gram Following the strategy proposed by Kanaris & Stamatatos (2009), we used the n-gram strategy for representing text. Despite their simplicity, n-gram models have shown to be a competitive baseline.\nWord2Vec Word2vec is an unsupervised learning algorithm that finds a vector representation for each word based on its context (Mikolov et al., 2013a). It has been shown that this model is able to find semantic and syntactic relationships using arithmetic operations between the vectors. Based on this property, we represent a movie as the average of the vectors of words in the plot outline. The main motivation to aggregate word2vec vectors is the property of additive compositionality that this representation has exposed over different set of tasks such as word analogies. The usual way to aggregate is to sum vectors. We instead take the average to avoid large input values to the neural network.\nRecurrent neural network Here we take the plot outline as a sequence of words and train a supervised recurrent neural network. We evaluated two variants. The first one (RNN w2v) is a transfer learning model that takes as input the word vectors of word2vec as representations. The second one learns the word vectors from scratch (RNN end2end)."
    }, {
      "heading" : "3.3 VISUAL REPRESENTATION",
      "text" : "In computer vision tasks, Convolutional neural networks have become the de facto standard. It has been shown that CNN models trained with a huge amount of data are able to learn common features shared across different domains. This characteristic is usually exploited by transfer learning approaches. For visual representation we explored 2 strategies: transfer learning and end-to-end training.\nVGG Transfer In this approach, the VGG Network (Simonyan & Zisserman, 2014) trained with the ImageNet dataset is used as feature extractor by taking the last hidden activations as the visual representation.\nEnd2End CNN Here, a CNN with 5 convolutional layers and an MLP (see Section 3.4) on top was trained from scratch."
    }, {
      "heading" : "3.4 CLASSIFICATION MODEL",
      "text" : "Based on the defined representation, we explored two methods to map from feature vectors to genre classification. In particular we explored a simple Logistic regression and a neural network architecture. This is a multilayer perceptron (MLP) with two fully connected layers and maxout activation function. In particular, the maxout activation function hi : Rn → R is a defined as:\nhi (s) = max j∈[1,k] zi,j (1)\nwhere s ∈ Rn is the input vector, zi,j = sTW···ij+bij is the output of the j-th linear transformation of the i-th hidden unit, and W ∈ Rd×m×k and b ∈ Rm×k are learned parameters. It has been shown that maxout models with just 2 hidden units behave as universal approximators, while are less prone to saturate units (Goodfellow et al., 2013).\n0 100 200 300 400 500 600 size (pixels)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n# o\nf sa\nm p le\ns\nwidth height\nFigure 4: Size distribution of movie posters. 0 100 200 300 400 500 600 # of words\n0\n500\n1000\n1500\n2000\n2500\n3000\n# o\nf sa\nm p le\ns\nFigure 5: Length distribution of movie plots."
    }, {
      "heading" : "4 EXPERIMENTAL EVALUATION",
      "text" : ""
    }, {
      "heading" : "4.1 MULTIMODAL IMDB DATASET",
      "text" : "With this work we will make publicly available the Multimodal IMDb (MM-IMDb)1 dataset. MMIMDb dataset is built with the IMDb id’s provided by the Movielens 20M dataset 2 that contains ratings of 27, 000 movies. Using the IMDbPY 3 library, movies which do not contain their poster image were filtered out. As the final result, the MM-IMDb dataset comprises 25, 959 movies along with their plot, poster, genres and other 50 additional metadata fields such as year, language, writer, director, aspect ratio, etc.\nNotice that one movie may belong to more than one genre. Figure 3 shows the co-occurrence matrix, where the color bar indicates the representative co-occurrence per row, while Figure 4 and Figure 5 depict the distribution of the movie poster sizes and length of movie plots respectively. Each plot contains on average 92.5 words, while the longest one contains 1, 431 words and the average of genres per movie is 2.48. In this work, we defined the task of movie genre prediction based on its plot and image poster. Nevertheless, the additional metadata information encourages other interesting tasks such as rating prediction and content-based retrieval, among others."
    }, {
      "heading" : "4.2 EXPERIMENTAL SETUP",
      "text" : "The MM-IMDb dataset has been split in three subsets. Train, development and test subsets contain 15552, 2608 and 7799 respectively. The distribution of samples is listed in Table 1. The sample was stratified so that training, dev and test sets comprises 60%, 10%, 30% samples of each genre respectively.\nIn the multilabel classification the performance evaluation can be more complex than traditional multi-class classification and the differences can be significant among several measures (Madjarov et al., 2012). Herein, four averages of the f-score (f1) are reported: samples computes the f-score per sample and then averages the results, micro computes the f-score using all predictions at once, macro computes the f-score per genre and then averages the results. weighted is the same as macro with a weighted average based on the number of positive samples per genre. Concretely, we calculate them as follows (Madjarov et al., 2012):\nfsample1 = 1\nN N∑ i=1 2× |ŷi ∩ yi| |ŷi|+ |yi| fmacro1 = 1 Q ∑Q j=1 2×pj×rj pj+rj fweighted1 = 1 Q2 Q∑ j=1 Qj 2× pj × rj pj + rj\n1http://lisi1.unal.edu.co/mmimdb/ 2http://grouplens.org/datasets/movielens/ 3http://imdbpy.sourceforge.net/\nTable 1: Genre distribution per subset\nGenre Train Dev Test Genre Train Dev Test\nDrama 8424 1401 4142 Family 978 172 518 Comedy 5108 873 2611 Biography 788 144 411 Romance 3226 548 1590 War 806 128 401 Thriller 3113 512 1567 History 680 118 345 Crime 2293 382 1163 Music 634 100 311 Action 2155 351 1044 Animation 586 105 306 Adventure 1611 278 821 Musical 503 85 253 Horror 1603 275 825 Western 423 72 210 Documentary 1234 219 629 Sport 379 64 191 Mystery 1231 209 617 Short 281 48 142 Sci-Fi 1212 193 586 Film-Noir 202 34 102 Fantasy 1162 186 585\npmicro = ∑Q j=1 tpj∑Q\nj=1 tpj + ∑Q j=1 fpj rmicro =\n∑Q j=1 tpj∑Q\nj=1 tpj+ ∑Q j=1 fnj fmicro1 = 2× pmicro × rmicro pmicro + rmicro\nWith N the number of examples; Q the number of labels; Qj the number of true instances for the j-th label; p the precision, r the recall; ŷi, yi ∈ (0, 1)Q the prediction and ground truth binary tuples respectively; tpj , fpjandfnj the number of true positives, false positives and false negatives for the j-th label respectively.\nTEXTUAL REPRESENTATION\nThe pretrained Google Word2vec4 embedding space was used. After intersecting the Google word2vec available words with the MM-IMDb plots, the final vocabulary contains 41,612 words. Other than lowercase, no text preprocessing was applied. Since it is our intention to measure how the network’s depth affects the performance of the model, we also evaluate the architecture with a single fully connected layer. In order to compare the performance of this textual representation, we evaluate it using two publicly available datasets: 7genre dataset that comprises 1,400 web pages with 7 disjoint genres and ki-04 dataset that comprises 1,239 samples classified under 8 genres. We compare the model with the state of the art results (Kanaris & Stamatatos, 2009) which used character n-grams with structured information from the HTML tags to predict the genre of web pages.\nVISUAL REPRESENTATION\nSince the first approach was to use VGG as a feature extractor. This model is referred as VGG Transfer. The second approach takes as input the raw images to a CNN. Since all the images do not have the same size, all images were scaled, and cropped when required, to 160 × 256 pixels keeping the aspect ratio. This CNN comprises 5 CNN layers of 5, 3, 3, 3, 3 squared filters and 2 × 2 pool sizes. Each convolutional layer has 16 hidden units. The convolutional layers are connected with the MaxoutMLP on top.\nMULTIMODAL REPRESENTATION\nWe evaluate 4 different ways to combine both modalities as baselines.\nAverage probability This can be seen as a late-fusion strategy. The probabilities obtained by the best model of each modality are averaged and thresholded.\nconcatenation Different works have found that a simple concatenation of representations of different modalities are good for combining the information (Suk & Shen, 2013; Pei et al., 2013; Kiela & Bottou, 2014). Herein, we concatenated both representations to train the MaxoutMLP architecture.\n4https://code.google.com/archive/p/word2vec/\nlinear sum Following the way Vinyals et al. (2015) combine text and images representation into a single space, this model adds a linear transformation for each modality so that both outputs have the same size to be summed up and then followed by the MaxoutMLP architecture.\nMoE The mixture of experts (MoE) (Jacobs et al., 1991) model was adapted for multilabel classification. two gating strategies were explored: tied, where a single gate multiplies all the logistics outputs, and untied where every logistic output has its own gate. Logistic regression and MaxoutMLP were evaluated as experts.\nNEURAL NETWORK TRAINING\nNeural network models were trained using using Batch Normalization scheme (Ioffe & Szegedy, 2015). This strategy applies a normalization step across samples that belong to the same batch, so that each hidden unit in the network receive a zero-mean and unit variance. Stochastic gradient descent with ADAM optimization (Kingma & Ba, 2014) was used to learn the weights of the neural network. Dropout and max-norm regularization were used to control overfitting. Hidden size ({64, 128, 256, 512}), learning rate ( [ 10−3, 10−1 ] ), dropout ([0.3, 0.7]), max-norm ([5, 20]) and\ninitialization ranges ( [ 10−3, 10−1 ] ) parameters were explored by training 25 models with random (uniform) hyperparameter initializations and the best was chosen according to validation performance. It has been reported that this strategy is preferable over grid search when training deep models (Bergstra & Bengio, 2012). All the implementation was carried on with the Blocks framework (Van Merriënboer et al., 2015)5.\nDuring the training process, we noticed that batch normalization considerably helped in terms of training time and convergence, resulting in less sensitivity to hyperparameters such as initialization ranges or learning rate. Also, dropout and max-norm regularization strategies helped to increase the performance at test time."
    }, {
      "heading" : "5 RESULTS",
      "text" : ""
    }, {
      "heading" : "5.1 EVALUATION OVER SYNTHETIC DATA",
      "text" : "In order to evaluate if the model is able to identify which modality is contributing more information to classify a particular sample, we created a synthetic task based on a generative model, which is depicted in Figure 6. In this model we define the random binary variable C as the target and xv, xt ∈ R2 as the input features. M is a random binary variable that decides which modality will contain the relevant information that determines the class. The input features of each modality can\n5https://github.com/johnarevalo/gmu-mmimdb\nbe generated by a random source, ŷv and ŷt, or by an informed source, yv and yt. The generative model is specified as follows:\nC ∼ Bernoulli(pC) M ∼ Bernoulli(pM ) yv ∼ N (γCv ) ŷv ∼ N (γ̂v)\nxv = Myv + (1−M)ŷv yt ∼ N (γCt ) ŷt ∼ N (γ̂t) xt = Mŷt + (1−M)yt\nWe trained a model with a single GMU and applied a sigmoid function over h, then the binary cross entropy was used as loss function. Using the generative model, 200 samples per class were generated for each experiment. 1000 synthetic experiments with different random seeds were run and the GMU outperformed a logistic regression classifier in 370 of them, while obtaining equal results in the remainder ones. Our goal in these simulations was to show that the model was able to learn a latent variable that determines which modality carries the useful information for the classification. An interesting result is that between M and the activations of the gate z there is a correlation of 1. This means the model was capable of learning such latent variable by only observing the xv and xt input features.\nWe also wanted to project back the z activations to the feature space in order to visualize regions depending on the modality. Figure 7 shows the activations in a synthetic experiment generated by the setup of Figure 6 for xv, xt ∈ R1. Each axis represents a modality, red and blue dots are the samples generated for the two classes and black Gaussian curves represent the γ̂v and γ̂t noises. The contour of the left figure (gray) represents the activation of z. Notice that in white regions (z = 1), the model gives more importance to the xv modality while in gray regions (z = 0) the xt modality is more relevant; i.e. the z gate is isolating the noise. The contour of the right figure (blue-red) represents the model prediction. It is noteworthy that the boundary defined by the gates still holds when the model solves the task. This also encourages the inclusion of non-linearities to the z gate so that it is able to discriminate more complex interactions between modalities."
    }, {
      "heading" : "5.2 GENRE CLASSIFICATION RESULTS",
      "text" : "Before using our text representation in the multimodal task, we wanted to be sure such representation was good enough to address the genre classification task. Thus, we evaluated it on 2 public datasets. We found MaxoutMLP w2v achieves the state of the art results on the ki-04 dataset and increases the performance in the 7Genre dataset from 0.841 to 0.854 (Kanaris & Stamatatos, 2009). Notice\nthat the baseline uses additional information from the HTML structure from the web page, while this representation uses only the text data.\nTable 2 shows the results in the proposed dataset. For the textual modality, the best performance is obtained by the combination of word2vec representation with an MLP classifier. The behavior of all representation methods are consistent across the performance measures. Learning from scratch the RNN model performed the worst. We hypothesize this has to do with the lack of data to learn meaningful relations among words. It has been shown that millions of words are required to train a model such as word2vec that is able to exploit common regularities between word co-occurrences.\nFor the visual modality, the usage of pretrained models works better than training the model from scratch. It seems it is still a small dataset to learn all the complexities of the posters. Now, comparing the performance independently per genre, as in Table 3, it is interesting to notice that in Animation the visual modality outperforms the textual one.\nIn the multimodal scenario, by adding the GMU as building block to learn the fusion we obtained the best performance, improving independent modalities in the averaged measures and in 16 of out 23 genres and outperforming all other evaluated fusion strategies. The concatenation or the linear\ncombination approaches were not enough to model the correlation between the modalities and MoE models did not perform better than simpler approaches. This is an expected behavior for MoE in a relatively small dataset because the data is fractionated over different experts, and thus it doesn’t make an efficient use of the training samples.\nIn order to evaluate which modality influences more the model when assigning a particular label, we averaged the activations of a subset of z gates of the test samples to which the model assigned them such label. We counted the number of samples that pays more attention to the textual modality (z <= 0.5) or to the visual modality (z > 0.5). The units were chosen taking into account the mutual information between the predictions and the z activations. The result of this analysis is depicted in Figure 8. As expected, the model is generally more influenced by the textual modality. But, in particular cases such as Animation or Family genres, the visual modality affects more the model. This is also consistent with results of Table 3 which reports better performances for visual modality.\nWe wanted to qualitative explore test examples in which performance was improved by a relative large margin. Table 4 illustrates cases where the model takes advantage of the most accurate modality, and in some cases removes false positives. It is noteworthy that some of these examples can be confusing for a human if one modality is missing, or additional context information is not given."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "This work presented a strategy to learn fusion transformations from multimodal sources. Similarly to the way recurrent models control the information flow, the proposed model is based on multiplicative gates. The Gated Multimodal Unit (GMU) receives two or more input sources and learns to determine how much each input modality affects the unit activation. In synthetic experiments the GMU was able to learn hidden latent variables, and in a real scenario it outperformed the singlemodality approaches. An interesting property of GMU is that, being a differentiable operation, it\nis easily coupled in any other neural network architecture and trained with standard gradient-based optimization algorithms. With this work we will also release a new dataset that contains around 27, 000 movie plots, images and other metadata. To the best of our knowledge, this is the biggest dataset used to perform movie genre classification based on multimodal information and the first one to be publicly available. In our future work we expect to explore deep architectures of GMU layers as well as integration with attention mechanism over the input modalities. Also, It will be interesting to explore in more depth the interpretability of the learned features."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "Arevalo thanks Colciencias for its support through a doctoral grant in call 617/2013. The authors also thank for K40 Tesla GPU donated by NVIDIA and which was used for some representation learning experiments."
    } ],
    "references" : [ {
      "title" : "Zero-Shot Learning with Structured Embeddings",
      "author" : [ "Zeynep Akata", "Honglak Lee", "Bernt Schiele" ],
      "venue" : "CoRR, abs/1409.8,",
      "citeRegEx" : "Akata et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Akata et al\\.",
      "year" : 2014
    }, {
      "title" : "Evaluating folksonomy information sources for genre prediction",
      "author" : [ "Deepa Anand" ],
      "venue" : "In Advance Computing Conference (IACC),",
      "citeRegEx" : "Anand.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anand.",
      "year" : 2014
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal fusion for multimedia analysis: a survey",
      "author" : [ "Pradeep K. Atrey", "M. Anwar Hossain", "Abdulmotaleb El Saddik", "Mohan S. Kankanhalli" ],
      "venue" : "Multimedia Systems,",
      "citeRegEx" : "Atrey et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Atrey et al\\.",
      "year" : 2010
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "James Bergstra", "Yoshua Bengio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bergstra and Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra and Bengio.",
      "year" : 2012
    }, {
      "title" : "Multimedia data mining: state of the art and challenges",
      "author" : [ "Chidansh Bhatt", "Mohan Kankanhalli" ],
      "venue" : "Multimedia Tools and Applications,",
      "citeRegEx" : "Bhatt and Kankanhalli.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bhatt and Kankanhalli.",
      "year" : 2011
    }, {
      "title" : "The importance of encoding versus training with sparse coding and vector quantization",
      "author" : [ "Adam Coates", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Coates and Ng.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates and Ng.",
      "year" : 2011
    }, {
      "title" : "A tutorial survey of architectures, algorithms, and applications for deep learning",
      "author" : [ "Li Deng" ],
      "venue" : "APSIPA Transactions on Signal and Information Processing,",
      "citeRegEx" : "Deng.,? \\Q2014\\E",
      "shortCiteRegEx" : "Deng.",
      "year" : 2014
    }, {
      "title" : "Constructing hierarchical image-tags bimodal representations for word tags alternative choice",
      "author" : [ "Fangxiang Feng", "Ruifan Li", "Xiaojie Wang" ],
      "venue" : "arXiv preprint arXiv:1307.1275,",
      "citeRegEx" : "Feng et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2013
    }, {
      "title" : "Multimodal PLSA for Movie Genre Classification",
      "author" : [ "Hao-Zhi Hong", "Jen-Ing G Hwang" ],
      "venue" : null,
      "citeRegEx" : "Hong and Hwang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hong and Hwang.",
      "year" : 2015
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume",
      "citeRegEx" : "Huang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "A Film Classifier Based on Low-level Visual Features",
      "author" : [ "Hui-Yu Huang", "Weir-Sheng Shih", "Wen-Hsing Hsu" ],
      "venue" : "IEEE 9th Workshop on Multimedia Signal Processing,",
      "citeRegEx" : "Huang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2007
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Movie posters classification into genres based on low-level features",
      "author" : [ "Marina Ivasic-Kos", "Miran Pobar", "Luka Mikec" ],
      "venue" : "doi: 10.1109/MIPRO.2014.6859750. URL http: //ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6859750",
      "citeRegEx" : "Ivasic.Kos et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ivasic.Kos et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic Movie Posters Classification into Genres",
      "author" : [ "Marina Ivasic-Kos", "Miran Pobar", "Ivo Ipsic" ],
      "venue" : null,
      "citeRegEx" : "Ivasic.Kos et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ivasic.Kos et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive mixtures of local experts",
      "author" : [ "Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Jacobs et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Jacobs et al\\.",
      "year" : 1991
    }, {
      "title" : "Densecap: Fully convolutional localization networks for dense captioning",
      "author" : [ "Justin Johnson", "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1511.07571,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to recognize webpage genres",
      "author" : [ "Ioannis Kanaris", "Efstathios Stamatatos" ],
      "venue" : "Information Processing and Management,",
      "citeRegEx" : "Kanaris and Stamatatos.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kanaris and Stamatatos.",
      "year" : 2009
    }, {
      "title" : "Deep learning to hash with multiple representations",
      "author" : [ "Yoonseop Kang", "Saehoon Kim", "Seungjin Choi" ],
      "venue" : "In 2012 IEEE 12th International Conference on Data Mining,",
      "citeRegEx" : "Kang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics",
      "author" : [ "Douwe Kiela", "Léon Bottou" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14),",
      "citeRegEx" : "Kiela and Bottou.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiela and Bottou.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Kiros et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel" ],
      "venue" : "arXiv preprint arXiv:1411.2539,",
      "citeRegEx" : "Kiros et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Learning: Methods and Applications",
      "author" : [ "Dong Yu Li Deng" ],
      "venue" : "NOW Publishers,",
      "citeRegEx" : "Deng.,? \\Q2014\\E",
      "shortCiteRegEx" : "Deng.",
      "year" : 2014
    }, {
      "title" : "Learning multimodal neural network with ranking examples",
      "author" : [ "Xinyan Lu", "Fei Wu", "Xi Li", "Yin Zhang", "Weiming Lu", "Donghui Wang", "Yueting Zhuang" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Multimedia,",
      "citeRegEx" : "Lu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2014
    }, {
      "title" : "An extensive experimental comparison of methods for multi-label learning",
      "author" : [ "Gjorgji Madjarov", "Dragi Kocev", "Dejan Gjorgjevikj", "Sašo Džeroski" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Madjarov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Madjarov et al\\.",
      "year" : 2012
    }, {
      "title" : "A movie genre prediction based on Multivariate Bernoulli model and genre correlations",
      "author" : [ "Eric Makita", "Artem Lenskiy" ],
      "venue" : "(May), mar 2016a. URL http://arxiv.org/abs/1604.08608",
      "citeRegEx" : "Makita and Lenskiy.,? \\Q2016\\E",
      "shortCiteRegEx" : "Makita and Lenskiy.",
      "year" : 2016
    }, {
      "title" : "Lenskiy. A multinomial probabilistic model for movie genre predictions",
      "author" : [ "Eric Makita", "Artem" ],
      "venue" : "2016b. URL http://arxiv.org/abs/1603.07849",
      "citeRegEx" : "Makita and Artem,? \\Q2016\\E",
      "shortCiteRegEx" : "Makita and Artem",
      "year" : 2016
    }, {
      "title" : "Explain images with multimodal recurrent neural networks",
      "author" : [ "Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille" ],
      "venue" : "arXiv preprint arXiv:1410.1090,",
      "citeRegEx" : "Mao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Multimodal Deep Learning",
      "author" : [ "J Ngiam", "A Khosla", "M Kim" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Ngiam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam et al\\.",
      "year" : 2011
    }, {
      "title" : "Animated movie genre detection using symbolic fusion of text and image descriptors",
      "author" : [ "Gregory Pais", "Patrick Lambert", "Daniel Beauchene", "Francoise Deloule", "Bogdan Ionescu" ],
      "venue" : "In 2012 10th International Workshop on Content-Based Multimedia Indexing (CBMI),",
      "citeRegEx" : "Pais et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pais et al\\.",
      "year" : 2012
    }, {
      "title" : "Unsupervised multimodal feature learning for semantic image segmentation",
      "author" : [ "Deli Pei", "Huaping Liu", "Yulong Liu", "Fuchun Sun" ],
      "venue" : "In The 2013 International Joint Conference on Neural Networks (IJCNN),",
      "citeRegEx" : "Pei et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2013
    }, {
      "title" : "Movie Classification Using k-Means and Hierarchical Clustering",
      "author" : [ "Dharak Shah", "Saheb Motiani", "Vishrut Patel" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Shah et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Zero-Shot Learning Through Cross-Modal Transfer",
      "author" : [ "Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Grounded Compositional Semantics for Finding and Describing Images with Sentences",
      "author" : [ "Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL),",
      "citeRegEx" : "Socher et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal Learning with Deep Boltzmann Machines",
      "author" : [ "Nitish Srivastava", "Ruslan Salakhutdinov" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Srivastava and Salakhutdinov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Srivastava and Salakhutdinov.",
      "year" : 2012
    }, {
      "title" : "Deep learning-based feature representation for AD/MCI classification",
      "author" : [ "Heung Il Suk", "Dinggang Shen" ],
      "venue" : "In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),",
      "citeRegEx" : "Suk and Shen.,? \\Q2013\\E",
      "shortCiteRegEx" : "Suk and Shen.",
      "year" : 2013
    }, {
      "title" : "Challenge Huawei challenge: Fusing multimodal features with deep neural networks for Mobile Video Annotation",
      "author" : [ "Jian Tu", "Zuxuan Wu", "Qi Dai", "Yu-Gang Jiang", "Xiangyang Xue" ],
      "venue" : "In Multimedia and Expo Workshops (ICMEW),",
      "citeRegEx" : "Tu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2014
    }, {
      "title" : "Blocks and fuel: Frameworks for deep learning",
      "author" : [ "Bart Van Merriënboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David WardeFarley", "Jan Chorowski", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1506.00619,",
      "citeRegEx" : "Merriënboer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Merriënboer et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Online multimodal deep similarity learning with application to image retrieval",
      "author" : [ "Pengcheng Wu", "Steven C.H. Hoi", "Hao Xia", "Peilin Zhao", "Dayong Wang", "Chunyan Miao" ],
      "venue" : "In Proceedings of the 21st ACM international conference on Multimedia - MM ’13,",
      "citeRegEx" : "Wu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2013
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Twenty years of mixture of experts",
      "author" : [ "Seniha Esen Yuksel", "Joseph N Wilson", "Paul D Gader" ],
      "venue" : "IEEE transactions on neural networks and learning systems,",
      "citeRegEx" : "Yuksel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yuksel et al\\.",
      "year" : 2012
    }, {
      "title" : "Topic Modeling of Multimodal Data: an Autoregressive Approach",
      "author" : [ "Yin Zheng", "YJ Zhang", "Hugo Larochelle" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Zheng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The increasing availability of multimodal databases from different sources has motivated the development of automatic analysis techniques to exploit the potential of these data as a source of knowledge in the form of patterns and structures that reveal complex relationships (Bhatt & Kankanhalli, 2011; Atrey et al., 2010).",
      "startOffset" : 275,
      "endOffset" : 322
    }, {
      "referenceID" : 2,
      "context" : "Strategies for visual question answering (Antol et al., 2015), or image captioning (Vinyals et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 43,
      "context" : ", 2015), or image captioning (Vinyals et al., 2015; Xu et al., 2015; Johnson et al., 2015) have developed interesting ways of combining different representation learning architectures.",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 45,
      "context" : ", 2015), or image captioning (Vinyals et al., 2015; Xu et al., 2015; Johnson et al., 2015) have developed interesting ways of combining different representation learning architectures.",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : ", 2015), or image captioning (Vinyals et al., 2015; Xu et al., 2015; Johnson et al., 2015) have developed interesting ways of combining different representation learning architectures.",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "Different reviews (Atrey et al., 2010; Bhatt & Kankanhalli, 2011; Li Deng, 2014; Deng, 2014) have summarized strategies that addressed multimodal analysis.",
      "startOffset" : 18,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "Different reviews (Atrey et al., 2010; Bhatt & Kankanhalli, 2011; Li Deng, 2014; Deng, 2014) have summarized strategies that addressed multimodal analysis.",
      "startOffset" : 18,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "A simple approach is to concatenate features to get a final representation (Kiela & Bottou, 2014; Pei et al., 2013; Suk & Shen, 2013).",
      "startOffset" : 75,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "Different reviews (Atrey et al., 2010; Bhatt & Kankanhalli, 2011; Li Deng, 2014; Deng, 2014) have summarized strategies that addressed multimodal analysis. Most of the collected works claimed the superiority of multimodal over unimodal approaches for automatic analysis tasks. A conventional multimodal analysis system receives as input two or more modalities that describe a particular concept. The most common multimodal sources are video, audio, images and text. In recent years there has been a consensus with respect to the use of representation learning models to characterize the information of this kind of sources (LeCun et al., 2015). However, the way that such extracted features are combined is still in exploration. Multimodal combination seeks to generate a single representation that makes easier automatic analysis tasks when building classifiers or other predictors. A simple approach is to concatenate features to get a final representation (Kiela & Bottou, 2014; Pei et al., 2013; Suk & Shen, 2013). Although it is a straightforward strategy, it ignores inherent correlations between different modalities. More complex fusion strategies include Restricted Boltzmann Machines (RBMs) and autoencoders. Ngiam et al. (2011) concatenated higher level representations and train two RBMs to reconstruct the original audio and video representations respectively.",
      "startOffset" : 19,
      "endOffset" : 1239
    }, {
      "referenceID" : 9,
      "context" : "Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.",
      "startOffset" : 89,
      "endOffset" : 238
    }, {
      "referenceID" : 19,
      "context" : "Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.",
      "startOffset" : 89,
      "endOffset" : 238
    }, {
      "referenceID" : 25,
      "context" : "Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.",
      "startOffset" : 89,
      "endOffset" : 238
    }, {
      "referenceID" : 29,
      "context" : "Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.",
      "startOffset" : 89,
      "endOffset" : 238
    }, {
      "referenceID" : 41,
      "context" : "Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.",
      "startOffset" : 89,
      "endOffset" : 238
    }, {
      "referenceID" : 44,
      "context" : "Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.",
      "startOffset" : 89,
      "endOffset" : 238
    }, {
      "referenceID" : 0,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 216
    }, {
      "referenceID" : 29,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 216
    }, {
      "referenceID" : 37,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 216
    }, {
      "referenceID" : 47,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 216
    }, {
      "referenceID" : 37,
      "context" : "In (Socher et al., 2013) a multimodal strategy to perform zero-shot classification was proposed.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "They trained a word-based neural network model (Huang et al., 2012) to represent textual information, whilst use unsupervised feature learning models proposed in (Coates & Ng, 2011) to get image representation.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 37,
      "context" : "This work outperforms scalability with respect to (Socher et al., 2013) from 2 to 20,000 unknown classes in the zero-shot learning task.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "The proposed model is closely related to the mixture of experts (MoE) approach (Jacobs et al., 1991).",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "combining predictors to address a supervised learning problem (Yuksel et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "In an interesting result, Ngiam et al. (2011) were able to mimic a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception known as McGurk effect.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "In an interesting result, Ngiam et al. (2011) were able to mimic a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception known as McGurk effect. A similar approach was proposed by Srivastava & Salakhutdinov (2012). They modified feature learning and reconstruction phases with Deep Boltzmann Machines.",
      "startOffset" : 26,
      "endOffset" : 263
    }, {
      "referenceID" : 0,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014). These strategies usually assume that there exists a common latent space where modalities can express the same semantic concept through a set of transformations of the raw data. The semantic embedding representations are such that two concepts are similar if and only if their semantic embeddings are close (Norouzi et al., 2014). In (Socher et al., 2013) a multimodal strategy to perform zero-shot classification was proposed. They trained a word-based neural network model (Huang et al., 2012) to represent textual information, whilst use unsupervised feature learning models proposed in (Coates & Ng, 2011) to get image representation. The fusion was done by learning an image linear mapping to project images into the semantic word space learned in the neural network model. Additionally a Bayesian framework was included to decide whether an image is of a seen or unseen class. Frome et al. (2013) learn the image representation using a CNN trained with the Imagenet dataset and a word-based neural language model (Mikolov et al.",
      "startOffset" : 91,
      "endOffset" : 1120
    }, {
      "referenceID" : 0,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014). These strategies usually assume that there exists a common latent space where modalities can express the same semantic concept through a set of transformations of the raw data. The semantic embedding representations are such that two concepts are similar if and only if their semantic embeddings are close (Norouzi et al., 2014). In (Socher et al., 2013) a multimodal strategy to perform zero-shot classification was proposed. They trained a word-based neural network model (Huang et al., 2012) to represent textual information, whilst use unsupervised feature learning models proposed in (Coates & Ng, 2011) to get image representation. The fusion was done by learning an image linear mapping to project images into the semantic word space learned in the neural network model. Additionally a Bayesian framework was included to decide whether an image is of a seen or unseen class. Frome et al. (2013) learn the image representation using a CNN trained with the Imagenet dataset and a word-based neural language model (Mikolov et al., 2013b) to represent the textual modality. To perform the fusion they re-train CNN using text representation as targets. This work outperforms scalability with respect to (Socher et al., 2013) from 2 to 20,000 unknown classes in the zero-shot learning task. A modified strategy of Frome et al. (2013) was presented by Norouzi et al.",
      "startOffset" : 91,
      "endOffset" : 1553
    }, {
      "referenceID" : 0,
      "context" : "An alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014). These strategies usually assume that there exists a common latent space where modalities can express the same semantic concept through a set of transformations of the raw data. The semantic embedding representations are such that two concepts are similar if and only if their semantic embeddings are close (Norouzi et al., 2014). In (Socher et al., 2013) a multimodal strategy to perform zero-shot classification was proposed. They trained a word-based neural network model (Huang et al., 2012) to represent textual information, whilst use unsupervised feature learning models proposed in (Coates & Ng, 2011) to get image representation. The fusion was done by learning an image linear mapping to project images into the semantic word space learned in the neural network model. Additionally a Bayesian framework was included to decide whether an image is of a seen or unseen class. Frome et al. (2013) learn the image representation using a CNN trained with the Imagenet dataset and a word-based neural language model (Mikolov et al., 2013b) to represent the textual modality. To perform the fusion they re-train CNN using text representation as targets. This work outperforms scalability with respect to (Socher et al., 2013) from 2 to 20,000 unknown classes in the zero-shot learning task. A modified strategy of Frome et al. (2013) was presented by Norouzi et al. (2014). Instead of re-train the CNN network, they built a convex combination with probabilities estimated by the classifier and semantic embedding vector of the unseen label.",
      "startOffset" : 91,
      "endOffset" : 1592
    }, {
      "referenceID" : 11,
      "context" : "Huang et al. (2007) were one of the first teams exploring this task.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Huang et al. (2007) were one of the first teams exploring this task. They classified movie previews into 3 genres by extracting handcrafted features from the video and training a decision tree classifier. They evaluated the model using 44 films. Using only textual modality, Shah et al. (2013) performed single-label genre classification of movie scripts using clustering algorithms with 260 movies.",
      "startOffset" : 0,
      "endOffset" : 294
    }, {
      "referenceID" : 11,
      "context" : "Huang et al. (2007) were one of the first teams exploring this task. They classified movie previews into 3 genres by extracting handcrafted features from the video and training a decision tree classifier. They evaluated the model using 44 films. Using only textual modality, Shah et al. (2013) performed single-label genre classification of movie scripts using clustering algorithms with 260 movies. Later, combining two modalities, Pais et al. (2012) classified movies between drama and non-drama using visual and textual features with 107 samples.",
      "startOffset" : 0,
      "endOffset" : 452
    }, {
      "referenceID" : 11,
      "context" : "Huang et al. (2007) were one of the first teams exploring this task. They classified movie previews into 3 genres by extracting handcrafted features from the video and training a decision tree classifier. They evaluated the model using 44 films. Using only textual modality, Shah et al. (2013) performed single-label genre classification of movie scripts using clustering algorithms with 260 movies. Later, combining two modalities, Pais et al. (2012) classified movies between drama and non-drama using visual and textual features with 107 samples. Hong & Hwang (2015) explored different PLSA models to combine 3 modalities: audio, image and text to predict genre of movie previews.",
      "startOffset" : 0,
      "endOffset" : 570
    }, {
      "referenceID" : 11,
      "context" : "Huang et al. (2007) were one of the first teams exploring this task. They classified movie previews into 3 genres by extracting handcrafted features from the video and training a decision tree classifier. They evaluated the model using 44 films. Using only textual modality, Shah et al. (2013) performed single-label genre classification of movie scripts using clustering algorithms with 260 movies. Later, combining two modalities, Pais et al. (2012) classified movies between drama and non-drama using visual and textual features with 107 samples. Hong & Hwang (2015) explored different PLSA models to combine 3 modalities: audio, image and text to predict genre of movie previews. It was single label classification with 4 genres for 140 movies taken from IMDb. Recently, Fu et al. (2015) used a set of handcrafted visual features for poster characterization and bag-of-words for synopsis.",
      "startOffset" : 0,
      "endOffset" : 792
    }, {
      "referenceID" : 1,
      "context" : "In this setup, Anand (2014) explores the efficiency of using keywords and users’ tags to perform multilabeling using the movies from MovieLens 1M dataset which contains 1, 700 movies.",
      "startOffset" : 15,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "An alternative approach was initially proposed by Bengio et al. (2003), by building a language model based on a neural network architecture (NNLM).",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 26,
      "context" : "In the multilabel classification the performance evaluation can be more complex than traditional multi-class classification and the differences can be significant among several measures (Madjarov et al., 2012).",
      "startOffset" : 186,
      "endOffset" : 209
    }, {
      "referenceID" : 26,
      "context" : "Concretely, we calculate them as follows (Madjarov et al., 2012):",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 34,
      "context" : "concatenation Different works have found that a simple concatenation of representations of different modalities are good for combining the information (Suk & Shen, 2013; Pei et al., 2013; Kiela & Bottou, 2014).",
      "startOffset" : 151,
      "endOffset" : 209
    }, {
      "referenceID" : 16,
      "context" : "MoE The mixture of experts (MoE) (Jacobs et al., 1991) model was adapted for multilabel classification.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 42,
      "context" : "linear sum Following the way Vinyals et al. (2015) combine text and images representation into a single space, this model adds a linear transformation for each modality so that both outputs have the same size to be summed up and then followed by the MaxoutMLP architecture.",
      "startOffset" : 29,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.",
    "creator" : "LaTeX with hyperref package"
  }
}