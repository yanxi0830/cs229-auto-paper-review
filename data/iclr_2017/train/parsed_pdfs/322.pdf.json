{
  "name" : "322.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NONPARAMETRIC NEURAL NETWORKS",
    "authors" : [ "George Philipp", "Jaime G. Carbonell" ],
    "emails" : [ "george.philipp@email.de;", "jgc@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Automatically choosing a neural network model for a given task without prior information is a challenging problem. Formally, let Θ be the space of all models considered. The goal of model selection is then, usually, to find the value of the hyperparameter θ ∈ Θ that minimizes a certain criterion c(θ), such as the validation error achieved by the model represented by θ when trained to convergence. Because Θ is large, structured and heterogeneous, c is complex, and gradients of c are generally not available, the most popular methods for optimizing c perform zero-order, black-box optimization and do not use any information about c except its value for certain values of θ. These methods select one or more values of θ, compute c at those values and, based on the results, select new values of θ until convergence is achieved or a time limit is reached. The most popular such methods are grid search, random search (e.g. Bergstra & Bengio (2012)) and Bayesian optimization using Gaussian processes (e.g. Snoek et al. (2012)). Others utilize random forests (Hutter et al., 2009), deep neural networks (Snoek et al., 2015) and recently Bayesian neural networks (Springenberg et al., 2016) and reinforcement learning (Zoph & Le, 2017).\nThese black-box methods have two drawbacks. (A) To obtain each value of c, they execute a full network training run. Each run can take days on many cores or multiple GPUs. (B) They do not exploit opportunities to improve the value of c further by altering θ during each training run. In this paper, we present a framework we term nonparametric neural networks for selecting network size. We dynamically and automatically shrink and expand the network as needed to select a good network size during a single training run. Further, by altering network size during training, the network ultimately chosen can achieve a higher accuracy than networks of the same size that are trained from scratch and, in some cases, achieve a higher accuracy than is possible by black-box methods.\nThere has been a recent surge of interest in eliminating unnecessary units from neural networks, either during training or after training is complete. This strategy is called pruning. Alvarez & Salzmann (2016) utilize an `2 penalty to eliminate units and Molchanov et al. (2017) compare a variety of strategies, whereas Figurnov et al. (2016) focuses on thinning convolutional layers in the spatial dimensions. While some of these methods even allow some previously pruned units to be added back in (e.g. Feng & Darrell (2015)), all of these strategies require a high-performing network model as a starting point from which to prune, something that is generally only available in well-studied vision and NLP tasks. We do not require such a starting point in this paper.\nIn section 2, we introduce the nonparametric framework and state its theoretical soundness, which we prove in section 7.1. In section 3, we develop the machinery for training nonparametric networks,\nincluding a novel normalization layer in section 3.2, CapNorm, and a novel training algorithm in section 3.3, AdaRad. We provide experimental evaluation and analysis in section 4, further relevant literature in section 5 and conclude in section 6."
    }, {
      "heading" : "2 NONPARAMETRIC NEURAL NETWORKS",
      "text" : "For the purpose of this section, we define a parametric neural network as a function f(x) = σL.(σL−1.(..σ2.(σ1.(xW1)W2)..)WL) of a d0-dimensional row vector x, where Wl ∈ Rdl−1∗dl , 1 ≤ l ≤ L are dense weight matrices of fixed dimension and σl : R → R, 1 ≤ l ≤ L are fixed non-linear transformations that are applied elementwise, as signified by the .() operator. The number of layers L is also fixed. Further, the weight matrices are trained by solving the minimization problem minW=(W )l 1 |D| ∑ (x,y)∈D e(f(W, x), y) + Ω(W), where D is the dataset, e is an error function that consumes a vector of fixed size dL and the label y, and Ω is the regularizer.\nWe define a nonparametric neural network in the same way, except that the dimensionality of the weight matrices is undetermined. Hence, the optimization problem becomes\nmin d=(d)l,dl∈Z+,1≤l≤L−1 min W=(W )l,Wl∈Rdl−1∗dl ,1≤l≤L\n1 |D| ∑\n(x,y)∈D\ne(f(W, x), y) + Ω(W) (1)\nNote that the dimensions d0 and dL are fixed because the data and the error function e are fixed. The parameter value now takes the form of a pair (d,W).\nThere is no guarantee that optimization problem 1 has a global minimum. We may be able to reduce the value of the objective further and further by using larger and larger networks. This would be problematic, because as networks become better and better with regards to the objective, they would become more and more undesirable in practice. It turns out that in an important case, this degeneration does not occur. Define the fan-in regularizer Ωin and the fan-out regularizer Ωout as\nΩin(W, λ, p) = λ L∑ l=1 dl∑ j=1 ||[Wl(1, j),Wl(2, j), ..,Wl(dl−1, j)]||p (2)\nΩout(W, λ, p) = λ L∑ l=1 dl−1∑ i=1 ||[Wl(i, 1),Wl(i, 2), ..,Wl(i, dl)]||p (3)\nIn plain language, we either penalize the incoming weights (fan-in) of each unit with a p-norm, or the outgoing weights (fan-out) of each unit. We now state the core theorem that justifies our formulation of nonparametric networks. The proof is found in the appendix in section 7.1.\nTheorem 1. Nonparametric neural networks achieve a global training error minimum at some finite dimensionality when Ω is a fan-in or a fan-out regularizer with λ > 0 and 1 ≤ p <∞."
    }, {
      "heading" : "3 TRAINING NONPARAMETRIC NETWORKS",
      "text" : "Training nonparametric networks is more difficult than training parametric networks, because the space over which we optimize the parameter (d,W) is no longer a space of form Rd, but is an infinite, discrete union of such spaces. However, we would still like to utilize local, gradient-based search. We notice, like (Wei et al., 2016), that there are pairs of parameter values with different dimensionality that are still in some sense “close” to one another. Specifically, we say that two parameter values (d1,W1) and (d2,W2) are f -equivalent if ∀x ∈ Rd0 , f(W1, x) = f(W2, x) where not necessarily d1 = d2. During iterative optimization, we can “jump” between those two parameter values while maintaining the output of f and thus preserving locality. We define a zero unit as any unit for which either the fan-in or fan-out or both are the zero vector. Given any parameter value, the most obvious way of generating another parameter value that is f -equivalent to it is to add a zero unit to any hidden layer l where σl(0) = 0 holds. Further, if we have a parameter value\nthat already contains a zero unit in such a hidden layer, removing it yields an f -equivalent parameter value.\nThus, we will use the following strategy for training nonparametric networks. We use gradient-based methods to adjust W while periodically adding and removing zero units. We use only nonlinearities that satisfy σ(0) = 0. It should be noted that while adding and removing zero units leaves the output of f invariant, it does change the value of the fan-in and fan-out regularizers and thus the value of the objective. While it is possible to design regularizers that do not penalize such zero units, this is highly undesirable as it would stifle the regularizers ability to “reign in” the growth of the network during training.\nTo be able to reduce the network size during training, we must produce zero units and, it turns out, the fan-in and fan-out regularizers naturally produce such units as they induce sparsity, i.e. they cause individual weights to become exactly zero. This is well studied under the umbrella of sparse regression (see e.g. Tibshirani (1996)). The cases p = 1 and p = 2 are especially attractive because it is computationally convenient to integrate them into a gradient-based optimization framework via a shrinkage / group shrinkage operator respectively (see e.g. Back & Teboulle (2006)). Further, p = 1 and p = 2 differ in their effect on the parameter value. p = 1 sets individual weights to zero and thus leads to sparse fan-ins and fan-outs and thus ultimately to sparse weight matrices. A unit can only become a zero unit if each weight in its fan-in or each weight in its fan-out has been set to zero individually. p = 2, on the other hand, sets entire fan-ins (for the fan-in regularizer) or fan-outs (for the fan-out regularizer) to zero at once. Once the resulting zero units are removed, we obtain dense weight matrices. (For a basic comparison of 1-norm and 2-norm regularizers, see Yuan & Lin (2006) and for a comparison in the context of neural networks, see Collins & Kohli (2014).) While there is recent interest in learning very sparse weight matrices (e.g. Guo et al. (2016)), current hardware is geared towards dense weight matrices (Wen et al., 2016). Hence, for the remainder of this paper, we will focus on the case p = 2. Further, we will focus on the fan-in rather than the fan-out regularizer.\nWhen a new zero unit is added, we must choose its fan-in and fan-out. While one of the two weight vectors must be zero, the other can have an arbitrary value. We make the simple choice of initializing the other weight vector randomly. Since we are going to use the fan-in regularizer, we will initialize the fan-out to zero and the fan-in randomly. This will give each new unit the chance to learn and become useful before the regularizer can shrink its fan-in to zero. If it does become zero nonetheless, the unit is eliminated."
    }, {
      "heading" : "3.1 SELF-SIMILAR NONLINEARITIES",
      "text" : "For layers 1 through L− 1, it is best to use nonlinearities that satisfy σ(cs) = cσ(s) for all c ∈ R≥0 and s ∈ R. We call such nonlinearities self-similar. ReLU (Dahl et al., 2013) is an example of this. Self-similarity also implies σ(0) = 0.\nRecall that the fan-in and fan-out regularizers shrink the values of weights during training. This in turn affects the scale of the values to which the nonlinearities are applied. (These values are called pre-activations.) The advantage of self-similar nonlinearities is that this change of scale does not affect the shape of the feature.\nIn contrast, the impact of a nonlinearity such as tanh on pre-activations varies greatly based on their scale. If the pre-activations have very large absolute values, tanh effectively has a binary output. If they have very small absolute values, tanh mimics a linear function. In fact, all nonlinearities that are differentiable at 0 behave approximately like a linear function if the pre-activations have sufficiently small absolute values. This would render the unit ineffective. Since we expect some units to have small pre-activations due to shrinkage, this is undesirable.\nBy being invariant to the scale of pre-activations, self-similar nonlinearities further eliminate the need to tune how much regularization to assign to each layer. This is expressed in the following proposition which is proved in section 7.2.\nProposition 1. If all nonlinearities in a nonparametric network model except possibly σL are selfsimilar, then the objective function 1 using a fan-in or fan-out regularizer with different regularization parameters λ1, .., λL for each layer is equivalent to the same objective function using the single regularization parameter λ = ( ∏L l=1 λl) 1 L for each layer, up to rescaling of weights.\n1 input: αr: radial step size; αφ: angular step size; λ: regularization hyperparameter; β: mixing rate; : numerical stabilizer; d0: initial dimensions; W0: initial weights; ν: unit addition rate; νfreq: unit addition frequency; T : number of iterations 2 φmax = 0; cmax = 0; d = d0; W = W0; 3 for l = 1 to L do 4 set φ̄l (angular quadratic running average) and cl (angular quadratic running average capacity) to zero vectors of size d0l ; 5 end 6 for t = 1 to T do 7 set Dt to mini-batch used at iteration t; 8 G = 1|D|∇W ∑ (x,y)∈Dt e(f(W, x), y);\n9 for l = L to 1 do 10 for j = dl to 1 do 11 decompose [Gl(i, j)]i into a component parallel to [Wl(i, j)]i (call it r) and a component orthogonal to [Wl(i, j)]i (call it φ) such that [Gl(i, j)]i = r + φ; 12 φ̄l(j) = (1− β)φ̄l(j) + β||φ||22; cl(j) = (1− β)cl(j) + β; 13 φmax = max(φmax, φ̄l(j)); cmax = max(cmax, cl(j)) ;\n14 φadj =\n√ φmax cmax√\nφ̄l(j) cl(j) +\nφ;\n15 [Wl(i, j)]i = [Wl(i, j)]i − αrr ; 16 rotate [Wl(i, j)]i by angle αφ||φadj||2 in direction − φadj||φadj||2 ; 17 shrink([Wl(i, j)]i, αrλ |Dt| |D| ); 18 if l < L and [Wl(i, j)]i is a zero vector then 19 remove column j from Wl; remove row j from Wl+1; remove element j from φ̄l and cl; decrement dl; 20 end 21 end 22 if t = 0 mod νfreq then 23 ν′ = ν; // if ν 6∈ Z, we can set e.g. ν′ = Poisson(ν) 24 add ν′ randomly initialized columns to Wl; add ν′ zero rows to Wl+1; add ν′ zero elements to φ̄l and cl; dl = dl + ν′; 25 end 26 end 27 end 28 return W;\nAlgorithm 1: AdaRad with `2 fan-in regularizer and the unit addition / removal scheme used in this paper in its most instructive (bot not fastest) order of computation. Note that []i notation is used to indicate a vector over index i.\n3.2 CAPPED BATCH NORMALIZATION (CapNorm)\nRecently, Ioffe & Szegedy (2015) proposed a strategy called batch normalization that quickly became the standard for keeping feed-forward networks well-conditioned during training. In our experiments, nonparametric networks trained without batch normalization could not compete with parametric networks trained with it. Batch normalization cannot be applied directly to nonparametric networks with a fan-in or fan-out regularizer, as it would allow us to shrink the absolute value of individual weights arbitrarily while compensating with the batch normalization layer, thus negating the regularizer. Hence, we make a small adjustment which results in a strategy we term capped batch normalization or CapNorm. We subtract the mean of the pre-activations of each hidden unit, but only scale their standard deviation if that standard deviation is greater than one. If it is less than one, we do not scale it. Also, after the normalization, we do not add or multiply the result with a free parameter. Hence, CapNorm replaces each pre-activation z with z−µmax(σ,1) , where µ is the mean and σ is the standard deviation of that unit’s pre-activations across the current mini-batch.\n3.3 ADAPTIVE RADIAL-ANGULAR GRADIENT DESCENT (AdaRad)\nThe staple method for training neural networks is stochastic gradient descent. Further, there are several popular variants: momentum and Nesterov momentum (Sutskever et al., 2013), AdaGrad (Duchi et al., 2011) and AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015). All of these methods center around two key principles: (1) averaging the gradient obtained over consecutive iterations to smooth out oscillations and (2) normalizing each component of the gradient so that each weight learns at roughly the same speed. Principle (2) turns out to be especially important for nonparametric neural networks. When a new unit is added, it does not initially contribute to the quality of the output of the network and so does not receive much gradient from the loss term. If the gradient is not normalized, that unit may take a very long time to learn anything useful. However, if we use a fan-in regularizer, we cannot normalize the components of the gradient outright as in e.g. RMSprop, as we would also have to scale the amount of shrinkage induced by the regularizer accordingly. This, in turn, would cause the fan-in of new units to become zero before they can learn anything useful.\nWe resolve this dilemma with a new training algorithm: Adaptive Radial-Angular Gradient Descent (AdaRad), shown in algorithm 1. Like in all the algorithms cited above, we begin each iteration by computing the gradient G of the loss term over the current mini-batch (line 8). Then, for each 1 ≤ l ≤ L and 1 ≤ j ≤ dl, we decompose the sub-vector [Gl(1, j), Gl(2, j), .., Gl(dl−1, j)] into a component parallel to its corresponding fan-in [Wl(1, j),Wl(2, j), ..,Wl(dl−1, j)] and a component orthogonal to it (line 11). Out of the two, we normalize only the orthogonal component (line 14) while the parallel component is left unaltered. Finally, the normalized orthogonal component of each sub-vector is added to its corresponding fan-in in radial-angular coordinates instead of cartesian coordinates (line 16). This ensures that it does not affect the length of the fan-in. Like the parallel component, we leave the induced shrinkage unaltered. Note that `2 shrinkage acts only to shorten the length of each fan-in, but does not alter its direction. Hence, AdaRad with an `2 regularizer applies a normalized shift to each fan-in that alters its direction but not its length (angular shift), as well as an un-normalized shift that includes shrinkage that alters the length of the fan-in but not its direction (radial shift, lines 15 and 17).\nAdaRad has two step sizes: One for the radial and one for the angular shift, αr and αφ respectively. This is desirable as they both control the behavior of the training algorithm in different ways. The radial step size controls how long it takes for the fan-in of a unit to be shrunk to zero, i.e. the time a unit has to learn something useful. On the other hand, the angular step size controls the general speed of learning and is tuned to achieve the quickest possible descent along the error surface.\nLike RMSprop and unlike Adam, AdaRad does not make use of the principle of momentum. We have developed a variant called AdaRad-M that does. It is described in the appendix in section 7.3.\nUsing AdaRad over SGD incurs additional computational cost. However, that cost scales more gracefully than the cost of, for example, RMSprop. AdaRad normalizes at the granularity of fan-ins instead of the granularity of individual weights, so many of its operations scale only with the number of units and not with the number of weights in the network. In Table 1, we compare the costs of SGD,\nAdaRad and RMSprop. Further, RMSprop has a larger memory footprint than AdaRad. Compared to SGD, it requires an additional cache of size equal to the number of weights, whereas AdaRad only requires 2 additional caches of size equal to the number of units."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We evaluated our framework using the network architecture shown in Figure 1 with ReLU nonlinearities and CapNorm, and using AdaRad as the training algorithm. We used two hidden layers (L = 3) and started off with ten units in each hidden layer and each fan-in initialized randomly with expected length 1. We add one new unit with random fan-in of expected length 1 and zero fan-out to each layer every epoch. While this does not lead to fast convergence - we have to wait until tens or hundreds of units are added - we believe that growing nets from scratch is a good test case for investigating the robustness of our framework. After the validation error stopped improving, we ceased adding units, allowing all remaining redundant units to be eliminated. We set αr = 150λ , as this allows each new unit ≈ 50 epochs to train before being eliminated by shrinkage, assuming the length of the fan-in is not altered by the gradient of the loss term.\nWhen training parametric networks, we replaced CapNorm with batch normalization, either with or without trainable free mean and variance parameters. We trained the network using one of the following algorithms: SGD, momentum, Nesterov momentum, RMSprop or Adam. Further experimental details can be found in the appendix in section 7.4."
    }, {
      "heading" : "4.1 PERFORMANCE",
      "text" : "In this section, we investigate our two core questions: (A) Do nonparametric networks converge to a good size? (B) Do nonparametric networks achieve higher accuracy than parametric networks?\nWe evaluated our framework using three standard benchmark datasets - the mnist dataset, the rectangles images dataset and the convex dataset (Bergstra & Bengio, 2012). We started by training nonparametric networks. Through preliminary experiments, we determined a good starting angular step size for all datasets. We chose to start with αφ = 30 and repeatedly divided αφ by 3 when the validation error stopped improving. By varying the random seed, we trained 10 nets each for several values of the regularization parameter λ per dataset and then chose a typical representative from among those 10 trained nets. Results are shown in black in figure 2. Values of λ are 3 ∗ 10−3, 10−3 and 3 ∗ 10−4 for MNIST, 3 ∗ 10−5 and 10−6 for rectangles images and 10−5 and 10−8 for convex.\nThen, we trained parametric networks of the same size as the chosen representatives. The top performers after an exhaustive grid search are shown in red in figure 2. Finally, we conducted an exhaustive random search where we also varied the size of both hidden layers. The top performers are shown in blue in the same figure.\nWe obtain different results for the three datasets. For mnist, nonparametric networks substantially outperform parametric networks of the same size. The best nonparametric network is close in performance to the best parametric network, while being substantially smaller (144 first layer units versus 694). For rectangles images, nonparametric networks underperform parametric networks of the same size when λ is large and outperform them when λ is small. Here, the best nonparametric network has the globally best performance, as measured by the median test error over 10 random reruns, using substantially fewer parameters than the best parametric network.\nWhile results for the first two datasets are very promising, nonparametric networks performed badly on the convex dataset. Parametric networks of the same size perform substantially better and also have a smaller range of performance across random reruns. Even if the model found by training nonparametric networks were re-trained as a parametric network, the apparent tendency of nonparametric networks to converge to relatively small sizes hurts us here as we would still miss out on a significant amount of performance.\nWe also conducted experiments with AdaRad-M, but found that performance was very similar to that of AdaRad. Hence, we omit the results. Similarly, we found no significant difference in performance between parametric networks trained with RMSprop and those trained with Adam."
    }, {
      "heading" : "4.2 ANALYSIS OF THE NONPARAMETRIC TRAINING PROCESS",
      "text" : "In this section, we analyze in detail a single training run of a nonparametric network. We chose mnist as dataset, set λ = 3 ∗ 10−4 and lowered the angular step size to 10 as we did not use step size annealing. We trained for 1000 epochs while adding one unit to each hidden layer per epoch, then trained another 1000 epochs without adding new units. The final network had 193 units in the first hidden layer and 36 units in the second hidden layer. The results are shown in figure 3.\nIn part (A), we show the validation classification error. As a comparison, we trained two parametric networks with 193 and 36 hidden units for 1000 epochs, once using SGD and the same step size and λ as the nonparametric network, and once using optimal settings (RMSprop, α = 300, λ = 0). It is not suprising that the parametric networks reach a good accuracy level faster, as the nonparametric network must wait for its units to be added. Also, the parametric network benefits from an increased step size - in this case α = 300. This was true throughout our experimental evaluation.\nIn (B), we show the training cross-entropy error for the same training runs. Interestingly, parametric networks reach an error very close to zero. In fact, the unregularized network reaches a value of ≈ 10−6 and the regularized network reaches a value of ≈ 10−4. Both made zero classification mistakes on the training set after training. In contrast, the nonparametric network did not have a near-zero training cross-entropy error. Towards the end of training, it still misclassified around 30 out of 50.000 training examples. However, this did not harm its performance on the validation or test set. In fact, the validation error of nonparametric networks tended to improve slowly for many epochs, whereas unregularized parametric networks (which were the best parametric networks when early stopping is used) tended to have a slightly increasing validation error in the long run.\nIn (C), we show the size of the two hidden layers during training. These curves are very typical of all training runs we examined. For the first ≈ 50 epochs, no units are eliminated. This is because we chose αr = 150λ , which guarantees that units that are added with a fan-in of length 1 take ≈ 50 epochs to be eliminated, assuming no impact from the gradient of the loss term. If the layer requires a relatively large number of units, it will keep growing linearly for a while and then either plateau or shrink slightly. Once we no longer add units after 1000 epochs, both layers shrink linearly by ≈ 50 units over ≈ 50 iterations, as the units that were added roughly between epochs 950 and 1000 are eliminated in succession. Overall, this process shows the value of controlling αφ and αr independently, as we can manage the “overhead” of extraneous units present during training while still ensuring an ideal speed of learning. In (D), we show the length of time individual units in the first hidden layer were present during training. On the x axis, we show the epoch during which a given unit was added. On the y axis, we show the number of epochs the unit was present. Green bars represent units that survived until the end, while black bars represent units that did not. As one might expect, units were more likely to survive the earlier they were added. Units that did not survive were eliminated in ≈ 50 epochs. The same graph for the second hidden layer is shown in figure 4.\nIn (E) and (F), we show the lengths of fan-ins (blue) and fan-outs (red) of units in the hidden layers. For each layer, we depict the following units in dark colors: three randomly chosen units that were initially present as well as units that were added at epochs 0, 25, 50, 100, 200, 300, .., 1000. In addition, in light colors, we show three units that were added late but not eliminated. We see a\nconsistent pattern for individual units. First, their length decreases linearly as the CapNorm layer filters the component of the gradient parallel to the fan-ins as long as the standard deviation of the pre-activations σ exceeds 1. During this period, the unit learns something useful and so the fan-out increases in length. When finally σ < 1, the parallel component of the gradient starts to slow down the decay and, if the unit has become useful enough, reverses it. If the decay is not reversed, the unit is eliminated. If it is reversed, both fan-in and fan-out will attain a length comparable to those of well-established units.\nFrom a global perspective, we notice that fan-ins in the first layer have lengths much less than 1. This is because first layer units encode primarily AND functions of highly correlated input features, meaning weights of small magnitude are sufficient to attain σ = 1. In contrast, lengths of fan-ins in the second layer are more chaotic. We found this is because σ = 1 is generally NOT attained in the second layer. In fact, the network compensated for lower activation values in the second layer by assigning fan-ins of stable lengths between 3.5 and 4.5 to the 10 output units. The network can assign these lengths dynamically without altering the output of the network because ReLU is self-similar, as described in section 3.1."
    }, {
      "heading" : "4.3 SCALABILITY",
      "text" : "Finally, we wanted to verify whether nonparametric networks could be applied to a large dataset. We visited OpenML http://www.openml.org/, a website containing many datasets as well as the performance of various machine learning models applied to those datasets. We applied nonparametric networks to the largest classification dataset 1 on OpenML meeting our standards 2. This was the poker dataset http://www.openml.org/d/354. It is a binary classification dataset with 1.025.010 datapoints and 14 features per datapoint. We had no prior information about this dataset. In general, we think that nonparametric networks are most useful in cases with no prior information and thus no possibility of choosing a good parametric model a priori.\nWe made the following changes to the experimental setup for poker: (i) we used 4 hidden layers instead of 2 (ii) we added a unit every tenth of an epoch instead of every epoch and (iii) we multiplied the radial step size by 10, i.e. αr = 15λ . The latter two changes were made as poker is approximately one order of magnitude larger than mnist, and we wanted to approximately preserve the rate of unit addition and elimination per mini-batch. Those changes were made a priori and were not based on examining their performance.\nAfter some exploration, we set the starting angular step size for nonparametric networks to 10. We trained nonparametric networks for various values of λ, obtaining nets of different sizes. We then trained parametric networks of those same sizes with RMSprop, where the step size was chosen by validation, independently for each network size.\n1in terms of number of datapoints 2our standards were: at least 10 published classification accuracy values; no published classification accu-\nracy values exceeding 95%; no extreme label imbalance\nThe results are shown in Table 2. Both parametric and nonparametric networks perform very well, achieving less than 1% test error even for small networks. The nonparametric networks had a higher error for larger values of λ and a slightly lower error for smaller values of λ. In fact, the best nonparametric network made no mistake on the test set of 100.000 examples. For comparison, we show that linear models perform roughly as well as random guessing on poker. Also, the best result published on OpenML, achieved by a decision tree classifier, vastly underperforms our 4-hidden layer networks.\nTo achieve convergence, networks required many more mini-batches on poker than they did on the smaller datasets used in section 4.1. However, since units were added to the nonparametric networks at roughly the same rate per mini-batch, the time it took those networks to converge to a stable network size (as in Figure 3C) was a much smaller fraction of the overall training time under poker compared to the smaller datasets. Thus, the downside of increased training time as shown in Figure 3A incurred when networks are built gradually was ameliorated."
    }, {
      "heading" : "5 FURTHER BACKGROUND",
      "text" : "Several strategies have been introduced to address the drawbacks of black-box model selection. Maclaurin et al. (2015) indeed calculate the gradient of the validation error after training with respect to certain hyperparameters, though their method only applies to specific networks trained with very specific algorithms. (Luketina et al., 2016) and (Larsen et al., 1998) train certain hyperparameters jointly with the network using second order information. Such methods are limited to continuous hyperparameters and are often applied specifically to regularization hyperparameters. Several papers try to speed up the global model search by estimating the validation error of trained networks without fully training them. Saxe et al. (2011) use the validation error with randomly initialized convolutional layers as a proxy. Klein et al. (2017) predict the validation error after training based on the progress made during the first few epochs.\nSeveral papers have achieved increased performance by growing networks during training. Our main inspiration was Wei et al. (2016), who utilize a notion similar to our f -equivalence, though they enlarge their network in a somewhat ad-hoc way. The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices.\nThe performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks.\nBayesian neural networks (e.g. McKay (1992), De Freitas (2003)) use a probabilistic prior instead of a regularizer to control the complexity of the network. Gaussian processes can been used to mimick “infinitely wide” neural networks (e.g. Williams (1997), Hazan & Jaakkola (2015)), thus eliminating the need to choose layer width and replacing it with the need to choose a kernel. Compared to these and other Bayesian approaches, we work within the popular feed-forward function optimization paradigm, which has advantages in terms of computational and algorithmic complexity.\nAdding units to a network one at a time is an idea with a long history. Ash (1989) adds units to a single hidden layer, whereas Gallant (1986) builds up pyramid and tower structures and Fahlman & Lebiere (1990) effectively create a new layer for each new unit. While these papers provided inspiration to us, the methods they present for determining when to add a new unit requires training the network to convergence first, which is impractical in modern settings. We circumvent this problem by adding units agnostically and providing a mechanism for removing unnecessary units."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We introduced nonparametric neural networks - a simple, general framework for automatically adapting and choosing the size of a neural network during a single training run. We improved the performance of the trained nets beyond what is achieved by regular parametric networks of the same size and obtained results competitive with those of an exhaustive random search, for two of three datasets. While we believe there is room for performance improvement in several areas - e.g. unit initialization, unit addition schedule, additional regularization and starting network size - we see this paper as validation of the basic concept. We also proved the theoretical soundness of the framework.\nIn future work, we plan to extend our framework to include convolutional layers and to automatically choosing the depth of networks, as done by e.g. Wen et al. (2016). Part of our motivation to develop nonparametric networks was to control the layer size via a continuous parameter. We want to make use of this by tuning λ during training, either by simple annealing or in a comprehensive framework such as the one introduced in Luketina et al. (2016). We want to use nonparametric networks to learn more complicated network topologies for e.g. semi-supervised or multi-task learning. Finally, we plan to investigate the possibility of sampling units with different nonlinearities and training an ever-growing network for lifelong learning."
    }, {
      "heading" : "7 APPENDIX",
      "text" : ""
    }, {
      "heading" : "7.1 PROOF OF THEOREM 1",
      "text" : "First, we restate the theorem formally. Theorem 1. For all\n• L, d0, dL ∈ Z+\n• finite datasets D of points (x, y) with x ∈ Rd0 and y ∈ Y for some set Y\n• sets of nonlinearities {σl : R → R, 1 ≤ l ≤ L} where each σl fulfils the following conditions:\n– There exists a function b1,l : R≥0 → R≥0 such that for all S ∈ R≥0, −S ≤ s ≤ S, we have |σl(s)| ≤ b1,l(S) ∗ |s|. – It is left- and right-differentiable everywhere. – There exists a function b2,l : R≥0 → R≥0 such that for all S ∈ R≥0, −S ≤ s ≤ S,\nwe have |σ←l (s)| ≤ b2,l(S) and |σ→l (s)| ≤ b2,l(S), where the superscripts indicate directional derivatives.\n• error functions e : (RdL × Y )→ R that fulfils the following conditions:\n– It is non-negative everywhere. – It is differentiable with respect to its first argument everywhere. – There exists a function b3 : R≥0 → R≥0 such that for all S ∈ R≥0, v ∈ RdL and y ∈ Y , we have e(v, y) ≤ S =⇒ ||de(v,y)dv ||∞ ≤ b3(S)\n• λ > 0 and 1 ≤ p <∞\n• Ω ∈ {Ωin,Ωout}\nwe have that\nE(d,W) = 1 |D| ∑\n(x,y)∈D\ne(f(W, x), y) + Ω(W, λ, p) (4)\nattains a global minimum.\nMost commonly used nonlinearities are admissible under this theorem as long as σ(0) = 0, i.e. the sigmoid non-linearity is not admissible, but the tanh non-linearity is. Note that nonlinearities, away from zero, are allowed to grow at an almost arbitrary pace. For example, polynomial or even exponential nonlinearities are possible. Note that the first condition on nonlinearities is technically implied by the other two as long as σ(0) = 0, though we will not prove this.\nThe conditions for the error function cover the two most popular choices: cross-entropy coupled with softmax (as in Figure 1) - and the square of the `2 distance.\nWe will prove this theorem through a sequence of lemmas. Throughout this process, all inputs to the main theorem are considered fixed and fulfilling their respective conditions. Lemma 1. Theorem 1 holds if d is fixed.\nI.e. in the parametric case, 4 attains a global minimum.\nProof. Let d be fixed. Let B = E(d,0), where 0 is the value of W of dimensionality d where all individual weights are set to zero. Then let WB be the space of all W of dimensionality d which have at least one individual weight with absolute value greater than Bλ . Clearly, E(d,W) > B for all W ∈WB . Since Rd\\WB is compact and E is continuous, there exists a point Wmin that is a minimum of E inside Rd\\WB . Further, Rd\\WB contains at least one point, namely 0, for which\nE ≤ B, so a minimum within Rd\\WB is indeed a global minimum, the existence of which was required.\nNow, some definitions:\n• We call a parameter value (d,W) a local minimum of E iff it is a local minimum in its second component, W.\n• We call a local minimum of E B-locally minimal for some B ∈ R iff the value of E at that minimum does not exceed B.\n• We call the proper dimensionality of W the dimensionality obtained when eliminating from W all units which have a zero fan-in or a zero fan-out or both.\n• We call a parameter value (d,W) proper if d is the proper dimensionality of W. We also call a local minimum with such a parameter value proper.\n• Denote (d1, .., dl) by d≤l and (W1, ..,Wl) by W≤l. • D = {(x(0), y(0)), (x(1), y(1)), .., (x(N), y(N))} • We denote intermediate computations of the neural network f(W, x) as follows:\nx0 := x (5) zl := xl−1Wl 1 ≤ l ≤ L (6) xl := σl.(zl) 1 ≤ l ≤ L (7)\nf(W, x) = xL (8)\n• We denote the gradients of e(f(W, x), y), when they are defined, as follows:\ngl := de(f(W, x), y)\ndxl 0 ≤ l ≤ L (9)\nhl := de(f(W, x), y)\ndzl 1 ≤ l ≤ L (10)\nGl := de(f(W, x), y)\ndWl 1 ≤ l ≤ L (11)\n• Vector and matrix indeces are written in brackets. For example, the j’th component of z(n)l is denoted by z(n)l (j). • We denote by square brackets a vector and by its subscript the index the vector is over, e.g. [vi]i is a vector over index i.\nLemma 2. Under the conditions of theorem 1 and the additional condition that the σl are differentiable everywhere, if Ω is the fan-in regularizer, then for all B, the set of values of d for which there exist proper B-local minima is bounded.\nLemma 3. Under the conditions of theorem 1 and the additional condition that the σl are differentiable everywhere, if Ω is the fan-out regularizer, then for all B, the set of values of d for which there exist proper B-local minima is bounded.\nLemmas 2 and 3 are the core segments of the overall proof. Here we show that that very large nets have no “good” local minima.\nProof of lemma 2. Throughout this proof, we consider B fixed.\nClaim 1a: There exist constants Bx,l, 0 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 0 ≤ l ≤ L, we have ||x(n)l ||1 ≤ Bx,l. Claim 1b: There exist constants Bz,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, we have ||z(n)l ||1 ≤ Bz,l.\nClaim 1c: There exist constants Bdσ,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, for all 1 ≤ j ≤ dl, we have |σ′(z(n)l (j))| ≤ Bdσ,l. First, we notice that it is sufficient to prove the bounds exist for a specific datapoint. The uniform bound across all datapoints is then simply the maximum of the individual bounds. Denote by (x, y) an arbitrary fixed datapoint throughout the proof of the above claims. Also, notice that the claims are trivially true if there are no proper B-local minima. Hence, throughout the proof of the claims, we assume there exists at least one such minimum.\nWe will prove the claims jointly by induction. The order of the induction follows the order of computation of the neural network. Our starting case will be x0, followed by z1, f ′1 and x1 etc.\nThe starting case is obvious as x0 = x is fixed and does not depend on the parameter (d,W). Hence we can choose Bx,0 = ||x||1. Now assume we have Bx,l−1 such that sup(d,W)properB-locally minimal ||xl−1||1 ≤ Bx,l−1. Then:\nsup (d,W)properB-locally minimal\n||zl||1 (12)\n= sup (d,W)properB-locally minimal\n||xl−1Wl||1 (13)\n≤ sup (d,W),||xl−1||1≤Bx,l−1,λ ∑dl j=1 ||[Wl(i,j)]i||p≤B ||xl−1Wl||1 (14)\n= sup d≤l ( sup Wl,λ ∑dl j=1 ||[Wl(i,j)]i||p≤B ( sup W<l,||xl−1||1≤Bx,l−1 ||xl−1Wl||1)) (15)\n≤ sup d≤l ( sup Wl,λ ∑dl j=1 ||[Wl(i,j)]i||p≤B ( sup u,dim(u)=dl−1,||u||1≤Bx,l−1 ||uTWl||1)) (16)\n= sup d≤l ( sup Wl,λ ∑dl j=1 ||[Wl(i,j)]i||p≤B ( sup u,dim(u)=dl−1,||u||1≤Bx,l−1 dl∑ j=1 |uT [Wl(i, j)]i|)) (17)\n= sup d≤l ( sup cj≥0, ∑dl j=1 cj≤ B λ ( sup Wl,||[Wl(i,j)]i||p=cj ( sup u,dim(u)=dl−1,||u||1≤Bx,l−1 dl∑ j=1 |uT [Wl(i, j)]i|)))(18)\n≤ sup d≤l ( sup cj≥0, ∑dl j=1 cj≤ B λ dl∑ j=1 ( sup Wl,||[Wl(i,j)]i||p=cj ( sup u,dim(u)=dl−1,||u||1≤Bx,l−1 |uT [Wl(i, j)]i|)))(19)\n= sup d≤l ( sup cj≥0, ∑dl j=1 cj≤ B λ dl∑ j=1 ( sup v,dim(v)=dl−1,||v||p=cj ( sup u,dim(u)=dl−1,||u||1≤Bx,l−1 |uT v|))) (20)\n≤ sup d≤l ( sup cj≥0, ∑dl j=1 cj≤ B λ dl∑ j=1 ( sup v,dim(v)=dl−1,||v||∞≤cj ,u,dim(u)=dl−1,||u||1≤Bx,l−1 |uT v|)) (21)\n≤ sup d≤l ( sup cj≥0, ∑dl j=1 cj≤ B λ dl∑ j=1 cjBx,l−1) (22)\n≤ sup d≤l BBx,l−1 λ\n(23)\n= BBx,l−1\nλ (24)\nA line-by-line explanation of the above is as follows:\n13 Replacing zl by its definition.\n14 Relaxing the conditions on (d,W) by replacing proper B-local minimality by two conditions that proper B-local minimality implies. The first condition is the induction hypothesis. The second condition follows because E ≤ B and so specifically Ωin(W) ≤ B and so specifically Ωin(Wl) ≤ B\n15 Breaking up the supremum into three stages. We drop components of d and W that are immaterial to the value of the objective of the supremum.\n16 We further relax the innermost sup by no longer requiring that xl−1 be the intermediate output of some neural network but an arbitrary vector of fixed size and limited length. W<l then becomes immaterial.\n17 Replacing the `1 norm by its definition.\n18 We fix the length of each fan-in in the second sup and add an additional sup over these lengths.\n19 Jensen’s inequality\n20 Simplifying the notation by replacing rows of Wl by vector v.\n21 Relaxing the conditions on v.\n22 Using an elementary property of norms.\n23 obvious\n24 obvious\nAnd therefore, we may choose Bz,l = BBx,l−1\nλ as required.\nNow consider the other inductive steps. Assuming we have a valid Bz,l, we have at all proper Blocal minima |σ′l(zl(j))| ≤ b2,l(Bz,l) because |zl(j)| ≤ ||zl||1 ≤ Bz,l and hence we can choose Bdσ,l = b2,l(Bz,l) as required. Finally, at all proper B-local minima, ||xl||1 = ||σl.(zl)||1 =∑dl j=1 |σl(zl(j))| ≤ ∑dl j=1 b1,l(Bz,l)|zl(j)| = b1,l(Bz,l)||zl||1 ≤ b1,l(Bz,l)Bz,l and so we can choose Bx,l = b1,l(Bz,l)Bz,l. This completes the proof of claims 1(a)-(c).\nClaim 2a: There exist constants Bg,l, 0 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 0 ≤ l ≤ L, for all 1 ≤ i ≤ dl, we have |g(n)l (i)| ≤ Bg,l. Claim 2b: There exist constants Bh,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, for all 1 ≤ i ≤ dl, we have |h(n)l (i)| ≤ Bh,l. Again, we can restrict our attention to a single datapoint and again, we will prove these claims by induction, but going backwards along the flow of the gradient. The starting case is gL. At all proper B-local minima, E ≤ B, so specifically e(xL, y) ≤ B and therefore we have ||gL||∞ = ||de(xL,y)dxL ||∞ ≤ b3(B) and so specifically |gL(i)| ≤ b3(B) and so we can choose Bg,L = b3(B) as required.\nNow we assume we have a valid Bg,l. At all proper B-local minima we have |hl(i)| = | dedzl(i) | = | dedxl(i) dxl(i) dzl(i)\n| = |gl(i)||σ′(zl(i))| ≤ Bg,lBdσ,l. Therefore we can choose Bh,l = Bg,lBdσ,l as required.\nFinally, assume we have Bh,l. Then we have |gl−1(i)| = | dedxl−1(i) | = | ∑dl j=1 de dzl(j) dzl(j) dxl(i)\n| = | ∑dl j=1 hl(j)Wl(i, j)| ≤ ∑dl j=1 |hl(j)||Wl(i, j)| ≤ Bh,l ∑dl j=1 |Wl(i, j)| ≤\nBh,l ∑dl j=1 ||[Wl(i, j)]i||p ≤ Bh,lB, so we can choose Bg,l−1 = Bh,lB as required.\nClaim 3: There is a constant B2, such that at all proper B-local minima, for all 1 ≤ l ≤ L, for all 1 ≤ j ≤ dl, we have ∑N n=1 |g (n) l (j)| ≥ B2.\nAt any proper parameter value, all fan-ins are non-zero. Therefore Ω is differentiable with respect to W, and therefore E is differentiable with respect to W. Hence, at any proper B-\nlocal minimum, we have ∇WE = 0, so in particular for any l, j we have dEd[Wl(i,j)]i = 0, so 1N ∑N n=1 de d[Wl(i,j)]i = − dΩd[Wl(i,j)]i and so specifically || 1 N ∑N n=1 de d[Wl(i,j)]i || p p−1 = || dΩd[Wl(i,j)]i || pp−1 , where p p−1 can take the value∞. Further analyzing the right hand side, we have || dΩd[Wl(i,j)]i || pp−1 = || d(λ||[Wl(i,j)]i||p) d[Wl(i,j)]i || p p−1 = λ. Therefore, at all proper B-local minima we have:\nλ (25)\n= 1 N || ∑ n\nde\nd[Wl(i, j)]i || p p−1\n(26)\n= 1 N || ∑ n de dxl(j) dxl(j) d[Wl(i, j)]i || p p−1\n(27)\n= 1 N || ∑ n gl(j) dxl(j) dzl(j) dzl(j) d[Wl(i, j)]i || p p−1\n(28)\n= 1 N || ∑ n gl(j)σ ′ l(zl(j))xl−1|| pp−1 (29)\n≤ 1 N || ∑ n gl(j)σ ′ l(zl(j))xl−1||1 (30)\n= 1\nN dl−1∑ i=1 | ∑ n gl(j)σ ′ l(zl(j))xl−1(i)| (31)\n≤ 1 N dl−1∑ i=1 ∑ n |gl(j)||σ′l(zl(j))||xl−1(i)| (32)\n= 1\nN ∑ n |gl(j)||σ′l(zl(j))|||xl−1||1 (33)\n≤ 1 N ∑ n |gl(j)|Bdσ,lBx,l−1 (34)\nHence, we can choose B2 = max1≤l≤L λNBdσ,lBx,l−1 as required. Note that in the above equations, we have omitted all (n) superscripts for brevity.\nClaim 4: There exist constants Dl, 1 ≤ l ≤ L such that at all proper B-local minima, for all 1 ≤ l ≤ L, we have dl ≤ Dl. Note that this claim is tantamount to proving the hypothesis of the Lemma.\nWe will prove this by induction going backwards. Since dL is fixed, we can simply pick DL = dL. Now assume we have a valid Dl+1. Then at all proper B-local minima we have\ndlB2 (35)\n≤ N∑ n=1 dl∑ i=1 |g(n)l (i)| (36)\n= N∑ n=1 dl∑ i=1 | dl+1∑ j=1 Wl+1(i, j)h (n) l+1(j)| (37) ≤ N∑ n=1 dl∑ i=1 dl+1∑ j=1 |Wl+1(i, j)h(n)l+1(j)| (38)\nTherefore, by the box principle, there exists an n′ and j′ such that ∑dl i=1 |Wl+1(i, j′)h (n′) l+1 (j\n′)| ≥ dlB2 dl+1N . So further, we have\ndlB2 dl+1N\n(39)\n≤ dl∑ i=1 |Wl+1(i, j′)h(n ′) l+1 (j ′)| (40)\n≤ |h(n ′)\nl+1 (j ′)| dl∑ i=1 |Wl+1(i, j′)| (41)\n≤ Bh,l+1||[Wl+1(i, j′)]i||1 (42) ≤ Bh,l+1d p−1 p l ||[Wl+1(i, j ′)]i||p (43) ≤ Bh,l+1d p−1 p l B (44)\nAnd therefore, dl ≤ (BBh,l+1Ndl+1B2 ) p and so we can choose Dl = ( BBh,l+1NDl+1 B2 )p, which completes the proof.\nFor brevity, we will only give a summary of the proof of lemma 3, as it is very similar to the proof of lemma 2.\nSketch of proof of lemma 3. As in the previous proof, we consider B fixed.\nClaim 1a: There exist constants Bx,l, 0 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 0 ≤ l ≤ L, for all 1 ≤ j ≤ dl, we have |x(n)l (j)| ≤ Bx,l. Claim 1b: There exist constants Bz,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, for all 1 ≤ j ≤ dl, we have |z(n)l (j)| ≤ Bz,l. Claim 1c: There exist constants Bdσ,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, for all 1 ≤ j ≤ dl, we have |σ′(z(n)l (j))| ≤ Bdσ,l. As in the previous proof, we proceed by induction along the order of feed-forward execution of the neural network. However, we use the arguments we used for Claims 2(a)-(b) in the previous proof. This is because the fan-out regularizer “appears” like a fan-in regularizer when the direction of signal flow is reversed.\nClaim 2a: There exist constants Bg,l, 0 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 0 ≤ l ≤ L, we have ||g(n)l ||1 ≤ Bg,l. Claim 2b: There exist constants Bh,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, we have ||h(n)l ||1 ≤ Bh,l. As in the previous proof, we proceed by induction along the flow of the gradient. However, we use the arguments we used for Claims 1(a)-(c) in the previous proof.\nClaim 3: There is a constant B2, such that at all proper B-local minima, for all 0 ≤ l ≤ L − 1, for all 1 ≤ j ≤ dl, we have ∑N n=1 |x (n) l (j)| ≥ B2.\nClaim 4: There exist constants Dl, 0 ≤ l ≤ L − 1 such that at all proper B-local minima, for all 0 ≤ l ≤ L− 1, we have dl ≤ Dl. The arguments mirror those of Claim 3 and 4 from the previous proof, but with the role of activation and gradient reversed. Also, here, Claim 4 is proved by induction along the order of feed-forward execution.\nLemma 4. Under the conditions of theorem 1, for all B, the set of values of d for which there exist proper B-local minima is bounded.\nThis is the stronger version of the previous lemmas where we only use directional differentiability of the σl instead of actual differentiability. The proof is a rather tedious extension of the previous two proofs and not very instructive, which is why we broke out the differential case as its own lemmas. Following this lemma, we immediately prove the main theorem.\nProof. We will describe how to amend the proof of lemma 2. The proof of lemma 3 can be amended similarly.\nFirst, we define a signature S with dimensionality d as a binary sequence of vectors. Then, for all d, x′ ∈ Rd0 , W′ of dimensionality d and S of dimensionality d, we define a linearized neural network that is linearized at point x′ with weights W′ and signature S f [S,W\n′,x′] as follows: First, obtain x′l and z′l for 1 ≤ l ≤ L by evaluating f(W′, x′) as usual. Then, for each σl used in f , define a vector of functions σ[S,W\n′,x′] l , where each element is a linear function with σ [S,W′,x′] l (j)(z ′ l(j)) = σl(z ′ l(j))\nand with dσ [S,W′,x′] l (j)(s)\nds = σ ← l (z ′ l(j)) if Sl(j) = 0 and with\ndσ [S,W′,x′] l (j)(s)\nds = σ → l (z ′ l(j)) if\nSl(j) = 1. Finally, we obtain f [S,W ′,x′] from f by, for each 1 ≤ l ≤ L, replacing σl that is applied elementwise with σ[S,W ′,x′]\nl where each component is applied to the respective component of zl.\nIn plain language, we linearize a neural network at a point by evaluating it at that point and replacing each nonlinearity by a straight line as indicated by the value and directional derivative of that nonlinearity wherever it is evaluated, where the direction of the derivative used is governed by S.\nSimilarly, define a partially linearized neural network f [S≥l,W ′,x′] in the same fashion, except only layers l and above are linearized. Finally, define a partially linearized neural network f [S≥l,i,W ′,x′] in the same fashion, except only layers l and above as well as unit i in layer l − 1 are linearized. e is composed of functions that are differentiable or directionally differentiable with respect to W, so e itself is directionally differentiable with respect to W. Specifically, let us analyze the directional derivative of e with respect to some perturbation of WL−1.\n∇δWL−1e(f(W, x), y) (45) = ∇δWL−1e(σL.(σL−1.(xL−2WL−1)WL), y) (46) = ∇δxL=∇δWL−1σL.(σL−1.(xL−2WL−1)WL)e(xL, y) (47)\n= de\ndxL ∇δWL−1(σL.(σL−1.(xL−2WL−1)WL))T (48)\n= dL∑ j=1 de dxL (j)∇δWL−1(σL.(σL−1.(xL−2WL−1)WL))(j) (49)\n= dL∑ j=1 de dxL (j)∇δzL=∇δWL−1 [σL−1.(xL−2WL−1)WL](σL.(zL))(j) (50)\n= dL∑ j=1 de dxL (j)σ∗L(zL(j))∇δWL−1(σL−1.(xL−2WL−1)WL)(j) (51)\n= dL∑ j=1 de dxL (j)σ∗L(zL(j)) dL−1∑ i=1 WL(i, j)∇δWL−1(σL−1.(xL−2WL−1))(i) (52)\n= ( de\ndxL . ∗ σ∗L.(zL))WTL∇δWL−1(σL−1.(xL−2WL−1))T (53)\n= dL−1∑ j=1 (( de dxL . ∗ σ∗L.(zL))WTL )(j)σ∗L−1(zL−1(j))∇δWL−1(xL−2WL−1)(j) (54)\n= dL−1∑ j=1 (( de dxL . ∗ σ∗L.(zL))WTL )(j)σ∗L−1(zL−1(j)) dL−2∑ i=1 xL−2(i)∇δWL−1WL−1(i, j) (55)\n= dL−1∑ j=1 (( de dxL . ∗ σ∗L.(zL))WTL )(j)σ∗L−1(zL−1(j)) dL−2∑ i=1 xL−2(i)δWL−1(i, j) (56)\n= (((( de\ndxL . ∗ σ∗L.(zL))WTL ). ∗ σ∗L−1.(zL−1))TxL−2).δWL−1(i, j) (57)\nHere, a ∗ superscript is a “wildcard” that can stand for a left or a right derivative. When combined with the .() elementwise operation, it can mean a different derivative (left or right) for each element.\nWe use the chain rule for directional derivatives (lines 47, 50 and 54), the linearity of the directional derivative (lines 52 and 55), the fact that the directional derivative of a differentiable function is the dot product of its gradient with the perturbation (line 48), and the fact that the directional derivative of a left- and right-differentiable scalar function is either the product of its left derivative with the perturbation or the product of its right derivative with the perturbation (lines 51 and 54).\nWe notice that the final expression in line 57 is the same expression we would obtain if the σl were differentiable, except with a ∗ instead of a ′ superscript. Now the linearized neural networks come into play. We can choose a signature S that matches the wildcards in the above directional derivative and get ∇δWL−1e(f(W, x), y) = de(f [S,W,x](W,x),y) dWL−1 .δWL−1, because the forward evaluation of f [S,W,x] and f are identical at x and the backward evaluation picks out the correct left and right derivatives. In fact, it is sufficient to choose a partially linearized network with signature S≥L−1 to achieve the above identity.\nSo far, we have investigated the directional derivative with respect to δWL−1. However, the same arguments hold for all 1 ≤ l ≤ L. We can expand ∇δWle(f(W, x), y) in the same way, except we repeat the transformation from line 48 to line 53 L− l times. Hence, we have what we will call claim 0.\nClaim 0: For all (d,W), for all x ∈ Rd0 , for all 1 ≤ l ≤ L, for all δWl, we can choose a signature S or a partial signature S≥l, such that∇δWle(f(W, x), y) = de(f [S,W,x](W,x),y) dWl .δWl.\nNow, we refer back to the proof of lemma 2. Claims 1(a)-(c) hold as before, except in Claim 1(c) we replace |σ′(z(n)l (j))| ≤ Bdσ,l with |σ←(z (n) l (j))| ≤ Bdσ,l and |σ→(z (n) l (j))| ≤ Bdσ,l. Further, note that claims 1(a)-(c) also hold for neural networks that are linearized at the respective B-local minimum and datapoint at which they are evaluated.\nClaims 2(a)-(b) are changed as follows:\nClaim 2a: There exist constants Bg,l, 0 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 0 ≤ l ≤ L, for all 1 ≤ i ≤ dl, for all signatures S or partial signatures S≥l+1 of matching dimensionality we have |g(n)[S,W,x\n(n)] l (i)| ≤ Bg,l.\nClaim 2b: There exist constants Bh,l, 1 ≤ l ≤ L, such that at all proper B-local minima, for all 1 ≤ n ≤ N , for all 1 ≤ l ≤ L, for all 1 ≤ i ≤ dl, for all signatures S or partial signatures S≥l+1,i of matching dimensionality we have |h(n)[S,W,x\n(n)] l (i)| ≤ Bh,l.\nThe proof is as before, where derivatives of the σl are again replaced by a left or right derivative as indicated by the signature.\nClaim 3 and its proof change somewhat.\nClaim 3: There exists a constant B2, such that at all proper B-local minima, for all 1 ≤ l ≤ L, for all 1 ≤ j ≤ dl, we have ∑N n=1 ∑ S≥l+1 |g(n)[S≥l+1,W,x (n)] l (j)| ≥ B2, where ∑ S≥l+1 is the sum over all partial signatures.\nAt all proper B-local minima, we have for all l, i and j:\n0 (58) ≤ ∇δWl(i,j)E (59)\n= ∇δWl(i,j)( 1\nN N∑ n=1 e(f(W, x(n)), y(n)) + Ω(W)) (60)\n= 1\nN N∑ n=1 ∇δWl(i,j)e(f(W, x (n)), y(n)) +∇δWl(i,j)Ω(Wl) (61)\n= 1\nN N∑ n=1 de(f [S i,j,n ≥l ,W,x (n)](W, x(n)), y(n)) dWl(i, j) δW (i, j) + dΩ(Wl) dWl(i, j) δW (i, j) (62)\nHere, ∇δWl(i,j) stands for the directional derivative with respect to a change in the scalar value Wl(i, j). Because this is a special case of a directional derivative with respect to δWl, we can use Claim 0 to obtain line 62. Note that to use Claim 0, we have to choose a different partial signature for each value of i, j, and n, which we indicate by superscript. Specifically, we now choose δWl(i, j) = −1 and corresponding signatures. Then, for all\nl, i and j we have 0 ≤ 1N ∑N n=1 de(f [S i,j,n ≥l ,W,x (n)] (W,x(n)),y(n)) dWl(i,j) (−1) + dΩ(Wl)dWl(i,j) (−1), and\nhence − 1N ∑N n=1 de(f [S i,j,n ≥l ,W,x (n)] (W,x(n)),y(n)) dWl(i,j) ≥ dΩ(Wl)dWl(i,j) . Since dΩ(Wl) dWl(i,j) ≥ 0, we have\n| 1N ∑N n=1 de(f [S i,j,n ≥l ,W,x (n)] (W,x(n)),y(n)) dWl(i,j) | ≥ | dΩ(Wl)dWl(i,j) |. So in particular for all l and j we have\n|| 1N [ ∑N n=1 de(f [S i,j,n ≥l ,W,x (n)] (W,x(n)),y(n)) dWl(i,j) ]i|| p p−1 ≥ ||[ dΩ(Wl)dWl(i,j) ]i|| pp−1 = λ. So further we have:\nλ (63)\n≤ || 1 N [ N∑ n=1 de(f [S i,j,n ≥l ,W,x (n)](W, x(n)), y(n)) dWl(i, j) ]i|| p p−1 (64)\n= 1\nN ||[ N∑ n=1 g (n)[Si,j,n≥l ,W,x (n)] l (j)(σ [Si,j,n≥l ,W,x (n)] l ) ′(z (n) l (j))x (n) l−1(i)]i|| pp−1 (65)\n≤ 1 N ||[ N∑ n=1 |g (n)[Si,j,n≥l+1,W,x (n)] l (j)||(σ [Si,j,n≥l ,W,x (n)] l ) ′(z (n) l (j))||x (n) l−1(i)|]i|| pp−1 (66)\n≤ 1 N Bdσ,l||[ N∑ n=1 |g (n)[Si,j,n≥l+1,W,x (n)] l (j)||x (n) l−1(i)|]i|| pp−1 (67)\n≤ 1 N Bdσ,l||[ N∑ n=1 ∑ S≥l+1 |g(n)[S≥l+1,W,x (n)] l (j)||x (n) l−1(i)|]i|| pp−1 (68)\n≤ 1 N Bdσ,l||[ N∑ n=1 ∑ S≥l+1 |g(n)[S≥l+1,W,x (n)] l (j)||x (n) l−1(i)|]i||1 (69)\n= 1\nN Bdσ,l dl−1∑ i=1 N∑ n=1 ∑ S≥l+1 |g(n)[S≥l+1,W,x (n)] l (j)||x (n) l−1(i)| (70)\n≤ 1 N Bdσ,lBx,l−1 N∑ n=1 ∑ S≥l+1 |g(n)[S≥l+1,W,x (n)] l (j)| (71)\nAt line 65, we use the chain rule. Note that x and z do not need the [Si,j,n≥l ,W, x (n)] superscript because the forward evaluation of f(W, x(n)) and f [S i,j,n ≥l ,W,x\n(n)](W, x(n)) are equivalent. So, we can set B2 = max1≤l≤L λNBdσ,lBx,l−1 , as required.\nFinally, claim 4 is the same as in Lemma 2. The only difference in the proof is that when we invoke the box principle, we choose a specific signature S′≥l+1 in addition to n\n′ and j′ such that∑dl i=1 |Wl+1(i, j′)h (n′)[S′≥l+1,W,x (n)] l+1 (j ′)| ≥ dlB2\ndl+1N2 Dl+1+..+DL\n. This is possible because of the\ninduction hypothesis, which bounds the number of partial signatures S≥l+1 by 2Dl+1+..+DL . Hence\nwe can set Dl = ( BBh,l+1NDl+12\nDl+1+..+DL\nB2 )p, which completes the proof.\nProof of theorem 1. Clearly, E is bounded below by zero. Therefore, it has a greatest lower bound, which we callB. Denote (t, t, .., t) by dt. If d is assumed to be fixed at dt, E has a global minimum by lemma 1. Let Wt denote one such global minimum. Let Et denote the value of E at (dt,Wt).\nNow let d′ and d′′ be two arbitrary values of d with d′l ≥ d′′l for all 0 ≤ l ≤ L. (Denote such a relation by d′ ≥ d′′.) Then, any value that E can attain with d = d′′ it can attain with d = d′ because we can change any d′′-dimensional value of W into a d′-dimensional value by adding d′l − d′′l units with zero fan-in and fan-out to each layer without changing E. In particular, this implies that (E)t is a decreasing sequence because dt+1 ≥ dt. Since it is also bounded below by B, it converges. Call its limit C.\nAssume C > B. Then there exists some (d′,W′) with E(d′,W′) < C. However, any value that E can attain with d = d′ it can attain with d = dt′ where t′ = maxl d′l, because dt′ ≥ d′. Therefore C > E(d′,W′) ≥ Et′ ≥ C. Contradiction. Therefore, C = B. Now assume that for some t, Wt has a unit that has zero fan-in but not zero fan-out, or vice versa. Then by setting the non-zero fan to zero, the output of f is unchanged for all x ∈ Rd0 and the value of Ω is reduced. Therefore, we reduce E, which contradicts the fact that (dt,Wt) is a global minimum of E when d is fixed to dt. Therefore, all units in Wt that have zero fan-in also have zero fan-out, and vice versa.\nLet dpropert be the proper dimensionality of Wt and W proper t be the result of removing all units with zero fan-in or fan-out from Wt. Indeed, as we have shown, all units removed had both zero fan-in and fan-out. Assume (dpropert ,W proper t ) is not a local minimum of E. Then there exists a W\n′ of dimensionality dpropert with E(d proper t ,W ′) < E(dpropert ,W proper t ). When we add the zero units that were removed from Wt to obtain W proper t back into W\n′, we obtain another weight parameter value we call W′′. Since E is invariant under the addition and removal of units with both zero fan-in and zero fan-out, we have both E(dpropert ,W ′) = E(dt,W ′′) and E(dpropert ,W proper t ) = E(dt,Wt). Therefore, we have E(dt,W′′) < E(dt,Wt), which contradicts that Wt is a global minimum of E when d is fixed to dt. Therefore, (d proper t ,W proper t ) is a local minimum of E. In particular, it is a proper Et-local minimum of E and therefore a proper E0-local minimum of E.\nFrom lemma 4, we know that the set of proper E0-local minima is bounded. Hence, the set {dpropert , t ≥ 0} is bounded, i.e there exists some dmax with dmax ≥ d proper t for all t. Hence, if we denote maxl dmaxl by T , we have dT ≥ d proper t for all t and therefore ET ≤ E(d proper t ,W proper t ). But E(dpropert ,W proper t ) = E(dt,Wt) = Et, and therefore Et ≥ ET for all t.\nBut (E)t converges to B from above. Therefore ET = B, therefore E(dT ,WT ) = B and so E attains its greatest lower bound which means it attains a global minimum, as required."
    }, {
      "heading" : "7.2 PROOF OF PROPOSITION 1",
      "text" : "Proposition 1. If all nonlinearities in a nonparametric network model except possibly σL are selfsimilar, then the objective function 1 using a fan-in or fan-out regularizer with different regularization parameters λ1, .., λL for each layer is equivalent to the same objective function using the single regularization parameter λ = ( ∏L l=1 λl) 1 L for each layer, up to rescaling of weights.\nProof. Choose arbitrary positive λ1, .., λL and let λ = ( ∏L l=1 λl) 1 L . We have:\nf(W, x) (72) = σL.(σL−1.(..σ2.(σ1.(xW1)W2)..)WL) (73)\n= σL.(σL−1.(..σ2.(σ1.(( L∏ l=1 λl λ )xW1)W2)..)WL) (74)\n= σL.(σL−1.(..σ2.(( L∏ l=2 λl λ )σ1.( λ1 λ xW1)W2)..)WL) (75)\n= σL.( λL λ σL−1.(..σ2.( λ2 λ σ1.( λ1 λ xW1)W2)..)WL) (76)\n= σL.(σL−1.(..σ2.(σ1.(x( λ1 λ W1))( λ2 λ W2))..)( λL λ WL)) (77)\nThe line-by-line explanation is as follows:\n73 Insert the definition of f .\n74 Insert a multiplicative factor of value 1.\n75 Utilize the self-similarity of σ1.\n76 Repeat the previous step L− 2 times.\n77 Utilize linearity.\nFurther, assuming we use a fan-in regularizer, we have:\nL∑ l=1 λl dl∑ j=1 ||[Wl(i, j)]i||p (78)\n= L∑ l=1 λl λ λ dl∑ j=1 ||[Wl(i, j)]i||p (79)\n= L∑ l=1 λ dl∑ j=1 ||[λl λ Wl(i, j)]i||p (80)\nThe argument is equivalent for the fan-out regularizer.\nWe find that the value of the objective is preserved when we replace all regularization parameters with the same value λ = ( ∏L l=1 λl) 1 L and rescale Wl by λlλ . This completes the proof."
    }, {
      "heading" : "7.3 ADARAD-M",
      "text" : "1 input: αr: radial step size; αφ: angular step size; λ: regularization hyperparameter; βarith: arithmetic mixing rate; βquad: quadratic mixing rate; : numerical stabilizer; d0: initial dimensions; W0: initial weights; ν: unit addition rate; νfreq: unit addition frequency; T : number of iterations 2 φmax = 0; cmax = 0; d = d0; W = W0; 3 for l = 1 to L do 4 set φ̃l (angular arithmetic running average) to the zero matrix of size d0l−1 × d0l ; 5 set φ̄l (angular quadratic running average), cl (quadratic running average capacity) and al (arithmetic running average capacity) to zero vectors of size d0l ; 6 end 7 for t = 1 to T do 8 set Dt to mini-batch used at iteration t; 9 G = 1|D|∇W ∑ (x,y)∈Dt e(f(W, x), y);\n10 for l = L to 1 do 11 alt = FALSE; 12 for j = dl to 1 do 13 decompose [Gl(i, j)]i into a component parallel to [Wl(i, j)]i (call it r) and a component orthogonal to [Wl(i, j)]i (call it φ) such that [Gl(i, j)]i = r + φ; 14 φ̄l(j) = (1− βquad)φ̄l(j) + βquad||φ||22; cl(j) = (1− βquad)cl(j) + βquad; 15 φmax = max(φmax, φ̄l(j)); cmax = max(cmax, cl(j)) ; 16 [φ̃l(i, j)]i = (1− βarith)[φ̃l(i, j)]i + βarithφ; al(j) = (1− βarith)al(j) + βarith;\n17 φadj =\n√ φmax cmax√\nφ̄l(j) cl(j) +\n[φ̃l(i,j)]i al(j) ;\n18 [Wl(i, j)]i = [Wl(i, j)]i − αrr; 19 rotate [Wl(i, j)]i by angle αφ||φadj||2 in direction − φadj||φadj||2 ; 20 rotate [φ̃l(i, j)]i by angle αφ||φadj||2 in direction [Wl(i,j)]i||[Wl(i,j)]i||2 ; 21 shrink([Wl(i, j)]i, αrλ |Dt| |D| ); 22 if l < L and [Wl(i, j)]i is a zero vector then 23 remove column j from Wl and φ̃l; remove row j from Wl+1 and φ̃l+1; remove element j from φ̄l, cl and al; decrement dl; 24 alt = TRUE; 25 end 26 end 27 if t = 0 mod νfreq then 28 ν′ = ν; // if ν 6∈ Z, we can set e.g. ν′ = Poisson(ν) 29 add ν′ randomly initialized columns to Wl; add ν′ zero columns to φ̃l; add ν′ zero rows to Wl+1 and φ̃l+1; add ν′ zero elements to φ̄l, cl and al; dl = dl + ν′; 30 end 31 if alt then 32 for j = 1 to dl+1 do 33 [φ̃l+1(i, j)]i = [φ̃l+1(i, j)]i − [φ̃l+1(i,j)]i.[Wl+1(i,j)]i||[Wl+1(i,j)]i||22 [Wl+1(i, j)]i; 34 end 35 end 36 end 37 end 38 return W;\nAlgorithm 2: AdaRad-M with `2 fan-in regularizer and the unit addition / removal scheme used in this paper in its most instructive (bot not fastest) order of computation.\nAdaRad-M is shown in algorithm 2. The main difference in comparison to AdaRad (see algorithm 1) is that, for each fan-in, we maintain an exponential running average of the orthogonal component [φ̃l(i, j)]i (line 16) which we use to compute the angular shift (line 17). Hence, AdaRad-M, like Adam but unlike RMSprop and AdaRad, makes use of the principle of momentum.\nOne issue of note is that the running average of the orthogonal component is not itself orthogonal to the current value of the fan-in. Hence, if some multiple of it was added to the fan-in in radialangular coordinates, it would change the length of the fan-in. This is undesirable as explained in section 3.3. Therefore, we take steps to the ensure that [φ̃l(i, j)]i is kept orthogonal to [Wl(i, j)]i. First, whenever we rotate [Wl(i, j)]i (line 19), we rotate [φ̃l(i, j)]i in the same manner (line 20). Second, whenever a unit in layer l and hence rows of Wl+1 and φ̃l+1 are deleted, we explicitly re-orthogonalize them (line 33)."
    }, {
      "heading" : "7.4 EXPERIMENTAL DETAILS",
      "text" : "In table 3, we show all hyperparameter values and related choices that were universal across all training runs and, unless specified otherwise, datasets."
    }, {
      "heading" : "7.4.1 PROTOCOL FOR SECTION 4.1",
      "text" : "1. We conducted a grid search over λ ∈ {10−2, 3 ∗ 10−3, 10−3, 3 ∗ 10−4, 10−4, 3 ∗ 10−5, 10−5, 3 ∗ 10−6, 10−6, 3 ∗ 10−7, 10−7, 3 ∗ 10−8, 10−8} and αφ ∈ {1, 3, 10, 30, 100, 300, 1.000, 3.000, 10.000, 30.000, 100.000} for nonparametric (NP) networks using AdaRad and a single random seed, for each of the mnist, rectangles-images and convex datasets. By examining validation classification error (VCE) and other metrics (but not test error), we chose the single value αφ = 30 for all NP experiments from now on. Further, we chose a few interesting values of λ for each dataset. From now on, all experiments were conducted independently for each dataset.\n2. We trained 10 NP networks for each chosen value of λ, with 10 different random seeds. Out of the 10 nets produced, we manually chose a single net as a typical representative by approximating the median of both network size, measured in number of weight parameters, and the test classification error (TCE) across the 10 runs. This representative, as well as the range of sizes and TCEs are shown in black in figure 2.\n3. For each chosen representative, we conducted a grid search for parametric (P) networks by fixing the size of the net to the size of the representative. The grid was over α ∈ {1, 3, 10, 30, 100, 300, 1.000, 3.000, 10.000, 30.000, 100.000}, over training algorithm (one of SGD, momentum, Nesterov momentum, RMSprop, Adam), and over whether batch normalization layers had free trainable mean and variance parameters. We introduced the last choice to more closely mimic CapNorm, which does not include free parameters. We set λ = 0 as `2 regularization is not compatible with regular (uncapped) batch normalization. In preliminary experiments, networks trained with `2 regularization and no batch normalization were not competitive. We used the same random seed as in step 1.\n4. We chose the 10 best performing settings from the grid search by VCE and produced 10 reruns for each setting using the same 10 random seeds as in step 2. Then we chose the best setting out of the 10 by median VCE. We depict the median as well as the range of TCE for that best setting in red in figure 2. Note that the setting that had the lowest median TCE in all cases also had the lowest median VCE.\n5. We conducted a random search for P networks with 500 random settings. We chose α uniformly from the interval [1, 100.000] in log scale. Training algorithm and type of batch normalization were chosen uniformly at random from the same sets as in step 3. The size of each hidden layer was chosen uniformly at random between the size of the corresponding layer in the largest NP representative, and 5 times that size. We used the same random seed as in step 1.\n6. We chose the 10 best settings by VCE and reran them 10 times, using the same 10 random seeds as in step 2. By considering network size and median VCE, we chose 2 or 3 settings to display in blue in figure 2, including the setting with the lowest median VCE. In each case, the setting with the lowest median VCE also had the lowest median TCE.\nFor NP networks, we trained until the VCE had not improved for 100 epochs. Then, we rewound the last 100 epochs and kept training without adding units. After no units had been eliminated and the VCE had not improved for 100 epochs, we set λ to zero, rewound the last 100 epochs and kept training. After the VCE had not improved for 100 epochs, we rewound again and divided the angular step size by 3. After the VCE had not improved for 5 epochs, we rewound and divided the angular step size by 3 again. We kept doing this until the angular step size was too small to change the VCE.\nFor P networks, we trained until the VCE had not improved for 100 epochs, then rewound and divided the step size by 3. We kept training until the VCE had not improved for 5 epochs, then rewound again and divided the step size by 3. We kept doing this until the step size was too small to change the VCE."
    }, {
      "heading" : "7.4.2 PROTOCOL FOR SECTION 4.3",
      "text" : "1. We conducted a grid search over λ ∈ {10−3, 3 ∗ 10−4, 10−4, 3 ∗ 10−5, 10−5, 3 ∗ 10−6, 10−6, 3 ∗ 10−7, 10−7} and αφ ∈ {1, 10, 100, 1.000, 10.000} for NP networks using AdaRad, a single random seed and the poker data set. By examining VCE and other metrics (but not test error), we chose the single value αφ = 10. For this value, we chose several values of λ. The size and TCE of the nets trained using those values of λ are shown in table 2.\n2. For each trained NP network shown in table 2, we trained P networks of the same size using RMSprop and each of the following step sizes: α ∈ {1, 3, 10, 30, 100, 300, 1.000, 3.000, 10.000}. For each network size, the TCE of the network with the lowest VCE is shown in table 2. For all network sizes, the network with the lowest TCE also had the lowest VCE.\nFor NP networks, we trained until the VCE had not improved for 10 epochs. Then, we rewound the last 10 epochs and kept training without adding units. After no units had been eliminated and the VCE had not improved for 10 epochs, we set λ to zero, rewound the last 10 epochs and kept training. After the VCE had not improved for 10 epochs, we rewound again and divided the angular step size by 3. After the VCE had not improved for 0.5 epochs, we rewound and divided the angular step size by 3 again. We kept doing this until the angular step size was too small to change the VCE.\nFor P networks, we trained until the VCE had not improved for 10 epochs, then rewound and divided the step size by 3. We kept training until the VCE had not improved for 0.5 epochs, then rewound again and divided the step size by 3. We kept doing this until the step size was too small to change the VCE."
    } ],
    "references" : [ {
      "title" : "Learning the number of neurons in deep networks",
      "author" : [ "Jose M. Alvarez", "Mathieu Salzmann" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Alvarez and Salzmann.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alvarez and Salzmann.",
      "year" : 2016
    }, {
      "title" : "Dynamic node creation in backpropagation networks. Institute for Cognitive Science, UCSD",
      "author" : [ "Timur Ash" ],
      "venue" : "Technical Report 8901,",
      "citeRegEx" : "Ash.,? \\Q1989\\E",
      "shortCiteRegEx" : "Ash.",
      "year" : 1989
    }, {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Lei Jimmy Ba", "Rich Caruana" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ba and Caruana.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba and Caruana.",
      "year" : 2014
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "Amir Back", "Marc Teboulle" ],
      "venue" : "SIAM J Imaging Sciences,",
      "citeRegEx" : "Back and Teboulle.,? \\Q2006\\E",
      "shortCiteRegEx" : "Back and Teboulle.",
      "year" : 2006
    }, {
      "title" : "Random search for hyper-parameter",
      "author" : [ "James Bergstra", "Yoshua Bengio" ],
      "venue" : "optimization. JMLR,",
      "citeRegEx" : "Bergstra and Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra and Bengio.",
      "year" : 2012
    }, {
      "title" : "Net2net: accelerating learning via knowledge transfer",
      "author" : [ "Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Memory bounded deep convolutional networks",
      "author" : [ "Maxwell D. Collins", "Pushmeet Kohli" ],
      "venue" : "CoRR, pp. abs/1412.1442,",
      "citeRegEx" : "Collins and Kohli.,? \\Q2014\\E",
      "shortCiteRegEx" : "Collins and Kohli.",
      "year" : 2014
    }, {
      "title" : "Improving deep neural networks for lvcsr using rectified linear units and dropout",
      "author" : [ "George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Dahl et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dahl et al\\.",
      "year" : 2013
    }, {
      "title" : "Bayesian methods for neural networks",
      "author" : [ "Juan F. De Freitas" ],
      "venue" : "PhD thesis, Trinity College,",
      "citeRegEx" : "Freitas.,? \\Q2003\\E",
      "shortCiteRegEx" : "Freitas.",
      "year" : 2003
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "JMLR, 12:2121–2159,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "The cascade-correlation learning architecture",
      "author" : [ "Scott Fahlman", "Christian Lebiere" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Fahlman and Lebiere.,? \\Q1990\\E",
      "shortCiteRegEx" : "Fahlman and Lebiere.",
      "year" : 1990
    }, {
      "title" : "Learning the structure of deep convolutional networks",
      "author" : [ "Jiashi Feng", "Trevor Darrell" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Feng and Darrell.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feng and Darrell.",
      "year" : 2015
    }, {
      "title" : "Perforatedcnns: Acceleration through elimination of redundant convolutions",
      "author" : [ "Michael Figurnov", "Aijan Ibraimova", "Dmitry Vetrov", "Pushmeet Kohli" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Figurnov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Figurnov et al\\.",
      "year" : 2016
    }, {
      "title" : "Three constructive algorithms for network learning",
      "author" : [ "Stephen Gallant" ],
      "venue" : "In Conference of the Cognitive Learning Society,",
      "citeRegEx" : "Gallant.,? \\Q1986\\E",
      "shortCiteRegEx" : "Gallant.",
      "year" : 1986
    }, {
      "title" : "Dynamic network durgery for efficient dnns",
      "author" : [ "Yiwen Guo", "Anbang Yao", "Yurong Chen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Guo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Steps toward deep kernel methods from infinite neural networks",
      "author" : [ "Tamir Hazan", "Tommi Jaakkola" ],
      "venue" : "arXiv preprint arXiv:1508.05133,",
      "citeRegEx" : "Hazan and Jaakkola.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan and Jaakkola.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequential model-based optimization for general algorithm configuration (extended version)",
      "author" : [ "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown" ],
      "venue" : "Tech. Rep. TR-2009-01,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2009
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Lei Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Dsd: Dense-sparse-dense training for deep neural networks",
      "author" : [ "Aaron Klein", "Stefan Falkner", "Jost Tobias Springenberg", "Frank Hutter" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Klein et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Adaptive regularization in neural network modeling",
      "author" : [ "Jan Larsen", "Claus Svarer", "Lars Nonboe Andersen", "Lars Kai Hansen" ],
      "venue" : "Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "Larsen et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 1998
    }, {
      "title" : "Scalable gradient-based tuning of continuous regularization hyperparameters",
      "author" : [ "Jelena Luketina", "Mathias Berglund", "Klaus Greff", "Raiko Tapani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Luketina et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luketina et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradient-based hyperparameter optimization through reversible learning",
      "author" : [ "Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Maclaurin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maclaurin et al\\.",
      "year" : 2015
    }, {
      "title" : "A practical bayesian framework for backpropagation networks",
      "author" : [ "David McKay" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "McKay.,? \\Q1992\\E",
      "shortCiteRegEx" : "McKay.",
      "year" : 1992
    }, {
      "title" : "Pruning convolutional neural networks for efficient inference",
      "author" : [ "Pavlo Molchanov", "Stephen Tyree", "Tero Karras", "Timo Aila", "Jan Kautz" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Molchanov et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Molchanov et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning by stretching deep networks",
      "author" : [ "Gaurav Pandey", "Ambedkar Dukkipati" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Pandey and Dukkipati.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pandey and Dukkipati.",
      "year" : 2014
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Romero et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Computing with infinite networks",
      "author" : [ "Andrew Saxe", "Pang Wei Koh", "Zhenghao Chen", "Maneesh Bhand", "Bipin Suresh", "Andrew Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2011
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Practical bayesian optimization of machine learning algorithms",
      "author" : [ "Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "Scalable bayesian optimization using deep neural networks",
      "author" : [ "Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md. Mostofa Ali Patwary", "Prabhat", "Ryan P. Adams" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2015
    }, {
      "title" : "Bayesian optimization with robust bayesian neural networks",
      "author" : [ "Jost Tobias Springenberg", "Aaron Klein", "Stefan Falkner", "Frank Hutter" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2016
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning",
      "author" : [ "Tijmen Tieleman", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "Learning structured sparsity in deep neural networks",
      "author" : [ "Wei Wen", "Chunpeng Wu", "Wandan Wang", "Yiran Chen", "Hai Li" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Wen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Computing with infinite networks",
      "author" : [ "Christopher K.I. Williams" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Williams.,? \\Q1997\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1997
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "Ming Yuan", "Yin Lin" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Yuan and Lin.,? \\Q2006\\E",
      "shortCiteRegEx" : "Yuan and Lin.",
      "year" : 2006
    }, {
      "title" : "Adadelta: An adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Neural architecture search with reinforcement learning",
      "author" : [ "Barret Zoph", "Quoc V. Le" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Zoph and Le.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zoph and Le.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Others utilize random forests (Hutter et al., 2009), deep neural networks (Snoek et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : ", 2009), deep neural networks (Snoek et al., 2015) and recently Bayesian neural networks (Springenberg et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 32,
      "context" : ", 2015) and recently Bayesian neural networks (Springenberg et al., 2016) and reinforcement learning (Zoph & Le, 2017).",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "Snoek et al. (2012)).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 16,
      "context" : "Others utilize random forests (Hutter et al., 2009), deep neural networks (Snoek et al., 2015) and recently Bayesian neural networks (Springenberg et al., 2016) and reinforcement learning (Zoph & Le, 2017). These black-box methods have two drawbacks. (A) To obtain each value of c, they execute a full network training run. Each run can take days on many cores or multiple GPUs. (B) They do not exploit opportunities to improve the value of c further by altering θ during each training run. In this paper, we present a framework we term nonparametric neural networks for selecting network size. We dynamically and automatically shrink and expand the network as needed to select a good network size during a single training run. Further, by altering network size during training, the network ultimately chosen can achieve a higher accuracy than networks of the same size that are trained from scratch and, in some cases, achieve a higher accuracy than is possible by black-box methods. There has been a recent surge of interest in eliminating unnecessary units from neural networks, either during training or after training is complete. This strategy is called pruning. Alvarez & Salzmann (2016) utilize an `2 penalty to eliminate units and Molchanov et al.",
      "startOffset" : 31,
      "endOffset" : 1195
    }, {
      "referenceID" : 16,
      "context" : "Others utilize random forests (Hutter et al., 2009), deep neural networks (Snoek et al., 2015) and recently Bayesian neural networks (Springenberg et al., 2016) and reinforcement learning (Zoph & Le, 2017). These black-box methods have two drawbacks. (A) To obtain each value of c, they execute a full network training run. Each run can take days on many cores or multiple GPUs. (B) They do not exploit opportunities to improve the value of c further by altering θ during each training run. In this paper, we present a framework we term nonparametric neural networks for selecting network size. We dynamically and automatically shrink and expand the network as needed to select a good network size during a single training run. Further, by altering network size during training, the network ultimately chosen can achieve a higher accuracy than networks of the same size that are trained from scratch and, in some cases, achieve a higher accuracy than is possible by black-box methods. There has been a recent surge of interest in eliminating unnecessary units from neural networks, either during training or after training is complete. This strategy is called pruning. Alvarez & Salzmann (2016) utilize an `2 penalty to eliminate units and Molchanov et al. (2017) compare a variety of strategies, whereas Figurnov et al.",
      "startOffset" : 31,
      "endOffset" : 1264
    }, {
      "referenceID" : 12,
      "context" : "(2017) compare a variety of strategies, whereas Figurnov et al. (2016) focuses on thinning convolutional layers in the spatial dimensions.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "(2017) compare a variety of strategies, whereas Figurnov et al. (2016) focuses on thinning convolutional layers in the spatial dimensions. While some of these methods even allow some previously pruned units to be added back in (e.g. Feng & Darrell (2015)), all of these strategies require a high-performing network model as a starting point from which to prune, something that is generally only available in well-studied vision and NLP tasks.",
      "startOffset" : 48,
      "endOffset" : 255
    }, {
      "referenceID" : 36,
      "context" : "(2016)), current hardware is geared towards dense weight matrices (Wen et al., 2016).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "Tibshirani (1996)).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 33,
      "context" : "Tibshirani (1996)). The cases p = 1 and p = 2 are especially attractive because it is computationally convenient to integrate them into a gradient-based optimization framework via a shrinkage / group shrinkage operator respectively (see e.g. Back & Teboulle (2006)).",
      "startOffset" : 0,
      "endOffset" : 265
    }, {
      "referenceID" : 33,
      "context" : "Tibshirani (1996)). The cases p = 1 and p = 2 are especially attractive because it is computationally convenient to integrate them into a gradient-based optimization framework via a shrinkage / group shrinkage operator respectively (see e.g. Back & Teboulle (2006)). Further, p = 1 and p = 2 differ in their effect on the parameter value. p = 1 sets individual weights to zero and thus leads to sparse fan-ins and fan-outs and thus ultimately to sparse weight matrices. A unit can only become a zero unit if each weight in its fan-in or each weight in its fan-out has been set to zero individually. p = 2, on the other hand, sets entire fan-ins (for the fan-in regularizer) or fan-outs (for the fan-out regularizer) to zero at once. Once the resulting zero units are removed, we obtain dense weight matrices. (For a basic comparison of 1-norm and 2-norm regularizers, see Yuan & Lin (2006) and for a comparison in the context of neural networks, see Collins & Kohli (2014).",
      "startOffset" : 0,
      "endOffset" : 890
    }, {
      "referenceID" : 33,
      "context" : "Tibshirani (1996)). The cases p = 1 and p = 2 are especially attractive because it is computationally convenient to integrate them into a gradient-based optimization framework via a shrinkage / group shrinkage operator respectively (see e.g. Back & Teboulle (2006)). Further, p = 1 and p = 2 differ in their effect on the parameter value. p = 1 sets individual weights to zero and thus leads to sparse fan-ins and fan-outs and thus ultimately to sparse weight matrices. A unit can only become a zero unit if each weight in its fan-in or each weight in its fan-out has been set to zero individually. p = 2, on the other hand, sets entire fan-ins (for the fan-in regularizer) or fan-outs (for the fan-out regularizer) to zero at once. Once the resulting zero units are removed, we obtain dense weight matrices. (For a basic comparison of 1-norm and 2-norm regularizers, see Yuan & Lin (2006) and for a comparison in the context of neural networks, see Collins & Kohli (2014).) While there is recent interest in learning very sparse weight matrices (e.",
      "startOffset" : 0,
      "endOffset" : 973
    }, {
      "referenceID" : 14,
      "context" : "Guo et al. (2016)), current hardware is geared towards dense weight matrices (Wen et al.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "ReLU (Dahl et al., 2013) is an example of this.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 33,
      "context" : "Further, there are several popular variants: momentum and Nesterov momentum (Sutskever et al., 2013), AdaGrad (Duchi et al.",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : ", 2013), AdaGrad (Duchi et al., 2011) and AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015).",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 39,
      "context" : ", 2011) and AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015).",
      "startOffset" : 21,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "(Luketina et al., 2016) and (Larsen et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : ", 2016) and (Larsen et al., 1998) train certain hyperparameters jointly with the network using second order information.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Maclaurin et al. (2015) indeed calculate the gradient of the validation error after training with respect to certain hyperparameters, though their method only applies to specific networks trained with very specific algorithms.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : ", 2016) and (Larsen et al., 1998) train certain hyperparameters jointly with the network using second order information. Such methods are limited to continuous hyperparameters and are often applied specifically to regularization hyperparameters. Several papers try to speed up the global model search by estimating the validation error of trained networks without fully training them. Saxe et al. (2011) use the validation error with randomly initialized convolutional layers as a proxy.",
      "startOffset" : 13,
      "endOffset" : 404
    }, {
      "referenceID" : 15,
      "context" : "Klein et al. (2017) predict the validation error after training based on the progress made during the first few epochs.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Klein et al. (2017) predict the validation error after training based on the progress made during the first few epochs. Several papers have achieved increased performance by growing networks during training. Our main inspiration was Wei et al. (2016), who utilize a notion similar to our f -equivalence, though they enlarge their network in a somewhat ad-hoc way.",
      "startOffset" : 0,
      "endOffset" : 251
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices.",
      "startOffset" : 12,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network.",
      "startOffset" : 12,
      "endOffset" : 363
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network.",
      "startOffset" : 12,
      "endOffset" : 540
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks.",
      "startOffset" : 12,
      "endOffset" : 650
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks.",
      "startOffset" : 12,
      "endOffset" : 785
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks. Bayesian neural networks (e.g. McKay (1992), De Freitas (2003)) use a probabilistic prior instead of a regularizer to control the complexity of the network.",
      "startOffset" : 12,
      "endOffset" : 903
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks. Bayesian neural networks (e.g. McKay (1992), De Freitas (2003)) use a probabilistic prior instead of a regularizer to control the complexity of the network.",
      "startOffset" : 12,
      "endOffset" : 922
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks. Bayesian neural networks (e.g. McKay (1992), De Freitas (2003)) use a probabilistic prior instead of a regularizer to control the complexity of the network. Gaussian processes can been used to mimick “infinitely wide” neural networks (e.g. Williams (1997), Hazan & Jaakkola (2015)), thus eliminating the need to choose layer width and replacing it with the need to choose a kernel.",
      "startOffset" : 12,
      "endOffset" : 1115
    }, {
      "referenceID" : 4,
      "context" : "The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks. Bayesian neural networks (e.g. McKay (1992), De Freitas (2003)) use a probabilistic prior instead of a regularizer to control the complexity of the network. Gaussian processes can been used to mimick “infinitely wide” neural networks (e.g. Williams (1997), Hazan & Jaakkola (2015)), thus eliminating the need to choose layer width and replacing it with the need to choose a kernel.",
      "startOffset" : 12,
      "endOffset" : 1140
    }, {
      "referenceID" : 1,
      "context" : "Ash (1989) adds units to a single hidden layer, whereas Gallant (1986) builds up pyramid and tower structures and Fahlman & Lebiere (1990) effectively create a new layer for each new unit.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "Ash (1989) adds units to a single hidden layer, whereas Gallant (1986) builds up pyramid and tower structures and Fahlman & Lebiere (1990) effectively create a new layer for each new unit.",
      "startOffset" : 0,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "Ash (1989) adds units to a single hidden layer, whereas Gallant (1986) builds up pyramid and tower structures and Fahlman & Lebiere (1990) effectively create a new layer for each new unit.",
      "startOffset" : 0,
      "endOffset" : 139
    } ],
    "year" : 2017,
    "abstractText" : "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce nonparametric neural networks, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an `p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an `2 penalty. We employ a novel optimization algorithm, which we term “Adaptive Radial-Angular Gradient Descent” or AdaRad, and obtain promising results.",
    "creator" : "LaTeX with hyperref package"
  }
}