{
  "name" : "750.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "LOW-SHOT LEARNING", "Zhuoyuan Chen", "Xiao Liu", "Wei Xu" ],
    "emails" : [ "chenzhuoyuan@baidu.com", "liuxiao@baidu.com", "wei.xu@baidu.com", "han.zhao@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The current success of deep learning hinges on the ability to apply gradient-based optimization to high-capacity models. It has achieved impressive results on many large-scale supervised tasks such as image classification Krizhevsky et al. (2012); He et al. (2016) and speech recognition Yu & Deng (2012). Notably, these models are extensively hungry for data.\nIn contrast, human beings have strong ability to learn novel concepts efficiently from very few or even one example. As pointed out in Lake et al. (2016), human learning is distinguished by its richness and efficiency. To test whether machines can approach this goal, Lake et al. propose the invaluable “Omniglot” hand-written character classification benchmark Lake et al. (2011), where each training class has very few examples and the ability to fast-learn is evaluated on never-seen classes with only one example.\nThere has been previous work on attaining rapid learning from sparse data, denoted as meta-learning or learning-to-learn Thrun. (1998); Baxter. (1998). Although used in numerous senses, the term generally refers to exploiting meta-knowledge within a single learning system across tasks or algorithms. In theory, a meta-learning is able to identify the right “inductive bias shifts” from previous experiences given enough data and many tasks Baxter. (1998). However, even if a well-designed convolutional neural network is a good “inductive bias shift” for a visual recognition task, it is still elusive to find the optimal parameter from a small training set without any prior knowledge.\nTo alleviate this issue, low-shot learning methods have been proposed to transfer knowledge from various priors to avoid over-fitting, such as Bart & Ullman (2005). Recently, Hariharan & Girshick. (2016) propose a novel prior of gradient penalty, which works pretty well experimentally. Although an intuitive explanation is provided in Hariharan & Girshick. (2016) that a good solution of the network parameters should be stable with small gradients, it is mysterious why adding such a regularization magically improves the low-shot task by a large margin. Mathematical derivation shows that gradient penalty is closely related to regularizing the feature representation.\nIn this paper, we give more analysis, both empirically and theoretically, on why adding a gradient regularization, or feature penalty performs so well. Moreover, we carefully carry out two case studies: (1) the simplest non-linear-separable XOR classification, and (2) a two-layer linear network for regression. The study does give insight on how the penalty centers feature representations and\nmake the learning task easier. Furthermore, we also theoretically show that adding another finallayer weight penalty is necessary to achieve better performance. To be added, close scrutiny reveals its inherent connection with batch normalization Ioffe & Szegedy (2015). From a Bayesian point of view, feature penalty essentially introduces a Gaussian prior which softly normalizes the feature representation and eases the following learning task."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "There is a huge body of literature on one-shot learning and it is beyond the scope of this paper to review the entire literature. We only discuss papers that introduce prior knowledge to adjust the neural network learning process, as that is the main focus of this work.\nPrior knowledge, or “inductive bias” Thrun. (1998), plays an important role in one-shot or low-shot learning. To reduce over-fitting problems, we need to regularize the learning process. Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios.\nVarious forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) .\nOur work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al. (2015); Goldberger et al. (2005); Chopra et al. (2005). The insight lies in that a well-trained representational model have strong ability to generalize well on new tasks. In a recent work Santoro et al. (2016), DeepMind proposed a Memory Augmented Neural Network (MANN) to leverage the Neural-Turing-Machine for one-shot tasks. However, in Vinyals et al. (2016), it is found that a good initialization such as VGG-Net largely improves the one-shot performance. In our opinion, a good feature representations still play a central role in low-shot tasks."
    }, {
      "heading" : "1.2 CONTRIBUTIONS",
      "text" : "The main contributions of our comprehensive analysis are three-fold:\n1. We carefully carry out two case studies on the influence of feature regularization on shallow neural networks. We observe how the regularization centers features and eases the learning problem. Moreover, we propose a better design to avoid degenerate solutions.\n2. From Bayesian point of view, close scrutiny reveals internal connections between feature regularization and batch normalization.\n3. Extensive experiments on synthetic, the “Omniglot” one-shot and the large-scale ImageNet datasets validate our analysis."
    }, {
      "heading" : "2 AN ANALYSIS OF FEATURE REGULARIZATION",
      "text" : "We briefly introduce the notations in our work: we denote uppercase A, bold a and lowercase a for matrices, vectors and scalars respectively. For a vector ai, we denotes ai,j as its j-th element. ||.||F stands for the Frobenius norm of a matrix. Given N examples {(xi, yi)|i = 1, ..., N}, we define E{.} as an expectation taken with respect to the empirical distribution generated by the training set. Following Hariharan & Girshick. (2016), we aim to learn a neural network model to extract the feature representations φ(xi) and make predictions ŷi = Wφ(xi) with W = [w1, ...,w|C|]. This setting includes both classification and regression problems, with |C| as the number of classes or target dimension, respectively. The problem can be generally formulated as:\nW ∗, φ∗ = arg min W,φ E{l(W,φ(xi), yi)} (1)\nwhere l(.) can be any reasonable cost function. In this paper, we focus on cross-entropy and L2 loss due to their convexity and universality.\nIn Hariharan & Girshick. (2016), it is suggested that adding a squared gradient magnitude loss (SGM) on every sample can regularize the learning process.\nW ∗, φ∗ = arg min W,φ E{l(W,φ(xi), yi) + λ||∇W l(W,φ(xi, yi))||2} (2)\nThe insight is that for a good solution, the parameter gradient should be small at convergence. However, we know that the convergence of a neural network optimization is a dynamic equilibrium. In other words, at a stationary point, we should have E{∇W l(W,φ(x))} → 0. Intuitively when close to convergence, about half of the data-cases recommend to update a parameter to move positive, while the other half recommend to move negative. It is not very clear why small gradients on every sample E{||∇W l(W,φ(x))||2} produces good generalization experimentally. Mathematical derivation shows that the optimization problem with gradient penalty is equivalent with adding a weighted L2 regularizer φ(xi):\narg min W,φ\nE{l(W,φ(xi), yi) + λαi||φ(xi)||2} (3)\nwhere the example-dependent αi measures the deviation between the prediction ŷi and the target yi. In a regression problem, we have αi = r2 = ||ŷi − yi||2, with the residual r = ŷi − yi; in a classification problem, we have αi = ∑ k(p i k − I(yi = k))2. Intuitively, the misclassified high-norm examples might be outliers, and in a low-shot learning scenario, such outliers can pull the learned weight vectors far away from the right solution. In Hariharan & Girshick. (2016), the authors compare dropping αi and directly penalizing ||φ(xi)||2, which performs almost equally well. In our work, we argue that a better and more reasonable design should be:\narg min W,φ\nE{l(W,φ(xi), yi) + λ1αi||φ(xi)||2}+ λ2||W ||2F (4)\nwhere we add another weight regularizer ||W ||2F , which is necessary to avoid degenerate solutions. We will give further explanation in our second case study. In following analysis, we denote the cost in Eqn(4) with example-dependent αi as weighted L2 feature penalty, and the example-independent (setting αi ≡ 1) as uniform L2 feature penalty. We carry out two case studies: (1) an XOR classification and (2) a regression problem, both empirically and theoretically to analyze how the uniform and weighted L2 feature penalty regularize the neural network. In our paper, we will focus on the uniform feature regularization and will also cover the weighted scenario as well."
    }, {
      "heading" : "2.1 CASE STUDY 1: AN EMPIRICAL ANALYSIS OF XOR CLASSIFICATION",
      "text" : "First, we study the simplest linear-non-separable problem– exclusive-or (XOR). Suppose that we have four two-dimensional input points x = [x1, x2]T ∈ R2: {x1 = [1, 1]T ,x2 = [0, 0]T } ∈ C+ belongs to the positive class, while {x3 = [1, 0]T ,x4 = [0, 1]T } ∈ C− the negative. As shown in Figure 1, we use a three-layer neural network to address the problem (left figure): h1 = x + b is a translation with b = [b1, b2]T ∈ R2 as the offset; h2 = h1,1 ∗ h1,2 is a non-linear layer, multiplying the first and second dimension of h1 and producing a scalar; y is a linear classifier on h2 parametrized by w. The original classification problem can be formulated as:\narg min w1,b,w 4∑ i=1 log(1 + exp(−yi ∗ w ∗ hi2))\nSuppose that we start from an initialization b = [0, 0]T , all three samples {x2,x3,x4} from different classes will produce the same representation h2 = 0, which is not separable at all. It takes efforts to tune the learning rate to back-propagate w,b updates from the target y.\nHowever, if we introduce the uniform L2 feature regularization as:\narg min w1,b,w 4∑ i=1 log(1 + exp(−yiwhi2)) + λ1 2 ||hi2||2\nThen, we have:\nλ1 2\n∂||h2||2\n∂b = λ1 ( E{h2(x2 + b2)} E{h1(x1 + b1)} ) (5)\nthe gradient descent pulls Eqn(5) towards zero, i.e., pulling b towards b1 = −E{x1} = −0.5 and b2 = −E{x2} = −0.5. As shown on the right of Figure 1, the gradient of feature regularization pulls h1 along the direction of red arrows. Then, we have h2 > 0 for positive examples and h2 < 0 for negative ones, which means h2 is linearly-separable. In summary, we can observe that: Empirically, the feature regularization centers the representation h2 = φ(x) and makes the following classification more learnable. For the weighted case, the offset b have similar effects. It can be derived that when converged the feature representation will satisfy E{h1} = 0 and E{h2}=0."
    }, {
      "heading" : "2.2 CASE STUDY 2: A COMPREHENSIVE ANALYSIS ON A REGRESSION PROBLEM",
      "text" : "Next, we analyze a two-layer linear neural network as shown in Figure 2. Denoting the input as X = [x1,x2, ...] and the target as Y = [y1,y2, ...]. The regression loss can be formulated as:\nE{||y −W2W1x||2} where the latent feature is h = φ(x) = W1x. The optimization of {W1,W2} in this multi-layer linear neural network is not trivial, since it satisfies following properties:\n1. The regression loss is non-convex and non-concave. It is convex on W1 (or W2) when the other parameter W2 (or W1) is fixed, but not convex on both simultaneously;\n2. Every local minimum is a global minimum; 3. Every critical point that is not a global minimum is a saddle point;\n4. IfW2∗W1 is full-rank, the Hessian at any saddle point has at least one negative eigenvalue.\nWe refer interested readers to Baldi & Hornik (1989); Kawaguchi (2016) for detailed analysis.\nIn case of the uniform L2 feature penalty, the problem becomes:\nE(W1,W2) = E{ 1\n2 ||y −W2W1x||2 + λ1 2 ||W1x||2}+ λ2 2 ||W2||2F (6)\nAt the global minimum {W ∗1 ,W ∗2 }, we should have: ∂E\n∂W1 |W∗1 = W T 2 Σxy −WT2 W2W1Σxx + λ1W1Σxx = 0 (7)\n∂E\n∂W2 |W∗2 = ΣxyW T 1 −W2W1ΣxxWT1 + λ2W2 = 0 (8)\nwhere we define the variance and covariance matrix as Σxx = E{xxT }, Σxy = E{yxT }. Carrying out Eqn(7) ∗WT1 −WT2 ∗ Eqn(8) = 0 reveals a very interesting conclusion:\nλ1E{||W1x||2} = λ2||W2||2F (9) This reads as the expected L2 feature penalty should be equal to final-layer weight regularizer when converged. Or equivalently, when close to convergence, the L2 feature penalty reduces overfitting by implicitly penalizing the corresponding weight matrix W . A more generalized form is:\nLemma 1 For a cost function of form in Eqn (4) with uniform L2 feature regularization:\narg min W,φ\nE{l(W,φ(xi), yi) + λ1||φ(xi))||2}+ λ2||W ||2F\nwe have: λ1E{||φ(x)||2} = λ2||W ||2F (10)\nThe φ(.) can take a quite general form of a convolutional neural network with many common nonlinear operations such as the ReLU, max-pooling and so on. One can follow the derivation of Eqn(9) to easily derive Lemma 1.\nLemma 1 also reveals the importance of adding the weight penalty ||W ||2F in Eqn(4). If we only include the the feature penalty and drop the weight penalty (λ2 = 0 in our case), then a scaling as φ(.) = γφ(.) and W = 1γW with γ < 1 will always decrease the energy and the solution will become very ill-conditioned with γ → 0."
    }, {
      "heading" : "2.2.1 L2 FEATURE REGULARIZATION MAKES OPTIMIZATION EASIER",
      "text" : "Moreover, we analyze numerically how the L2 feature penalty influences the optimization process in our regression problem.\nWe study a special case {x ∈ Rd, y ∈ R}withW1 ∈ R1×m,W2 ∈ R and include offsets b1 ∈ R and b2 ∈ R to make the problem more general. Then, the latent representation becomes h = W1x + b1 and the prediction is ŷ = W2h+ b2. The cost function of Eqn(4) becomes:\nE(W1, b1,W2, b2) = 1\n2 E{(W2h+ b2 − y)2 + λ1 2 r2†h 2}+ λ2 2 W 22 (11)\nWe define the prediction residual r and r† as ŷ − y for better readability, and substitute αi = (ri†)2. Numerically, we apply a two-step process: in the first step, we calculate the sample-dependent ri† in the feed-forward process to obtain our L2 feature regularization weights αi for each (xi, yi); in the second step, we treat r† as a constant in the optimize. The gradient and Hessian matrix of Eqn(11) can be derived as: ∂E\n∂W2 = E{rhT }+ λ2W2\n∂E ∂b2 = E{r} ∂E ∂hi = W2r i + λ1(r i †) 2hi\n∂E\n∂W1 = W2E{rxT }+ λ1E{r2†hxT }\n∂E ∂b1 = W2E{r}+ λ1E{r2†h}\nand ∂2E\n∂W 22 = E{hhT }+ λ2\n∂2E ∂b22 = 1\n∂2E\n∂(hi)2 = W 22 + λ1(r i †) 2\n∂2E ∂W 21 = (W 22 + λ1E{r2†})E{xxT } ∂2E ∂b21 = W 22 + λ1E{r2†}\nSuppose that we apply a second-order optimization algorithm for the network parameter θ ∈ {w1, w2, b1, b2}, the updates should be M θ = −η ∗ (∂2E/∂θ2)−1(∂E/∂θ) with η > 0 as the step-size. If we unluckily start from a bad initialization point W2 → 0, the updates of hi, W1 and b1 are of the form:\nM hi = −η(W 22 + λ1(ri†)2})−1(W2ri + λ1(ri†)2hi) M w1 = −η[(W 22 + λ1E{r2†})E{xxT }]−1\n(W2E{r}+ λ1E{r2†h})E{x} M b1 = −η(W 22 + λ1E{r2†})−1(W2E{r}+ λ1E{r2†h})\nThe updates will become very ill-conditioned without the regularization term (λ1 = 0), since spectrum of the Hessian matrix is two-orders of infinitesimal O(W 22 ) and the gradient is of one-order O(W2). In comparison, with a reasonable choice of λ1 > 0, the computation can be largely stabilized when E{r2†} 6= 0.\nWhen the algorithm finally converges to a local minimum {W ∗1 , b∗1,W ∗2 , b∗2}, the expectation of parameter and latent feature should have gradient close to 0:\n∂E\n∂W2 = E{rhT }+ λ2W2 → 0\n∂E ∂b2 = E{r} → 0\nSubstituting this in the analysis of b1, we have: ∂E\n∂b1 = W2E{r}+ λ1E{αh} =⇒ E{αh} → 0 (12)\nIn other words, the feature penalty centralizes the final hidden layer representation h(x) = φ(x). Especially, in the uniformL2-feature penalty case, we simply drop α in Eqn (12) and have E{h} = E{φ(x)} → 0. In summary, the feature penalty improves the numerical stability of the optimization process in the regression problem. The conclusion also holds for the classification.\nSimilar results for φ(x) can be extended to deeper multilayer perceptrons with convex differentiable non-linear activation functions such as ReLU and max-pooling. In an m-layer model parametrized by {W1,W2, ...,Wm}, the Hessian matrix of hidden layers becomes strongly convex by backpropagating from the regularizer ||σm−1(Wm−1 ∗ σm−2(W2 ∗ (... ∗ σ1(W1x))))||2."
    }, {
      "heading" : "2.3 UNIFORM L2 FEATURE PENALTY IS A SOFT BATCH NORMALIZATION",
      "text" : "In our two case studies, we can observe that L2 feature penalty centers the representation φ(x), which reminds us of the batch normalization Ioffe & Szegedy (2015) with similar whitening effects. We reveal here that the two methods are indeed closely related in spirit.\nFrom the Bayesian point view, we analyze a binary classification problem: the probability of prediction ŷ given x observes a Bernoulli distribution p(ŷ|x, φ,W ) = Ber(ŷ|sigm(Wφ(x)), where φ(x) ∈ Rd is a neural network parametrized by φ and W ∈ R1×d. Assuming a factorized Gaussian prior on φ(x) ∼ N(0,Diag(σ21)) and W ∼ N(0,Diag(σ22)), we have the posterior as:\np(φ,W |{(xi, yi)}) = p({(xi, yi)}|φ,W )p(φ)p(W )\n∝ ∏ i ( 1 1 + e−Wφ(xi) )y i ( 1 1 + eWφ(xi) )1−y i exp(− ||φ(x i)||2 2σ21 ) ( √ 2πσ1)d exp(− ||W || 2 F 2σ22 ) ( √ 2πσ2)d\n(13)\nApplying the maximum-a-posteriori (MAP) principle to Eqn(13) leads to the following objective:\nE{l(W,φ(xi), yi) + 1 2σ21 ||φ(xi)||2}+ 1 2σ22 ||W ||2F + C (14)\nwith C = −d ln( √ 2πσ1) − d ln( √\n2πσ2) is a constant. This is exactly the uniform L2 version of Equation (4) with λ1 = 12σ21 , λ2 = 1 2σ22\nand αi = 1. The i.i.d. Gaussian prior on W and φ(x) has the effect of whitening the final representation.\nThe difference between uniform L2 feature penalty and batch normalization is that the former whitens φ(x) implicitly with an i.i.d. Gaussian prior during training, while the latter explicitly normalizes the output by keeping moving average and variance. We could say that the uniform L2 penalty is a soft batch normalization.\nAs analyzed in Wiesler & Ney (2011) this kind of whitening improved the numerical behavior and makes the optimization converge faster. In summary, the L2 feature regularization on φ(x) indeed tends to reduce internal covariate shift."
    }, {
      "heading" : "2.4 FEATURE REGULARIZATION MAY IMPROVE GENERALIZATION",
      "text" : "As pointed out in Hariharan & Girshick. (2016), the gradient penalty or feature regularization improves the performance of low-shot learning experimentally. Here, we give some preliminary analysis on how and why our modified model in Eqn 4 may improve generalization performance in neural network learning. Denoting the risk functional (testing error) R(W ) as:\nR(W ) = 1 2 E{||y −Wφ(x)||2} = 1 2 ∫ (x,y)∼P (x,y) ||y −Wφ(x)||2dP (x, y)\nand empirical risk function (training error) Remp(W ) on {(xi, yi)|i = 1, ..., N} as:\nRemp(W ) = 1\n2N N∑ i=1 ||yi −Wφ(xi)||2\nAs we know, the training and testing discrepancy depends on the model complexity Vapnik (1998):\nR(W )−Remp(W ) ≤ ν(W ) +O∗( h− ln η N ) (15)\nwhere O∗(.) denotes order of magnitude up to logarithmic factor. The upper bound 15 holds true with probability 1 − η for a chosen 0 < η < 1. In the equation, h is a non-negative integer named the VC-dimension. The right side of Eqn 15 contains two term: the first one ν(W ) is the error on training set while the second one models complexity of the learning system. In a multilayer neural network with ρ parameters and non-linear activations, the system has a VC dimension Sontag (1998) of O(ρ log ρ).\nIn our model in Eqn 4, the final cost function includes both feature and weight penalty as:\narg min W,φ\nE{l(W,φ(xi), yi) + λ1αi||φ(xi)||2}+ λ2||W ||2F\nEmpirically, the term λ2||W ||2F enforces large margin which limits the selection space of W . The term λ1||φ(xi)||2 not only improves numerical stability by feature whitening as discussed in the above sections, but also limits the selection of hidden layer parameter. These regularization terms thus reduces the VC-dimension of our learning. According to Eqn 15, the reduction of VC dimension of our model further reduces the theoretical discrepancy of training and testing performance."
    }, {
      "heading" : "3 EXPERIMENTAL RESULTS",
      "text" : "We evaluate our algorithm on three datasets: synthetic XOR datasets, the Omniglot low-shot benchmark and the large-scale ImageNet dataset. We use our own implementation of SGM approach Hariharan & Girshick. (2016) for a fair comparison."
    }, {
      "heading" : "3.1 SYNTHETIC DATASETS",
      "text" : "We first evaluate our model on the XOR dataset. Without loss of generality, we assume the data points x = [x1, x2]T are uniformly sampled from a rectangle x1 ∈ [−1, 1], x2 ∈ [−1, 1], and y(x) = I(x1 ∗ x2 > 0). The structure we use is a two-layer non-linear neural network with one\nlatent layer h = σ(W1x+b) where σ(.) is the rectified linear unit. ADAM Kingma & Ba (2014) is leveraged to numerically optimize the cross entropy loss.\nDuring training, we make use of only 4 points {[1, 1], [−1, 1], [1,−1], [−1,−1]}, while randomly sampled points from the whole rectangle as the test set. It is a very low-shot task. As shown in Figure 3, our model with both feature and weight regularization outperforms the gradient penalty Hariharan & Girshick. (2016) and no regularization. We use uniform feature penalty as in Eqn(4) and set λ1 = λ2 = 0.1 in our experiments."
    }, {
      "heading" : "3.2 LOW-SHOT LEARNING ON OMNIGLOT",
      "text" : "Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Omniglot training set contains 964 characters from different alphabets with only 20 examples per each character. The one-shot evaluation is a pairwise matching task on completely unseen alphabets.\nFollowing Vinyals et al. (2016), we use a simple yet powerful CNN as feature representation model, consisting of a stack of modules, each of which is a 3 × 3 convolution with 128 filters followed by batch normalizationIoffe & Szegedy (2015), a ReLU and 2× 2 max-pooling. We resized all images to 28×28 so that the resulting feature shape satisfies φ(x) ∈ R128. A fully connected layer followed by a softmax non-linearity is used to define the Baseline Classifier.\nWe set λ1=1e-4 in SGM Hariharan & Girshick. (2016) and λ1=λ2=1e-4 in our model. A nearest neighbor approach with L2 distance of feature φ(x) is applied for one-shot evaluation. As shown in Table 1, we can see that our model with both feature and weight penalty is able to achieve satisfactory performance of one-shot 91.5% accuracy, highly competitive with the state-of-the-art Matching NetworkVinyals et al. (2016) with CNN warm-start and RNN hyper-parameter tuning."
    }, {
      "heading" : "3.3 LARGE-SCALE LOW-SHOT LEARNING ON IMAGENET",
      "text" : "Our last experiment is on the ImageNet benchmark Russakovsky et al. (2015). It contains a wide array of classes with significant intra-class variation. We divide the 1000 categories randomly into 400 base for training and evaluate our feature representation on the 600 novel categories.\nWe use a 50-layer residual network He et al. (2016) as our baseline. Evaluation is measured by top-1 accuracy on the 600 test-set in a 20-way setup, i.e., we randomly choose 1 sample from 20 test classes, and applies a nearest neighbor matching. As shown in Table 2, we can see that our model learns meaningful representations for unseen novel categories even with large intra-class variance."
    }, {
      "heading" : "3.4 COMPARISON WITH BATCH-NORMALIZATION",
      "text" : "As discussed in Section 2.3, feature penalty has similar effects with batch normalization Ioffe & Szegedy (2015). It is of interest to compare the influence of two modules influence training performance of neural networks. We study the performance of the classification with and without each modules on four classic image classification benchmarks. For CIFAR-10 and ImageNet, we applied the Residual Net architecture He et al. (2016), while stacked convolution layers with ReLU and max-pooling is applied for MNIST and Omniglot. For ImageNet benchmark evaluation, we test the top-1 accuracy on the validation set with 50,000 images.\nSince in our model the feature penalty regularizer is applied only on the last hidden layer, we still keep the batch-normalization modules in previous layers in our “FP” model. As shown in Figure 4, we observe that baseline models with neither “BN” nor “FP” takes much longer to converge and achieve inferior performance; our “FP” regularizer achieves almost the same performance on MNIST (both 99%), CIFAR-10 (both 93%) and Omniglot 1-shot (88% BN v.s. 89% FP); on ImageNet, “BN” performs better than our “FP” (75% BN v.s. 74% FP). With both batch-normalization\nand feature penalty modules added, we achieve the best classification performance on all four benchmarks."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "In this work, we conduct an analysis, both empirically and theoretically, on how feature regularization influences and improves low-shot learning performance. By exploiting an XOR classification and two-layer linear regression, we find that the regularization of feature centers the representation, which in turn makes the learning problem easier with better numerical behavior. From the Bayesian point of view, the feature regularization is closely related to batch normalization. Evaluation on synthetic, “Omniglot” one-shot and large-scale ImageNet benchmark validates our analysis."
    } ],
    "references" : [ {
      "title" : "Neural networks and principal component analysis: Learning from examples without local minima",
      "author" : [ "Pierre Baldi", "Kurt Hornik" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Baldi and Hornik.,? \\Q1989\\E",
      "shortCiteRegEx" : "Baldi and Hornik.",
      "year" : 1989
    }, {
      "title" : "Cross-generalization: Learning novel classes from a single example by feature replacement",
      "author" : [ "Evgeniy Bart", "Shimon Ullman" ],
      "venue" : null,
      "citeRegEx" : "Bart and Ullman.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bart and Ullman.",
      "year" : 2005
    }, {
      "title" : "Theoretical models of learning to learn. Learning to learn",
      "author" : [ "Jonathan Baxter" ],
      "venue" : null,
      "citeRegEx" : "Baxter.,? \\Q1998\\E",
      "shortCiteRegEx" : "Baxter.",
      "year" : 1998
    }, {
      "title" : "Neural networks for pattern recogsontag, eduardo d. ”vc dimension of neural networks.” nato asi series f computer and systems sciences",
      "author" : [ "Christopher Bishop" ],
      "venue" : "69-96.nition. Oxford university press,",
      "citeRegEx" : "Bishop.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1998
    }, {
      "title" : "Learning a similarity metric discriminatively, with application to face verification",
      "author" : [ "Sumit Chopra", "Raia Hadsell", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Chopra et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2005
    }, {
      "title" : "Reducing overfitting in deep networks by decorrelating representations",
      "author" : [ "Michael Cogswell", "Faruk Ahmed", "Ross Girshick", "Larry Zitnick", "Dhruv Batra" ],
      "venue" : null,
      "citeRegEx" : "Cogswell et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cogswell et al\\.",
      "year" : 2016
    }, {
      "title" : "Object classification from a single example utilizing class relevance metrics",
      "author" : [ "Michael Fink" ],
      "venue" : null,
      "citeRegEx" : "Fink.,? \\Q2005\\E",
      "shortCiteRegEx" : "Fink.",
      "year" : 2005
    }, {
      "title" : "Neighborhood components analysis",
      "author" : [ "Jacob Goldberger", "Sam Roweis", "Geoff Hinton", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Goldberger et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Goldberger et al\\.",
      "year" : 2005
    }, {
      "title" : "Low-shot visual object recognition",
      "author" : [ "Bharath Hariharan", "Ross Girshick" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Hariharan and Girshick.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hariharan and Girshick.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "Kenji Kawaguchi" ],
      "venue" : null,
      "citeRegEx" : "Kawaguchi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kawaguchi.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Siamese neural networks for one-shot image recognition",
      "author" : [ "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov" ],
      "venue" : "ICML Deep Learning workshop,",
      "citeRegEx" : "Koch et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koch et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "Brenden Lake", "Tomer Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "One shot learning of simple visual concepts",
      "author" : [ "Brenden M. Lake", "Ruslan Salakhutdinov", "Jason Gross", "Joshua B. Tenenbaum" ],
      "venue" : "In CogSci,",
      "citeRegEx" : "Lake et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2011
    }, {
      "title" : "The multiverse loss for robust transfer learning",
      "author" : [ "Etai Littwin", "Lior Wolf" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Littwin and Wolf.,? \\Q2015\\E",
      "shortCiteRegEx" : "Littwin and Wolf.",
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "One-shot learning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Santoro et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Vc dimension of neural networks",
      "author" : [ "Eduardo Sontag" ],
      "venue" : "NATO ASI Series F Computer and Systems Sciences, pp",
      "citeRegEx" : "Sontag.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sontag.",
      "year" : 1998
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Lifelong learning algorithms",
      "author" : [ "Sebastian Thrun" ],
      "venue" : "Learning to learn, pp",
      "citeRegEx" : "Thrun.,? \\Q1998\\E",
      "shortCiteRegEx" : "Thrun.",
      "year" : 1998
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "Vladimir Vapnik" ],
      "venue" : "Data mining and knowledge discovery,",
      "citeRegEx" : "Vapnik.,? \\Q1998\\E",
      "shortCiteRegEx" : "Vapnik.",
      "year" : 1998
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "A convergence analysis of log-linear training",
      "author" : [ "Simon Wiesler", "Hermann Ney" ],
      "venue" : null,
      "citeRegEx" : "Wiesler and Ney.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wiesler and Ney.",
      "year" : 2011
    }, {
      "title" : "Automatic speech recognition",
      "author" : [ "Dong Yu", "Li Deng" ],
      "venue" : null,
      "citeRegEx" : "Yu and Deng.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu and Deng.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "It has achieved impressive results on many large-scale supervised tasks such as image classification Krizhevsky et al. (2012); He et al.",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "(2012); He et al. (2016) and speech recognition Yu & Deng (2012).",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "(2012); He et al. (2016) and speech recognition Yu & Deng (2012). Notably, these models are extensively hungry for data.",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "(2012); He et al. (2016) and speech recognition Yu & Deng (2012). Notably, these models are extensively hungry for data. In contrast, human beings have strong ability to learn novel concepts efficiently from very few or even one example. As pointed out in Lake et al. (2016), human learning is distinguished by its richness and efficiency.",
      "startOffset" : 8,
      "endOffset" : 275
    }, {
      "referenceID" : 8,
      "context" : "(2012); He et al. (2016) and speech recognition Yu & Deng (2012). Notably, these models are extensively hungry for data. In contrast, human beings have strong ability to learn novel concepts efficiently from very few or even one example. As pointed out in Lake et al. (2016), human learning is distinguished by its richness and efficiency. To test whether machines can approach this goal, Lake et al. propose the invaluable “Omniglot” hand-written character classification benchmark Lake et al. (2011), where each training class has very few examples and the ability to fast-learn is evaluated on never-seen classes with only one example.",
      "startOffset" : 8,
      "endOffset" : 502
    }, {
      "referenceID" : 8,
      "context" : "(2012); He et al. (2016) and speech recognition Yu & Deng (2012). Notably, these models are extensively hungry for data. In contrast, human beings have strong ability to learn novel concepts efficiently from very few or even one example. As pointed out in Lake et al. (2016), human learning is distinguished by its richness and efficiency. To test whether machines can approach this goal, Lake et al. propose the invaluable “Omniglot” hand-written character classification benchmark Lake et al. (2011), where each training class has very few examples and the ability to fast-learn is evaluated on never-seen classes with only one example. There has been previous work on attaining rapid learning from sparse data, denoted as meta-learning or learning-to-learn Thrun. (1998); Baxter.",
      "startOffset" : 8,
      "endOffset" : 774
    }, {
      "referenceID" : 2,
      "context" : "(1998); Baxter. (1998). Although used in numerous senses, the term generally refers to exploiting meta-knowledge within a single learning system across tasks or algorithms.",
      "startOffset" : 8,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "(1998); Baxter. (1998). Although used in numerous senses, the term generally refers to exploiting meta-knowledge within a single learning system across tasks or algorithms. In theory, a meta-learning is able to identify the right “inductive bias shifts” from previous experiences given enough data and many tasks Baxter. (1998). However, even if a well-designed convolutional neural network is a good “inductive bias shift” for a visual recognition task, it is still elusive to find the optimal parameter from a small training set without any prior knowledge.",
      "startOffset" : 8,
      "endOffset" : 328
    }, {
      "referenceID" : 2,
      "context" : "(1998); Baxter. (1998). Although used in numerous senses, the term generally refers to exploiting meta-knowledge within a single learning system across tasks or algorithms. In theory, a meta-learning is able to identify the right “inductive bias shifts” from previous experiences given enough data and many tasks Baxter. (1998). However, even if a well-designed convolutional neural network is a good “inductive bias shift” for a visual recognition task, it is still elusive to find the optimal parameter from a small training set without any prior knowledge. To alleviate this issue, low-shot learning methods have been proposed to transfer knowledge from various priors to avoid over-fitting, such as Bart & Ullman (2005). Recently, Hariharan & Girshick.",
      "startOffset" : 8,
      "endOffset" : 724
    }, {
      "referenceID" : 2,
      "context" : "(1998); Baxter. (1998). Although used in numerous senses, the term generally refers to exploiting meta-knowledge within a single learning system across tasks or algorithms. In theory, a meta-learning is able to identify the right “inductive bias shifts” from previous experiences given enough data and many tasks Baxter. (1998). However, even if a well-designed convolutional neural network is a good “inductive bias shift” for a visual recognition task, it is still elusive to find the optimal parameter from a small training set without any prior knowledge. To alleviate this issue, low-shot learning methods have been proposed to transfer knowledge from various priors to avoid over-fitting, such as Bart & Ullman (2005). Recently, Hariharan & Girshick. (2016) propose a novel prior of gradient penalty, which works pretty well experimentally.",
      "startOffset" : 8,
      "endOffset" : 764
    }, {
      "referenceID" : 2,
      "context" : "(1998); Baxter. (1998). Although used in numerous senses, the term generally refers to exploiting meta-knowledge within a single learning system across tasks or algorithms. In theory, a meta-learning is able to identify the right “inductive bias shifts” from previous experiences given enough data and many tasks Baxter. (1998). However, even if a well-designed convolutional neural network is a good “inductive bias shift” for a visual recognition task, it is still elusive to find the optimal parameter from a small training set without any prior knowledge. To alleviate this issue, low-shot learning methods have been proposed to transfer knowledge from various priors to avoid over-fitting, such as Bart & Ullman (2005). Recently, Hariharan & Girshick. (2016) propose a novel prior of gradient penalty, which works pretty well experimentally. Although an intuitive explanation is provided in Hariharan & Girshick. (2016) that a good solution of the network parameters should be stable with small gradients, it is mysterious why adding such a regularization magically improves the low-shot task by a large margin.",
      "startOffset" : 8,
      "endOffset" : 925
    }, {
      "referenceID" : 13,
      "context" : "Prior knowledge, or “inductive bias” Thrun. (1998), plays an important role in one-shot or low-shot learning.",
      "startOffset" : 37,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick.",
      "startOffset" : 48,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios.",
      "startOffset" : 48,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al.",
      "startOffset" : 48,
      "endOffset" : 313
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al.",
      "startOffset" : 48,
      "endOffset" : 447
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al.",
      "startOffset" : 48,
      "endOffset" : 476
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) .",
      "startOffset" : 48,
      "endOffset" : 625
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) .",
      "startOffset" : 48,
      "endOffset" : 671
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) . Our work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al.",
      "startOffset" : 48,
      "endOffset" : 854
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) . Our work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al. (2015); Goldberger et al.",
      "startOffset" : 48,
      "endOffset" : 874
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) . Our work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al. (2015); Goldberger et al. (2005); Chopra et al.",
      "startOffset" : 48,
      "endOffset" : 900
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) . Our work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al. (2015); Goldberger et al. (2005); Chopra et al. (2005). The insight lies in that a well-trained representational model have strong ability to generalize well on new tasks.",
      "startOffset" : 48,
      "endOffset" : 922
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) . Our work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al. (2015); Goldberger et al. (2005); Chopra et al. (2005). The insight lies in that a well-trained representational model have strong ability to generalize well on new tasks. In a recent work Santoro et al. (2016), DeepMind proposed a Memory Augmented Neural Network (MANN) to leverage the Neural-Turing-Machine for one-shot tasks.",
      "startOffset" : 48,
      "endOffset" : 1078
    }, {
      "referenceID" : 3,
      "context" : "Common techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016), a gradient penalty is introduced, which works well experimentally in low-shot scenarios. Various forms of feature regularization have been proposed to improve generalization: Dropout Srivastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have also been proposed to improve transfer learning performance, such as minimizing the correlation of features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) . Our work is also closely related to metric learning and nearest neighbor methods, in which representations from previous experience are applied in cross-domain settings Fink (2005); Koch et al. (2015); Goldberger et al. (2005); Chopra et al. (2005). The insight lies in that a well-trained representational model have strong ability to generalize well on new tasks. In a recent work Santoro et al. (2016), DeepMind proposed a Memory Augmented Neural Network (MANN) to leverage the Neural-Turing-Machine for one-shot tasks. However, in Vinyals et al. (2016), it is found that a good initialization such as VGG-Net largely improves the one-shot performance.",
      "startOffset" : 48,
      "endOffset" : 1230
    }, {
      "referenceID" : 11,
      "context" : "We refer interested readers to Baldi & Hornik (1989); Kawaguchi (2016) for detailed analysis.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "As we know, the training and testing discrepancy depends on the model complexity Vapnik (1998):",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 20,
      "context" : "In a multilayer neural network with ρ parameters and non-linear activations, the system has a VC dimension Sontag (1998) of O(ρ log ρ).",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "7% MANN Santoro et al. (2016) 36.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "7% MANN Santoro et al. (2016) 36.4% CNN Metric Learning 72.9% CNN (Our implementation) 85.0% Low-shot Hariharan & Girshick. (2016) 89.",
      "startOffset" : 8,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "7% MANN Santoro et al. (2016) 36.4% CNN Metric Learning 72.9% CNN (Our implementation) 85.0% Low-shot Hariharan & Girshick. (2016) 89.5% Matching Network Vinyals et al. (2016) 93.",
      "startOffset" : 8,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "Table 1: Experimental results of our algorithm on the Omniglot benchmark Lake et al. (2011).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Omniglot training set contains 964 characters from different alphabets with only 20 examples per each character.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Omniglot training set contains 964 characters from different alphabets with only 20 examples per each character. The one-shot evaluation is a pairwise matching task on completely unseen alphabets. Following Vinyals et al. (2016), we use a simple yet powerful CNN as feature representation model, consisting of a stack of modules, each of which is a 3 × 3 convolution with 128 filters followed by batch normalizationIoffe & Szegedy (2015), a ReLU and 2× 2 max-pooling.",
      "startOffset" : 72,
      "endOffset" : 321
    }, {
      "referenceID" : 15,
      "context" : "Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Omniglot training set contains 964 characters from different alphabets with only 20 examples per each character. The one-shot evaluation is a pairwise matching task on completely unseen alphabets. Following Vinyals et al. (2016), we use a simple yet powerful CNN as feature representation model, consisting of a stack of modules, each of which is a 3 × 3 convolution with 128 filters followed by batch normalizationIoffe & Szegedy (2015), a ReLU and 2× 2 max-pooling.",
      "startOffset" : 72,
      "endOffset" : 530
    }, {
      "referenceID" : 15,
      "context" : "Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Omniglot training set contains 964 characters from different alphabets with only 20 examples per each character. The one-shot evaluation is a pairwise matching task on completely unseen alphabets. Following Vinyals et al. (2016), we use a simple yet powerful CNN as feature representation model, consisting of a stack of modules, each of which is a 3 × 3 convolution with 128 filters followed by batch normalizationIoffe & Szegedy (2015), a ReLU and 2× 2 max-pooling. We resized all images to 28×28 so that the resulting feature shape satisfies φ(x) ∈ R. A fully connected layer followed by a softmax non-linearity is used to define the Baseline Classifier. We set λ1=1e-4 in SGM Hariharan & Girshick. (2016) and λ1=λ2=1e-4 in our model.",
      "startOffset" : 72,
      "endOffset" : 801
    }, {
      "referenceID" : 15,
      "context" : "Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Omniglot training set contains 964 characters from different alphabets with only 20 examples per each character. The one-shot evaluation is a pairwise matching task on completely unseen alphabets. Following Vinyals et al. (2016), we use a simple yet powerful CNN as feature representation model, consisting of a stack of modules, each of which is a 3 × 3 convolution with 128 filters followed by batch normalizationIoffe & Szegedy (2015), a ReLU and 2× 2 max-pooling. We resized all images to 28×28 so that the resulting feature shape satisfies φ(x) ∈ R. A fully connected layer followed by a softmax non-linearity is used to define the Baseline Classifier. We set λ1=1e-4 in SGM Hariharan & Girshick. (2016) and λ1=λ2=1e-4 in our model. A nearest neighbor approach with L2 distance of feature φ(x) is applied for one-shot evaluation. As shown in Table 1, we can see that our model with both feature and weight penalty is able to achieve satisfactory performance of one-shot 91.5% accuracy, highly competitive with the state-of-the-art Matching NetworkVinyals et al. (2016) with CNN warm-start and RNN hyper-parameter tuning.",
      "startOffset" : 72,
      "endOffset" : 1166
    }, {
      "referenceID" : 18,
      "context" : "Table 2: Experimental results of our algorithm on the ImageNet benchmark Russakovsky et al. (2015) with the 20-way one-shot setting.",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "Our last experiment is on the ImageNet benchmark Russakovsky et al. (2015). It contains a wide array of classes with significant intra-class variation.",
      "startOffset" : 49,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "We use a 50-layer residual network He et al. (2016) as our baseline.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "For CIFAR-10 and ImageNet, we applied the Residual Net architecture He et al. (2016), while stacked convolution layers with ReLU and max-pooling is applied for MNIST and Omniglot.",
      "startOffset" : 68,
      "endOffset" : 85
    } ],
    "year" : 2017,
    "abstractText" : "Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously datahungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close scrutiny reveals the centering effect of feature representation, as well as the intrinsic connection with batch normalization. Extensive experiments on synthetic datasets, the one-shot learning benchmark “Omniglot”, and large-scale ImageNet validate our analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}