{
  "name" : "692.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Zhigang Yuan", "Yuting Hu", "Yongfeng Huang" ],
    "emails" : [ "hu-yt12}@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Sentiment classification, a key problem in sentiment analysis, has drawn lots of interest since 2000s (Pang & Lee, 2008). Traditional machine learning based methods often use one-hot features to represent a text. In these methods, each word (or n-gram) is treated as an independent token, and a text is represented as the words within. Though convenient intuitively, these methods cannot model the semantic dependencies between words, and often lead to the curse of dimensionality problem.\nNeural models have shown their great performance in learning representation. Bengio et al. (2003) proposed the feed forward neural network language model. Ever since, many neural network based models have been widely used in many natural language processing (NLP) tasks, such as named entity recognization (Collobert & Weston, 2008), machine translation (Cho et al., 2014) and sentiment analysis (dos Santos & Gatti, 2014). Compared to one-hot representation, words are represented as distributed, dense vectors (also called word embeddings) in these neural models. Mikolov et al. (2013b) showed that, through proper training, these vectors can model syntactic and semantic relationships between words. More importantly, these vectors can be calculated naturally, which leads to another problem — semantic composition.\nFor many NLP tasks, we often face the problem of representing a sentence or paragragh using word embeddings of its containing words. Simply methods such as concatenation or weighted sum cannot give us satisfactory results. Many neural models, such as recursive neural network (Socher et al., 2011), recurrent neural network (Mikolov et al., 2010) and convolutional neural networks (Collobert & Weston, 2008) have been proposed to solve this problem.\nSince a sentence or paragraph is naturally a sequence of word tokens, recurrent neural networks (RNN) can be used to composite word embeddings into a sentence or paragraph embedding. Recurrent networks utilize special units which are connected to themselves to maintain a hidden state. The internal hidden state can be seen as a representation of the contexts. However, basic recurrent network suffers from the vanishing or exploding gradients problem (Hochreiter et al., 2001), due to its direct connection between states in adjacent timesteps. Long short term memory (LSTM) network alleviates this problem by introducing a cell that has the ability to read, write or reset its internal state (Graves, 2012). Even so, LSTM has the bias of prefering recent inputs, making it difficult to capture the important information within a long sequence.\nRecently, many studies have been done to help RNNs improve their semantic composition performance. A common way is to introduce another kind of network, such as CNN into RNN (Lai et al., 2015). In these combined models, different parts share complementary advantages. Lately, the attention mechanism (Bahdanau et al., 2014) has drawn a lot of attention. In an attention model, we utilize states in all timesteps, rather than simply force the network to represent the context into one fixed-length vector. More importantly, by visualizing the attention weights, we can get a deeper insight of how the network works (Bahdanau et al., 2014; Rocktäschel et al., 2015).\nInspired by the attention thought, we propose a framework which is similar to how we human beings read a text and then answer an related question. This framework consists of two parts: the first part is a bidirectional LSTM (Bi-LSTM) to learn a rough representation of the whole text, the second part is another Bi-LSTM with attention, which can learn local contexts and incorporate them to get a refined context representation."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In this section, we introduce the related work concerning sentiment analysis, neural semantic composition and attention mechanism."
    }, {
      "heading" : "2.1 SENTIMENT ANALYSIS",
      "text" : "Sentiment analysis, also called opinion mining or subjectivity analysis, is the field of study that analyzes people’s sentiments or emotions towards various entities (Liu, 2012). Due to its great importance, sentiment analysis has a wide range of applications, such as finding opinions of certain products, predicting the stock market or political elections.\nGenerally, existing methods for sentiment classification fall into two categories: machine-learningbased methods and semantic-based methods. Semantic methods relies more on lexcial and linguistic resources. Pang et al. (2002) first tried using machine learning methods to classifiy texts based on their sentiments. Ever since, a lot more classification models and features have been tested. For machine learning procedures, feature design is always the key step for a good result. While in most cases, a well-designed feature needs much human effort and domain knowledge, which severely limits its applications.\nDeep learning models have drawn lots of attention due to their automatic representation learning. Deep learning is about learning multiple levels of representations, obtained by combining non-linear modules (LeCun et al., 2015). Various models have been tested for sentiment analysis task, such as recursive autoencoder (Socher et al., 2011), paragraph vector (Le & Mikolov, 2014), convolutional network (Kim, 2014), etc."
    }, {
      "heading" : "2.2 NEURAL LANGUAGE MODELS AND SEMANTIC COMPOSITION",
      "text" : "Bengio et al. (2003) proposed a feed-forward neural network language model, which used distributed representations for words. With the rapid progress of neural language models for NLP tasks, the distributed representations of words have become more and more important.\nIn most neural language models, the words (or even characters) are represented as dense, distributed vectors. These embeddings are usually used as the inputs of a network. The subsequent layers then conduct semantic composition operations over these word/character embeddings to obtain higher level semantic represetations. This kind of representations based on embeddings of different seman-\ntic levels (character, word, phrase, sentence, paragraph, etc.) have shown their great performance on many NLP tasks.\nAfter Bengio’s pioneering work, Collobert & Weston (2008) replaced the probabilistic output with a scoring scheme, and provided a unified architecture for many NLP tasks. Mnih & Hinton (2007) used a log-bilinear model to get the hidden representations. They future provided a hierarchical version (Mnih & Hinton, 2009), which uses a hierarchical softmax at the output layer to reduce calculation amount. Huang et al. (2012) added the global context information into C&W model, which can improve word embeddings as well as solve the polysemy problem. Their idea of using global context inspired our work, while in our work we use the global context as attention for the following training. Socher et al. (2011; 2012; 2013) proposed a family of recursive autoencoder models, which construct higher level represetations using a tree structure. Mikolov et al. (2013a) provided their word2vec (CBOW and Skip-gram) model. The word2vec model is actually a simple but effective log-linear model, imbibing advantages of previous neural models. Word embeddings pretrained using word2vec on a large corpus can be used for many other NLP tasks, often used as initializations or auxiliary features. Pennington et al. (2014) performed the training on aggregated global word-word co-occurrence statistics from a corpus, and their resulting global vectors (GloVe) showcase interesting linear substructures of the word vector space. Le & Mikolov (2014) extended the idea of word2vec model from word to paragragh, and the paragraph vectors they got can improve the performance on various NLP tasks.\nAlthough seemingly different, these models all conduct semantic compositions directly or indirectly at intermediate layers. Currently, various neural models have been tried for semantic composition, such as multi-layer perceptron, recusive network, convolutional network and recurrent network. When representing higher level language units, convolutinal netwoks are better at capturing local information, while recurrent networks excel at storing history information. The network structure has also been explored, such as sequencial or tree structure (Li et al., 2015).\nOf all these models, recurrent networks are highly valued due to its human alike reading style. More importantly, recurrent networks are capable of modeling sequencial inputs or outputs, which is essential for tasks such as machine translation (Cho et al., 2014). When applying recurrent networks for NLP tasks, it is typical to encode the semantic information of all words within a sentence into one fixed-length vector. The resulting representation is then used for generating related output words or simply used for classification."
    }, {
      "heading" : "2.3 ATTENTION MECHANISM",
      "text" : "Attention mechanism is first used for machine translation (Bahdanau et al., 2014) in a encoderdecoder framework. The key idea of attention is to allow the network revisit all parts of a source sentence for an output decision, instead of trying to encode all information of a source sentence into a fixed-length vector. By this mechanism, we can get a deeper insight of how the network works. In the work of Bahdanau et al. (2014), they found the alignment between words by visualizing the attention weights.\nSuccessive studies on attention mechanism include image caption (Xu et al., 2015), semantic entailment (Rocktäschel et al., 2015), aspect level sentiment analysis (Tang et al., 2016), etc. The most related work to ours is an attention model for sequence classification (Shen & Lee, 2016), in which they used LSTM output to weight word embeddings. While in our work, we use the global context to selectively choose the most important local contexts, which provides mutual promotions for both global and local context representations."
    }, {
      "heading" : "3 LEARNING TO UNDERSTAND: THE MODEL",
      "text" : "It’s widely accepted that RNN alone, even with LSTM cells, cannot store long-time information. But, we may ask, do we really have to get a refined document or text embedding? Taking the reading comprehension task as an example, after a quick look at the text given, what we human beings got is also a rough memory of this text. Even so, this did not affect us to make a right decision. What we typically do next is to look at the problem, and rescan the text with a rough memory of the whole text in mind. With this global context information, we can easily locate the important parts for our\nproblem. In the end, even if we have not understood the whole text thoroughly, we can make right decisions for the problem in hand.\nThis thought inspired us to propose this attention model, which incorporates local contexts with a rough global context attention."
    }, {
      "heading" : "3.1 BASIC MODEL: BI-LSTM",
      "text" : "Long short term memory network can be seen as a variant of the basic recurrent network, mainly to address the long-time dependency probem. It has special units that can pass their states to the next timestep. LSTM network replaces the hidden units with more flexible cells. The cell itself is a small network, which can decide its own reading, writing or resetting behavior.\nThe LSTM network we use is similar to Graves (2012). We don’t consider peephole connections. Given an input sequence s = (x1, x2, · · · , xn), (xi ∈ Rm), the update procedure from timestep t− 1 to t can be described as follows:\nit = σ(W ixxt + U ihht−1 + b i) (1)\nft = σ(W fxxt + U fhht−1 + b f ) (2)\not = σ(W oxxt + U ohht−1 + b o) (3)\nC̃t = φ(W cxxt + U chht−1 + b c) (4)\nCt = ft Ct−1 + it C̃t (5) ht = ot φ(ct) (6)\nwhere denotes the element-wise multiplication, it, ft, ot ∈ Rh stands for the input gate, forget gate, output gate respectively. The LSTM cell maintains an internal cell state. In one timestep, the input gate determines how much the outside information can come in, the output gate determines how much the internal information can go out, and the forget gate determines how much the internal information should be forgotten when passed to next timestep.\nFor a given sequence s = (x1, x2, · · · , xn), if we feed the token from left to right one by one into the LSTM network, the final internal state −→h will contain more information of the latter tokens due to the model bias. We feed the sequences backwards into the LSTM again, thus getting a backwardpropagating internal state←−h . The final representation of the global context is calculated by\nh = −→ h ⊕←−h (7)\nwhere ⊕ denotes concatenation. This is a typical bidirectional LSTM (Bi-LSTM) network. Again, we point out that this gloal context representation may be just a rough one. We use it as attention rather than directly treat it as a refined representation for the task afterwards."
    }, {
      "heading" : "3.2 TWO-SCAN APPROACH WITH ATTENTION",
      "text" : "In this part, we introduce our Two-Scan Approach with Attention (TS-ATT) model.\nThe TS-ATT model contains two parts: a Bi-LSTM network first scans the whole text to get a global context represetation, another Bi-LSTM with attention scans the text again to get local contexts around each token, and composite them using the global context attention. The model are shown in Figure (1).\nFirst Scan: we first use a Bi-LSTM network to gain a global context representation. As described in Sec. (3.1), the final global context vector h is a concatention of the final output of left-LSTM←−h and right-LSTM −→h . This global context serves as attention for the second scan. To distinguish the final context representation in the second scan, we denote h as hscan1, which is\nhscan1 = ←− h ⊕−→h (8)\nSecond Scan: During the second scan of the text, we use another Bi-LSTM networks to learn local context for each word. Since already with the rough global context in mind, when looking at one word, we actually see its context meaning rather than the word itself. Thus, we use a word’s local context vector instead of its word embedding here. Inspired by the work of Lai et al. (2015), for word wi, we concatenate its left context c (l) i , right context c (r) i and embedding ei to construct its final local context represetation ci, which is:\nc (l) i = f\n( W (lc)c\n(l) i+1 +W (le)ei\n) (9)\nc (r) i = f\n( W (rc)c\n(r) i−1 +W (re)ei\n) (10)\nci = c (l) i ⊕ ei ⊕ c (r) i (11)\nAttention: As we may expect, during the second scan, we don’t have to encode every word into the text representation. Just paying attention to the parts which are important for our problem in hand would be enough. Thus, we use the global context vector hscan1 as the attention of all the local contexts. Attention weights are acquired by one-layer feed-forward network, which is:\nyatti = f ( W att(hscan1 ⊕ ci) + batt ) (12)\nwhere W att and batt are attention parameters. All the attention weights are then fed into a softmax layer to obtain probabilistic attention weights. The final representation used for classification (denoted as hscan2) is the weighted sum of all the local context vectors, which is:\nαi = exp(y (a) i )∑n\ni=1 exp(y (a) i )\n(13)\nhscan2 =\nn∑\ni=1\nαici (14)\nThe context vector hscan2 is then fed into a fully-connected layer and a softmax layer to do the classification task. The whole network is training to minimize the cross-entropy error E = 1N ∑N\ni=1H(y|p) = − 1N ∑N i=1 ∑n j=1 yj log pj over all training examples, where p denotes the predictions and y the true labels. Parameters are updated using the RMSProp (Tieleman & Hinton, 2012) rule."
    }, {
      "heading" : "3.3 SINGLE-SCAN APPROACH WITH ATTENTION",
      "text" : "In the proposed TS-ATT model, we use two Bi-LSTM netwoks to conduct these two scans. However, sometimes we may adopt another faster, but may equally good approach: a single scan. In this way, we first look at the problems to be solved, then scan the original text looking for clues. During this process, we maintain a rough gloal context and pay attention to the important parts simultaneously.\nInspired by this thought, we proposed the Single-Scan Approach with Attention (SS-ATT) model, which is a reduced version of TS-ATT model. In this model, the final outputs of the second BiLSTM are directly used as attention, saving the need of the first Bi-LSTM network. The SS-ATT model is shown in Figure (2)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We evaluated our models on some benchmark datasets for sentiment classification. These datasets includes:\n• Amazon1 by Blitzer et al. (2007). Amazon dataset contains product reviews from Amazon.com from many product types. This dataset is origially used for domain adaption, here, we only use it for open-domain classification, regardless of the domain difference.\n• IMDB2 by Maas et al. (2011), a large movie review dataset collected from imdb.com website.\n• Yelp 20133 User reviews and recommendations dataset, which are built by Tang et al. (2015).\nThe statistical information of these datasets are listed in Table (1).\nFor IMDB, the original dataset already has a train/test split, we stick to this behavior in our experiments. For Amazon, we split 20% of the data as the test set. For Yelp datasets, they also have a train/validation/test split.\nFor datasets without an explicit validation split (like IMDB and Amazon), we randomly select 10% of the training data as the validation data, which is used for hyperparameter tuning and early-stop in training."
    }, {
      "heading" : "4.1 BASELINES",
      "text" : "We compare our attention model with some models related to our work. These models contain traditional machine learning methods such as support vector machine (SVM) as well as neural models such as LSTM.\nTraditional machine learning methods:\n(1) SVM+BOW: Support Vector Machine with Bag Of Words representation. SVM uses hinge loss to maximize the margin between different classes.\n(2) NB+BOW: Naive Bayes with Bag Of Words representation. NB chooses the class with the largest posterior probability, which is calculated through priori probability and likelihood based on training data.\nNeural model methods:\n(3) CNN: Convolutional Neural Network for sentence classification model proposed by Kim (2014). We use 100 filters to evaluation this model.\n(4) LSTM: Long Short Term Memory network (Graves, 2012)\n(5) Bi-LSTM: Bi-directional LSTM network (Graves, 2012)\n(6) RCNN: Recurrent Convolutional Neural Network proposed by Lai et al. (2015), which uses a bi-directional recurrent network to replace the convolution operation in a typical convolutional network.\n(7) NAM: Neural Attention Model proposed by Shen & Lee (2016). This model uses LSTM output as attention directly for word embeddings."
    }, {
      "heading" : "4.2 SETUP",
      "text" : "For texts of these datasets, we split them into word tokens using punctuations and spaces as delimiters. Unlike some studies earlier, we process each text as a single unit rather than split them into sentences, which actually provides us more challenges. We process texts this way mainly to test the model’s ability to construct representation of long texts, which requires the model to learn long-time dependency.\nFor the neural models, we set the word embedding dimention to be 300, and LSTM state dimention to be 300. For training, we use stochastic gradient descent based on mini-batches with a batch size of 32. The training process is monitored by the validation accuracy for early-stopping. After training, the model is evaluated by a separate test dataset.\nDue to the different pre-processing and model parameters selecting scheme, our results may not look the same as they are reported in other papers. For a fair comparison, we rerun all the models using the same platform of our experiment. No tricks such as dropout, batch-normaliztion are used, since we want a comparison of these models themselves. Similarly, we use random initial word embeddings instead of pretrained embeddings such as word2vec (Mikolov et al., 2013a)."
    }, {
      "heading" : "4.3 RESULTS AND DISCUSSIONS",
      "text" : "Our experimental results are shown in Table (2).\nAs shown in Table (2), our methods (TS-ATT or SS-ATT) achieve the best accuracy on two of three datasets (IMDN and Yelp2013). For Amazon dataset, the result of our method is almost as good as the best model (CNN). These results demonstrate the effectiveness of our proposed models.\nFrom Tabel (2), we can see CNN shows good performance in these sentiment classification tasks. But, CNN has many hyper-parameters to tune, such as filter numbers, filter size, pooling size, etc. Changes of these hyper-parameters affect CNN’s performance to a large scale. Zhang & Wallace (2015) did a sensitivity analysis of CNNs for sentence classification. Compared with CNNs, recurrent based networks have fewer hypermeters to fine-tune. Even so, basic neural network such as LSTM doesn’t perform so well, as shown in Table (2). Bi-LSTM fails to make significant improvements on these tasks.\nComparing with RCNN, which uses a max-pooling operation after an Bi-LSTM, our model surpass it on all three datasets (1.87%, 3.99% and 1.45% respectively). This result demonstrates the effectiveness of using global context (though a rough one) as attention. Further, our SS-ATT model can be seen as a single Bi-LSTM network with global attention. The great improvement compared to Bi-LSTM also witnesses the key role that global attention plays.\nComparing with NAM, which use global attention directly on word embeddings, our model also achieves big or small improvements (4.02%, 0.72% and 1.61% respectively). These improvements over NAM model indicate that incorporating local contexts with attention would be more suitable for text understanding.\nAs for our proposed models, we can see TS-ATT and SS-ATT can achieve approximately equally good results. The key difference of these two models is that SS-ATT uses only one Bi-LSTM, improving global and local context representation mutually when training. These results are consistent with our intuition of human-like reading style. Imagine when in a reading comprehension test, we may skip reading the whole texts at first, but directly look at the texts after knowing the questions. In this way, we are actually using single-scan approach to save time. But as long as we have the big picture (global context) in mind, this kind of process won’t harm our right decisions."
    }, {
      "heading" : "5 ATTENTION VISUALIZATION AND CASE STUDY",
      "text" : "In order to get a deeper insight into our global-local context attention model, we track the attention weights when the model is trying to make a decision. Take the IMDB dataset as example, when evaluating our model, we accumulate the attention weights for each word appeared, then calculate their average attentions. By this way, we can get a better knowledge about what kind of words this model favors when trying to predict the sentiment of a text. The words with largest average attention are shown in Table (3). We select those words with a minimal appearing frequency (for IMDB we set it to 100, change of this number won’t affect the results so much).\nFrom Table (3), we can see that our attention model can effectively find the most discriminative features. Word such as surprised is actually negative in most cases, while in our model it’s classified as positive correctly, since we usually use surprised to express our pleasance when some movie is beyond our expectation. Also, we notice that the word recommended is favored by\nboth polarities. In this case, a model has to rely its local context (such as not recommended or recommended) to make a right decision. This will be illustrated in following case study.\nSince our method incorporates local contexts to work, we next provide some case studies where important local contexts are automatically selected for right decisions. We use cases where the word recommended serving a high attention and then dive into its surrounding words to support our thought above.\nFigure (3) shows the attention weights of each word in the sample sentence segment or music of the and greeks - this movie is strongly recommended for lovers of the music. Figure (4) shows another negative sentence segment the new zealand film commission fourthly - a friend recommended it to me - however i was utterly disappointed.\n-10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4\nwords around \"recommended\"\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\n0.009\na tt\ne n ti\no n w\ne ig\nh ts\normusic\nof the\nand\ngreeks this movie is strongly recommended\nfor\nlovers of\nthe\nFigure 3: A positive example -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 words around \"recommended\"\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\n0.009\n0.010\na tt\ne n ti\no n w\ne ig\nh ts\nthe new\nzealand\nfilm commission fourthly a friend recommended\nit to\nme\nhowever\ni\nwas\nutterly\ndisappointed\nthe\nFigure 4: A negtive example\nFrom Figure (3) and Figure (4), we can see that the model actually concentrations on important local contexts rather than single embeddings. This further verifies our initial idea that, using a global context to incorporate local contexts is a suitable way for sentiment or more general text classification task."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "In this paper, we proposed a global-local context attention framework for sentiment analysis. This method is similar to human’s reading behavior in a reading comprehension situation. First, this model uses a Bi-LSTM network to extract a global context representation. This global representation maybe inaccurate, but we only use it as attention for important local parts. In a second scan of the text, the model automatically finds the most important local contexts and incorporates them to make a final decision. A simple version of this model only needs one-scan of text, which can reduce the model complexity but almost equally effective. Experimental results demonstrate that our model performs better than existing related models. Using global context representation as attention, our model can effectively find the most important local parts to make a right decision."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This research is supported by the Key Program of National Natural Science Foundation of China (Grant nos. U1536201 and U1405254), the National Natural Science Foundation of China (Grant no. 61472092), the National High Technology Research and Development Program of China (863 Program) (Grant no. 2015AA020101), the National Science and Technology Support Program of China (Grant no. 2014BAH41B00), and the Initiative Scientific Research Program of Tsinghua University."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin" ],
      "venue" : "journal of machine learning research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Blitzer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Collobert and Weston.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Deep convolutional neural networks for sentiment analysis of short texts",
      "author" : [ "Cı́cero Nogueira dos Santos", "Maira Gatti" ],
      "venue" : "In COLING, pp",
      "citeRegEx" : "Santos and Gatti.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Gatti.",
      "year" : 2014
    }, {
      "title" : "Neural networks. In Supervised Sequence Labelling with Recurrent Neural Networks, pp. 15–35",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "Graves.,? \\Q2012\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2012
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning",
      "author" : [ "Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jürgen Schmidhuber" ],
      "venue" : "long-term dependencies,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume",
      "citeRegEx" : "Huang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "arXiv preprint arXiv:1408.5882,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Recurrent convolutional neural networks for text classification",
      "author" : [ "Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Lai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V Le", "Tomas Mikolov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Le and Mikolov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "When are tree structures necessary for deep learning of representations",
      "author" : [ "Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky", "Eudard Hovy" ],
      "venue" : "arXiv preprint arXiv:1503.00185,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Sentiment analysis and opinion mining",
      "author" : [ "Bing Liu" ],
      "venue" : "Synthesis lectures on human language technologies,",
      "citeRegEx" : "Liu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
      "citeRegEx" : "Maas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Mnih and Hinton.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2007
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Andriy Mnih", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mnih and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2009
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "Foundations and trends in information retrieval,",
      "citeRegEx" : "Pang and Lee.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "Thumbs up?: sentiment classification using machine learning techniques",
      "author" : [ "Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan" ],
      "venue" : "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume",
      "citeRegEx" : "Pang et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2002
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Phil Blunsom" ],
      "venue" : "arXiv preprint arXiv:1509.06664,",
      "citeRegEx" : "Rocktäschel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural attention models for sequence classification: Analysis and application to key term extraction and dialogue act detection",
      "author" : [ "Sheng-syun Shen", "Hung-yi Lee" ],
      "venue" : null,
      "citeRegEx" : "Shen and Lee.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen and Lee.",
      "year" : 2016
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning semantic representations of users and products for document level sentiment classification",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Tang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Aspect level sentiment classification with deep memory network",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu" ],
      "venue" : "arXiv preprint arXiv:1605.08900,",
      "citeRegEx" : "Tang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Lecture 6.5-rmsprop, coursera: Neural networks for machine learning",
      "author" : [ "T Tieleman", "G Hinton" ],
      "venue" : "Technical report, Technical report,",
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification",
      "author" : [ "Ye Zhang", "Byron Wallace" ],
      "venue" : "arXiv preprint arXiv:1510.03820,",
      "citeRegEx" : "Zhang and Wallace.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang and Wallace.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Ever since, many neural network based models have been widely used in many natural language processing (NLP) tasks, such as named entity recognization (Collobert & Weston, 2008), machine translation (Cho et al., 2014) and sentiment analysis (dos Santos & Gatti, 2014).",
      "startOffset" : 199,
      "endOffset" : 217
    }, {
      "referenceID" : 25,
      "context" : "Many neural models, such as recursive neural network (Socher et al., 2011), recurrent neural network (Mikolov et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : ", 2011), recurrent neural network (Mikolov et al., 2010) and convolutional neural networks (Collobert & Weston, 2008) have been proposed to solve this problem.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Bengio et al. (2003) proposed the feed forward neural network language model.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Bengio et al. (2003) proposed the feed forward neural network language model. Ever since, many neural network based models have been widely used in many natural language processing (NLP) tasks, such as named entity recognization (Collobert & Weston, 2008), machine translation (Cho et al., 2014) and sentiment analysis (dos Santos & Gatti, 2014). Compared to one-hot representation, words are represented as distributed, dense vectors (also called word embeddings) in these neural models. Mikolov et al. (2013b) showed that, through proper training, these vectors can model syntactic and semantic relationships between words.",
      "startOffset" : 0,
      "endOffset" : 512
    }, {
      "referenceID" : 7,
      "context" : "However, basic recurrent network suffers from the vanishing or exploding gradients problem (Hochreiter et al., 2001), due to its direct connection between states in adjacent timesteps.",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "Long short term memory (LSTM) network alleviates this problem by introducing a cell that has the ability to read, write or reset its internal state (Graves, 2012).",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "A common way is to introduce another kind of network, such as CNN into RNN (Lai et al., 2015).",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Lately, the attention mechanism (Bahdanau et al., 2014) has drawn a lot of attention.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "More importantly, by visualizing the attention weights, we can get a deeper insight of how the network works (Bahdanau et al., 2014; Rocktäschel et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "More importantly, by visualizing the attention weights, we can get a deeper insight of how the network works (Bahdanau et al., 2014; Rocktäschel et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "Sentiment analysis, also called opinion mining or subjectivity analysis, is the field of study that analyzes people’s sentiments or emotions towards various entities (Liu, 2012).",
      "startOffset" : 166,
      "endOffset" : 177
    }, {
      "referenceID" : 25,
      "context" : "Various models have been tested for sentiment analysis task, such as recursive autoencoder (Socher et al., 2011), paragraph vector (Le & Mikolov, 2014), convolutional network (Kim, 2014), etc.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : ", 2011), paragraph vector (Le & Mikolov, 2014), convolutional network (Kim, 2014), etc.",
      "startOffset" : 70,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Sentiment analysis, also called opinion mining or subjectivity analysis, is the field of study that analyzes people’s sentiments or emotions towards various entities (Liu, 2012). Due to its great importance, sentiment analysis has a wide range of applications, such as finding opinions of certain products, predicting the stock market or political elections. Generally, existing methods for sentiment classification fall into two categories: machine-learningbased methods and semantic-based methods. Semantic methods relies more on lexcial and linguistic resources. Pang et al. (2002) first tried using machine learning methods to classifiy texts based on their sentiments.",
      "startOffset" : 167,
      "endOffset" : 585
    }, {
      "referenceID" : 12,
      "context" : "The network structure has also been explored, such as sequencial or tree structure (Li et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "More importantly, recurrent networks are capable of modeling sequencial inputs or outputs, which is essential for tasks such as machine translation (Cho et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Huang et al. (2012) added the global context information into C&W model, which can improve word embeddings as well as solve the polysemy problem.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "Huang et al. (2012) added the global context information into C&W model, which can improve word embeddings as well as solve the polysemy problem. Their idea of using global context inspired our work, while in our work we use the global context as attention for the following training. Socher et al. (2011; 2012; 2013) proposed a family of recursive autoencoder models, which construct higher level represetations using a tree structure. Mikolov et al. (2013a) provided their word2vec (CBOW and Skip-gram) model.",
      "startOffset" : 0,
      "endOffset" : 460
    }, {
      "referenceID" : 7,
      "context" : "Huang et al. (2012) added the global context information into C&W model, which can improve word embeddings as well as solve the polysemy problem. Their idea of using global context inspired our work, while in our work we use the global context as attention for the following training. Socher et al. (2011; 2012; 2013) proposed a family of recursive autoencoder models, which construct higher level represetations using a tree structure. Mikolov et al. (2013a) provided their word2vec (CBOW and Skip-gram) model. The word2vec model is actually a simple but effective log-linear model, imbibing advantages of previous neural models. Word embeddings pretrained using word2vec on a large corpus can be used for many other NLP tasks, often used as initializations or auxiliary features. Pennington et al. (2014) performed the training on aggregated global word-word co-occurrence statistics from a corpus, and their resulting global vectors (GloVe) showcase interesting linear substructures of the word vector space.",
      "startOffset" : 0,
      "endOffset" : 807
    }, {
      "referenceID" : 7,
      "context" : "Huang et al. (2012) added the global context information into C&W model, which can improve word embeddings as well as solve the polysemy problem. Their idea of using global context inspired our work, while in our work we use the global context as attention for the following training. Socher et al. (2011; 2012; 2013) proposed a family of recursive autoencoder models, which construct higher level represetations using a tree structure. Mikolov et al. (2013a) provided their word2vec (CBOW and Skip-gram) model. The word2vec model is actually a simple but effective log-linear model, imbibing advantages of previous neural models. Word embeddings pretrained using word2vec on a large corpus can be used for many other NLP tasks, often used as initializations or auxiliary features. Pennington et al. (2014) performed the training on aggregated global word-word co-occurrence statistics from a corpus, and their resulting global vectors (GloVe) showcase interesting linear substructures of the word vector space. Le & Mikolov (2014) extended the idea of word2vec model from word to paragragh, and the paragraph vectors they got can improve the performance on various NLP tasks.",
      "startOffset" : 0,
      "endOffset" : 1032
    }, {
      "referenceID" : 0,
      "context" : "Attention mechanism is first used for machine translation (Bahdanau et al., 2014) in a encoderdecoder framework.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 31,
      "context" : "Successive studies on attention mechanism include image caption (Xu et al., 2015), semantic entailment (Rocktäschel et al.",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : ", 2015), semantic entailment (Rocktäschel et al., 2015), aspect level sentiment analysis (Tang et al.",
      "startOffset" : 29,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : ", 2015), aspect level sentiment analysis (Tang et al., 2016), etc.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Attention mechanism is first used for machine translation (Bahdanau et al., 2014) in a encoderdecoder framework. The key idea of attention is to allow the network revisit all parts of a source sentence for an output decision, instead of trying to encode all information of a source sentence into a fixed-length vector. By this mechanism, we can get a deeper insight of how the network works. In the work of Bahdanau et al. (2014), they found the alignment between words by visualizing the attention weights.",
      "startOffset" : 59,
      "endOffset" : 430
    }, {
      "referenceID" : 6,
      "context" : "The LSTM network we use is similar to Graves (2012). We don’t consider peephole connections.",
      "startOffset" : 38,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Inspired by the work of Lai et al. (2015), for word wi, we concatenate its left context c (l) i , right context c (r) i and embedding ei to construct its final local context represetation ci, which is:",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "• Amazon1 by Blitzer et al. (2007). Amazon dataset contains product reviews from Amazon.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "• Amazon1 by Blitzer et al. (2007). Amazon dataset contains product reviews from Amazon.com from many product types. This dataset is origially used for domain adaption, here, we only use it for open-domain classification, regardless of the domain difference. • IMDB2 by Maas et al. (2011), a large movie review dataset collected from imdb.",
      "startOffset" : 13,
      "endOffset" : 289
    }, {
      "referenceID" : 2,
      "context" : "• Amazon1 by Blitzer et al. (2007). Amazon dataset contains product reviews from Amazon.com from many product types. This dataset is origially used for domain adaption, here, we only use it for open-domain classification, regardless of the domain difference. • IMDB2 by Maas et al. (2011), a large movie review dataset collected from imdb.com website. • Yelp 20133 User reviews and recommendations dataset, which are built by Tang et al. (2015).",
      "startOffset" : 13,
      "endOffset" : 445
    }, {
      "referenceID" : 6,
      "context" : "(4) LSTM: Long Short Term Memory network (Graves, 2012) (5) Bi-LSTM: Bi-directional LSTM network (Graves, 2012) (6) RCNN: Recurrent Convolutional Neural Network proposed by Lai et al.",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "(4) LSTM: Long Short Term Memory network (Graves, 2012) (5) Bi-LSTM: Bi-directional LSTM network (Graves, 2012) (6) RCNN: Recurrent Convolutional Neural Network proposed by Lai et al.",
      "startOffset" : 97,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "Neural model methods: (3) CNN: Convolutional Neural Network for sentence classification model proposed by Kim (2014). We use 100 filters to evaluation this model.",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "(4) LSTM: Long Short Term Memory network (Graves, 2012) (5) Bi-LSTM: Bi-directional LSTM network (Graves, 2012) (6) RCNN: Recurrent Convolutional Neural Network proposed by Lai et al. (2015), which uses a bi-directional recurrent network to replace the convolution operation in a typical convolutional network.",
      "startOffset" : 42,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "(4) LSTM: Long Short Term Memory network (Graves, 2012) (5) Bi-LSTM: Bi-directional LSTM network (Graves, 2012) (6) RCNN: Recurrent Convolutional Neural Network proposed by Lai et al. (2015), which uses a bi-directional recurrent network to replace the convolution operation in a typical convolutional network. (7) NAM: Neural Attention Model proposed by Shen & Lee (2016). This model uses LSTM output as attention directly for word embeddings.",
      "startOffset" : 42,
      "endOffset" : 373
    }, {
      "referenceID" : 9,
      "context" : "98 CNN (Kim, 2014) 83.",
      "startOffset" : 7,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "67 LSTM (Graves, 2012) 79.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "74 Bi-LSTM (Graves, 2012) 80.",
      "startOffset" : 11,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "35 RCNN (Lai et al., 2015) 81.",
      "startOffset" : 8,
      "endOffset" : 26
    } ],
    "year" : 2016,
    "abstractText" : "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.",
    "creator" : "LaTeX with hyperref package"
  }
}