{
  "name" : "755.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "DEMYSTIFYING RESNET", "Sihan Li", "Jiantao Jiao", "Yanjun Han", "Tsachy Weissman" ],
    "emails" : [ "lisihan13@mails.tsinghua.edu.cn", "jiantao@stanford.edu", "yjhan@stanford.edu", "tsachy@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a clean shortcut path, residual networks will perform even better.\nMany attempts have been made to improve ResNet to a further extent. “ResNet in ResNet” (Targ et al., 2016) adds more convolution layers and data paths to each layer, making it capable of representing several types of residual units. “ResNets of ResNets” (Zhang et al., 2016) construct multilevel shortcut connections, which means there exist shortcuts that skip multiple residual units. Wide Residual Networks (Zagoruyko & Komodakis, 2016) makes the residual network shorter but wider, and achieves state of the art results on several datasets while using a shallower network. Moreover, some existing models are also reported to be improved by shortcut connections, including Inceptionv4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train.\nWhy are residual networks so easy to train? He et al. (2015a) suggests that layers in residual networks are learning residual mappings, making them easier to represent identity mappings, which prevents the networks from degradation when the depths of the networks increase. However, Veit et al. (2016) claims that ResNets are actually ensembles of shallow networks, which means they do not solve the problem of training deep networks completely.\nWe propose a theoretical explanation for the great performance of ResNet. We concur with He et al. (2015a) that the key contribution of ResNet should be some special structure of the loss function that makes training very deep models no more difficult than shallow ones. Analysis, however, seems non-trivial. Quoting He et al. (2015a):\n“But if F has only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which we have not observed advantages. ”\n“Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets. So the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs. ”\nTheir empirical observations are inspiring. First, the 1-shortcuts mentioned in the first paragraph do not work. Second, noting that the non-bottleneck ResNets have 2-shortcuts, but the bottleneck ResNets use 3-shortcuts, one sees that shortcuts with depth three also do not work. Hence, a reasonable theoretical explanation must be able to distinguish the 2-shortcut from shortcuts of other depths, and clearly demonstrate why the 2-shortcuts are special and are able to ease the optimization process so significantly for deep models, while shortcuts of other depths may not do the job.\nFIXAiming at explaining the performance of 2-shortcuts, we need to eliminate other variables that maycontribute to the success of ResNet. Indeed, one may argue that the deep structure of ResNet may give it better representation power (lower approximation error), which contributes to lower training errors. To eliminate this effect, we focus on deep linear networks, where deeper models do not have better approximation properties. The special role of 2-shortcuts naturally arises in the study."
    }, {
      "heading" : "2 MAIN RESULTS",
      "text" : "Our work reveals that non-degenerate depth-invariant initial condition numbers, a unique property of residual networks with 2-shortcuts, contributed to the success of ResNet. In fact, in a linear network that will be defined rigorously later, the condition number of Hessian of the Frobenius loss function at the zero initial point is\ncond(H) = √ cond((ΣXX − ΣY X)T (ΣXX − ΣY X)), (1)\nwhich is independent of the number of layers. Here ΣXX and ΣY X denote the input-input and the output-input correlation matrices, defined in Section 3.3. The condition number of a possibly non-PSD matrix is defined as: Definition 1. The condition number of a matrix A is defined as\ncond(A) = σmax(A)\nσmin(A) , (2)\nwhere σmax(A) and σmin(A) are the maximum and minimum of singular values of A. In particular, if A is normal, i.e. ATA = AAT , the definition can be simplified to 1\ncond(A) = |λ(A)|max |λ(A)|min , (3)\nwhere |λ(A)|max and |λ(A)|min are the maximum and minimum of the absolute values of eigenvalues of A.\nMoreover, the zero initial point for ResNet with 2-shortcuts is in fact a so-called strict saddle point (Ge et al., 2015), which are proved to be easy to escape from.\nWhy shortcuts of other depths do not work? We show that the Hessian at the zero initial point for the 1-shortcut ResNet has condition number growing unboundedly for deep nets. As is well known in convex optimization theory, large condition numbers can have enormous adversarial impact on the convergence of first order methods (Nemirovski, 2005). Hence, it is quite clear that starting training at a point with a huge condition number would make the algorithm very difficult to escape from the initial point, making 1-shortcut ResNet no better than conventional approaches.\n1The equivalence of Equation (2) and Equation (3) can be proved easily using the eigenvalue decomposition ofA. Note that as Hessians are symmetric (if all the second derivatives are continuous), we will use Equation (3) for their condition numbers. As the |λ|min of Hessian is usually very unstable, we calculated |λ|max|λ|(0.1) to represent condition numbers instead, where |λ|(0.1) is the 10th percentile of the absolute values of eigenvalues. Note that the Hessian at zero initial point for 2-shortcut ResNet also has a nice structure of spectrum: see Theorem 1 for details.\nFor shortcuts with depth deeper than two, the Hessian at the zero initial point is a zero matrix, making it a higher-order stationary point. Intuitively, the higher order the stationary point is, the harder it is to escape from it. Indeed, it is supported both in theory (Anandkumar & Ge, 2016) and by our experiments.\nOne may still ask: why are we interested in the Hessian at the zero initial point? It is because in order for the outputs of deep neural networks not explode, the singular values of the mapping of each layer are not supposed to be deviating too much from one. Indeed, it is because it is extremely challenging to keep ∏num of layers i=1 λi from exploding or vanishing without keeping all of the λi having unit norm. However, by design ResNet with shortcuts already have an identity mapping every few layers, which forces the mappings inside the shortcuts to have small operator norms. Hence, analyzing the network at zero initial point gives a decent characterization of the searching environment of the optimization algorithm.\nFIXOur results also shows that the form of Hessian is more important than the existance of nonlinearitieswhen training the networks. The behaviors of the networks we studied are consistent across both linear and nonlinear structures, where networks with clearer Hessians are much easier to achieve lower training errors.\nOn the other hand, our experiments reveal that orthogonal initialization (Saxe et al., 2013) is suboptimal. Although better than Xavier initialization (Glorot & Bengio, 2010), the initial condition numbers of the networks still explode as the networks become deeper, which means the networks are still initialized on “bad” submanifolds that are hard to optimize using gradient descent."
    }, {
      "heading" : "3 MODEL",
      "text" : ""
    }, {
      "heading" : "3.1 DEEP LINEAR NETWORKS",
      "text" : "Deep linear networks are feed-forward neural networks that only contain linear units, which means their input-output mappings are simply linear transformations. Apparently, increasing their depths will not affect the representational power of the networks. However, linear networks with depth deeper than one show nonlinear dynamics of training (Saxe et al., 2013). As a result, analyzing the training of deep linear networks gives us a better understanding of the training of non-linear networks.\nMuch theoretical work has been done on deep linear networks. Kawaguchi (2016) extended the work of Choromanska et al. (2015a;b) and proved that with few assumptions, every local minimum point in deep linear networks is a global minimum point. This means that the difficulties in the training of deep linear networks mostly come from saddle points on the loss surfaces, which are also the main causes of slow learning in nonlinear networks (Pascanu et al., 2014).\nSaxe et al. (2013) studied the dynamics of training using gradient descent. They found that for a special class of initial conditions, which could be obtained from greedy layerwise pre-training, the training time for a deep linear network with an infinity depth can still be finite. Furthermore, they found that by setting the initial weights to random orthogonal matrices (produced by performing QR or SVD decompositions on random Gaussian matrices), the network will still have a depth independent learning time. They argue that it is caused by the eigenvalue and singular value spectra of the end-to-end linear transformation. When using orthogonal initialization, the overall transformation is an orthogonal matrix, which has all the singular values equal to 1. In the meantime, when using scaled Gaussian initialization, most of the singular values are close to zero, making the network unsuitable for backpropagating errors. However, this explanation is not sufficient to prove that the training difficulty of orthogonal initialized networks is depth-invariant. It only gives us an intuition on why orthogonal initialization performs better than scaled Gaussian initialization.\nThus, we use deep linear networks to study the effect of shortcut connections. After adding the shortcuts, the overall model is still linear and the global minimum does not change."
    }, {
      "heading" : "3.2 NETWORK STRUCTURE",
      "text" : "We first generalize a linear network by adding shortcuts to it to make it a linear residual network. We organize the network into R residual units. The r-th residual unit consists of Lr layers whose\nweights are W r,1, . . . ,W r,Lr−1, denoted as the transformation path, as well as a shortcut Sr connecting from the first layer to the last one, denoted as the shortcut path. The input-output mapping can be written as\ny = R∏ r=1 ( Lr−1∏ l=1 W r,l + Sr)x = Wx, (4)\nwhere x ∈ Rdx , y ∈ Rdy ,W ∈ Rdy×dx . Here if b ≥ a, ∏b i=aW\ni denotes W bW (b−1) · · ·W (a+1)W a, otherwise it denotes an identity mapping. The matrix W represents the combination of all the linear transformations in the network. Note that by setting all the shortcuts to zeros, the network will go back to a ( ∑ r(Lr − 1) + 1)-layer plain linear network.\nInstead of analyzing the general form, we concentrate on a special kind of linear residual networks, where all the residual units are the same.\nDefinition 2. A linear residual network is called an n-shortcut linear network if\n1. its layers have the same dimension (so that dx = dy);\n2. its shortcuts are identity matrices;\n3. its shortcuts have the same depth n.\nThe input-output mapping for such a network becomes\ny = R∏ r=1 ( n∏ l=1 W r,l + Idx)x = Wx, (5)\nwhere W r,l ∈ Rdx×dx .\nThen we add some activation functions to the networks. We concentrate on the case where activation functions are on the transformation paths, which is also the case in the latest ResNet (He et al., 2016).\nDefinition 3. An n-shortcut linear network becomes an n-shortcut network if element-wise activation functions σpre(x), σmid(x), σpost(x) are added at the transformation paths, where on a transformation path, σpre(x) is added before the first weight matrix, σmid(x) is added between two weight matrixes and σpost(x) is added after the last weight matrix.\nNote that n-shortcut linear networks are special cases of n-shortcut networks, where all the activation functions are identity mappings."
    }, {
      "heading" : "3.3 OPTIMIZATION",
      "text" : "We denote the collection of all the variable weight parameters in an n-shortcut linear network as w. Consider m training samples {xµ, yµ}, µ = 1, . . . ,m. Using Frobenius loss, for an n-shortcut linear network, we define the loss function as follows:\nL(w) = 1\n2m m∑ µ=1 ‖yµ −Wxµ‖22 = 1 2m ‖Y −WX‖2F , (6)\nwhere xµ, yµ are the µ-th columns of X,Y , and ‖·‖F denotes the Frobenius norm. Using gradient descent with learning rate α, we have the weights updating rules as\n∆W r,l = α(W rafterW r,l after) T (ΣY X −WΣXX)(W r,lbeforeW r before) T , (7)\nwhere ΣXX and ΣY X denote the input-input and the output-input correlation matrices, defined as\nΣXX = 1\nm m∑ µ=1 xµ(xµ)T (8)\nΣY X = 1\nm m∑ µ=1 yµ(xµ)T . (9)\nHere W rbefore,W r after denote the linear mappings before and after the r-th residual unit, W r,lbefore,W r,l after denote the linear mappings before and after W\nr,l within the transformation path of the r-th residual unit. In other words, the overall transformation can be represented as\ny = W rafter(W r,l afterW r,lW r,lbefore + Idx)W r beforex. (10)"
    }, {
      "heading" : "4 THEORETICAL STUDY",
      "text" : ""
    }, {
      "heading" : "4.1 INITIAL POINT PROPERTIES",
      "text" : "Before we analyze the initial point properties of n-shortcut networks, we have to choose the way to initialize them.\nNEW ResNet uses MSRA initialization (He et al., 2015b). It is a kind of scaled Gaussian\ninitialization that tries to keep the variances of signals along a transformation path, which is also the idea behind Xavier initialization (Glorot & Bengio, 2010). However, because of the shortcut paths, the output variance of the entire network will actually explode as the network becomes deeper. Batch normalization units partly solved this problem in ResNet, but still they cannot prevent the large output variance in a deep network.\nNEWA simple idea is to zero initialize all the weights, so that the output variances of residual unitsstay the same along the network. It is worth noting that as found in He et al. (2015a), the deeper ResNet has smaller magnitudes of layer responses. This phenomenon has been confirmed in our experiments. As illustrated in Figure 2 and Figure 3, the deeper a residual network is, the small its average Frobenius norm of weight matrixes is, both during the traning process and when the training ends. Also, Hardt & Ma (2016) proves that if all the weight matrixes have small norms, a linear residual network will have no critical points other than the global optimum.\nNEWAll these evidences indicate that zero is spacial in a residual network: as the network becomesdeeper, the training tends to end up around it. Thus, we are looking into the Hessian at zero. As the zero is a saddle point, in our experiments we use zero initialization with small random perturbations to escape from it. We first Xavier initialize the weight matrixes, and then multiply a small constant (0,01) to them.\nWe begin with the definition of k-th order stationary point.\nDefinition 4. Suppose function f(x) admits k-th order Taylor expansion at point x0. We say that the point x0 is a k-th order stationary point of f(x) if the corresponding k-th order Taylor expansion of f(x) at x = x0 is a constant: f(x) = f(x0) + o(‖x− x0‖k2).\nNow we state our main theorem, whose proof can be found in Appendix A.\nTheorem 1. Assume that σmid(0) = σpost(0) = 0 and all of σ (k) pre(0), σ (k) mid(0), σ (k) post(0), 1 ≤ k ≤ max(n− 1, 2) exist. For the loss function of an n-shortcut network, at point zero,\n1. if n ≥ 2, it is an (n− 1)th-order stationary point. In particular, if n ≥ 3, the Hessian is a zero matrix;\n2. if n = 2, the Hessian can be written as\nH =  0 AT A 0 0 AT\nA 0 . . .\n , (11)\nwhose condition number is cond(H) = √ cond((ΣXσpre(X) − ΣY σpre(X))T (ΣXσpre(X) − ΣY σpre(X))), (12)\nwhere A only depends on the training set and the activation functions. Except for degenerate cases, it is a strict saddle point (Ge et al., 2015).\n3. if n = 1, the Hessian can be written as\nH =  B AT AT · · · AT A B AT · · · AT A A B ...\n... ... . . . AT A A · · · A B  (13) where A,B only depend on the training set and the activation functions.\nTheorem 1 shows that the condition numbers of 2-shortcut networks are depth-invariant with a nice structure of eigenvalues. Indeed, the eigenvalues of the Hessian H at the zero initial point are multiple copies of ± √ eigs(ATA), and the number of copies is equal to the number of shortcut connections.\nThe Hessian at zero initial point for the 1-shortcut linear network follows block Toeplitz structure, which has been well studied in the literature. In particular, its condition number tends to explode as the number of layers increase (Gray, 2006).\nThe assumptions hold for most activation functions including tanh, symmetric sigmoid and ReLU (Nair & Hinton, 2010). Note that although ReLU does not have derivatives at zero, one may do a local polynomial approximation to yield σ(k), 1 ≤ k ≤ max(n− 1, 2). To get intuitive explanations of the theorems, imagine changing parameters in an n-shortcut network. One has to change at least n parameters to make any difference in the loss. So zero is an (n− 1)thorder stationary point. Notice that the higher the order of a stationary point, the more difficult for a first order method to escape from it.\nOn the other hand, if n = 2, one will have to change two parameters in the same residual unit but different weight matrices to affect the loss, leading to a clear block diagonal Hessian."
    }, {
      "heading" : "4.2 LEARNING DYNAMICS",
      "text" : "To understand Equation (7) better, we can take n-shortcut linear networks to two extremes. First, when n = 1, let V r,1 = W r,1 + Idx , r = 1, . . . , R− 1. As Idx is a constant, we have\n∆V r,1 = α( R−1∏ r′=r+1 V r ′,1)T (ΣY X − ( R−1∏ r′=1 V r ′,1)ΣXX)( r−1∏ r′=1 V r ′,1)T , (14)\nwhich can be seen as a linear network with identity initialization, a special case of orthogonal initialization, if the original 1-shortcut network is zero initialized.\nOn the other side, if the number of shortcut connections R = 1, the shortcut will only change the distribution of the output training set from Y to Y − X . These two extremes are illustrated in Figure 4"
    }, {
      "heading" : "4.3 LEARNING RESULTS",
      "text" : "The optimal weights of an n-shortcut linear network can be easily computed via least squares, which leads to\nW = Y XT (XXT )−1 = ΣY X(ΣXX)−1, (15)\nand the minimum of the loss function is\nLmin = 1\n2m ‖Y − ΣY X(ΣXX)−1X‖2F , (16)\nwhere ‖·‖F denotes the Frobenius norm and (ΣXX)−1 denotes any kind of generalized inverse of ΣXX . So given a training set, we can pre-compute its Lmin and use it to evaluate any n-shortcut linear network."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We compare networks with Xavier initialization (Glorot & Bengio, 2010), networks with orthogonal initialization (Saxe et al., 2013) and 2-shortcut networks with zero initialization. The training dynamics of 1-shortcut networks are similar to that of linear networks with orthogonal initialization in our experiments. Setup details can be found in Appendix B."
    }, {
      "heading" : "5.1 INITIAL POINT",
      "text" : "We first compute the initial condition numbers for different kinds of linear networks with different depths.\nAs can be seen in Figure 5, 2-shortcut linear networks have constant condition numbers as expected. On the other hand, when using Xavier or orthogonal initialization in linear networks, the initial condition numbers will go to infinity as the depths become infinity, making the networks hard to train. This also explains why orthogonal initialization is helpful for a linear network, as its initial condition number grows slower than the Xavier initialization."
    }, {
      "heading" : "5.2 LEARNING DYNAMICS",
      "text" : "Having a good beginning does not guarantee an easy trip on the loss surface. In order to depict the loss surfaces encountered from different initial points, we plot the maxima and 10th percentiles (instead of minima, as they are very unstable) of the absolute values of Hessians eigenvalues at different losses.\nAs shown in Figure 6 and Figure 7, the condition numbers of 2-shortcut networks at different losses are always smaller, especially when the loss is large. Also, notice that the condition numbers roughly evolved to the same value for both orthogonal and 2-shortcut linear networks. This may be explained by the fact that the minimizers, as well as any point near them, have similar condition numbers.\nAnother observation is the changes of negative eigenvalues ratios. Index (ratio of negative eigenvalues) is an important characteristic of a critical point. Usually for the critical points of a neural network, the larger the loss the larger the index (Dauphin et al., 2014). In our experiments, the index of a 2-shortcut network is always smaller, and drops dramatically at the beginning, as shown in Figure 8, left. This might make the networks tend to stop at low critical points.\nThis is because the initial point is near a saddle point, thus it tends to go towards negative curvature directions, eliminating some negative eigenvalues at the beginning. This phenomenon matches the observation that the gradient reaches its maximum when the index drops dramatically, as shown in Figure 8, right."
    }, {
      "heading" : "5.3 LEARNING RESULTS",
      "text" : "We run different networks for 1000 epochs using different learning rates at log scale, and compare the average final losses of the optimal learning rates.\nFigure 9 shows the results for linear networks. Just like their depth-invariant initial condition numbers, the final losses of 2-shortcut linear networks stay close to optimal as the networks become deeper. Higher learning rates can also be applied, resulting in fast learning in deep networks.\nThen we add ReLUs to the mid positions of the networks. To make a fair comparison, the numbers of ReLU units in different networks are the same when the depths are the same, so 1-shortcut and 3-shortcut networks are omitted. The result is shown in Figure 10.\nNote that because of the nonlinearities, the optimal losses vary for different networks with different depths. It is usually thought that deeper networks can represent more complex models, leading to smaller optimal losses. However, our experiments show that linear networks with Xavier or orthogonal initialization have difficulties finding these optimal points, while 2-shortcut networks find these optimal points easily as they did without nonlinear units."
    }, {
      "heading" : "6 FUTURE DIRECTIONS",
      "text" : "Further studies should concentrate on the behavior of shortcut connections on convolution networks, as well as the influences of batch normalization units (Ioffe & Szegedy, 2015) in ResNet. Meanwhile, it would be very interesting to extend the insights obtained in this paper to recurrent neural networks such as LSTM (Sainath et al., 2013)."
    }, {
      "heading" : "A PROOFS OF THEOREMS",
      "text" : "Definition 5. The elements in Hessian of an n-shortcut network is defined as\nHind(w1),ind(w2) = ∂2L\n∂w1∂w2 , (17)\nwhere L is the loss function, and the indices ind(·) is ordered lexicographically following the four indices (r, l, j, i) of the weight variable wr,li,j . In other words, the priority decreases along the index of shortcuts, index of weight matrix inside shortcuts, index of column, and index of row.\nNote that the collection of all the weight variables in the n-shortcut network is denoted as w. We study the behavior of the loss function in the vicinity of w = 0.\nLemma 1. Assume that w1 = wr1,l1i1,j1 , · · · , wN = w rN ,lN iN ,jN are N parameters of an n-shortcut network. If ∂ 2L\n∂w1···∂wN ∣∣∣ w=0\nis nonzero, there exists r and k1, · · · , kn such that rkm = r and lkm = m for m = 1, · · · , n.\nProof. Assume there does not exist such r and k1, · · · , kn, then for all the shortcut units r = 1, · · · , R, there exists a weight matrix l such that none of w1, · · · , wN is in W r,l, so all the transformation paths are zero, which means W = Idx . Then ∂2L ∂w1···∂wN ∣∣∣ w=0\n= 0, leading to a contradiction.\nLemma 2. Assume that w1 = wr1,l1i1,j1 , w2 = w r2,l2 i2,j2 , r1 ≤ r2. Let L0(w1, w2) denotes the loss function with all the parameters except w1 and w2 set to 0, w′1 = w 1,l1 i1,j1 , w′2 = w 1+1(r1 6=r2),l2 i2,j2 . Then ∂ 2L0(w1,w2) ∂w1∂w2 |(w1,w2)=0 = ∂2L0(w ′ 1,w ′ 2) ∂w′1∂w ′ 2 |(w′1,w′2)=0.\nProof. As all the residual units expect unit r1 and r2 are identity transformations, reordering residual units while preserving the order of units r1 and r2 will not affect the overall transformation, i.e. L0(w1, w2)|w1=a,w2=b = L′0(w′1, w′2)|w′1=a,w′2=b. So ∂2L0(w1,w2) ∂w1∂w2\n|(w1,w2)=0 = ∂2L0(w ′ 1,w ′ 2)\n∂w′1∂w ′ 2 |(w′1,w′2)=0.\nProof of Theorem 1. Now we can prove Theorem 1 with the help of the previously established lemmas.\n1. Using Lemma 1, for an n-shortcut network, at zero, all the k-th order partial derivatives of the loss function are zero, where k ranges from 1 to n − 1. Hence, the initial point zero is a (n− 1)th-order stationary point of the loss function.\n2. Consider the Hessian in n = 2 case. Using Lemma 1 and Lemma 2, the form of Hessian can be directly written as Equation (11), as illustrated in Figure 11.\nSo we have\neigs(H) = eigs(\n[ 0 AT\nA 0\n] ) = ± √ eigs(ATA). (18)\nThus cond(H) = √\ncond(ATA), which is depth-invariant. Note that the dimension of A is d2x × d2x. To get the expression of A, consider two parameters that are in the same residual unit but different weight matrices, i.e. w1 = w r,2 i1,j1 , w2 = w r,1 i2,j2 .\nResidual Unit 1\nRe sid\nua l U\nni t 1\nResidual Unit 2\nRe sid\nua l U\nni t 2\n…\n…\n… …\nSo the eigenvalues of H becomes eigs(H) = ±σ′mid(0)σ′post(0) √\neigs((ΣXσpre(X) − ΣY σpre(X))T (ΣXσpre(X) − ΣY σpre(X))), (22)\nwhich leads to Equation (12).\n3. Now consider the Hessian in the n = 1 case. Using Lemma 2, the form of Hessian can be directly written as Equation (13).\nTo get the expressions of A and B in σpre(x) = σpost(x) = x case, consider two parameters that are in the same residual units, i.e. w1 = w r,1 i1,j1 , w2 = w r,1 i2,j2 .\nWe have\nB(j1−1)dx+i1,(j2−1)dx+i2 = ∂2L\n∂w1∂w2\n∣∣∣ w=0\n(23)\n=\n{ 1 m ∑m µ=1 x µ j1 xµj2 i1 = i2\n0 i1 6= i2 (24)\nRearrange the order of variables using P , we have\nB = P Σ XX\n. . . ΣXX PT . (25) Then consider two parameters that are in different residual units, i.e. w1 = w r1,1 i1,j1 , w2 = wr2,1i2,j2 , r1 > r2.\nWe have\nA(j1−1)dx+i1,(j2−1)dx+i2 = ∂2L\n∂w1∂w2\n∣∣∣ w=0\n(26)\n=  1 m ∑m µ=1(x µ i1 − yµi1)x µ j2 + xµj1x µ j2 j1 = i2, i1 = i2 1 m ∑m µ=1(x µ i1 − yµi1)x µ j2 j1 = i2, i1 6= i2 1 m ∑m µ=1 x µ j1 xµj2 j1 6= i2, i1 = i2\n0 j1 6= i2, i1 6= i2\n(27)\nIn the same way, we can rewrite A as\nA = Σ XX − ΣY X\n. . . ΣXX − ΣY X\nPT +B. (28)"
    }, {
      "heading" : "B EXPERIMENT SETUP",
      "text" : "We took the experiments on whitened versions of MNIST. 10 greatest principal components are kept for the dataset inputs. The dataset outputs are represented using one-hot encoding. The network was trained using gradient descent. For every epoch, the Hessians of the networks were calculated using the method proposed in (Bishop, 1992). As the |λ|min of Hessian is usually very unstable, we calculated |λ|max|λ|(0.1) to represent condition number instead, where |λ|(0.1) is the 10\nth percentile of the absolute values of eigenvalues.\nAs pre, mid or post positions are not defined in linear networks without shortcuts, when comparing Xavier or orthogonal initialized linear networks to 2-shortcut networks, we added ReLUs at the same positions in linear networks as in 2-shortcuts networks."
    } ],
    "references" : [ {
      "title" : "Efficient approaches for escaping higher order saddle points in non-convex optimization",
      "author" : [ "Anima Anandkumar", "Rong Ge" ],
      "venue" : "arXiv preprint arXiv:1602.05908,",
      "citeRegEx" : "Anandkumar and Ge.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anandkumar and Ge.",
      "year" : 2016
    }, {
      "title" : "Exact calculation of the hessian matrix for the multilayer perceptron",
      "author" : [ "Chris Bishop" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Bishop.,? \\Q1992\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1992
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Open problem: The landscape of the loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Yann LeCun", "Gérard Ben Arous" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, COLT",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Escaping from saddle pointsonline stochastic gradient for tensor decomposition",
      "author" : [ "Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, pp",
      "citeRegEx" : "Ge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In Aistats,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Toeplitz and circulant matrices: A review",
      "author" : [ "Robert M Gray" ],
      "venue" : "now publishers inc,",
      "citeRegEx" : "Gray.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gray.",
      "year" : 2006
    }, {
      "title" : "Identity matters in deep learning",
      "author" : [ "Moritz Hardt", "Tengyu Ma" ],
      "venue" : "arXiv preprint arXiv:1611.04231,",
      "citeRegEx" : "Hardt and Ma.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hardt and Ma.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "Kenji Kawaguchi" ],
      "venue" : "arXiv preprint arXiv:1605.07110,",
      "citeRegEx" : "Kawaguchi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kawaguchi.",
      "year" : 2016
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Efficient methods in convex programming",
      "author" : [ "Arkadi Nemirovski" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 2005
    }, {
      "title" : "On the saddle point problem for non-convex optimization",
      "author" : [ "Razvan Pascanu", "Yann N Dauphin", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1405.4604,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional, Long short-term memory, fully connected deep neural networks",
      "author" : [ "T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak" ],
      "venue" : "Journal of Chemical Information and Modeling,",
      "citeRegEx" : "Sainath et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sainath et al\\.",
      "year" : 2013
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M Saxe", "James L McClelland", "Surya Ganguli" ],
      "venue" : "arXiv preprint arXiv:1312.6120,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh K Srivastava", "Klaus Greff", "Jürgen Schmidhuber" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. feb 2016",
      "author" : [ "Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke" ],
      "venue" : null,
      "citeRegEx" : "Szegedy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Resnet in resnet: Generalizing residual architectures",
      "author" : [ "Sasha Targ", "Diogo Almeida", "Kevin Lyman" ],
      "venue" : "arXiv preprint arXiv:1603.08029,",
      "citeRegEx" : "Targ et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Targ et al\\.",
      "year" : 2016
    }, {
      "title" : "Residual networks are exponential ensembles of relatively shallow networks",
      "author" : [ "Andreas Veit", "Michael Wilber", "Serge Belongie" ],
      "venue" : "arXiv preprint arXiv:1605.06431,",
      "citeRegEx" : "Veit et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Veit et al\\.",
      "year" : 2016
    }, {
      "title" : "Wide residual networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "arXiv preprint arXiv:1605.07146,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2016
    }, {
      "title" : "Residual networks of residual networks: Multilevel residual networks",
      "author" : [ "Ke Zhang", "Miao Sun", "Tony X Han", "Xingfang Yuan", "Liru Guo", "Tao Liu" ],
      "venue" : "arXiv preprint arXiv:1608.02908,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : ", 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "“ResNet in ResNet” (Targ et al., 2016) adds more convolution layers and data paths to each layer, making it capable of representing several types of residual units.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "“ResNets of ResNets” (Zhang et al., 2016) construct multilevel shortcut connections, which means there exist shortcuts that skip multiple residual units.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "Moreover, some existing models are also reported to be improved by shortcut connections, including Inceptionv4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train.",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014).",
      "startOffset" : 63,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before.",
      "startOffset" : 63,
      "endOffset" : 381
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a clean shortcut path, residual networks will perform even better.",
      "startOffset" : 63,
      "endOffset" : 671
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a clean shortcut path, residual networks will perform even better. Many attempts have been made to improve ResNet to a further extent. “ResNet in ResNet” (Targ et al., 2016) adds more convolution layers and data paths to each layer, making it capable of representing several types of residual units. “ResNets of ResNets” (Zhang et al., 2016) construct multilevel shortcut connections, which means there exist shortcuts that skip multiple residual units. Wide Residual Networks (Zagoruyko & Komodakis, 2016) makes the residual network shorter but wider, and achieves state of the art results on several datasets while using a shallower network. Moreover, some existing models are also reported to be improved by shortcut connections, including Inceptionv4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train. Why are residual networks so easy to train? He et al. (2015a) suggests that layers in residual networks are learning residual mappings, making them easier to represent identity mappings, which prevents the networks from degradation when the depths of the networks increase.",
      "startOffset" : 63,
      "endOffset" : 1603
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a clean shortcut path, residual networks will perform even better. Many attempts have been made to improve ResNet to a further extent. “ResNet in ResNet” (Targ et al., 2016) adds more convolution layers and data paths to each layer, making it capable of representing several types of residual units. “ResNets of ResNets” (Zhang et al., 2016) construct multilevel shortcut connections, which means there exist shortcuts that skip multiple residual units. Wide Residual Networks (Zagoruyko & Komodakis, 2016) makes the residual network shorter but wider, and achieves state of the art results on several datasets while using a shallower network. Moreover, some existing models are also reported to be improved by shortcut connections, including Inceptionv4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train. Why are residual networks so easy to train? He et al. (2015a) suggests that layers in residual networks are learning residual mappings, making them easier to represent identity mappings, which prevents the networks from degradation when the depths of the networks increase. However, Veit et al. (2016) claims that ResNets are actually ensembles of shallow networks, which means they do not solve the problem of training deep networks completely.",
      "startOffset" : 63,
      "endOffset" : 1843
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a clean shortcut path, residual networks will perform even better. Many attempts have been made to improve ResNet to a further extent. “ResNet in ResNet” (Targ et al., 2016) adds more convolution layers and data paths to each layer, making it capable of representing several types of residual units. “ResNets of ResNets” (Zhang et al., 2016) construct multilevel shortcut connections, which means there exist shortcuts that skip multiple residual units. Wide Residual Networks (Zagoruyko & Komodakis, 2016) makes the residual network shorter but wider, and achieves state of the art results on several datasets while using a shallower network. Moreover, some existing models are also reported to be improved by shortcut connections, including Inceptionv4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train. Why are residual networks so easy to train? He et al. (2015a) suggests that layers in residual networks are learning residual mappings, making them easier to represent identity mappings, which prevents the networks from degradation when the depths of the networks increase. However, Veit et al. (2016) claims that ResNets are actually ensembles of shallow networks, which means they do not solve the problem of training deep networks completely. We propose a theoretical explanation for the great performance of ResNet. We concur with He et al. (2015a) that the key contribution of ResNet should be some special structure of the loss function that makes training very deep models no more difficult than shallow ones.",
      "startOffset" : 63,
      "endOffset" : 2094
    }, {
      "referenceID" : 9,
      "context" : "1 INTRODUCTION Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016). It followed a principled approach to add shortcut connections every two layers to a VGG-style network (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM (Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a clean shortcut path, residual networks will perform even better. Many attempts have been made to improve ResNet to a further extent. “ResNet in ResNet” (Targ et al., 2016) adds more convolution layers and data paths to each layer, making it capable of representing several types of residual units. “ResNets of ResNets” (Zhang et al., 2016) construct multilevel shortcut connections, which means there exist shortcuts that skip multiple residual units. Wide Residual Networks (Zagoruyko & Komodakis, 2016) makes the residual network shorter but wider, and achieves state of the art results on several datasets while using a shallower network. Moreover, some existing models are also reported to be improved by shortcut connections, including Inceptionv4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train. Why are residual networks so easy to train? He et al. (2015a) suggests that layers in residual networks are learning residual mappings, making them easier to represent identity mappings, which prevents the networks from degradation when the depths of the networks increase. However, Veit et al. (2016) claims that ResNets are actually ensembles of shallow networks, which means they do not solve the problem of training deep networks completely. We propose a theoretical explanation for the great performance of ResNet. We concur with He et al. (2015a) that the key contribution of ResNet should be some special structure of the loss function that makes training very deep models no more difficult than shallow ones. Analysis, however, seems non-trivial. Quoting He et al. (2015a): “But if F has only a single layer, Eqn.",
      "startOffset" : 63,
      "endOffset" : 2322
    }, {
      "referenceID" : 5,
      "context" : "Moreover, the zero initial point for ResNet with 2-shortcuts is in fact a so-called strict saddle point (Ge et al., 2015), which are proved to be easy to escape from.",
      "startOffset" : 104,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "As is well known in convex optimization theory, large condition numbers can have enormous adversarial impact on the convergence of first order methods (Nemirovski, 2005).",
      "startOffset" : 151,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, our experiments reveal that orthogonal initialization (Saxe et al., 2013) is suboptimal.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : "However, linear networks with depth deeper than one show nonlinear dynamics of training (Saxe et al., 2013).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "This means that the difficulties in the training of deep linear networks mostly come from saddle points on the loss surfaces, which are also the main causes of slow learning in nonlinear networks (Pascanu et al., 2014).",
      "startOffset" : 196,
      "endOffset" : 218
    }, {
      "referenceID" : 11,
      "context" : "Kawaguchi (2016) extended the work of Choromanska et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "Kawaguchi (2016) extended the work of Choromanska et al. (2015a;b) and proved that with few assumptions, every local minimum point in deep linear networks is a global minimum point. This means that the difficulties in the training of deep linear networks mostly come from saddle points on the loss surfaces, which are also the main causes of slow learning in nonlinear networks (Pascanu et al., 2014). Saxe et al. (2013) studied the dynamics of training using gradient descent.",
      "startOffset" : 38,
      "endOffset" : 421
    }, {
      "referenceID" : 11,
      "context" : "We concentrate on the case where activation functions are on the transformation paths, which is also the case in the latest ResNet (He et al., 2016).",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "NEW ResNet uses MSRA initialization (He et al., 2015b). It is a kind of scaled Gaussian initialization that tries to keep the variances of signals along a transformation path, which is also the idea behind Xavier initialization (Glorot & Bengio, 2010). However, because of the shortcut paths, the output variance of the entire network will actually explode as the network becomes deeper. Batch normalization units partly solved this problem in ResNet, but still they cannot prevent the large output variance in a deep network. NEW A simple idea is to zero initialize all the weights, so that the output variances of residual units stay the same along the network. It is worth noting that as found in He et al. (2015a), the deeper ResNet has smaller magnitudes of layer responses.",
      "startOffset" : 37,
      "endOffset" : 718
    }, {
      "referenceID" : 9,
      "context" : "NEW ResNet uses MSRA initialization (He et al., 2015b). It is a kind of scaled Gaussian initialization that tries to keep the variances of signals along a transformation path, which is also the idea behind Xavier initialization (Glorot & Bengio, 2010). However, because of the shortcut paths, the output variance of the entire network will actually explode as the network becomes deeper. Batch normalization units partly solved this problem in ResNet, but still they cannot prevent the large output variance in a deep network. NEW A simple idea is to zero initialize all the weights, so that the output variances of residual units stay the same along the network. It is worth noting that as found in He et al. (2015a), the deeper ResNet has smaller magnitudes of layer responses. This phenomenon has been confirmed in our experiments. As illustrated in Figure 2 and Figure 3, the deeper a residual network is, the small its average Frobenius norm of weight matrixes is, both during the traning process and when the training ends. Also, Hardt & Ma (2016) proves that if all the weight matrixes have small norms, a linear residual network will have no critical points other than the global optimum.",
      "startOffset" : 37,
      "endOffset" : 1054
    }, {
      "referenceID" : 5,
      "context" : "Except for degenerate cases, it is a strict saddle point (Ge et al., 2015).",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "In particular, its condition number tends to explode as the number of layers increase (Gray, 2006).",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "5 EXPERIMENTS We compare networks with Xavier initialization (Glorot & Bengio, 2010), networks with orthogonal initialization (Saxe et al., 2013) and 2-shortcut networks with zero initialization.",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Usually for the critical points of a neural network, the larger the loss the larger the index (Dauphin et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 116
    } ],
    "year" : 2017,
    "abstractText" : "We provide a theoretical explanation for the great performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depthinvariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process.",
    "creator" : "LaTeX with hyperref package"
  }
}