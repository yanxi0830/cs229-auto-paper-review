{
  "name" : "440.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "NEURAL NETWORKS", "Stefan Depeweg", "José Miguel Hernández-Lobato", "Steffen Udluft" ],
    "emails" : [ "stefan.depeweg@siemens.com", "jmh233@cam.ac.uk", "finale@seas.harvard.edu", "steffen.udluft@siemens.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In model-based reinforcement learning, an agent uses its experience to first learn a model of the environment and then uses that model to reason about what action to take next. We consider the case in which the agent observes the current state st, takes some action a, and then observes the next state st+1. The problem of learning the model corresponds then to learning a stochastic transition function p(st+1|st,a) specifying the conditional distribution of st+1 given st and a. Most classic control theory texts, e.g. Bertsekas (2002), will start with the most general model of dynamical systems:\nst+1 = f(st,a, z,W)\nwhere f is some deterministic function parameterized by weightsW that takes as input the current state st, the control signal a, and some stochastic disturbance z.\nHowever, to date, we have not been able to robustly learn dynamical system models to such a level of generality. Popular modes for transition functions include Gaussian processes (Rasmussen et al., 2003; Ko et al., 2007; Deisenroth & Rasmussen, 2011), fixed bases such as Laguerre functions (Wahlberg, 1991), and adaptive basis functions or neural networks (Draeger et al., 1995). All of these methods assume deterministic transition functions, perhaps with some addition of Gaussian observation noise. Thus, they are severely limited in the kinds of stochasticity—or transition noise—they can express. In many real-world scenarios stochasticity may often arise due to some unobserved environmental feature that can affect the dynamics in complex ways (such as unmeasured gusts of wind on a boat).\nIn this work we use Bayesian neural networks (BNNs) in conjunction with a random input noise source z to express stochastic dynamics. We take advantage of a very recent inference advance based on α-divergence minimization (Hernández-Lobato et al., 2016), with α = 0.5, to learn with\nhigh accuracy BNN transition functions that are both scalable and expressive in terms of stochastic patterns. Previous work achieved one but not both of these two characteristics.\nWe focus our evaluation on the off-policy batch reinforcement learning scenario, in which we are given an initial batch of data from an already-running system and are asked to find a better (ideally near-optimal) policy. Such scenarios are common in real-world industry settings such as turbine control, where exploration is restricted to avoid possible damage to the system. We propose an algorithm that uses random roll-outs and stochastic optimization for learning an optimal policy from the predictions of BNNs. This method produces (to our knowledge) the first model-based solution of a 20-year-old benchmark problem: the Wet-Chicken (Tresp, 1994). We also obtain very promising results on a real-world application on controlling gas turbines and on an industrial benchmark."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 MODEL-BASED REINFORCEMENT LEARNING",
      "text" : "We consider reinforcement learning problems in which an agent acts in a stochastic environment by sequentially choosing actions over a sequence of time steps, in order to minimize a cumulative cost. We assume that our environment has some true dynamics Ttrue(st+1|s,a), and we are given a cost function c(st). In the model-based reinforcement learning setting, our goal is to learn an approximation Tapprox(st+1|s,a) for the true dynamics based on collected samples (st,a, st+1). The agent then tries to solve the control problem in which Tapprox is assumed to be the true dynamics."
    }, {
      "heading" : "2.2 BAYESIAN NEURAL NETWORKS WITH STOCHASTIC INPUTS",
      "text" : "Given dataD = {xn,yn}Nn=1, formed by feature vectors xn ∈ RD and targets yn ∈ R K , we assume that yn = f(xn, zn;W) + n, where f(·, ·;W) is the output of a neural network with weightsW . The network receives as input the feature vector xn and the random disturbance zn ∼ N (0, γ). The activation functions for the hidden layers are rectifiers: ϕ(x) = max(x, 0). The activation functions for the output layers are the identity function: ϕ(x) = x. The network output is corrupted by the additive noise variable n ∼ N (0,Σ) with diagonal covariance matrix Σ. The role of the noise disturbance zn is to capture unobserved stochastic features that can affect the network’s output in complex ways. Without zn, randomness is only given by the additive Gaussian observation noise n, which can only describe limited stochastic patterns. The network has L layers, with Vl hidden units in layer l, andW = {Wl}Ll=1 is the collection of Vl × (Vl−1 + 1) weight matrices. The +1 is introduced here to account for the additional per-layer biases.\nOne could argue why n is needed at all when we are already using the more flexible stochastic model based on zn. The reason for this is that, in practice, we make predictions with the above model by averaging over a finite number of samples of zn and W . By using n, we obtain a predictive distribution whose density is well defined and given by a mixture of Gaussians. If we eliminate n, the predictive density is degenerate and given by a mixture of delta functions.\nLet Y be an N ×K matrix with the targets yn and X be an N ×D matrix of feature vectors xn. We denote by z the N -dimensional vector with the values of the random disturbances z1, . . . , zN that were used to generate the data. The likelihood function is\np(Y |W, z,X) = N∏ n=1 p(yn |W, z,xn) = N∏ n=1 K∏ k=1 N (yn,k | f(xn, zn;W),Σ) . (1)\nThe prior for each entry in z is N (0, γ). We also specify a Gaussian prior distribution for each entry in each of the weight matrices inW . That is,\np(z) = N∏ n=1 N (zn|0, γ) , p(W) = L∏ l=1 Vl∏ i=1 Vl−1+1∏ j=1 N (wij,l | 0, λ) , (2)\nwhere wij,l is the entry in the i-th row and j-th column of Wl and γ and λ are a prior variances. The posterior distribution for the weightsW and the random disturbances z is given by Bayes’ rule:\np(W, z | D) = p(Y |W, z,X)p(W)p(z) p(Y |X) . (3)\nGiven a new input vector x?, we can then make predictions for y? using the predictive distribution p(y? |x?,D) = ∫ [∫ N (y? | f(x?, z?;W),Σ)N (z?|0, 1) dz? ] p(W, z | D) dW dz . (4)\nUnfortunately, the exact computation of (4) is intractable and we have to use approximations.\n2.3 α-DIVERGENCE MINIMIZATION\nWe approximate the exact posterior distribution p(W, z | D) with the factorized Gaussian distribution\nq(W, z) =  L∏ l=1 Vl∏ i=1 Vl−1+1∏ j=1 N (wij,l|mwij,l, vwij,l) [ N∏ n=1 N (zn |mzn, vzn) ] . (5)\nThe parameters mwij,l, v w ij,l and m z n, v z n are determined by minimizing a divergence between p(W, z | D) and the approximation q. After fitting q, we make predictions by replacing p(W, z | D) with q in (4) and approximating the integrals in (4) with empirical averages over samples ofW ∼ q. We aim to adjust the parameters of (5) by minimizing the α-divergence between p(W, z | D) and q(W, z) (Minka, 2005):\nDα[p(W, z | D)||q(W, z)] = 1\nα(α− 1)\n( 1− ∫ p(W, z | D)αq(W, z)(1−α) ) dW dz , (6)\nwhich includes a parameter α ∈ R that controls the properties of the optimal q. Figure 1 illustrates these properties for the one-dimensional case. When α ≥ 1, q tends to cover the whole posterior distribution p. When α ≤ 0, q tends to fit a local mode in p. The value α = 0.5 is expected to achieve a balance between these two tendencies. Importantly, when α→ 0, the solution obtained is the same as with variational Bayes (VB) (Wainwright & Jordan, 2008).\nThe direct minimization of (6) is infeasible in practice for arbitrary α. Instead, we follow HernándezLobato et al. (2016) and optimize an energy function whose minimizer corresponds to a local minimization of α-divergences, with one α-divergence for each of the N likelihood factors in (1). Since q is Gaussian and the priors p(W) and p(z) are also Gaussian, we represent q as\nq(W, z) ∝ [ N∏ n=1 f(W)fn(zn) ] p(W)p(z) , (7)\nwhere f(W) is a Gaussian factor that approximates the geometric mean of the N likelihood factors in (1) as a function ofW . Each fn(zn) is also a Gaussian factor that approximates the n-th likelihood factor in (1) as a function of zn. We adjust f(W) and the fn(zn) by minimizing local α-divergences. In particular, we minimize the energy function\nEα(q) = − logZq − 1\nα N∑ n=1 log EW,zn∼ q [( p(yn |W,xn, zn,Σ) f(W)fn(zn) )α] , (8)\n(Hernández-Lobato et al., 2016), where f(W) and fn(zn) are in exponential Gaussian form and parameterized in terms of the parameters of q and the priors p(W) and p(zn), that is,\nf(W) = exp  L∑ l=1 Vl∑ i=1 Vl−1+1∑ j=1 1 N ( λvwi,j,l λ− vwi,j,l w2i,j,l + mwi,j,l vwi,j,l wi,j,l ) ∝ [ q(W) p(W) ] 1 N , (9)\nfn(zn) = exp { γvzn γ − vzn z2n + mzn vzn zn } ∝ q(zn) p(zn) , (10)\nand logZq is the logarithm of the normalization constant of the exponential Gaussian form of q:\nlogZq = L∑ l=1 Vl∑ i=1 Vl−1+1∑ j=1 1 2 log ( 2πvwi,j,l ) + ( mwi,j,l )2 vwi,j,l + N∑ n=1 [ 1 2 log (2πvzn) + (mzn) 2 vzn ] . (11)\nThe scalable optimization of (8) is done in practice by using stochastic gradient descent. For this, we subsample the sums for n = 1, . . . , N in (8) and (11) using mini-batches and approximate the expectations over q in (8) with an average over K samples drawn from q. We can then use the reparametrization trick (Kingma et al., 2015) to obtain gradients from the resulting stochastic approximator to (8). The hyper-parameters Σ, λ and γ can also be tuned by minimizing (8). In practice we only tune Σ and keep λ = 1 and γ = d. The latter means that the prior scale of each zn grows with the data dimensionality. This guarantees that, a priori, the effect of each zn in the neural network’s output does not diminish when more and more features are available.\nMinimizing (8) when α→ 0 is equivalent to running the method VB (Hernández-Lobato et al., 2016), which has recently been used to train Bayesian neural networks in reinforcement learning problems (Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016). However, we propose to minimize (8) using α = 0.5, which often results in better test log-likelihood values.\nWe have also observed α = 0.5 to be more robust than VB when q(z) is not fully optimized. In particular, α = 0.5 can still capture complex stochastic patterns even when we do not learn q(z) and instead keep it fixed to the prior p(z). By contrast, VB fails completely in this case (see Appendix A)."
    }, {
      "heading" : "3 POLICY SEARCH USING BNNS WITH STOCHASTIC INPUTS",
      "text" : "We now describe a gradient-based policy search algorithm that uses the BNNs with stochastic disturbances from the previous section. The motivation for our approach lies in its applicability to industrial systems: we wish to estimate a policy in parametric form, using only an available batch of state transitions obtained from an already-running system. We assume that the true dynamics present stochastic patterns that arise due to some unobserved process affecting the system in complex ways.\nModel-based policy search methods include two key parts (Deisenroth et al., 2013). The first part consists in learning a dynamics model from data in the form of state transitions (st,at, st+1), where st denotes the current state, at is the action applied and st+1 is the resulting state. The second part consists in learning the parametersWπ of a deterministic policy function π that returns the optimal action at = π(st;Wπ) as function of the current state st. The policy function can be a neural network with deterministic weights given byWπ . The first part in the aforementioned procedure is a standard regression task, which we solve by using the modeling approach from the previous section. We assume the dynamics to be stochastic with the following true transition model:\nst = ftrue(st−1,at−1, zt;Wtrue) , zt ∼ N (0, γ) . (12) where the input disturbances zt ∼ N (0, γ) account for the stochasticity in the dynamics. When the Markov state st is hidden and we are given only observations ot , we can use the time embedding theorem using a suitable window of length n and approximate:\nŝ(t) = [ot−n, · · · ,ot] . (13) The transition model in equation 12 specifies a probability distribution p(st|st−1,at−1) that we approximate using a BNN with stochastic inputs:\np(st|st−1,at−1) ≈ ∫ N (st|f(st−1,at−1, zt;W),Σ)q(W)N (zt|0, γ) dW dzt , (14)\nAlgorithm 1 Model-based policy search using Bayesian neural networks with stochastic inputs.\n1: Input: D = {sn, an,∆n} for n ∈ 1..N 2: Fit q(W) and Σ by optimizing (8). 3: function UNFOLD(s0) 4: sample{W1, ..,WK} from q(W) 5: C ← 0 6: for k = 1 : K do 7: for t = 0 : T do 8: zkt+1 ∼ N (0, γ) 9: ∆t ← f(st, π(st;Wπ), zkt+1;Wk)\n10: kt+1 ∼ N (0,Σ) 11: st+1 ← st + ∆t + kt+1 12: C ← C + c(st+1) 13: return C/K 14: FitWπ by optimizing 1N ∑N n=1 UNFOLD(sn)\n0 1 2 3 4 5 yt+1\n0\n0.6\np (y t+\n1 )\nst = (2.0, 3.0) , at = (0, 0.0) GP\nV B α = 0.5 Ground Truth\n0 1 2 3 4 5 yt+1\n0\n0.6\np (y t+\n1 )\nst = (2.0, 4.0) , at = (−1.0, 0.0) GP\nV B α = 0.5 Ground Truth\n0 1 2 3 4 5 yt+1\n0\n0.6\np (y t+\n1 )\nst = (4.3, 3.0) , at = (0.3, 1.0) GP\nV B α = 0.5 Ground Truth\n0 1 2 3 4 5 yt+1\n0\n0.6\np (y t+\n1 )\nst = (0.0, 1.5) , at = (0.0, 0.0) GP\nV B α = 0.5 Ground Truth\nFigure 2: Predictive distribution of yt given by different methods in four different scenarios. Ground truth (red) is obtained by sampling from the real dynamics.\nwhere the feature vectors in our BNN are now st−1 and at−1 and the targets are given by st. In this expression, the integration with respect toW accounts for stochasticity arising from lack of knowledge of the model parameters, while the integration with respect to zt accounts for stochasticity arising from unobserved processes that cannot be modeled. In practice, these integrals are approximated by an average over samples of zt ∼ N (0, γ) andW ∼ q. In the second part of our model-based policy search algorithm, we optimize the parametersWπ of a policy that minimizes the sum of expected cost over a finite horizon T with respect to our belief q(W). This expected cost is obtained by averaging over multiple virtual roll-outs. For each roll-out we sampleWi ∼ q and then simulate state trajectories using the model st+1 = f(st,at, zt;Wi) + t+1 with policy at = π(st;Wπ), input noise zt ∼ N (0, γ) and additive noise t+1 ∼ N (0,Σ). This procedure allows us to obtain estimates of the policy’s expected cost for any particular cost function. If model, policy and cost function are differentiable, we are then able to tune Wπ by stochastic gradient descent over the roll-out average.\nGiven a cost function c(st), the objective to be optimized by our policy search algorithm is J(Wπ) = E [∑T t=1 c(st) ] . (15)\nWe approximate (15) by using (14), replacing at with π(st;Wπ) and using sampling to approximate the expectations:\nJ(Wπ) = ∫ [ T∑\nt=1\nc(st) ][ T∏ t=1 ∫ N (st|f(st−1, π(st−1;Wπ), zt;W),Σ)q(W)N (zt|0, γ) dW dzt ] p(s0)ds0 · · · dsT\n= ∫ [ T∑ t=1 c(s W,{z1,...,zt},{ 1,..., t},Wπ t ) ] q(W)dW [ T∏ t=1 N ( t|0,Σ)N (zt|0, γ)d tdzt ] p(s0) ds0\n≈ 1 K K∑ k=1 [ T∑ t=1 c(s Wk,{zk1 ,...,z k t },{ k 1 ,..., k t },Wπ t ) ] . (16)\nThe first line in (16) is obtained by using the assumption that the dynamics are Markovian with respect to the current state and the current action and by replacing p(st|st−1,at−1) with the right-hand side of (14). In the second line, sW,{z1,...,zt},{ 1,..., t},Wπt is the state that is obtained at time t in a roll-out generated by using a policy with parametersWπ , a transition function parameterized byW and input noise z1, . . . , zt, with additive noise values 1, . . . , t. In the last line we have approximated the integration with respect toW, z1, . . . , zT , 1, . . . , T and s0 by averaging over K samples of these variables. To sample s0, we draw this variable uniformly from the available transitions (st,at, st+1).\nThe expected cost (15) can then be optimized by stochastic gradient descent using the gradients of the Monte Carlo approximation given by the last line of (16). Algorithm 1 computes this Monte\nCarlo approximation. The gradients can then be obtained using automatic differentiation tools such as Theano (Theano Development Team). Note that Algorithm 1 uses the BNNs to make predictions for the change in the state ∆t = st+1 − st instead of for the next state st+1 since this approach often performs better in practice (Deisenroth & Rasmussen, 2011)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We now evaluate the performance of our algorithm for policy search in different benchmark problems. These problems are chosen based on two reasons. First, they contain complex stochastic dynamics and second, they represent real-world applications common in industrial settings. A theano implementation of algorithm 1 is available online1. See the appendix B for a short introduction to all methods we compare to and appendix C for the hyper-parameters used."
    }, {
      "heading" : "4.1 WET-CHICKEN BENCHMARK",
      "text" : "The Wet-Chicken benchmark (Tresp, 1994) is a challenging problem for model-based policy search that presents both bi-modal and heteroskedastic transition dynamics. We use the two-dimensional version of the problem (Hans & Udluft, 2009) and extend it to the continuous case.\nIn this problem, a canoeist is paddling on a two-dimensional river. The canoeist’s position at time t is (xt, yt). The river has width w = 5 and length l = 5 with a waterfall at the end, that is, at yt = l. The canoeist wants to move as close to the waterfall as possible because at time t he gets reward rt = −(l − yt). However, going beyond the waterfall boundary makes the canoeist fall down, having to start back again at the origin (0, 0). At time t the canoeist can choose an action (at,x, at,y) ∈ [−1, 1]2 that represents the direction and magnitude of his paddling. The river dynamics have stochastic turbulences st and drift vt that depend on the canoeist’s position on the x axis. The larger xt, the larger the drift and the smaller xt, the larger the turbulences. The underlying dynamics are given by the following system of equations. The drift and the turbulence magnitude are given by vt = 3xtw −1 and st = 3.5− vt, respectively. The new location (xt+1, yt+1) is given by the current\n1https://github.com/siemens/policy_search_bb-alpha\nlocation (xt, yt) and current action (at,x, at,y) using\nxt+1 =  0 if xt + at,x < 0 0 if ŷt+1 > l w if xt + at,x > w xt + at,x otherwise , yt+1 =  0 if ŷt+1 < 0 0 if ŷt+1 > l ŷt+1 otherwise , (17)\nwhere ŷt+1 = yt + (at,y − 1) + vt + stτt and τt ∼ Unif([−1, 1]) is a random variable that represents the current turbulence. These dynamics result in rich transition distributions depending on the position as illustrated by the plots in Figure 2. As the canoeist moves closer to the waterfall, the distribution for the next state becomes increasingly bi-modal (see Figure 1c) because when he is close to the waterfall, the change in the current location can be large if the canoeist falls down the waterfall and starts again at (0, 0). The distribution may also be truncated uniform for states close to the borders (see Figure 1d). Furthermore the system has heteroskedastic noise, the smaller the value of xt the higher the noise variance (compare Figure 1a with 1b). Because of these properties, the Wet-Chicken problem is especially difficult for model-based reinforcement learning methods. To our knowledge it has only been solved using model-free approaches after a discretization of the state and action sets (Hans & Udluft, 2009). For model training we use a batch 2500 random state transitions.\nThe predictive distributions of different models for yt+1 are shown in Figure 2 for specific choices of (xt, yt) and (ax,t, ay,t). These plots show that BNNs with α = 0.5 are very close to the ground-truth. While it is expected that Gaussian processes fail to model multi-modalities in Figure 1c, the FTIC approximation allows them to model the heteroskedasticity to an extent. VB captures the stochastic patterns on a global level, but often under or over-estimates the true probability density in specific regions. The test-loglikelihood and test MSE in y-dimension are reported in Table 2 for all methods. (the transitions for x are deterministic given y).\nAfter fitting the models, we train policies using Algorithm 1 with a horizon of size T = 5. Table 1 shows the average reward obtained by each method. BNNs with α = 0.5 perform best and produce policies that are very close to the optimal upper bound, as indicated by the performance of the particle swarm optimization policy (PSO-P). In this problem VB seems to lack robustness and has much larger empirical variance across experiment repetitions than α = 0.5 or α = 1.0.\nFigure 3 shows three example policies, πVB ,πα=0.5 and πGP (Figure 3a,3b and 3c, respectively). The policies obtained by BNNs with random inputs (VB and α = 0.5) show a richer selection of actions. The biggest differences are in the middle-right regions of the plots, where the drift towards the waterfall is large and the bi-modal transition for y (missed by the GP) is more important."
    }, {
      "heading" : "4.2 INDUSTRIAL APPLICATIONS",
      "text" : "We now present results on two industrial cases. First, we focus on data generated by a real gas turbine and second, we consider a recently introduced simulator called the \"industrial benchmark\", with code publicly available2 (Hein et al., 2016b). According to the authors: \"The \"industrial benchmark\" aims at being realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications.\"\n2http://github.com/siemens/industrialbenchmark"
    }, {
      "heading" : "4.2.1 GAS TURBINE DATA",
      "text" : "For the experiment with gas turbine data we simulate a task with partial observability. To that end we use 40,000 observations of a 30 dimensional time-series of sensor recordings from a real gas turbine. We are also given a cost function that evaluates the performance of the current state of the turbine. The features in the time-series are grouped into three different sets: a set of environmental variables Et (e.g. temperature and measurements from sensors in the turbine) that cannot be influenced by the agent, a set of variables relevant for the cost function Nt (e.g. the turbines current pollutant emission) and a set of steering variables At that can be manipulated to control the turbine.\nWe first train a world model as a reflection of the real turbine dynamics. To that end we define the world model’s transitions for Nt to have the functional form Nt = f(Et−5, .., Et, At−5, ..At). The world model assumes constant transitions for the environmental variables: Et+1 = Et. To make fair comparisons, our world model is given by a non-Bayesian neural network with deterministic weights and with additive Gaussian output noise.\nWe then use the world model to generate an artificial batch of data for training the different methods. The inputs in this batch are still the same as in the original turbine data, but the outputs are now sampled from the world model. After generating the artificial data, we only keep a small subset of the original inputs to the world model. The aim of this experiment is to learn policies that are robust to noise in the dynamics. This noise would originate from latent factors that cannot be controlled, such as the missing features that were originally used to generate the outputs by the world model but which are no longer available. After training the models for the dynamics, we use algorithm 1 for policy optimization. The resulting policies are then finally evaluated in the world model.\nTables 2 and 1 show the respective model and policy performances for each method. The experiment was repeated 5 times and we report average results. We observe that α = 0.5 performs best in this scenario, having the highest test log-likelihood and best policy performance."
    }, {
      "heading" : "4.2.2 INDUSTRIAL BENCHMARK",
      "text" : "In this benchmark the hidden Markov state space st consists of 27 variables, whereas the observable state ot is only 5 dimensional. This observable state consists of 3 adjustable steering variables At: the velocity v(t), the gain g(t) and the shift s(t). We also observe the fatigue f(t) and consumption\nc(t) that together form the reward signal R(t) = −(3f(t) + c(t)). Also visible is the setpoint S, a constant hyper-parameter of the benchmark that indicates the complexity of the dynamics.\nFor each setpoint S ∈ {10, 20, · · · , 100} we generate 7 trajectories of length 1000 using random exploration. This batch with 70, 000 state transitions forms the training set. We use 30, 000 state transitions, consisting of 3 trajectories for each setpoint, as test set.\nFor data preprocessing, in addition to the standard normalization process, we apply a log transformation to the reward variable. Because the reward is bounded in the interval [0, Rmax], we use a logit transformation to map this interval into the real line. We define the functional form for the dynamics as Rt = f(At−15, · · · , At, Rt−15, · · · , Rt−1). The test errors and log-likelihood are given in Table 2. We see that BNNs with α = 0.5 and α = 1.0 perform best here, whereas Gaussian processes or the MLP obtain rather poor results.\nEach row in Figure 4 visualizes long term predictions of the MLP and BNNs trained with VB and α = 0.5 in two specific cases. In the top row we see that while all three methods produce wrong predictions in expectation (compare dark blue curve to red curve). However, BNNs trained with V B and with α = 0.5 exhibit a bi-modal distribution of predicted trajectories, with one mode following the ground-truth very closely. By contrast, the MLP misses the upper mode completely. The bottom row shows that the VB and α = 0.5 also produce more tight confident bands in other settings.\nNext, we learn policies using the trained models. Here we use a relatively long horizon of T = 75 steps. Table 1 shows average rewards obtained when applying the policies to the real dynamics. Because both benchmark and models have an autoregressive component, we do an initial warm-up phase using random exploration before we apply the policies to the system and start to measure rewards.\nWe observe that GPs perform very poorly in this benchmark. We believe the reason for this is the long search horizon, which makes the uncertainties in the predictive distributions of the GPs become very large. Tighter confidence bands, as illustrated in Figure 4 seem to be key for learning good policies. Overall, α = 1.0 performs best with α = 0.5 being very close."
    }, {
      "heading" : "5 RELATED WORK",
      "text" : "There has been relatively little attention to using Bayesian neural networks for reinforcement learning. In Blundell et al. (2015) a Thompson sampling approach is used for a contextual bandits problem; the focus is tackling the exploration-exploitation trade-off, while the work in Watter et al. (2015) combines variational auto-encoder with stochastic optimal control for visual data. Compared to our approach the first of these contributions focusses on the exploration/exploitation dilemma, while the second one uses a stochastic optimal control approach to solve the learning problem. By contrast, our work seeks to find an optimal parameterized policy.\nPolicy gradient techniques are a prominent class of policy search algorithms (Peters & Schaal, 2008). While model-based approaches were often used in discrete spaces (Wang & Dietterich, 2003), model-free approaches tended to be more popular in continuous spaces (e.g. Peters & Schaal (2006)).\nOur work can be seen as a Monte-Carlo model-based policy gradient technique in continuous stochastic systems. Similar work was done using Gaussian processes (Deisenroth & Rasmussen, 2011) and with recurrent neural networks (Schaefer et al., 2007) . The Gaussian process approach, while restricted to a Gaussian state distribution, allows propagating beliefs over the roll-out procedure. More recently Gu et al. (2016) augment a model-free learning procedure with data generated from model-based roll-outs."
    }, {
      "heading" : "6 CONCLUSION AND FUTURE WORK",
      "text" : "We have extended the standard Bayesian neural network (BNN) model with the addition of a random input noise source z. This enables principled Bayesian inference over complex stochastic functions. We have shown that our BNNs with random inputs can be trained with high accuracy by minimizing α-divergences, with α = 0.5, which often produces better results than variational Bayes. We have\nalso presented an algorithm that uses random roll-outs and stochastic optimization for learning a parameterized policy in a batch scenario. This algorithm particular suited for industry domains.\nOur BNNs with random inputs have allowed us to solve a challenging benchmark problem where model-based approaches usually fail. They have also shown promising results on industry benchmarks including real-world data from a gas turbine. In particular, our experiments indicate that a BNN trained with α = 0.5 as divergence measure in conjunction with the presented algorithm for policy optimization is a powerful black-box tool for policy search.\nAs future work we will consider safety and exploration. For safety, we believe having uncertainty over the underlaying stochastic functions will allows us to optimize policies by focusing on worst case results instead of on average performance. For exploration, having uncertainty on the stochastic functions will be useful for efficient data collection."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "José Miguel Hernández-Lobato acknowledges support from the Rafael del Pino Foundation. The authors would like to thank Ryan P. Adams, Hans-Georg Zimmermann, Matthew J. Johnson, David Duvenaud and Justin Bayer for helpful discussions."
    }, {
      "heading" : "B METHODS",
      "text" : "In the experiments we compare to the following methods:\nStandard MLP. The standard multi-layer preceptron (MLP) is equivalent to our BNNs, but does not have uncertainty over the weightW and does not include any stochastic inputs. We train this method using early stopping on a subset of the training data. When we perform roll-outs using algorithm 1, the predictions of the MLP are made stochastic by adding Gaussian noise to its output. The noise variance is fixed by maximum likelihood on some validation data after model training.\nVariational Bayes (VB). The most prominent approach in training modern BNNs is to optimize the variational lower bound (Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016). This is in practice equivalent to α-divergence minimization when α→ 0 (Hernández-Lobato et al., 2016). In our experiments we use α-divergence minimization with α = 10−6 to implement this method.\nGaussian Processes (GPs). Gaussian Processes have recently been used for policy search under the name of PILCO (Deisenroth & Rasmussen, 2011). For each dimension of the target variables, we fit a different sparse GP using the FITC approximation (Snelson & Ghahramani, 2005). In particular, each sparse GP is trained using 150 inducing inputs by using the method stochastic expectation propagation (Bui et al., 2016). After this training process we approximate the sparse GP by using a feature expansion with random basis functions (see supplementary material of Hernández-Lobato et al. 2014). This allows us to draw samples from the GP posterior distribution over functions, enabling the use of Algorithm 1 for policy training. Note that PILCO will instead moment-match at every roll-out step as it works by propagating Gaussian distributions. However, in our experiments we obtained better performance by avoiding the moment matching step with the aforementioned approximation based on random basis functions.\nParticle Swarm Optimization Policy(PSO-P). We use this method to estimate an upper bound for reward performance. PSO-P is a model predictive control (MPC) method that uses the true dynamics when applicable (Hein et al., 2016a). For a given state st, the best action is selected using the standard receding horizon approach on the real environment. Note that this is not a benchmark method to compare to, we use it instead as an indicator of what the best possible reward can be achieved for a fixed planning horizon T ."
    }, {
      "heading" : "C MODEL PARAMETERS",
      "text" : "For all tasks we will use a standard MLP with two hidden layer with 20 hidden units each as policy representation. The activation functions for the hidden units are rectifiers: ϕ(x) = max(x, 0). If present, bounding of the actions is realized using the tanh activation function on the outputs of the policy. All models based on neural network will share the same hyperparameter. We use ADAM as learning algorithm in all tasks.\nWetChicken The neural network models are set to 2 hidden layers and 20 hidden units per layer. We use 2500 random state transitions for training. We found that assuming no observation noise by setting Γ to a constant of 10−5 helped the models converge to lower energy values.\nFor policy training we use a horizon of size T = 5 and optimize the policy network for 100 epochs, averaging over K = 20 samples in each gradient update, with mini-batches of size 10 and learning rate set to 10−5.\nTurbine The world model and the BNNs have two hidden layers with 50 hidden units each. For policy training and world-model evaluation we perform a roll-out with horizon T = 20. For learning the policy we use minibaches of size 10 and draw K = 10 samples from q.\nIndustrial Benchmark For the neural network models we use two hidden layers with 75 hidden units.We use a horizon of T = 75, training for 500 epochs with batches of size 50 and K = 25 samples for each rollout."
    }, {
      "heading" : "D COMPUTATIONAL COMPLEXITY",
      "text" : "MODEL TRAINING\nAll models were trained using theano and a single GPU. Training the standard neural network is fast, the training time for this method was between 5 - 20 minutes, depending on data set size and dimensionality of the benchmark. In theano, the computational graph of the BNNs is similar to that of an ensemble of standard neural networks. The training time for the BNNs varied between 30 minutes to 5 hours depending on data size and dimensionality of benchmark. The sparse Gaussian Process was optimized using an expectation propagation algorithm and after training, it was approximated with a Bayesian linear model with fixed basis functions whose weights are initialized randomly (see Appendix B). We choose the inducing points in the GPs and the number of training epochs for these models so that the resulting training time was comparable to that of the BNNs.\nPOLICY SEARCH\nFor policy training we used a single CPU. All methods are of similar complexity as they are all trained using Algorithm 1. Depending on the horizon, data set size and network topology, training took between 20 minutes (Wet-Chicken, T = 5), 3-4 hours (Turbine, T = 20) and 14-16 hours (industrial benchmark, T = 75)."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming and Optimal Control. Athena Scientific optimization and computation series",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : null,
      "citeRegEx" : "Bertsekas.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2002
    }, {
      "title" : "Weight uncertainty in neural networks",
      "author" : [ "Charles Blundell", "Julien Cornebise", "Koray Kavukcuoglu", "Daan Wierstra" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Blundell et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Blundell et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep Gaussian processes for regression using approximate expectation propagation",
      "author" : [ "Thang D Bui", "Daniel Hernández-Lobato", "Yingzhen Li", "José Miguel Hernández-Lobato", "Richard E Turner" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bui et al\\.",
      "year" : 2016
    }, {
      "title" : "PILCO: A model-based and data-efficient approach to policy search",
      "author" : [ "Marc Deisenroth", "Carl E Rasmussen" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Deisenroth and Rasmussen.,? \\Q2011\\E",
      "shortCiteRegEx" : "Deisenroth and Rasmussen.",
      "year" : 2011
    }, {
      "title" : "A survey on policy search for robotics",
      "author" : [ "Marc Peter Deisenroth", "Gerhard Neumann", "Jan Peters" ],
      "venue" : "Foundations and Trends in Robotics,",
      "citeRegEx" : "Deisenroth et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Deisenroth et al\\.",
      "year" : 2013
    }, {
      "title" : "Model predictive control using neural networks",
      "author" : [ "A. Draeger", "S. Engell", "H. Ranke" ],
      "venue" : "IEEE Control Systems,",
      "citeRegEx" : "Draeger et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Draeger et al\\.",
      "year" : 1995
    }, {
      "title" : "Improving PILCO with Bayesian neural networks dynamics models",
      "author" : [ "Yarin Gal", "Rowan Mcallister", "Carl Rasmussen" ],
      "venue" : "In Data-Efficient Machine Learning workshop,",
      "citeRegEx" : "Gal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gal et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous deep q-learning with model-based acceleration",
      "author" : [ "Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine" ],
      "venue" : "arXiv preprint arXiv:1603.00748,",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient uncertainty propagation for reinforcement learning with limited data",
      "author" : [ "Alexander Hans", "Steffen Udluft" ],
      "venue" : "In ICANN,",
      "citeRegEx" : "Hans and Udluft.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hans and Udluft.",
      "year" : 2009
    }, {
      "title" : "Reinforcement learning with particle swarm optimization policy (PSO-P) in continuous state and action",
      "author" : [ "Daniel Hein", "Alexander Hentschel", "Thomas A Runkler", "Steffen Udluft" ],
      "venue" : "spaces. IJSIR,",
      "citeRegEx" : "Hein et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hein et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to the\" industrial benchmark",
      "author" : [ "Daniel Hein", "Alexander Hentschel", "Volkmar Sterzing", "Michel Tokic", "Steffen Udluft" ],
      "venue" : "arXiv preprint arXiv:1610.03793,",
      "citeRegEx" : "Hein et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hein et al\\.",
      "year" : 2016
    }, {
      "title" : "Predictive entropy search for efficient global optimization of black-box functions",
      "author" : [ "José Miguel Hernández-Lobato", "Matthew W Hoffman", "Zoubin Ghahramani" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hernández.Lobato et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hernández.Lobato et al\\.",
      "year" : 2014
    }, {
      "title" : "Black-box α-divergence minimization",
      "author" : [ "José Miguel Hernández-Lobato", "Yingzhen Li", "Mark Rowland", "Daniel Hernández-Lobato", "Thang Bui", "Richard E Turner" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Hernández.Lobato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hernández.Lobato et al\\.",
      "year" : 2016
    }, {
      "title" : "VIME: Variational information maximizing exploration",
      "author" : [ "Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Houthooft et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Houthooft et al\\.",
      "year" : 2016
    }, {
      "title" : "Variational dropout and the local reparameterization trick",
      "author" : [ "Diederik P Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Gaussian processes and reinforcement learning for identification and control of an autonomous blimp",
      "author" : [ "Jonathan Ko", "Daniel J Klein", "Dieter Fox", "Dirk Haehnel" ],
      "venue" : "In IEEE Robotics and Automation,",
      "citeRegEx" : "Ko et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ko et al\\.",
      "year" : 2007
    }, {
      "title" : "Divergence measures and message passing",
      "author" : [ "Tom Minka" ],
      "venue" : "Technical report, Microsoft Research,",
      "citeRegEx" : "Minka.,? \\Q2005\\E",
      "shortCiteRegEx" : "Minka.",
      "year" : 2005
    }, {
      "title" : "Policy gradient methods for robotics",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "In IROS, pp. 2219–2225,",
      "citeRegEx" : "Peters and Schaal.,? \\Q2006\\E",
      "shortCiteRegEx" : "Peters and Schaal.",
      "year" : 2006
    }, {
      "title" : "Reinforcement learning of motor skills with policy gradients",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Peters and Schaal.,? \\Q2008\\E",
      "shortCiteRegEx" : "Peters and Schaal.",
      "year" : 2008
    }, {
      "title" : "Gaussian processes in reinforcement learning",
      "author" : [ "Carl Edward Rasmussen", "Malte Kuss" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Rasmussen and Kuss,? \\Q2003\\E",
      "shortCiteRegEx" : "Rasmussen and Kuss",
      "year" : 2003
    }, {
      "title" : "The recurrent control neural network",
      "author" : [ "Anton Maximilian Schaefer", "Steffen Udluft", "Hans-Georg Zimmermann" ],
      "venue" : "In ESANN, pp",
      "citeRegEx" : "Schaefer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Schaefer et al\\.",
      "year" : 2007
    }, {
      "title" : "Sparse Gaussian processes using pseudo-inputs",
      "author" : [ "Edward Snelson", "Zoubin Ghahramani" ],
      "venue" : "In NIPS, pp. 1257–1264,",
      "citeRegEx" : "Snelson and Ghahramani.,? \\Q2005\\E",
      "shortCiteRegEx" : "Snelson and Ghahramani.",
      "year" : 2005
    }, {
      "title" : "The wet game of chicken",
      "author" : [ "V. Tresp" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Tresp.,? \\Q1994\\E",
      "shortCiteRegEx" : "Tresp.",
      "year" : 1994
    }, {
      "title" : "System identification using Laguerre models",
      "author" : [ "Bo Wahlberg" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Wahlberg.,? \\Q1991\\E",
      "shortCiteRegEx" : "Wahlberg.",
      "year" : 1991
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Wainwright and Jordan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan.",
      "year" : 2008
    }, {
      "title" : "Model-based policy gradient reinforcement learning",
      "author" : [ "Xin Wang", "Thomas G Dietterich" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Wang and Dietterich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wang and Dietterich.",
      "year" : 2003
    }, {
      "title" : "Embed to control: A locally linear latent dynamics model for control from raw images",
      "author" : [ "Manuel Watter", "Jost Springenberg", "Joschka Boedecker", "Martin Riedmiller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Watter et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Watter et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Popular modes for transition functions include Gaussian processes (Rasmussen et al., 2003; Ko et al., 2007; Deisenroth & Rasmussen, 2011), fixed bases such as Laguerre functions (Wahlberg, 1991), and adaptive basis functions or neural networks (Draeger et al.",
      "startOffset" : 66,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : ", 2007; Deisenroth & Rasmussen, 2011), fixed bases such as Laguerre functions (Wahlberg, 1991), and adaptive basis functions or neural networks (Draeger et al.",
      "startOffset" : 78,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : ", 2007; Deisenroth & Rasmussen, 2011), fixed bases such as Laguerre functions (Wahlberg, 1991), and adaptive basis functions or neural networks (Draeger et al., 1995).",
      "startOffset" : 144,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "We take advantage of a very recent inference advance based on α-divergence minimization (Hernández-Lobato et al., 2016), with α = 0.",
      "startOffset" : 88,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Bertsekas (2002), will start with the most general model of dynamical systems: st+1 = f(st,a, z,W) where f is some deterministic function parameterized by weightsW that takes as input the current state st, the control signal a, and some stochastic disturbance z.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "This method produces (to our knowledge) the first model-based solution of a 20-year-old benchmark problem: the Wet-Chicken (Tresp, 1994).",
      "startOffset" : 123,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "Figure source Minka (2005).",
      "startOffset" : 14,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "We aim to adjust the parameters of (5) by minimizing the α-divergence between p(W, z | D) and q(W, z) (Minka, 2005):",
      "startOffset" : 102,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "(Hernández-Lobato et al., 2016), where f(W) and fn(zn) are in exponential Gaussian form and parameterized in terms of the parameters of q and the priors p(W) and p(zn), that is,",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "We can then use the reparametrization trick (Kingma et al., 2015) to obtain gradients from the resulting stochastic approximator to (8).",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Minimizing (8) when α→ 0 is equivalent to running the method VB (Hernández-Lobato et al., 2016), which has recently been used to train Bayesian neural networks in reinforcement learning problems (Blundell et al.",
      "startOffset" : 64,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : ", 2016), which has recently been used to train Bayesian neural networks in reinforcement learning problems (Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : ", 2016), which has recently been used to train Bayesian neural networks in reinforcement learning problems (Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : ", 2016), which has recently been used to train Bayesian neural networks in reinforcement learning problems (Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "Model-based policy search methods include two key parts (Deisenroth et al., 2013).",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "The Wet-Chicken benchmark (Tresp, 1994) is a challenging problem for model-based policy search that presents both bi-modal and heteroskedastic transition dynamics.",
      "startOffset" : 26,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : "Similar work was done using Gaussian processes (Deisenroth & Rasmussen, 2011) and with recurrent neural networks (Schaefer et al., 2007) .",
      "startOffset" : 113,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "In Blundell et al. (2015) a Thompson sampling approach is used for a contextual bandits problem; the focus is tackling the exploration-exploitation trade-off, while the work in Watter et al.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "In Blundell et al. (2015) a Thompson sampling approach is used for a contextual bandits problem; the focus is tackling the exploration-exploitation trade-off, while the work in Watter et al. (2015) combines variational auto-encoder with stochastic optimal control for visual data.",
      "startOffset" : 3,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "In Blundell et al. (2015) a Thompson sampling approach is used for a contextual bandits problem; the focus is tackling the exploration-exploitation trade-off, while the work in Watter et al. (2015) combines variational auto-encoder with stochastic optimal control for visual data. Compared to our approach the first of these contributions focusses on the exploration/exploitation dilemma, while the second one uses a stochastic optimal control approach to solve the learning problem. By contrast, our work seeks to find an optimal parameterized policy. Policy gradient techniques are a prominent class of policy search algorithms (Peters & Schaal, 2008). While model-based approaches were often used in discrete spaces (Wang & Dietterich, 2003), model-free approaches tended to be more popular in continuous spaces (e.g. Peters & Schaal (2006)).",
      "startOffset" : 3,
      "endOffset" : 844
    }, {
      "referenceID" : 1,
      "context" : "In Blundell et al. (2015) a Thompson sampling approach is used for a contextual bandits problem; the focus is tackling the exploration-exploitation trade-off, while the work in Watter et al. (2015) combines variational auto-encoder with stochastic optimal control for visual data. Compared to our approach the first of these contributions focusses on the exploration/exploitation dilemma, while the second one uses a stochastic optimal control approach to solve the learning problem. By contrast, our work seeks to find an optimal parameterized policy. Policy gradient techniques are a prominent class of policy search algorithms (Peters & Schaal, 2008). While model-based approaches were often used in discrete spaces (Wang & Dietterich, 2003), model-free approaches tended to be more popular in continuous spaces (e.g. Peters & Schaal (2006)). Our work can be seen as a Monte-Carlo model-based policy gradient technique in continuous stochastic systems. Similar work was done using Gaussian processes (Deisenroth & Rasmussen, 2011) and with recurrent neural networks (Schaefer et al., 2007) . The Gaussian process approach, while restricted to a Gaussian state distribution, allows propagating beliefs over the roll-out procedure. More recently Gu et al. (2016) augment a model-free learning procedure with data generated from model-based roll-outs.",
      "startOffset" : 3,
      "endOffset" : 1264
    } ],
    "year" : 2017,
    "abstractText" : "We present an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The system dynamics are described with Bayesian neural networks (BNNs) that include stochastic input variables. These input variables allow us to capture complex statistical patterns in the transition dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed by alternative modeling approaches. After learning the dynamics, our BNNs are then fed into an algorithm that performs random roll-outs and uses stochastic optimization for policy learning. We train our BNNs by minimizing α-divergences with α = 0.5, which usually produces better results than other techniques such as variational Bayes. We illustrate the performance of our method by solving a challenging problem where model-based approaches usually fail and by obtaining promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.",
    "creator" : "LaTeX with hyperref package"
  }
}