{
  "name" : "552.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ROTATION PLANE DOUBLY ORTHOGONAL RECUR-",
    "authors" : [ "Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel" ],
    "emails" : [ "pabbeel}@berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep Neural Networks have shown increasingly impressive performance on a wide variety of practical tasks. Recurrent Neural Networks (RNNs) are powerful sequence modeling tools that have found successful applications in speech recognition, natural language processing, image captioning, and many more (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014). One fundamental problem with RNNs is the so called vanishing and exploding gradients problem (Hochreiter, 1991; Bengio, 1994; Hochreiter et al., 2001). These problems occur when training RNNs using gradient descent to model long sequences where the gradient magnitude either goes towards zero or infinity, respectively, as the length of the sequence increases.\nSeveral heuristics have been proposed to alleviate these problems. The Long Short Term Memory (LSTM) in Hochreiter et al. (1997) and Gated Recurrent Units (GRU) in Cho et al. (2014) have been incredibly successful recurrent transition architectures to model complicated dependencies for sequences up to several hundred timesteps long and are the main RNN architectures in use today. The IRNN model modifies the standard RNN transition to initialize at the identity, which increases the timestep length modeling capability (Le et al., 2015). Stabilizing the forward hidden state norm can have a positive effect on hidden state gradient norm preservation (Krueger & Memisevic, 2015; Ba et al., 2016; Cooijmans et al., 2016). A simple gradient norm clipping during hidden state backpropagation can also help to alleviate the exploding gradient problem for these architectures (Graves, 2013).\nRecently, there has been a surge of interest in orthogonal and unitary transition architectures. Orthogonal and unitary transitions exactly preserve forward and gradient norms passed through them. Theory developed for the linear neural networks suggests that training time can be vastly shorter when the weight matrices are orthogonal, and even independent of depth (Saxe et al., 2013). The Unitary Evolution Recurrent Neural Networks utilizes a unitary recurrent transition followed by a contractive nonlinearity to exactly solve the exploding gradient problem and greatly alleviate the\nvanishing gradient problem (Arjovsky et al., 2015). A very recent extension expands on this work to increase the expressive power of these transitions and increases the validated architecture up to 2000 timestep dependencies in Wisdom et al. (2016). Analytic solutions to the most common long term dependency example tasks, the memory copy and addition problems, are provided with linear orthogonal recurrent neural networks in Henaff et al. (2016). A nonlinear activation function that is locally orthogonal is proposed in Chernodub & Nowicki and shown to have great potential. This particular activation scheme, however, is discontinuous (not just nondifferentiable), and could increase optimization difficulty in some cases.\nAn open question that we try to partially address in this work is whether a recurrent transition architecture can be fully orthogonal or unitary (and thus linear) and still learn expressive models to solve practical tasks. To address this problem, we propose the rotation plane doubly orthogonal RNN, a novel recurrent transition architecture that provably preserves forward hidden state activation norm and backpropagated gradient norm and thus does not suffer from exploding or vanishing gradients. The doubly orthogonal refers to the fact that the RNN transition architecture updates the hidden state multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. Rotation plane refers to the parameterization we use to represent the orthogonal matrices. We evaluate our approach on a simplified 1-bit version of the memory copying task, and find that our architecture can scale to 5,000 timesteps on this task, outperforming past approaches."
    }, {
      "heading" : "2 DOUBLY ORTHOGONAL RNN ARCHITECTURE",
      "text" : "In this section we describe our proposed architecture, which we call the Doubly Orthogonal Recurrent Neural Net (DORNN). We also show that the DORNN provably preserves forward norm and gradient norm.\nWe briefly review the definition of an orthogonal or unitary matrix, since it is fundamental to the definition and properties of our transition architecture. Orthogonal matrices, or rotation matrices, are defined as matrices Q ∈ Rn×n such that: QTQ = I . We restrict our attention to the special orthogonal matrices SO(n) such that det(Q) = 1. The set of special orthogonal matrices forms a matrix lie group. The complex analogue of an orthogonal matrix is a unitary matrix and is defined similarly as matrices U ∈ Cn×n such that UHU = I ."
    }, {
      "heading" : "2.1 RECURRENCE EQUATIONS",
      "text" : "Our recurrence equation is defined below:\nht+1 = Rxh(xt)Rhhht\nwhere Rxh(xt) is an input modulated orthogonal or unitary matrix, and Rhh is a time invariant orthogonal or unitary matrix that is applied at every timestep. We parameterize Rhh by θhh and Rxh by φxh(xt), where φ is a function of the input xt. Figure 1 shows a graphical depiction of this transition architecture.\nOur transition is different from most recurrent neural network transitions since it is fully multiplicative and contains no addition terms. The input enters into the equation by modulating the rotation applied to the hidden state. This allows for more expressivity in the hidden transition than a constant linear transition, though not as much as a nonlinear hidden state dependent transition. By having an input dependent transition, the hidden dynamics are more complicated than a constant linear transition and are nonlinear with respect to the input xt. Linear time-varying models can express very complicated policies, such as in Levine et al. (2016). Our model can be viewed as a linear orthogonal time-varying model where the variation due to time is due to different input signals applied."
    }, {
      "heading" : "2.2 FORWARD ACTIVATION NORM AND GRADIENT NORM PRESERVATION",
      "text" : "Here we prove that this recurrent transition architecture exactly preserves (up to numerical precision) the forward hidden state activation norm and the backwards gradient norm. All proofs carry forward\nwith unitary matrices instead of orthogonal matrices when transposes are replaced with Hermitian transposes.\n||ht+1|| = ||Rxh(xt)Rhhht|| = ||Rcombined(xt)ht|| = ||ht||\nwhere Rcombined = Rxh(xt)Rhh. The last equality follows since orthogonal matrices are a group and so Rcombined ∈ SO(n), and ||Qv|| = ||v|| for any Q ∈ SO(n) and any v ∈ Rn, since ||Qv|| = √ vTQTQv = √ vT Iv = √ vT v = ||v|| by the definition of Q (orthogonal matrices preserve norm).\nNow let C(hT ) be a scalar cost function. The vanishing gradients problem occurs if || ∂C∂h1 || → 0 as T →∞ and the exploding gradient problem occurs if || ∂C∂h1 || → ∞ as T →∞.\n∂C ∂ht = ∂C ∂hT ∂hT ∂ht = ∂C ∂hT T−1∏ i=t ∂hi+1 ∂hi = ∂C ∂hT T−1∏ i=t RThhRxh(xi) T\nand so\n∣∣∣∣∣∣∣∣ ∂C∂ht ∣∣∣∣∣∣∣∣ = ∣∣∣∣∣ ∣∣∣∣∣ ∂C∂hT T−1∏ i=t RThhRxh(xi) T ∣∣∣∣∣ ∣∣∣∣∣ = ∣∣∣∣∣∣∣∣ ∂C∂hT ∣∣∣∣∣∣∣∣\nwhere the last equality follows from ( ∏T−1 i=t R T hhRxh(xi)\nT ) ∈ SO(n) : an orthogonal matrix’s transpose is its inverse and the inverse of a group element is in the group. So the norm of the gradient of the cost C at hidden state ht is the same as the final norm of the gradient at hidden state hT , and the transition does not suffer from vanishing or exploding gradients."
    }, {
      "heading" : "3 ROTATION PLANE DOUBLY ORTHOGONAL RNN",
      "text" : "Within the Doubly Orthogonal RNN, there is a choice in how the orthogonal (alternatively, unitary), transformations are parameterized. This choice determines the number of parameters, how the gradient propagates from the hidden state to the input, and much more. There are a wide variety of possible DORNN architectures, since there are a wide variety of different ways to parameterize orthogonal and unitary matrices, each with their pros and cons. We provide a particular instantiation of the Doubly Orthogonal RNN by parameterizing the orthogonal matrices in terms of the composition of many plane rotations. We call this RNN architecture the Rotation Plane Doubly Orthogonal RNN, or RP-DORNN.\nWe note that while we focus on this architecture within the context of a recurrent neural network, the rotation plane parameterization of orthogonal matrices could be equally useful for parameterizing very deep feedforward weight matrices."
    }, {
      "heading" : "3.1 ROTATION PLANE REPRESENTATION OF ORTHOGONAL MATRICES",
      "text" : "First we show how we parameterize a single plane rotation. The full architecture is generated by a composition of a sequence of these plane rotations.\nWell known in numerical linear algebra, any orthogonal matrix Q ∈ SO(n) can be generated as the product of n Householder reflection matrices H = I − 2 uu T\n||u||22 for nonzero vectors u ∈ Rn.\nWork in Geometric Algebra (for example see chapter 6 of Dorst et al. (2009)) gives us further insight into this parameterization. Two subsequent Householder reflections generate a plane rotation in the plane spanned by the reflection vectors. The generated rotation is twice the angle between the two reflection vectors. We can use this to parameterize rotation planes in term of the desired angle of rotation, by generating two reflection vectors that produce the rotation. By rotating in several planes in sequence, we can generate arbitrary orthogonal matrices. Thus we can view the rotation angle in a given plane as either a parameter to be optimized or as an input from below in the neural network, both of which we utilize in our proposed recurrent architecture.\nConcretely, for a plane spanned by two orthonormal vectors, w0, and w1, we generate a rotation of angle θ from w0 towards w1 in the w0−w1 plane by generating the following two reflection vectors v0 and v1 and composing their reflections.\nv0 = w0\nv1 = cos(θ/2)w0 + sin(θ/2)w1\nthen Rθ = (I − 2v1vT1 )(I − 2v0vT0 ). We don’t need to divide by the magnitude of v0 or v1 since by construction they are unit vectors. When we apply Rθ to a vector or batch of vectors B, we don’t have to generate the matrix Rθ, since\nRθB = (I − 2v1vT1 )(I − 2v0vT0 )B = (I − 2v1vT1 )(B − 2v0(vT0 B)) =\n= B − 2v1(vT1 B)− 2v0(vT0 B) + 4v1(vT1 (v0(vT0 B)))\nand so we never have to form the full Rθ matrix, we only need to perform matrix multiplies of a vector with a matrix and the intermediate dimensions are much smaller than forming the dense Rθ matrix.\nIn the next section we treatw0, w1 as random constant orthonormal vectors and treat θ as a parameter or a function of inputs."
    }, {
      "heading" : "3.2 RP-DORNN",
      "text" : "We generateRhh andRxh as a sequence of plane rotations in the Rotation Plane Doubly Orthogonal Recurrent Neural Network (RP-DORNN).\nRhh = k∏ i=1 Rθi\nand\nRxh(xt) = l∏ i=1 Rφi(xt)\nfor l, k ≤ bn2 c. Each Rθi and Rφi are plane rotations in randomized orthogonal planes (where the planes from Rhh are orthogonal to one another and the planes from Rxh are orthogonal to one another but the planes from Rhh intersect with the ones from Rxh randomly), parameterized by the angle of rotation. For Rφi , the angle of rotation is a function of xt, and for Rθi . In our exploratory experiments we investigated several choices of l and k but for our final experiments we used l = k = bn2 c, so Rxh and Rhh can each affect the entire hidden space. For our experiments we generated the planes to rotate in for each of Rhh and Rxh by initializing a random Gaussian matrix and projecting to an orthogonal matrix, and taking consecutive pairs of columns as the orthonormal vectors that span a rotation plane (w0 and w1 for each plane in the notation of Section 3.1).\nThere are several possible choices for the parameterization of the angle parameters. In our exploratory experiments we investigated directly generating the angle value as a parameter but this suffers from topology wrapping: θ and θ +m2π for any integer m generate the same transformation. We settled on generating θi = 2πσ(αi) for real parameters αi, where σ is the sigmoid function and φi(xt) = πσ(Wxt + b) for learned affine transform Wxt + b. This only allows positive angle rotations for the Rxh and it places negative and positive angle rotations on the opposite side of the parameter space for Rhh (negative αi produces positive angle rotations and positive αi produce negative angle rotations). In addition, with this parameterization, σ(αi) and σ(Wxt + b) logarithmically approach 0 for very negative αi and Wxt + b, which is useful since for long timescales the transformations may be applied exponentially many times. In our experiments, αi were drawn uniformly between −3 and 0, W was initialized with a Gaussian, and b was initialized as 0. The Rotation Plane representation allows a fixed transformation subspace via the unvarying planes of rotation, but with a small number of angle parameters that are actively modulated to produce vastly different transformations for different inputs. Other orthogonal matrix representations do not give as much utility to be modulated by a small number of parameters that can be then generated by the input."
    }, {
      "heading" : "4 BACKGROUND: ALTERNATIVE ORTHOGONAL MATRIX REPRESENTATIONS AND OPTIMIZATION",
      "text" : ""
    }, {
      "heading" : "4.1 UNITARY REPRESENTATION USED IN URNN",
      "text" : "The approach in Arjovsky et al. (2015) parameterizes their unitary matrices as a combination of unitary building blocks, such as complex reflections, complex unit diagonal scalings, permutations, and the FFT and iFFT."
    }, {
      "heading" : "4.2 OPTIMIZATION REPRESENTATION ON ORTHOGONAL MATRICES",
      "text" : "Another way to parameterize an orthogonal matrix is to start with an arbitrary orthogonal matrix and then ensure that the optimization process produces an orthogonal matrix at each time step.\nSeveral ways of optimizing orthogonal matrices. The defining equation can be used as a regularizer on a regular matrix transition, i.e. ||I − QTQ||. This approach is used in some experiments in the ORNN paper, but it does not produce an exactly orthogonal matrix, and as such still suffers from exploding and vanishing gradients. Another possibility is to perform gradient descent on a matrix and reproject to the orthogonal matrices by SVD and setting the singular values to 1. The Lie algebra representation is Qt+1 = Qtq when q = exp(A) if A is skew-symmetric: AT +A = 0.\nThen the representation optimizes w in skew symmetric matrices. This approach is used in Hyland & Rätsch (2016). The main downside of this approach is that you need to calculate gradient through exp, the matrix exponential, which we are not aware of a closed-form solution, and in Hyland & Rätsch (2016) the authors used finite differences to calculate the derivative of the exponential, which is incompatible with backpropagation. Recently proposed full capacity uRNN uses the Cayley transform: Qt+1 = (I + λ2A)\n−1(I − λ2A)Qt for A skew symmetric (skew-Hermitian for unitary Q) to stay on the manifold of orthogonal (unitary) matrices."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "Our experiments seek to answer the following questions:\nHow do the theoretically guaranteed preservation properties of the DORNN architecture contribute to practical training to learn extremely long term dependencies, and how is training time to success influenced by the length of a long term dependency.\nWe partially address these questions on our RP-DORNN architecture on a simplified 1-bit version of the memory copy task that is commonly used to test long term dependencies in RNN architectures. The task is as follows:\nThe input is a sequence of four dimension one-hot vectors (categorical variables with four categories) of length T +2 for a given T > 1. The first input is either a 1 or 2, followed by a string of T 0’s, and finally ends with a 3. All of the outputs except for the last output are arbitrary and unpenalized. The last output should be the same category as the first input (1 or 2, whichever was given). The loss is the categorical cross entropy of the last output timestep.\nThe simplifications from the usual memory copy experiment are that the number of categories is four instead of the usual ten, that the sequence to be copied is of length one instead of the usual ten, and that the loss only takes into account the portion of the output that corresponds to the copied output (the final timestep in our case), instead of the entire output sequence. These simplifications were performed in order to increase the number of experiments that could be run by training smaller models, and to function as a minimal experiment on the effect of increasing the sequence length. The third simplification was performed in order to increase the signal to noise ratio in the loss for very long sequences, since the majority of the loss comes from the intermediate outputs for very large T . The model would otherwise learn to use most of its capacity to reduce error in the irrelevant intermediate output stage instead of the relevant copied sequence stage. A useful side effect is that the entire gradient signal comes from the relevant copy output error.\nWe successfully trained the architecture on the task with T = 600, 800, 1600, 2400, 3200, and 5000, with a minibatch size of 128. There are only two possible input and output sequences, so the minibatch size is somewhat redundant, but the different proportion of 1 and 2 inputs at each timestep helps inject extra randomness to the training procedure. Figure 2 shows the training loss curves for each of the timesteps, compared against the baseline of outputting 1 or 2 with equal probability on the final timestep, which achieves ln(2) ≈ 0.693 loss. Figure 3 shows the accuracy of the network over training applied on the input minibatches. None of the hyperparameters were changed for scaling the timestep from 500 to 5000.\nNote that for longer T , the training became increasingly unstable (although it was still able to succeed). A common phenomenon is that the loss would jump far above the baseline for a brief period but then return to a lower loss than before the jump. A possible explanation is that the training process becomes increasingly sensitive to small transition parameter changes as T increases, since the transition is applied T times, and so the final hidden state would land in a completely different region after a small parameter change and the network would have to adjust to the new regions.\nTraining was terminated for each T after perfect accuracy had been achieved for several hundred training iterations, but the loss would continue to asymptotically approach 0 if training were to progress as the network increases confidence in its predictions.\nSaxe et al. (2013) suggests that training time should be independent of sequence length for a fully linear orthogonal neural network. We observed a roughly linear training time to success with respect to T . The discrepancy might be due to the nonlinearities present elsewhere in the network or due to the parameterization of the orthogonal matrices.\nThis experiments section will be expanded upon as more experimental results and comparisons are available."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "While we describe one particular orthogonal recurrent transition, the rotation plane parameterization describes a rich space of orthogonal transitions. Our input to hidden dependent transition is expressive enough for simplified memory copy task and can learn extremely long term dependencies up to 5,000 time steps long. The architecture still needs to be validated on more complicated tasks. Future work:\nThe architecture we described here is linear with the transition dependent on the input, but nonlinear locally orthogonal transitions are possible discontinuous. We experimented with different locally orthogonal transitions but found them harder to train successfully (likely they are more unstable due to the discontinuity, especially amplified over the hundreds or thousands of timesteps). It might be possible to find a nonlinear transition that still produces an orthogonal Jacobian that is continuous: the equations don’t outright prevent it, but it results from the interplay of several constraints and so finding such an architecture is harder. An open question is whether is it possible to construct something like an LSTM within the space of orthogonal or unitary matrices, to get the best of both worlds? It is not clear how much expressivity in the recurrent transition is lost by having linear input dependent model instead of a nonlinear hidden to hidden transition.\nA possible future direction is the combinations of orthogonal or unitary transitions for long term dependencies and regular RNN or LSTM units for complicated short term dependencies.\nIn this work we randomly fixed the planes to rotate in, but using the Cayley transform as in the full capacity uRNN the rotation planes could be optimized jointly, and so we could combine with our angle based parameterization to get full capacity input dependent rotations instead of being sensitive to the randomly initialized planes."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This research was funded in part by Darpa through the SIMPlEX program and the FunLOL program, by NSF through a Career award. Zoe McCarthy is also supported by an NSF Graduate Fellowship. Peter Chen is also supported by a Berkeley AI Research (BAIR) lab Fellowship."
    } ],
    "references" : [ {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Martin Arjovsky", "Amar Shah", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06464,",
      "citeRegEx" : "Arjovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bengio.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 1994
    }, {
      "title" : "On the Properties of Neural Machine Translation: EncoderDecoder Approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent batch normalization",
      "author" : [ "Tim Cooijmans", "Nicolas Ballas", "César Laurent", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1603.09025,",
      "citeRegEx" : "Cooijmans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cooijmans et al\\.",
      "year" : 2016
    }, {
      "title" : "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
      "author" : [ "Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell", "U T Austin", "Umass Lowell", "U C Berkeley" ],
      "venue" : "Cvpr, pp",
      "citeRegEx" : "Donahue et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Geometric Algebra for Computer Science (Revised Edition): An Object-Oriented Approach to Geometry",
      "author" : [ "Leo Dorst", "Daniel Fontijne", "Stephen Mann" ],
      "venue" : "URL http: //www.amazon.de/dp/0123749425",
      "citeRegEx" : "Dorst et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dorst et al\\.",
      "year" : 2009
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Orthogonal RNNs and Long-Memory Tasks, 2016",
      "author" : [ "Mikael Henaff", "Arthur Szlam", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Henaff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen netzen",
      "author" : [ "S. Hochreiter" ],
      "venue" : null,
      "citeRegEx" : "Hochreiter.,? \\Q1991\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "S Hochreiter", "Jürgen Schmidhuber", "J Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning",
      "author" : [ "Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jrgen Schmidhuber" ],
      "venue" : "long-term dependencies,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "Learning Unitary Operators with Help From u(n). jul 2016",
      "author" : [ "Stephanie L. Hyland", "Gunnar Rätsch" ],
      "venue" : "URL http://arxiv.org/abs/1607.04903",
      "citeRegEx" : "Hyland and Rätsch.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hyland and Rätsch.",
      "year" : 2016
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Fei Fei Li" ],
      "venue" : "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Karpathy and Li.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy and Li.",
      "year" : 2015
    }, {
      "title" : "Regularizing RNNs by Stabilizing Activations",
      "author" : [ "David Krueger", "Roland Memisevic" ],
      "venue" : null,
      "citeRegEx" : "Krueger and Memisevic.,? \\Q2015\\E",
      "shortCiteRegEx" : "Krueger and Memisevic.",
      "year" : 2015
    }, {
      "title" : "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-End Training of Deep Visuomotor Policies",
      "author" : [ "Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Levine et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2016
    }, {
      "title" : "Addressing the Rare Word Problem in Neural Machine Translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : "Arxiv, pp",
      "citeRegEx" : "Luong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2014
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M. Saxe", "James L. McClelland", "Surya Ganguli" ],
      "venue" : null,
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Recurrent Neural Networks (RNNs) are powerful sequence modeling tools that have found successful applications in speech recognition, natural language processing, image captioning, and many more (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014).",
      "startOffset" : 194,
      "endOffset" : 321
    }, {
      "referenceID" : 1,
      "context" : "Recurrent Neural Networks (RNNs) are powerful sequence modeling tools that have found successful applications in speech recognition, natural language processing, image captioning, and many more (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014).",
      "startOffset" : 194,
      "endOffset" : 321
    }, {
      "referenceID" : 5,
      "context" : "Recurrent Neural Networks (RNNs) are powerful sequence modeling tools that have found successful applications in speech recognition, natural language processing, image captioning, and many more (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014).",
      "startOffset" : 194,
      "endOffset" : 321
    }, {
      "referenceID" : 17,
      "context" : "Recurrent Neural Networks (RNNs) are powerful sequence modeling tools that have found successful applications in speech recognition, natural language processing, image captioning, and many more (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014).",
      "startOffset" : 194,
      "endOffset" : 321
    }, {
      "referenceID" : 9,
      "context" : "One fundamental problem with RNNs is the so called vanishing and exploding gradients problem (Hochreiter, 1991; Bengio, 1994; Hochreiter et al., 2001).",
      "startOffset" : 93,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "One fundamental problem with RNNs is the so called vanishing and exploding gradients problem (Hochreiter, 1991; Bengio, 1994; Hochreiter et al., 2001).",
      "startOffset" : 93,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "One fundamental problem with RNNs is the so called vanishing and exploding gradients problem (Hochreiter, 1991; Bengio, 1994; Hochreiter et al., 2001).",
      "startOffset" : 93,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "The IRNN model modifies the standard RNN transition to initialize at the identity, which increases the timestep length modeling capability (Le et al., 2015).",
      "startOffset" : 139,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "Stabilizing the forward hidden state norm can have a positive effect on hidden state gradient norm preservation (Krueger & Memisevic, 2015; Ba et al., 2016; Cooijmans et al., 2016).",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "A simple gradient norm clipping during hidden state backpropagation can also help to alleviate the exploding gradient problem for these architectures (Graves, 2013).",
      "startOffset" : 150,
      "endOffset" : 164
    }, {
      "referenceID" : 18,
      "context" : "Theory developed for the linear neural networks suggests that training time can be vastly shorter when the weight matrices are orthogonal, and even independent of depth (Saxe et al., 2013).",
      "startOffset" : 169,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014). One fundamental problem with RNNs is the so called vanishing and exploding gradients problem (Hochreiter, 1991; Bengio, 1994; Hochreiter et al., 2001). These problems occur when training RNNs using gradient descent to model long sequences where the gradient magnitude either goes towards zero or infinity, respectively, as the length of the sequence increases. Several heuristics have been proposed to alleviate these problems. The Long Short Term Memory (LSTM) in Hochreiter et al. (1997) and Gated Recurrent Units (GRU) in Cho et al.",
      "startOffset" : 8,
      "endOffset" : 602
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Bahdanau et al., 2014; Wu et al., 2016; Donahue et al., 2015; Karpathy & Li, 2015; Luong et al., 2014). One fundamental problem with RNNs is the so called vanishing and exploding gradients problem (Hochreiter, 1991; Bengio, 1994; Hochreiter et al., 2001). These problems occur when training RNNs using gradient descent to model long sequences where the gradient magnitude either goes towards zero or infinity, respectively, as the length of the sequence increases. Several heuristics have been proposed to alleviate these problems. The Long Short Term Memory (LSTM) in Hochreiter et al. (1997) and Gated Recurrent Units (GRU) in Cho et al. (2014) have been incredibly successful recurrent transition architectures to model complicated dependencies for sequences up to several hundred timesteps long and are the main RNN architectures in use today.",
      "startOffset" : 8,
      "endOffset" : 655
    }, {
      "referenceID" : 0,
      "context" : "vanishing gradient problem (Arjovsky et al., 2015).",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "vanishing gradient problem (Arjovsky et al., 2015). A very recent extension expands on this work to increase the expressive power of these transitions and increases the validated architecture up to 2000 timestep dependencies in Wisdom et al. (2016). Analytic solutions to the most common long term dependency example tasks, the memory copy and addition problems, are provided with linear orthogonal recurrent neural networks in Henaff et al.",
      "startOffset" : 28,
      "endOffset" : 249
    }, {
      "referenceID" : 0,
      "context" : "vanishing gradient problem (Arjovsky et al., 2015). A very recent extension expands on this work to increase the expressive power of these transitions and increases the validated architecture up to 2000 timestep dependencies in Wisdom et al. (2016). Analytic solutions to the most common long term dependency example tasks, the memory copy and addition problems, are provided with linear orthogonal recurrent neural networks in Henaff et al. (2016). A nonlinear activation function that is locally orthogonal is proposed in Chernodub & Nowicki and shown to have great potential.",
      "startOffset" : 28,
      "endOffset" : 449
    }, {
      "referenceID" : 16,
      "context" : "Linear time-varying models can express very complicated policies, such as in Levine et al. (2016). Our model can be viewed as a linear orthogonal time-varying model where the variation due to time is due to different input signals applied.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "Work in Geometric Algebra (for example see chapter 6 of Dorst et al. (2009)) gives us further insight into this parameterization.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "The approach in Arjovsky et al. (2015) parameterizes their unitary matrices as a combination of unitary building blocks, such as complex reflections, complex unit diagonal scalings, permutations, and the FFT and iFFT.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "Saxe et al. (2013) suggests that training time should be independent of sequence length for a fully linear orthogonal neural network.",
      "startOffset" : 0,
      "endOffset" : 19
    } ],
    "year" : 2016,
    "abstractText" : "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem. The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hidden state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients. We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.",
    "creator" : "LaTeX with hyperref package"
  }
}