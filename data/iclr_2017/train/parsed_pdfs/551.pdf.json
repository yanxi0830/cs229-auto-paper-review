{
  "name" : "551.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "rico.jonschkowski@tu-berlin.de", "oliver.brock@tu-berlin.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nTraditionally, computer scientists solve problems by designing algorithms. Recently, this practice has received competition from machine learning methods that automatically extract solutions from data. One example of this development is the field of computer vision, where the state of the art is based on deep neural networks rather than on human-designed algorithms (He et al., 2015). But these two approaches to problem solving—algorithms and learning—are not mutually exclusive; in fact, they can complement each other. Effective problem solving exploits all available information, whether it be encoded in algorithms or captured by data. This paper presents a step towards tightly combining these sources of information.\nWe demonstrate the combination of problem-specific algorithms with generic machine learning in the context of state estimation in robotics. The state estimation problem exhibits a clear algorithmic structure, captured in a provably optimal way by Bayes filters (Thrun et al., 2005). But the use of such a filter requires the specification of a motion model and a measurement model that is specific to a particular problem instance. We want to leverage the general knowledge captured in the Bayes filter, while extracting the instance-specific models from data using deep learning (Goodfellow et al., 2016). We achieve this by implementing a differentiable version of the histogram filter—a specific type of Bayes filter that represents probability distributions with histograms—including learnable motion and measurement models (see Fig. 1). With this implementation, we can learn these models end-to-end using backpropagation, while still taking advantage of the structure encoded in Bayes filters. Interestingly, this combination also enables unsupervised learning.\nOur contributions are both conceptual and technical. Our conceptual contribution is the principle of tightly combining algorithms and machine learning to balance data-efficiency and generality. Our technical contribution is the end-to-end learnable histogram filter, which enables the use of this Bayes filter variant in a more generic way. Our experiments show that our method is more data-efficient than generic neural networks, improves performance compared to standard histogram filters, and—most importantly—enables unsupervised learning of recursive state estimation loops."
    }, {
      "heading" : "2 COMBINING ALGORITHMS AND MACHINE LEARNING",
      "text" : "Every information that is contained in the solution to a problem must either be provided as prior knowledge (prior for short) or learned from data. Different approaches balance these sources of information differently. In the classic approach to computer science, all required information is provided by a human (e.g. in the form of algorithms and models). In the machine learning approach, only a minimal amount of prior knowledge is provided (in form of a learning algorithm) while most information is extracted from data (see Fig. 2). When trading-off how\nmuch and which information should be provided as a prior or emerge from data, we should consider the entire spectrum rather than limit ourselves to these two end points.\nIn the context of robotics, for example, it is clear that the left end of this spectrum will not enable intelligent robots, because we cannot foresee and specify every detail for solving a wide range of tasks in initially unknown environments. Robots need to collect data and learn from them. But if we go all the way to the right end of the spectrum, we need large amounts of data, which is very difficult to obtain in robotics where data collection is slow and costly. Luckily, robotic tasks include rich structure that can be used as prior. Physics, for example, governs the interaction of any robot and its environment and physics-based priors can substantially improve learning (Scholz et al., 2014; Jonschkowski & Brock, 2015). But robotic tasks include additional structure that can be exploited.\nEvery algorithm that has proven successful in robotics implicitly encodes information about the structure of robotic tasks. We propose to use this robotics-specific information captured by robotic algorithms and combine it with machine learning to fill in the task-specific details based on data. By tightly combining algorithms and machine learning, we can strike the right balance between generality and data-efficiency."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "Algorithms and machine learning can be combined in different ways, using algorithms either 1) as fixed parts of solutions, 2) as parts of the learning process, or 3) as both. The first approach learns task-specific models in isolation and then combines them with algorithms in the solution. Examples for this approach are numerous, e.g. a Go player that applies a planning algorithm on learned models (Silver et al., 2016), a perception pipeline that combines the iterative closest point algorithm with learned object segmentation (Zeng et al., 2016), or robot control based on learned motion models (Nguyen-Tuong & Peters, 2011).\nThe second approach uses algorithms as teachers to generate training data. With this data, we can learn a function that generalizes beyond the capabilities of the original algorithm or that can be finetuned to a specific problem instance. For example, self-play in Go (using the algorithm as part of the solution) can be used to create new samples to learn from (Silver et al., 2016), training data for learning segmentation can be generated by simple algorithms such as background subtraction (Zeng et al., 2016), and reinforcement learning problems can be solved using training samples generated via trajectory optimization (Levine & Koltun, 2013).\nThe third approach—the one that we are focusing on in this paper—uses the same algorithms in the learning process and in the solution. The main idea is to optimize the models for the algorithms that use them rather than learning them in isolation. To achieve this, the algorithms need to be differentiable, such that we can compute how changes in the model affect the output of the algorithm, which allows to train the models end-to-end. This idea has been applied to different algorithms, e.g. in the form of neural Turing machines (Graves et al., 2014) and neural programmer-interpreters (Reed & de Freitas, 2015). In the context of robotics, Tamar et al. (2016) have presented a differentiable planning algorithm based on value iteration. And, most directly related to our work, Haarnoja et al. (2016) have applied this idea to Kalman filters, showing that measurement models based on visual input can be learned end-to-end as part of the filter. Our work differs from this by representing the belief with a histogram rather than a Gaussian, which allows to track multiple hypotheses—a neces-\nsity for many robotic tasks. Furthermore, we focus on tasks where the robot has information about its actions and learn both the measurement model and the motion model jointly. Our paper extends an earlier workshop submission (Jonschkowski & Brock, 2016)."
    }, {
      "heading" : "4 PRELIMINARIES: HISTOGRAM FILTERS AND OTHER BAYES FILTERS",
      "text" : "A Bayes filter (Thrun et al., 2005) is an algorithm to recursively estimate a probability distribution over a latent state s (e.g. robot pose) conditioned on the history of observations o (e.g. camera images) and actions a (e.g. velocity commands). This posterior over states is also called belief, Bel(st) = p(st|a1:t−1, o1:t). A histogram filter is a type of Bayes filter that represents the belief as a histogram; a discretization of the state space with one probability value per discrete state s. One of the key assumptions in Bayes filters is the Markov property of states, from which follows that the current belief Bel(st) summarizes all information of the entire history of observations and actions that is relevant for predicting the future.\nOther key assumptions determine how the belief is recursively updated using two alternating steps: the prediction step based on the last action at−1 and the measurement update step based on the current measurement ot. Note that these two sources of information are separated, which results from the assumption of conditional independence of observation and action given the state.\nThe prediction step assumes actions to change the state according to the known motion model p(st | st−1, at−1). After performing an action at−1, the new belief for a given state st is computed by summing over all possible ways through which state st could have come about,\nBel(st) = ∑ st−1 p(st | st−1, at−1)Bel(st−1). (1)\nThe measurement update step assumes observations to only depend on the current state as defined by a known measurement model p(ot | st). After receiving an observation ot, the belief for every state st is updated using Bayes’ rule,\nBel(st) ∝ p(ot | st)Bel(st). (2)\nIf motion model and measurement model are unknown, we want the robot to learn these models from data. Apart from the assumptions already mentioned, learning explicit models allow us to restrict their hypothesis space according to assumptions (e.g. linear motion). Our goal is to train these models end-to-end such that we find the models that optimize state estimation performance, while preserving the useful assumptions of Bayes filters. Towards this end, we formulate the belief, the prediction, the measurement update, and the corresponding models in the deep learning framework."
    }, {
      "heading" : "5 END-TO-END LEARNABLE HISTOGRAM FILTERS",
      "text" : "An end-to-end learnable histogram filter (E2E-HF) is a differentiable implementation of a histogram filter that allows both motion model and measurement model to be learned end-to-end by backpropagation through time (Werbos, 1990). Alternatively, we can view the E2E-HF as a new recurrent neural network architecture that implements the structure of a histogram filter (see Fig. 3)."
    }, {
      "heading" : "5.1 END-TO-END LEARNING AND DIFFERENTIABILITY",
      "text" : "If we want to use the structure of a histogram filter as a prior and fit the measurement model and the motion model to data, we can essentially do one of two things: a) learn the models in isolation to optimize a quality measure of the model or b) learn the models end-to-end, i.e. train the models as part of the entire system and optimize the end-to-end performance.\nIn either way, we might want to optimize the models using gradient descent, for example by computing the gradient of the learning objective with respect to the model parameters using backpropagation (repeated application of the chain rule). Therefore, the motion model and the measurement model need to be differentiable regardless of whether we choose option a) or option b). For b) endto-end learning, we need to backpropagate the gradient through the histogram filter algorithm (not to\nchange the algorithm but to compute how to change the models to improve the algorithm’s output). Therefore, in addition to the models, the algorithm itself needs to be differentiable.\nThe remainder of this section describes how histogram filters can be implemented in a differentiable way and how they can be learned in isolation or end-to-end. To comply with the deep learning framework, we will define the E2E-HF using vector and matrix operations. We will also introduce additional priors for computational or data efficiency. For the sake of readability, we assume a one-dimensional state space here. All formulas can easily be adapted to higher dimensions.\n5.2 BELIEF\nThe histogram over states is implemented as a vector b of probabilities with one entry per bin,\nbt = [Bel(St = 1), Bel(St = 2), . . . , Bel(St = |S|)].\nWe can also think of the belief as a neural network layer where the activation of each unit represents the value of a histogram bin. The belief bt constitutes the output of the histogram filter at the current step t and an input at the next step t+ 1—together with an action at and an observation ot+1 (see Fig. 3)."
    }, {
      "heading" : "5.3 PREDICTION (MOTION UPDATE)",
      "text" : "The most direct implementation of the prediction step (which we replace shortly) defines a learnable function f for the motion model, f : st, st−1, at−1 7→ p(st | st−1, at−1), and employs f in the prediction step (Eq. 1). The equation can be vectorized for computational efficiency by defining a |S| × |S| matrix F with Fi,j(a) = f(i, j, a), such that bt = F (at−1)bt−1.\nHowever, this approach is computationally expensive because it requires |S|2 evaluations of f for a single prediction step. We can make this computation more efficient, if we assume robot motion to be local and consistent across the state space, i.e.\np(st | st−1, at−1) = p(∆st | at−1), ∀t|∆st| ≤ k,\nwhere ∆st = st−st−1 and k is the maximum state change. Accordingly, we define a new learnable function for the motion model, g : ∆st, at−1 7→ p(∆st | at−1) and use g instead of f . For vectorization, we define a (2k+1)-dimensional vector g(a), whose elements gi(a) = g(i−k−1, a) represent the probabilities of all positive and negative state changes up to k. We can now reformulate the prediction step (Eq. 1) as a convolution (∗),\nbt = bt−1 ∗ g(at−1),\nwhere the belief bt−1 is convolved with the motion kernel g(at−1) for action at−1 (see Fig. 3)."
    }, {
      "heading" : "5.3.1 MOTION MODEL",
      "text" : "The learnable motion model g can be implemented as any feedforward network that maps ∆s and a to a probability. The prior that g(a) represents a probability mass function, i.e. that the elements of g(a) should be positive and sum to one, can be enforced using the softmax nonlinearity on the vector of unnormalized network outputs g̃(a), such that gi(a) = e\ng̃i(a)∑ j e g̃j(a) .\nAnother useful prior for g is smoothness with respect to ∆s and a, i.e. that similar combinations of ∆s and a lead to similar probabilities. This smoothness is the reason why (for standard feedforward networks), we should use ∆s as an input rather than as index for different output dimensions. With additional knowledge about robot motion, we can replace smoothnes by a stronger prior. For the\nexperiments in this paper, we assumed linear motion with zero mean Gaussian noise, and therefore defined the motion model with only two learnable parameters α and σ,\ng̃(∆s, a) = e− (∆s−αa)2 σ2 ,\nand the obligatory normalization, g(∆s, a) = g̃(∆s,a)∑k j=−k g̃(j,a) ."
    }, {
      "heading" : "5.4 MEASUREMENT UPDATE",
      "text" : "Analogously to the motion model in the prediction step, we define a learnable function h that represents the measurement model for the measurement update, h : st, ot 7→ p(ot | st). To vectorize the update equation (Eq. 2), we define a vector h(o) with elements hi(o) = h(i, o), such that the measurement update corresponds to element-wise multiplication ( ) with this vector,\nb̃t = h(o) bt,\nfollowed by a normalization, bt = b̃t∑ j b̃t,j (see Fig. 3)."
    }, {
      "heading" : "5.4.1 MEASUREMENT MODEL",
      "text" : "The learnable function h that represents the measurement model can again be implemented by any feedforward network. Since h corresponds to p(ot | st)—a probability distribution over observations—it needs to be normalized across observations, not across states. To realize the correct normalization, we need to compute the unnormalized likelihood vector h̃(o) for every observation o and compute the softmax over the corresponding scalars in different vectors rather than over the scalars of the same vector: h(o) = e\nh̃(o)∑ o′ e\nh̃(o′) . If the observations are continuous instead of discrete, this summation must be approximated using sampled observations.\nFor the experiments in this paper, we represented h by a network with three hidden layers of 32 rectified linear units (Nair & Hinton, 2010), followed by a linear function and a normalization as described above. Using the observation and state as input rather than output dimensions again incorporates the smoothness prior on these quantities."
    }, {
      "heading" : "5.5 LEARNING",
      "text" : "We can learn the motion model g and the measurement model h using different learning objectives based on different sequences of data. We will first look at a number of supervised learning objectives that require o1:T , a1:T , s1:T , and sometimes x1:T —the underlying continuous state. Then, we will describe unsupervised learning that only needs o1:T and a1:T ."
    }, {
      "heading" : "5.5.1 SUPERVISED LEARNING IN ISOLATION",
      "text" : "Both models can be learned in isolation by optimizing an objective function, e.g. the cross-entropy between experienced state change / observation and the corresponding outputs of g and h,\nLg = − 1\nT − 1 T∑ t=2 e(∆st−k−1) log(g(at−1)),\nLh = − 1\nT T∑ t=1 e(ot) log(h(ot)),\nwhere e(i) denotes a standard basis vector with all zeros except for a one at position i, that is the position that represents the experienced state change or observation."
    }, {
      "heading" : "5.5.2 SUPERVISED END-TO-END LEARNING",
      "text" : "Due to our differentiable implementation, the models can also be learned end-to-end using backpropagation through time (Werbos, 1990), which we apply on several overlapping subsequences\nof length C (in our experiments, C = 32). In the corresponding learning objectives, we compare the belief at the final time step of this subsequence with the true state. If we want to optimize the accuracy of the filter with respect to its discrete states, we can again use a cross-entropy loss,\nLacc. = − 1\nT − C T∑ t=C+1 e(st) log(b (t−C:t) t ),\nwhere b(t−C:t)t denotes the final belief at time step t when the histogram filter is applied on the subsequence that spans steps t− C to t. Alternatively, we might want to optimize other objectives, e.g. the mean square error with respect to the underlying continuous state,\nLmse = − 1\nT − C T∑ t=C+1 (xt − xb(t−C:t)t )2,\nwhere x denotes a vector of the continuous values to which the discrete states correspond, such that xb (t−C:t) t is the weighted average of these values according to the final belief in this subsequence."
    }, {
      "heading" : "5.5.3 UNSUPERVISED END-TO-END LEARNING",
      "text" : "By exploiting the structure of the histogram filter algorithm and the differentiability, we can even train the models without any state labels by predicting future observations, but later use the models for state estimation. Similarly to supervised end-to-end learning, we apply the filter on different subsequences of length C, but then we follow this with D steps without performing the measurement update (in our experiments, D = 32). Instead, we use the measurement model to predict the observations. Pred(ot) = ∑ st p(ot | st)Bel(st) = h(ot)bt. To predict the probabilities for all observations, we define a matrix H with elements Hi,j = h(i, j) as defined in Section 5.4. Putting everything together, we get the following loss for unsupervised end-to-end learning:\nLunsup. = − 1\n(T − C)D T∑ t=C+1 D∑ d=1 e(ot+d) log(H>b (t−C:t+d) t+d )."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "We consider the problem of learning to estimate the robot’s state in unknown environments with partial observations. In this problem, we compare histogram filters for which the models are learned in isolation (HF), end-to-end learnable histogram filters (E2E-HFs), and two-layer long-short-term memory networks (LSTMs, Hochreiter & Schmidhuber, 1997). The models of the HFs are learned by optimizing the loss functions Lg and Lh presented in the previous section. For the E2E-HFs and LSTMs, we compare end-to-end learning using Lacc., Lmse, and Lunsup..\nOur results show that 1) the algorithmic prior in HFs and E2E-HFs increases data efficiency for learning localization compared to generic LSTMs, 2) end-to-end learning improves the performance of E2E-HFs compared to HFs, and 3) E2E-HFs are able to learn state estimation without state labels."
    }, {
      "heading" : "6.1 PROBLEM: LEARNING RECURSIVE STATE ESTIMATION IN UNKNOWN ENVIRONMENTS",
      "text" : "An important state estimation problem in partially observable environments is localization: a robot moves through an environment by performing actions and receives partial observations, such that it needs to filter this information over time to estimate its state, i.e. its position. In our experiments, the robot does not know the environment beforehand and thus has to learn state estimation from data.\nWe performed experiments in two localization tasks: a) a hallway localization task (Thrun et al., 2005) and b) a drone localization task (see Fig. 4). The tasks are similar in that they have continuous actions and binary observations (door/wall and purple/white tile), both of which are subject to 10% random error. The tasks differ in their dimensionality. In the hallway task, the robot only needs to estimate a one-dimensional state (its position along the hallway), which for all methods is discretized into 100 states. The drone localization task has a two-dimensional state, which is discretized into 50 bins per dimension resulting in 2500 bins in total. The challenge in both tasks is that the door/tile\nlocations, the scale of the actions, and the amount of random noise are unknown and need to be learned from data, i.e. a sequence of observations, actions, and—in the supervised setting—states produced by the robot moving randomly through the environment. More details about the tasks, the experimental setting, learning parameters, etc. can be found in Appendix A."
    }, {
      "heading" : "6.2 RESULTS: IMPROVED DATA-EFFICIENCY",
      "text" : "Hallway task: We performed multiple experiments in the hallway localization task with different amounts of training data. The learning curves with respect to mean squared error for supervised learning show large differences in data efficiency (see solid lines in Fig. 5a): E2E-HFs require substantially less training samples than LSTMs to achieve good performance (2000 rather than > 8000). HFs are even more data-efficient but quickly stop improving with additional data.\nDrone task: For the drone localization task, we performed an experiment using 4000 training steps (see Table 1). Our results show that this data is sufficient for the E2E-HF (but not for the LSTM) to achieve good performance. Our method only required a similar amount of data as for the 1D hallway task, even though the histogram size had increased from 100 to 2500 bins.\nDiscussion: The priors encoded in the E2E-HF improve data efficiency because any information contained in these priors does not need to be extracted from data. This leads to better generalization, e.g. the ability to robustly and accurately track multiple hypotheses (see Fig.6).\nNote on computational limits: The size of the histogram is exponential in the number of state dimensions. A comparison between the 1D and the 2D task suggests that data might not be the bottleneck for applying the method to higher dimensional problems, since the data requirements were similar. However, the increased histogram size directly translates into longer training times, such that computation quickly becomes the bottleneck for scaling this method to higher-dimensional problems. Addressing this problem will require to change the belief representation, e.g. to particles or a mixture of Gaussians, which is an important direction for future work."
    }, {
      "heading" : "6.3 RESULTS: OPTIMIZATION OF END-TO-END PERFORMANCE",
      "text" : "Hallway task: While HFs excel with very few data, E2E-HFs surpass them if more than 2000 training samples are available (see gray and yellow lines in Fig. 5a). For the mean squared error metric, the best method is the E2E-HF with a mean squared error objective (yellow line). However, if we care about a different metric, e.g. accuracy of estimating the discrete state, the methods rank differently (see Fig. 5b). The best method for the previous metric (yellow line) is outperformed by HFs (gray line) and even more so by E2E-HFs that are optimized for accuracy (teal line). For yet another metric, i.e. accuracy of predicting future observations, HFs outperform both other approaches but are equal to E2E-HFs optimized for predicting future observations (see Fig. 5c).\nDrone task: The results of the drone localization task show the same pattern (see Table 1). The best method for every metric is the E2E-HF that optimizes this metric.\nDiscussion: E2E-HFs perform better than HFs because they optimize the models for the filtering process (with respect to the metric they were trained for) rather than optimizing model accuracy. This can be advantageous because “inaccurate” models can improve end-to-end performance (compare the HF model learned in isolation to the models learned end-to-end in Fig. 6a)."
    }, {
      "heading" : "6.4 RESULTS: ENABLING UNSUPERVISED LEARNING",
      "text" : "Hallway and drone tasks: In both tasks, unsupervised E2E-HFs were similar to HFs and better than all other methods for predicting future observations. Interestingly, they also had comparatively low mean squared error for state estimation even though they had never seen any state labels (see dashed green line in Fig. 5 and second line in Table 1). In fact, the qualitative results for both tasks show a remarkable similarity between the learned models and the estimated belief between HFs and unsupervised E2E-HFs (compare HF and E2E-HF (unsup.) in Fig. 6) and Fig. 7.\nDiscussion: E2E-HFs can learn state estimation purely based on observations and actions. By predicting future observations using the structure of the histogram filter algorithm, the method discovers a state representation that works well with this algorithm, which is surprisingly close to the “correct” models learned by HFs, although no state labels are used."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We proposed to tightly combine prior knowledge captured in algorithms with the ability to learn from data. We demonstrated the feasibility and the advantages of this idea in the context of state estimation in robotics. Algorithmic priors lead to data-efficient learning, as knowledge about the problem structure encoded in the algorithm is provided explicitly and does not have to be extracted from data. The ability to learn from data enables the use of algorithms when task-specifics are unknown. The tight combination of both improves performance as the models are optimized for use in the algorithm. Furthermore, the explicit algorithmic structure enables unsupervised learning. We view our results as a proof of concept and are convinced that the combination of algorithms and machine learning will help solve novel problems, while balancing data efficiency and generality."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We gratefully acknowledge the funding provided by the Alexander von Humboldt foundation and the Federal Ministry of Education and Research (BMBF)."
    }, {
      "heading" : "A ADDITIONAL EXPERIMENT DETAILS",
      "text" : "A.1 HALLWAY LOCALIZATION TASK\nThe hallway has a length of 10 meters, where every full meter is either occupied by a door or by a wall. At the beginning of every experiment trial, 5 doors are randomly arranged in the 10 spots in the hallway. The binary observation of the robot senses whether the center of the robot is next to a door or next to a wall. With probability 0.1, the observation returns the wrong information, e.g. “wall” instead of “door” if the robot is next to a door.\nThe robot is represented as a single point. It moves with a velocity between -1 and 1 meter per time step and stops when it reaches either end of the hallway. The action information that the robot receives is the step that it performed as measured by odometry. This odometry measurement is corrupted with zero mean Gaussian noise with standard deviation of 10% of it’s actual movement. Additionally, the odometry is scaled by a number between 0.5 and 5.0, which is randomly sampled at the beginning of every trial, i.e. the robot does not know its exact embodiment. This makes the exact motion model unknown, such that the robot needs to learn it from data.\nBoth during training and during testing the robot moves randomly, i.e. it randomly accelerates by a value between -0.5 and 0.5 at each time step. Apart from this acceleration, its velocity is affected by 10% friction at each time step and is set to zero when the robot reaches either end of the hallway. For each trial, the training data consists of a single random walk of the robot of length between 500 steps and 8000 steps. The data for the unsupervised learning, includes only the sequence of noisy observations and actions. For supervised learning, it additionally includes the groundtruth continuous and discrete state, i.e. the position of the robot.\nThe test data consisted of 1000 short time sequences of the robot moving in the same fashion starting from a random position. For all performance metrics, the belief was tracked for 32 steps. For the metric that measured observation prediction accuracy, the task was to predict 32 future observations given a sequence of 32 actions based on the current belief.\nA.2 DRONE LOCALIZATION TASK\nThe area for the drone localization task has a size of 5 times 5 meters, where every one meter tile is either purple or white. At the beginning of every experiment, the color of each tile is decided by a fair coin flip. Analogously to the hallway task, the binary observations inform the robot about the color of the tile which is directly underneath it. With probability of 0.1, this observation returns the wrong color.\nThe drone is represented as a single point in 2D space. It moves with velocities between -0.5 and 0.5 meter per time step and stops when it reaches the boundary of the area. The other aspects of its movement, the noisy odometry, and the movement generation for training and test data are analogous to the hallway localization task.\nA.3 EXPERIMENTAL DETAILS\nLSTM baseline: The LSTM baseline consists of two LSTM layers with 32 units per layer, followed by a softmax layer.\nTraining procedure: All methods where trained via minibatch stochastic gradient descent with batch size 32 using Adam (Kingma & Ba, 2014) with learning rate 0.001. The training length was determined using early stopping with patience, where 20% of the training data was used for validation. After 100 epochs without an improvement on the validation data, the parameters that achieved highest validation performance were returned.\nA.4 SOFTWARE\nWe used v-rep for simulation (E. Rohmer, 2013) and theano (Theano Development Team, 2016) with Lasagne (Dieleman et al., 2015) as deep learning framework for our implementation."
    } ],
    "references" : [ {
      "title" : "Lasagne: First release",
      "author" : [ "Sander Dieleman", "Jan Schlüter", "Colin Raffel", "Eben Olson", "Sren Kaae Snderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma" ],
      "venue" : null,
      "citeRegEx" : "Dieleman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dieleman et al\\.",
      "year" : 2015
    }, {
      "title" : "V-REP: a Versatile and Scalable Robot Simulation Framework",
      "author" : [ "M. Freese E. Rohmer", "S.P.N. Singh" ],
      "venue" : "In Proc. of The International Conference on Intelligent Robots and Systems (IROS),",
      "citeRegEx" : "Rohmer and Singh.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rohmer and Singh.",
      "year" : 2013
    }, {
      "title" : "Neural Turing Machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : null,
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Backprop KF: Learning Discriminative Deterministic State Estimators",
      "author" : [ "Tuomas Haarnoja", "Anurag Ajay", "Sergey Levine", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1605.07148,",
      "citeRegEx" : "Haarnoja et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Haarnoja et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "[cs],",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Learning state representations with robotic priors",
      "author" : [ "Rico Jonschkowski", "Oliver Brock" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Jonschkowski and Brock.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jonschkowski and Brock.",
      "year" : 2015
    }, {
      "title" : "Towards Combining Robotic Algorithms and Machine Learning: End-To-End Learnable Histogram Filters. In Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics 2016",
      "author" : [ "Rico Jonschkowski", "Oliver Brock" ],
      "venue" : null,
      "citeRegEx" : "Jonschkowski and Brock.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jonschkowski and Brock.",
      "year" : 2016
    }, {
      "title" : "Guided Policy Search",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Levine and Koltun.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levine and Koltun.",
      "year" : 2013
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Model learning for robot control: a survey",
      "author" : [ "Duy Nguyen-Tuong", "Jan Peters" ],
      "venue" : "Cognitive Processing,",
      "citeRegEx" : "Nguyen.Tuong and Peters.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nguyen.Tuong and Peters.",
      "year" : 2011
    }, {
      "title" : "Neural Programmer-Interpreters",
      "author" : [ "Scott Reed", "Nando de Freitas" ],
      "venue" : "[cs],",
      "citeRegEx" : "Reed and Freitas.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reed and Freitas.",
      "year" : 2015
    }, {
      "title" : "A Physics-Based Model Prior for Object-Oriented MDPs",
      "author" : [ "Jonathan Scholz", "Martin Levihn", "Charles L. Isbell", "David Wingate" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Scholz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Scholz et al\\.",
      "year" : 2014
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Value Iteration Networks",
      "author" : [ "Aviv Tamar", "Yi Wu", "Garrett Thomas", "Sergey Levine", "Pieter Abbeel" ],
      "venue" : null,
      "citeRegEx" : "Tamar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tamar et al\\.",
      "year" : 2016
    }, {
      "title" : "Probabilistic Robotics",
      "author" : [ "S. Thrun", "W. Burgard", "D. Fox" ],
      "venue" : null,
      "citeRegEx" : "Thrun et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Thrun et al\\.",
      "year" : 2005
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "Paul J. Werbos" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Werbos.,? \\Q1990\\E",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "Multi-view Self-supervised Deep Learning for 6d Pose Estimation in the Amazon Picking Challenge",
      "author" : [ "Andy Zeng", "Kuan-Ting Yu", "Shuran Song", "Daniel Suo", "Ed Walker Jr.", "Alberto Rodriguez", "Jianxiong Xiao" ],
      "venue" : "[cs],",
      "citeRegEx" : "Zeng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "One example of this development is the field of computer vision, where the state of the art is based on deep neural networks rather than on human-designed algorithms (He et al., 2015).",
      "startOffset" : 166,
      "endOffset" : 183
    }, {
      "referenceID" : 15,
      "context" : "The state estimation problem exhibits a clear algorithmic structure, captured in a provably optimal way by Bayes filters (Thrun et al., 2005).",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "Physics, for example, governs the interaction of any robot and its environment and physics-based priors can substantially improve learning (Scholz et al., 2014; Jonschkowski & Brock, 2015).",
      "startOffset" : 139,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "a Go player that applies a planning algorithm on learned models (Silver et al., 2016), a perception pipeline that combines the iterative closest point algorithm with learned object segmentation (Zeng et al.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : ", 2016), a perception pipeline that combines the iterative closest point algorithm with learned object segmentation (Zeng et al., 2016), or robot control based on learned motion models (Nguyen-Tuong & Peters, 2011).",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "For example, self-play in Go (using the algorithm as part of the solution) can be used to create new samples to learn from (Silver et al., 2016), training data for learning segmentation can be generated by simple algorithms such as background subtraction (Zeng et al.",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : ", 2016), training data for learning segmentation can be generated by simple algorithms such as background subtraction (Zeng et al., 2016), and reinforcement learning problems can be solved using training samples generated via trajectory optimization (Levine & Koltun, 2013).",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "in the form of neural Turing machines (Graves et al., 2014) and neural programmer-interpreters (Reed & de Freitas, 2015).",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "in the form of neural Turing machines (Graves et al., 2014) and neural programmer-interpreters (Reed & de Freitas, 2015). In the context of robotics, Tamar et al. (2016) have presented a differentiable planning algorithm based on value iteration.",
      "startOffset" : 39,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "in the form of neural Turing machines (Graves et al., 2014) and neural programmer-interpreters (Reed & de Freitas, 2015). In the context of robotics, Tamar et al. (2016) have presented a differentiable planning algorithm based on value iteration. And, most directly related to our work, Haarnoja et al. (2016) have applied this idea to Kalman filters, showing that measurement models based on visual input can be learned end-to-end as part of the filter.",
      "startOffset" : 39,
      "endOffset" : 310
    }, {
      "referenceID" : 15,
      "context" : "A Bayes filter (Thrun et al., 2005) is an algorithm to recursively estimate a probability distribution over a latent state s (e.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "An end-to-end learnable histogram filter (E2E-HF) is a differentiable implementation of a histogram filter that allows both motion model and measurement model to be learned end-to-end by backpropagation through time (Werbos, 1990).",
      "startOffset" : 216,
      "endOffset" : 230
    }, {
      "referenceID" : 16,
      "context" : "Due to our differentiable implementation, the models can also be learned end-to-end using backpropagation through time (Werbos, 1990), which we apply on several overlapping subsequences",
      "startOffset" : 119,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "We performed experiments in two localization tasks: a) a hallway localization task (Thrun et al., 2005) and b) a drone localization task (see Fig.",
      "startOffset" : 83,
      "endOffset" : 103
    } ],
    "year" : 2017,
    "abstractText" : "Problem-specific algorithms and generic machine learning approaches have complementary strengths and weaknesses, trading-off data efficiency and generality. To find the right balance between these, we propose to use problem-specific information encoded in algorithms together with the ability to learn details about the problem-instance from data. We demonstrate this approach in the context of state estimation in robotics, where we propose end-to-end learnable histogram filters— a differentiable implementation of histogram filters that encodes the structure of recursive state estimation using prediction and measurement update but allows the specific models to be learned end-to-end, i.e. in such a way that they optimize the performance of the filter, using either supervised or unsupervised learning.",
    "creator" : "LaTeX with hyperref package"
  }
}