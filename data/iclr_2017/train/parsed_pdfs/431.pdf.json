{
  "name" : "431.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PALEO: A PERFORMANCE MODEL FOR DEEP NEURAL NETWORKS",
    "authors" : [ "Hang Qi", "Evan R. Sparks", "Ameet Talwalkar" ],
    "emails" : [ "hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep learning has been successfully applied in many areas including natural language processing and computer vision. The scale of modern datasets and the millions to billions of parameters in these deep networks pose new challenges when designing computational systems that leverage parallel and distributed computing. Indeed, several important open questions remain:\n• How fast can we train or evaluate a model on a user’s given hardware? • For a given architecture, how can a user best leverage parallel and distributed computation? • How can we design a new neural network architecture that can be trained and evaluated efficiently\nunder common hardware setups?\nIn response to these fundamental questions, various software packages and systemshave been painstakingly developed, e.g. DistBelief (Dean et al., 2012), TensorFlow (Abadi et al., 2015), MXNet (Chen et al., 2015), SparkNet (Moritz et al., 2015), FireCaffe (Iandola et al., 2016). Moreover, expensive benchmarking efforts, e.g., Chintala et al. (2016), have performed brute-force profiling on some of these deep learning systems on a handful network architectures.\nIn this work we aim to tackle these questions by taking an analytical approach to model the performance of arbitrary learning systems. Our work hinges on the observation that a neural network architecture is a declarative specification of the forward and backward propagation steps required for training and deploying the network. However, given this specification, there is a rich design space of algorithms, hardware choices, and communications strategies to most efficiently execute these specifications. We build a novel performance model called PALEO1 that maps this declarative specification to arbitrary points in this design space to estimate the execution time of training and\n1Open-sourced at https://github.com/TalwalkarLab/paleo.\ndeploying deep neural networks.2 PALEO applies broadly to a wide variety of neural network architectures and for arbitrary learning systems within this design space, and thus can serve as a valuable tool for practitioners and developers to answer the questions mentioned above."
    }, {
      "heading" : "2 BACKGROUND AND RELATED WORK",
      "text" : "Training deep neural networks can be very time and resource consuming, and it is not uncommon for the training of a model to take days across tens or hundreds of machines. Several high-level strategies have been proposed to accelerate this process, and these strategies collectively define the design space considered by PALEO.\nHardware acceleration approaches are designed to accelerate the computation of the forward and backward passes and often make use of specialized hardware, such as GPUs (Coates et al., 2013), or more recently custom hardware designed specifically for deep learning (Jouppi, 2016). PALEO accepts constants associated with hardware as input (e.g., peak FLOPS, network bandwidth) and automatically adapts to changes in this input.\nSoftware acceleration via specialized libraries, e.g., cuda-convnet (Krizhevsky, 2014a) and cuDNN (Chetlur et al., 2014), and highly-optimized algorithms for commonly used primitives, e.g., Chetlur et al. (2014) and Lavin (2016), can also be used to accelerate deep model training. PALEO dynamically picks among the best available implementation for each layer at execution time.\nParallelization is a natural approach to consider, and can involve training a neural network with many computational devices (e.g. CPUs, GPUs) on a single machine, or across a network. There are two major parallelization strategies when it comes to training deep neural network models at scale: data parallelism and model parallelism. In classical data parallel systems, each worker stores an identical copy of the model and computes gradients only on a shard of the training examples, and these gradients are aggregated to update the model. In contrast, model parallel systems shard the model itself across the workers, while the training data may be stored on each worker or sharded across the workers. PALEO models both data and model parallel settings.\nCommunication schemes have also been explored to accelerate incremental model updates across distributed workers. Three of the most common schemes are (Iandola et al., 2016; Zhao & Canny, 2013): (i) the OneToAll scheme has a 2KT communication time as a master node must communicate with all K workers individually, where T is the time for communicating data through one link in the network; (ii) the Tree AllReduce scheme takes 2 log2(K)T for weights to be aggregated and broadcasted to all workers following a tree topology; and (iii) the Butterfly AllReduce scheme in which all workers receive aggregated weights in log2(K)T using a butterfly network. We restrict the focus of PALEO to distributed communication schemes that return equivalent results to serial executions, and we thus do not consider the recently introduced butterfly mixing scheme of Zhao & Canny (2013), or non-deterministic asynchronous parameter servers."
    }, {
      "heading" : "3 PALEO",
      "text" : "We now present PALEO, a model for the lean consumption of resources during the training of DNNs. PALEO decomposes the total execution time into computation time and communication time; both are estimated for each pass of a neural network’s evaluation given user specified choices within the design space of algorithms, hardware, and communications strategies. Figure 1 illustrates the overall idea. The computation time is calculated from factors including the size of the computation inputs imposed by the network architecture, the complexity of the algorithms and operations involved in the network layers, and the performance of the hardware to be used. The communication time is estimated based on the computational dependencies imposed by the network, the communication bandwidth of the hardware, and the assumed parallelization schemes. Once the network architecture and design space choices are fixed, all of the key factors in PALEO can be derived, and we can estimate execution time without actually implementing the entire network and/or an underlying software package.\n2Training a neural network involves both forward and backward propagation, whereas deploying a trained network on a new data point involves only forward propagation. Thus, estimating the execution time of model training encompasses both model training and deployment, and is the focus of this work."
    }, {
      "heading" : "3.1 COMPUTATION MODELING",
      "text" : "We first describe the computation model on a single machine. The computation in a neural network can be expressed as a directed graph N = 〈{u(i)}ni=1, {(u(i), u(j))}〉, where each node u(i) is associated with an operation f (i) on a device d(i); each directed edge (u(i), u(j)) represents the dependency that operation f (j) cannot be executed until f (i) is finished. We use Pa(u(j)) to represent the set of immediate parent nodes of u(j). We model each layer in the neural network as a node, and the connections between layers as edges. In the following text, we omit the superscript index when there is no ambiguity."
    }, {
      "heading" : "3.1.1 COMPUTATION TIME FOR A SINGLE LAYER",
      "text" : "To model the runtime of a layer u, we consider the operation f and decompose the execution time of this operation into three terms (as shown in Figure 2a): the time to fetch the input produced by its parent layers R(Pa(u)); the time to perform the computation of f on the designated device d, i.e., C(f, d); and the time to write the outputs to the local memoryW(f, d). Assuming a sequential execution, the runtime for a node u can be written as a simple summation:\nT (u) = R(Pa(u)) + C(f, d) +W(f, d). (1)\nAmong the three terms, the computation time C(f, d) is calculated as the FLOP (floating-point operation) counts of the operation divided by the computation speed (FLOPS; floating-point operation per second) of the device: C(f, d) = FLOPs(f)/speed(d). The IO times R(Pa(u)) and W(u) are calculated as the size of memory footprints involved in the computation divided by the IO bandwidth of the device. When inputs must be fetched from other devices, e.g. in the case of model parallelism, this IO bandwidth refers to the communication bandwidth between two devices. PALEO treats the speed and bandwidth of devices as parameters given to the model so that users can configure them to reflect user-specific configurations.\nUsing this per-layer model, we will next describe how to model the computation time of an entire network. We will subsequently we present FLOP counts for layer operations commonly used in modern DNNs in Section 4."
    }, {
      "heading" : "3.1.2 COMPUTATION TIME FOR NETWORKS",
      "text" : "We first consider simple sequential structures where layers are constructed one after another, as in Figure 2b. The total execution time can be calculated as the sum of execution time of all layers T (N ) = ∑n i=1 T (u\n(i)). While this calculation may seem trivial at first glance, it forms the foundation for modeling execution time for more complex architectures.\nParallel structures are not uncommon in DNNs; for example, the Inception model (Szegedy et al., 2015a) contains layers that can be evaluated simultaneously, and layers on different workers can run in parallel in model parallel setups (Dean et al., 2012). Figure 2c illustrates a parallel structure, where two convolutional layers (each followed by a pooling layer) are scheduled to be executed on two devices.\nTo model computation time of parallel structures, we identify synchronization barriers before and after every parallel structure and introduce a notation of supernode U = {G(i)}ki=1 as a set of disjoint subgraphs sandwiched by the synchronization barriers in the computation graph. When substituting the subgraphs with the supernode, the network is reduced to a sequential structure described above. For the supernode, the execution time T (U) is within the range [maxi T (G(i)), ∑ i T (G(i))], where the lower bound corresponds to perfect parallelization, the upper bound corresponds to sequential execution. Note that the execution time of a subgraph T (G(i)) can be calculated recursively."
    }, {
      "heading" : "3.1.3 COMPUTATION MODELING FOR LAYER OPERATIONS",
      "text" : "In modern DNNs, the convolutional layer is one of the most commonly used and computationally intensive type of layer. For this reason, there has been many heavily optimized implementations (Chetlur et al., 2014; Vasilache et al., 2015; Lavin, 2016). Deriving plausible FLOP counts for other types of layers is a straightforward process, and in this section, we consider two leading implementations for convolutional operations: matrix multiplication and Fast Fourier Transform.\nFollowing the notation used by Chetlur et al. (2014), a 2D convolutional layer during forward propagation3 takes an input feature map DN×C×H×W (which has a batch of N input feature maps with shape H ×W and C channels) and a set of convolutional filters FK×C×R×S (K filters with shape R×S and C channels). It produces N ×K feature maps each of shape P ×Q which can be calculated from the shapes of inputs and filters together with additional striding and padding parameters. The FLOP counts for the convolution operation can be expressed as 2KCRSNPQ. A commonly used implementation is to reduce convolution operations to matrix multiplications, which can be efficiently computed with well-optimized SGEMM routines on various platforms. Although these FLOP counts ignore auxiliary operations (e.g. indexing arithmetic in efficient implementations), they nonetheless provide a good estimate of FLOP counts for matrix multiplication implementations.\nAnother implementation is based on Fast Fourier Transform (Vasilache et al., 2015): both input feature maps and filters are transformed into the frequency domain, then element-wise multiplications are performed followed by an inverse Fourier transform. This implementation introduces computation and memory overhead in discrete Fourier transforms, but reduces the computation complexity to O(NCKHW +(NC+CK+NK)HW log(HW )). Convolutional layers with large filters or a\n3Our arguments generalize to N-dimensional settings, and similar arguments apply for the backward pass.\nlarge problem size can benefit from FFT implementations. When counting FLOPs, it is not possible to get exact counts without knowing the underlying implementation details. In PALEO, we adopt the commonly used FFT complexity 5n log2 n as the FLOP counts for complex-valued transformations of size n (Cooley & Tukey, 1965). To account for the IO overhead caused by auxiliary memories, PALEO estimates the memory size required for complex-valued matrices in the frequency domain and incorporates it into the data reading and writing terms. For FFT-based implementations with tilings, PALEO estimates the number of tiles from the convolution specifications.\nThe choice of algorithm – matrix multiplication or FFT – is problem specific, as it depends on the filter size, strides, input size of the convolutional layers, and memory workspace. In order to derive reasonable estimations for user-specific DNNs comparable to real executions, it is important for PALEO to make decisions comparable to real-world systems. Two common approaches are employed in existing DNN software frameworks and libraries to choose between these algorithms: (i) using predefined heuristics based on offline benchmarks; (ii) autotuning to empirically evaluate available algorithms on the given specification. Since autotuning is tied to platform and software implementations, for maximum generality PALEO by default takes the first approach. In particular, PALEO uses heuristics from cuDNN to make algorithm choices while also accounting for user preferences."
    }, {
      "heading" : "3.2 COMMUNICATION MODELING",
      "text" : "We now describe our modeling for communication among multiple workers. Let |D| be the size of data to be communicated between two workers, and define B as the bandwidth of the communication channel. Then the communication time can simply be written as Tcomm = |D|/B. By using different bandwidth configurations, PALEO works for both scale-up setups (multiple GPUs on one machine) and scale-out setups (multiple machines in a cluster). Moreover, in data-parallel settings, an AllReduce operation is performed to synchronize model parameters across all workers after every backward pass. PALEO considers three communications schemes: OneToAll, Tree AllReduce, and Butterfly AllReduce. The communication time under these three schemes is described in Section 2."
    }, {
      "heading" : "3.3 PLATFORM PERCENT OF PEAK",
      "text" : "Thus far, we have assumed that deep learning software platforms make perfect use of their underlying hardware. That is, that the CPUs and GPUs are operating at “peak FLOPS”, and that network and IO links are fully saturated. This has allowed our model to be platform independent.\nHowever, this assumption is unreasonable in practice. For instance, achieving peak FLOPS is a difficult proposition, usually requiring customized libraries developed by organizations with intimate knowledge of the underlying hardware, e.g., Intel’s MKL (int, 2009), ATLAS (Whaley & Petitet, 2005), and cuDNN. Even these specially tuned libraries may fall short of peak execution by as much as 40% (atl). Further, any computation done outside the scope of PALEO (e.g. job scheduling, data copying) will exacerbate the observed inefficiency in practice. Sometimes such inefficiencies are warranted from the perspective of ease of programmability or maintenance of the learning platforms.\nRather than trying to measure and capture every source of inefficiency in every learning framework, we take a small number of representative deep learning workloads which contain convolutions, pooling, dropout, and fully connected layers and run them for a short time on a single GPU. Given observed total throughput and estimated total throughput on this benchmark we fit a scaling constant to estimate a platform percent of peak (PPP) parameter which captures the average relative inefficiency of the platform compared to peak FLOPS. Highly specialized frameworks (e.g. cuDNN) will in general have a computational PPP that is close to 100%, while frameworks with higher overheads may have PPP constants closer to 50% or less.\nWe follow a similar benchmarking procedure to estimate PPP for the communication link for TensorFlow. For the FireCaffe experiments, we estimate the communication PPP based on the empirical results for communication reported in Table 4 of the paper."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We now present empirical results which illustrate that PALEO is robust to the choice of network architecture, hardware, communication schemes, and parallelization strategies."
    }, {
      "heading" : "4.1 LAYER-WISE EVALUATION",
      "text" : "We first compare PALEO-estimated runtimes with actual runtimes measured from TensorFlow4 (Abadi et al., 2015) execution in two popular CNN architectures: the one-tower variant of AlexNet (Krizhevsky, 2014b) and the 16-layer VGG network (Simonyan & Zisserman, 2014). PALEO uses cuDNN heuristics to choose algorithms and the auto-tuning mechanism in TensorFlow is disabled. Experiments are run on a NVIDIA TITAN X GPU with a 4 GB workspace limit.\nFor convolutional and fully connected layers, we evaluate forward computation, backward computation with respect to layer inputs, and backward computation with respect to filters separately (see Figure 4 in the appendix for the plots of layer-by-layer comparison.) Table 1 shows a comparison of full forward pass and backward pass with all layers included. PALEO’s per layer estimates are quite close to the actual TensorFlow execution, with only one layer – ‘fc6’ – consistently being underestimated by PALEO.5 In spite of this issue with ‘fc6’, our full pass estimates are remarkably accurate."
    }, {
      "heading" : "4.2 CASE STUDIES",
      "text" : "We now revisit the questions posed at the beginning of the paper and demonstrate how PALEO can help in answering them. In this subsection we present three case studies. We extract experiment setups including network architectures, hardware specifications, communication schemes, and parallelization strategies from selected publications focusing on scalability of CNNs. We then plug those configurations into PALEO and compare the simulated scalability results with the reported results in the original publications. Table 2 summaries the configurations of PALEO in these experiments.\n4TensorFlow 0.9 with cuDNN 4 backend. 5Examining the TensorFlow execution with the NVIDIA profiler revealed that TensorFlow spent two-thirds of its reported ‘fc6’ time in transforming data layout between NHWC and NCHW when calling the underlying cuBLAS primitives.\n6Total time of forward pass, backward pass, and parameter update for one mini-batch on one worker. 7Reported times for Cases 1 and 3 are derived approximately from information in the publications. For Case\n2 no run time information is provided."
    }, {
      "heading" : "4.2.1 CASE 1: NIN WITH FIRECAFFE",
      "text" : "FireCaffe (Iandola et al., 2016) adopts the Tree AllReduce communication scheme when training a NiN model (Lin et al., 2013) in data parallel settings with up to 128 servers on the Titan supercomputer. They report a 38× speedup for NiN with batch size 1024 relative to single-GPU performance. Tabel 3 shows the results from PALEO compared with the results reported by FireCaffe."
    }, {
      "heading" : "4.2.2 CASE 2: INCEPTION WITH TENSORFLOW",
      "text" : "Murray et al. (2016) reported their results in synchronously training the Inception model (Szegedy et al., 2015b) with TensorFlow and achieved a 56× speedup with 100 workers. They apply a weak scaling strategy with batch size 256 to keep GPUs saturated. Although Murray et al. (2016) leveraged a distributed parameter server rather than one of the three communications schemes considered in PALEO, the communication cost of Butterfly AllReduce can be viewed as a lower bound (Zhao & Canny, 2013). To account for the fact that they train with worker nodes each of which have 8 GPUs, we assumes a linear speedup for GPUs on the same host. Figure 3a shows a comparison between reported speedups and PALEO estimated speedups. For absolute runtime, in one of the experiments, their model completes 20 epochs of training after 100 hours when using 8 Tesla K40’s and a batch size 256. PALEO projects a 111 hours runtime under the same setting."
    }, {
      "heading" : "4.2.3 CASE 3: ALEXNET WITH HYBRID PARALLELISM",
      "text" : "Krizhevsky (2014b) describes a hybrid model and data parallelism approach for training AlexNet using up to 8 GPUs with a weak scaling strategy. In his setup, each of the two CPUs connects to 4 GPUs, the communication bandwidth is penalized by 50% across the two groups as mentioned in the paper. Table 4 shows the comparison between PALEO’s projection and the original result, which are quite similar. Moreover, whereas Krizhevsky (2014b) does not quantify the speedup of hybrid parallelism relative to strict data parallelism, PALEO simulates training the entire network with only data parallelism (see last two columns of Table 4) in order to estimate this speedup."
    }, {
      "heading" : "4.3 HYPOTHETICAL SETUPS",
      "text" : "In this subsection, we use PALEO in two hypothetical setups to analyze the scalability of AlexNet and a GAN model under different communication schemes."
    }, {
      "heading" : "4.3.1 ALEXNET IN A CLOUD-BASED SETUP",
      "text" : "In this study, we present an analysis of data parallel training of AlexNet. We assume a modern cloud setup with a cluster of servers each equipped with a NVIDIA K80 GPU connected to a 20 Gbps network. In contrast to the Inception model with 23 million parameter, the one-tower variant of AlexNet has 50 million parameters and therefore doubles communication workload when training with data parallelism.\nWe show strong scaling for all three communication schemes in Figure 3c. Even when assuming a fairly large batch size of 2048 which is beneficial in distributed settings, we see very modest speedups. The OneToAll scheme achieves a max speedup of less than a 2× using 4 workers, while the communication-efficient Butterfly AllReduce scheme achieves a max speedup of roughly 5× when using 32 workers. The weak scaling results, shown in Figure 3b, show drastically improved scaling results, as we observe nearly linear speedups as we increase the number of workers. However, it is important to note that we are increasing the effective batch size as we increase the number of workers, and it is well-known that training with large effective batch-sizes can yield models with substandard accuracy (Breuel, 2015)."
    }, {
      "heading" : "4.3.2 GAN ARCHITECTURE",
      "text" : "PALEO can be applied to architectures other than CNNs. We profile a generative adversarial network (GAN) inspired by Radford et al. (2015) for the LSUN dataset with the same hardware assumptions as the previous case study. Table 5 shows that PALEO estimations are close to empirical TensorFlow run time for both the discriminator and generator networks. Figure 3d plots the estimated speedups for training the model with a batch size 256 on up to 128 workers under strong scaling. Without communication-intensive fully-connected layers, while training this GAN architecture is more scalable than AlexNet, PALEO still only predicts an 8× sub-linear speedup with 64 workers."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We introduced PALEO – an analytical performance model for exploring the space of scalable deep learning systems. By extracting computational requirements carried by neural network architectures and mapping them to the design space of software, hardware, and communication strategies, PALEO can effectively and accurately model the expected scalability and performance of a putative deep learning system."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous systems, 2015",
      "author" : [ "Martın Abadi" ],
      "venue" : "Software available from tensorflow. org,",
      "citeRegEx" : "Abadi,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi",
      "year" : 2015
    }, {
      "title" : "The effects of hyperparameters on sgd training of neural networks",
      "author" : [ "Thomas Breuel" ],
      "venue" : null,
      "citeRegEx" : "Breuel.,? \\Q2015\\E",
      "shortCiteRegEx" : "Breuel.",
      "year" : 2015
    }, {
      "title" : "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems",
      "author" : [ "Tianqi Chen" ],
      "venue" : null,
      "citeRegEx" : "Chen,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen",
      "year" : 2015
    }, {
      "title" : "cudnn: Efficient primitives for deep learning",
      "author" : [ "Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer" ],
      "venue" : null,
      "citeRegEx" : "Chetlur et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chetlur et al\\.",
      "year" : 2014
    }, {
      "title" : "convnet-benchmarks, 2016. URL https://github.com/soumith/ convnet-benchmarks",
      "author" : [ "Soumith Chintala" ],
      "venue" : null,
      "citeRegEx" : "Chintala,? \\Q2016\\E",
      "shortCiteRegEx" : "Chintala",
      "year" : 2016
    }, {
      "title" : "Deep learning with cots hpc systems",
      "author" : [ "Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew" ],
      "venue" : "In Proceedings of the 30th international conference on machine learning,",
      "citeRegEx" : "Coates et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2013
    }, {
      "title" : "An algorithm for the machine calculation of complex fourier series",
      "author" : [ "James W Cooley", "John W Tukey" ],
      "venue" : "Mathematics of computation,",
      "citeRegEx" : "Cooley and Tukey.,? \\Q1965\\E",
      "shortCiteRegEx" : "Cooley and Tukey.",
      "year" : 1965
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Jeffrey Dean" ],
      "venue" : "In NIPS, pp. 1223–1231,",
      "citeRegEx" : "Dean,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean",
      "year" : 2012
    }, {
      "title" : "Firecaffe: near-linear acceleration of deep neural network training on compute clusters",
      "author" : [ "Forrest N Iandola", "Khalid Ashraf", "Mattthew W Moskewicz", "Kurt Keutzer" ],
      "venue" : null,
      "citeRegEx" : "Iandola et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Iandola et al\\.",
      "year" : 2016
    }, {
      "title" : "cuda-convnet2, 2014a. URL https://github.com/akrizhevsky/ cuda-convnet2",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2014\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2014
    }, {
      "title" : "One weird trick for parallelizing convolutional neural networks",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2014\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2014
    }, {
      "title" : "Fast algorithms for convolutional neural networks",
      "author" : [ "Andrew Lavin" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Lavin.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lavin.",
      "year" : 2016
    }, {
      "title" : "Sparknet: Training deep networks in spark",
      "author" : [ "Philipp Moritz", "Robert Nishihara", "Ion Stoica", "Michael I Jordan" ],
      "venue" : null,
      "citeRegEx" : "Moritz et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Moritz et al\\.",
      "year" : 2015
    }, {
      "title" : "Announcing tensorflow 0.8 now with distributed computing support!, 2016",
      "author" : [ "Derek Murray" ],
      "venue" : "URL https: //research.googleblog.com/2016/04/announcing-tensorflow-08-now-with. html",
      "citeRegEx" : "Murray,? \\Q2016\\E",
      "shortCiteRegEx" : "Murray",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "arXiv preprint arXiv:1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast convolutional nets with fbfft: A gpu performance evaluation",
      "author" : [ "Nicolas Vasilache", "Jeff Johnson", "Michael Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Vasilache et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vasilache et al\\.",
      "year" : 2015
    }, {
      "title" : "Minimizing development and maintenance costs in supporting persistently optimized BLAS",
      "author" : [ "R. Clint Whaley", "Antoine Petitet" ],
      "venue" : "Software: Practice and Experience,",
      "citeRegEx" : "Whaley and Petitet.,? \\Q2005\\E",
      "shortCiteRegEx" : "Whaley and Petitet.",
      "year" : 2005
    }, {
      "title" : "Butterfly mixing: Accelerating incremental-update algorithms on clusters",
      "author" : [ "Huasha Zhao", "John Canny" ],
      "venue" : "In SIAM Conf. on Data Mining. SIAM,",
      "citeRegEx" : "Zhao and Canny.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhao and Canny.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : ", 2015), SparkNet (Moritz et al., 2015), FireCaffe (Iandola et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : ", 2015), FireCaffe (Iandola et al., 2016).",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : ", 2012), TensorFlow (Abadi et al., 2015), MXNet (Chen et al., 2015), SparkNet (Moritz et al., 2015), FireCaffe (Iandola et al., 2016). Moreover, expensive benchmarking efforts, e.g., Chintala et al. (2016), have performed brute-force profiling on some of these deep learning systems on a handful network architectures.",
      "startOffset" : 21,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "Hardware acceleration approaches are designed to accelerate the computation of the forward and backward passes and often make use of specialized hardware, such as GPUs (Coates et al., 2013), or more recently custom hardware designed specifically for deep learning (Jouppi, 2016).",
      "startOffset" : 168,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : ", cuda-convnet (Krizhevsky, 2014a) and cuDNN (Chetlur et al., 2014), and highly-optimized algorithms for commonly used primitives, e.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "Three of the most common schemes are (Iandola et al., 2016; Zhao & Canny, 2013): (i) the OneToAll scheme has a 2KT communication time as a master node must communicate with all K workers individually, where T is the time for communicating data through one link in the network; (ii) the Tree AllReduce scheme takes 2 log2(K)T for weights to be aggregated and broadcasted to all workers following a tree topology; and (iii) the Butterfly AllReduce scheme in which all workers receive aggregated weights in log2(K)T using a butterfly network.",
      "startOffset" : 37,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : ", cuda-convnet (Krizhevsky, 2014a) and cuDNN (Chetlur et al., 2014), and highly-optimized algorithms for commonly used primitives, e.g., Chetlur et al. (2014) and Lavin (2016), can also be used to accelerate deep model training.",
      "startOffset" : 46,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : ", cuda-convnet (Krizhevsky, 2014a) and cuDNN (Chetlur et al., 2014), and highly-optimized algorithms for commonly used primitives, e.g., Chetlur et al. (2014) and Lavin (2016), can also be used to accelerate deep model training.",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : ", cuda-convnet (Krizhevsky, 2014a) and cuDNN (Chetlur et al., 2014), and highly-optimized algorithms for commonly used primitives, e.g., Chetlur et al. (2014) and Lavin (2016), can also be used to accelerate deep model training. PALEO dynamically picks among the best available implementation for each layer at execution time. Parallelization is a natural approach to consider, and can involve training a neural network with many computational devices (e.g. CPUs, GPUs) on a single machine, or across a network. There are two major parallelization strategies when it comes to training deep neural network models at scale: data parallelism and model parallelism. In classical data parallel systems, each worker stores an identical copy of the model and computes gradients only on a shard of the training examples, and these gradients are aggregated to update the model. In contrast, model parallel systems shard the model itself across the workers, while the training data may be stored on each worker or sharded across the workers. PALEO models both data and model parallel settings. Communication schemes have also been explored to accelerate incremental model updates across distributed workers. Three of the most common schemes are (Iandola et al., 2016; Zhao & Canny, 2013): (i) the OneToAll scheme has a 2KT communication time as a master node must communicate with all K workers individually, where T is the time for communicating data through one link in the network; (ii) the Tree AllReduce scheme takes 2 log2(K)T for weights to be aggregated and broadcasted to all workers following a tree topology; and (iii) the Butterfly AllReduce scheme in which all workers receive aggregated weights in log2(K)T using a butterfly network. We restrict the focus of PALEO to distributed communication schemes that return equivalent results to serial executions, and we thus do not consider the recently introduced butterfly mixing scheme of Zhao & Canny (2013), or non-deterministic asynchronous parameter servers.",
      "startOffset" : 46,
      "endOffset" : 1958
    }, {
      "referenceID" : 3,
      "context" : "For this reason, there has been many heavily optimized implementations (Chetlur et al., 2014; Vasilache et al., 2015; Lavin, 2016).",
      "startOffset" : 71,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "For this reason, there has been many heavily optimized implementations (Chetlur et al., 2014; Vasilache et al., 2015; Lavin, 2016).",
      "startOffset" : 71,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "For this reason, there has been many heavily optimized implementations (Chetlur et al., 2014; Vasilache et al., 2015; Lavin, 2016).",
      "startOffset" : 71,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "Another implementation is based on Fast Fourier Transform (Vasilache et al., 2015): both input feature maps and filters are transformed into the frequency domain, then element-wise multiplications are performed followed by an inverse Fourier transform.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "For this reason, there has been many heavily optimized implementations (Chetlur et al., 2014; Vasilache et al., 2015; Lavin, 2016). Deriving plausible FLOP counts for other types of layers is a straightforward process, and in this section, we consider two leading implementations for convolutional operations: matrix multiplication and Fast Fourier Transform. Following the notation used by Chetlur et al. (2014), a 2D convolutional layer during forward propagation3 takes an input feature map DN×C×H×W (which has a batch of N input feature maps with shape H ×W and C channels) and a set of convolutional filters FK×C×R×S (K filters with shape R×S and C channels).",
      "startOffset" : 72,
      "endOffset" : 413
    }, {
      "referenceID" : 8,
      "context" : "FireCaffe (Iandola et al., 2016) adopts the Tree AllReduce communication scheme when training a NiN model (Lin et al.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Table 4: Comparison between PALEO estimation and Krizhevsky (2014b) for training AlexNet.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "However, it is important to note that we are increasing the effective batch size as we increase the number of workers, and it is well-known that training with large effective batch-sizes can yield models with substandard accuracy (Breuel, 2015).",
      "startOffset" : 230,
      "endOffset" : 244
    }, {
      "referenceID" : 13,
      "context" : "Sp ee du p Paleo: OneToAll Paleo: Tree AllReduce Paleo: Butterfly AllReduce Murray el at. (2016)",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "We profile a generative adversarial network (GAN) inspired by Radford et al. (2015) for the LSUN dataset with the same hardware assumptions as the previous case study.",
      "startOffset" : 62,
      "endOffset" : 84
    } ],
    "year" : 2017,
    "abstractText" : "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration. In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called PALEO. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, PALEO can efficiently and accurately model the expected scalability and performance of a putative deep learning system. We show that PALEO is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.",
    "creator" : "LaTeX with hyperref package"
  }
}