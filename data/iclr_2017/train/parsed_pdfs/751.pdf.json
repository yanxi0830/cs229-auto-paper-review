{
  "name" : "751.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "COMBATING DEEP REINFORCEMENT LEARNING’S SISYPHEAN CURSE WITH INTRINSIC FEAR",
    "authors" : [ "Zachary C. Lipton", "Jianfeng Gao", "Lihong Li", "Jianshu Chen", "Li Deng" ],
    "emails" : [ "zlipton@cs.ucsd.edu", "deng}@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Following success on Atari games (Mnih et al., 2015) and the board game Go (Silver et al., 2016), many researchers have begun exploring practical applications of deep reinforcement learning (DRL). With DRL, we might hope to replace simple hand-coded control policies with continuously learning agents. Some investigated applications include robotics (Levine et al., 2016), dialogue systems (Fatemi et al., 2016; Lipton et al., 2016), energy management (Night, 2016), and self-driving cars (Shalev-Shwartz et al., 2016). Amid this push to apply DRL, we might ask, can we trust these agents in the wild?\nAgents acting in real world environments might possess the ability to cause catastrophic outcomes. Consider a self-driving car that might hit pedestrians, or a domestic robot that might injure a child. We might hope to prevent DRL agents from ever making catastrophic mistakes. But doing so requires extensive prior knowledge of the environment in order to constrain the exploration of policy space (Garcıa & Fernández, 2015). Such knowledge may not always exist. In this paper, we don’t assume prior knowledge of the state-space.\nAlready, we should note that multiple, conflicting definitions of safety and catastrophe exist, a problem that invites further philosophical consideration. We will return to this matter in Section 3. For now, we introduce a specific but plausible notion of a catastrophe. Suppose that catastrophes are a subset of states that are terminal, yielding a return of 0 and that returns are greater than or equal to\n∗http://zacklipton.com\n0 for non-catastrophic states. Moreover, we assume that an optimal policy never even comes near a catastrophe state. Note that this doesn’t hold in general RL problems. In a cliff world, the pot of gold might lie on the ledge of a cliff. But for a self-driving car, we don’t suspect that it ever should have to come within a split-second decision of committing murder. Finally, while we don’t assume prior knowledge of which states are dangerous, we do assume the existence of catastrophe detector. After encountering a catastrophic state, an agent can realize this and take action to avoid dangerous states in the future.\nGiven this definition, we address two challenges: First, can we expect DRL agents, after experiencing some number of catastrophic failures, to avoid perpetually making the same mistakes? Second, can we use our prior knowledge that catastrophes should be kept at distance to accelerate learning of a DRL agent? Our experiments show that even on toy problems, the deep Q-network (DQN), a basic algorithm behind many of today’s state of the art DRL systems, struggles on both counts. Even in toy environments, DQNs may encounter thousands of catastrophes before avoiding them and may periodically repeat old errors so long as they continue to learn. We call this latter problem the Sisyphean curse.\nThis poses a formidable obstacle to using DQNs in the real world. How can we hand over responsibility for any consequential actions (control of a car, say) to a DRL agent if it may be doomed to periodically remake every kind of mistake, however grave, so long as it continues to learn? Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it’s undesirable.\nNote that traditional tabular agents do not suffer from the Sisyphean curse. In the tabular setting, an RL agent never forgets the learned dynamics of its environment, even as its policy evolves. Thus, if the Markovian assumption holds, eventual convergence to a globally optimal policy is guaranteed. Unfortunately, the tabular approach becomes infeasible in high-dimensional, continuous state spaces.\nThe trouble owes to the use of function approximation (Murata & Ozawa, 2005). When training a DQN, we successively update a neural network based on experiences. These experiences might be sampled in an online fashion, from a trailing window (experience replay buffer), or uniformly from all past experiences. Regardless of which mode we use to train the network, eventually, states that a learned policy never encounters will come to form an infinitesimally small region of the training distribution. At such time, our networks are subject to the classic problem of catastrophic interference McCloskey & Cohen (1989); McClelland et al. (1995). Nothing prevents the DQN’s policy from drifting back towards one that revisits catastrophic, but long-forgotten mistakes.\nMore formally, we could characterize the failures as:\n• Training under distribution D, our agent produces a safe policy πs that avoids catastrophes • Collecting data generated under πs yields a distribution D′ • Training under D′, the agent produces πd, a policy that once again experiences catastrophes\nIn this paper, we illustrate the brittleness of modern deep reinforcement learning algorithms. We introduce a simple pathological problem called Adventure Seeker. This problem consists of a one-dimensional continuous state, two actions, simple dynamics and a clear analytic solution. Nevertheless, the DQN fails. We then show that similar dynamics exist in the classic RL environment Cart-Pole. Finally, we present preliminary findings on the Atari game Seaquest, suggesting that the intrinsic fear model may also be useful for improving performance in more complex environments.\nTo combat these problems, we propose intrinsic fear (Figure 1). In this heuristic approach, alongside the DQN, we train a supervised danger model. The danger model predicts which states are likely to lead to a catastrophe within some number of steps kr. The output of this model (a probability) is then scaled by a fear factor and used to penalize the Q-learning target. Our approach bears some resemblance to intrinsic motivation Chentanez et al. (2004). But instead of perturbing the reward function to encourage the discovery of novel states, we perturb it to discourage the rediscovery of catastrophic states."
    }, {
      "heading" : "2 BACKGROUND: DEEP Q-LEARNING",
      "text" : "For context we briefly review deep Q-learning. Over a series of turns, an agent interacts with its environment via a Markov decision process, or MDP, (S,A, T ,R, γ). At each step t, an agent\nobserves a state s ∈ S. The agent then chooses an action a ∈ A according to some policy π. In turn, the environment transitions to a new state st+1 ∈ S according to transition dynamics T (st+1|st, at) and generates a reward rt with expectationR(s, a).. This cycle continues until each episode terminates.\nThe goal of an agent is to maximize the cumulative discounted return ∑T t=0 γ\ntrt. Temporaldifferences (TD) methods (Sutton, 1988) such as Q-learning (Watkins & Dayan, 1992) model the Q-function, which gives the optimal discounted total reward of a state–action pair; the greedy policy w.r.t. the Q-function is optimal (Sutton & Barto, 1998). Most problems of practical interests have large state spaces, thus the Q-function has to be approximated by parametric models such as neural networks.\nIn deep Q-learning, this is typically accomplished by alternately collecting experiences by acting greedily with respect to Q(s, a; θQ) and updating the parameters θQ. Updates proceed as follows. For a given experiences (st, at, rt, st+1), we minimize the squared Bellman error:\nL = (Q(st, at; θQ)− yt)2 (1)\nfor yt = rt + γ · maxa′ Q(st+1, a′; θQ). Traditionally, the parameterised Q(s, a; θ) is trained by stochastic approximation, estimating the loss on each experience as it is encountered, yielding the update:\nθt+1 ←θt + α(yt −Q(st, at; θt))∇Q(st, at; θt). (2)\nQ-learning methods also require an exploration strategy, and for simplicity we consider only the -greedy heuristic. However, our techniques apply equally for Thompson-sampling based exploration. For a thorough overview of RL fundamentals, we refer the reader to Sutton & Barto (1998).\nA few tricks are often useful to stabilize Q-learning with function approximation. Of particular relevance to this work is experience replay (Lin, 1992): the RL agent maintains a buffer of past experiences, applying TD-learning on randomly selected mini-batches of experience to update the Qfunction. This technique has proven effective to make Q-learning more stable and more data-efficient (Lin, 1992; Mnih et al., 2015)."
    }, {
      "heading" : "3 SAFE REINFORCEMENT LEARNING",
      "text" : "At the outset we ought to consider a more formal treatment of safe reinforcement learning. Precisely what do we mean by safety, catastrophe, or danger? The literature offers several notions of safety in reinforcement learning. Garcıa & Fernández (2015) provide a comprehensive review of this literature, suggesting that existing approaches can be divided into two categories.\nThe first approach consists of modifying the objective function. Traditionally, in a Markov decision process, a reinforcement learner seeks to maximize expected return. Hans et al. (2008) defines a fatality as a return below some threshold τ . In this view, an agent might seek to minimize the worst\ncase scenario instead of maximizing the expected return. Along these lines, Heger (1994) suggested the Q̂-learning objective:\nQ̂ = min(Q̂(st, at), rt+1 + γ max at+1∈A Q̂(st+1, at+1))\nOther relevant papers suggest modifying the objective to address risk. While classic Q-learning objective addresses only expected returns, alternative objectives penalize policies for returns with high-variance Garcıa & Fernández (2015). Other suggest incorporating external knowledge into the exploration process. In the extreme case, we might confine our policy search to the subset of policies known to be safe.\nWe see the following problem. The first category of methods acts only on the summary statistics of the return. This can be beneficial because it requires no foreknowledge. But the drawback is that they fundamentally change the objective everywhere, even in states that we might not be worried about. On the other hand, processes relying on expert knowledge may presume an unreasonable level of foreknowledge.\nIn this paper, we propose a new formulation of the safety problem. We suppose there exists a subset C ⊂ S of catastrophic states. These states have minimum return, and thus resemble the fatalities of Hans et al. (2008). Additionally we assume that it’s reasonable for policy to avoid even getting close to these states.\nIn other words, there exist good policies that never get within a few hops of a catastrophe state. Just as a child shouldn’t run with scissors, a self-driving car should never be one split-second decision away from killing a passenger or pedestrian. Note, this imposes some prior knowledge. In a general, unknown environment, it might be that a good policy should get very close to a fatality in order to maximize reward.\nWe don’t assume advanced knowledge of C, but we do assume the agent possesses a catastrophe detector. In the case of a self-driving car, this might be an impact detector. Even if we can’t anticipate all the situations which might lead to an accident, we can recognize one when it has already happened and take measures to avoid danger zones around these states in the future.\nThe notion of danger zone suggests some measure of proximity. Informally, we conceive of danger as the likelihood of entering a catastrophe state c ∈ C within a short number of steps. Note, this probability depends on the policy. In this paper, we collect the experiences used to train the danger model from the entire training period. So the likelihood here is determined based on the observations of many policies over the course of training. Explicitly modeling danger zones can be useful for two reasons. First, it provides a mechanism to avoid oscillating policy learners from repeatedly encountering catastrophe states. Second, reward-shaping to avoid danger zones can significantly reduce the sample complexity of exploration."
    }, {
      "heading" : "4 TWO PATHOLOGICAL FAILURE CASES",
      "text" : "We now present two pathological environments. In the Adventure Seeker (Figure 2a) environment, we imagine a player placed on a hill, sloping upward to the right. At each turn, the player can move to the right (up the hill) or left (down the hill). The environment adjusts the player’s position accordingly, adding some random noise. Between the left and right edges of the hill, the player gets more reward for spending time higher on the hill. But if the player goes too far to the right, she will fall off (a catrastrophic state), terminating the episode and receiving a return of 0.\nFormally, the state consists of a single continuous variable s ∈ [0, 1.0], denoting the player’s position. The starting position for each episode is chosen uniformly at random in the interval [.25, .75]. The available actions consist only of {−1,+1} (left and right). Given an action at in state st, T (st+1|st, at) gives successor state st+1 ← st + .01 · at + η where η ∼ N (0, .012). At each turn, the player gets reward equal to st (proportional to height). The player falls off the hill, entering the catastrophic terminating state, whenever an action would result in successor state st+1 > 1.0 or st+1 < 0.0.\nThis game admits an obvious analytic solution. There exists some threshold above which the agent should always choose to go left, and below which it should always go right. Even an infant could grasp the gist of this solution. And yet a state-of-the-art DQN model learning online or with experience\nreplay successively plunges to its death. To be clear, the DQN does learn a near-optimal thresholding policy quickly. But over the course of continued training, the agent oscillates between a reasonable thresholding policy and one which always moves right, regardless of the state. The pace of this oscillation evens out and all networks (over multiple runs) quickly reach a constant catastrophe per turn rate (Figure 3a) that does not attenuate with continued training. How could we ever trust a system that can’t solve Adventure Seeker to make consequential real-world decisions?\nCart-Pole (Figure 2b) is a classic RL environment in which an agent tries to balance a pole atop a cart. Qualitatively, the game exhibits four distinct catastrophe modes. The pole could fall down to the right or fall down to the left. Additionally, the cart could run off the right boundary of the screen or run off the left.\nFormally, at each time, the agent observes a four-dimensional state vector (x, v, θ, ω) consisting respectively of the cart position, cart velocity, pole angle, and the pole’s angular velocity. At each time step, the agent chooses an action, applying a force of either −1 or +1. For every time step that the pole remains upright and the cart remains on the screen, the agent receives a reward of 1. If the pole falls, the episode terminates, giving a return of 0 from the penultimate state. In experiments, we use the implementation CartPole-v0 contained in the openAI gym Brockman et al. (2016). Like Adventure Seeker, this problem admits an analytic solution. A perfect policy should never drop the pole. But, as with Adventure Seeker, a DQN converges to a constant rate of catastrophes per turn.\nIn addition to these pathological cases, we also present some preliminary experiments extending these results to the Atari game environment. We consider Seaquest, a game in which a player swims under water. The player can navigate up, down, left, right, and shoot a gun. Periodically, as the oxygen gets low, she must rise to the surface for oxygen. Additionally, fish swim across the screen. The player gains points each time she shoots a fish. Colliding with a fish or running out of oxygen result in death. The player has 3 lives, and the final death is a terminal state. In this paper, because the environment doesn’t explicitly report intermediate deaths, we consider only the final death to be the catastrophe state. In the future work, we might hack the game to include all deaths."
    }, {
      "heading" : "5 INTRINSIC FEAR",
      "text" : "We now introduce intrinsic fear (Algorithm 1), a novel mechanism for avoiding catastrophes when learning online with function approximation. In our approach, we maintain both a DQN and a separate, supervised danger model. Our danger model provides an auxiliary source of reward, penalizing the Q-learner for entering dangerous states.\nThe goal in learning this smooth danger zone is twofold. First the knowledge that regions around certain states should be avoided represents a source of prior knowledge that can accelerate learning in some environments. Second, by using danger zones to shape rewards away from catastrophic states, we allow oscillating policies to drift close (but not too close) to catastrophe states, giving them opportunity to adjust trajectories away, without having to re-experience the catastrophe. We draw some inspiration from the idea of a parent scolding a child for running around with a knife. The child can learn to adjust its behavior without actually having to stab someone.\nWe make the following conservative assumption. While we don’t know anything in advance about the state space, we possess a catastrophe detector which can identify each catastrophe as it happens.\nWe also assume that the agent collects a buffer of previously observed states. After it detects each catastrophe, the agent can refer back to the trajectory that led up to it.\nAlgorithm 1: Training DQN with Intrinsic Fear 1 function Train( Q(·, ·; ·), d(·; ·), T , λ, kλ, kr);\nInput :Two model architectures: Q (a DQN) and d (the danger model), time limit T , fear factor λ, fear phase-in length kλ, fear radius kr\nOutput :Learned parameters θQ and θd 2 Initialize parameters θQ and θd randomly 3 Initialize replay memory De, danger states Dd, and safe states Ds 4 for t in 1:T do 5 With probability select random action at 6 Otherwise, select action at = maxa′ Q(st, a′; θQ) 7 Execute action at in environment, observing reward rt and successor state st+1 8 Store transition (st, at, rt, st+1) in D 9 if st+1 is a terminal state then\n10 Add states st−kr through st to Dd 11 Add states st−ne through st−kr−1 to Ds 12 end 13 Sample random minibatch of transitions (sτ , aτ , rτ , sτ+1) from D 14 Set λt ← min(λ, λ·tkλ )\n15 Set yt ← { rt − λt, for terminal sτ+1 rt +maxa′ Q(st+1, a ′; θQ)− λ · d(st+1; θd) for non-terminal st+1 } 16 Apply SGD step θQ ← θQ − η · ∇θQ(yτ −Q(sτ , aτ ; θQ))2 17 Sample random mini-batch sj with 50% of examples from Dd and 50% from Ds\n18 Set yj ← {\n1, for sj ∈ Dd 0, for sj ∈ Ds } 19 Apply SGD step θd ← θd − η · ∇θd lossd(yj , d(sj ; θd)) 20 end\nThe technique works as follows: In addition to the DQN, we maintain a binary classifier that we term a danger model. In our case, it is a neural network of the same architecture as the DQN (but for the output layer). In general, it could be any supervised model. The danger model’s purpose is to identify the likelihood that any state will lead to catastrophe within k moves. In the course of training, our agent adds each experience (s, a, r, s′) to the experience replay buffer. As each catastrophe is reached at the nth turn of an episode, we add the kr (fear radius) states leading up to the catastrophe to a list of danger states. We add the preceding n− kr states to a list of safe states. When n < kr, all states for that episode are added to the list of danger states.\nThen after each turn, in addition to making one update to the Q-network, we make one mini-batch update to the danger model. To make this update, we sample 50% of states from the danger states, assigning them label 1 and 50% of states from the safe states, assigning them label 0.\nFor each update to the DQN, we perturb the TD target yt. Instead of updating Q(st, at; θQ) towards rt +maxa′ Q(st+1, a ′; θQ), we introduce the intrinsic fear to the model via the target:\nyIFt = rt +max a′ Q(st+1, a ′; θQ)− λ · d(st+1; θd)\nwhere d(s; θd) is the danger model and λ is a fear factor determining the scale of the impact of intrinsic fear on the Q update.\nNote that our method perturbs the objective function. As such, one might be reasonably concerned that it might imply a different optimal policy. We can address this concern in the following way. Let us assume the optimal policy never enters a danger zone of radius kr. Let us also assume that over course of training, the danger model approaches perfect accuracy, meaning that all safe states s are given danger score d(s) = 0. Then because our penalty is equal to 0 for all safe states and >= 0 for all danger states, the optimal policy receives the same return as under the original reward function. However, the subset of suboptimal policies that enter danger zones receive strictly worse returns.\nTherefore, as the danger model approaches perfect performance wrt false positives, the optimal policy should be unchanged.\nWe also note that the danger zone depends on the policy. We could determine its size objectively by reference to the worst possible policy. In practice, however, we have found that we achieve better results by setting a wider fear radius than this worst-case analysis suggests.\nFinally, we observe that the setting of the fear radius encodes prior knowledge of a problem. However, this prior knowledge is expressed far more succinctly. It’s not obvious how we could communicate information about the high-dimensional, continuous state space with similar efficiency.\nOver the course of our experiments, we discovered the following pattern. To be effective, the fear radius kr should be large enough that the model can detect danger states at a safe distance where they can still be avoided. When the fear radius is too small, the danger probability is only nonzero at a points from which catastrophes are inevitable and intrinsic fear doesn’t help. On the other hand, when the fear radius is too large, all states are predicted to be danger states. For example, early in training, all Cart-Pole experiences are shorter than 20 experiences. So a fear radius of 20 leads our models model to diverge. To overcome this problem, we gradually phase in the fear factor from 0 to λ reaching full strength at predetermined time step kλ. In our Cart-Pole experiments we phase in over 1M steps."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "To assess the effectiveness of the intrinsic fear model, we evaluate both a standard DQN (DQNNoFear) and one enhanced by intrinsic fear (DQN-Fear). In both cases, we use multilayer perceptrons (MLPs) with a single hidden layer and 128 hidden nodes. We train all MLPs by stochastic gradient descent using the Adam optimizer Kingma & Ba (2015) to adaptively tune the learning rate. Some might wonder whether the problems we observe truly owe to distributional shift or if they actually stem from the well-known problems of off-policy learning. To show that learning on-policy learning does not mitigate these issues, we also present results for an expected-SARSA variant of the DQN for Adventure Seeker and Cart-Pole.\nBecause, for the Adventure Seeker problem, an agent can escape from danger with only a few time steps of notice, we set the fear radius kr to 5. We phase in the fear factor quickly, reaching full strength in just 1000 moves. On this problem we set the fear factor λ to 40.\nFor Cart-Pole, we set a wider fear radius of kr = 20. We initially tried training this model with a shorter fear radius but made the following observation. Some models would learn well surviving for millions of experiences, with just a few hundred catastrophes. This compared to a DQN (Figure 3) which would typically suffer 4000-5000 catastrophes. When examining the output from the danger models on successful vs unsuccessful runs, we noticed that the unsuccessful models would output danger of probability greater than .5 for precisely the 5 moves before a catastrophe. But by that time it would be too late for an agent to correct course. In contrast, on the more successful runs, the danger model would often output predictions in the range .1− .5. We suspect that this gradation between mildly dangerous states and those where danger is imminent provided a richer reward signal to the DQN. Accordingly, we lengthened the fear radius to 20 so that some states might be ambiguous (could show up in either danger list Dd or safe list Ds. On both the Adventure Seeker and Cart-Pole environments, the DQNs augments by intrinsic fear far outperform their otherwise identical counterparts (Figure 3). We compared this approach against some traditional approaches, like memory based methods for preferentially sampling failure cases but they could not improve over the DQN.\nOn the Atari game Seaquest, we use a fear radius of 10 and a fear factor of .1. Interestingly, the models trained with Intrinsic Fear have indistinguishable catastrophe rates but achieve significantly higher reward. Using a much higher fear factor and wider radius we are able to significantly lower the catastrophe rate but this improvement comes at the expense of total reward. These results suggest an interesting interplay between the various reward signals that warrants further exploration."
    }, {
      "heading" : "7 RELATED WORK",
      "text" : "The paper addresses safety in RL, intrinsically motivated RL, and the stability of Q-learning with function approximation under distributional shift. Our work also has some connection reward shaping. While space constraints preclude adequate treatment of all prior work, we attempt to highlight the most relevant papers here.\nThe Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996). But these papers address neither AI safety nor RL. Murata & Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution. Outside of reinforcement learning, a number of papers address problems related to non-stationary data, including a book on covariate shift (Sugiyama & Kawanabe, 2012).\nSafety in Reinforcement Learning: Several papers address safety in RL. Garcıa & Fernández (2015) provide a thorough review on the topic, dividing the existing works into those which perturb the objective function and those which use external knowledge to improve the safety of exploration. Hans et al. (2008) defines a fatal state as one from which one receives a return below some safety threshold τ . They propose a solution comprised of two components. One, the safety function, identifies unsafe states. The other, denoted the backup model, is responsible for navigating away from the critical state. Their work does not address function approximation, instead focusing on a domain where a tabular approach is viable. Moldovan & Abbeel (2012) gives a definition of safety based on ergodicity. They consider a fatality to be a state from which one cannot return to the start state. Shalev-Shwartz et al. (2016) provides theoretical analysis considering how strong a penalty should be to discourage accidents. They also consider hard constraints to ensure safety, an approach that differs from ours. None of the above works address the case where distributional shift dooms an agent to perpetually revisit catastrophic failure modes.\nIntrinsically Motivated Reinforcement Learning: A number of papers have investigated the idea of intrinsically motivated reinforcement learners. Intrinsic motivation refers to an intrinsically assigned reward, in contrast to the extrinsic reward that comes from the environment. Typically\nintrinsic motivation is proposed as a way to encourage exploration of an environment (Schmidhuber, 1991; Bellemare et al., 2016) and to acquire a modular set of skills Chentanez et al. (2004). In principle, such motivation can lead agents to explore intelligently even when extrinsic rewards are sparse. Some papers refer to the intrinsic reward for discovery as curiosity. Like classic work on intrinsic motivation, our methods operate by perturbing the reward function. But instead of assigning bonuses to encourage discovery of novel transitions, we assign penalties to discourage re-discovery of catastrophic transitions."
    }, {
      "heading" : "8 DISCUSSION",
      "text" : "Our experiments suggest that DQNs may be too brittle for use in real-world applications where harm can come of actions. While it’s easy to visualize these problems on toy examples, similar dynamics are embedded in more complex domains. Consider a domestic robot acting as a barber. The robot might receive positive feedback for giving a closer shave. This reward encourages closer contact at a steeper angle. Of course, the shape of this reward function belies the catastrophe lurking just past the optimal shave. Similar dynamics might be found in a vehicle which is rewarded for traveling faster but could risk an accident with excessive speed.\nThese scenarios are not so unlike the pathological case presented in Adventure Seeker. We might even say these shortcomings seem analogous to the fundamental inability of linear models to solve XOR. But while XOR can be solved by simply incorporating hidden layers of representation, it’s less obvious how to overcome these pitfalls.\nThese early successes of the intrinsic fear model suggest that for some classes of problems, we might avoid perpetual catastrophe and still enjoy the benefits of function approximation. In this work we assume the ability to recognize a catastrophe once it has happened. But we don’t assume anything in advance about the precise form that the danger states might take in the state space. Our approaches seem appropriate for problems with some notion of proximity. A self-driving car can’t run over a passenger in the next second if no people are in close proximity. But out methods might not be appropriate for other problems. For example, in a problem where a single action, from any state, can produce a catastrophe with high probability, our methods might fail.\nThis work represents a first step towards combating AI safety issues stemming from the use of function approximation in deep reinforcement learning. In follow-up work, we hope to formalize the notion of danger presented here. We hope to critically examine competing definitions of catastrophe and of danger, to organize them into a taxonomy and to develop theory addressing the most promising ones. We also hope to explore the effectiveness of our technique on more complex domains. In precisely what sorts of environments should our approach work? Under what conditions can we expect it to fail?"
    } ],
    "references" : [ {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "Leemon Baird" ],
      "venue" : null,
      "citeRegEx" : "Baird,? \\Q1995\\E",
      "shortCiteRegEx" : "Baird",
      "year" : 1995
    }, {
      "title" : "Unifying count-based exploration and intrinsic motivation",
      "author" : [ "Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos" ],
      "venue" : null,
      "citeRegEx" : "Bellemare et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalization in reinforcement learning: Safely approximating the value function",
      "author" : [ "Justin Boyan", "Andrew W Moore" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Boyan and Moore.,? \\Q1995\\E",
      "shortCiteRegEx" : "Boyan and Moore.",
      "year" : 1995
    }, {
      "title" : "Intrinsically motivated reinforcement learning",
      "author" : [ "Nuttapong Chentanez", "Andrew G Barto", "Satinder P Singh" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chentanez et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Chentanez et al\\.",
      "year" : 2004
    }, {
      "title" : "Policy networks with two-stage training for dialogue systems",
      "author" : [ "Mehdi Fatemi", "Layla El Asri", "Hannes Schulz", "Jing He", "Kaheer Suleman" ],
      "venue" : "In SIGDIAL,",
      "citeRegEx" : "Fatemi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fatemi et al\\.",
      "year" : 2016
    }, {
      "title" : "A comprehensive survey on safe reinforcement learning",
      "author" : [ "Javier Garcıa", "Fernando Fernández" ],
      "venue" : null,
      "citeRegEx" : "Garcıa and Fernández.,? \\Q2015\\E",
      "shortCiteRegEx" : "Garcıa and Fernández.",
      "year" : 2015
    }, {
      "title" : "Chattering in SARSA(λ) - a CMU learning lab internal report",
      "author" : [ "Geoffrey J Gordon" ],
      "venue" : null,
      "citeRegEx" : "Gordon.,? \\Q1996\\E",
      "shortCiteRegEx" : "Gordon.",
      "year" : 1996
    }, {
      "title" : "Safe exploration for reinforcement learning",
      "author" : [ "Alexander Hans", "Daniel Schneegaß", "Anton Maximilian Schäfer", "Steffen Udluft" ],
      "venue" : "In ESANN,",
      "citeRegEx" : "Hans et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hans et al\\.",
      "year" : 2008
    }, {
      "title" : "Consideration of risk in reinforcement learning",
      "author" : [ "Matthias Heger" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "Heger.,? \\Q1994\\E",
      "shortCiteRegEx" : "Heger.",
      "year" : 1994
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Sergey Levine" ],
      "venue" : null,
      "citeRegEx" : "Levine,? \\Q2016\\E",
      "shortCiteRegEx" : "Levine",
      "year" : 2016
    }, {
      "title" : "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "author" : [ "Long-Ji Lin" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Lin.,? \\Q1992\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 1992
    }, {
      "title" : "Efficient exploration for dialogue policy learning with BBQ networks & replay buffer spiking",
      "author" : [ "Zachary C Lipton" ],
      "venue" : "In NIPS Workshop on Deep Reinforcement Learning,",
      "citeRegEx" : "Lipton,? \\Q2016\\E",
      "shortCiteRegEx" : "Lipton",
      "year" : 2016
    }, {
      "title" : "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
      "author" : [ "James L McClelland", "Bruce L McNaughton", "Randall C O’Reilly" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "McClelland et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "McClelland et al\\.",
      "year" : 1995
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J Cohen" ],
      "venue" : "Psychology of learning and motivation,",
      "citeRegEx" : "McCloskey and Cohen.,? \\Q1989\\E",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih" ],
      "venue" : "Nature,",
      "citeRegEx" : "Mnih,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih",
      "year" : 2015
    }, {
      "title" : "Safe exploration in markov decision processes",
      "author" : [ "Teodor Mihai Moldovan", "Pieter Abbeel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Moldovan and Abbeel.,? \\Q2012\\E",
      "shortCiteRegEx" : "Moldovan and Abbeel.",
      "year" : 2012
    }, {
      "title" : "A memory-based reinforcement learning model utilizing macroactions",
      "author" : [ "Makoto Murata", "Seiichi Ozawa" ],
      "venue" : "In Adaptive and Natural Computing Algorithms. Springer,",
      "citeRegEx" : "Murata and Ozawa.,? \\Q2005\\E",
      "shortCiteRegEx" : "Murata and Ozawa.",
      "year" : 2005
    }, {
      "title" : "The AI that cut googles energy bill could soon help you",
      "author" : [ "Will Night" ],
      "venue" : "MIT Tech Review,",
      "citeRegEx" : "Night.,? \\Q2016\\E",
      "shortCiteRegEx" : "Night.",
      "year" : 2016
    }, {
      "title" : "A possibility for implementing curiosity and boredom in model-building neural controllers. In From animals to animats: proceedings of the first international conference on simulation of adaptive behavior (SAB90)",
      "author" : [ "Jurgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "Schmidhuber.,? \\Q1991\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1991
    }, {
      "title" : "Safe, multi-agent, reinforcement learning for autonomous driving",
      "author" : [ "Shai Shalev-Shwartz", "Shaked Shammah", "Amnon Shashua" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2016
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "David Silver" ],
      "venue" : null,
      "citeRegEx" : "Silver,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver",
      "year" : 2016
    }, {
      "title" : "Machine learning in non-stationary environments: Introduction to covariate shift adaptation",
      "author" : [ "Masashi Sugiyama", "Motoaki Kawanabe" ],
      "venue" : null,
      "citeRegEx" : "Sugiyama and Kawanabe.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sugiyama and Kawanabe.",
      "year" : 2012
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "Richard S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Richard S. Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : ", 2016), dialogue systems (Fatemi et al., 2016; Lipton et al., 2016), energy management (Night, 2016), and self-driving cars (Shalev-Shwartz et al.",
      "startOffset" : 26,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : ", 2016), energy management (Night, 2016), and self-driving cars (Shalev-Shwartz et al.",
      "startOffset" : 27,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : ", 2016), energy management (Night, 2016), and self-driving cars (Shalev-Shwartz et al., 2016).",
      "startOffset" : 64,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "At such time, our networks are subject to the classic problem of catastrophic interference McCloskey & Cohen (1989); McClelland et al. (1995). Nothing prevents the DQN’s policy from drifting back towards one that revisits catastrophic, but long-forgotten mistakes.",
      "startOffset" : 117,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "Our approach bears some resemblance to intrinsic motivation Chentanez et al. (2004). But instead of perturbing the reward function to encourage the discovery of novel states, we perturb it to discourage the rediscovery of catastrophic states.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "Temporaldifferences (TD) methods (Sutton, 1988) such as Q-learning (Watkins & Dayan, 1992) model the Q-function, which gives the optimal discounted total reward of a state–action pair; the greedy policy w.",
      "startOffset" : 33,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "For a thorough overview of RL fundamentals, we refer the reader to Sutton & Barto (1998).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "Of particular relevance to this work is experience replay (Lin, 1992): the RL agent maintains a buffer of past experiences, applying TD-learning on randomly selected mini-batches of experience to update the Qfunction.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "This technique has proven effective to make Q-learning more stable and more data-efficient (Lin, 1992; Mnih et al., 2015).",
      "startOffset" : 91,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "Hans et al. (2008) defines a fatality as a return below some threshold τ .",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "Along these lines, Heger (1994) suggested the Q̂-learning objective:",
      "startOffset" : 19,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "These states have minimum return, and thus resemble the fatalities of Hans et al. (2008). Additionally we assume that it’s reasonable for policy to avoid even getting close to these states.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "The Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996).",
      "startOffset" : 170,
      "endOffset" : 225
    }, {
      "referenceID" : 0,
      "context" : "The Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996). But these papers address neither AI safety nor RL. Murata & Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution.",
      "startOffset" : 192,
      "endOffset" : 300
    }, {
      "referenceID" : 0,
      "context" : "The Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996). But these papers address neither AI safety nor RL. Murata & Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution. Outside of reinforcement learning, a number of papers address problems related to non-stationary data, including a book on covariate shift (Sugiyama & Kawanabe, 2012). Safety in Reinforcement Learning: Several papers address safety in RL. Garcıa & Fernández (2015) provide a thorough review on the topic, dividing the existing works into those which perturb the objective function and those which use external knowledge to improve the safety of exploration.",
      "startOffset" : 192,
      "endOffset" : 714
    }, {
      "referenceID" : 0,
      "context" : "The Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996). But these papers address neither AI safety nor RL. Murata & Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution. Outside of reinforcement learning, a number of papers address problems related to non-stationary data, including a book on covariate shift (Sugiyama & Kawanabe, 2012). Safety in Reinforcement Learning: Several papers address safety in RL. Garcıa & Fernández (2015) provide a thorough review on the topic, dividing the existing works into those which perturb the objective function and those which use external knowledge to improve the safety of exploration. Hans et al. (2008) defines a fatal state as one from which one receives a return below some safety threshold τ .",
      "startOffset" : 192,
      "endOffset" : 926
    }, {
      "referenceID" : 0,
      "context" : "The Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996). But these papers address neither AI safety nor RL. Murata & Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution. Outside of reinforcement learning, a number of papers address problems related to non-stationary data, including a book on covariate shift (Sugiyama & Kawanabe, 2012). Safety in Reinforcement Learning: Several papers address safety in RL. Garcıa & Fernández (2015) provide a thorough review on the topic, dividing the existing works into those which perturb the objective function and those which use external knowledge to improve the safety of exploration. Hans et al. (2008) defines a fatal state as one from which one receives a return below some safety threshold τ . They propose a solution comprised of two components. One, the safety function, identifies unsafe states. The other, denoted the backup model, is responsible for navigating away from the critical state. Their work does not address function approximation, instead focusing on a domain where a tabular approach is viable. Moldovan & Abbeel (2012) gives a definition of safety based on ergodicity.",
      "startOffset" : 192,
      "endOffset" : 1364
    }, {
      "referenceID" : 0,
      "context" : "The Stability of RL with Function Approximation: The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identified (Boyan & Moore, 1995; Baird et al., 1995; Gordon, 1996). But these papers address neither AI safety nor RL. Murata & Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution. Outside of reinforcement learning, a number of papers address problems related to non-stationary data, including a book on covariate shift (Sugiyama & Kawanabe, 2012). Safety in Reinforcement Learning: Several papers address safety in RL. Garcıa & Fernández (2015) provide a thorough review on the topic, dividing the existing works into those which perturb the objective function and those which use external knowledge to improve the safety of exploration. Hans et al. (2008) defines a fatal state as one from which one receives a return below some safety threshold τ . They propose a solution comprised of two components. One, the safety function, identifies unsafe states. The other, denoted the backup model, is responsible for navigating away from the critical state. Their work does not address function approximation, instead focusing on a domain where a tabular approach is viable. Moldovan & Abbeel (2012) gives a definition of safety based on ergodicity. They consider a fatality to be a state from which one cannot return to the start state. Shalev-Shwartz et al. (2016) provides theoretical analysis considering how strong a penalty should be to discourage accidents.",
      "startOffset" : 192,
      "endOffset" : 1531
    }, {
      "referenceID" : 19,
      "context" : "intrinsic motivation is proposed as a way to encourage exploration of an environment (Schmidhuber, 1991; Bellemare et al., 2016) and to acquire a modular set of skills Chentanez et al.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "intrinsic motivation is proposed as a way to encourage exploration of an environment (Schmidhuber, 1991; Bellemare et al., 2016) and to acquire a modular set of skills Chentanez et al.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "intrinsic motivation is proposed as a way to encourage exploration of an environment (Schmidhuber, 1991; Bellemare et al., 2016) and to acquire a modular set of skills Chentanez et al. (2004). In principle, such motivation can lead agents to explore intelligently even when extrinsic rewards are sparse.",
      "startOffset" : 105,
      "endOffset" : 192
    } ],
    "year" : 2016,
    "abstractText" : "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might be injured exhibit a special structure. We know a priori that catastrophes are not only bad, but that agents need not ever get near to a catastrophe state. In this paper, we exploit this structure to learn a reward shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective, shaping the reward function away from catastrophic states.",
    "creator" : "LaTeX with hyperref package"
  }
}