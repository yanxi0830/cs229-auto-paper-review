{
  "name" : "579.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Tao Lin", "Tian Guo", "Karl Aberer" ],
    "emails" : [ "karl.aberer}@epfl.ch" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Time series, which is a sequence of data points in time order, is being generated in a wide spectrum of domains, such as daily fluctuation of the stock market, power consumption records of households, performance monitoring data of clusters in data centres, and so on. In many applications, users are interested in understanding the evolving trend in time series and forecasting the trend, since the conventional prediction on specific data points could deliver very little information about the semantics and dynamics of the underlying process generating the time series. For instance, time series in Figure 1 are from the household power consumption dataset1. Figure 1(a) shows some raw data points of time series. Though pointA andB have approximately the same value, the underlying system is likely to be in two different states when it outputs A and B, because A is in an upward trend whileB is in a downward trend (Wang et al., 2011; Matsubara et al., 2014). On the other hand, even when two points with the similar value are both in the upward trend, e.g, point A and C, the different slopes and durations of the trends where point A and C locate, could also indicate different states of the underlying process.\nParticularly, in this paper we are interested in the local trend of time series which measures the intermediate local behaviour, i.e., upward or downward pattern of time series that characterized by the slope and duration (Wang et al., 2011). For instance, in Figure 1(b) the linear segments over raw data points of time series represent the local trends extracted from a real household power consumption time series. For the ease of presentation, we will use the term trend and local trend interchangeably in the rest of the paper. Learning and forecasting local trends are quite useful in a wide range of applications. For instance, in the stock market, due to its high volatility and noisy environment, in reality predicting stock price trends is preferred over the prediction of the stock market absolute values (Atsalakis & Valavanis, 2009). Predicting the local trend of stock price time series empowers\n∗These two authors contributed equally. 1 https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\ntraders to design profitable trading strategies (Chang et al., 2012b; Atsalakis & Valavanis, 2009). In the smart energy domain, knowing the predictive local trend of power consumption time series enables energy providers to schedule power supply and maximize energy utilization (Zhao & Magoulès, 2012).\nMeanwhile, in recent years neural networks have shown the dramatical power in a wide spectrum of domains, e.g., natural language processing, computer vision, speech recognition, time series analysis, etc. (Wang et al., 2016b; Sutskever et al., 2014; Yang et al., 2015; Lipton et al., 2015). For time series data, two mainstream architectures, convolutional neural network (CNN) and recurrent neural network (RNN) have been exploited in different time series related tasks, e.g., RNN in time series classification (Lipton et al., 2015) and CNN in activity recognition and snippet learning (Liu et al., 2015; Yang et al., 2015). RNN is powerful in discovering the dependency in sequence data (Jain et al., 2014; Graves, 2012) and particularly the Long Short-Term Memory (LSTM) RNN works well on sequence data with long-term dependencies (Chung et al., 2014; Hochreiter & Schmidhuber, 1997) due to the internal memory mechanism. CNN excels in exacting effective representation of local salience from raw data of time series by enforcing a local connectivity between neurons. (Yang et al., 2015; Hammerla et al., 2016).\nIn this paper, we focus on learning and forecasting the local trends in time series via neural networks. This involves learning different aspects of the data. On one hand, the sequence of historical local trends describes the long-term contextual information of time series and thus naturally affects the evolution of the following local trend. On the other hand, the recent raw data points of time series (Wang et al., 2011; Batal et al., 2012), which represent the local variation and behaviour of time series, affect the evolving of the following trend as well and have particular predictive power for abruptly changing local trends (Wang et al., 2011). For instance, in Figure 1(c), trend 1, 2 and 3 present a continuous upward pattern. Then when we aim at predicting the subsequent trend of time series at the end of the third local trend, the previous three successive upward trends outline a probable increasing trend afterwards. However, the local data around the end of the third trend, e.g., data points in the red circle, indicate that time series could stabilize and even decrease. The data points after the third trend indeed present a decreasing trend indicated by the red dotted segment. In this case, the subsequent trend has more dependency on the local data points. Therefore, it is highly desired to develop a systematic way to model such various hidden and complementary dependencies in time series for the local trend forecasting problem.\nTo this end, we propose a end-to-end hybrid neural network, referred to as TreNet. In particular, it consists of a LSTM recurrent neural network to capture the long dependency in historical local trends, a convolutional neural network to extract local features from local raw data of time series, and a feature fusion layer to learn joint representation to take advantage of both features drawn from CNN and LSTM. Such joint representation is used for the local trend forecasting. The experimental analysis on real datasets demonstrates that TreNet outperforms individual recurrent neural network, convolutional neural network and a variety of baselines in term of local trend prediction accuracy.\nThe rest of the paper is organized as follows. Section 2 presents related work, while Section 3 defines the problem to be solved and introduces the notations. In Section 4, we present the proposed TreNet. Section 5 demonstrates the performance of our method and baselines on real datasets. Finally, the paper is concluded in Section 6. Refer to Section 7 and Section 8 for more experiment results and discussion."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Traditional learning approaches over local trends of time series mainly make use of Hidden Markov Models (HMMs) (Wang et al., 2011; Matsubara et al., 2014). HMMs maintain short-term state dependences, i.e., the memoryless Markov property and predefined number of states, which requires significant task specific knowledge. RNNs instead use high dimensional, distributed hidden states that could take into account long-term dependencies in sequence data. Previous time series segmentation approaches (Keogh et al., 2001; Matsubara et al., 2014; Yuan, 2015) focus on achieving a meaningful segmentation and finding patterns, rather than modeling the relation in segments and therefore are not suitable for forecasting local trends. Multi-step ahead prediction is another way to realize local trend prediction by fitting the predicted values to estimate the local trend. However, multi-step ahead prediction is a non-trivial problem itself (Chang et al., 2012a). In this paper, we concentrate on directly learning local trends through neural networks.\nRNNs have recently shown promising results in a variety of applications, especially when there exist sequential dependencies in data (Lyu & Zhu, 2014; Chung et al., 2014; Sutskever et al., 2014). Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997; Lyu & Zhu, 2014; Chung et al., 2014), a class of recurrent neural networks with sophisticated recurrent hidden and gated units, are particularly successful and popular due to its ability to learn hidden long-term sequential dependencies. (Lipton et al., 2015) uses LSTMs to recognize patterns in multivariate time series, especially for multi-label classification of diagnoses. (Chauhan & Vig, 2015; Malhotra et al., 2015) evaluate the ability of LSTMs to detect anomalies in ECG time series. Bidirectional LSTM (Graves & Schmidhuber, 2005) is usually intended for speech processing rather than time series forecasting problems. Our paper focuses on using LSTM to capture the dependency in the sequence of historical local trends and meanwhile the hidden states in LSTM are further used to learn joint feature representations for the local trend forecasting.\nCNN is often used to learn effective representation of local salience from raw data (Vinyals et al., 2015; Donahue et al., 2015; Karpathy et al., 2014). (Hammerla et al., 2016; Yang et al., 2015; Lea et al., 2016) make use of CNNs to extract features from raw time series data for activity/action recognition. (Liu et al., 2015) focuses on the prediction of periodical time series values by using CNN and embedding time series with the potential neighbors in the temporal domain. Our proposed TreNet will combine the strengths of both LSTM and CNN and form a novel and unified neural network architecture for local trend forecasting.\nHybrid neural networks, which combines the strengths of various neural networks, are receiving increasing interest in the computer vision domain, such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Donahue et al., 2015), image classification (Wang et al., 2016a), protein structure prediction (Li & Yu, 2016), action recognition (Ballas et al., 2015; Donahue et al., 2015) and so on. But efficient exploitation of such hybrid architectures has not been well studied for time series data, especially the trend forecasting problem. (Li & Yu, 2016; Ballas et al., 2015) utilize CNNs over images in cascade of RNNs in order to capture the temporal features for classification. (Bashivan et al., 2015) transforms EEG data into a sequence of topology-preserving multi-spectral images and then trains a cascaded convolutional-recurrent network over such images for EEG classification. (Wang et al., 2016a; Mao et al., 2014) propose the CNN-RNN framework to learn a shared representation for image captioning and classification problems. In our proposed TreNet, LSTM and CNN first respectively learn the trend evolution and local raw data of time series and then TreNet fuses the features captured by LSTM and CNN to predict the trend."
    }, {
      "heading" : "3 PROBLEM FORMULATION",
      "text" : "In this section, we provide the formal definition of the trend learning and forecasting problem in this paper.\nWe define time series as a sequence of data points X = {x1, . . . , xT }, where each data point xt is real-valued and subscript t represents the time instant. The corresponding local trend sequence of X is a series of piecewise linear representations of X , denoted by T = {〈`k, sk〉}. Each element of T , e.g., 〈`k, sk〉 describes a linear function over a certain subsequence (or segment) of X and corresponds to a local trend in X . Such local trends in T are extracted from X by time series segmentation and fitting a linear function w.r.t. time t over each segment (Keogh et al., 2001; Wang\net al., 2011). `k and sk respectively represent the duration and slope of trend k. `k is measured in terms of the time range covered by trend k. Local trends in T are time ordered and non-overlapping. The durations of all the local trends in T address ∑ k `k = T . In addition, a local trend sequence\nending by time t is denoted by T (t) = {〈`k, sk〉 | ∑ k `k ≤ t}.\nMeanwhile, as we discussed in Section 1, local raw data of time series affects the varying of trend as well and thus we define the local data w.r.t. a certain time instant t as a sequence of data points in a window of size w, denoted by L(t) = {xt−w, . . . , xt}. At certain time t, trend forecasting is meant to predict the duration and slope of the following trend based on a given sequence of historical trends T (t) and local data set L(t). The predicted duration and slope at time t are denoted by ˆ̀t and ŝt. Our proposed TreNet can be trained for predicting either ˆ̀t or ŝt. For simplicity, we use ŷt to represent the predicted value of TreNet throughout the paper.\nTherefore, given the training datasetD = X∪T , we aim to propose a neural network based approach to learn a function ŷt = f(T (t),L(t)) for the trend forecasting. In this paper, we focus on univariate time series. The proposed method can be naturally generalized to multivariate time series as well by augmenting the input to the neural network. Refer to Section 8 for more discussion."
    }, {
      "heading" : "4 HYBRID NEURAL NETWORKS FOR TREND LEARNING AND FORECASTING",
      "text" : "In this section, we first present an overview about the proposed TreNet for the trend forecasting. Then we will detail the components of TreNet."
    }, {
      "heading" : "Overview.",
      "text" : "The idea of our TreNet is to combine CNN with LSTM to utilize their representation abilities on different aspects of training data D (D = X ∪ T ) and then to learn a joint feature for the trend prediction. Technically, TreNet is designed to learn a predictive function ŷt = f(R(T (t)), C(L(t))). R(T (t)) is derived by training the LSTM over sequence T to capture the dependency in the trend evolving, while C(L(t)) corresponds to local features extracted by CNN from L(t). The long-term and local features captured by LSTM and CNN, i.e., R(T (t)) and C(L(t)) convey complementary information pertaining to the trend varying. Therefore, the feature fusion layer is supposed to take advantages of both features to produce a fused representation for improved performance. Finally, the trend prediction is realized by the function f(·, ·), which corresponds to the feature fusion and output layers in Figure 2.\nLearning the dependency in the trend sequence. During the training phase, the duration `k and slope sk of each local trend k in sequence T are fed into the LSTM layer of TreNet. Each j-th neuron in the LSTM layer maintains a memory cjk at step k. The output hjk or the activation of this neuron is then expressed as (Hochreiter & Schmidhuber,\n1997; Chung et al., 2014):\nhjk = o j ktanh(c j k) (1)\nwhere ojk is an output gate and calculated as:\nojk = σ(Wo[`k sk] +Uohk−1 + Vock) j (2)\nwhere [`k sk] is the concatenation of the duration and slope of the trend k, hk−1 and ck are the vectorization of the activations of {hjk−1} and {c j k}, and σ is a logistic sigmoid function. Then, the memory cell cjk is updated through partially forgetting the existing memory and adding a new memory content c̃jk:\ncjk = f j kc j k−1 + i j k c̃ j k , c̃ j k = tanh(Wc[`k sk] +Uchk−1) j (3)\nThe extent to which the existing memory is forgotten is modulated by a forget gate f jk , and the degree to which the new memory content is added to the memory cell is modulated by an input gate ijk. Then, such gates are computed by\nf jk = σ(Wf [`k sk] +Ufhk−1 + Vfck−1) j (4)\nijk = σ(Wi[`k sk] +Uihk−1 + Vick−1) j (5)\nAt each step k, the hidden activation hk is the output to the feature fusion layer. Specifically, given a T (t) containing n local trends (i.e., |T (t)| = n), the output of R(T (t)) is R(T (t)) = hn.\nLearning features from the local raw data of time series. When the k-th trend in T is fed to LSTM, the corresponding local raw time series data input to\nthe CNN part of TreNet is L(t), where t = k∑\ni=1\n`i. CNN consists of H stacked layers of 1-d\nconvolutional, activation and pooling operations. Denote by ai the input signal of layer i and thus at the first layer a1 = L(t). Each layer has a specified number of filters ni of a specified filter size di. Each filter on a layer sweeps through the entire input signal to exact local features as follows:\nvi,jm = φ(b i,j + m+di/2∑ z=m−di/2 W i,jz a i z) ,∀m = 1, . . . , |ai| (6)\nwhere vi,jm is the activation of j-th filter of layer i on m position of the input signal. Here φ is the Leaky Rectified Linear Unit, which is shown to perform better (Xu et al., 2015). Then the maxpooling is performed over the vi,jm of each filter.\nFinally, the output of CNN in TreNet is the concatenation of max-pooling of each filter on the last layer H , namely:\nC(L(t)) = [p1, . . . , pn H\n], pj = [ max 1≤z≤q ({vH,jm+z})], ∀j = 1, . . . , nH (7)\nwhere q is the pooling size.\nFeature fusion and output layers. The feature fusion layer combines the representations R(T (t)) and C(L(t)), to form a joint feature. Then, such joint feature is fed to the output layer to provide the trend prediction. Particularly, we first map R(T (t)) and C(L(t)) to the same feature space and add them together to obtain the activation of the feature fusion layer (Mao et al., 2014). The output layer is a fully-connect layer following the feature fusion layer. Mathematically, the prediction of TreNet is expressed as:\nŷt = f(R(T (t)), C(L(t))) =W o · φ(W r ·R(T (t)) +W c · C(L(t)))︸ ︷︷ ︸ feature fusion +bo (8)\nwhere φ(·) is element-wise leaky ReLU activation function and + denotes the element-wise addition. W o and bo are the weights and bias of the output layer.\nTo train TreNet, we adopt the squared error function plus a regularization term as:\nJ(W , b ; T ,X ) = 1 |T | |T |∑ k=1 (ŷk − yk)2 + λ‖W ‖2 (9)\nwhere W and b represent the weight and bias parameters in TreNet, λ is a hyperparameter for the regularization term and yk is the true value of trend slope or duration.\nThe cost function is differentiable and the architecture of TreNet allows the gradients from the loss function (9) to be backpropagated to both LSTM and CNN parts. TreNet can be trained respectively for the slope and duration of local trends using T and X . When performing forecasting, T (t) and L(t) are fed to TreNet and the prediction value ŷk could be either the slope or duration depending on the training target."
    }, {
      "heading" : "5 EXPERIMENTAL ANALYSIS",
      "text" : "In this section, we conduct extensive experiments to demonstrate the prediction performance of TreNet by comparing to a variety of baselines. Due to the page limit, refer to Section 7 for more experiment results."
    }, {
      "heading" : "5.1 EXPERIMENT SETUP",
      "text" : "Dataset: We test our method and baselines on three real time series datasets.\n• Daily Household Power Consumption (HousePC). This dataset2 contains measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available. We use the voltage time series throughout the experiments.\n• Gas Sensor (GasSensor). This dataset3 contains the recordings of chemical sensors exposed to dynamic gas mixtures at varying concentrations. The measurement was constructed by the continuous acquisition of the sensor array signals for a duration of about 12 hours without interruption. We mainly use the gas mixture time series regarding Ethylene and Methane in air.\n• Stock Transaction (Stock): This dataset is extracted from Yahoo Finance and contains the daily stock transaction information in New York Stock Exchange from 1950-10 to 2016-4.\nAll datasets are preprocessed by (Keogh et al., 2001) to extract local trends. Alternative time series segmentation and local trend extraction approaches can be used as well. We choose (Keogh et al., 2001) here due to its high efficiency. Totally, we obtain 42591, 4720 and 1316 local trends respectively from above datasets. For the ease of experimental result interpretation, the slope of extracted local trends is represented by the angle of the corresponding linear function and thus in a bounded value range [−90, 90]. The duration of local trends is measured by the number of data points within the local trend. Then, the obtained trend sequences and the set of local data are split into training (80%), validation (10%) and test (10%) datasets.\nBaselines: We compare TreNet with the following six baselines:\n• CNN. This baseline method predicts the trend by only using CNN over the set of local raw data of time series to learn features for the forecasting. The size of local data is set at w as is defined in Section 3.\n• LSTM. This method uses LSTM to learn dependencies in the trend sequence T and predicts the trend only using the trained LSTM.\n• Support Vector Regression (SVR). A family of support vector regression based approaches with different kernel methods is used for the trend forecasting. We consider three\n2 https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption 3 https://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+under+dynamic+gas+mixtures\ncommonly used kernels (Liu et al., 2015), i.e., Radial Basis kernel (SVRBF), Polynomial kernel (SVPOLY), Sigmoid kernel (SVSIG). The trend sequence and the corresponding set of local time series data are concatenated as the input features to such SVR approaches.\n• Pattern-based Hidden Markov Model (pHMM). (Wang et al., 2011) proposed a patternbased hidden Markov model (HMM), which segments the time series and models the dependency in segments via HMM. The derived HMM model is used to predict the state of time series and then to estimate the trend based on the state.\n• Naive. This is the naive approach which takes the duration and slope of the last trend as the prediction for the next one.\n• ConvNet+LSTM(CLSTM). It is based on the cascade structure of ConvNet and LSTM in (Bashivan et al., 2015) which feeds the features learnt by ConvNet over time series to a LSTM and obtains the prediction from the LSTM.\nEvaluation metric: We evaluate the predictive performance of TreNet and baselines in terms of Root Mean Square Error (RMSE). The lower the RMSE, the more accurate the predictions.\nTraining: The training procedure of TreNet and baselines in our paper follows the schema below.\nThe CNN and LSTM components in TreNet share the same network structure (e.g., number of layers, neurons in each layer) as CNN and LSTM baselines. CNN has two stacked convolutional layers, which have 32 filters of size 2 and 4. The number of memory cells in LSTM is 600. For baseline CNN and LSTM, we tune the learning rate for each approach from {10−1, 10−2, 10−3, 10−4, 10−5} (Sutskever et al., 2013), in order to achieve the least prediction errors and then fix the learning rate. For TreNet, in addition to the learning rate, the number of neurons in the feature fusion layer is chosen from the range {300, 600, 900, 1200} to achieve the best performance. We use dropout and L2 regularization to control the capacity of neural networks to prevent overfitting, and set the values to 0.5 and 5 × 10−4 respectively for all datasets (Mao et al., 2014). The Adam optimizer (Kingma & Ba, 2014) is chosen to learn the weights in neural networks.\nRegarding the SVR based approaches, we carefully tune the parameters c (error penalty), d (degree of kernel function), and γ (kernel coefficient) for kernels. Each parameter is selected from the sets c ∈ {10−5, 10−4, . . . , 1, . . . , 104, 105}, d ∈ {1, 2, 3}, γ ∈ {10−5, 10−4, . . . , 1, . . . , 105} respectively. We iterate through candidate values of each combination of c, d and γ to train our model, and keep the parameters that generate the lowest RMSE on the validation set, and then use them to predict on the test set.\nThe training datasets of SVR and pHMM baselines are consistent as that of TreNet. Likewise, CNN and LSTM baselines are respectively fed by the set of local data and the trend sequence of the same size as TreNet. In addition, since the window size of local data is tunable, we vary the window size of local data, i.e. w, from the range {100, 300, 500, 700, 900}, so as to investigate how the size of local data influences the predication performance. The results will be presented in Section 5.2. The model’s performance on the validation set will be evaluated after each epoch of training. Each model is trained for at least 50 epochs. Meanwhile, the training process adopts early stopping if no further improvement in the performance of validation shows up after 50 epochs."
    }, {
      "heading" : "5.2 EXPERIMENT RESULTS",
      "text" : "Table 1 studies the prediction performances of TreNet and baselines. For each dataset, the window size of local data is constant for approaches (i.e., CNN, SVRBF, SVPOLY, SVSIG, pHMM and TreNet) that take local data as input. Then, the results of each approach are obtained by tuning the corresponding parameter as described in Section 5.1.\nIn Table 1, we observe that TreNet consistently outperforms baselines on the duration and slope prediction by achieving around 30% less errors at the maximum. It verifies that the hybrid architecture of TreNet can improve the performance by utilizing the information captured by both CNN and LSTM. Specifically, pHMM method performs worse due to the limited representation capability of HMM. On the slope prediction, SVR based approaches can get comparable results as TreNet.\nIn the following group of experiments, we investigate the effect of local data size (i.e., w) on the prediction. In particular, we tune the value of local data size for the approaches whose input fea-\ntures contains local data and observe the prediction errors. Such approaches include CNN, SVRBF, SVPOLY, SVSIG, pHMM and TreNet. LSTM only consumes the trend sequence and thus is not included. Due to the page limit, we report the results on the HousePC dataset in Table 2 and Table 3. The results on Stock and GasSensor datasets can be referred to Section 7.\nBaseline Naive has no original time series data as input CLSTM works on the whole time series and has no local data. Thus they are excluded from this set of experiments.\nIn Table 2, we observe that compared to baselines TreNet has the lowest errors on the duration prediction across different window sizes. pHMM requires sufficient data points to model the relations of segments and fails to work on 100 size. As the window size increases and more local data points are fed to the training process, the prediction errors of CNN and TreNet decrease or nearly stabilize. This could be because only the certain amount of local data has predictive power. The filtering and pooling mechanism enables CNN to focus on the certain local data having strong predictive power and thus giving more local data only gives rise to marginal improvements. Such similar phenomenon is observed on the slope prediction as is shown in Table 3. For more results and discussion, please refer to Section 7."
    }, {
      "heading" : "Window Size CNN SVRBF SVPOLY SVSIG pHMM TreNet",
      "text" : ""
    }, {
      "heading" : "Window Size CNN SVRBF SVPOLY SVSIG pHMM TreNet",
      "text" : ""
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper we propose TreNet, a novel hybrid neural network to learn and predict the local trend behaviour of time series. The experimental results demonstrate that such a hybrid framework can indeed utilize complementary information extracted by CNN and LSTM to enhance the prediction performance. Moreover, such architecture is generic and extendible in that additional exogenous time series can be fed to TreNet, so as to boost the performance and investigate the effect of different data sources on the trend evolving."
    }, {
      "heading" : "7 APPENDIX",
      "text" : ""
    }, {
      "heading" : "7.1 DATA PRE-PROCESSING",
      "text" : "In this part, we describe the data pre-processing, which extracts the local trend sequence from raw time series data for the subsequent neural network training and testing.\nWe convert the raw time series data into a piecewise linear representation, namely consecutive segments (Keogh et al., 2001; Wang et al., 2011). Each segment corresponds to a local trend and is fitted by a linear function of time series value w.r.t. time, e.g., xt = β1t+β0+ over the time range [t1, t2) of this segment. Then, the slope and duration are derived from the coefficient β1 and [t1, t2).\nTechnically, we adopt the bottom-up approach in (Keogh et al., 2001), since it can achieve lower approximate errors compared with top-down and sliding window methods. The process is illustrated in Figure 3. Initially, we approximate time series X with bT2 c line segments (T is the length of the\ntime series). Then, we iteratively merge the neighbouring segments to build longer ones. In each iteration, neighbouring segments with the minimal approximation error are merged into a new one. The merging process repeats until every possible merge gives rise to a segment with errors above a specified threshold. We use the relative mean squared error as the error metric and specify the threshold as 0.05."
    }, {
      "heading" : "7.2 ADDITIONAL EXPERIMENT RESULTS",
      "text" : "In this group of experiments, we visualize the trend prediction using the sample testing data instance from each dataset in Figure 4. We can observe that in HousePC TreNet successfully predicts the changed trend, though there are successive upward trends before. In Stock and GasSensor datasets, the succeeding upward and downward trends are correctly predicted as well."
    }, {
      "heading" : "Window Size CNN SVRBF SVPOLY SVSIG pHMM TreNet",
      "text" : ""
    }, {
      "heading" : "Window Size CNN SVRBF SVPOLY SVSIG pHMM TreNet",
      "text" : "Then, we provide the RMSE w.r.t. the varying window size on Stock and GasSensor datasets in Table 4, Table 5, Table 6 and Table 7.\nFrom the results, we observe that TreNet outperforms baselines almost on all window sizes. Meanwhile, the prediction errors often present the decreasing and stable pattern as the window size varies.\nWindow size of local data: The observation in above experiments w.r.t. the varying window size provides inspiration for choosing the window size of local data. Given the training dataset, we can find out the maximum duration of local trends and takes it as the local data size. This is because doing so can ensure that the range of local data in each training instance can cover the most recent local trend, whose raw data is believed to have strong predictive power for the subsequent trend. Additionally, we observe that setting the window size of local data of CNN and TreNet in this way can achieve comparable prediction errors compared to the cases with larger window sizes ."
    }, {
      "heading" : "Window Size CNN SVRBF SVPOLY SVSIG pHMM TreNet",
      "text" : ""
    }, {
      "heading" : "Window Size CNN SVRBF SVPOLY SVSIG pHMM TreNet",
      "text" : ""
    }, {
      "heading" : "8 DISCUSSION",
      "text" : "For multivariate time series, we can augment the input of TreNet by including the trend sequences and local data of exogenous time series and then train TreNet for a certain target time series to predict its trend. Another line of research is to explore equipping TreNet with multi-task learning. This is motivated by the observation that if we decompose the trend forecasting problem into classification and regression respectively for the slope and duration, we can utilize the correlation between slope\nand duration to boost the prediction performance. In addition, there could be alternative frameworks to combine the outputs of CNN and LSTM and our work opens the door for applying hybrid neural networks for trend analysis in time series."
    } ],
    "references" : [ {
      "title" : "Forecasting stock market short-term trends using a neuro-fuzzy based methodology",
      "author" : [ "George S Atsalakis", "Kimon P Valavanis" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Atsalakis and Valavanis.,? \\Q2009\\E",
      "shortCiteRegEx" : "Atsalakis and Valavanis.",
      "year" : 2009
    }, {
      "title" : "Delving deeper into convolutional networks for learning video representations",
      "author" : [ "Nicolas Ballas", "Li Yao", "Chris Pal", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1511.06432,",
      "citeRegEx" : "Ballas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ballas et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning representations from eeg with deep recurrent-convolutional neural networks",
      "author" : [ "Pouya Bashivan", "Irina Rish", "Mohammed Yeasin", "Noel Codella" ],
      "venue" : "arXiv preprint arXiv:1511.06448,",
      "citeRegEx" : "Bashivan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bashivan et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforced two-step-ahead weight adjustment technique for online training of recurrent neural networks",
      "author" : [ "Li-Chiu Chang", "Pin-An Chen", "Fi-John Chang" ],
      "venue" : "IEEE transactions on neural networks and learning systems,",
      "citeRegEx" : "Chang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2012
    }, {
      "title" : "A novel model by evolving partially connected neural network for stock price trend forecasting",
      "author" : [ "Pei-Chann Chang" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Chang,? \\Q2012\\E",
      "shortCiteRegEx" : "Chang",
      "year" : 2012
    }, {
      "title" : "Anomaly detection in ecg time signals via deep long shortterm memory networks",
      "author" : [ "Sucheta Chauhan", "Lovekesh Vig" ],
      "venue" : "In Data Science and Advanced Analytics (DSAA),",
      "citeRegEx" : "Chauhan and Vig.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chauhan and Vig.",
      "year" : 2015
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.3555,",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Donahue et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Supervised Sequence Labelling with Recurrent Neural Networks",
      "author" : [ "A. Graves" ],
      "venue" : "Studies in Computational Intelligence. Springer,",
      "citeRegEx" : "Graves.,? \\Q2012\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2012
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Graves and Schmidhuber.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2005
    }, {
      "title" : "Deep, convolutional, and recurrent models for human activity recognition using wearables",
      "author" : [ "Nils Y Hammerla", "Shane Halloran", "Thomas Ploetz" ],
      "venue" : "arXiv preprint arXiv:1604.08880,",
      "citeRegEx" : "Hammerla et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hammerla et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A review of online learning in supervised neural networks",
      "author" : [ "Lakhmi C Jain", "Manjeevan Seera", "Chee Peng Lim", "P Balasubramaniam" ],
      "venue" : "Neural Computing and Applications,",
      "citeRegEx" : "Jain et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li FeiFei" ],
      "venue" : "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "An online algorithm for segmenting time series",
      "author" : [ "Eamonn Keogh", "Selina Chu", "David Hart", "Michael Pazzani" ],
      "venue" : "In Data Mining,",
      "citeRegEx" : "Keogh et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Keogh et al\\.",
      "year" : 2001
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Temporal convolutional networks: A unified approach to action segmentation",
      "author" : [ "Colin Lea", "Rene Vidal", "Austin Reiter", "Gregory D Hager" ],
      "venue" : "arXiv preprint arXiv:1608.08242,",
      "citeRegEx" : "Lea et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lea et al\\.",
      "year" : 2016
    }, {
      "title" : "Protein secondary structure prediction using cascaded convolutional and recurrent neural networks",
      "author" : [ "Zhen Li", "Yizhou Yu" ],
      "venue" : "arXiv preprint arXiv:1604.07176,",
      "citeRegEx" : "Li and Yu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li and Yu.",
      "year" : 2016
    }, {
      "title" : "Learning to diagnose with lstm recurrent neural networks",
      "author" : [ "Zachary C Lipton", "David C Kale", "Charles Elkan", "Randall Wetzell" ],
      "venue" : "arXiv preprint arXiv:1511.03677,",
      "citeRegEx" : "Lipton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lipton et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal embedding in convolutional neural networks for robust learning of abstract snippets",
      "author" : [ "Jiajun Liu", "Kun Zhao", "Brano Kusy", "Ji-rong Wen", "Raja Jurdak" ],
      "venue" : "arXiv preprint arXiv:1502.05113,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisit long short-term memory: An optimization perspective",
      "author" : [ "Qi Lyu", "Jun Zhu" ],
      "venue" : "In Advances in neural information processing systems workshop on deep Learning and representation Learning,",
      "citeRegEx" : "Lyu and Zhu.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lyu and Zhu.",
      "year" : 2014
    }, {
      "title" : "Long short term memory networks for anomaly detection in time series",
      "author" : [ "Pankaj Malhotra", "Lovekesh Vig", "Gautam Shroff", "Puneet Agarwal" ],
      "venue" : "In European Symposium on Artificial Neural Networks,",
      "citeRegEx" : "Malhotra et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Malhotra et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "author" : [ "Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille" ],
      "venue" : "arXiv preprint arXiv:1412.6632,",
      "citeRegEx" : "Mao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2014
    }, {
      "title" : "Autoplait: Automatic mining of coevolving time sequences",
      "author" : [ "Yasuko Matsubara", "Yasushi Sakurai", "Christos Faloutsos" ],
      "venue" : "In Proceedings of the 2014 ACM SIGMOD international conference on Management of data,",
      "citeRegEx" : "Matsubara et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Matsubara et al\\.",
      "year" : 2014
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 30th international conference on machine learning",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Cnn-rnn: A unified framework for multi-label image classification",
      "author" : [ "Jiang Wang", "Yi Yang", "Junhua Mao", "Zhiheng Huang", "Chang Huang", "Wei Xu" ],
      "venue" : "arXiv preprint arXiv:1604.04573,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Morphological segmentation with window lstm neural networks",
      "author" : [ "Linlin Wang", "Zhu Cao", "Yu Xia", "Gerard de Melo" ],
      "venue" : "In Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding semantics in time series",
      "author" : [ "Peng Wang", "Haixun Wang", "Wei Wang" ],
      "venue" : "In Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,",
      "citeRegEx" : "Wang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Empirical evaluation of rectified activations in convolutional network",
      "author" : [ "Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li" ],
      "venue" : "arXiv preprint arXiv:1505.00853,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep convolutional neural networks on multichannel time series for human activity recognition",
      "author" : [ "Jian Bo Yang", "Minh Nhut Nguyen", "Phyo Phyo San", "Xiao Li Li", "Shonali Krishnaswamy" ],
      "venue" : "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised machine condition monitoring using segmental hidden markov models",
      "author" : [ "Chao Yuan" ],
      "venue" : "In Proceedings of the 24th International Conference on Artificial Intelligence,",
      "citeRegEx" : "Yuan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yuan.",
      "year" : 2015
    }, {
      "title" : "A review on the prediction of building energy consumption",
      "author" : [ "Hai-xiang Zhao", "Frédéric Magoulès" ],
      "venue" : "Renewable and Sustainable Energy Reviews,",
      "citeRegEx" : "Zhao and Magoulès.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhao and Magoulès.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Though pointA andB have approximately the same value, the underlying system is likely to be in two different states when it outputs A and B, because A is in an upward trend whileB is in a downward trend (Wang et al., 2011; Matsubara et al., 2014).",
      "startOffset" : 203,
      "endOffset" : 246
    }, {
      "referenceID" : 23,
      "context" : "Though pointA andB have approximately the same value, the underlying system is likely to be in two different states when it outputs A and B, because A is in an upward trend whileB is in a downward trend (Wang et al., 2011; Matsubara et al., 2014).",
      "startOffset" : 203,
      "endOffset" : 246
    }, {
      "referenceID" : 29,
      "context" : ", upward or downward pattern of time series that characterized by the slope and duration (Wang et al., 2011).",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "(Wang et al., 2016b; Sutskever et al., 2014; Yang et al., 2015; Lipton et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 84
    }, {
      "referenceID" : 31,
      "context" : "(Wang et al., 2016b; Sutskever et al., 2014; Yang et al., 2015; Lipton et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "(Wang et al., 2016b; Sutskever et al., 2014; Yang et al., 2015; Lipton et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : ", RNN in time series classification (Lipton et al., 2015) and CNN in activity recognition and snippet learning (Liu et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : ", 2015) and CNN in activity recognition and snippet learning (Liu et al., 2015; Yang et al., 2015).",
      "startOffset" : 61,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : ", 2015) and CNN in activity recognition and snippet learning (Liu et al., 2015; Yang et al., 2015).",
      "startOffset" : 61,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "RNN is powerful in discovering the dependency in sequence data (Jain et al., 2014; Graves, 2012) and particularly the Long Short-Term Memory (LSTM) RNN works well on sequence data with long-term dependencies (Chung et al.",
      "startOffset" : 63,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "RNN is powerful in discovering the dependency in sequence data (Jain et al., 2014; Graves, 2012) and particularly the Long Short-Term Memory (LSTM) RNN works well on sequence data with long-term dependencies (Chung et al.",
      "startOffset" : 63,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : ", 2014; Graves, 2012) and particularly the Long Short-Term Memory (LSTM) RNN works well on sequence data with long-term dependencies (Chung et al., 2014; Hochreiter & Schmidhuber, 1997) due to the internal memory mechanism.",
      "startOffset" : 133,
      "endOffset" : 185
    }, {
      "referenceID" : 31,
      "context" : "(Yang et al., 2015; Hammerla et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "(Yang et al., 2015; Hammerla et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : "On the other hand, the recent raw data points of time series (Wang et al., 2011; Batal et al., 2012), which represent the local variation and behaviour of time series, affect the evolving of the following trend as well and have particular predictive power for abruptly changing local trends (Wang et al.",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : ", 2012), which represent the local variation and behaviour of time series, affect the evolving of the following trend as well and have particular predictive power for abruptly changing local trends (Wang et al., 2011).",
      "startOffset" : 198,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "Traditional learning approaches over local trends of time series mainly make use of Hidden Markov Models (HMMs) (Wang et al., 2011; Matsubara et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "Traditional learning approaches over local trends of time series mainly make use of Hidden Markov Models (HMMs) (Wang et al., 2011; Matsubara et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "Previous time series segmentation approaches (Keogh et al., 2001; Matsubara et al., 2014; Yuan, 2015) focus on achieving a meaningful segmentation and finding patterns, rather than modeling the relation in segments and therefore are not suitable for forecasting local trends.",
      "startOffset" : 45,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Previous time series segmentation approaches (Keogh et al., 2001; Matsubara et al., 2014; Yuan, 2015) focus on achieving a meaningful segmentation and finding patterns, rather than modeling the relation in segments and therefore are not suitable for forecasting local trends.",
      "startOffset" : 45,
      "endOffset" : 101
    }, {
      "referenceID" : 32,
      "context" : "Previous time series segmentation approaches (Keogh et al., 2001; Matsubara et al., 2014; Yuan, 2015) focus on achieving a meaningful segmentation and finding patterns, rather than modeling the relation in segments and therefore are not suitable for forecasting local trends.",
      "startOffset" : 45,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "RNNs have recently shown promising results in a variety of applications, especially when there exist sequential dependencies in data (Lyu & Zhu, 2014; Chung et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 133,
      "endOffset" : 194
    }, {
      "referenceID" : 25,
      "context" : "RNNs have recently shown promising results in a variety of applications, especially when there exist sequential dependencies in data (Lyu & Zhu, 2014; Chung et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 133,
      "endOffset" : 194
    }, {
      "referenceID" : 6,
      "context" : "Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997; Lyu & Zhu, 2014; Chung et al., 2014), a class of recurrent neural networks with sophisticated recurrent hidden and gated units, are particularly successful and popular due to its ability to learn hidden long-term sequential dependencies.",
      "startOffset" : 30,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "(Lipton et al., 2015) uses LSTMs to recognize patterns in multivariate time series, especially for multi-label classification of diagnoses.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "(Chauhan & Vig, 2015; Malhotra et al., 2015) evaluate the ability of LSTMs to detect anomalies in ECG time series.",
      "startOffset" : 0,
      "endOffset" : 44
    }, {
      "referenceID" : 26,
      "context" : "CNN is often used to learn effective representation of local salience from raw data (Vinyals et al., 2015; Donahue et al., 2015; Karpathy et al., 2014).",
      "startOffset" : 84,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "CNN is often used to learn effective representation of local salience from raw data (Vinyals et al., 2015; Donahue et al., 2015; Karpathy et al., 2014).",
      "startOffset" : 84,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "CNN is often used to learn effective representation of local salience from raw data (Vinyals et al., 2015; Donahue et al., 2015; Karpathy et al., 2014).",
      "startOffset" : 84,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "(Hammerla et al., 2016; Yang et al., 2015; Lea et al., 2016) make use of CNNs to extract features from raw time series data for activity/action recognition.",
      "startOffset" : 0,
      "endOffset" : 60
    }, {
      "referenceID" : 31,
      "context" : "(Hammerla et al., 2016; Yang et al., 2015; Lea et al., 2016) make use of CNNs to extract features from raw time series data for activity/action recognition.",
      "startOffset" : 0,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "(Hammerla et al., 2016; Yang et al., 2015; Lea et al., 2016) make use of CNNs to extract features from raw time series data for activity/action recognition.",
      "startOffset" : 0,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "(Liu et al., 2015) focuses on the prediction of periodical time series values by using CNN and embedding time series with the potential neighbors in the temporal domain.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 22,
      "context" : "Hybrid neural networks, which combines the strengths of various neural networks, are receiving increasing interest in the computer vision domain, such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Donahue et al., 2015), image classification (Wang et al.",
      "startOffset" : 171,
      "endOffset" : 233
    }, {
      "referenceID" : 26,
      "context" : "Hybrid neural networks, which combines the strengths of various neural networks, are receiving increasing interest in the computer vision domain, such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Donahue et al., 2015), image classification (Wang et al.",
      "startOffset" : 171,
      "endOffset" : 233
    }, {
      "referenceID" : 7,
      "context" : "Hybrid neural networks, which combines the strengths of various neural networks, are receiving increasing interest in the computer vision domain, such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Donahue et al., 2015), image classification (Wang et al.",
      "startOffset" : 171,
      "endOffset" : 233
    }, {
      "referenceID" : 1,
      "context" : ", 2016a), protein structure prediction (Li & Yu, 2016), action recognition (Ballas et al., 2015; Donahue et al., 2015) and so on.",
      "startOffset" : 75,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : ", 2016a), protein structure prediction (Li & Yu, 2016), action recognition (Ballas et al., 2015; Donahue et al., 2015) and so on.",
      "startOffset" : 75,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "(Li & Yu, 2016; Ballas et al., 2015) utilize CNNs over images in cascade of RNNs in order to capture the temporal features for classification.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "(Bashivan et al., 2015) transforms EEG data into a sequence of topology-preserving multi-spectral images and then trains a cascaded convolutional-recurrent network over such images for EEG classification.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 22,
      "context" : "(Wang et al., 2016a; Mao et al., 2014) propose the CNN-RNN framework to learn a shared representation for image captioning and classification problems.",
      "startOffset" : 0,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "Here φ is the Leaky Rectified Linear Unit, which is shown to perform better (Xu et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 22,
      "context" : "Particularly, we first map R(T (t)) and C(L(t)) to the same feature space and add them together to obtain the activation of the feature fusion layer (Mao et al., 2014).",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "All datasets are preprocessed by (Keogh et al., 2001) to extract local trends.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "We choose (Keogh et al., 2001) here due to its high efficiency.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "commonly used kernels (Liu et al., 2015), i.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 29,
      "context" : "(Wang et al., 2011) proposed a patternbased hidden Markov model (HMM), which segments the time series and models the dependency in segments via HMM.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "It is based on the cascade structure of ConvNet and LSTM in (Bashivan et al., 2015) which feeds the features learnt by ConvNet over time series to a LSTM and obtains the prediction from the LSTM.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "For baseline CNN and LSTM, we tune the learning rate for each approach from {10−1, 10−2, 10−3, 10−4, 10−5} (Sutskever et al., 2013), in order to achieve the least prediction errors and then fix the learning rate.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "5 and 5 × 10−4 respectively for all datasets (Mao et al., 2014).",
      "startOffset" : 45,
      "endOffset" : 63
    } ],
    "year" : 2017,
    "abstractText" : "Local trends of time series characterize the intermediate upward and downward patterns of time series. Learning and forecasting the local trend in time series data play an important role in many real applications, ranging from investing in the stock market, resource allocation in data centers and load schedule in smart grid. Inspired by the recent successes of neural networks, in this paper we propose TreNet, a novel end-to-end hybrid neural network that predicts the local trend of time series based on local and global contextual features. TreNet leverages convolutional neural networks (CNNs) to extract salient features from local raw data of time series. Meanwhile, considering long-range dependencies existing in the sequence of historical local trends, TreNet uses a long-short term memory recurrent neural network (LSTM) to capture such dependency. Furthermore, for predicting the local trend, a feature fusion layer is designed in TreNet to learn joint representation from the features captured by CNN and LSTM. Our proposed TreNet demonstrates its effectiveness by outperforming conventional CNN, LSTM, HMM method and various kernel based baselines on real datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}