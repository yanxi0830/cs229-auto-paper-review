{
  "name" : "605.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TREE-STRUCTURED VARIATIONAL AUTOENCODER",
    "authors" : [ "Richard Shin", "Alexander A. Alemi", "Geoffrey Irving" ],
    "emails" : [ "ricshin@cs.berkeley.edu", "alemi@google.com", "geoffreyi@google.com", "vinyals@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "A significant amount of recent and ongoing work has explored the use of neural networks for modeling and generating various kinds of data. Newer techniques like the variational autoencoder (Rezende et al., 2014; Kingma & Welling, 2013) and generative-adversarial networks (Goodfellow et al., 2014) enable training of graphical models where the likelihood function is a complicated neural network which normally makes it infeasible to specify and optimize the marginal distribution analytically. Another family of techniques involves choosing an ordering of the dimensions of the data (which is particularly natural for sequences such as sentences) and training a neural network to estimate the distribution over the value of the next dimension given all the dimensions we have observed so far.\nThese techniques have led to significant advances in modeling images, text, sounds, and other kinds of complicated data. Language modeling with sequential neural models have halved perplexity (roughly, the error at predicting each word) compared to n-gram methods (Jozefowicz et al., 2016). Neural machine translation using sequence-to-sequence methods have closed half of the gap in quality between prior machine translation efforts and human translation (Wu et al., 2016). Generative image models have similarly progressed such that they can generate samples largely indistinguishable from the original data, at least for relatively small and simple images (Gregor et al., 2015; 2016; Kingma et al., 2016; Salimans et al., 2016; van den Oord et al., 2016), although the quality of the model here is harder to measure in an automated way (Theis et al., 2015).\nHowever, many kinds of data we might wish to model are naturally structured as a tree. Computer program code follows a rigorous grammar, and the usual first step in processing it involves parsing it into an abstract syntax tree, which simultaneously discards aspects of the code irrelevant to the semantics such as whitespace and extraneous parentheses, and makes it more convenient to further interpret, analyze, or manipulate. Statements made in formal logic similarly have a hierarchical structure, which determines arguments to predicates and functions, the scoping of variables and quantifiers, and the application of logical connectives. Natural language sentences also contain a latent syntactic structure which is necessary for determining the meaning of the sentence, although\n∗Majority of work done while at Google.\nmost sentences admit many possible parses due to the multiple meanings of each word and ambiguous groupings.\nIn this paper, we explore how we can adapt the variational autoencoder to modeling tree-structured data. In general, it is possible to treat the tree as a sequence and then use a sequential model. However, a model which follows the structure of the tree may better capture long-range dependencies: recurrent models sometimes have trouble learning to remember and use information from the distant past when it is relevant to the current context, but these distant parts of the input may be close to each other within the tree structure.\nIn our proposed approach, we decide the identity of each node in the tree by using a top-down recursive neural network, causing the distributed representation which decides the identity of each node in the tree to be computed as a function of the identity and relative location of its parent nodes.\nBy using the architecture of the variational autoencoder (Rezende et al., 2014; Kingma & Welling, 2013), our model can learn to capture various features of the trees within continuous latent variables, which are added as further inputs into the top-down recursive neural network and conditions the overall generation process. These latent variables allow us to generate different parts of the tree in parallel; specifically, given a parent node n and its children c1 and c2, the generation of (the distribution placed over different values of) c1 and its descendants is independent of the generation of c2 and its descendants (and vice versa), once we condition upon the latent variables. By structuring the model this way, while our model generates one dimension (the identity of each node within the tree) of the data a time, it is not autoregressive as the probability distribution for one dimension is not a function of the previously generated nodes.\nWe evaluate our model on a variety of datasets, some synthetic and some real. Our experimental results show that it achieves comparable test set log likelihood to autoregressive sequential models which do not use any latent variables, while offering the following properties:\n• For balanced trees, generation requires O(log n) rather than O(n) timesteps required for a sequential model because the children of each node can be generated in parallel. • It is straightforward to resample a subtree while keeping the other parts of the tree intact. • The generated trees are syntactically valid by construction. • The model produces a latent representation for each tree, which may prove useful in other\napplications."
    }, {
      "heading" : "2 BACKGROUND AND RELATED WORK",
      "text" : ""
    }, {
      "heading" : "2.1 TREE-STRUCTURED MODELS",
      "text" : "Recursive neural nets, which processes a tree in a bottom-up way, have been popular in natural language processing for a variety of tasks, such as sentiment analysis (Socher et al., 2013), question answering (Iyyer et al., 2014), and semantic relation extraction (Socher et al., 2012). Starting from the leaves of the tree, the model computes a representation for each node by combining the representations of its child nodes. In the case of natural language processing, each tree typically represents one sentence, with the leaf nodes corresponding to the words in the sentence and the structure of the internal nodes determined by the constituency parse tree for the sentence.\nIf we restrict ourselves to binary trees (given that it is possible to binarize arbitrary trees in a lossless way), the we compute the k-dimensional representation rn ∈ Rk for a node n by combining the representations of nodes nleft and nright:\nrn = f(Wrnleft + V rnright)\nwhere W and V are square matrices (in Rd×d) and f is a nonlinear activation function, applied elementwise. Leaf nodes are represented by embedding the content of the leaf node into a ddimensional vector, by specifying a lookup table from words to embedding vectors for instance.\nVariations and extensions of this approach specify more complicated relationships between rn and the childen’s representations rnleft and rnright , or allow internal nodes to have variable numbers of children. For example, Tai et al. (2015) extend LSTMs to tree-structured models by dividing the vector\nrepresentation of each node into a memory cell and a hidden state and updating them accordingly with gating.\nNeural network models which generate or explore a tree top-down have received less attention, but have been applied to generation and parsing tasks. Zhang et al. (2015) generate natural language sentences along with their dependency parses simultaneously. In their specification of a dependency parse tree, each word is connected to one parent word and has a variable number of left and right children (a depth-first in-order traversal on this tree recovers the original sentence). Their model generates these trees by conditioning the probability of generating each word on its ancestor words and the previously-generated sibling words using various LSTMs. Dyer et al. (2016) generate sentences jointly with a corresponding constituency parse tree. They use a shift-reduce parser architecture where shift is replaced with word-generation action, and so the sentence and the tree can be generated by performing a sequence of actions corresponding to a depth-first pre-order traversal of the tree. Each action in the sequence is predicted based upon the tree constructed so far, the words (the tree’s terminal nodes) generated so far, and the previous actions performed. Dong & Lapata (2016) generate tree-structured logical forms using LSTMs, where the LSTM state branches along with the tree’s structure; they focus on generating these logical forms when conditioned upon a natural-language description of it."
    }, {
      "heading" : "2.2 VARIATIONAL AUTOENCODERS",
      "text" : "The variational autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), or VAE for short, provides a way to train a generative model with a fixed prior p(z) and a neural network used to specify pθ(x | z). Typically, the prior p(z) is taken to be a standard multivariate normal distribution (mean at 0) with diagonal unit covariance. Naively, in order to optimize log p(x), we need to compute the following integral:\nlog pθ(x) = log ∫ z pθ(x | z)p(z)dz\nwhich can be tractable when pθ(x | z) is simple but not when we want to use a neural network to represent it. Inference of the posterior p(z | x) also becomes intractable. Instead, we learn a second neural network qφ(z | x) to approximate the true posterior, and use the following variational bound:\nlog p(x) ≥ −DKL(qφ(z | x) ‖ p(z)) + Eqφ(z|x)[log pθ(x | z)]\nwhere DKL represents the Kullback-Leibler divergence between the two distributions. Given that we represent qφ(z | x) with a neural network which outputs the mean and diagonal covariance for a normal distribution, we can analytically compute the KL divergence term and then use the reparameterization trick:\nEqφ(z|x)[log pθ(x | z)] = Ep( )[log pθ(x | z = µ+ σ · )]\nwhere p( ) is a standard multivariate normal distribution, and µ and σ are outputs of the neural network implementing qφ(z | x). These two techniques combined allow us to compute stochastic gradients (by sampling , treating it as constant, and backpropagating through the model) and use standard neural network training techniques (such as SGD, Adagrad, and Adam) to train the model.\nAnother interpretation of the variational autoencoder follows from a modification of the regular autoencoder, where we would like to learn a mapping x → z from the data to a more compact representation z, and an inverse mapping z → x. In the VAE, we replace the deterministic x → z with a probabilistic q(z | x), and as a form of regularization, we ensure that this distribution is close to a prior p(z)."
    }, {
      "heading" : "3 TREE-STRUCTURED VARIATIONAL AUTOENCODER",
      "text" : "In this section, we describe how we combine the variational autoencoder and recursive neural networks in order to build our model."
    }, {
      "heading" : "3.1 TREES",
      "text" : "We consider arbitrarily-branching typed trees where each node contains a type, and either child nodes or a terminal value. Each type may be a terminal type or a non-terminal type; nodes of terminal type contain a value, and nodes of non-terminal type have (zero or more) child nodes.\nA non-terminal type T comes with a specification for how many children a node NT of type T should have, and the types permissible for each child location. We distinguish three types of child nodes:\n• NT may have some number of singular child nodes. For the ith singular child, we specify SINGULARCHILD(T, i) = {T1, · · · , Tn} as the set of types that child node can have. If the singular child node is optional, we denote this by including φ in this set. SINGULARCHILDCOUNT(T ) gives the number of singular child nodes in T .\n• NT may have an arbitrary number of repeated child nodes. Each repeated child node must have type belonging within REPEATEDCHILDREN(T ) = {T1, · · · }. If this set is empty, no repeated child nodes are allowed. These children may be of different types.\nFor each terminal type, we have a list of values that a node of this type can have. We also have a list of types that the root node can have.\nThe above specification serves as an extension of context-free grammars, which are commonly used to specify formal languages. The main difference is in optional and repeated children, which makes it easier to specify an equivalent grammar with fewer non-terminal types.\nAs an example, consider the for loop in the C programming language. A node representing this contains three singular children: an initializer expression, the condition expression (evaluated to check whether the loop should continue running), and the iteration statement, which runs at the end of each loop iteration. It also has repeated children, one child per statement in the loop body."
    }, {
      "heading" : "3.2 BUILDING A TREE",
      "text" : "Now that we have specified the kinds of trees we consider, let us look at how we might build one. First, we describe the basic building block that we use to create one node, then we will look at how to compose these together to build an entire tree.\nAssume that we know the node that we are about to construct should have type T , and that we have a hidden state h ∈ Rk which contains further information about the node.\n• If T is a terminal type, we use WHICHTERMINALVALUET (h), producing a probability distribution over the set of possible values, and sample from this distribution to choose the value.\n• If T is a non-terminal type, we use the following procedure GENERATENODE(T,h): 1. Compute m = SINGULARCHILDCOUNT(T ) + 1{REPEATEDCHILDREN(T ) 6= ∅}.\nIn other words, count the number of singular children, and add 1 if the type allows repeated children.\n2. Compute g1, · · · ,gm = SPLITT (h). The function SPLITT (h) : Rk → Rk×· · ·×Rk maps the k-dimensional vector h into n separate k-dimensional vectors g1 to gm.\n3. For each singular child i: (a) Sample Ti ∼ WHICHCHILDTYPET,i(gi) from a distribution over the types in\nREQUIREDCHILD(T, i). (b) If Ti 6= ∅, use GENERATENODE(Ti,gi) to build the child node recursively.\n4. If T specifies repeated children: (a) Compute gcur,gnext = SPLITREPEATEDT (gm). (b) Sample s ∼ STOPREPEATT (gcur) from a Bernoulli distribution. If s = 1, stop\ngenerating repeated children. (c) Sample Tchild ∼ WHICHCHILDTYPET,repeated(gcur), a probability distribution\nover the types in REPEATEDCHILDREN(T ). (d) Use GENERATENODE(Tchild,gcur) to build this child recursively. (e) Set gm := gnext and repeat this loop.\nFor building the entire tree starting from the root, we assume that we have an embedding z which encodes information about the entire tree (we describe how we obtain this in the next section). We sample Troot ∼ WHICHROOTTYPE(z), the type for the root node, and then run GENERATENODE(Troot, z)."
    }, {
      "heading" : "3.3 ENCODING A TREE",
      "text" : "Recall that the variational autoencoder framework involves training two models simultaneously: p(x | z), the generative (or decoding) model, and q(z | x), the inference (or encoding) model. The previous section described how we specify p(x | z), so we now turn our attention to q(z | x). Overall, we build the inference model by inverting the flow of data in the generative model. Specifically, we use ENCODE(n) to encode a node n with type T :\n• If T is a terminal type, return EMBEDDING(v) ∈ Rk by performing a lookup of the contained value v within a table.\n• If T is a non-terminal type: 1. Compute gi = ENCODE(ni) for each singular child ni of n. If ni is missing, then gi = 0.\n2. If T specifies repeated children, set grepeated := 0 and nchild to the last repeated child of n, and then run: (a) Compute gchild = ENCODE(nchild). (b) Set grepeated := MERGEREPEATEDT (grepeated, gchild) ∈ Rk. (c) Move nchild to the previous repeated child, and repeat (until we run out of repeated children). 3. Return MERGET (g1, . . . , gm, goptional, grepeated) ∈ Rk.\nThus hroot = ENCODE(nroot) gives a summary of the entire tree as a k-dimensional embedding. We then construct q(z | x) = N (µ, σ) where µ = Wµhroot and σ = softplus(Wσhroot). Applying softplus(x) = log(1 + ex) as a nonlinearity gives us a way to ensure that σ is positive as required.\n3.4 IMPLEMENTING SPLIT, MERGE, AND WHICH FUNCTIONS\nIn the previous two sections, we described how the model traverses the tree bottom-up to produce an encoding of it (q(z | x)), and how we can generate a tree top-down from the encoding (p(x | z)). In this section, we explicate the details of how we implemented the SPLIT, MERGE, and WHICH functions that we used previously.\nCOMBINE. We can consider SPLIT : Rk → Rk × · · · × Rk and MERGE : Rk × · · · × Rk → Rk functions to be specializations of a more general function COMBINE : Rk×· · ·×Rk → Rk×· · ·×Rk which takes m inputs and produces n outputs.\nA straightforward implementation of COMBINE is the following: y1, . . . ,yn := COMBINE(x1, . . . ,xm)\n[y1 · · · yn] = f(W [x1 · · · xm] + b) where we have taken xi and yi to be column vectors Rk, [x1 · · ·xm] stacks the vectors xi vertically, W ∈ Rn·k×m·k and b ∈ Rn·k are the learned weight matrix and bias vector respectively, and f is a nonlinearity applied elementwise.\nFor WHICH : Rk → Rd, which computes a probability distribution over d choices, we use a specialization of COMBINE with one input and one (d-sized rather than k-sized) output, and use softmax as the nonlinearity f .\nWhile this basic implementation sufficed initially, we discovered that two modifications led to better performance, which we describe subsequently.\nGating. We added a form of multiplicative gating, similar to those used in Gated Recurrent Units (Chung et al., 2014), Highway Networks (Srivastava et al., 2015), and Gated PixelCNN (van den Oord et al., 2016). The multiplicative gate enables the COMBINE function to more easily pass through information in the inputs to the outputs if that is preferable to transforming the input. Furthermore, the multiplicative interactions used in computing the gates may help the neural network learn more complicated functions.\nFirst, we compute candidate values for yi using a linear layer as before: [ŷ1 · · · ŷn] = f(W [x1 · · · xm] + b)\nThen we compute multiplicative gates for each ŷi and each (xi,yj) combination, or (m+ 1)n gate variables (recall that m is the number of inputs and n is the number of outputs).\n[gy1 · · · gyn ] = σ(Wgy [x1 · · · xm] + bgy )[ g(x1,y1) · · · g(x1,yn) ] = σ(Wg1 [x1 · · · xm] + bg1)\n...[ g(xm,y1) · · · g(xm,yn) ] = σ(Wgm [x1 · · · xm] + bgm)\nThen we compute the final outputs yi: yi = gyi ŷi + g(x1,yi) x1 + · · ·+ g(xm,yi) xm. σ is the sigmoid function σ(x) = 1/(1 + e−x) and is the elementwise product. We initialized bgi = 1 and bgy = −1 so that gyi would start out as a small value and g(xi,yj) would be large, encouraging copying of the inputs xi to the outputs yi.\nLayer normalization. We found that using layer normalization (Ba et al., 2016) also helps stabilize the learning process. For our model, it is difficult to use batch normalization because the connections of each layer (the functions MERGE, SPLIT, WHICH) occur at variable points according to the particular tree we are considering.\nFollowing the procedure in the appendix of Ba et al. (2016), we replace each instance of f(W [x1 · · · xm] + b) with f(LN(W1x1;α1) + · · · + LN(Wmxm;αm) + b) where Wi ∈ Rnk×k are horizontal slices of W and αi ∈ R are learned multiplicative constants. We use LN(z;α) = α · (z − µ)/σ where µ ∈ R is the mean of z ∈ Rk and σ ∈ R is the standard deviation of z."
    }, {
      "heading" : "3.5 WEIGHT SHARING",
      "text" : "In the above model, each function with a different name has different weights. For example, if we have two types PLUS and MINUS each with two required children, then SPLITPLUS and SPLITMINUS will have different weights even though they both have the same signature Rk → Rk × Rk. However, this may be troublesome when we have a very large number of types, because in this scheme the amount of weights increases linearly with the number of types. For such cases, we can apply some of the following modifications:\n• Replace all instances of SPLITT : Rk → Rk × · · · × Rk and SPLITREPEATED with a single SPLITREC : Rk → Rk×Rk. We can apply SPLITREC recurrently to get the desired number of child embeddings.\n• Similarly, replace instances of MERGE with MERGEREC. • Share weights across the WHICH functions: a WHICH function which produces a distribu-\ntion over T1, . . . , Tn contains weights and a bias for each Ti. We can share these weights and biases across all WHICH functions where Ti appears."
    }, {
      "heading" : "3.6 VARIABLE-SIZED LATENT STATE",
      "text" : "In order to achieve low reconstruction error Eqφ(z|x)[log pθ(x | z)], the encoder and decoder networks must learn how to encode all information about a tree in z and then be able to reproduce the tree from this representation. If the tree is large, it becomes a difficult optimization problem to learn how to do this effectively, and may require higher-capacity networks in order to succeed at all which would require more time to train.\nInstead, we can encode the tree with a variable number of latent state vectors. For each node ni in the tree, we specify q(zni | x) = N (µni , σni) where[\nµni σni\n] = ( id\nsoftplus )[ Wµ Wσ ] ENCODE(ni)\nThen when computing GENERATENODE(T,h), we first sample zni ∼ q(zni | x) at training time or zni ∼ p(z) at generation time, and then use ĥ = MERGELATENT(h, zni) in lieu of h. We fixed the prior of each latent vector zi to be the standard multivariate normal distribution with diagonal unit covariance, and did not investigate computing the prior as a function of other samples of zi as in Chung et al. (2015) or Fraccaro et al. (2016) which also used a variable number of latent state vectors."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 TYPE-AWARE SEQUENTIAL MODEL",
      "text" : "For purposes of comparison, we implemented a standard LSTM model for generating each node of the tree sequentially with a depth-first traversal, similar to Vinyals et al. (2015). The model receives each non-terminal type, and terminal value, as a separate token. We begin the sequence with a special 〈BOS〉 token. Whenever an optional child is not present, or at the end of a sequence of repeated children, we insert 〈END〉. This allows us to unambiguously specify a tree following a given grammar as a sequence of tokens.\nAt generation time, we keep track of the partially-generated tree in order to only consider those tokens which would be syntactically allowed to appear at that point. We also tried using this constraint at training time: when computing the output probability distribution, only consider the syntactically allowed tokens and leave the unnormalized log probabilities of the others unconstrained. However, we found that for our datasets, this did not help much with performance and led to overfitting."
    }, {
      "heading" : "4.2 SYNTHETIC ARITHMETIC DATA",
      "text" : "To evaluate the performance of our models in a controlled way, we created a synthetic dataset consisting of arithmetic expressions of a given depth which evaluate to a particular value.\nGrammar. We have two non-terminal types, PLUS and MINUS, and one terminal type NUMBER. PLUS and MINUS have two required children, left and right, each of which can be any of PLUS, MINUS, or NUMBER. For NUMBER, we allowed terminal values 0 to 9.\nGenerating the data. We generate trees with a particular depth, defined as the maximal distance from the root to any terminal node. As such, we consider 1 + (2 + 3) and (1 + 2) − (3 + 4) to both have depth 3. To get trees of depth d which evaluate to v, we first sampled 1,000,000 trees\nuniformly at random from all binary tree structures up to depth d− 1, and randomly assigning each non-terminal node to PLUS or MINUS and setting each terminal node to a random integer between 0 and 9. Then we randomly pick two such trees, which when combined with PLUS or MINUS, evaluate to v to build a tree of depth d.\nAs training data, we generated 100,000 trees of depth 5, 7, 9, and 11. Within each set of trees, each quarter evaluates to −10, −5, 5, and 10 respectively. We use a test set of 1,024 trees, which we generated by first sampling a new set of 1,000,000 subtrees independently.\nResults. Table 1 shows statistics on the datasets and the experimental results we obtained from training various models. The tree variational autoencoder model achieves better performance on deeper trees. In particular, the sequential model fails to learn well on depth 11 trees. However, it appears that a tree-structured model but with a fixed z performs similarly, although consistently worse than with the VAE."
    }, {
      "heading" : "4.3 FIRST-ORDER LOGIC PROOF CLAUSES",
      "text" : "We next consider a dataset derived from Alemi et al. (2016): fragments of automatically-generated proofs for mathematical theorems stated in first-order logic. An automated theorem prover tries to prove a hypothesis given some premises, producing a series of steps until we conclude the hypothesis follows from the premises. Many theorem provers work by resolution; it negates the hypothesis and shows that a contradiction follows. However, figuring out the intermediate steps in the proof is a highly nontrivial search procedure. If we can use machine learning to generate clauses which are likely to appear as a proof step, we may be able to speed up automated theorem proving significantly.\nGrammar. We generate first-order logic statements which are clauses, or a disjunction of literals. Each literal either contains one predicate invocation or asserts that two expressions are equal. Each predicate invocation contains a name, which also determines a fixed number of arguments; each argument is an expression. An expression is either a function invocation, a number, or a variable. A function invocation is structurally identical to a predicate invocation.\nWe consider the set of functions and predicates to be closed. Furthermore, given that each function and predicate has a fixed number of arguments, we made each of these its own type in the grammar. To avoid having a very large number of weights as a consequence, we applied the modifications described in Section 3.5.\nResults. Table 2 describes our results on this dataset. We trained on 955,529 trees and again tested on 1,024 trees. The sequential model demonstrates slightly better log likelihood compared to the tree variational autoencoder model. However, on this dataset we observe a significant improvement\nin log likelihood by adding the variational autoencoder to the tree model, unlike on the arithmetic datasets."
    }, {
      "heading" : "5 DISCUSSION AND FUTURE WORK",
      "text" : "Conditioning on an outside context. In many applications for modeling tree-structured data, we have an outside context that informs which trees are more likely than others. For example, when generating clauses which may appear in a proof, the hypothesis in question greatly influences the content of the clauses. We leave this question to future work.\nScaling to larger trees. Currently, training the model requires processing an entire tree at once, first to encode it into z and then to decode z to reproduce the original tree. This can blow up the memory requirements, requiring an undesirably small batch size. For autoregressive sequence models, truncated backpropagation through time provides a workaround as a partial form of the objective function can be computed on arbitrary subsequences. In our case, adapting methods from Gruslys et al. (2016) and others may prove necessary.\nImproving log likelihood. In terms of log likelihood, our model performed significantly better than an autoregressive sequential model only on one of the datasets we tested, and about the same or slightly worse on the others. Adding depth to the tree structure (Irsoy & Cardie, 2014), and a more sophisticated posterior (Rezende & Mohamed, 2015; Kingma et al., 2016; Sønderby et al., 2016), are some modifications which might help with learning a more powerful model. Introducing more dependencies between the dimensions of x during generation is another possibility but one which may reduce the usefulness of of the latent representation (Bowman et al., 2015)."
    }, {
      "heading" : "A MODEL HYPERPARAMETERS",
      "text" : "We used the Adam optimizer with a learning rate of 0.01, multiplied by 0.98 every 10000 steps. We clipped gradients to have L2 norm 3. For the synthetic arithmetic data, we used a batch size of 64; for the first-order logic proof clauses, we used a batch size of 256. For the tree-structured variational autoencoder, we set k = 256 and used the ELU nonlinearity wherever another one was not explicitly specified. For the sequential models, we used two stacked LSTMs each with hidden state size 256, no dropout. We always unrolled the network to the full length of the sequence during training, and did not perform any bucketing of sequences by length."
    }, {
      "heading" : "B KL DIVERGENCE DYNAMICS DURING TRAINING",
      "text" : "Optimizing the variational autoencoder objective turned out to be a significant optimization challenge, as pointed out by prior work (Bowman et al., 2015; Sønderby et al., 2016; Kingma et al., 2016). Specifically, it is easy for the KL divergence term DKL(qφ(z | x) ‖ p(z)) to collapse to zero, which means that qφ(z | x) is equal to the prior and does not convey any information about x. This leads to uninteresting latent representations and reduces the generative model to one that does not use a latent representation at all.\nAs explained by Kingma et al. (2016), this phenomenon occurs as at the beginning of training it is much easier for the optimization process to move qφ(z | x) closer to the prior p(z) than to improve p(x | z), especially when qφ(z | x) has not yet learned how to convey any useful information. To combat this, we use a combination of two techniques described in the previous work:\n• Anneal the weight on the KL cost term slowly from 0 to 1. Similar to Bowman et al. (2015), our schedule was a shifted and horizontally-scaled sigmoid function.1\n• Set a floor on the KL cost, i.e. use −max(DKL(qφ(z | x) ‖ p(z)), λ) instead of DKL(qφ(z | x) ‖ p(z)) in the objective (Kingma et al., 2016). This change means that the model receives no penalty for producing a KL divergence below λ, and as the other part of the objective (the reconstruction term) benefits from a higher KL divergence, it naturally learns a more informative qφ(z | x) at least λ in KL divergence.\nWe found that at least one of these techniques were required to avoid collapse of the KL divergence to 0. However, as shown in Figure 2, we found that different combinations of these techniques could led to different overall results, suggesting that finding the desired equilibrium necessitates a hyperparameter search.\n1To anneal from a to b, we used σ ( step − a+b\n2\n) /10 to weight the KL cost as a function of the number of\noptimization steps taken."
    } ],
    "references" : [ {
      "title" : "Deepmathdeep sequence models for premise selection",
      "author" : [ "Alex A Alemi", "Francois Chollet", "Geoffrey Irving", "Christian Szegedy", "Josef Urban" ],
      "venue" : "arXiv preprint arXiv:1606.04442,",
      "citeRegEx" : "Alemi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alemi et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06349,",
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Importance weighted autoencoders",
      "author" : [ "Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1509.00519,",
      "citeRegEx" : "Burda et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2015
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.3555,",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "A recurrent latent variable model for sequential data",
      "author" : [ "Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata" ],
      "venue" : "arXiv preprint arXiv:1601.01280,",
      "citeRegEx" : "Dong and Lapata.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith" ],
      "venue" : "arXiv preprint arXiv:1602.07776,",
      "citeRegEx" : "Dyer et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequential neural models with stochastic layers",
      "author" : [ "Marco Fraccaro", "Søren Kaae Sønderby", "Ulrich Paquet", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1605.07571,",
      "citeRegEx" : "Fraccaro et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fraccaro et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1502.04623,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards conceptual compression",
      "author" : [ "Karol Gregor", "Frederic Besse", "Danilo Jimenez Rezende", "Ivo Danihelka", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1604.08772,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2016
    }, {
      "title" : "Memory-efficient backpropagation through time",
      "author" : [ "Audrūnas Gruslys", "Remi Munos", "Ivo Danihelka", "Marc Lanctot", "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1606.03401,",
      "citeRegEx" : "Gruslys et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gruslys et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep recursive neural networks for compositionality in language",
      "author" : [ "Ozan Irsoy", "Claire Cardie" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Irsoy and Cardie.,? \\Q2014\\E",
      "shortCiteRegEx" : "Irsoy and Cardie.",
      "year" : 2014
    }, {
      "title" : "A neural network for factoid question answering over paragraphs",
      "author" : [ "Mohit Iyyer", "Jordan L Boyd-Graber", "Leonardo Max Batista Claudino", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Iyyer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : "arXiv preprint arXiv:1602.02410,",
      "citeRegEx" : "Jozefowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Diederik P Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1606.04934,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2016
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo J Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed" ],
      "venue" : "arXiv preprint arXiv:1505.05770,",
      "citeRegEx" : "Rezende and Mohamed.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rezende and Mohamed.",
      "year" : 2015
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "arXiv preprint arXiv:1606.03498,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Ladder variational autoencoders",
      "author" : [ "Casper Kaae Sønderby", "Tapani Raiko", "Lars Maaløe", "Søren Kaae Sønderby", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1602.02282,",
      "citeRegEx" : "Sønderby et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sønderby et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1503.00075,",
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Lucas Theis", "Aäron van den Oord", "Matthias Bethge" ],
      "venue" : "arXiv preprint arXiv:1511.01844,",
      "citeRegEx" : "Theis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional image generation with pixelcnn decoders",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1606.05328,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey" ],
      "venue" : "arXiv preprint arXiv:1609.08144,",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Top-down tree long short-term memory networks",
      "author" : [ "Xingxing Zhang", "Liang Lu", "Mirella Lapata" ],
      "venue" : "arXiv preprint arXiv:1511.00060,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "2016). Specifically, it is easy for the KL divergence term DKL(qφ(z | x) ‖ p(z)) to collapse to zero, which means that qφ(z | x) is equal to the prior and does not convey any information about x. This leads to uninteresting latent representations and reduces the generative model to one that does not use a latent representation at all",
      "author" : [ "2015 Bowman et al", "2016 Sønderby et al", "Kingma" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "2016), this phenomenon occurs as at the beginning of training it is much easier for the optimization process to move qφ(z | x) closer to the prior p(z) than to improve p(x | z), especially when qφ(z | x) has not yet learned how to convey any useful information. To combat this, we use a combination of two techniques described in the previous work: • Anneal the weight on the KL cost term slowly from 0 to 1",
      "author" : [ "Kingma" ],
      "venue" : null,
      "citeRegEx" : "Kingma,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Newer techniques like the variational autoencoder (Rezende et al., 2014; Kingma & Welling, 2013) and generative-adversarial networks (Goodfellow et al.",
      "startOffset" : 50,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : ", 2014; Kingma & Welling, 2013) and generative-adversarial networks (Goodfellow et al., 2014) enable training of graphical models where the likelihood function is a complicated neural network which normally makes it infeasible to specify and optimize the marginal distribution analytically.",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Language modeling with sequential neural models have halved perplexity (roughly, the error at predicting each word) compared to n-gram methods (Jozefowicz et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 168
    }, {
      "referenceID" : 27,
      "context" : "Neural machine translation using sequence-to-sequence methods have closed half of the gap in quality between prior machine translation efforts and human translation (Wu et al., 2016).",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "Generative image models have similarly progressed such that they can generate samples largely indistinguishable from the original data, at least for relatively small and simple images (Gregor et al., 2015; 2016; Kingma et al., 2016; Salimans et al., 2016; van den Oord et al., 2016), although the quality of the model here is harder to measure in an automated way (Theis et al.",
      "startOffset" : 184,
      "endOffset" : 282
    }, {
      "referenceID" : 16,
      "context" : "Generative image models have similarly progressed such that they can generate samples largely indistinguishable from the original data, at least for relatively small and simple images (Gregor et al., 2015; 2016; Kingma et al., 2016; Salimans et al., 2016; van den Oord et al., 2016), although the quality of the model here is harder to measure in an automated way (Theis et al.",
      "startOffset" : 184,
      "endOffset" : 282
    }, {
      "referenceID" : 19,
      "context" : "Generative image models have similarly progressed such that they can generate samples largely indistinguishable from the original data, at least for relatively small and simple images (Gregor et al., 2015; 2016; Kingma et al., 2016; Salimans et al., 2016; van den Oord et al., 2016), although the quality of the model here is harder to measure in an automated way (Theis et al.",
      "startOffset" : 184,
      "endOffset" : 282
    }, {
      "referenceID" : 24,
      "context" : ", 2016), although the quality of the model here is harder to measure in an automated way (Theis et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "By using the architecture of the variational autoencoder (Rezende et al., 2014; Kingma & Welling, 2013), our model can learn to capture various features of the trees within continuous latent variables, which are added as further inputs into the top-down recursive neural network and conditions the overall generation process.",
      "startOffset" : 57,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "Recursive neural nets, which processes a tree in a bottom-up way, have been popular in natural language processing for a variety of tasks, such as sentiment analysis (Socher et al., 2013), question answering (Iyyer et al.",
      "startOffset" : 166,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : ", 2013), question answering (Iyyer et al., 2014), and semantic relation extraction (Socher et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : ", 2014), and semantic relation extraction (Socher et al., 2012).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : ", 2013), question answering (Iyyer et al., 2014), and semantic relation extraction (Socher et al., 2012). Starting from the leaves of the tree, the model computes a representation for each node by combining the representations of its child nodes. In the case of natural language processing, each tree typically represents one sentence, with the leaf nodes corresponding to the words in the sentence and the structure of the internal nodes determined by the constituency parse tree for the sentence. If we restrict ourselves to binary trees (given that it is possible to binarize arbitrary trees in a lossless way), the we compute the k-dimensional representation rn ∈ R for a node n by combining the representations of nodes nleft and nright: rn = f(Wrnleft + V rnright) where W and V are square matrices (in Rd×d) and f is a nonlinear activation function, applied elementwise. Leaf nodes are represented by embedding the content of the leaf node into a ddimensional vector, by specifying a lookup table from words to embedding vectors for instance. Variations and extensions of this approach specify more complicated relationships between rn and the childen’s representations rnleft and rnright , or allow internal nodes to have variable numbers of children. For example, Tai et al. (2015) extend LSTMs to tree-structured models by dividing the vector",
      "startOffset" : 29,
      "endOffset" : 1291
    }, {
      "referenceID" : 27,
      "context" : "Zhang et al. (2015) generate natural language sentences along with their dependency parses simultaneously.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Dyer et al. (2016) generate sentences jointly with a corresponding constituency parse tree.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "Dyer et al. (2016) generate sentences jointly with a corresponding constituency parse tree. They use a shift-reduce parser architecture where shift is replaced with word-generation action, and so the sentence and the tree can be generated by performing a sequence of actions corresponding to a depth-first pre-order traversal of the tree. Each action in the sequence is predicted based upon the tree constructed so far, the words (the tree’s terminal nodes) generated so far, and the previous actions performed. Dong & Lapata (2016) generate tree-structured logical forms using LSTMs, where the LSTM state branches along with the tree’s structure; they focus on generating these logical forms when conditioned upon a natural-language description of it.",
      "startOffset" : 0,
      "endOffset" : 533
    }, {
      "referenceID" : 17,
      "context" : "The variational autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), or VAE for short, provides a way to train a generative model with a fixed prior p(z) and a neural network used to specify pθ(x | z).",
      "startOffset" : 28,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "We added a form of multiplicative gating, similar to those used in Gated Recurrent Units (Chung et al., 2014), Highway Networks (Srivastava et al.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "We fixed the prior of each latent vector zi to be the standard multivariate normal distribution with diagonal unit covariance, and did not investigate computing the prior as a function of other samples of zi as in Chung et al. (2015) or Fraccaro et al.",
      "startOffset" : 214,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "We fixed the prior of each latent vector zi to be the standard multivariate normal distribution with diagonal unit covariance, and did not investigate computing the prior as a function of other samples of zi as in Chung et al. (2015) or Fraccaro et al. (2016) which also used a variable number of latent state vectors.",
      "startOffset" : 214,
      "endOffset" : 260
    }, {
      "referenceID" : 26,
      "context" : "For purposes of comparison, we implemented a standard LSTM model for generating each node of the tree sequentially with a depth-first traversal, similar to Vinyals et al. (2015). The model receives each non-terminal type, and terminal value, as a separate token.",
      "startOffset" : 156,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "To estimate a tighter bound for log p(x), we use IWAE (Burda et al., 2015) with 50 samples of z.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "We next consider a dataset derived from Alemi et al. (2016): fragments of automatically-generated proofs for mathematical theorems stated in first-order logic.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "In our case, adapting methods from Gruslys et al. (2016) and others may prove necessary.",
      "startOffset" : 35,
      "endOffset" : 57
    } ],
    "year" : 2016,
    "abstractText" : "Many kinds of variable-sized data we would like to model contain an internal hierarchical structure in the form of a tree, including source code, formal logical statements, and natural language sentences with parse trees. For such data it is natural to consider a model with matching computational structure. In this work, we introduce a variational autoencoder-based generative model for tree-structured data. We evaluate our model on a synthetic dataset, and a dataset with applications to automated theorem proving. By learning a latent representation over trees, our model can achieve similar test log likelihood to a standard autoregressive decoder, but with the number of sequentially dependent computations proportional to the depth of the tree instead of the number of nodes in the tree.",
    "creator" : "LaTeX with hyperref package"
  }
}