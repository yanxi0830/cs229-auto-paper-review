{
  "name" : "329.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "IMPROVING GENERATIVE ADVERSARIAL NETWORKS WITH DENOISING FEATURE MATCHING",
    "authors" : [ "David Warde-Farley", "Yoshua Bengio" ],
    "emails" : [ "david.warde-farley@umontreal.ca", "yoshua.bengio@umontreal.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Generative adversarial networks (Goodfellow et al., 2014a) (GANs) have become well known for their strength at realistic image synthesis. The objective function for the generative network is an implicit function of a learned discriminator network, estimated in parallel with the generator, which aims to tell apart real data from synthesized. Ideally, the discriminator learns to capture distinguishing features of real data, which the generator learns to imitate, and the process iterates until real data and synthesized data are indistinguishable.\nIn practice, GANs are well known for being quite challenging to train effectively. The relative model capacities of the generator and discriminator must be carefully balanced in order for the generator to effectively learn. Compounding the problem is the lack of an unambiguous and computable convergence criterion. Nevertheless, particularly when trained on image collections from relatively narrow domains such as bedroom scenes (Yu et al., 2015) and human faces (Liu et al., 2015), GANs have been shown to produce very compelling results.\nFor diverse image collections comprising a wider variety of the visual world, the results have generally been less impressive. For example, samples from models trained on ImageNet (Russakovsky et al., 2014) roughly match the local and global statistics of natural images but yield few recognizable objects. Recent work (Salimans et al., 2016) has sought to address this problem by training the discriminator in a semi-supervised fashion, granting the discriminator’s internal representations knowledge of the class structure of (some fraction of) the training data it is presented. This technique markedly increases sample quality, but is unsatisfying from the perspective of GANs as a tool for unsupervised learning.\nWe propose to augment the generator’s training criterion with a second training objective which guides the generator towards samples more like those in the training set by explicitly modeling the data density in addition to the adversarial discriminator. Rather than deploy a second computationally expensive convolutional network for this task, the additional objective is computed in the space of features learned by the discriminator. In that space, we train a denoising auto-encoder, a family of models which is known to estimate the energy gradient of the data on which it is trained. We evaluate the denoising auto-encoder on samples drawn from the generator, and use the “denoised” features as targets – nearby feature configurations which are more likely than those of the generated sample, according to the distribution estimated by the denoiser.\nWe show that this yields generators which consistently produce recognizable objects on the CIFAR10 dataset without the use of label information as in Salimans et al. (2016). The criterion appears to improve stability and possesses a degree of natural robustness to the well known “collapse” pathology. We further investigate the criterion’s performance on two larger and more diverse collections of images, and validate our qualitative observations quantitatively with the Inception score proposed in Salimans et al. (2016)."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "The generative adversarial networks paradigm (Goodfellow et al., 2014a) estimates generative samplers by means of a training procedure which pits a generator G against a discriminator D. D is trained to tell apart training examples from samples produced by G, while G is trained to increase the probability of its samples being incorrectly classified as data. In the original formulation, the training procedure defines a continuous minimax game\narg min G arg max D\nEx∼D logD(x) + Ez∼p(z) log (1−D (G(z))) (1)\nwhere D is a data distribution on Rn, D is a function that maps Rn to the unit interval, and G is a function that maps a noise vector z ∈ Rm, drawn from a simple distribution p(z), to the ambient space of the training data, Rn. The idealized algorithm can be shown to converge and to minimize the Jensen-Shannon divergence between the data generating distribution and the distribution parameterized by G.\nGoodfellow et al. (2014a) found that in practice, minimizing (1) with respect to the parameters of G proved difficult, and elected instead to optimize an alternate objective,\narg max G Ez∼p(z) logD (G(z)) (2)\nat the same time as D is optimized as above. logD(G(z)) yields more favourably scaled persample gradients forGwhenD confidently identifies a sample as counterfeit, avoiding the vanishing gradients arising in that case with the − log(1−D(G(z))) objective. Subsequent authors have investigated applications and extensions of GANs; for a review of this body of literature, see Warde-Farley & Goodfellow (2016). Of particular note for our purposes is Radford et al. (2015), who provide a set of general guidelines for the successful training of generative adversarial networks, and Salimans et al. (2016), who build upon these techniques with a number of useful heuristics and explore a variant in which the discriminator D is trained to correctly classify labeled training data, resulting in gradients with respect to the discriminator evidently containing a great deal of information relevant to generating “object-like” samples."
    }, {
      "heading" : "2.2 CHALLENGES AND LIMITATIONS OF GANS",
      "text" : "While Goodfellow et al. (2014a) provides a theoretical basis for the GAN criterion, the theory relies on certain assumptions that are not satisfied in practice. Proofs demonstrate convergence of the GAN criterion in the unconstrained space of arbitrary functions; in practice, finitely parameterized families of functions such as neural networks are employed. As a consequence, the “inner loop” of the idealized algorithm – maximizing (1) with respect to (the parameters of) D, is infeasible to perform exactly, and in practice only one or a few gradient steps stand in for this maximization. This results in a de facto criterion for G which minimizes a lower bound on the correct objective (Goodfellow, 2014).\nA commonly observed failure mode is that of full or partial collapse, where G maps a large fraction of probable regions under p(z) to only a few, low-volume regions of Rn; in the case of images, this manifests as the appearance of many near-duplicate images in independent draws from G, as well as a lower diversity of samples and modes than what is observed in the dataset. As G and D are typically trained via mini-batch stochastic gradient descent, several authors have proposed heuristics that penalize such duplication within each mini-batch (Salimans et al., 2016; Zhao et al., 2016).\nGANs represent a departure from traditional probabilistic models based on maximum likelihood and its approximations in that they parameterize a sampler directly and lack a closed form for the likelihood. This makes objective, quantitative evaluation difficult. While previous results in the literature have reported approximate likelihoods based on Parzen window estimates, Theis et al. (2015) has convincingly argued that these estimates can be quite misleading for high-dimensional data. In this work, we adopt the Inception score proposed by Salimans et al. (2016), which uses a reference Inception convolutional neural network (Szegedy et al., 2015) to compute\nI({x}N1 ) = exp (E [DKL(p(y|x)‖p(y)])) (3) where p(y|x) is provided by the output of the Inception network and p(y) = ∫ x p(x)p(y|x)dx u 1 N ∑ p(y|xi). Note that this score can be made larger by a low-entropy per-sample posterior (i.e. the Inception network classifies a given sample with greater certainty) as well as a higher entropy aggregate posterior (i.e. the Inception network identifies a wide variety of classes among the samples presented to it). Salimans et al. (2016) found this score correlated well with human evaluations of samplers trained on CIFAR-10; we therefore employ the Inception score here as a quantitative measure of visual fidelity of the samples, following the previous work’s protocol of evaluating the average Inception score over 10 independent groups of 5,000 samples each. Error estimates correspond to standard deviations, in keeping with previously reported results."
    }, {
      "heading" : "3 IMPROVING UNSUPERVISED GAN TRAINING ON DIVERSE DATASETS",
      "text" : "In this work, we focus on the apparent difficulty of training GANs to produce “object-like” samples when trained on diverse collections of natural images. While Salimans et al. (2016) make progress on this problem by employing labeled data and training the discriminator, here we aim to make progress on the unsupervised case. Nevertheless, our methods would be readily applicable to supervised, semi-supervised or (with slight modifications) conditional setting.\nWe begin from the slightly subtle observation that in realistic manifestations of the GAN training procedure, the discriminator’s (negative) gradient with respect to a sample points in a direction of (infinitesimal) local improvement with respect to the discriminator’s estimate of the sample being data; it does not necessarily point in the direction of a draw from the data distribution. Indeed, the literature is replete with instances of gradient descent with respect to the input of a classification model, particularly wide-domain natural image classifiers, producing ghostly approximations to a particular class exemplar (Le et al., 2012; Erhan et al., 2009; Yosinski et al., 2015) when this procedure is carried out without additional guidance, to say nothing of the problems posed by adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014b) and fooling examples (Nguyen et al., 2015).\nWhile the gradient of the loss function defined by the discriminator may be a source of information mostly relevant to very local improvements, the discriminator itself is a potentially valuable source of compact descriptors of the training data. Many authors have noted the remarkable versatility of high-level features learned by convolutional networks (Donahue et al., 2014; Yosinski et al., 2014) and the degree to which high-level semantics can be reconstructed from even the deepest layers of a network (Dosovitskiy & Brox, 2016). Although non-stationary, the distribution of the highlevel activations of the discriminator when evaluated on data is ripe for exploitation as an additional source of knowledge about salient aspects of the data distribution.\nWe propose in this work to track this distribution with a denoising auto-encoder r(·) trained on the discriminator’s hidden states when evaluated on training data. Alain & Bengio (2014) showed that a denoising auto-encoder trained on data from a distribution q(h) estimates via r(h) − h the gradient of the true log-density, ∂ log q(h)∂h . Hence, if we train the denoising auto-encoder on the transformed training data h = Φ(x) with x ∼ D, then r(Φ(x′))− Φ(x′) with x′ = G(z) indicates in which direction x′ should be changed in order to make h = Φ(x′) more like those features seen with the data. Minimizing ||r(Φ(x′)) − Φ(x′)||2 with respect to x′ would thus push x′ towards higher probability configurations according to the data distribution in the feature space Φ(x). We thus evaluate the discriminator features Φ(x), and the denoising auto-encoder, on samples from the generator, and treat the denoiser’s output reconstruction as a fixed target for the generator. We refer to this procedure as denoising feature matching, and employ it as a learning signal for the generator in addition to the traditional GAN generator objective.\nFormally, letG be the generator parameterized by θG, andD = d◦Φ be our discriminator composing feature extractor Φ(·) : Rn → Rk and a classifier d(·) : Rk → [0, 1]. Let C(·) : Rk → Rk be a corruption function to be applied at the input of the denoising auto-encoder when it is trained to denoise. The parameters of the discriminator D, comprising the parameters of both d and Φ, is trained as in Goodfellow et al. (2014a), while the generator is trained according to\narg min θG\nEz∼p(z) [ λdenoise‖Φ(G(z))− r(Φ(G(z)))‖|2 − λadv logD(G(z)) ] (4)\nwhere r(G(z)) is treated as constant with respect to gradient computations. Simultaneously, the denoiser r(·) is trained according to the objective\narg min θr\nEx∼D‖Φ(x)− r(C(Φ(x)))‖2 (5)"
    }, {
      "heading" : "3.1 EFFECT OF Φ",
      "text" : "The theory surrounding denoising auto-encoders applies when estimating a denoising function from a data distribution p(vecx). Here, we propose to estimate the denoising auto-encoder in the space of discriminator features, giving rise to a distribution q(Φ(x)). A natural question is what effect this has on the gradient being backpropagated. This is difficult to analyze in general, as for most choices the mapping Φ will not be invertible, though it is instructive to examine the invertible case. Assuming an invertible Φ : Rn → Rn, let J = ∂Φ(x)∂x be the Jacobian of Φ, and q(Φ(x)) = p(x)|J |. By the inverse function theorem, J is also invertible (and is in fact the Jacobian of the inverse Φ−1). Applying the chain rule and re-arranging terms, taking advantage of the invertibility of J , we arrive at a straightforward relationship between the score of q and the score of p:\n∂ log q(Φ(x)) ∂Φ(x) = ∂ log [p(x) |J |] ∂Φ(x) (6)\n= ∂ log p(x) ∂Φ(x) + ∂ log ∣∣∣∂Φ(x)∂x ∣∣∣ ∂Φ(x)\n(7)\n=\n( ∂ log p(x)\n∂x + ∂ log |J | ∂x\n) J−1 (8)\nwhere\n∂ log |J | ∂xk = Tr\n( J−1 dJ\ndxk\n) (9)\nand dJdxk is a matrix of scalar derivatives of elements of J with respect to xk. Thus, we see that the gradient backpropagated to the generator in an ideal setting is the gradient of the data distribution p(x) along with an additive term which accounts for the changes in the rate of volume expansion/contraction in Φ locally around x. In practice, Φ is not invertible, but the added benefit of the denoiser-targeted gradient appears to reduce underfitting to the modes of p in the generator, irrespective of any distortions Φ may introduce."
    }, {
      "heading" : "4 RELATED WORK",
      "text" : "Denoising feature matching was originally inspired by feature matching introduced by Salimans et al. (2016) as an alternative training criterion for GAN generators, namely (in our notation)\narg min θG ∣∣‖Ex∼D [Φ(x)]− Ez∼p(z) [Φ(G(z))] ‖∣∣2 (10) Feature matching is equivalent to linear maximum mean discrepancy (Gretton et al., 2006), employing linear first moment matching in the space of discriminator features Φ(·) rather than the more familiar kernelized formulation. When performed on features in the penultimate layer, Salimans et al. (2016) found that the feature matching criterion was useful for the purpose of improving\nresults on semi-supervised classification, using classification of samples from the generator as a sophisticated form of data augmentation. Feature matching was, however, less successful at producing samples with high visual fidelity. This is somewhat unsurprising given that the criterion is insensitive to higher-order statistics of the respective feature distributions. Indeed, a degenerate G which deterministically reproduces a single sample m̂ such that Φ(m̂) = Ex∈DΦ(x) trivially minimizes (10); in practice the joint training dynamics of D and G do not appear to yield such degenerate solutions.\nRather than aiming to merely reduce linear separability between data and samples in the feature space defined by Φ(·), denoising feature matching selects a more probable (according to the feature distribution implied by the data, as captured by the denoiser) feature space target for each sample produced by G and regresses G towards it. While an early loss of entropy in G could result in the generator locking on to one or a few attractors in the denoiser’s energy landscape, we observe that this does not happen when used in conjunction with the traditional GAN objective, and in fact that the combination of the two objectives is notably robust to the collapses often observed in GAN training, even without taking additional measures to prevent them.\nThis work also draws inspiration from Alain & Bengio (2014), which showed that a suitably trained denoiser learns an operator which locally maps a sample towards regions of high probability under the data distribution. They further showed that a suitably trained1 reconstruction function r(·) behaves such that\nr(x)− x ∝ ∂ log p(x) ∂x\n(11)\nThat is, r(x) − x estimates the score of the data generating distribution, up to a multiplicative constant. Our use of denoising auto-encoders necessarily departs from idealized conditions in that the denoiser is estimated online from an ever-changing distribution of features.\nSeveral approaches to GAN-like models have cast the problem in terms of learning an energy function. Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator. The energy-based GAN formulation of Zhao et al. (2016) resembles our work in their use of an auto-encoder which is trained to faithfully reconstruct (in our case, a corrupted, function of) the training data. The energy-based GAN replaces the discriminator with an auto-encoder, which is trained to assign low energy (L2 reconstruction error) to training data and higher energy to samples from G. To discourage generator collapses, a “pull-away term” penalizes the normalized dot product in a feature space defined by the auto-encoder’s internal representation. In this work, we preserve the discriminator, trained in the usual discriminative fashion, and in fact preserve the traditional generator loss, instead augmenting it with a source of complementary information provided by targets obtained from the denoiser. The energy-based GAN can be viewed as training the generator to seek fixed points of the autoencoding function (i.e. by backpropagating through the decoder and encoder in order to decrease reconstruction error), whereas we treat the output of r(·) as constant with respect to the optimization as in Lee et al. (2015). That is to say, rather than using backpropagation to steer the dynamics of the autoencoder, we instead employ our denoising autoencoder to augment the gradient information obtained by ordinary backpropagation.\nClosest to our own approach, concurrent work on model-based super-resolution by Sønderby et al. (2016) trains a denoising auto-encoder on high-resolution ground truth and evaluates it on synthesized super-resolution images, using the difference between the original synthesized image and the denoiser’s output as an additional training signal for refining the output of the super-resolution network. Both Sønderby et al. (2016) and our own work are motivated by the results of Alain & Bengio (2014) discussed above. Aside from addressing a different application area, our denoiser is learned on-the-fly from a high-level feature representation which is itself learned.\n1In the limit of infinite training data, with isotropic Gaussian noise of some standard deviation σ."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We evaluate denoising feature matching on learning synthesis models from three datasets of increasing diversity and size: CIFAR-10, STL-10, and ImageNet. Although several authors have described GAN-based image synthesis models operating at 128 × 128 (Salimans et al., 2016; Zhao et al., 2016) and 256× 256 (Zhao et al., 2016) resolution, we carry out our investigations at relatively low resolutions, both for computational ease and because we believe that the problem of unconditional modeling of diverse image collections is not well solved even at low resolutions; making progress in this regime is likely to yield insights that apply to the higher-resolution case.\nIn all experiments, we employ isotropic Gaussian corruption noise with σ = 1. Although we experimented with annealing σ towards 0 (as also performed in Sønderby et al. (2016)), an annealing schedule which consistently outperformed fixed noise remained elusive. We experimented with convolutional denoisers, but our best results to date were obtained with deep, fully-connected denoisers using the ReLU nonlinearity on the penultimate layer of the discriminator. The number of hidden units was fixed to the same value in all denoiser layers, and the procedure is apparently robust to this hyperparameter choice, as long as it is greater than or equal to the input dimensionality.\nOur generator and discriminator architectures follow the methods outlined in Radford et al. (2015). Accordingly, batch normalization (Ioffe & Szegedy, 2015) was used in the generator and discriminator in the same manner as Radford et al. (2015), and in all layers of the denoiser except the output layer. In particular, as in Radford et al. (2015), we separately batch normalize data and generator samples for the discriminator and denoiser with respect to each source’s statistics. We calculate updates with respect to all losses with the parameters of all three networks fixed, and update all parameters simultaneously.\nAll networks were trained with the Adam optimizer Kingma & Ba (2014) with a learning rate of 10−4 and β1 = 0.5. The Adam optimizer is scale invariant, and so it suffices to e.g. tune λdenoise and fix λadv to 1. In our experiments, we set λdenoise to 0.03/nh, where nh is the number of discriminator hidden units fed as input to the denoiser; this division decouples the scale of the first term of (4) from the dimensionality of the representation used, reducing the need to adjust this hyperparameter simply because we altered the architecture of the discriminator."
    }, {
      "heading" : "5.1 CIFAR-10",
      "text" : "CIFAR-10 (Krizhevsky & Hinton, 2009) is a small, well-studied dataset consisting of 50,000 32×32 pixel RGB training images and 10,000 test images from 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\nSamples from our model trained on CIFAR-10 are shown in Figure 1, and Inception scores for several methods, including those reported in Salimans et al. (2016) and scores computed from samples generated from a model presented in Dumoulin et al. (2016), are presented in Table 1. We achieve a mean Inception score of 7.72, falling slightly short of Salimans et al. (2016), which employed a supervised discriminator network (the same work reports a score of 4.36 ± .04 when labels are omitted from their training procedure). Qualitatively, the samples include recognizable cars, boats and various animals. The best performing generator network consisted of the 32 × 32 ImageNet architecture from Radford et al. (2015) with half the number of parameters at each layer, and less than 40% of the parameters of the CIFAR-10 generator presented in Salimans et al. (2016)."
    }, {
      "heading" : "5.2 STL-10",
      "text" : "STL-10 (Coates et al., 2011) is a dataset consisting of a small labeled set and larger (100,000) unlabeled set of 96 × 96 RGB images. The unlabeled set is a subset of ImageNet that is more diverse than CIFAR-10 (or the labeled set of STL-10), but less diverse than full ImageNet. We downsample by a factor of 2 on each dimension and train our networks at 48× 48. Inception scores for our model and a baseline, consisting of the same architecture trained without denoising feature matching (both trained for 50 epochs), are shown in Table 2. Samples are displayed in Figure 2."
    }, {
      "heading" : "5.3 IMAGENET",
      "text" : "The ImageNet database (Russakovsky et al., 2014) is a large-scale database of natural images. We train on the designated training set of the most widely used release, the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2012), consisting of a highly unbalanced split among 1,000 object classes. We preprocess the dataset as rescaled central crops following the procedure of Krizhevsky et al. (2012), except at 32 × 32 resolution to facilitate comparison with Radford et al. (2015).\nImageNet poses a particular challenge for unsupervised GANs due to its high level of diversity and class skew. With a generator and discriminator architecture identical to that used for the same dataset in Radford et al. (2015), we achieve a higher Inception score using denoising feature matching, using denoiser with 10 hidden layers of 2,048 rectified linear units each. Both fall far short of the score assigned to real data at this resolution; there is still plenty of room for improvement. Samples are displayed in Figure 3."
    }, {
      "heading" : "6 DISCUSSION AND FUTURE DIRECTIONS",
      "text" : "We have shown that training a denoising model on high-level discriminator activations in a GAN, and using the denoiser to propose high-level feature targets for the generator, can usefully improve\nGAN image models. Higher Inception scores, as well as visual inspection, suggest that the procedure captures class-specific features of the training data in a manner superior to the original adversarial objective alone. That being said, we do not believe we are yet making optimal use of the paradigm. The non-stationarity of the feature distribution on which the denoiser is trained could be limiting the ability of the denoiser to obtain a good fit, and the information backpropagated to the generator is always slightly stale. Steps to reduce this non-stationarity may be fruitful; we experimented briefly with historical averaging as explored in Salimans et al. (2016) but did not observe a clear benefit thus far. Structured denoisers, including denoisers that learn an energy function for multiple hidden layers at once, could conceivably aid in obtaining a better fit. Learning a partially stochastic transition operator rather than a deterministic denoiser could conceivably capture interesting multimodalities that are “blurred” by a unimodal denoising function.\nOur method is orthogonal and could conceivably be used in combination with several other GAN extensions. For example, methods incorporating an encoder component (Donahue et al., 2016; Dumoulin et al., 2016), various existing conditional architectures (Mirza & Osindero, 2014; Denton et al., 2015; Reed et al., 2016), or the semi-supervised variant employed in Salimans et al. (2016), could all be trained with an additional denoising feature matching objective.\nWe have proposed a useful heuristic, but a better theoretical grounding regarding how GANs are trained in practice is a necessary direction for future work, including grounded criteria for assessing mode coverage and mass misassignment, and principled criteria for assessing convergence or performing early stopping."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Ian Goodfellow, Laurent Dinh, Yaroslav Ganin and Kyle Kastner for helpful discussions. We thank Vincent Dumoulin and Ishmael Belghazi for making available code and model parameters used in comparison to ALI, as well as Alec Radford for making available the code and model parameters for his ImageNet model. We would like to thank Antonia Creswell and Hiroyuki Yamazaki for pointing out an error in the initial version of this manuscript, and anonymous reviewers for valuable feedback. We thank the University of Montreal and Compute Canada for the computational resources used for this investigation, as well as the authors of Theano (Al-Rfou et al., 2016), Blocks and Fuel (van Merriënboer et al., 2015). We thank CIFAR, NSERC, Google, Samsung and Canada Research Chairs for funding."
    } ],
    "references" : [ {
      "title" : "A learning algorithm for boltzmann machines",
      "author" : [ "David H Ackley", "Geoffrey E Hinton", "Terrence J Sejnowski" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "Ackley et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Ackley et al\\.",
      "year" : 1985
    }, {
      "title" : "Theano: A python framework for fast computation of mathematical expressions",
      "author" : [ "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi" ],
      "venue" : "CoRR, abs/1605.02688,",
      "citeRegEx" : "Al.Rfou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2016
    }, {
      "title" : "What regularized auto-encoders learn from the data-generating distribution",
      "author" : [ "Guillaume Alain", "Yoshua Bengio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Alain and Bengio.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alain and Bengio.",
      "year" : 2014
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "A. Coates", "H. Lee", "A.Y. Ng" ],
      "venue" : "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS",
      "citeRegEx" : "Coates et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Donahue et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2014
    }, {
      "title" : "Adversarial feature learning",
      "author" : [ "Jeff Donahue", "Philipp Krähenbühl", "Trevor Darrell" ],
      "venue" : "arXiv preprint arXiv:1605.09782,",
      "citeRegEx" : "Donahue et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2016
    }, {
      "title" : "Inverting visual representations with convolutional networks",
      "author" : [ "Alexey Dosovitskiy", "Thomas Brox" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Dosovitskiy and Brox.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy and Brox.",
      "year" : 2016
    }, {
      "title" : "Adversarially learned inference",
      "author" : [ "Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.00704,",
      "citeRegEx" : "Dumoulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualizing higher-layer features of a deep network",
      "author" : [ "Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent" ],
      "venue" : "University of Montreal,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2009
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "On distinguishability criteria for estimating generative models",
      "author" : [ "Ian J Goodfellow" ],
      "venue" : "arXiv preprint arXiv:1412.6515,",
      "citeRegEx" : "Goodfellow.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow.",
      "year" : 2014
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "A kernel method for the two-sample-problem",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte Rasch", "Bernhard Schölkopf", "Alex J Smola" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2006
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Deep directed generative models with energy-based probability estimation",
      "author" : [ "Taesup Kim", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1606.03439,",
      "citeRegEx" : "Kim and Bengio.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim and Bengio.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Quoc Le", "Marc’Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Greg Corrado", "Kai Chen", "Jeff Dean", "Andrew Ng" ],
      "venue" : "In ICML’2012,",
      "citeRegEx" : "Le et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2012
    }, {
      "title" : "Difference target propagation",
      "author" : [ "Dong-Hyun Lee", "Saizheng Zhang", "Asja Fischer", "Yoshua Bengio" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Lee et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In Proceedings of International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional generative adversarial nets",
      "author" : [ "Mehdi Mirza", "Simon Osindero" ],
      "venue" : "arXiv preprint arXiv:1411.1784,",
      "citeRegEx" : "Mirza and Osindero.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza and Osindero.",
      "year" : 2014
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune" ],
      "venue" : "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial text to image synthesis",
      "author" : [ "Scott E. Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "CoRR, abs/1605.05396,",
      "citeRegEx" : "Reed et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "CoRR, abs/1606.03498,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Amortised map inference for image super-resolution",
      "author" : [ "Casper Kaae Sønderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Huszár" ],
      "venue" : "arXiv preprint arXiv:1610.04490,",
      "citeRegEx" : "Sønderby et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sønderby et al\\.",
      "year" : 2016
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Lucas Theis", "Aäron van den Oord", "Matthias Bethge" ],
      "venue" : "arXiv preprint arXiv:1511.01844,",
      "citeRegEx" : "Theis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2015
    }, {
      "title" : "Blocks and fuel: Frameworks for deep learning",
      "author" : [ "Bart van Merriënboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David WardeFarley", "Jan Chorowski", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1506.00619,",
      "citeRegEx" : "Merriënboer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Merriënboer et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial perturbations of deep neural networks",
      "author" : [ "D Warde-Farley", "I Goodfellow" ],
      "venue" : "Perturbation, Optimization and Statistics,",
      "citeRegEx" : "Warde.Farley and Goodfellow.,? \\Q2016\\E",
      "shortCiteRegEx" : "Warde.Farley and Goodfellow.",
      "year" : 2016
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding neural networks through deep visualization",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson" ],
      "venue" : "arXiv preprint arXiv:1506.06579,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2015
    }, {
      "title" : "LSUN: construction of a largescale image dataset using deep learning with humans",
      "author" : [ "Fisher Yu", "Yinda Zhang", "Shuran Song", "Ari Seff", "Jianxiong Xiao" ],
      "venue" : "in the loop. CoRR,",
      "citeRegEx" : "Yu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    }, {
      "title" : "Energy-based generative adversarial network",
      "author" : [ "Junbo Zhao", "Michael Mathieu", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1609.03126,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "Nevertheless, particularly when trained on image collections from relatively narrow domains such as bedroom scenes (Yu et al., 2015) and human faces (Liu et al.",
      "startOffset" : 115,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : ", 2015) and human faces (Liu et al., 2015), GANs have been shown to produce very compelling results.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : "Recent work (Salimans et al., 2016) has sought to address this problem by training the discriminator in a semi-supervised fashion, granting the discriminator’s internal representations knowledge of the class structure of (some fraction of) the training data it is presented.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "We show that this yields generators which consistently produce recognizable objects on the CIFAR10 dataset without the use of label information as in Salimans et al. (2016). The criterion appears to improve stability and possesses a degree of natural robustness to the well known “collapse” pathology.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 26,
      "context" : "We show that this yields generators which consistently produce recognizable objects on the CIFAR10 dataset without the use of label information as in Salimans et al. (2016). The criterion appears to improve stability and possesses a degree of natural robustness to the well known “collapse” pathology. We further investigate the criterion’s performance on two larger and more diverse collections of images, and validate our qualitative observations quantitatively with the Inception score proposed in Salimans et al. (2016).",
      "startOffset" : 150,
      "endOffset" : 524
    }, {
      "referenceID" : 10,
      "context" : "Goodfellow et al. (2014a) found that in practice, minimizing (1) with respect to the parameters of G proved difficult, and elected instead to optimize an alternate objective,",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "Subsequent authors have investigated applications and extensions of GANs; for a review of this body of literature, see Warde-Farley & Goodfellow (2016). Of particular note for our purposes is Radford et al.",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "Subsequent authors have investigated applications and extensions of GANs; for a review of this body of literature, see Warde-Farley & Goodfellow (2016). Of particular note for our purposes is Radford et al. (2015), who provide a set of general guidelines for the successful training of generative adversarial networks, and Salimans et al.",
      "startOffset" : 134,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "Subsequent authors have investigated applications and extensions of GANs; for a review of this body of literature, see Warde-Farley & Goodfellow (2016). Of particular note for our purposes is Radford et al. (2015), who provide a set of general guidelines for the successful training of generative adversarial networks, and Salimans et al. (2016), who build upon these techniques with a number of useful heuristics and explore a variant in which the discriminator D is trained to correctly classify labeled training data, resulting in gradients with respect to the discriminator evidently containing a great deal of information relevant to generating “object-like” samples.",
      "startOffset" : 134,
      "endOffset" : 346
    }, {
      "referenceID" : 11,
      "context" : "This results in a de facto criterion for G which minimizes a lower bound on the correct objective (Goodfellow, 2014).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "As G and D are typically trained via mini-batch stochastic gradient descent, several authors have proposed heuristics that penalize such duplication within each mini-batch (Salimans et al., 2016; Zhao et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 214
    }, {
      "referenceID" : 36,
      "context" : "As G and D are typically trained via mini-batch stochastic gradient descent, several authors have proposed heuristics that penalize such duplication within each mini-batch (Salimans et al., 2016; Zhao et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 214
    }, {
      "referenceID" : 10,
      "context" : "While Goodfellow et al. (2014a) provides a theoretical basis for the GAN criterion, the theory relies on certain assumptions that are not satisfied in practice.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "(2016), which uses a reference Inception convolutional neural network (Szegedy et al., 2015) to compute I({x}1 ) = exp (E [DKL(p(y|x)‖p(y)])) (3) where p(y|x) is provided by the output of the Inception network and p(y) = ∫",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "While previous results in the literature have reported approximate likelihoods based on Parzen window estimates, Theis et al. (2015) has convincingly argued that these estimates can be quite misleading for high-dimensional data.",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : "In this work, we adopt the Inception score proposed by Salimans et al. (2016), which uses a reference Inception convolutional neural network (Szegedy et al.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 26,
      "context" : "Salimans et al. (2016) found this score correlated well with human evaluations of samplers trained on CIFAR-10; we therefore employ the Inception score here as a quantitative measure of visual fidelity of the samples, following the previous work’s protocol of evaluating the average Inception score over 10 independent groups of 5,000 samples each.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Indeed, the literature is replete with instances of gradient descent with respect to the input of a classification model, particularly wide-domain natural image classifiers, producing ghostly approximations to a particular class exemplar (Le et al., 2012; Erhan et al., 2009; Yosinski et al., 2015) when this procedure is carried out without additional guidance, to say nothing of the problems posed by adversarial examples (Szegedy et al.",
      "startOffset" : 238,
      "endOffset" : 298
    }, {
      "referenceID" : 9,
      "context" : "Indeed, the literature is replete with instances of gradient descent with respect to the input of a classification model, particularly wide-domain natural image classifiers, producing ghostly approximations to a particular class exemplar (Le et al., 2012; Erhan et al., 2009; Yosinski et al., 2015) when this procedure is carried out without additional guidance, to say nothing of the problems posed by adversarial examples (Szegedy et al.",
      "startOffset" : 238,
      "endOffset" : 298
    }, {
      "referenceID" : 34,
      "context" : "Indeed, the literature is replete with instances of gradient descent with respect to the input of a classification model, particularly wide-domain natural image classifiers, producing ghostly approximations to a particular class exemplar (Le et al., 2012; Erhan et al., 2009; Yosinski et al., 2015) when this procedure is carried out without additional guidance, to say nothing of the problems posed by adversarial examples (Szegedy et al.",
      "startOffset" : 238,
      "endOffset" : 298
    }, {
      "referenceID" : 28,
      "context" : ", 2015) when this procedure is carried out without additional guidance, to say nothing of the problems posed by adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014b) and fooling examples (Nguyen et al.",
      "startOffset" : 133,
      "endOffset" : 181
    }, {
      "referenceID" : 23,
      "context" : ", 2014b) and fooling examples (Nguyen et al., 2015).",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "Many authors have noted the remarkable versatility of high-level features learned by convolutional networks (Donahue et al., 2014; Yosinski et al., 2014) and the degree to which high-level semantics can be reconstructed from even the deepest layers of a network (Dosovitskiy & Brox, 2016).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 33,
      "context" : "Many authors have noted the remarkable versatility of high-level features learned by convolutional networks (Donahue et al., 2014; Yosinski et al., 2014) and the degree to which high-level semantics can be reconstructed from even the deepest layers of a network (Dosovitskiy & Brox, 2016).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "While Salimans et al. (2016) make progress on this problem by employing labeled data and training the discriminator, here we aim to make progress on the unsupervised case.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "Many authors have noted the remarkable versatility of high-level features learned by convolutional networks (Donahue et al., 2014; Yosinski et al., 2014) and the degree to which high-level semantics can be reconstructed from even the deepest layers of a network (Dosovitskiy & Brox, 2016). Although non-stationary, the distribution of the highlevel activations of the discriminator when evaluated on data is ripe for exploitation as an additional source of knowledge about salient aspects of the data distribution. We propose in this work to track this distribution with a denoising auto-encoder r(·) trained on the discriminator’s hidden states when evaluated on training data. Alain & Bengio (2014) showed that a denoising auto-encoder trained on data from a distribution q(h) estimates via r(h) − h the gradient of the true log-density, ∂ log q(h) ∂h .",
      "startOffset" : 109,
      "endOffset" : 701
    }, {
      "referenceID" : 10,
      "context" : "The parameters of the discriminator D, comprising the parameters of both d and Φ, is trained as in Goodfellow et al. (2014a), while the generator is trained according to",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "Denoising feature matching was originally inspired by feature matching introduced by Salimans et al. (2016) as an alternative training criterion for GAN generators, namely (in our notation)",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Feature matching is equivalent to linear maximum mean discrepancy (Gretton et al., 2006), employing linear first moment matching in the space of discriminator features Φ(·) rather than the more familiar kernelized formulation.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "Feature matching is equivalent to linear maximum mean discrepancy (Gretton et al., 2006), employing linear first moment matching in the space of discriminator features Φ(·) rather than the more familiar kernelized formulation. When performed on features in the penultimate layer, Salimans et al. (2016) found that the feature matching criterion was useful for the purpose of improving",
      "startOffset" : 67,
      "endOffset" : 303
    }, {
      "referenceID" : 0,
      "context" : "Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator.",
      "startOffset" : 300,
      "endOffset" : 321
    }, {
      "referenceID" : 0,
      "context" : "Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator. The energy-based GAN formulation of Zhao et al. (2016) resembles our work in their use of an auto-encoder which is trained to faithfully reconstruct (in our case, a corrupted, function of) the training data.",
      "startOffset" : 301,
      "endOffset" : 460
    }, {
      "referenceID" : 0,
      "context" : "Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator. The energy-based GAN formulation of Zhao et al. (2016) resembles our work in their use of an auto-encoder which is trained to faithfully reconstruct (in our case, a corrupted, function of) the training data. The energy-based GAN replaces the discriminator with an auto-encoder, which is trained to assign low energy (L2 reconstruction error) to training data and higher energy to samples from G. To discourage generator collapses, a “pull-away term” penalizes the normalized dot product in a feature space defined by the auto-encoder’s internal representation. In this work, we preserve the discriminator, trained in the usual discriminative fashion, and in fact preserve the traditional generator loss, instead augmenting it with a source of complementary information provided by targets obtained from the denoiser. The energy-based GAN can be viewed as training the generator to seek fixed points of the autoencoding function (i.e. by backpropagating through the decoder and encoder in order to decrease reconstruction error), whereas we treat the output of r(·) as constant with respect to the optimization as in Lee et al. (2015). That is to say, rather than using backpropagation to steer the dynamics of the autoencoder, we instead employ our denoising autoencoder to augment the gradient information obtained by ordinary backpropagation.",
      "startOffset" : 301,
      "endOffset" : 1539
    }, {
      "referenceID" : 0,
      "context" : "Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator. The energy-based GAN formulation of Zhao et al. (2016) resembles our work in their use of an auto-encoder which is trained to faithfully reconstruct (in our case, a corrupted, function of) the training data. The energy-based GAN replaces the discriminator with an auto-encoder, which is trained to assign low energy (L2 reconstruction error) to training data and higher energy to samples from G. To discourage generator collapses, a “pull-away term” penalizes the normalized dot product in a feature space defined by the auto-encoder’s internal representation. In this work, we preserve the discriminator, trained in the usual discriminative fashion, and in fact preserve the traditional generator loss, instead augmenting it with a source of complementary information provided by targets obtained from the denoiser. The energy-based GAN can be viewed as training the generator to seek fixed points of the autoencoding function (i.e. by backpropagating through the decoder and encoder in order to decrease reconstruction error), whereas we treat the output of r(·) as constant with respect to the optimization as in Lee et al. (2015). That is to say, rather than using backpropagation to steer the dynamics of the autoencoder, we instead employ our denoising autoencoder to augment the gradient information obtained by ordinary backpropagation. Closest to our own approach, concurrent work on model-based super-resolution by Sønderby et al. (2016) trains a denoising auto-encoder on high-resolution ground truth and evaluates it on synthesized super-resolution images, using the difference between the original synthesized image and the denoiser’s output as an additional training signal for refining the output of the super-resolution network.",
      "startOffset" : 301,
      "endOffset" : 1853
    }, {
      "referenceID" : 0,
      "context" : "Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator. The energy-based GAN formulation of Zhao et al. (2016) resembles our work in their use of an auto-encoder which is trained to faithfully reconstruct (in our case, a corrupted, function of) the training data. The energy-based GAN replaces the discriminator with an auto-encoder, which is trained to assign low energy (L2 reconstruction error) to training data and higher energy to samples from G. To discourage generator collapses, a “pull-away term” penalizes the normalized dot product in a feature space defined by the auto-encoder’s internal representation. In this work, we preserve the discriminator, trained in the usual discriminative fashion, and in fact preserve the traditional generator loss, instead augmenting it with a source of complementary information provided by targets obtained from the denoiser. The energy-based GAN can be viewed as training the generator to seek fixed points of the autoencoding function (i.e. by backpropagating through the decoder and encoder in order to decrease reconstruction error), whereas we treat the output of r(·) as constant with respect to the optimization as in Lee et al. (2015). That is to say, rather than using backpropagation to steer the dynamics of the autoencoder, we instead employ our denoising autoencoder to augment the gradient information obtained by ordinary backpropagation. Closest to our own approach, concurrent work on model-based super-resolution by Sønderby et al. (2016) trains a denoising auto-encoder on high-resolution ground truth and evaluates it on synthesized super-resolution images, using the difference between the original synthesized image and the denoiser’s output as an additional training signal for refining the output of the super-resolution network. Both Sønderby et al. (2016) and our own work are motivated by the results of Alain & Bengio (2014) discussed above.",
      "startOffset" : 301,
      "endOffset" : 2178
    }, {
      "referenceID" : 0,
      "context" : "Kim & Bengio (2016) extends GANs by modeling the data distribution simultaneously with an energy function parameterized by a deep neural network (playing the role of the discriminator) and the traditional generator, carrying out learning with a learning rule resembling that of the Boltzmann machine (Ackley et al., 1985), where the “negative phase” gradient is estimated from samples from the generator. The energy-based GAN formulation of Zhao et al. (2016) resembles our work in their use of an auto-encoder which is trained to faithfully reconstruct (in our case, a corrupted, function of) the training data. The energy-based GAN replaces the discriminator with an auto-encoder, which is trained to assign low energy (L2 reconstruction error) to training data and higher energy to samples from G. To discourage generator collapses, a “pull-away term” penalizes the normalized dot product in a feature space defined by the auto-encoder’s internal representation. In this work, we preserve the discriminator, trained in the usual discriminative fashion, and in fact preserve the traditional generator loss, instead augmenting it with a source of complementary information provided by targets obtained from the denoiser. The energy-based GAN can be viewed as training the generator to seek fixed points of the autoencoding function (i.e. by backpropagating through the decoder and encoder in order to decrease reconstruction error), whereas we treat the output of r(·) as constant with respect to the optimization as in Lee et al. (2015). That is to say, rather than using backpropagation to steer the dynamics of the autoencoder, we instead employ our denoising autoencoder to augment the gradient information obtained by ordinary backpropagation. Closest to our own approach, concurrent work on model-based super-resolution by Sønderby et al. (2016) trains a denoising auto-encoder on high-resolution ground truth and evaluates it on synthesized super-resolution images, using the difference between the original synthesized image and the denoiser’s output as an additional training signal for refining the output of the super-resolution network. Both Sønderby et al. (2016) and our own work are motivated by the results of Alain & Bengio (2014) discussed above.",
      "startOffset" : 301,
      "endOffset" : 2249
    }, {
      "referenceID" : 26,
      "context" : "Although several authors have described GAN-based image synthesis models operating at 128 × 128 (Salimans et al., 2016; Zhao et al., 2016) and 256× 256 (Zhao et al.",
      "startOffset" : 96,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "Although several authors have described GAN-based image synthesis models operating at 128 × 128 (Salimans et al., 2016; Zhao et al., 2016) and 256× 256 (Zhao et al.",
      "startOffset" : 96,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : ", 2016) and 256× 256 (Zhao et al., 2016) resolution, we carry out our investigations at relatively low resolutions, both for computational ease and because we believe that the problem of unconditional modeling of diverse image collections is not well solved even at low resolutions; making progress in this regime is likely to yield insights that apply to the higher-resolution case.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "Although several authors have described GAN-based image synthesis models operating at 128 × 128 (Salimans et al., 2016; Zhao et al., 2016) and 256× 256 (Zhao et al., 2016) resolution, we carry out our investigations at relatively low resolutions, both for computational ease and because we believe that the problem of unconditional modeling of diverse image collections is not well solved even at low resolutions; making progress in this regime is likely to yield insights that apply to the higher-resolution case. In all experiments, we employ isotropic Gaussian corruption noise with σ = 1. Although we experimented with annealing σ towards 0 (as also performed in Sønderby et al. (2016)), an annealing schedule which consistently outperformed fixed noise remained elusive.",
      "startOffset" : 97,
      "endOffset" : 690
    }, {
      "referenceID" : 24,
      "context" : "Our generator and discriminator architectures follow the methods outlined in Radford et al. (2015). Accordingly, batch normalization (Ioffe & Szegedy, 2015) was used in the generator and discriminator in the same manner as Radford et al.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "Our generator and discriminator architectures follow the methods outlined in Radford et al. (2015). Accordingly, batch normalization (Ioffe & Szegedy, 2015) was used in the generator and discriminator in the same manner as Radford et al. (2015), and in all layers of the denoiser except the output layer.",
      "startOffset" : 77,
      "endOffset" : 245
    }, {
      "referenceID" : 24,
      "context" : "Our generator and discriminator architectures follow the methods outlined in Radford et al. (2015). Accordingly, batch normalization (Ioffe & Szegedy, 2015) was used in the generator and discriminator in the same manner as Radford et al. (2015), and in all layers of the denoiser except the output layer. In particular, as in Radford et al. (2015), we separately batch normalize data and generator samples for the discriminator and denoiser with respect to each source’s statistics.",
      "startOffset" : 77,
      "endOffset" : 348
    }, {
      "referenceID" : 24,
      "context" : "Our generator and discriminator architectures follow the methods outlined in Radford et al. (2015). Accordingly, batch normalization (Ioffe & Szegedy, 2015) was used in the generator and discriminator in the same manner as Radford et al. (2015), and in all layers of the denoiser except the output layer. In particular, as in Radford et al. (2015), we separately batch normalize data and generator samples for the discriminator and denoiser with respect to each source’s statistics. We calculate updates with respect to all losses with the parameters of all three networks fixed, and update all parameters simultaneously. All networks were trained with the Adam optimizer Kingma & Ba (2014) with a learning rate of 10−4 and β1 = 0.",
      "startOffset" : 77,
      "endOffset" : 691
    }, {
      "referenceID" : 24,
      "context" : "Samples from our model trained on CIFAR-10 are shown in Figure 1, and Inception scores for several methods, including those reported in Salimans et al. (2016) and scores computed from samples generated from a model presented in Dumoulin et al.",
      "startOffset" : 136,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "(2016) and scores computed from samples generated from a model presented in Dumoulin et al. (2016), are presented in Table 1.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "(2016) and scores computed from samples generated from a model presented in Dumoulin et al. (2016), are presented in Table 1. We achieve a mean Inception score of 7.72, falling slightly short of Salimans et al. (2016), which employed a supervised discriminator network (the same work reports a score of 4.",
      "startOffset" : 76,
      "endOffset" : 218
    }, {
      "referenceID" : 8,
      "context" : "(2016) and scores computed from samples generated from a model presented in Dumoulin et al. (2016), are presented in Table 1. We achieve a mean Inception score of 7.72, falling slightly short of Salimans et al. (2016), which employed a supervised discriminator network (the same work reports a score of 4.36 ± .04 when labels are omitted from their training procedure). Qualitatively, the samples include recognizable cars, boats and various animals. The best performing generator network consisted of the 32 × 32 ImageNet architecture from Radford et al. (2015) with half the number of parameters at each layer, and less than 40% of the parameters of the CIFAR-10 generator presented in Salimans et al.",
      "startOffset" : 76,
      "endOffset" : 563
    }, {
      "referenceID" : 8,
      "context" : "(2016) and scores computed from samples generated from a model presented in Dumoulin et al. (2016), are presented in Table 1. We achieve a mean Inception score of 7.72, falling slightly short of Salimans et al. (2016), which employed a supervised discriminator network (the same work reports a score of 4.36 ± .04 when labels are omitted from their training procedure). Qualitatively, the samples include recognizable cars, boats and various animals. The best performing generator network consisted of the 32 × 32 ImageNet architecture from Radford et al. (2015) with half the number of parameters at each layer, and less than 40% of the parameters of the CIFAR-10 generator presented in Salimans et al. (2016).",
      "startOffset" : 76,
      "endOffset" : 711
    }, {
      "referenceID" : 26,
      "context" : "? as reported in Salimans et al. (2016); semisupervised † computed from samples drawn using author-provided model parameters and implementation.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "STL-10 (Coates et al., 2011) is a dataset consisting of a small labeled set and larger (100,000) unlabeled set of 96 × 96 RGB images.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "We preprocess the dataset as rescaled central crops following the procedure of Krizhevsky et al. (2012), except at 32 × 32 resolution to facilitate comparison with Radford et al.",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "We preprocess the dataset as rescaled central crops following the procedure of Krizhevsky et al. (2012), except at 32 × 32 resolution to facilitate comparison with Radford et al. (2015). ImageNet poses a particular challenge for unsupervised GANs due to its high level of diversity and class skew.",
      "startOffset" : 79,
      "endOffset" : 186
    }, {
      "referenceID" : 18,
      "context" : "We preprocess the dataset as rescaled central crops following the procedure of Krizhevsky et al. (2012), except at 32 × 32 resolution to facilitate comparison with Radford et al. (2015). ImageNet poses a particular challenge for unsupervised GANs due to its high level of diversity and class skew. With a generator and discriminator architecture identical to that used for the same dataset in Radford et al. (2015), we achieve a higher Inception score using denoising feature matching, using denoiser with 10 hidden layers of 2,048 rectified linear units each.",
      "startOffset" : 79,
      "endOffset" : 415
    }, {
      "referenceID" : 6,
      "context" : "For example, methods incorporating an encoder component (Donahue et al., 2016; Dumoulin et al., 2016), various existing conditional architectures (Mirza & Osindero, 2014; Denton et al.",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "For example, methods incorporating an encoder component (Donahue et al., 2016; Dumoulin et al., 2016), various existing conditional architectures (Mirza & Osindero, 2014; Denton et al.",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : ", 2016), various existing conditional architectures (Mirza & Osindero, 2014; Denton et al., 2015; Reed et al., 2016), or the semi-supervised variant employed in Salimans et al.",
      "startOffset" : 52,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : ", 2016), various existing conditional architectures (Mirza & Osindero, 2014; Denton et al., 2015; Reed et al., 2016), or the semi-supervised variant employed in Salimans et al.",
      "startOffset" : 52,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "Steps to reduce this non-stationarity may be fruitful; we experimented briefly with historical averaging as explored in Salimans et al. (2016) but did not observe a clear benefit thus far.",
      "startOffset" : 120,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : ", 2016), various existing conditional architectures (Mirza & Osindero, 2014; Denton et al., 2015; Reed et al., 2016), or the semi-supervised variant employed in Salimans et al. (2016), could all be trained with an additional denoising feature matching objective.",
      "startOffset" : 77,
      "endOffset" : 184
    } ],
    "year" : 2017,
    "abstractText" : "We propose an augmented training procedure for generative adversarial networks designed to address shortcomings of the original by directing the generator towards probable configurations of abstract discriminator features. We estimate and track the distribution of these features, as computed from data, with a denoising auto-encoder, and use it to propose high-level targets for the generator. We combine this new loss with the original and evaluate the hybrid criterion on the task of unsupervised image synthesis from datasets comprising a diverse set of visual categories, noting a qualitative and quantitative improvement in the “objectness” of the resulting samples.",
    "creator" : "LaTeX with hyperref package"
  }
}