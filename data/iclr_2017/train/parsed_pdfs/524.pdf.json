{
  "name" : "524.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "FEATURE SPACE", "Terrance DeVries" ],
    "emails" : [ "terrance@uoguelph.ca", "gwtaylor@uoguelph.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "One of the major catalysts for the resurgence of neural networks as “deep learning” was the influx of the availability of data. Labeled data is crucial for any supervised machine learning algorithm to work, even moreso for deep architectures which are easily susceptible to overfitting. Deep learning has flourished in a few domains (e.g. images, speech, text) where labeled data has been relatively simple to acquire. Unfortunately most of the data that is readily available is unstructured and unlabeled and this has prevented recent successes from propagating to other domains. In order to leverage the power of supervised learning, data must be manually labeled, a process which requires investment of human effort. An alternative to labeling unlabeled data is to generate new data with known labels. One variant of this approach is to create synthetic data from a simulation such as a computer graphics engine (Shotton et al., 2013; Richter et al., 2016), however, this may not work if the simulation is not a good representation of the real world domain. Another option is dataset augmentation, wherein the existing data is transformed in some way to create new data that appears to come from the same (conditional) data generating distribution (Bengio et al., 2011). The main challenge with such an approach is that domain expertise is required to ensure that the newly generated data respects valid transformations (i.e. those that would occur naturally in that domain).\nIn this work, we consider augmentation not by a domain-specific transformation, but by perturbing, interpolating, or extrapolating between existing examples. However, we choose to operate not in input space, but in a learned feature space. Bengio et al. (2013) and Ozair & Bengio (2014) claimed that higher level representations expand the relative volume of plausible data points within the feature space, conversely shrinking the space allocated for unlikely data points. As such, when traversing along the manifold it is more likely to encounter realistic samples in feature space than compared to input space. Unsupervised representation learning models offer a convenient way of learning useful feature spaces for exploring such transformations. Recently, there has been a return to interest in such techniques, leading to, e.g., variational autoencoders (Kingma & Welling, 2014), generative adversarial networks (Goodfellow et al., 2014), and generative stochastic networks (Alain et al., 2016), each of which could be used to generate useful feature spaces for augmentation.\nBy manipulating the vector representation of data within a learned feature space a dataset can be augmented in a number of ways. One of the most basic transformations that can be applied to the\ndata is to simply add random noise to the context vector. In the context of class-imbalanced data, Chawla et al. (2002) proposed interpolating between samples in feature space. Similarly extrapolation between samples could also be applied. We investigate some of these methods to see which is most effective for improving the performance of supervised learning models when augmented data is added to the dataset.\nIn this work, we demonstrate that extrapolating between samples in feature space can be used to augment datasets and improve the performance of supervised learning algorithms. The main benefit of our approach is that it is domain-independent, requiring no specialized knowledge, and can therefore be applied to many different types of problems. We show that models trained on datasets that have been augmented using our technique outperform models trained only on data from the original dataset. Just as dataset augmentation in input space has become standard for visual recognition tasks, we recommend dataset augmentation in feature space as a domain-agnostic, general-purpose framework to improve generalization when limited labeled data is available."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "For many years, dataset augmentation has been a standard regularization technique used to reduce overfitting while training supervised learning models. Data augmentation is particularly popular for visual recognition tasks as new data can be generated very easily by applying image manipulations such as shifting, scaling, rotation, and other affine transformations. When training LeNet5, one of the most early and well-known convolutional neural network architectures, LeCun et al. (1998) applied a series of transformations to the input images in order to improve the robustness of the model. Krizhevsky et al. (2012) also used image transformations to generate new data when training the renowned AlexNet model for the 2012 Large Scale Visual Recognition Challenge (ILSVRC). They claimed that dataset augmentation reduced the error rate of the model by over 1%. Creating new data has since been a crucial component of all recent large-scale image recognition models.\nUnfortunately, dataset augmentation is not as straightforward to apply in all domains as it is for images. For example, Schlüter & Grill (2015) investigated a variety of data augmentation techniques for application to singing voice detection. These include adding Gaussian noise to the input, shifting the pitch of the audio signal, time stretching, varying the loudness of the audio signal, applying random frequency filters, and interpolating between samples in input space. They found that only pitch shifting and random frequency filtering appeared to improve model performance. While performing well on audio data, these augmentation techniques cannot be applied to other domains. As such, the process of designing, implementing, and evaluating new data augmentation techniques would need to be repeated for each new problem.\nImportant to our work are sequence-to-sequence learning (seq2seq) models which were first developed independently by Cho et al. (2014) and Sutskever et al. (2014). Generally these models convert a sequence of inputs from one domain into a fixed-length context vector which is then used to generate an output sequence, usually from a different domain. For example, the first application of seq2seq learning by Cho and Sutskever was to translate between English and French. Sequenceto-sequence learning has recently been used to achieve state-of-the-art results on a large variety of sequence learning tasks including image captioning (Vinyals et al., 2015b), video captioning (Venugopalan et al., 2015), speech recognition ((Chan et al., 2016), (Bahdanau et al., 2016)), machine translation ((Jean et al., 2015), (Luong et al., 2015)), text parsing (Vinyals et al., 2015a), and conversational modeling (Vinyals & Le, 2015). The seq2seq architecture can also be used to create sequence autoencoders (SA) by creating a model that learns to reconstruct input sequences in its output (Srivastava et al., 2015; Dai & Le, 2015). We use a variant of sequence autoencoders in our work to create a feature space within which we can manipulate data to augment a training set."
    }, {
      "heading" : "3 MODEL",
      "text" : "Our dataset augmentation technique works by first learning a data representation and then applying transformations to samples mapped to that representation. Our hypothesis is that, due to manifold unfolding in feature space, simple transformations applied to encoded rather than raw inputs will result in more plausible synthetic data. While any number of representation learning models could\nbe explored, we use a sequence autoencoder to construct a feature space. The main reason we adopt SA is that we favour a generic method that can be used for either time series or static data."
    }, {
      "heading" : "3.1 SEQUENCE AUTOENCODER",
      "text" : "An autoencoder consists of two parts: an encoder and a decoder. The encoder receives data as input and, by applying one or more parametrized nonlinear transformations, converts it into a new representation, classically lower-dimensional than the original input. The decoder takes this representation and tries to reconstruct the original input, also by applying one or more nonlinear transformations. Various regularized forms of autoencoders have been proposed to learn overcomplete representations.\nA sequence autoencoder works in a similar fashion as the standard autoencoder except that the encoder and decoder use one or more recurrent layers so that they can encode and decode variablelength sequences. In all of our experiments, we use a stacked LSTM (Li & Wu, 2015) with two layers for both the encoder and decoder (Figure 1a). During the forward pass, the hidden states of the recurrent layers are propagated through the layer stack. The encoder’s hidden state at the final time step, called the context vector, is used to seed the hidden state of the decoder at its first time step.\nThe main difference between our implementation of the SA and that of Dai & Le (2015) is how the context vector is used in the decoder. Dai and Le follow the original seq2seq approach of Sutskever et al. (2014) and use the context vector as input to the decoder only on the first time step, then use the output of the previous times step as inputs for all subsequent time steps as follows:\ny0 = f(s0, c)\nyt = f(st−1,yt−1)\nwhere f is the LSTM function, s is the state of the LSTM (both hidden and cell state), c is the context vector, and y is the output of the decoder. We instead modify the above equation so that the decoder is conditioned on the context vector at each time step as was done in (Cho et al., 2014):\ny0 = f(s0, c)\nyt = f(st−1,yt−1, c).\nWe found that conditioning the decoder on the context vector each time step resulted in improved reconstructions, which we found to be critical to the success of the data augmentation process."
    }, {
      "heading" : "3.2 AUGMENTATION IN FEATURE SPACE",
      "text" : "In order to augment a dataset, each example is projected into feature space by feeding it through the sequence encoder, extracting the resulting context vector, and then applying a transformation in feature space (Figure 1b). The simplest transform is to simply add noise to the context vectors, however, there is a possibility with this method that the resulting vector may not resemble the same class as the original, or even any of the known classes. In our experiments, we generate noise by drawing from a Gaussian distribution with zero mean and per-element standard deviation calculated across all context vectors in the dataset. We include a γ parameter to globally scale the noise:\nc′i = ci + γX, X ∼ N{0, σ2i } (1) where i indexes the elements of a context vector which corresponds to data points from the training set. A more directed approach for data augmentation follows the techniques introduced by Chawla et al. (2002). For each sample in the dataset, we find itsK nearest neighbours in feature space which share its class label. For each pair of neighbouring context vectors, a new context vector can then be generated using interpolation:\nc′ = (ck − cj)λ + cj (2)\nwhere c′ is the synthetic context vector, ci and cj are neighbouring context vectors, and λ is a variable in the range {0, 1} that controls the degree of interpolation. In our experiments, we use λ = 0.5 so that the new sample balances properties of both original samples. In a similar fashion, extrapolation can also be applied to the context vectors:\nc′j = (cj − ck)λ + cj . (3)\nIn the case of extrapolation, λ is a value in the range {0,∞} which controls the degree of extrapolation. While λ could be drawn from a random distribution for each new sample we found that setting λ = 0.5 worked well as a default value in most cases, so we use this setting in all of our tests.\nOnce new context vectors have been created, they can either be used directly as input for a learning task, or they can be decoded to generate new sequences (Figure 1c). When interpolating between two samples, the resulting decoded sequence is set to be the average length of the two inputs. When extrapolating between two samples the length of the new sequence is set to be the same as that of cj ."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In all experiments, we trained a LSTM-based sequence autoencoder in order to learn a feature space from the available training examples. Each hidden layer, including the context vector, had the same number of hidden units and a dropout probability of p = 0.2. The autoencoders were trained using Adam (Kingma & Ba, 2015) with an initial learning rate of 0.001, which was reduced by half whenever no improvement was observed in the validation set for 10 epochs. Finally, we reversed the order of the input sequences as suggested by Sutskever et al. (2014). We found that reversing the order of input sequences caused the model to train faster and achieve better final solutions.\nFor all classification experiments where interpolation or extrapolation was applied to generate new samples, we applied the following procedure unless otherwise stated. For each sample in the dataset we found the 10 nearest in-class neighbours by searching in feature space. We then interpolated or extrapolated between each neighbour and the original sample to produce a synthetic example which was added to the augmented dataset. For all tests, the baseline model and the augmented dataset model(s) were trained for the same number of weight updates regardless of dataset size."
    }, {
      "heading" : "4.1 VISUALIZATION - SINUSOIDS",
      "text" : "To gain an intuition of the method we start by working with a synthetic dataset of sinusoids. Sinusoids work well as a test case for this technique as they have a known behaviour and only two dimensions (amplitude and time), so we can easily observe the effects of the dataset augmentation process. To create a training set, sinusoids were generated with amplitude, frequency, and phase drawn from a uniform distribution.\nFor this toy problem, we trained a sequence autoencoder with 32 hidden units in each layer. We then applied different data augmentation strategies to observe the effects on the “synthetic” sinusoids.\nFor each test we extracted the context vectors of two input sinusoids, performed an operation, then decoded the resulting context vectors to generate new sequences.\nWe first augmented data by adding random noise to the context vectors before decoding. The noise magnitude parameter γ from Equation 1 was set to 0.5. In Figure 2a the blue and green “parent” samples are shown in bold while the augmented “child” samples are thinner, lighter lines. Importantly, we observe that all new samples are valid sinusoids with stable, regular repeating patterns. Although mimicking the major properties of their parents the generated samples have small changes in amplitude, frequency, and phase, as would be the expected effect for the addition of random noise.\nFor a more directed form of data augmentation we experimented with interpolating between sinusoids within the space of the context vectors. Figure 2b demonstrates interpolation between two sinusoids using Equation 2 while varying the λ parameter from 0 to 1. Unlike the results obtained by Bengio et al. (2013) where the transition between classes occurs very suddenly we find that the samples generated by our model smoothly transition between the two parent sinusoids. This is an exciting observation as it suggests that we can control characteristics of the generated samples by combining two samples which contain the desired properties.\nIn a similar fashion to interpolation we can also extrapolate between two samples using Equation 3. For this experiment we again vary the λ parameter from 0 to 1 to generate a range of samples. As seen in Figure 2c, this appears to have the effect of exaggerating properties of each sinusoid with respect to the properties of the other sinusoid. For example, we see that new samples generated from the blue parent sinusoid increase in amplitude and decrease in phase shift. Conversely, samples generated from the green parent sinusoid decrease in amplitude and increase in phase shift. The behaviour of the extrapolation operation could prove very beneficial for data augmentation as it could be used to generate extra samples of rare or underrepresented cases within the dataset, which is a common failure case."
    }, {
      "heading" : "4.2 VISUALIZATION - UJI PEN CHARACTERS",
      "text" : "The UJI Pen Characters dataset (v2) contains 11,640 instances of 97 different characters handwritten by 60 participants (Llorens et al., 2008). All samples were collected using a tablet PC and a stylus. Characters are defined by a sequence of X and Y coordinates, and include upper and lower case ASCII letters, Spanish non-ASCII letters, the 10 digits, and other common punctuation and symbols. As with the sinusoids in Section 4.1, handwritten characters are suitable for evaluating dataset augmentation methods as they have an expected shape and can be easily visualized.\nAs a preprocessing step for this dataset we first applied local normalization to each sample to get a fixed size, followed by a global normalization across the dataset as a whole. A sequence autoencoder with 128 hidden units per layer was trained to construct the feature space within which data augmentation could take place. Figure 3a demonstrates the effects of interpolating between characters in feature space. In this example we use the “@” symbol. We see that the resulting characters share\ncharacteristics of the two parent inputs, such as the length of the symbol’s tail or the shape of the central “a”. Visually the majority of generated samples appear very similar to their parents, which is expected from interpolation, but is not necessarily useful from the perspective of data augmentation.\nWhen augmenting data for the purpose of improving performance of machine learning algorithms it is desirable to create samples that are different from the data that is already common in the dataset. To this end, extrapolating between samples is preferable, as shown in Figure 3b. Extrapolated data displays a wider variety compared to samples created by interpolation. We hypothesize that it is this added variability that is necessary in order for data augmentation to be useful."
    }, {
      "heading" : "4.3 SPOKEN ARABIC DIGITS",
      "text" : "For our first quantitative test we use the Arabic Digits dataset (Lichman, 2013) which contains 8,800 samples of time series mel-frequency cepstrum coefficients (MFCCs) extracted from audio clips of spoken Arabic digits. Thirteen MFCCs are available for each time step in this dataset. To preprocess the data we apply global normalization. To evaluate our data augmentation techniques we used the official train/test split and trained ten models with different random weight initializations.\nAs a baseline model we trained a simple two layer MLP on the context vectors produced by a SA. Both models used 256 hidden units in each hidden layer. The MLP applied dropout with p = 0.5 after each dense layer. To evaluate the usefulness of different data augmentation techniques we trained a new baseline model on datasets that had been augmented with newly created samples. The techniques we evaluated were: adding random noise to context vectors, interpolating between two random context vectors from the same class, interpolating between context vectors and their nearest neighbours from the same class, and extrapolating between context vectors and their nearest neighbours from the same class. The results of our tests are summarized in Table 1.\nWe find that our simple baseline model achieves competitive performance after training on the extracted context vectors, demonstrating the feature extracting capability of the sequence autoencoder. The naı̈ve data augmentation approach of adding random noise to the context vectors further improves performance. Of interest, we find that adding new samples generated using interpolation techniques diminishes the performance of the model, which confirms our hypothesis that good data augmentation techniques should add variability to the dataset. Of the two interpolation techniques,\nwe see that interpolating between neighbouring samples performs better than simply interpolating with randomly chosen samples of the same class. Finally we observe that extrapolating between samples improves model performance significantly, reducing the baseline error rate by almost half. Our results rival those of Hammami et al. (2012), which to our knowledge are state-of-the-art on this dataset."
    }, {
      "heading" : "4.4 AUSTRALIAN SIGN LANGUAGE SIGNS (AUSLAN)",
      "text" : "Our second quantitative test was conducted on the Australian Sign Language Signs dataset (AUSLAN). AUSLAN was produced by Kadous (2002) and contains 2,565 samples of a native signer signing 95 different words or phrases while wearing high quality position tracking gloves. Each time series sample is, on average, 57 frames in length and includes 22 features: roll, pitch, yaw, finger bend, and the 3D coordinates of each hand. To preprocess the raw data we first locally centre each sample and then apply global normalization. For evaluation, we perform cross validation with 5 folds, as is common practice for the AUSLAN dataset.\nThe baseline model for these tests was a two layer MLP with 512 hidden units in each layer, with dropout (p = 0.5) applied on each. Similar to Arabic Digits, dataset we find that the simple MLP can achieve competitive results when trained on the context vectors extracted from the sequence autoencoder (see Table 2). In this case, however, we observe that adding random noise to the context vectors did not improve performance. One possible explanation for this outcome is that the AUSLAN dataset has much more classes than the Arabic Digits dataset (95 versus 10) so there is higher probability of a randomly augmented context vector jumping from one class manifold to another. Traversing instead along the representational manifold in a directed manner by extrapolating between neighbouring samples results in improved performance over that of the baseline model. Our results also match the performance of Rodrı́guez et al. (2005), which to our knowledge is the best 5-fold cross validation result for the AUSLAN dataset."
    }, {
      "heading" : "4.5 UCFKINECT",
      "text" : "The final time series dataset we considered was the UCF Kinect action recognition dataset (Ellis et al., 2013). It contains motion capture data of participants performing 16 different actions such as run, kick, punch, and hop. The motion capture data consists of 3-dimensional coordinates for 15 skeleton joints for a total of 45 attributes per frame. In total there are 1,280 samples within the dataset. To preprocess the dataset we first shift the coordinates of each sample so that the central shoulder joint of the first frame is located at the origin. Global normalization is also applied.\nWith the UCFKinect dataset our main goal was to determine the effectiveness of interpolation in feature space for generating new sequences that combine the characteristics and actions of the two “seed” examples. We found that in order to produce natural looking results, the two actions to be combined must already share some properties. For example, Figure 4a and 4b show motion capture sequences of a person stepping forward and a person stepping to the left, respectively. Both of these actions take approximately the same amount of time to perform, and each skeleton moves their left leg first, then their right leg. Due to these preexisting similarities the action sequences can be interpolated in feature space to produce a natural looking sequence of a skeleton stepping diagonally forward and to the left (Figure 4c). These results emulate what was previously observed in Section 4.3, which indicated that similar properties are necessary for successful blending of examples.\nOur secondary goal with the UCFKinect dataset was to quantitatively evaluate the performance of extrapolation-based data augmentation. To compare to previous results, we used 4-fold cross validation (see Table 3 for a summary of results). We found that extrapolating between samples in\nrepresentational space improved the performance of our untuned model by more than 1%, which is quite significant. Our results are 2.5 percentage points below the current state-of-the-art result produced by Beh et al. (2014), but further tuning of the model could improve results."
    }, {
      "heading" : "4.6 IMAGE CLASSIFICATION: MNIST AND CIFAR-10",
      "text" : "Having successfully applied dataset augmentation in feature space to improve the accuracy of sequence classification tasks, we now experiment with applying our technique to static data. For these experiments we concentrate on the image domain where manual data augmentation is already prevalent. We find that augmenting datasets by extrapolating within a learned feature space improves classification accuracy compared to no data augmentation, and in some cases surpasses traditional (manual) augmentation in input space.\nIn our experiments we consider two commonly used small-scale image datasets: MNIST and CIFAR-10. MNIST consists of 28×28 greyscale images containing handwritten digits from 0 to 9. There are 60,000 training images and 10,000 test images in the official split. CIFAR-10 consists of 32×32 colour images containing objects in ten generic object categories. This dataset is typically split into 50,000 training and 10,000 test images.\nIn all of our image experiments, we apply the same sequence autoencoder (SA) architecture as shown in Figure 1a to learn a representation. No pre-processing beyond a global scaling is applied to the MNIST dataset. For CIFAR-10 we apply global normalization and the same crop and flip operations\nthat Krizhevsky et al. (2012) used for input space data augmentation when training AlexNet (we crop to 24×24). To simulate sequence input the images are fed into the network one row of pixels per time step similar to the SA setup in (Dai & Le, 2015).\nFor each dataset we train a 2-layer MLP on the context vectors produced by the sequence encoder. Both MLP and SA use the same number of hidden units in each layer: 256 per layer for MNIST and 1024 per layer for CIFAR-10. We conduct four different test scenarios on the MNIST dataset. To control for the representation, as a baseline we trained the classifier only on context vectors from the original images (i.e. SA with no augmentation). We then compare this to training with various kinds of dataset augmentation: traditional affine image transformations in input space (shifting, rotation, scaling), extrapolation between nearest neighbours in input space, and extrapolation between nearest neighbours in representational space. For both extrapolation experiments we use three nearest neighbours per sample and γ = 0.5 when generating new data. For CIFAR-10, our baseline is trained using context vectors extracted from cropped and flipped images. Against this baseline we test the addition of extrapolation between nearest neighbours in representational space, using the same setup as the MNIST test. Due to the size of the datasets we apply an approximate nearest neighbour algorithm (Wan et al., 2016).\nResults are reported in Table 4. For MNIST, we find that extrapolating in feature space not only performs better than the baseline, but it also achieves a lower error rate compared to domain-specific data augmentation in input space. A similar outcome is observed in CIFAR-10, where feature space extrapolation reduces error rate by 0.3%. Interestingly, we note that the baseline test for this dataset already leveraged image transformations to improve performance, so the additional reduction in error rate could indicate that both kinds of augmentation, extrapolation in feature space and manual transformation in pixel space, could complement each other."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we demonstrate a new domain-independent data augmentation technique that can be used to improve performance when training supervised learning models. We train a sequence autoencoder to construct a learned feature space in which we extrapolate between samples. This technique allows us to increase the amount of variability within the dataset, ultimately resulting in a more robust model. We demonstrate our technique quantitatively on five datasets from different domains (speech, sensor processing, motion capture, and images) using the same simple architecture and achieve near state-of-the-art results on two of them. Moreover, we show that data augmentation in feature space may complement domain-specific augmentation.\nAn important finding is that the extrapolation operator, when used in feature space, generated useful synthetic examples while noise and interpolation did not. Additional synthetic data experiments where we could control the complexity of the decision boundary revealed that extrapolation only improved model performance in cases where there were complex class boundaries. In cases with simple class boundaries, such as linear separability or one class encircling another, extrapolation hindered model performance, while interpolation helped. Our current hypothesis is that interpolation tends to tighten class boundaries and unnecessarily increase confidence, leading to overfitting. This behaviour may cause the model to ignore informative extremities that can describe a complex decision boundary and as a result produce an unnecessarily smooth decision boundary. As most high-dimensional, real datasets will typically have complex decision boundaries, we find extrapolation to be well suited for feature space dataset augmentation."
    } ],
    "references" : [ {
      "title" : "GSNs: generative stochastic networks",
      "author" : [ "Guillaume Alain", "Yoshua Bengio", "Li Yao", "Jason Yosinski", "Éric Thibodeau-Laufer", "Saizheng Zhang", "Pascal Vincent" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Alain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alain et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end attentionbased large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio" ],
      "venue" : "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Hidden Markov model on a unit hypersphere space for gesture trajectory recognition",
      "author" : [ "Jounghoon Beh", "David K Han", "Ramani Durasiwami", "Hanseok Ko" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Beh et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Beh et al\\.",
      "year" : 2014
    }, {
      "title" : "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Chan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2016
    }, {
      "title" : "Smote: synthetic minority over-sampling technique",
      "author" : [ "Nitesh V. Chawla", "Kevin W. Bowyer", "Lawrence O. Hall", "W. Philip Kegelmeyer" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Chawla et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chawla et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Çalar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dai and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "Exploring the trade-off between accuracy and observational latency in action recognition",
      "author" : [ "Chris Ellis", "Syed Zain Masood", "Marshall F Tappen", "Joseph J Laviola Jr.", "Rahul Sukthankar" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Ellis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ellis et al\\.",
      "year" : 2013
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Spoken Arabic digits recognition using MFCC based on GMM",
      "author" : [ "Nacereddine Hammami", "Mouldi Bedda", "Nadir Farah" ],
      "venue" : "In Sustainable Utilization and Development in Engineering and Technology (STUDENT),",
      "citeRegEx" : "Hammami et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hammami et al\\.",
      "year" : 2012
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal classification: Extending the classification paradigm to multivariate time series",
      "author" : [ "Mohammed Waleed Kadous" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Kadous.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kadous.",
      "year" : 2002
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In The International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik Kingma", "Max Welling" ],
      "venue" : "In The International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition",
      "author" : [ "Xiangang Li", "Xihong Wu" ],
      "venue" : "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Li and Wu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li and Wu.",
      "year" : 2015
    }, {
      "title" : "The UJIpenchars database: a pen-based database of isolated handwritten characters",
      "author" : [ "David Llorens", "Federico Prat", "Andrés Marzal", "Juan Miguel Vilar", "Marı́a José Castro", "Juan-Carlos Amengual", "Sergio Barrachina", "Antonio Castellanos", "Salvador España Boquera", "JA Gómez" ],
      "venue" : "In LREC,",
      "citeRegEx" : "Llorens et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Llorens et al\\.",
      "year" : 2008
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep directed generative autoencoders",
      "author" : [ "Sherjil Ozair", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1410.0630,",
      "citeRegEx" : "Ozair and Bengio.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ozair and Bengio.",
      "year" : 2014
    }, {
      "title" : "Playing for data: Ground truth from computer games",
      "author" : [ "Stephan R Richter", "Vibhav Vineet", "Stefan Roth", "Vladlen Koltun" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Richter et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Richter et al\\.",
      "year" : 2016
    }, {
      "title" : "Support vector machines of intervalbased features for time series classification",
      "author" : [ "Juan José Rodrı́guez", "Carlos J Alonso", "José A Maestro" ],
      "venue" : "Knowledge-Based Systems,",
      "citeRegEx" : "Rodrı́guez et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rodrı́guez et al\\.",
      "year" : 2005
    }, {
      "title" : "Exploring data augmentation for improved singing voice detection with neural networks",
      "author" : [ "Jan Schlüter", "Thomas Grill" ],
      "venue" : "In International Society for Music Information Retrieval Conference (ISMIR),",
      "citeRegEx" : "Schlüter and Grill.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schlüter and Grill.",
      "year" : 2015
    }, {
      "title" : "Real-time human pose recognition in parts from single depth images",
      "author" : [ "Jamie Shotton", "Toby Sharp", "Alex Kipman", "Andrew Fitzgibbon", "Mark Finocchio", "Andrew Blake", "Mat Cook", "Richard Moore" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Shotton et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shotton et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised learning of video representations using lstms",
      "author" : [ "Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence-video to text",
      "author" : [ "Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Venugopalan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Venugopalan et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "In International Conference on Machine Learning: Deep Learning Workshop,",
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Under review as a conference paper at ICLR",
      "author" : [ "Ji Wan", "Sheng Tang", "Yongdong Zhang", "Jintao Li", "Pengcheng Wu", "Steven CH Hoi" ],
      "venue" : "HDidx:",
      "citeRegEx" : "Wan et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "One variant of this approach is to create synthetic data from a simulation such as a computer graphics engine (Shotton et al., 2013; Richter et al., 2016), however, this may not work if the simulation is not a good representation of the real world domain.",
      "startOffset" : 110,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "One variant of this approach is to create synthetic data from a simulation such as a computer graphics engine (Shotton et al., 2013; Richter et al., 2016), however, this may not work if the simulation is not a good representation of the real world domain.",
      "startOffset" : 110,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : ", variational autoencoders (Kingma & Welling, 2014), generative adversarial networks (Goodfellow et al., 2014), and generative stochastic networks (Alain et al.",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : ", 2014), and generative stochastic networks (Alain et al., 2016), each of which could be used to generate useful feature spaces for augmentation.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : ", 2013; Richter et al., 2016), however, this may not work if the simulation is not a good representation of the real world domain. Another option is dataset augmentation, wherein the existing data is transformed in some way to create new data that appears to come from the same (conditional) data generating distribution (Bengio et al., 2011). The main challenge with such an approach is that domain expertise is required to ensure that the newly generated data respects valid transformations (i.e. those that would occur naturally in that domain). In this work, we consider augmentation not by a domain-specific transformation, but by perturbing, interpolating, or extrapolating between existing examples. However, we choose to operate not in input space, but in a learned feature space. Bengio et al. (2013) and Ozair & Bengio (2014) claimed that higher level representations expand the relative volume of plausible data points within the feature space, conversely shrinking the space allocated for unlikely data points.",
      "startOffset" : 8,
      "endOffset" : 810
    }, {
      "referenceID" : 18,
      "context" : ", 2013; Richter et al., 2016), however, this may not work if the simulation is not a good representation of the real world domain. Another option is dataset augmentation, wherein the existing data is transformed in some way to create new data that appears to come from the same (conditional) data generating distribution (Bengio et al., 2011). The main challenge with such an approach is that domain expertise is required to ensure that the newly generated data respects valid transformations (i.e. those that would occur naturally in that domain). In this work, we consider augmentation not by a domain-specific transformation, but by perturbing, interpolating, or extrapolating between existing examples. However, we choose to operate not in input space, but in a learned feature space. Bengio et al. (2013) and Ozair & Bengio (2014) claimed that higher level representations expand the relative volume of plausible data points within the feature space, conversely shrinking the space allocated for unlikely data points.",
      "startOffset" : 8,
      "endOffset" : 836
    }, {
      "referenceID" : 4,
      "context" : "In the context of class-imbalanced data, Chawla et al. (2002) proposed interpolating between samples in feature space.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : ", 2015b), video captioning (Venugopalan et al., 2015), speech recognition ((Chan et al.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : ", 2015), speech recognition ((Chan et al., 2016), (Bahdanau et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2016), (Bahdanau et al., 2016)), machine translation ((Jean et al.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : ", 2016)), machine translation ((Jean et al., 2015), (Luong et al.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : ", 2015), (Luong et al., 2015)), text parsing (Vinyals et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 24,
      "context" : "The seq2seq architecture can also be used to create sequence autoencoders (SA) by creating a model that learns to reconstruct input sequences in its output (Srivastava et al., 2015; Dai & Le, 2015).",
      "startOffset" : 156,
      "endOffset" : 197
    }, {
      "referenceID" : 10,
      "context" : "When training LeNet5, one of the most early and well-known convolutional neural network architectures, LeCun et al. (1998) applied a series of transformations to the input images in order to improve the robustness of the model.",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "Krizhevsky et al. (2012) also used image transformations to generate new data when training the renowned AlexNet model for the 2012 Large Scale Visual Recognition Challenge (ILSVRC).",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "Krizhevsky et al. (2012) also used image transformations to generate new data when training the renowned AlexNet model for the 2012 Large Scale Visual Recognition Challenge (ILSVRC). They claimed that dataset augmentation reduced the error rate of the model by over 1%. Creating new data has since been a crucial component of all recent large-scale image recognition models. Unfortunately, dataset augmentation is not as straightforward to apply in all domains as it is for images. For example, Schlüter & Grill (2015) investigated a variety of data augmentation techniques for application to singing voice detection.",
      "startOffset" : 0,
      "endOffset" : 519
    }, {
      "referenceID" : 3,
      "context" : "Important to our work are sequence-to-sequence learning (seq2seq) models which were first developed independently by Cho et al. (2014) and Sutskever et al.",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "Important to our work are sequence-to-sequence learning (seq2seq) models which were first developed independently by Cho et al. (2014) and Sutskever et al. (2014). Generally these models convert a sequence of inputs from one domain into a fixed-length context vector which is then used to generate an output sequence, usually from a different domain.",
      "startOffset" : 117,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "We instead modify the above equation so that the decoder is conditioned on the context vector at each time step as was done in (Cho et al., 2014): y0 = f(s0, c) yt = f(st−1,yt−1, c).",
      "startOffset" : 127,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Dai and Le follow the original seq2seq approach of Sutskever et al. (2014) and use the context vector as input to the decoder only on the first time step, then use the output of the previous times step as inputs for all subsequent time steps as follows: y0 = f(s0, c) yt = f(st−1,yt−1) where f is the LSTM function, s is the state of the LSTM (both hidden and cell state), c is the context vector, and y is the output of the decoder.",
      "startOffset" : 0,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "A more directed approach for data augmentation follows the techniques introduced by Chawla et al. (2002). For each sample in the dataset, we find itsK nearest neighbours in feature space which share its class label.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 25,
      "context" : "Finally, we reversed the order of the input sequences as suggested by Sutskever et al. (2014). We found that reversing the order of input sequences caused the model to train faster and achieve better final solutions.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "The UJI Pen Characters dataset (v2) contains 11,640 instances of 97 different characters handwritten by 60 participants (Llorens et al., 2008).",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "11 (Hammami et al., 2012) 0.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Our results rival those of Hammami et al. (2012), which to our knowledge are state-of-the-art on this dataset.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "AUSLAN was produced by Kadous (2002) and contains 2,565 samples of a native signer signing 95 different words or phrases while wearing high quality position tracking gloves.",
      "startOffset" : 23,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "AUSLAN was produced by Kadous (2002) and contains 2,565 samples of a native signer signing 95 different words or phrases while wearing high quality position tracking gloves. Each time series sample is, on average, 57 frames in length and includes 22 features: roll, pitch, yaw, finger bend, and the 3D coordinates of each hand. To preprocess the raw data we first locally centre each sample and then apply global normalization. For evaluation, we perform cross validation with 5 folds, as is common practice for the AUSLAN dataset. The baseline model for these tests was a two layer MLP with 512 hidden units in each layer, with dropout (p = 0.5) applied on each. Similar to Arabic Digits, dataset we find that the simple MLP can achieve competitive results when trained on the context vectors extracted from the sequence autoencoder (see Table 2). In this case, however, we observe that adding random noise to the context vectors did not improve performance. One possible explanation for this outcome is that the AUSLAN dataset has much more classes than the Arabic Digits dataset (95 versus 10) so there is higher probability of a randomly augmented context vector jumping from one class manifold to another. Traversing instead along the representational manifold in a directed manner by extrapolating between neighbouring samples results in improved performance over that of the baseline model. Our results also match the performance of Rodrı́guez et al. (2005), which to our knowledge is the best 5-fold cross validation result for the AUSLAN dataset.",
      "startOffset" : 23,
      "endOffset" : 1465
    }, {
      "referenceID" : 21,
      "context" : "26 (Rodrı́guez et al., 2005) 1.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "The final time series dataset we considered was the UCF Kinect action recognition dataset (Ellis et al., 2013).",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "5 percentage points below the current state-of-the-art result produced by Beh et al. (2014), but further tuning of the model could improve results.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "61 (Beh et al., 2014) 1.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "that Krizhevsky et al. (2012) used for input space data augmentation when training AlexNet (we crop to 24×24).",
      "startOffset" : 5,
      "endOffset" : 30
    } ],
    "year" : 2017,
    "abstractText" : "Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.",
    "creator" : "LaTeX with hyperref package"
  }
}