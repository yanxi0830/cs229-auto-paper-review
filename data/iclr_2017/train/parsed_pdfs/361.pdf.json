{
  "name" : "361.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Aaron Klein", "Stefan Falkner", "Jost Tobias Springenberg", "Frank Hutter" ],
    "emails" : [ "kleinaa@cs.uni-freiburg.de", "sfalkner@cs.uni-freiburg.de", "springj@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. The same information can be exploited in automatic hyperparameter optimization by means of a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep learning has celebrated many successes, but its performance relies crucially on good hyperparameter settings. Bayesian optimization (e.g, Brochu et al. (2010); Snoek et al. (2012); Shahriari et al. (2016)) is a powerful method for optimizing the hyperparameters of deep neural networks (DNNs). However, its traditional treatment of DNN performance as a black box poses fundamental limitations for large and computationally expensive data sets, for which training a single model can take weeks. Human experts go beyond this blackbox notion in their manual tuning and exploit cheaper signals about which hyperparameter settings work well: they estimate overall performance based on runs using subsets of the data or initial short runs to weed out bad parameter settings; armed with these tricks, human experts can often outperform Bayesian optimization.\nRecent extensions of Bayesian optimization and multi-armed bandits therefore also drop the limiting blackbox assumption and exploit the performance of short runs (Swersky et al., 2014; Domhan et al., 2015; Li et al., 2017), performance on small subsets of the data (Klein et al., 2017), and performance on other, related data sets (Swersky et al., 2013; Feurer et al., 2015).\nWhile traditional solutions for scalable Bayesian optimization include approximate Gaussian process models (e.g., Hutter et al.; Swersky et al. (2014)) and random forests (Hutter et al., 2011), a recent trend is to exploit the flexible model class of neural networks for this purpose (Snoek et al., 2015; Springenberg et al., 2016). In this paper, we study this model class for the prediction of learning curves. Our contributions in this paper are:\n1. We study how well Bayesian neural networks can fit learning curves for various architectures and hyperparameter settings, and how reliable their uncertainty estimates are.\n2. Building on the parametric learning curve models of Domhan et al. (2015), we develop a specialized neural network architecture with a learning curve layer that improves learning curve predictions.\n3. We compare different ways to generate Bayesian neural networks: probabilistic back propagation (Hernández-Lobato and Adams, 2015) and two different stochastic gradient based Markov Chain Monte Carlo (MCMC) methods – stochastic gradient Langevin dynamics (SGLD (Welling and Teh, 2011)) and stochastic gradient Hamiltonian MCMC (SGHMC (Chen et al., 2014)) – for standard Bayesian neural networks and our specialized architecture and show that SGHMC yields better uncertainty estimates.\n4. We evaluate the predictive quality for both completely new learning curves and for extrapolating partially-observed curves, showing better performance than the parametric function approach by Domhan et al. (2015) at stages were learning curves have not yet converged.\n5. We extend the recent multi-armed bandit strategy Hyperband (Li et al., 2017) by sampling using our model rather than uniformly at random, thereby enabling it to approach nearoptimal configurations faster than traditional Bayesian optimization."
    }, {
      "heading" : "2 PROBABILISTIC PREDICTION OF LEARNING CURVES",
      "text" : "In this section, we describe a general framework to model learning curves of iterative machine learning methods. We first describe the approach by Domhan et al. (2015) which we will dub LCExtrapolation from here on. Afterwards, we discuss a more general joint model across time steps and hyperparameter values that can exploit similarities between hyperparameter configurations and predict for unobserved learning curves. We also study the observation noise of different hyperparameter configurations and show how we can adapt our model to capture this noise."
    }, {
      "heading" : "2.1 LEARNING CURVE PREDICTION WITH BASIS FUNCTION",
      "text" : "An intuitive model for learning curves proposed by Domhan et al. (2015) uses a set of k different parametric functions φi(θi, t) ∈ {φ1(θ1, t), ..., φk(θk, t)} to extrapolate learning curves (y1, . . . , yn) from the first n time steps. Each parametric function φi depends on a time step t ∈ [1, T ] and on a parameter vector θi. The individual functions are combined into a single model by a weighted linear combination\nf̂(t|Θ,w) = k∑\ni=1\nwiφi(t,θi) , (1)\nwhere Θ = (θ1, . . . ,θk) denotes the combined vector of all parameters θ1, . . . ,θk, and w = (w1, . . . , wk) is the concatenated vector of the respective weights of each function. Assuming observation noise around the true but unknown value f(t), i.e., assuming yt ∼ N (f̂(t|Θ,w), σ2), Domhan et al. (2015) define a prior for all parameters P (Θ,w, σ2) and use a gradient-free MCMC method (Foreman-Mackey et al., 2013) to obtain S samples, (Θ1,w1, σ21), . . . , (ΘS ,wS , σ 2 S), from the posterior\nP (Θ,w, σ2|y1 . . . , yn) ∝ P (y1, . . . , yn|Θ,w, σ2)P (Θ,w, σ2) (2) using the likelihood\nP (y1, . . . , yn|Θ,w, σ2) = n∏\nt=1\nN (yt; f̂(t|Θ,w), σ2) . (3)\nThese samples then yield probabilistic extrapolations of the learning curve for future time steps m, with mean and variance predictions\nŷm = E[ym|y1, . . . yn] ≈ 1\nS\nS∑\ns=1\nf̂(m|Θs,ws) , and\nvar(ŷm) ≈ 1\nS\nS∑\ns=1\n(f̂(m|Θs,ws)− ŷm)2 + S∑\ns=1\nσ2s .\n(4)\nFor our experiments, we use the original implementation by Domhan et al. (2015) with one modification: the original code included a term in the likelihood that enforced the prediction at t = T to be strictly greater than the last value of that particular curve. This biases the estimation to never underestimate the accuracy at the asymptote. We found that in some of our benchmarks, this led to instabilities, especially with very noisy learning curves. Removing it cured that problem, and we did not observe any performance degradation on any of the other benchmarks.\nThe ability to include arbitrary parametric functions makes this model very flexible, and Domhan et al. (2015) used it successfully to terminate evaluations of poorly-performing hyperparameters early for various different architectures of neural networks (thereby speeding up Bayesian optimization by a factor of two). However, the model’s major disadvantage is that it does not use previously evaluated hyperparameters at all and therefore can only make useful predictions after observing a substantial initial fraction of the learning curve."
    }, {
      "heading" : "2.2 LEARNING CURVE PREDICTION WITH BAYESIAN NEURAL NETWORKS",
      "text" : "In practice, similar hyperparameter configurations often lead to similar learning curves, and modelling this dependence would allow predicting learning curves for new configurations without the need to observe their initial performance. Swersky et al. (2014) followed this approach based on an approximate Gaussian process model. Their Freeze-Thaw method showed promising results for finding good hyperparameters of iterative machine learning algorithms using learning curve prediction to allocate most resources for well-performing configurations during the optimization. The method introduces a special covariance function corresponding to exponentially decaying functions to model the learning curves. This results in an analytically tractable model, but using different functions to account for cases where the learning curves do not converge exponentially is not trivial.\nHere, we formulate the problem using Bayesian neural networks. We aim to model the validation accuracy g(x, t) of a configuration x ∈ X ⊂ Rd at time step t ∈ (0, 1] based on noisy observations y(x, t) ∼ N (g(x, t), σ2). For each configuration x trained for Tx time steps, we obtain Tx data points for our model; denoting the combined data by D = {(x1, t1, y11)), (x1, t2, y12), . . . , (xn, Txn , ynTxn )} we can then write the joint probability of the data D and the network weights W as\nP (D,W ) = P (W )P (σ2) |D|∏\ni=1\nN (yi; ĝ(xi, ti|W ), σ2) . (5)\nwhere ĝ(xi, ti|W ) is the prediction of a neural network. It is intractable to compute the posterior weight distribution p(W |D), but we can use MCMC to sample it, in particular stochastic gradient\nMCMC methods, such as SGLD (Welling and Teh, 2011) or SGHMC (Chen et al., 2014). Given M samples W 1, . . . ,WM , we can then obtain the mean and variance of the predictive distribution p(g|x, t,D) as\nµ̂(x, t|D) = 1 M\nM∑\ni=1\nĝ(x, t|W i) , and\nσ̂2(x, t|D) = 1 M\nM∑\ni=1\n( ĝ(x, t|W i)− µ̂(x, t|D) )2 ,\n(6)\nrespectively. We will write these shorthand as µ̂(x, t) and σ̂2(x, t). This is similar to Eqs. 4 and exactly the model that Springenberg et al. (2016) used for (blackbox) Bayesian optimization with Bayesian neural networks; the only difference is in the input to the model: here, there is a data point for every time step of the curve, whereas Springenberg et al. (2016) only used a single data point per curve (for its final time step)."
    }, {
      "heading" : "2.3 HETEROSCEDASTIC NOISE OF HYPERPARAMETER CONFIGURATION",
      "text" : "In the model described above, we assume homoscedastic noise across hyperparameter configurations. To evaluate how realistic this assumption is, we sampled 40 configurations of a fully connected network (see Section 3.1 for a more detailed description of how the data was generated and Table 2 for the list of hyperparameters) and evaluated each configuration R = 10 times with different pseudorandom number seeds (see the right panel of Figure 3 for some examples). Figure 3 (left) shows on the vertical axis the noise σ̄2(x, t) = 1R ∑R r=1(y(x, t)− µ̄(x, t))2 and on the horizontal axis the rank of each configuration based on their asymptotic sample mean performance µ̄(x, t = 1) = 1R ∑R r=1 y(x, t = 1).\nMaybe not surprisingly, the noise seems to correlate with the asymptotic performance of a configuration. The fact that the noise between different configurations varies on different orders of magnitudes\nsuggests a heteroscedastic noise model to best describe this behavior. We incorporate this observation by making the noise dependent on the input data to allow to predict different noise levels for different hyperparameters. In principle, one could also model a t dependent noise, but we could not find the same trend across different datasets."
    }, {
      "heading" : "2.4 NEW BASIS FUNCTION LAYER FOR LEARNING CURVE PREDICTION WITH BAYESIAN",
      "text" : "NEURAL NETWORKS\nweights w, and the noise σ̂2.\nWe now combine Bayesian neural networks with the parametric functions to incorporate more knowledge about learning curves into the network itself. Instead of obtaining the parameters Θ and w by sampling from the posterior, we use a Bayesian neural network to learn several mappings simultaneously:\n1. µ̂∞: X → R, the asymptotic value of the learning curve\n2. Θ: X → RK , the parameters of a parametric function model (see Figure 2 for some example curves from our basis functions)\n3. w: X → Rk, the corresponding weights for each function in the model\n4. σ̂2: X → R+, the observational noise for this hyperparameter configuration\nWith these quantities, we can compute the likelihood in (3) which allows training the network.\nPhrased differently, we use a neural network to predict the model parameters Θ and weightsw of our parametric functions, yielding the following form for our network’s mean predictions:\nĝ(xi, ti|W ) = f̂(ti|Θ(xi,W ),w(xi,W )). (7)\nA schematic of this is shown in Fig. 4. For training, we will use the same MCMC methods, namely SGLD and SGHMC mentioned above."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "We now empirically evaluate the predictive performance of Bayesian neural networks, with and without our special learning curve layer. For both networks, we used a 3-layer architecture with tanh activations and 64 units per layer. We also evaluate two different sampling methods for both types of networks: stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian MCMC (SGHMC), following the approach of Springenberg et al. (2016) to automatically adapt the noise estimate and the preconditioning of the gradients.\nAs baselines, we compare to other approaches suitable for this task. Besides the aforementioned work by Domhan et al. (2015), we also compare against random forests Breimann (2001) with empirical variance estimates (Hutter et al., 2014), a Gaussian process (GP) using the learning curve kernel from Swersky et al. (2014), another Bayesian neural network technique called probabilistic back propagation (PBP) (Hernández-Lobato and Adams, 2015), and the simple heuristic of using the last seen value (LastSeenValue) of each learning curve for extrapolation. The last model has been successfully used by Li et al. (2017) despite its simplicity."
    }, {
      "heading" : "3.1 DATASETS",
      "text" : "For our empirical evaluation, we generated the following four datasets of learning curves, in each case sampling hyperparameter configurations at random from the hyperparameter spaces detailed in Table 2 in the appendix (see also Section D for some characteristic of these datasets):\n• CNN: We sampled 256 configurations of 5 different hyperparameters of a 3-layer convolutional neural network (CNN) and trained each of them for 40 epochs on the CIFAR10 (Krizhevsky, 2009) benchmark. • FCNet: We sampled 4096 configurations of 10 hyperparameters of a 2-layer feed forward\nneural network (FCNet) on MNIST (LeCun et al., 2001), with batch normalization, dropout and ReLU activation functions, annealing the learning rate over time according to a power function. We trained the neural network for 100 epochs. • LR: We sampled 1024 configurations of the 4 hyperparameters of logistic regression (LR)\nand also trained it for 100 epochs on MNIST. • VAE: We sampled 1024 configuration of the 4 hyperparameters of a variational auto-\nencoder (VAE) (Kingma and Welling, 2014). We trained the VAE on MNIST, optimizing the approximation of the lower bound for 300 epochs."
    }, {
      "heading" : "3.2 PREDICTING ASYMPTOTIC VALUES OF PARTIALLY OBSERVED CURVES",
      "text" : "We first study the problem of predicting the asymptotic values of partially-observed learning curves tackled by Domhan et al. (2015). The LC-Extrapolation method by Domhan et al. (2015), the GP, and the last seen value work on individual learning curves and do not allow to model performance across hyperparameter configurations. Thus, we trained them separately on individual partial learning curves. The other models, including our Bayesian neural networks, on the other hand, can use training data from different hyperparameter configurations. Here, we used training data with the same number of epochs for every partial learning curve.1\nThe left panel of Figure 5 visualizes the extrapolation task, showing a learning curve from the CNN dataset and the prediction of the various models trained only using the first 12 of 40 epochs of the learning curve. The right panel shows the corresponding predictive distributions obtained with these models. LastSeenValue does not yield a distribution and uncertainties are not defined.\nFor a more quantitative evaluation, we used all models to predict the asymptotic value of all learning curves, evaluating predictions based on observing between 10% and 90% of the learning curves.\n1We note that when used inside Bayesian optimization, we would have access to a mix of fully-converged and partially-converged learning curves as training data, and could therefore expect better extrapolation performance.\nFigure 6 shows the mean squared error between the true asymptotic value and the models’ predictions (left) and the average log-likelihood of the true value given each model as a function of how much of the learning curves has been observed. Note that we removed some methods from the plots to avoid clutter; the same plots with all methods can be found in the supplementary material. We notice several patterns:\n1. Throughout, our specialized network architecture performs better than the standard Bayesian neural networks, and SGHMC outperformed SGLD (see Appendix C).\n2. PBP shows mixed results for the extrapolated mean, and does not provide reliable uncertainties (also see Appendix C). We hypothesize that this may be due to its linear activation functions.\n3. If no uncertainties are required, LastSeenValue is hard to beat. This is because many configurations approach their final performance quite quickly.\n4. The GP mean predictions are very competitive, but the average log-likelihood indicates overconfidence, especially for short learning curves. We assume that the prior assumption of an exponential function is not flexible enough in practice and that after observing some data points the Gaussian process becomes too certain of its predictions.\n5. The random forest’s mean predictions almost match the quality of the GP. The uncertainty estimates are better, but still too aggressive when only a small fraction of the learning curve was observed. Random forests do not extrapolate and similar to LastSeenValue also achieve a very good mean prediction if the learning curve has almost converged.\n6. Local models for single curves clearly outperform global models for almost complete learning curves. Global models, like our BNN approach, on the other hand, are trained on many configurations and need to generalize across these, yielding somewhat worse performance for this (arguably easy) task.2"
    }, {
      "heading" : "3.3 PREDICTING UNOBSERVED LEARNING CURVES",
      "text" : "As mentioned before, training a joint model across hyperparameters and time steps allows us to make predictions for completely unobserved learning curves of new configurations. To estimate how well Bayesian neural networks perform in this task, we used the datasets from Section 3.1 and split all of them into 16 folds, allowing us to perform cross-validation of the predictive performance. For each fold, we trained all models on the full learning curves in the training set and let them predict the held-out learning curves.\nTable 1 shows the mean squared error and the average log-likelihood (both computed for all points in each learning curve) across the 16 folds. We make two observations: firstly, both neural network architectures lead to a reasonable mean squared error and average log-likelihood, for both SGLD and SGHMC. Except on the VAE dataset our learning curve layer seems to improve mean squared error and average log likelihood. Secondly, SGHMC performed better than SGLD, with the latter resulting in predictions with too small variances. Figure 7 visualizes the results for our CNN dataset in more detail, showing that true and predicted accuracies correlate quite strongly.\n2In the future, we aim to improve our BNN architecture for this case of partially-observed learning curves by also giving the network access to the partial learning curve it should extrapolate."
    }, {
      "heading" : "3.4 MODEL-BASED HYPERBAND",
      "text" : "In this section, we show how our model can be used to improve hyperparameter optimization of iterative machine learning algorithms. For this, we extended the multi-armed bandit strategy Hyperband (Li et al., 2017), which in each iteration i first samples Ni hyperparameter configurations C = {x1, . . . ,xNi} and then uses successive halving (Jamieson and Talwalkar, 2016) to iteratively discard poorly-performing configurations from C. While the original Hyperband method samples configurations C from a uniform distribution U over hyperparameter configurations, our extension instead samples them based on our model, with all other parts remaining unchanged. More precisely, we sample Mi >> Ni configurations uniformly, Ĉ = {x1, . . . ,xMi} ∼ U , and pick the Ni configurations with the highest predicted asymptotic mean a(x) = µ̂(x, t = 1) or upper confidence value a(x) = µ̂(x, t = 1) + σ̂(x, t = 1).\nFor a thorough empirical evaluation and to reduce computational requirements we reused the data from Section 3.1 to construct a surrogate benchmark based on a random forest (see Appendix E for more details). After each iteration we report the final performance of the best observed configuration so far, along with the wall clock time that would have been needed for optimizing the true objective function. Reporting the (predicted) wall clock time (rather than the number of iterations) also takes our optimizer’s overhead into account. This is important since one may worry that this overhead may be substantial due to our use of a model; however, as we show it does not harm performance.\nFigure 8 shows the immediate regret on the CNN benchmark as a function of wallclock time, for three optimizers: Hyperband, our model-based extension of Hyperband, as well as standard Bayesian optimization with a Gaussian process (a comparison to additional baselines can be found in Appendix E). Standard Bayesian optimization does not make use of learning curves and thus needs to evaluate each configuration for the full amount of epochs. Nevertheless, since training one configuration to the end takes less time than one round of successive halving, Bayesian optimization produces its first results earlier than Hyperband. In accordance with results by Li et al. (2017), in this experiment Hyperband found a configuration with good performance faster than standard Bayesian optimization, but its random sampling did not suffice to quickly approach the best configuration; given enough time Bayesian optimization performed better. However, extended by our model, Hyperband both found a good configuration fast and approached the global optimum fastest.\nWe would like to emphasize that our model-based extension of Hyperband is not limited to our particular Bayesian neural networks as the underlying model. Since Hyperband stops the majority of the configurations very early, the setting is quite similar to that of very short partially observed learning curves in Section 3.2, with the differences that some configurations have been evaluated for longer and that the model now has to generalize to unseen configurations. For example, for the CNN and LR benchmarks, the experiments in Section 3.2 showed that random forests achieve strong average log-likelihoods for very short partial learning curves, and we would therefore also expect them to work well when combined with Hyperband. However, given our model’s more robust likelihoods and mean squared error values we believe it to be less sensitive to the underlying data. Additionally, we can incorporate more prior knowledge about the general shape of the curves into our model, something that is not easily done for many other model classes."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "We studied Bayesian neural networks for modelling the learning curves of iterative machine learning methods, such as stochastic gradient descent for convolutional neural networks. Based on the parametric learning curve models of Domhan et al. (2015), we also developed a specialized neural network architecture with a learning curve layer that improves learning curve predictions. In future work, we aim to study recurrent neural networks for predicting learning curves and will extend Bayesian optimization methods with Bayesian neural networks (Springenberg et al., 2016) based on our learning curve models."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work has partly been supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme under grant no. 716721, by the European Commission under grant no. H2020-ICT-645403-ROBDREAM, and by the German Research Foundation (DFG) under Priority Programme Autonomous Learning (SPP 1527, grant HU 1900/3-1)."
    }, {
      "heading" : "A EXPERIMENTAL SETUP – DETAILS",
      "text" : "Table 2 shows the hyperparameters of our 4 benchmarks described in Section 3.1 and their ranges."
    }, {
      "heading" : "B DESCRIPTION OF THE BASIS FUNCTIONS",
      "text" : "To reduce complexity, we used a subset of the basis function from Domhan et al. (2015) which we found to be sufficient for learning curve prediction. We adapted these functions to model the residual between the asymptotic value y∞ and we scaled the parameters Θ to be in [0, 1]. Table 3 shows the exact equations we used."
    }, {
      "heading" : "C PREDICTING ASYMPTOTIC VALUES OF PARTIALLY OBSERVED CURVES",
      "text" : "Figure 9 shows the mean squared error and the average log-likelihood of predicting the asymptotic value after observing different amounts of the learning curve for all methods. See Section 3.2 in the main text for more details."
    }, {
      "heading" : "D DATASET CHARACTERISTICS",
      "text" : "Figure 10 shows the distributions over runtimes for all random configurations of different benchmarks. As it can be seen there is a high variance of the runtime between different configurations for all benchmarks and some configurations need order of magnitudes longer than others.\nFigure 11 shows the empirical cumulative distribution of the asymptotic performance of random configurations. These plots give an intuition about the difficulty of a benchmark as they show how many random configurations one has to sample in order to achieve a good performance."
    }, {
      "heading" : "E OPTIMIZATION ON SURROGATE BENCHMARKS",
      "text" : "We follow the approach by Eggensperger et al. (2015) and used the generated datasets from Section 3.1 to build surrogates of the objective functions. This allows us to compare different optimizers on this benchmark very efficiently while (approximately) preserving the characteristics of the underlying benchmark. Using surrogates instead of optimizing the real benchmarks allows us to carry out a more thorough empirical evaluation (since single function evaluations are cheap and we therefore can afford more and longer optimization runs). We used random forests as surrogates following Eggensperger et al. (2015) since they do not introduce a bias for our approach (in contrast to, e.g., surrogates based on standard feed forward neural networks trained with stochastic gradient descent ). For each benchmark, we used all configurations to train a random forest predicting the validation accuracy of a configuration at a certain time step as well as the wall clock time for its evaluation.\nAfter each round of successive halving, we return the current best observed configuration and its asymptotic performance. For each function evaluation, we predict the accuracy, and the runtime. By adding the optimization overhead, we can predict the wallclock time necessary to optimize the real objective function. Reporting the wallclock time rather than the number of iterations also takes the additional overhead from running our method into account. Furthermore, we argue that the wallclock time is more interesting in practice.\nAs baselines, we compare to Gaussian process based Bayesian optimization with the upper confidence bound acquisition function (GP-BO-UCB) and the (log) expected improvement acquisition function (GP-BO-Log-EI), as well as Bayesian optimization with Bayesian neural networks (BOHAMIANN) (Springenberg et al., 2016). As an additional baseline, we use standard Bayesian neural networks (SGHMC-BNN) inside Hyperband instead of our specialized neural network architecture (SGHMCLCNet). We compare our model-based version of Hyperband against all these baselines with sampling from the asymptotic mean (SGHMC-LCNet-mean-sampling) and the asymptotic upper confidence bound (SGHMC-LCNet-ucb-sampling). Figure 12 shows that our specialized neural network architecture helps to speed up the convergence of Hyperband on the CNN and the FCNet benchmark. For the LR benchmarks our model still improves over Hyperband but does not perform better than Bayesian optimization. Hyperband as well as our method find the optimum already after the first round of successive halving for the VAE benchmark, which leads to an regret of 0."
    } ],
    "references" : [ {
      "title" : "Freeze-thaw Bayesian optimization",
      "author" : [ "K. Swersky", "J. Snoek", "R. Adams" ],
      "venue" : "A Review of Bayesian Optimization. Proc. of the IEEE,",
      "citeRegEx" : "Swersky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Swersky et al\\.",
      "year" : 2015
    }, {
      "title" : "Initializing Bayesian hyperparameter optimization via",
      "author" : [ "M. Feurer", "T. Springenberg", "F. Hutter" ],
      "venue" : null,
      "citeRegEx" : "Feurer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Feurer et al\\.",
      "year" : 2017
    }, {
      "title" : "Algorithm runtime prediction: Methods",
      "author" : [ "L. Breimann" ],
      "venue" : "Random forests. MLJ,",
      "citeRegEx" : "2013",
      "shortCiteRegEx" : "2013",
      "year" : 2001
    }, {
      "title" : "Gradient-based learning applied to document",
      "author" : [ "Toronto", "2009. Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : null,
      "citeRegEx" : "Toronto et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Toronto et al\\.",
      "year" : 2009
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D. Kingma", "M. Welling" ],
      "venue" : "In Proc. of ICLR’14,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recent extensions of Bayesian optimization and multi-armed bandits therefore also drop the limiting blackbox assumption and exploit the performance of short runs (Swersky et al., 2014; Domhan et al., 2015; Li et al., 2017), performance on small subsets of the data (Klein et al., 2017), and performance on other, related data sets (Swersky et al., 2013; Feurer et al., 2015). While traditional solutions for scalable Bayesian optimization include approximate Gaussian process models (e.g., Hutter et al.; Swersky et al. (2014)) and random forests (Hutter et al.",
      "startOffset" : 163,
      "endOffset" : 527
    }, {
      "referenceID" : 0,
      "context" : "Swersky et al. (2014) followed this approach based on an approximate Gaussian process model.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : ", 2014), a Gaussian process (GP) using the learning curve kernel from Swersky et al. (2014), another Bayesian neural network technique called probabilistic back propagation (PBP) (Hernández-Lobato and Adams, 2015), and the simple heuristic of using the last seen value (LastSeenValue) of each learning curve for extrapolation.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : ", 2014), a Gaussian process (GP) using the learning curve kernel from Swersky et al. (2014), another Bayesian neural network technique called probabilistic back propagation (PBP) (Hernández-Lobato and Adams, 2015), and the simple heuristic of using the last seen value (LastSeenValue) of each learning curve for extrapolation. The last model has been successfully used by Li et al. (2017) despite its simplicity.",
      "startOffset" : 70,
      "endOffset" : 389
    }, {
      "referenceID" : 4,
      "context" : "• VAE: We sampled 1024 configuration of the 4 hyperparameters of a variational autoencoder (VAE) (Kingma and Welling, 2014).",
      "startOffset" : 97,
      "endOffset" : 123
    } ],
    "year" : 2017,
    "abstractText" : "Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. The same information can be exploited in automatic hyperparameter optimization by means of a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer.",
    "creator" : "LaTeX with hyperref package"
  }
}