{
  "name" : "522.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Yuandong Tian" ],
    "emails" : [ "yuandong@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∑K j=1 σ(w ᵀ j x),\nwhere σ(·) is ReLU nonlinearity. We assume that the input x follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters w∗ using l2 loss. We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w∗ with at least (1 − )/2 probability, if random weight initializations of proper standard derivation (∼ 1/ √ d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. For networks with many ReLU nodes (K ≥ 2), we apply our close form dynamics and prove that when the teacher parameters {w∗j}Kj=1 forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to w∗ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with l2 loss. Simulations verify our theoretical analysis."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al. (2012)]. However, till now, how and why it works remains elusive due to a lack of theoretical understanding. First, how simple approaches like gradient descent can solve a very complicated non-convex optimization effectively. Second, how the deep models, especially deep convolutional models, achieve generalization power despite massive parameters.\nIn this paper, we focus on the first problem and use dynamical system to analyze the nonlinear gradient descent dynamics of certain two-layered nonlinear network in the following form:\ng(x;w) = K∑ j=1 σ(wᵀj x) (1)\nwhere σ(x) = max(x, 0) is the ReLU nonlinearity. We consider the following setting: a student network learns the parameters that minimize the l2 distance between its prediction and the supervision provided by the teacher network of the same size with a fixed set of parameters w∗. We assume all inputs x to follow Gaussian distribution and thus the network is bias-free. Eqn. 1 is highly nonconvex and could contain exponential number of symmetrically equivalent solutions.\nTo analyze this, we first derive novel and concise gradient update rules for multilayer ReLU networks (See Lemma 2.1) in the teacher-student setting under l2 loss. Then for K = 1, we prove that the nonlinear gradient dynamics of Eqn. 1 has a close form and converges to w∗ with at least (1 −\n)/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K ≥ 2, we prove that when the teacher parameters {wj}Kj=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w∗, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed ‖w∗‖, showing that such a convergence behavior is beyond the local convex structure at w∗. To our knowledge, this is the first proof of its kind.\nPrevious works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function σ(x) when its derivatives σ′, σ′′, σ′′′ are bounded and σ′ > 0. Similar to our approach, [Saad & Solla (1996)] also uses the student-teacher setting and analyzes the dynamics of student network when the teacher’s parameters w∗ forms a orthonomal bases; however, it uses σ(x) = erf(x) as the nonlinearity and only analyzes the local behaviors of the two critical points (the saddle point in symmetric initializations, and w∗). In contrast, we prove the global convergence behavior in certain symmetry-breaking cases.\nMany previous works analyze nonlinear network based on the assumption of independent activations: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutually independent. For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network. [Janzamin et al. (2015)] gives guarantees on recovering the parameters of a 2-layered neural network learnt with tensor decomposition. In comparison, we analyze ReLU networks trained with gradient descent, which is a more popular setting in practice.\nThe paper is organized as follows. Sec. 2 introduces the basic formulation and some interesting novel properties of ReLU in multilayered ReLU networks. Sec. 3 and Sec. 4 then analyze the twolayered model Eqn. 1 for K = 1 and K ≥ 2, respectively. Sec. 5 shows that simulation results are consistent with theoretical analysis. Finally Sec. 7 gives detailed proofs for all theorems."
    }, {
      "heading" : "2 PRELIMINARY",
      "text" : ""
    }, {
      "heading" : "2.1 NOTATION",
      "text" : "Denote X as a N -by-d input data matrix and w∗ is the parameter of the teacher network with desired N -by-1 output u = g(X;w∗). Now suppose we have an estimator w and the estimated output v = g(X;w). We want to know with l2 loss E(w) = 12‖u − v‖ 2 = 12‖u − g(X;w)‖ 2, whether gradient descent will converge to the desired solution w∗.\nThe gradient descent update is w(t+1) = w(t) + η∆w(t), where ∆w(t) ≡ −∇E(w(t)). If we let η → 0, then the update rule becomes a first-order differential equation dw/dt = −∇E(w), or more concisely, ẇ = −∇E(w). In this case, Ė = ∇E(w)ᵀẇ = −‖∇E(w)‖2 ≤ 0, i.e., the function value E is nonincreasing over time. The key is to check whether there exist other critical points w 6= w∗ so that ∇E(w) = 0. In our analysis, we assume entries of inputX follow Gaussian distribution. In this situation, the gradient is a random variable and ∆w = −E [∇E(w)]. The expected E [E(w)] is also nonincreasing no matter whether we follow the expected gradient or the gradient itself, because\nE [ Ė ] = −E [∇E(w)ᵀ∇E(w)] ≤ −E [∇E(w)]ᵀ E [∇E(w)] ≤ 0 (2)\nTherefore, we analyze the behavior of expected gradient E [∇E(w)] rather than∇E(w)."
    }, {
      "heading" : "2.2 PROPERTIES OF RELU",
      "text" : "In this paper, we discover a few useful properties of ReLU that make our analysis much simpler. Denote D = D(w) = diag(Xw > 0) as a N -by-N diagonal matrix. The l-th diagnonal element of D is a binary variable showing whether the neuron is on for sample l. Using this notation, we could write σ(Xw) = DXw. Note that D only depends on the direction of w but not its magnitude.\nNote that for ReLU,D is also “tranparent” on derivatives. For example, the Jacobian Jw[σ(Xw)] = σ′(Xw)X = DX at differentiable regions. This gives a very concise rule for gradient descent in ReLU network: suppose we have negative gradient inflow vector g (of dimension N -by-1) on the current ReLU node with weights w, then we can simply write the update ∆w as:\n∆w = Jw[σ(Xw)] ᵀg = XᵀDg (3)\nThis can be easily applied to multilayer ReLU network. Denote j ∈ [c] if node j is in layer c, dc as the width of layer c, and uj and vj as the output of teacher network and student network, respectively. A simple deduction yields the following lemma:\nLemma 2.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher network of the same size, the negative gradient inflow gj for node j at layer c has the following form:\ngj = Lj ∑ j′ (L∗j′uj′ − Lj′vj′) (4)\nwhere Lj and L∗j are N -by-N diagonal matrices. For any k ∈ [c+ 1], Lk = ∑ j∈[c] wjkDjLj and similarly for L∗k. For the first layer, L = L ∗ = I .\nThe intuition here is to start from g = u − v (true for l2 loss) at the top layer and use induction. With this formulation, we could write the finite dynamics for wc (all parameters in layer c). Denote the N -by-dc+1dc matrix Rc = [LjDj ]j∈[c]Xc and R∗c = [L ∗ jD ∗ j ]j∈[c]X ∗ c . Using gradient descent rules:\n∆wj = X ᵀ cDjgj = X ᵀ cDjLj ∑ j′ L∗j′D ∗ j′X ∗ cw ∗ j′ − ∑ j′ Lj′Dj′Xcwj′  (5) = XᵀcDjLj (R ∗ cw ∗ c −Rcwc) (6)\nTherefore we have: ∆wc = R ᵀ c (R ∗ cw ∗ c −Rcwc) (7)"
    }, {
      "heading" : "3 SINGLE RELU CASE",
      "text" : "Let’s start with the simplest case where there is only one ReLU node, K = 1. At iteration t, following Eqn. 3, the gradient update rule is:\n∆w(t) = XᵀD(t)g(t) = XᵀD(t)(D∗Xw∗ −D(t)Xw(t)) (8)\nNote here how the notation ofD(t) comes into play (andD(t)D(t) = D(t)). Indeed, when the neuron is cut off at sample l, then (D(t))ll is zero and will block the corresponding gradient component.\nLinear case. In this situationD(t) = D∗ = I (no gating in either forward or backward propagation) and:\nw(t+1) = w(t) + η\nN XᵀX(w∗ −w(t)) (9)\nwhere η/N is the learning rate. When it is sufficiently small so that the spectral radius ρ(I − η NX\nᵀX) < 1, w(t+1) will converge to w∗ when t→ +∞. Note that this convergence is guaranteed for any initial condition w(1), if XᵀX is full rank with suitable η. This is consistent with its convex nature. If entries of X follow i.i.d Gaussian distribution, then E [ 1 NX ᵀX ]\n= I and the condition satisfies.\nNonlinear (ReLU) case. In this case, ∆w = XᵀD(D∗Xw∗ −DXw) in which D is a function of w. Intuitively, this term goes to zero when w → w∗, and should be approximated to be N2 (w\n∗ − w) in the i.i.d Gaussian case, since roughly half of the samples are blocked. However, once we make such approximation, we lost the nonlinear behavior of the network and would draw the wrong conclusion of global convergence.\nThen how should we analyze it? Notice that in ∆w, both of the two terms have the form F (e,w) = XᵀD(e)D(w)Xw. Using this form, E [∆w] = E [F (w/‖w‖,w∗)] − E [F (w/‖w‖,w)]. Here e is a unit vector called the “projected” weight. In the following, we will show that E [F (e,w)] has the following close form under i.i.d Gaussian assumption on X:\nLemma 3.1 Denote F (e,w) = XᵀD(e)D(w)Xw where e is a unit vector, X = [x1,x2, · · · ,xN ]ᵀ is N -by-d sample matrix and D(w) = diag(Xw > 0) is a binary diagonal matrix. If xi ∼ N(0, I) and are i.i.d (and thus bias-free), then:\nE [F (e,w)] = N\n2π [(π − θ)w + ‖w‖ sin θe] (10)\nwhere θ = ∠(e,w) ∈ [0, π] is the angle between e and w.\nNote that the expectation analysis smooths out the non-differentiable property of ReLU, leaving only one singularity at e = 0. The intuition is that expectation analysis involves an integration over the data distribution. With simple algebraic manipulation, E [∆w] takes the following closed form:\nE [∆w] = N 2 (w∗ −w) + N 2π (α sin θw − θw∗) (11)\nwhere α = ‖w∗‖/‖w‖ and θ ∈ [0, π] is the angle between w and w∗. The first term is expected while the last two terms show the nonlinear behavior. Using Lyapunov’s method, we show that the dynamics (if treated continuously) converges to w∗ when w(1) ∈ Ω = {w : ‖w −w∗‖ < ‖w∗‖}:\nLemma 3.2 When w(1) ∈ Ω = {w : ‖w −w∗‖ < ‖w∗‖}, following the dynamics of Eqn. 11, the Lyapunov function V (w) = 12‖w − w\n∗‖2 has V̇ < 0 and the system is asymptotically stable and thus w(t) → w∗ when t→ +∞.\nSee Appendix for the proof. The intuition is to represent V as a 2-by-2 bilinear form of vector [‖w‖, ‖w∗‖], and the bilinear coefficient matrix is positive definite. One question arises: will the same approach show the dynamics converges when the initial conditions lie outside the region Ω, in particular for any region that includes the origin? The answer is probably no. Note that w = 0 is a singularity in which ∆w is not continuous (if approaching from different directions towards w = 0, ∆w is different). It is due to the fact that ReLU function is not differentiable at the origin. We could remove this singularity by “smoothing out” ReLU around the origin. This will yield ∆w→ 0 when w → 0. In this case, V̇ (0) = 0 so Lyapunov method could only tell that the dynamics is stable but not convergent. Note that for ReLU activation, σ′(x) = 0 for certain negative x even after a local smoothing, so the global convergence claim in [Mei et al. (2016)] for l2 loss does not apply.\nRandom Initialization. Then we study how to sample w(1) so that w(1) ∈ Ω. We would like to sample within Ω, but we don’t know where is w∗. Sampling around origin with big radius r ≥ 2‖w∗‖ is inefficient in particular in high-dimensional space. This is because when the sample is uniform, the probability of hitting the ball is proportional to (r/‖w∗‖)d ≤ 2−d, which is exponentially small.\nA better idea is to sample around the origin with very small radius (but not at w = 0), so that the convergent hypersphere behaves like a hyperplane near the origin, and thus almost half of the samples is useful (Fig. 2(a)), as shown in the following theorem:\nTheorem 3.3 The dynamics in Eqn. 11 converges to w∗ with probability at least (1 − )/2, if the initial value w(1) is sampled uniformly from Br = {w : ‖w‖ ≤ r} with r ≤ √ 2π d+1‖w ∗‖.\nThe intution here is to lower-bound the probability of the shaded area (Fig. 2(b)). From the proof, the conclusion could be made stronger to show r ∼ 1/ √ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w∗ yields the convergence. This is consistent with the analysis above."
    }, {
      "heading" : "4 MULTIPLE RELUS CASE",
      "text" : "Now we are ready to analyze the network g(x) = ∑K j=1 σ(w ᵀ j x) for K ≥ 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)]. In this case, Lj = L∗j = I for 1 ≤ j ≤ K. Then we have the following nonlinear dynamics from Eqn. 7:\n∆wj = K∑ j′=1 f(wj ,wj′ ,w ∗ j′) (12)\nwhere f = F (wj/‖wj‖,w∗j′)− F (wj/‖wj‖,wj′). Therefore, using Eqn. 10, its expectation is:\n2π N E [ f(wj ,wj′ ,w ∗ j′) ] = (π − θ∗j ′ j )w ∗ j′ − (π − θ j′ j )wj′ + (‖w∗j′‖ ‖wj‖ sin θ∗j ′ j − ‖wj′‖ ‖wj‖ sin θj ′ j ) wj (13) where θ∗j ′ j ≡ ∠(wj ,w∗j′) and θ j′\nj ≡ ∠(wj ,wj′). Eqn. 12 (and its expected version) gives very complicated nonlinear dynamics and could be hard to solve in general. Unlike K = 1, a similar approach with Lyaponov function does not yield a decisive conclusion. However, if we consider the symmetric case: wj = Pjw and w∗j = Pjw ∗ where Pj is a cyclic permutation matrix that maps index j′ + 1 to (j′ + j mod K) + 1 (and P1 is the identity matrix), then RHS of the expected version of Eqn. 12 can be simplified as follows:\nE [∆wj ] = ∑ j′ E [ f(wj ,wj′ ,w ∗ j′) ] = ∑ j′ E [f(Pjw, Pj′w, Pj′w∗)]\n= ∑ j′′ E [f(Pjw, PjPj′′w, PjPj′′w∗)] ({Pj}Kj=1 is a group)\n= Pj ∑ j′′ E [f(w, Pj′′w, Pj′′w∗)] (‖Pw1‖ = ‖w1‖, ∠(Pw1, Pw2) = ∠(w1,w2))\n= PjE [∆w1] (14)\nwhich means that if all wj and w∗j are symmetric under the action of cyclic group, so does their expected gradient. Therefore, the trajectory {w(t)} keeps such cyclic structure. Instead of solving a system of K equations, we only need to solve one:\nE [∆w] = K∑ j=1 E [f(w, Pjw, Pjw∗)] (15)\nSurprisingly, there is another layer of symmetry in Eqn. 15 when {w∗j} forms an orthonomal basis (w∗j′ ᵀw∗j = δjj′ ). In this case, if we start with w (1) = xw∗ + y ∑ j 6=1 Pjw\n∗ then we could show that the trajectory keeps this structure and Eqn. 15 can be further reduced into the following 2D nonlinear dynamics:\n2π N E [ ∆x ∆y ] = − { [(π − φ)(x− 1 + (K − 1)y)] [ 1 1 ] + [ θ φ∗ − φ ] + φ [ x− 1 y ]} + [(K − 1)(α sinφ∗ − sinφ) + α sin θ] [ x y ] (16)\nHere the symmetrical factor (α ≡ ‖w∗j′‖/‖wj‖, θ ≡ θ ∗j j , φ ≡ θ\nj′\nj , φ ∗ ≡ θ∗j\n′\nj ) are defined as follows:\nα = (x2 + (K−1)y2)−1/2, cos θ = αx, cosφ∗ = αy, cosφ = α2(2xy+ (K−2)y2) (17) For this 2D dynamics, we thus have the following theorem:\nTheorem 4.1 For any K ≥ 2, the 2D dynamics (Eqn. 16) shows the following behaviors:\n(1) Symmetric case. If the initial condition x(1) = y(1) ∈ (0, 1], then the dynamics reduces to 1D and converges to a saddle point x = y = 1πK ( √ K − 1− arccos(1/ √ K) + π).\n(2) Symmetry-Breaking. If (x(1), y(1)) ∈ Ω = {x ∈ (0, 1], y ∈ [0, 1], x > y}, then dynamics always converges to (x, y) = (1, 0).\nFrom (x(t), y(t)) we could recover w(t)j = x (t)w∗j + y (t) ∑ j′ 6=j w ∗ j′ . Obviously, a convergence of Eqn. 16 to (1, 0) means Eqn. 12 converges to {w∗j}, i.e, the teacher parameters are recovered:\nCorollary 4.2 For a bias-free two-layered ReLU network g(x;w) = ∑ j σ(w ᵀ j x) that takes Gaussian i.i.d inputs (Fig. 1), if the teacher’s parameters {w∗j} form orthogonal bases, then when the student parameters is initialized in the form of w(1)j = x (1)w∗j + y (1) ∑ j′ 6=j w ∗ j′ where (x(1), y(1)) ∈ Ω = {x ∈ (0, 1], y ∈ [0, 1], x > y}, then the dynamics (Eqn. 12) converges to {w∗j} without being trapped into local minima.\nWhen symmetry is broken, since the closure of Ω includes the origin, there exists a path starting at arbitrarily small neighborhood of origin to w∗, regardless of how large ‖w∗‖ is. In contrast to traditional convex analysis that only gives the local parameter-dependent convergence basin around w∗j , here we obtain a convergence basin that is parameter-independent. In comparison, [Saad & Solla (1996)] uses a different activation function (σ(x) = erf(x)) and only analyzes local behaviors near the two fixed points (the symmetric saddle point and the teacher’s weights w∗), leaving symmetry breaking an empirical procedure. Here we show that it is possible to give global convergence analysis on certain symmetry breaking cases for two-layered ReLU network.\nBy symmetry, Corollary 4.1 immediately suggests that when w(1) = y(1) ∑K j=1 w ∗ j + (x\n(1) − y(1))w∗j′ , then the dynamics will converge to Pj′w\n∗. Since x > y but can be arbitrarily close, a slighest preturbation on the symmetric solution x = y leads to a different fixed point, which is a permutation of w∗. This is very similar to Spontaneously Symmetric-Breaking (SSB) procedure in physics, in which a high energy state with full symmetry goes to a low energy state and only retains part of the symmetry. In this case, the energy is the objective function E, the high energy state is the initialization that is almost symmetrical but with small fluctuation, and the low energy state is the fixed point the dynamics converges into.\nFrom the simulation shown in Fig. 4, we could see that gradient descent takes a detour to reach the desired solution w∗, even when the initialization is aligned with w∗. This is because in the first stage, all ReLU nodes receive the residue and try to explain the data in the same way (both x and y increases); when the “obvious” component has been explained away, then the residue changes its direction and pushes some ReLU nodes to explain other components as well (x increases but y decreases).\nEmpirically this path also converges to w∗ under noise. We leave it a conjecture that the system converges in the presence of reasonably large noise. If this conjecture is true, then with high probability a random initialization stays in the convergence basin and converges to a permutation of w∗. The reason is that a random initialization almost never gives ties. Without a tie, there exists one leading component which will dominate the convergence.\nConjecture 4.3 When the initialization w(1) = x(1)w∗j + y(1) ∑ j′ 6=j w ∗ j′ + , where is Gaussian noise and (x(1), y(1)) ∈ Ω, then the dynamics Eqn. 12 also converges to w∗ without trapped into local minima."
    }, {
      "heading" : "5 SIMULATION",
      "text" : ""
    }, {
      "heading" : "5.1 CLOSE FORM SOLUTION FOR ONE RELU NODE",
      "text" : "We verify our close form expression of E [F (e,w)] = E [XᵀD(e)D(w)Xw] (Eqn. 10) with simulation. We randomly pick e and w so that their angle ∠(e,w) is uniformly distributed in [0, π]. We prepare the input data X with standard Gaussian distribution and compare the close form solution E [F (e,w)] with F (e,w), the actual data term in gradient descent without expectation. We use relative RMS error: err = ‖E [F (e,w)] − F (e,w)‖/‖F (e,w)‖. As shown in Fig. 3(a), The error distribution on angles shows the properties of the close-form solution. For small θ, D(w) and\nvery large noise is present. Both teacher and student networks use g(x) =\n∑K\nj=1 σ(w ᵀ j x). Each\nexperiment has 8 runs. Bottom row: Convergence when we use g2(x) = ∑K j=1 ajσ(w ᵀ j x). Here the top weights aj is fixed at different numbers (rather than 1). Large positive aj correponds to fast convergence. When aj has positive/negative components, the network does not converge to w∗.\nD(e) overlaps sufficiently, giving a reliable estimation for the gradient. When θ → π, D(w) and D(e) tend not to overlap, leaving very few data involved in the gradient computation. As a result, the variance grows. Note that all our analysis operate on θ ∈ [0, π/2] and is not affected by this behavior. In the following, angles are sampled from [0, π/2].\nFig. 3(a) shows that the close form expression becomes more accurate with more samples. We also examine other zero-mean distributions of X , e.g., uniform distribution in [−1/2, 1/2]. As shown in Fig. 3(d), the close form expression still works for large d, showing that it could be quite general. Note that the error is computed up to a scaling constant, due to the difference in normalization constants among different distributions. We leave it to the future work to prove its usability for broader distributions."
    }, {
      "heading" : "5.2 CONVERGENCE FOR MULTIPLE RELU NODES",
      "text" : "Fig. 4(a) and (b) shows the 2D vector field given by the 2D dynamics (Eqn. 16) and Fig. 4(c) shows the 2D trajectory towards convergence to the teacher’s parameters w∗. Interestingly, even when we initialize the weights as (10−3, 0), aligning with w∗, the gradient descent takes detours to reach the destination. One explanation is, at the beginning all nodes move similar direction trying to explain the data, once the data have been explained partly, specialization follows (y decreases).\nFig. 5 shows empirical convergence for K ≥ 2, when the initialization deviates from symmetric initialization in Thm. 4.1. Unless the deviation is large, gradient descent converges to w∗. We also check the convergence of a more general network g2(x) = ∑K j=1 ajσ(w ᵀ j x). When aj > 0 convergence follows; however, when some aj is negative, the network does not converge to w∗, even that the student network already knows the ground truth value of {aj}Kj=1."
    }, {
      "heading" : "6 CONCLUSION AND FUTURE WORK",
      "text" : "In this paper, we analyze the nonlinear dynamical behavior of certain two-layered bias-free ReLU networks in the form of g(x;w) = ∑K j=1 σ(w ᵀ j x), where σ = max(x, 0) is the ReLU node. We assume that the input x follows Gaussian distribution and the output is generated by a teacher network with parameters w∗. In K = 1 we show a close-form nonlinear dynamics can be obtained and its convergence to w∗ can be proven, if we sample the initialization properly. Such initialization is consistent with common practice [Glorot & Bengio (2010); He et al. (2015)] and is independent of the value of w∗. ForK ≥ 2, when the teacher parameters {w∗j} form a orthonormal bases, we prove that the trajectory from symmetric initialization is trapped into a saddle point, while certain symmetric breaking initialization converges to w∗ without trapped into any local minima. Future work includes analysis of general cases (or symmetric case plus noise) for K ≥ 2, and a generalization to multilayer ReLU (or other nonlinear) networks."
    }, {
      "heading" : "7 APPENDIX",
      "text" : "Here we list all detailed proof for all the theorems."
    }, {
      "heading" : "7.1 PROPERTIES OF RELU NETWORKS",
      "text" : "Lemma 7.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher network of the same size, the negative gradient inflow gj for node j at layer c has the following form:\ngj = Lj ∑ j′ (L∗j′uj′ − Lj′vj′) (18)\nwhere Lj and L∗j are N -by-N diagonal matrices. For any k ∈ [c+ 1], Lk = ∑ j∈[c] wjkDjLj and similarly for L∗k.\nProof We prove by induction on layer. For the first layer, there is only one node with g = u − v, therefore Lj = Lj′ = I . Suppose the condition holds for all node j ∈ [c]. Then for node k ∈ [c+1], we have:\ngk = ∑ j wjkDjgj = ∑ j wjkDjLj ∑ j′ L∗j′uj′ − ∑ j′ Lj′vj′  =\n∑ j wjkDjLj ∑ j′ L∗j′ ∑ k′ D∗j′w ∗ jk′uk′ − ∑ j′ Lj′ ∑ k′ Dj′wjk′vk′  =\n∑ j wjkDjLj ∑ j′ L∗j′D ∗ j′ ∑ k′ w∗jk′uk′ − ∑ j wjkDjLj ∑ j′ Lj′Dj′ ∑ k′ wjk′vk′\n= ∑ k′ ∑ j wjkDjLj ∑ j′ L∗j′D ∗ j′w ∗ jk′ uk′ −∑ k′ ∑ j wjkDjLj ∑ j′ Lj′Dj′wjk′ vk′ Setting Lk = ∑ j wjkDjLj and L ∗ k = ∑ j w ∗ jkD ∗ jL ∗ j (both are diagonal matrices), we thus have:\ngk = ∑ k′ LkL ∗ k′uk′ − LkLk′vk′ = Lk ∑ k′ L∗k′uk′ − Lk′vk′ (19)"
    }, {
      "heading" : "7.2 ONE RELU CASE",
      "text" : "Lemma 7.2 Suppose F (e,w) = XᵀD(e)D(w)Xw where e is a unit vector and X = [x1,x2, · · · ,xN ]ᵀ is N -by-d sample matrix. If xi ∼ N(0, I) and are i.i.d, then:\nE [F (e,w)] = N\n2π ((π − θ)w + ‖w‖ sin θe) (20)\nwhere θ ∈ [0, π] is the angle between e and w.\nProof Note that F can be written in the following form: F (e,w) = ∑\ni:xᵀi e≥0,x ᵀ i w≥0\nxix ᵀ iw (21)\nwhere xi are samples so that X = [x1,x2, · · · ,xn]ᵀ. We set up the axes related to e and w as in Fig. 6, while the rest of the axis are prependicular to the plane. In this coordinate system, any vector x = [r sinφ, r cosφ, x3, . . . , xd]. We have an orthonomal set of bases: e, e⊥ = −e−w/‖w‖ cos θsin θ (and any set of bases that span the rest of the space). Under the basis, the representation for e and w is [1,0d−1] and [‖w‖ cos θ,−‖w‖ sin θ,0d−2]. Note that here θ ∈ (−π, π]. The angle θ is positive when e “chases after” w, and is otherwise negative.\nNow we consider the quality R(φ0) = E [ 1 N ∑ i:φi∈[0,φ0] xix ᵀ i ] . If we take the expectation and use polar coordinate only in the first two dimensions, we have:\nR(φ0) = E  1 N ∑ i:φi∈[0,φ0] xix ᵀ i  = E [xixᵀi |φi ∈ [0, φ0]]P [φi ∈ [0, φ0]] =\n∫ +∞ 0 ∫∫ +∞ −∞ ∫ φ0 0 r sinφr cosφ. . . xd  [r sinφ r cosφ . . . xd] p(r)p(θ) d∏ k=3 p(xk)rdrdφdx3 . . . dxd\nwhere p(r) = e−r 2/2 and p(θ) = 1/2π. Note that R(φ0) is a d-by-d matrix. The first 2-by-2 block can be computed in close form (note that ∫ +∞ 0\nr2p(r)rdr = 2). Any off-diagonal element except for the first 2-by-2 block is zero due to symmetric property of i.i.d Gaussian variables. Any diagonal element outside the first 2-by-2 block will be P [φi ∈ [0, φ0]] = φ0/2π. Finally, we have:\nR(φ0) = E  1 N ∑ i:φi∈[0,φ0] xix ᵀ i  = 1 4π [ 2φ0 − sin 2φ0 1− cos 2φ0 0 1− cos 2φ0 2φ0 + sin 2φ0 0 0 0 2φ0Id−2 ] (22)\n= φ0 2π Id + 1 4π\n[ − sin 2φ0 1− cos 2φ0 0 1− cos 2φ0 sin 2φ0 0\n0 0 0\n] (23)\nWith this equation, we could then compute E [F (e,w)]. When θ ≥ 0, the condition {i : xᵀi e ≥ 0,xᵀiw ≥ 0} is equivalent to {i : φi ∈ [θ, π]} (Fig. 6(a)). Using w = [‖w‖ cos θ,−‖w‖ sin θ,0d−2] and we have:\nE [F (e,w)] = N (R(π)−R(θ))w (24)\n= N\n4π\n( 2(π − θ)w − ‖w‖ [ − sin 2θ 1− cos 2θ 0 1− cos 2θ sin 2θ 0\n0 0 0\n][ cos θ − sin θ\n0\n]) (25)\n= N\n2π\n( (π − θ)w + ‖w‖ [ sin θ 0 ]) (26)\n= N\n2π ((π − θ)w + ‖w‖ sin θe) (27)\nFor θ < 0, the condition {i : xᵀi e ≥ 0,x ᵀ iw ≥ 0} is equivalent to {i : φi ∈ [0, π + θ]} (Fig. 6(b)), and similarly we get\nE [F (e,w)] = N (R(π + θ)−R(0))w = N 2π ((π + θ)w − ‖w‖ sin θe) (28)\nNotice that by abuse of notation, the θ appears in Eqn. 20 is the absolute value and Eqn. 20 follows.\nLemma 7.3 In the region ‖w(1) −w∗‖ < ‖w∗‖, following the dynamics (Eqn. 11), the Lyapunov function V (w) = 12‖w−w\n∗‖2 has V̇ < 0 and the system is asymptotically stable and thus w(t) → w∗ when t→ +∞.\nProof Denote that Ω = {w : ‖w(1) −w∗‖ < ‖w∗‖}. Note that\nV̇ = (w −w∗)ᵀ∆w = −yᵀMy (29) where y = [‖w∗‖, ‖w‖]ᵀ and M is the following 2-by-2 matrix:\nM = 1\n2\n[ sin 2θ + 2π − 2θ −(2π − θ) cos θ − sin θ\n−(2π − θ) cos θ − sin θ 2π\n] (30)\nIn the following we will show that M is positive definite when θ ∈ (0, π/2]. It suffices to show that M11 > 0, M22 > 0 and det(M) > 0. The first two are trivial, while the last one is:\n4det(M) = 2π(sin 2θ + 2π − 2θ)− [(2π − θ) cos θ + sin θ]2 (31) = 2π(sin 2θ + 2π − 2θ)− [ (2π − θ)2 cos2 θ + (2π − θ) sin 2θ + sin2 θ ] (32)\n= (4π2 − 1) sin2 θ − 4πθ + 4πθ cos2 θ − θ2 cos2 θ + θ sin 2θ (33) = (4π2 − 4πθ − 1) sin2 θ + θ cos θ(2 sin θ − θ cos θ) (34)\nNote that 4π2 − 4πθ − 1 = 4π(π − θ) − 1 > 0 for θ ∈ [0, π/2], and g(θ) = sin θ − θ cos θ ≥ 0 for θ ∈ [0, π/2] since g(0) = 0 and g′(θ) ≥ 0 in this region. Therefore, when θ ∈ (0, π/2], M is positive definite.\nWhen θ = 0, M(θ) = π[1,−1;−1, 1] and is semi-positive definite, with the null eigenvector being√ 2 2 [1, 1], i.e., ‖w‖ = ‖w\n∗‖. However, along θ = 0, the only w that satisfies ‖w‖ = ‖w∗‖ is w = w∗. Therefore, V̇ = −yᵀMy < 0 in Ω. Note that although this region could be expanded to the entire open half-spaceH = {w : wᵀw∗ > 0}, it is not straightforward to prove the convergence inH, since the trajectory might go outsideH. On the other hand, Ω is the level set V < 12‖w\n∗‖2 so the trajectory starting within Ω remains inside.\nTheorem 7.4 The dynamics in Eqn. 11 converges to w∗ with probability at least (1 − )/2, if the initial value w(1) is sampled uniformly from Br = {w : ‖w‖ ≤ r} with:\nr ≤ √ 2π\nd+ 1 ‖w∗‖ (35)\nProof Given a ball of radius r, we first compute the “gap” δ of sphere cap (Fig. 2(b)). First cos θ = r\n2‖w∗‖ , so δ = r cos θ = r2\n2‖w∗‖ . Then a sufficient condition for the probability argument to hold, is to ensure that the volume Vshaded of the shaded area is greater than 1− 2 Vd(r), where Vd(r) is the volume of d-dimensional ball of radius r. Since Vshaded ≥ 12Vd(r)− δVd−1, it suffices to have:\n1 2 Vd(r)− δVd−1 ≥ 1− 2 Vd(r) (36)\nwhich gives\nδ ≤ 2 Vd Vd−1\n(37)\nUsing δ = r 2\n2‖w∗‖ and Vd(r) = Vd(1)r d, we thus have:\nr ≤ Vd(1) Vd−1(1) ‖w∗‖ (38)\nwhere Vd(1) is the volume of the unit ball. Since the volume of d-dimensional unit ball is\nVd(1) = πd/2\nΓ(d/2 + 1) (39)\nwhere Γ(x) = ∫∞ 0 tx−1e−tdt. So we have\nVd(1) Vd−1(1) = √ π Γ(d/2 + 1/2) Γ(d/2 + 1) (40)\nFrom Gautschi’s Inequality\nx1−s < Γ(x+ 1)\nΓ(x+ s) < (x+ s)1−s x > 0, 0 < s < 1 (41)\nwith s = 1/2 and x = d/2 we have:( d+ 1\n2\n)−1/2 < Γ(d/2 + 1/2)\nΓ(d/2 + 1) <\n( d\n2\n)−1/2 (42)\nTherefore, it suffices to have\nr ≤ √ 2π\nd+ 1 ‖w∗‖ (43)\nNote that this upper bound is tight when δ → 0 and d→ +∞, since all inequality involved asymptotically becomes equal."
    }, {
      "heading" : "7.3 TWO LAYER CASE",
      "text" : "Lemma 7.5 For φ∗, θ and φ defined in Eqn. 17:\nα ≡ (x2 + (K − 1)y2)−1/2 (44) cos θ ≡ αx (45) cosφ∗ ≡ αy (46) cosφ ≡ α2(2xy + (K − 2)y2) (47)\nwe have the following relations in the triangular region Ω 0 = {(x, y) : x ≥ 0, y ≥ 0, x ≥ y + 0} (Fig. 6(c)):\n(1) φ, φ∗ ∈ [0, π/2] and θ ∈ [0, θ0) where θ0 = arccos 1√K .\n(2) cosφ = 1− α2(x− y)2 and sinφ = α(x− y) √ 2− α2(x− y)2.\n(3) φ∗ ≥ φ (equality holds only when y = 0) and φ∗ > θ.\nProof Propositions (1) and (2) are computed by direct calculations. In particular, note that since cos θ = αx = 1/ √ 1 + (K − 1)(y/x)2 and x > y ≥ 0, we have cos θ ∈ (1/ √ K, 1] and θ ∈ [0, θ0). For Preposition (3), φ∗ = arccosαy > θ = arccosαx because x > y. Finally, for x > y > 0, we have\ncosφ cosφ∗ = α2(2xy + (K − 2)y2) αy = α(2x+ (K − 2)y) > α(x+ (K − 1)y) > 1 (48)\nThe final inequality is because K ≥ 2, x, y > 0 and thus (x + (K − 1)y)2 > x2 + (K − 1)2y2 > x2 + (K − 1)y2 = α−2. Therefore φ∗ > φ. If y = 0 then φ∗ = φ.\nTheorem 7.6 For the dynamics defined in Eqn. 16, there exists 0 > 0 so that the trianglar region Ω 0 = {(x, y) : x ≥ 0, y ≥ 0, x ≥ y + 0} (Fig. 6(c)) is a convergent region. That is, the flow goes inwards for all three edges and any trajectory starting in Ω 0 stays.\nProof We discuss the three boundaries as follows:\nCase 1: y = 0, 0 ≤ x ≤ 1, horizontal line. In this case, θ = 0, φ = π/2 and φ∗ = π/2. The y component of the dynamics in this line is:\nf1 ≡ 2π N ∆y = −π 2 (x− 1) ≥ 0 (49)\nSo ∆y points to the interior of Ω.\nCase 2: x = 1, 0 ≤ y ≤ 1, vertical line. In this case, α ≤ 1 and the x component of the dynamics is:\nf2 ≡ 2π\nN ∆x = −(π − φ)(K − 1)y − θ + (K − 1)(α sinφ∗ − sinφ) + α sin θ (50)\n= −(K − 1) [(π − φ)y − (α sinφ∗ − sinφ)] + (α sin θ − θ) (51) Note that since α ≤ 1, α sin θ ≤ sin θ ≤ θ, so the second term is non-positive. For the first term, we only need to check whether (π − φ)y − (α sinφ∗ − sinφ) is nonnegative. Note that\n(π − φ)y − (α sinφ∗ − sinφ) (52) = (π − φ)y + α(x− y) √ 2− α2(x− y)2 − α √ 1− α2y2 (53)\n= y [ π − φ− α √ 2− α2(x− y)2 ] + α [ x √ 2− α2(x− y)2 − √ 1− α2y2 ]\n(54)\nIn Ω we have (x − y)2 ≤ 1, combined with α ≤ 1, we have 1 ≤ √ 2− α2(x− y)2 ≤ √\n2 and√ 1− α2y2 ≤ 1. Since x = 1, the second term is nonnegative. For the first term, since α ≤ 1,\nπ − φ− α √\n2− α2(x− y)2 ≥ π − π 2 − √ 2 > 0 (55)"
    }, {
      "heading" : "So (π − φ)y − (α sinφ∗ − sinφ) ≥ 0 and ∆x ≤ 0, pointing inwards.",
      "text" : "Case 3: x = y + , 0 ≤ y ≤ 1, diagonal line. We compute the inner product between (∆x,∆y) and (1,−1), the inward normal of Ω at the line. Using φ ≤ π2 sinφ for φ ∈ [0, π/2] and φ\n∗ − θ = arccosαy − arccosαx ≥ 0 when x ≥ y, we have:\nf3(y, ) ≡ 2π\nN\n[ ∆x ∆y ]ᵀ [ 1 −1 ] = φ∗ − θ − φ+ [(K − 1)(α sinφ∗ − sinφ) + α sin θ] (56)\n≥ (K − 1) [ α sinφ∗ − ( 1 +\nπ\n2(K − 1)\n) sinφ ] = α(K − 1) [√ 1− α2y2 − ( 1 + π\n2(K − 1)\n)√ 2− α2 2 ] Note that for y > 0:\nαy = 1√\n(x/y)2 + (K − 1) = 1√ (1 + /y)2 + (K − 1) ≤ 1√ K\n(57)\nFor y = 0, αy = 0 < √ 1/K. So we have √ 1− α2y2 ≥ √ 1− 1/K. And √ 2− α2 2 ≤ √\n2. Therefore f3 ≥ α(K−1)(C1− C2) withC1 ≡ √ 1− 1/K > 0 andC2 ≡ √ 2(1+π/2(K−1)) > 0. With = 0 > 0 sufficiently small, f3 > 0.\nLemma 7.7 (Reparametrization) Denote = x − y > 0. The terms αx, αy and α involved in the trigometric functions in Eqn. 16 has the following parameterization:\nα [ y x ] = 1\nK\n[ β − β2\nβ + (K − 1)β2 Kβ2\n] (58)\nwhere β2 = √\n(K − β2)/(K − 1). The reverse transformation is given by β =√ K − (K − 1)α2 2. Here β ∈ [1, √ K) and β2 ∈ (0, 1]. In particular, the critical point (x, y) = (1, 0) corresponds to (β, ) = (1, 1). As a result, all trigometric functions in Eqn. 16 only depend on the single variable β. In particular, the following relationship is useful:\nβ = cos θ + √ K − 1 sin θ (59)\nProof This transformation can be checked by simple algebraic manipulation. For example:\n1\nαK (β − β2) =\n1\nK\n(√ K\nα2 − (K − 1) 2 −\n) = 1\nK\n(√ (Ky + )2 − ) = y (60)\nTo prove Eqn. 59, first we notice that K cos θ = Kαx = β + (K − 1)β2. Therefore, we have (K cos θ − β)2 − (K − 1)2β22 = 0, which gives β2 − 2β cos θ + 1 −K sin2 θ = 0. Solving this quadratic equation and notice that β ≥ 1, θ ∈ [0, π/2] and we get:\nβ = cos θ + √ cos2 θ +K sin2 θ − 1 = cos θ + √ K − 1 sin θ (61)\nLemma 7.8 After reparametrization (Eqn. 58), f3(β, ) ≥ 0 for ∈ (0, β2/β]. Furthermore, the equality is true only if (β, ) = (1, 1) or (y, ) = (0, 1).\nProof Applying the parametrization (Eqn. 58) to Eqn. 56 and notice that α = β2 = β2(β), we could write f3 = h1(β)− (φ+ (K − 1) sinφ) (62) When β is fixed, f3 now is a monotonously decreasing function with respect to > 0. Therefore, f3(β, ) ≥ f3(β, ′) for 0 < ≤ ′ ≡ β2/β. If we could prove f3(β, ′) ≥ 0 and only attain zero at known critical point (β, ) = (1, 1), the proof is complete.\nDenote f3(β, ′) = f31 + f32 where\nf31(β, ′) = φ∗ − θ − ′φ+ ′α sin θ (63) f32(β, ′) = (K − 1)(α sinφ∗ − sinφ) ′ (64)\nFor f32 it suffices to prove that ′(α sinφ∗ − sinφ) = β2 sinφ∗ − β2β sinφ ≥ 0, which is equivalent to sinφ∗ − sinφ/β ≥ 0. But this is trivially true since φ∗ ≥ φ and β ≥ 1. Therefore, f32 ≥ 0. Note that the equality only holds when φ∗ = φ and β = 1, which corresponds to the horizontal line x ∈ (0, 1], y = 0. For f31, since φ∗ ≥ φ, φ∗ > θ and ′ ∈ (0, 1], we have the following:\nf31 = ′(φ∗ − φ) + (1− ′)(φ∗ − θ)− ′θ + β2 sin θ ≥ − ′θ + β2 sin θ ≥ β2 ( sin θ − θ\nβ\n) (65)\nAnd it reduces to showing whether β sin θ − θ is nonnegative. Using Eqn. 59, we have:\nf33(θ) = β sin θ − θ = 1\n2 sin 2θ +\n√ K − 1 sin2 θ − θ (66)\nNote that f ′33 = cos 2θ + √ K − 1 sin 2θ − 1 = √ K cos(2θ − θ0) − 1, where θ0 = arccos 1√K . By Prepositions 1 in Lemma 7.5, θ ∈ [0, θ0). Therefore, f ′33 ≥ 0 and since f33(0) = 0, f33 ≥ 0. Again the equity holds when θ = 0, φ∗ = φ and ′ = 1, which is the critical point (β, ) = (1, 1) or (y, ) = (0, 1).\nTheorem 7.9 For the dynamics defined in Eqn. 16, the only critical point (∆x = 0 and ∆y = 0) within Ω is (y, ) = (0, 1).\nProof We prove by contradiction. Suppose (β, ) is a critical point other than w∗. A necessary condition for this to hold is f3 = 0 (Eqn. 56). By Lemma 7.8, > ′ = β2/β > 0 and\n− 1 +Ky = 1 α (β2 − α+ β − β2) = β − α α = β − β2/ α > β − β2/ ′ α = 0 (67)\nSo − 1 +Ky is strictly greater than zero. On the other hand, the condition f3 = 0 implies that\n((K − 1)(α sinφ∗ − sinφ) + α sin θ) = −1 (φ∗ − θ) + φ (68)\nUsing φ ∈ [0, π/2], φ∗ ≥ φ and φ∗ > θ, we have: 2π\nN ∆y = −(π − φ)( − 1 +Ky)− (φ∗ − φ)− φy + ((K − 1)(α sinφ∗ − sinφ) + α sin θ) y\n= −(π − φ)( − 1 +Ky)− (φ∗ − φ)− 1 (φ∗ − θ)y < 0 (69)\nSo the current point (β, ) cannot be a critical point.\nTheorem 7.10 Any trajectory in Ω 0 converges to (y, ) = (1, 0), following the dynamics defined in Eqn. 16.\nProof We have Lyaponov function V = E [E] so that V̇ = −E [∆wᵀ∆w] ≤ −E [∆w]ᵀ E [∆w] ≤ 0. By Thm. 7.9, other than the optimal solution w∗, there is no other symmetric critical point, ∆w 6= 0 and thus V̇ < 0. On the other hand, by Thm. 7.6, the triangular region Ω 0 is convergent, in which the 2D dynamics isC∞ differentiable. Therefore, any 2D solution curve ξ(t) will stay within. By PoincareBendixson theorem, when there is a unique critical point, the curve either converges to a limit circle or the critical point. However, limit cycle is not possible since V is strictly monotonous decreasing along the curve. Therefore, ξ(t) will converge to the unique critical point, which is (y, ) = (1, 0) and so does the symmetric system (Eqn. 12).\nTheorem 7.11 When x = y ∈ (0, 1], the 2D dynamics (Eqn. 16) reduces to the following 1D case: 2π\nN ∆x = −πK(x− x∗) (70)\nwhere x∗ = 1πK ( √ K − 1− arccos(1/ √ K) + π). Furthermore, x∗ is a convergent critical point.\nProof The 1D system can be computed with simple algebraic manipulations (note that when x = y, φ = 0 and θ = φ∗ = arccos(1/ √ K)). Note that the 1D system is linear and its close form solution is x(t) = x0 + Ce−K/2Nt and thus convergent."
    } ],
    "references" : [ {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "Gérard Ben", "LeCun", "Yann" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Open problem: The landscape of the loss surfaces of multilayer networks",
      "author" : [ "Choromanska", "Anna", "LeCun", "Yann", "Arous", "Gérard Ben" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, COLT",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Local minima and plateaus in hierarchical structures of multilayer perceptrons",
      "author" : [ "Fukumizu", "Kenji", "Amari", "Shun-ichi" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2000
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In Aistats,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "Computer Vision anad Pattern Recognition",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima" ],
      "venue" : "CoRR abs/1506.08473,",
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "Kawaguchi", "Kenji" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kawaguchi and Kenji.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kawaguchi and Kenji.",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient backprop",
      "author" : [ "LeCun", "Yann A", "Bottou", "Léon", "Orr", "Genevieve B", "Müller", "Klaus-Robert" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2012
    }, {
      "title" : "The landscape of empirical risk for non-convex losses",
      "author" : [ "Mei", "Song", "Bai", "Yu", "Montanari", "Andrea" ],
      "venue" : "arXiv preprint arXiv:1607.06534,",
      "citeRegEx" : "Mei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamics of on-line gradient descent learning for multilayer neural networks",
      "author" : [ "Saad", "David", "Solla", "Sara A" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Saad et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Saad et al\\.",
      "year" : 1996
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya" ],
      "venue" : "arXiv preprint arXiv:1312.6120,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2015
    }, {
      "title" : "No bad local minima: Data independent training error guarantees for multilayer neural networks",
      "author" : [ "Soudry", "Daniel", "Carmon", "Yair" ],
      "venue" : "arXiv preprint arXiv:1605.08361,",
      "citeRegEx" : "Soudry et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Soudry et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w∗ with at least (1 − )/2 probability, if random weight initializations of proper standard derivation (∼ 1/ √ d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al.",
      "startOffset" : 277,
      "endOffset" : 294
    }, {
      "referenceID" : 4,
      "context" : "We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w∗ with at least (1 − )/2 probability, if random weight initializations of proper standard derivation (∼ 1/ √ d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)].",
      "startOffset" : 277,
      "endOffset" : 315
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al.",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al.",
      "startOffset" : 108,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al.",
      "startOffset" : 108,
      "endOffset" : 177
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al.",
      "startOffset" : 108,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al.",
      "startOffset" : 108,
      "endOffset" : 258
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al. (2012)].",
      "startOffset" : 108,
      "endOffset" : 304
    }, {
      "referenceID" : 2,
      "context" : ")/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al.",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : ")/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],.",
      "startOffset" : 165,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : ")/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K ≥ 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w∗, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed ‖w∗‖, showing that such a convergence behavior is beyond the local convex structure at w∗. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network.",
      "startOffset" : 165,
      "endOffset" : 849
    }, {
      "referenceID" : 2,
      "context" : ")/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K ≥ 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w∗, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed ‖w∗‖, showing that such a convergence behavior is beyond the local convex structure at w∗. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network.",
      "startOffset" : 165,
      "endOffset" : 924
    }, {
      "referenceID" : 2,
      "context" : ")/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K ≥ 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w∗, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed ‖w∗‖, showing that such a convergence behavior is beyond the local convex structure at w∗. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function σ(x) when its derivatives σ′, σ′′, σ′′′ are bounded and σ′ > 0.",
      "startOffset" : 165,
      "endOffset" : 1116
    }, {
      "referenceID" : 2,
      "context" : ")/2 probability, if initialized randomly with standard derivation on the order of 1/ √ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K ≥ 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w∗, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed ‖w∗‖, showing that such a convergence behavior is beyond the local convex structure at w∗. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function σ(x) when its derivatives σ′, σ′′, σ′′′ are bounded and σ′ > 0. Similar to our approach, [Saad & Solla (1996)] also uses the student-teacher setting and analyzes the dynamics of student network when the teacher’s parameters w∗ forms a orthonomal bases; however, it uses σ(x) = erf(x) as the nonlinearity and only analyzes the local behaviors of the two critical points (the saddle point in symmetric initializations, and w∗).",
      "startOffset" : 165,
      "endOffset" : 1291
    }, {
      "referenceID" : 0,
      "context" : "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions.",
      "startOffset" : 14,
      "endOffset" : 217
    }, {
      "referenceID" : 0,
      "context" : "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations.",
      "startOffset" : 14,
      "endOffset" : 335
    }, {
      "referenceID" : 0,
      "context" : "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network.",
      "startOffset" : 14,
      "endOffset" : 810
    }, {
      "referenceID" : 0,
      "context" : "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network. [Janzamin et al. (2015)] gives guarantees on recovering the parameters of a 2-layered neural network learnt with tensor decomposition.",
      "startOffset" : 14,
      "endOffset" : 952
    }, {
      "referenceID" : 11,
      "context" : "Note that for ReLU activation, σ′(x) = 0 for certain negative x even after a local smoothing, so the global convergence claim in [Mei et al. (2016)] for l2 loss does not apply.",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "From the proof, the conclusion could be made stronger to show r ∼ 1/ √ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al.",
      "startOffset" : 148,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "From the proof, the conclusion could be made stronger to show r ∼ 1/ √ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)].",
      "startOffset" : 148,
      "endOffset" : 186
    }, {
      "referenceID" : 4,
      "context" : "From the proof, the conclusion could be made stronger to show r ∼ 1/ √ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w∗ yields the convergence. This is consistent with the analysis above. 4 MULTIPLE RELUS CASE Now we are ready to analyze the network g(x) = ∑K j=1 σ(w T j x) for K ≥ 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)].",
      "startOffset" : 148,
      "endOffset" : 585
    }, {
      "referenceID" : 4,
      "context" : "From the proof, the conclusion could be made stronger to show r ∼ 1/ √ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w∗ yields the convergence. This is consistent with the analysis above. 4 MULTIPLE RELUS CASE Now we are ready to analyze the network g(x) = ∑K j=1 σ(w T j x) for K ≥ 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)].",
      "startOffset" : 148,
      "endOffset" : 609
    }, {
      "referenceID" : 4,
      "context" : "From the proof, the conclusion could be made stronger to show r ∼ 1/ √ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w∗ yields the convergence. This is consistent with the analysis above. 4 MULTIPLE RELUS CASE Now we are ready to analyze the network g(x) = ∑K j=1 σ(w T j x) for K ≥ 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)].",
      "startOffset" : 148,
      "endOffset" : 634
    }, {
      "referenceID" : 4,
      "context" : "Such initialization is consistent with common practice [Glorot & Bengio (2010); He et al. (2015)] and is independent of the value of w∗.",
      "startOffset" : 80,
      "endOffset" : 97
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of g(x;w) = ∑K j=1 σ(w T j x), where σ(·) is ReLU nonlinearity. We assume that the input x follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters w∗ using l2 loss. We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w∗ with at least (1 − )/2 probability, if random weight initializations of proper standard derivation (∼ 1/ √ d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. For networks with many ReLU nodes (K ≥ 2), we apply our close form dynamics and prove that when the teacher parameters {w∗ j}j=1 forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to w∗ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with l2 loss. Simulations verify our theoretical analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}