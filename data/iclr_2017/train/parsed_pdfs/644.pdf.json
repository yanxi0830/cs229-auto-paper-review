{
  "name" : "644.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS",
    "authors" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasaath R. Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra" ],
    "emails" : [ "ashwinkv@vt.edu", "cogswell@vt.edu", "ram21@vt.edu", "sunqing@vt.edu", "steflee@vt.edu", "djcran@indiana.edu,", "dbatra@vt.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the last few years, Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) or more generally, neural sequence models have become the standard choice for modeling time-series data for a wide range of applications including speech recognition (Graves et al., 2013), machine translation (Bahdanau et al., 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al., 2015). RNN based sequence generation architectures model the conditional probability, Pr(y|x) of an output sequence y = (y1, . . . , yT ) given an input x (possibly also a sequence); where the output tokens yt are from a finite vocabulary, V . Inference in RNNs. Maximum a Posteriori (MAP) inference for RNNs is the task of finding the most likely output sequence given the input. Since the number of possible sequences grows as |V|T , exact inference is NP-hard – so, approximate inference algorithms like beam search (BS) are commonly employed. BS is a heuristic graph-search algorithm that maintains the B top-scoring partial sequences expanded in a greedy left-to-right fashion. Fig. 1 shows a sample BS search tree.\nLack of Diversity in BS. Despite the widespread usage of BS, it has long been understood that solutions decoded by BS are generic and lacking in diversity (Finkel et al., 2006; Gimpel et al.,\na\ntrain\nsteam black locomotive\nis traveling\non\nengine train train\ncoming down a\nthe\ntrain engine\ndown track train tracks traveling is the with near track down\nthrough\ntracks\na with train tracks a\nin\na tracks\nforest lush\na\nan\ntrain steam\nan the\nis engine\nold train\na an\ncoming train\ntrain steam\ntrain black\ntraveling is\nengine locomotive\ntrain and\ndown through\ntrain is\nis white\ntrain a\nis traveling\ncoming on\ntracks\nforest\ndown through\nthe a\nBeam Search\nDiverse Beam Search\nA steam engine train travelling down train tracks. A steam engine train travelling down tracks. A steam engine train travelling through a forest. A steam engine train travelling through a lush green forest. A steam engine train travelling through a lush green countryside A train on a train track with a sky background.\nA steam engine travelling down train tracks. A steam engine train travelling through a forest. An old steam engine train travelling down train tracks. An old steam engine train travelling through a forest. A black train is on the tracks in a wooded area. A black train is on the tracks in a rural area.\nSingle engine train rolling down the tracks. A steam locomotive is blowing steam. A locomotive drives along the tracks amongst trees and bushes. An old fashion train with steam coming out of its pipe. A black and red train moving down a train track.\nAn engine is coming down the train track. Ground Truth Captions\nFigure 1: Comparing image captioning outputs decoded by BS (top) and our method, Diverse Beam Search (middle) – we notice that BS captions are near-duplicates with similar shared paths in the search tree and minor variations in the end. In contrast, DBS captions are significantly diverse and similar to the variability in human-generated ground truth captions (bottom).\n2013; Li et al., 2015; Li & Jurafsky, 2016). Comparing the human (bottom) and BS (top) generated captions shown in Fig. 1 demonstrates this deficiency. While this behavior of BS is disadvantageous for many reasons, we highlight the three most crucial ones here:\ni) The production of near-identical beams make BS a computationally wasteful algorithm, with essentially the same computation being repeated for no significant gain in performance.\nii) Due to loss-evaluation mismatch (i.e. improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics), it is common practice to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016). This treatment of an optimization algorithm as a hyperparameter is not only intellectually dissatisfying but also has a significant practical side-effect – it leads to the decoding of largely bland, generic, and “safe” outputs, e.g. always saying “I don’t know” in conversation models (Kannan et al., 2016).\niii) Most importantly, lack of diversity in the decoded solutions is fundamentally crippling in AI problems with significant ambiguity – e.g. there are multiple ways of describing an image or responding in a conversation that are “correct” and it is important to capture this ambiguity by finding several diverse plausible hypotheses.\nOverview and Contributions. To address these shortcomings, we propose Diverse Beam Search (DBS) – a general framework to decode a set of diverse sequences that can be used as an alternative to BS. At a high level, DBS decodes diverse lists by dividing the given beam budget into groups and enforcing diversity between groups of beams. Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms – the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ. This diversity-augmented model score is optimized in a doubly greedy manner – greedily optimizing along both time (like BS) and groups (like DivMBest).\nOur primary technical contribution is Diverse Beam Search, a doubly greedy approximate inference algorithm to decode diverse sequences from neural sequence models. We report results on image captioning, machine translation, conversations and visual question generation to demonstrate the broad applicability of DBS. Results show that DBS produces consistent improvements on both task-specific oracle and other diversity-related metrics while maintaining run-time and memory requirements similar to BS. We also evaluate human preferences between image captions generated by BS or DBS. Further experiments show that DBS is robust over a wide range of its parameter values and is capable of encoding various notions of diversity through different forms of the diversty term.\nOverall, our algorithm is simple to implement and consistently outperforms BS in a wide range of domains without sacrificing efficiency. Our implementation is publicly available at https: //github.com/ashwinkalyan/dbs. Additionally, we provide an interactive demonstration of DBS for image captioning at http://dbs.cloudcv.org."
    }, {
      "heading" : "2 PRELIMINARIES: DECODING RNNS WITH BEAM SEARCH",
      "text" : "We begin with a refresher on BS, before describing our generalization, Diverse Beam Search. For notational convenience, let [n] denote the set of natural numbers from 1 to n and let v[n] = [v1, . . . , vn]\nᵀ index the first n elements of a vector v ∈ Rm. The Decoding Problem. RNNs are trained to estimate the likelihood of sequences of tokens from a finite dictionary V given an input x. The RNN updates its internal state and estimates the conditional probability distribution over the next output given the input and all previous output tokens. We denote the logarithm of this conditional probability distribution over all tokens at time t as θ(yt) = log Pr(yt|yt−1, . . . , y1,x). To avoid notational clutter, we index θ(·) with a single variable yt, but it should be clear that it depends on all previous outputs, y[t−1]. We write the log probability of a partial solution (i.e. the sum of log probabilities of all tokens decoded so far) as Θ(y[t]) =∑ τ∈[t] θ(yτ ). The decoding problem is then the task of finding a sequence y that maximizes Θ(y).\nAs each output is conditioned on all the previous outputs, decoding the optimal length-T sequence in this setting can be viewed as MAP inference on a T -order Markov chain with nodes corresponding to output tokens at each time step. Not only does the size of the largest factor in such a graph grow as |V|T , but computing these factors also requires repetitively evaluating the sequence model. Thus, approximate algorithms are employed and the most prevalent method is beam search (BS).\nBeam search is a heuristic search algorithm which stores the topB highest scoring partial candidates at each time step; where B is known as the beam width. Let us denote the set of B solutions held by BS at the start of time t as Y[t−1] = {y1,[t−1], . . . ,yB,[t−1]}. At each time step, BS considers all possible single token extensions of these beams given by the set Yt = Y[t−1] × V and retains the B highest scoring extensions. More formally, at each step the beams are updated as\nY[t] = argmax y1,[t],...,yB,[t]∈Yt ∑ b∈[B] Θ(yb,[t]) s.t. yi,[t] 6= yj,[t] ∀i 6= j. (1)\nThe above objective can be trivially maximized by sorting all B × |V| members of Yt by their log probabilities and selecting the top B. This process is repeated until time T and the most likely sequence is selected by ranking the B complete beams according to their log probabilities.\nWhile this method allows for multiple sequences to be explored in parallel, most completions tend to stem from a single highly valued beam – resulting in outputs that are often only minor perturbations of a single sequence (and typically only towards the end of the sequences)."
    }, {
      "heading" : "3 DIVERSE BEAM SEARCH: FORMULATION AND ALGORITHM",
      "text" : "To overcome this, we augment the objective in Eq. 1 with a dissimilarity term ∆(Y[t]) that measures the diversity between candidate sequences, assigning a penalty ∆(Y[t])[c] to each possible sequence completion c ∈ V . Jointly optimizing this augmented objective for allB candidates at each time step is intractable as the number of possible solutions grows with |V|B (easily 1060 for typical language modeling settings). To avoid this, we opt for a greedy procedure that divides the beam budget B into G groups and promotes diversity between these groups. The approximation is doubly greedy – across both time and groups – so ∆(Y[t]) is constant with respect to other groups and we can sequentially optimize each group using regular BS. We now explain the specifics of our approach.\nDiverse Beam Search. As joint optimization is intractable, we form G smaller groups of beams and optimize them sequentially. Consider a partition of the set of beams Y[t] into G smaller sets Y g[t], g∈[G] of B\n′ = B/G beams each (we pick G to divide B). In the example shown in Fig. 2, B = 6 beams are divided into G = 3 differently colored groups containing B′ = 2 beams each.\nConsidering diversity only between groups, reduces the search space at each time step; however, inference remains intractable. To enforce diversity efficiently, we consider a greedy strategy that steps each group forward in time sequentially while considering the others fixed. Each group can then evaluate the diversity term with respect to the fixed extensions of previous groups, returning the search space to B′ × |V|. In the snapshot shown in Fig. 2, the third group is being stepped forward at time step t = 4 and the previous groups have already been completed. With this staggered beamfront, the diversity term of the third group can be computed using these completions. Here we use\nhamming diversity, which adds diversity penalty -1 for each appearance of a possible extension word at the same time step in a previous group – ‘birds’, ‘the’, and ‘an’ in the example – and 0 to all other possible completions. We discuss other forms for the diversity function in Section 5.1.\nAs we optimize each group with the previous groups fixed, extending group g at time t amounts to a standard BS using dissimilarity augmented log probabilities and can be written as:\nY g[t] = argmax yg 1,[t] ,...,yg B′,[t]∈Y g t\n∑ b∈[B′] Θ ( ygb,[t] ) + λ∆ ( g−1⋃ h=1 Y h[t] ) [ygb,t], (2)\ns.t. λ ≥ 0, ygi,[t] 6= y g j,[t]∀i 6= j\nwhere λ is scalar controlling the strength of the diversity term. The full procedure to obtain diverse sequences using our method, Diverse Beam Search (DBS), is presented in Algorithm 1. It consists of two main steps for each group at each time step –\n1) augmenting the log probabilities of each possible extension with the diversity term computed from previously advanced groups (Algorithm 1, Line 5) and,\n2) running one step of a smaller BS with B′ beams using the augmented log probabilities to extend the current group (Algorithm 1, Line 6).\nNote that the first group (g = 1) is not ‘conditioned’ on other groups during optimization, so our method is guaranteed to perform at least as well as a beam search of size B′.\nAlgorithm 1: Diverse Beam Search 1 Perform a diverse beam search with G groups using a beam width of B 2 for t = 1, . . . T do\n// perform one step of beam search for first group without diversity 3 Y 1[t] ← argmax(y11,[t],...,y1B′,[t]) ∑ b∈[B′] Θ(y 1 b,[t]) 4 for g = 2, . . . G do // augment log probabilities with diversity penalty 5 Θ(ygb,[t])← Θ(y g b,[t]) + λ∆( ⋃g−1 h=1 Y h [t])[y g b,t] b ∈ [B′],y g b,[t] ∈ Y g t and λ > 0 // perform one step of beam search for the group 6 Y g[t] ← argmax(yg\n1,[t] ,...,yg B′,[t]\n)∑ b∈[B′] Θ(y g b,[t]) s.t. yi,[t] 6= yj,[t] ∀i 6= j\n7 Return set of B solutions, Y[T ] = ⋃G g=1 Y g [T ]"
    }, {
      "heading" : "4 RELATED WORK",
      "text" : "Diverse M-Best Lists. The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. Kirillov et al. (2015) show how these solutions can be found\njointly (non-greedily) for certain kinds of energy functions. The techniques developed by Kirillov are not directly applicable to decoding from RNNs, which do not satisfy the assumptions made.\nMost related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference.\nDiverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists.\nLi & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective (as in DBS) rather than via a procedural heuristic provides easier generalization to incorporate different notions of diversity and control the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms the method of Li & Jurafsky (2016).\nLi et al. (2015) introduced a novel decoding objective that maximizes mutual information between inputs and predicted outputs to penalize generic sequences. This operates on a principle orthogonal and complementary to DBS and Li & Jurafsky (2016). It works by penalizing utterances that are generally more frequent (diversity independent of input) rather than penalizing utterances that are similar to other utterances produced for the same input (diversity conditioned on input). Furthermore, the input-independent approach requires training a new language model for the target language while DBS just requires a diversity function ∆. Combination of these complementary techniques is left as interesting future work.\nIn other recent work, Wu et al. (2016) modify the beam search objective by introducing lengthnormalization to favor longer sequences and a coverage penalty that favors sequences that account for the complete input sequence. While the coverage term does not generalize to all neural sequence models, the length-normalization term can be implemented by modifying the joint-log-probability of each sequence. Although the goal of this method is not to produce diverse lists and hence not directly comparable, it is a complementary technique that can be used in conjunction with our diverse decoding method."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In this section, we evaluate our approach on image captioning, machine translation, conversation and visual question generation tasks to demonstrate both its effectiveness against baselines and its general applicability to any inference currently supported by beam search. We also analyze the effects of DBS parameters, explore human preferences for diversity, and discuss diversity’s importance in explaining complex images. We first explain the baselines and evaluations used in this paper.\nBaselines & Metrics. Apart from classical beam search, we compare DBS with the diverse decoding method proposed in Li & Jurafsky (2016). We also compare against two other complementary decoding techniques proposed in Li et al. (2015) and Wu et al. (2016). Note that these two techniques are not directly comparable with DBS since the goal is not to produce diverse lists. We now provide a brief description of the comparisons mentioned:\n- Li & Jurafsky (2016): modify BS by introducing an intra-sibling rank. For each partial solution, the set of |V| beam extensions are sorted and assigned intra-sibling ranks k ∈ [|V|] in order\nof decreasing log probabilities, θt(yt). The log probability of an extension is then reduced in proportion to its rank, and continuations are re-sorted under these modified log probabilities to select the top B ‘diverse’ beam extensions.\n- Li et al. (2015): train an additional unconditioned target sequence model U(y) and perform BS decoding on an augmented objective P (y|x)− λU(y), penalizing input-independent decodings.\n- Wu et al. (2016) modify the beam-search objective by introducing length-normalization that favors longer sequences. The joint log-probability of completed sequences is divided by a factor, (5 + |y|)α/(5 + 1)α, where α ∈ [0, 1].\nWe compare to our own implementations of these methods as none are publicly available. Both Li & Jurafsky (2016) and Li et al. (2015) develop and use re-rankers to pick a single solution from the generated lists. Since we are interested in evaluating the quality of the generated lists and in isolating the gains due to diverse decoding, we do not implement any re-rankers, simply sorting by log-probability.\nWe evaluate the performance of the generated lists using the following two metrics:\n- Oracle Accuracy: Oracle or top k accuracy w.r.t. some task-specific metric, such as BLEU (Papineni et al., 2002) or SPICE (Anderson et al., 2016), is the maximum value of the metric achieved over a list of k potential solutions. Oracle accuracy is an upper bound on the performance of any re-ranking strategy and thus measures the maximum potential of a set of outputs.\n- Diversity Statistics: We count the number of distinct n-grams present in the list of generated outputs. Similar to Li et al. (2015), we divide these counts by the total number of words generated to bias against long sentences.\nSimultaneous improvements in both metrics indicate that output sequences have increased diversity without sacrificing fluency and correctness with respect to target tasks."
    }, {
      "heading" : "5.1 SENSITIVITY ANALYSIS AND EFFECT OF DIVERSITY FUNCTIONS",
      "text" : "Here we discuss the impact of the number of groups, strength of diversity , and various forms of diversity for language models. Note that the parameters of DBS (and other baselines) were tuned on a held-out validation set for each experiment. The supplement provides further discussion and experimental details.\nNumber of Groups (G). Setting G=B allows for the maximum exploration of the search space, while settingG=1 reduces DBS to BS, resulting in increased exploitation of the search-space around the 1-best decoding. Empirically, we find that maximum exploration correlates with improved oracle accuracy and hence use G=B to report results unless mentioned otherwise. See the supplement for a comparison and more details.\nDiversity Strength (λ). The diversity strength λ specifies the trade-off between the model score and diversity terms. As expected, we find that a higher value of λ produces a more diverse list; however, very large values of λ can overpower model score and result in grammatically incorrect outputs. We set λ via grid search over a range of values to maximize oracle accuracies achieved on the validation set. We find a wide range of λ values (0.2 to 0.8) work well for most tasks and datasets.\nChoice of Diversity Function (∆). In Section 3, we defined ∆(·) as a function over a set of partial solutions that outputs a vector of dissimilarity scores for all possible beam completions. Assuming that each of the previous groups influences the completion of the current group independently, we can simplify ∆( ⋃g−1 h=1 Y h [t]) as the sum of each group’s contributions as ∑g−1 h=1 ∆(Y h [t]). In Section 3, we illustrated a simple hamming diversity of this form that penalizes selection of tokens proportionally to the number of time it was used in previous groups. However, this factorized diversity term can take various forms in our model – with hamming diversity being the simplest. For language models, we study the effect of using cumulative (i.e. considering all past time steps), n-gram and neural embedding based diversity functions. Each of these forms encode differing notions of diversity and result in DBS outperforming BS. We find simple hamming distance to be effective and report results based on this diversity measure unless otherwise specified. More details about these forms of the diversity term are provided in the supplementary."
    }, {
      "heading" : "5.2 IMAGE CAPTIONING",
      "text" : "Dataset and Models. We evaluate on two datasets – COCO (Lin et al., 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing (with 200 held out images used to tune hyperparameters). We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository.\nResults. Table 1 shows Oracle (top k) SPICE for different values of k. DBS consistently outperforms BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.14% and 4.65% SPICE@20 improvements over BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets, implying DBS may be better suited for real-world applications.\nTable 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average – DBS obtains an average maximum log probability of -6.53 opposed to -6.91 found by BS of the same beam width. This empirical evidence suggests that using DBS as a replacement to BS may lead to lower inference approximation error.\nHuman Studies. To evaluate human preference between captions generated by DBS and BS, we perform a human study via Amazon Mechanical Turk using all 1000 images of PASCAL-50S. For each image, both DBS and standard BS captions are shown to 5 different users. They are then asked – “Which of the two robots understands the image better?” In this forced-choice test, DBS captions were preferred over BS 60% of the time by human annotators.\nIs diversity always needed? While these results show that diverse outputs are important for systems that interact with users, is diversity always beneficial? While images with many objects (e.g., a park or a living room) can be described in multiple ways, the same is not true when there are few objects (e.g., a close up of a cat or a selfie). This notion is studied by Ionescu et al. (2016), which defines a “difficulty score”: the human response time for solving a visual search task. On the PASCAL50S dataset, we observe a positive correlation (ρ = 0.73) between difficulty scores and humans preferring DBS to BS. Moreover, while DBS is generally preferred by humans for ‘difficult’ images, both are about equally preferred on ‘easier’ images. Details are provided in the supplement."
    }, {
      "heading" : "5.3 MACHINE TRANSLATION",
      "text" : "We use the WMT’14 dataset containing 4.5M sentences to train our machine translation models. We train stacking LSTM models as detailed in Luong et al. (2015), consisting of 4 layers and 1024- dimensional hidden states. While decoding sentences, we employ the same strategy to replace UNK tokens. We train our models using the publicly available seq2seq-attn2 code repository. We report results on news-test-2013 and news-test-2014 and use the news-test-2012 to tune the parameters of DBS. We use sentence level BLEU scores to compute oracle metrics and report distinct n-grams\n1https://github.com/karpathy/neuraltalk2 2https://github.com/harvardnlp/seq2seq-attn\nsimilar to image captioning. Results are shown in Table 2 and we again find that DBS consistently outperforms all baselines."
    }, {
      "heading" : "5.4 DIALOG GENERATION",
      "text" : "Dialog generation is a task that is inherently diverse as there are multiple valid ways to respond to a statement. We train a seq2seq model consisting of LSTMs as in Vinyals & Le (2015) on the Cornell Movie Dialogs Corpus (Danescu-Niculescu-Mizil & Lee, 2011) using the neuralconvo3 repository. The training dataset consists of 222,182 conversational exchanges between movie characters. Since automatic evaluation of dialog generation responses is an open research problem with existing metrics being poorly correlated with human judgement (Liu et al., 2016), we show qualitative results to demonstrate the effectiveness of DBS. Table 3 compares BS and DBS at B=3."
    }, {
      "heading" : "5.5 VISUAL QUESTION GENERATION",
      "text" : "We also report results on Visual Question Generation (VQG) (Mostafazadeh et al., 2016), where a model is trained to produce questions about an image. Generating visually focused questions is interesting because it requires reasoning about multiple problems that are central to vision – e.g., object attributes, relationships between objects, and natural language. Furthermore, many questions could make sense for one image, so it is important that lists of generated questions be diverse.\nWe use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning architectures. Instead of captions, the training set now consists of 3 questions per image. Similar to previous results, using beam search to sample outputs results in similarly worded questions (see Fig. 3) and DBS brings out new details captured by the model. Counting the number of types of questions generated (as defined by Antol et al. (2015)) allows us to measure this diversity. We observe that the number of question types generated per image increases from 2.3 for BS to 3.7 for DBS (at B = 6)."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "Beam search is widely a used approximate inference algorithm for decoding sequences from neural sequence models; however, it suffers from a lack of diversity. Producing multiple highly similar and generic outputs is not only wasteful in terms of computation but also detrimental for tasks with\n3https://github.com/macournoyer/neuralconvo\ninherent ambiguity like many involving language. In this work, we modify Beam Search with a diversity-augmented sequence decoding objective to produce Diverse Beam Search. We develop a ‘doubly greedy’ approximate algorithm to minimize this objective and produce diverse sequence decodings. Our method consistently outperforms beam search and other baselines across all our experiments without extra computation or task-specific overhead. DBS is task-agnostic and can be applied to any case where BS is used, which we demonstrate in multiple domains. Our implementation available at https://github.com/ashwinkalyan/dbs."
    }, {
      "heading" : "APPENDIX",
      "text" : ""
    }, {
      "heading" : "SENSIVITY STUDIES",
      "text" : "Number of Groups. Fig. 4 presents snapshots of the transition from BS to DBS at B = 6 and G = {1, 3, 6}. As beam width moves from 1 to G, the exploration of the method increases resulting in more diverse lists.\nDiversity Strength. As noted in Section 5.1, our method is robust to a wide range of values of the diversity strength (λ). Fig. 5a shows a grid search of λ for image-captioning on the PASCAL-50S dataset.\nChoice of Diversity Function. The diversity function can take various forms ranging from simple hamming diversity to neural embedding based diversity. We discuss some forms for language modelling below:\n- Hamming Diversity. This form penalizes the selection of tokens used in previous groups proportional to the number of times it was selected before.\n- Cumulative Diversity. Once two sequences have diverged sufficiently, it seems unnecessary and perhaps harmful to restrict that they cannot use the same words at the same time. To encode this ‘backing-off’ of the diversity penalty we introduce cumulative diversity which keeps a count of identical words used at every time step, indicative of overall dissimilarity. Specifically, ∆(Y h[t])[y g [t]] = exp{−( ∑ τ∈t ∑ b∈B′ I[y h b,τ 6=y g b,τ ])/Γ} where Γ is a temperature parameter control-\nling the strength of the cumulative diversity term and I[·] is the indicator function. - n-gram Diversity. The current group is penalized for producing the same n-grams as previous\ngroups, regardless of alignment in time – similar to Gimpel et al. (2013). This is proportional to the number of times each n-gram in a candidate occurred in previous groups. Unlike hamming diversity, n-grams capture higher order structures in the sequences.\n- Neural-embedding Diversity. While all the previous diversity functions discussed above perform exact matches, neural embeddings such as word2vec (Mikolov et al., 2013) can penalize semantically similar words like synonyms. This is incorporated in each of the previous diversity functions by replacing the hamming similarity with a soft version obtained by computing the cosine similarity between word2vec representations. When using with n-gram diversity, the representation of the n-gram is obtained by summing the vectors of the constituent words.\nEach of these various forms encode different notions of diversity. Hamming diversity ensures different words are used at different times, but can be circumvented by small changes in sequence alignment. While n-gram diversity captures higher order statistics, it ignores sentence alignment. Neural-embedding based encodings can be seen as a semantic blurring of either the hamming or n-gram metrics, with word2vec representation similarity propagating diversity penalties not only to exact matches but also to close synonyms. Fig. 5b shows the oracle performace of various forms of the diversity function described in Section 5.1. We find that using any of the above functions help outperform BS in the tasks we examine; hamming diversity achieves the best oracle performance despite its simplicity."
    }, {
      "heading" : "IMAGE CAPTIONING EVALUATION",
      "text" : "While we report oracle SPICE values in the paper, our method consistently outperforms baselines and classical BS on other standard metrics such as CIDEr (Table 4), METEOR (Table 5) and ROUGE (Table 6). We provide these additional results in this section."
    }, {
      "heading" : "Dataset Method Oracle Accuracy (CIDEr)",
      "text" : "Modified SPICE evaluation. To measure both the quality and the diversity of the generated captions, we compute SPICE-score by comparing the graph union of all the generated hypotheses with the ground truth scene graph. This measure rewards all the relevant relations decoded as against oracle accuracy that compares to relevant relations present only in the top-scoring caption. We observe that DBS outperforms both baselines under this measure with a score of 18.345 as against a score of 16.988 (beam search) and 17.452 (Li & Jurafsky, 2016)."
    }, {
      "heading" : "HUMAN STUDIES",
      "text" : "For image-captioning, we conduct a human preference study between BS and DBS captions as explained in Section 5. A screen shot of the interface used to collect human preferences for captions generated using DBS and BS is presented in Fig. 6. The lists were shuffled to guard the task from being gamed by a turker.\nAs mentioned in Section 5, we observe that difficulty score of an image and human preference for DBS captions are positively correlated. The dataset contains more images that are less difficulty and so, we analyze the correlation by dividing the data into three bins. For each bin, we report the % of images for which DBS captions were preferred after a majority vote (i.e. at least 3/5 turkers voted in favor of DBS) in Table 7. At low difficulty scores consisting mostly of iconic images – one might expect that BS would be preferred more often than chance. However, mismatch between the statistics of the training and testing data results in a better performance of DBS. Some examples for this case are provided in Fig. 7. More general qualitative examples are provided in Fig. 8."
    }, {
      "heading" : "DISCUSSION",
      "text" : "Are longer sentences better? Many recent works propose a scoring or a ranking objective that depends on the sequence length. These favor longer sequences, reasoning that they tend to have more details and resulting in improved accuracies. We measure the correlation between length of a sequence and its accuracy (here, SPICE) and observe insignificant correlation between SPICE and sequence length. On the PASCAL-50S dataset, we find that BS and DBS have are negatively correlated (ρ = −0.003 and ρ = −0.015 respectively), while (Li & Jurafsky, 2016) is correlated positively (ρ = 0.002). Length is not correlated with performance in this case.\nEfficient utilization of beam budget. In this experiment, we emperically show that DBS makes efficient use of the beam budget in exploring the search space for better solutions. Fig. 9 shows the variation of oracle SPICE (@B) with the beam size. At really high beam widths, all decoding techniques achieve similar oracle accuracies. However, diverse decoding techniques like DBS achieve the same oracle at much lower beam widths. Hence, DBS not only produces sequence lists that are significantly different but also efficiently utilizes the beam budget to decode better solutions."
    } ],
    "references" : [ {
      "title" : "Spice: Semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Anderson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "VQA: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Diverse M-Best Solutions in Markov Random Fields",
      "author" : [ "Dhruv Batra", "Payman Yadollahpour", "Abner Guzman-Rivera", "Gregory Shakhnarovich" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Batra et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Batra et al\\.",
      "year" : 2012
    }, {
      "title" : "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs",
      "author" : [ "Cristian Danescu-Niculescu-Mizil", "Lillian Lee" ],
      "venue" : "In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,",
      "citeRegEx" : "Danescu.Niculescu.Mizil and Lee.,? \\Q2011\\E",
      "shortCiteRegEx" : "Danescu.Niculescu.Mizil and Lee.",
      "year" : 2011
    }, {
      "title" : "Visual storytelling",
      "author" : [ "Francis Ferraro", "Ishan Mostafazadeh", "Nasrinand Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiadong He", "Pushmeet Kohli", "Dhruv Batra", "C Lawrence Zitnick" ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT),",
      "citeRegEx" : "Ferraro et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ferraro et al\\.",
      "year" : 2016
    }, {
      "title" : "Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines",
      "author" : [ "Jenny Rose Finkel", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Finkel et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2006
    }, {
      "title" : "A systematic exploration of diversity in machine translation",
      "author" : [ "K. Gimpel", "D. Batra", "C. Dyer", "G. Shakhnarovich" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Gimpel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gimpel et al\\.",
      "year" : 2013
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "How hard can it be? Estimating the difficulty of visual search in an image",
      "author" : [ "Radu Tudor Ionescu", "Bogdan Alexe", "Marius Leordeanu", "Marius Popescu", "Dim Papadopoulos", "Vittorio Ferrari" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Ionescu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ionescu et al\\.",
      "year" : 2016
    }, {
      "title" : "Smart reply: Automated reeponse suggestion for email",
      "author" : [ "Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "László Lukács", "Marina Ganea", "Peter Young" ],
      "venue" : "In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Kannan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Karpathy and Fei.Fei.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Inferring m-best diverse labelings in a single one",
      "author" : [ "Alexander Kirillov", "Bogdan Savchynskyy", "Dmitrij Schlesinger", "Dmitry Vetrov", "Carsten Rother" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Kirillov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kirillov et al\\.",
      "year" : 2015
    }, {
      "title" : "Mutual information and diverse decoding improve neural machine translation",
      "author" : [ "Jiwei Li", "Dan Jurafsky" ],
      "venue" : "arXiv preprint arXiv:1601.00372,",
      "citeRegEx" : "Li and Jurafsky.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li and Jurafsky.",
      "year" : 2016
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT),",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Vlad Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau" ],
      "venue" : "URL http://arxiv.org/abs/1603",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1508.04025,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Generating natural questions about an image",
      "author" : [ "Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende" ],
      "venue" : "Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Mostafazadeh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "N-best maximal decoders for part models",
      "author" : [ "Dennis Park", "Deva Ramanan" ],
      "venue" : "In Proceedings of IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Park and Ramanan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Park and Ramanan.",
      "year" : 2011
    }, {
      "title" : "Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets",
      "author" : [ "Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Prasad et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2014
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Vedantam et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence-video to text",
      "author" : [ "Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Venugopalan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Venugopalan et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "arXiv preprint arXiv:1506.05869,",
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey" ],
      "venue" : "arXiv preprint arXiv:1609.08144,",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "METEOR Oracle accuracy on COCO and PASCAL-50S datasets for image captioning at B = 20",
      "author" : [ "Wu" ],
      "venue" : null,
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 2016
    }, {
      "title" : "Modified SPICE evaluation. To measure both the quality and the diversity of the generated captions, we compute SPICE-score by comparing the graph union of all the generated hypotheses with",
      "author" : [ "Wu" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "HUMAN STUDIES For image-captioning, we conduct a human preference study between BS and DBS captions",
      "author" : [ "Wu" ],
      "venue" : null,
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "In the last few years, Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) or more generally, neural sequence models have become the standard choice for modeling time-series data for a wide range of applications including speech recognition (Graves et al., 2013), machine translation (Bahdanau et al.",
      "startOffset" : 263,
      "endOffset" : 284
    }, {
      "referenceID" : 2,
      "context" : ", 2013), machine translation (Bahdanau et al., 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : ", 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al.",
      "startOffset" : 80,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : ", 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al.",
      "startOffset" : 80,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : ", 2015), and visual question answering (Antol et al., 2015).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics), it is common practice to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016).",
      "startOffset" : 234,
      "endOffset" : 304
    }, {
      "referenceID" : 5,
      "context" : "improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics), it is common practice to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016).",
      "startOffset" : 234,
      "endOffset" : 304
    }, {
      "referenceID" : 10,
      "context" : "always saying “I don’t know” in conversation models (Kannan et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms – the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.",
      "startOffset" : 117,
      "endOffset" : 181
    }, {
      "referenceID" : 21,
      "context" : "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms – the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.",
      "startOffset" : 117,
      "endOffset" : 181
    }, {
      "referenceID" : 12,
      "context" : "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms – the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.",
      "startOffset" : 117,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).",
      "startOffset" : 105,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).",
      "startOffset" : 105,
      "endOffset" : 191
    }, {
      "referenceID" : 21,
      "context" : "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).",
      "startOffset" : 105,
      "endOffset" : 191
    }, {
      "referenceID" : 3,
      "context" : "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity.",
      "startOffset" : 128,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. Kirillov et al. (2015) show how these solutions can be found",
      "startOffset" : 128,
      "endOffset" : 432
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged.",
      "startOffset" : 53,
      "endOffset" : 870
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference.",
      "startOffset" : 53,
      "endOffset" : 1058
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation.",
      "startOffset" : 53,
      "endOffset" : 1291
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation.",
      "startOffset" : 53,
      "endOffset" : 1316
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists. Li & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists.",
      "startOffset" : 53,
      "endOffset" : 1666
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists. Li & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective (as in DBS) rather than via a procedural heuristic provides easier generalization to incorporate different notions of diversity and control the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms the method of Li & Jurafsky (2016). Li et al.",
      "startOffset" : 53,
      "endOffset" : 2130
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists. Li & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective (as in DBS) rather than via a procedural heuristic provides easier generalization to incorporate different notions of diversity and control the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms the method of Li & Jurafsky (2016). Li et al. (2015) introduced a novel decoding objective that maximizes mutual information between inputs and predicted outputs to penalize generic sequences.",
      "startOffset" : 53,
      "endOffset" : 2148
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists. Li & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective (as in DBS) rather than via a procedural heuristic provides easier generalization to incorporate different notions of diversity and control the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms the method of Li & Jurafsky (2016). Li et al. (2015) introduced a novel decoding objective that maximizes mutual information between inputs and predicted outputs to penalize generic sequences. This operates on a principle orthogonal and complementary to DBS and Li & Jurafsky (2016). It works by penalizing utterances that are generally more frequent (diversity independent of input) rather than penalizing utterances that are similar to other utterances produced for the same input (diversity conditioned on input).",
      "startOffset" : 53,
      "endOffset" : 2378
    }, {
      "referenceID" : 7,
      "context" : "Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences – suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists. Li & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective (as in DBS) rather than via a procedural heuristic provides easier generalization to incorporate different notions of diversity and control the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms the method of Li & Jurafsky (2016). Li et al. (2015) introduced a novel decoding objective that maximizes mutual information between inputs and predicted outputs to penalize generic sequences. This operates on a principle orthogonal and complementary to DBS and Li & Jurafsky (2016). It works by penalizing utterances that are generally more frequent (diversity independent of input) rather than penalizing utterances that are similar to other utterances produced for the same input (diversity conditioned on input). Furthermore, the input-independent approach requires training a new language model for the target language while DBS just requires a diversity function ∆. Combination of these complementary techniques is left as interesting future work. In other recent work, Wu et al. (2016) modify the beam search objective by introducing lengthnormalization to favor longer sequences and a coverage penalty that favors sequences that account for the complete input sequence.",
      "startOffset" : 53,
      "endOffset" : 2888
    }, {
      "referenceID" : 14,
      "context" : "We also compare against two other complementary decoding techniques proposed in Li et al. (2015) and Wu et al.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "We also compare against two other complementary decoding techniques proposed in Li et al. (2015) and Wu et al. (2016). Note that these two techniques are not directly comparable with DBS since the goal is not to produce diverse lists.",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "We also compare against two other complementary decoding techniques proposed in Li et al. (2015) and Wu et al. (2016). Note that these two techniques are not directly comparable with DBS since the goal is not to produce diverse lists. We now provide a brief description of the comparisons mentioned: - Li & Jurafsky (2016): modify BS by introducing an intra-sibling rank.",
      "startOffset" : 80,
      "endOffset" : 323
    }, {
      "referenceID" : 19,
      "context" : "some task-specific metric, such as BLEU (Papineni et al., 2002) or SPICE (Anderson et al.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", 2002) or SPICE (Anderson et al., 2016), is the maximum value of the metric achieved over a list of k potential solutions.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "- Li et al. (2015): train an additional unconditioned target sequence model U(y) and perform BS decoding on an augmented objective P (y|x)− λU(y), penalizing input-independent decodings.",
      "startOffset" : 2,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "- Li et al. (2015): train an additional unconditioned target sequence model U(y) and perform BS decoding on an augmented objective P (y|x)− λU(y), penalizing input-independent decodings. - Wu et al. (2016) modify the beam-search objective by introducing length-normalization that favors longer sequences.",
      "startOffset" : 2,
      "endOffset" : 206
    }, {
      "referenceID" : 13,
      "context" : "- Li et al. (2015): train an additional unconditioned target sequence model U(y) and perform BS decoding on an augmented objective P (y|x)− λU(y), penalizing input-independent decodings. - Wu et al. (2016) modify the beam-search objective by introducing length-normalization that favors longer sequences. The joint log-probability of completed sequences is divided by a factor, (5 + |y|)/(5 + 1), where α ∈ [0, 1]. We compare to our own implementations of these methods as none are publicly available. Both Li & Jurafsky (2016) and Li et al.",
      "startOffset" : 2,
      "endOffset" : 528
    }, {
      "referenceID" : 13,
      "context" : "- Li et al. (2015): train an additional unconditioned target sequence model U(y) and perform BS decoding on an augmented objective P (y|x)− λU(y), penalizing input-independent decodings. - Wu et al. (2016) modify the beam-search objective by introducing length-normalization that favors longer sequences. The joint log-probability of completed sequences is divided by a factor, (5 + |y|)/(5 + 1), where α ∈ [0, 1]. We compare to our own implementations of these methods as none are publicly available. Both Li & Jurafsky (2016) and Li et al. (2015) develop and use re-rankers to pick a single solution from the generated lists.",
      "startOffset" : 2,
      "endOffset" : 549
    }, {
      "referenceID" : 0,
      "context" : ", 2002) or SPICE (Anderson et al., 2016), is the maximum value of the metric achieved over a list of k potential solutions. Oracle accuracy is an upper bound on the performance of any re-ranking strategy and thus measures the maximum potential of a set of outputs. - Diversity Statistics: We count the number of distinct n-grams present in the list of generated outputs. Similar to Li et al. (2015), we divide these counts by the total number of words generated to bias against long sentences.",
      "startOffset" : 18,
      "endOffset" : 399
    }, {
      "referenceID" : 22,
      "context" : ", 2014) and PASCAL-50S (Vedantam et al., 2015).",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO.",
      "startOffset" : 24,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing (with 200 held out images used to tune hyperparameters). We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. Table 1 shows Oracle (top k) SPICE for different values of k. DBS consistently outperforms BS and Li & Jurafsky (2016) on both datasets.",
      "startOffset" : 24,
      "endOffset" : 427
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing (with 200 held out images used to tune hyperparameters). We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. Table 1 shows Oracle (top k) SPICE for different values of k. DBS consistently outperforms BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.14% and 4.65% SPICE@20 improvements over BS and Li & Jurafsky (2016)) than COCO.",
      "startOffset" : 24,
      "endOffset" : 573
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing (with 200 held out images used to tune hyperparameters). We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. Table 1 shows Oracle (top k) SPICE for different values of k. DBS consistently outperforms BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.14% and 4.65% SPICE@20 improvements over BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets, implying DBS may be better suited for real-world applications. Table 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average – DBS obtains an average maximum log probability of -6.53 opposed to -6.91 found by BS of the same beam width. This empirical evidence suggests that using DBS as a replacement to BS may lead to lower inference approximation error. Table 1: Oracle accuracy and distinct n-grams on COCO and PASCAL-50S datasets for image captioning at B = 20. While we report SPICE, we observe similar trends in other metrics (reported in supplement). Dataset Method Oracle Accuracy (SPICE) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 4.933 7.046 7.949 8.747 0.12 0.57 1.35 2.50 Li & Jurafsky (2016) 5.",
      "startOffset" : 24,
      "endOffset" : 1810
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing (with 200 held out images used to tune hyperparameters). We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. Table 1 shows Oracle (top k) SPICE for different values of k. DBS consistently outperforms BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.14% and 4.65% SPICE@20 improvements over BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets, implying DBS may be better suited for real-world applications. Table 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average – DBS obtains an average maximum log probability of -6.53 opposed to -6.91 found by BS of the same beam width. This empirical evidence suggests that using DBS as a replacement to BS may lead to lower inference approximation error. Table 1: Oracle accuracy and distinct n-grams on COCO and PASCAL-50S datasets for image captioning at B = 20. While we report SPICE, we observe similar trends in other metrics (reported in supplement). Dataset Method Oracle Accuracy (SPICE) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 4.933 7.046 7.949 8.747 0.12 0.57 1.35 2.50 Li & Jurafsky (2016) 5.083 7.248 8.096 8.917 0.15 0.97 2.43 5.31 PASCAL-50S DBS 5.357 7.357 8.269 9.293 0.18 1.26 3.67 7.33 Wu et al. (2016) 5.",
      "startOffset" : 24,
      "endOffset" : 1930
    }, {
      "referenceID" : 14,
      "context" : "45 Li et al. (2015) 5.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "45 Li et al. (2015) 5.129 7.175 8.168 8.560 0.13 1.15 3.58 8.42 Beam Search 16.278 22.962 25.145 27.343 0.40 1.51 3.25 5.67 Li & Jurafsky (2016) 16.",
      "startOffset" : 3,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "45 Li et al. (2015) 5.129 7.175 8.168 8.560 0.13 1.15 3.58 8.42 Beam Search 16.278 22.962 25.145 27.343 0.40 1.51 3.25 5.67 Li & Jurafsky (2016) 16.351 22.715 25.234 27.591 0.54 2.40 5.69 8.94 COCO DBS 16.783 23.081 26.088 28.096 0.56 2.96 7.38 13.44 Wu et al. (2016) 16.",
      "startOffset" : 3,
      "endOffset" : 268
    }, {
      "referenceID" : 14,
      "context" : "45 Li et al. (2015) 5.129 7.175 8.168 8.560 0.13 1.15 3.58 8.42 Beam Search 16.278 22.962 25.145 27.343 0.40 1.51 3.25 5.67 Li & Jurafsky (2016) 16.351 22.715 25.234 27.591 0.54 2.40 5.69 8.94 COCO DBS 16.783 23.081 26.088 28.096 0.56 2.96 7.38 13.44 Wu et al. (2016) 16.642 22.643 25.437 27.783 0.54 2.42 6.01 7.08 Li et al. (2015) 16.",
      "startOffset" : 3,
      "endOffset" : 333
    }, {
      "referenceID" : 9,
      "context" : "This notion is studied by Ionescu et al. (2016), which defines a “difficulty score”: the human response time for solving a visual search task.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 16,
      "context" : "We train stacking LSTM models as detailed in Luong et al. (2015), consisting of 4 layers and 1024dimensional hidden states.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "54 Wu et al. (2016) 20.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "50 Li et al. (2015) 20.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Since automatic evaluation of dialog generation responses is an open research problem with existing metrics being poorly correlated with human judgement (Liu et al., 2016), we show qualitative results to demonstrate the effectiveness of DBS.",
      "startOffset" : 153,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : "We also report results on Visual Question Generation (VQG) (Mostafazadeh et al., 2016), where a model is trained to produce questions about an image.",
      "startOffset" : 59,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning architectures.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning architectures. Instead of captions, the training set now consists of 3 questions per image. Similar to previous results, using beam search to sample outputs results in similarly worded questions (see Fig. 3) and DBS brings out new details captured by the model. Counting the number of types of questions generated (as defined by Antol et al. (2015)) allows us to measure this diversity.",
      "startOffset" : 24,
      "endOffset" : 438
    } ],
    "year" : 2017,
    "abstractText" : "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top B candidates. This tends to result in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. We observe that our method not only improved diversity but also finds better top 1 solutions by controlling for the exploration and exploitation of the search space. Moreover, these gains are achieved with minimal computational or memory overhead compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation, conversation and visual question generation using both standard quantitative metrics and qualitative human studies. We find that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.",
    "creator" : "LaTeX with hyperref package"
  }
}