{
  "name" : "555.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recent advances in deep neural networks (DNNs) have led to breakthroughs in fields such as image classification (He et al., 2015; Krizhevsky et al., 2012) and speech recognition (Yu et al., 2010; Dahl et al., 2012). One reason for the effectiveness of DNNs is their ability to integrate low, mid and highlevel features in a natural way (Zeiler & Fergus, 2014). While recent work such as (Simonyan & Zisserman, 2014) suggests that in many cases the depth of the architecture is crucial, the emergence of more complex architectures (He et al., 2015; Szegedy et al., 2015) demonstrates that depth alone often does not suffice.\nWhile DNNs have been highly effective in several domains, their application in additional fields is yet to become widespread. We argue that this is the case due to two challenges. The first is the difficulty of designing effective architectures for domains in which there is little or no previous knowledge on the application of deep learning. Moreover, since designing DNN architectures is not intuitive for most people, this task is likely to fall to experts whose time is in high demand. The second challenge, which is strongly coupled with the first, is the large amounts of computing power and time required to evaluate multiple DNNs. These traits constrain the number of DNN architectures that can be evaluated, thus further limiting one’s ability to explore new architectures or respond to changing circumstances.\nIn this study we explore the possibility of applying architectures that are effective for one domain to another. We do so by generating a large number of architectures and evaluate their performance on multiple tabular datasets in order to determine whether the architectures are transferable. We also explore the feasibility of architectures with parallel layers and compare their effectiveness to that of their “linear” counterparts. Our results show that while architectures do not perform well across multiple datasets, parallel architectures are surprisingly effective.\nWhen attempting to apply DNNs to an unknown domain, one way of approaching the problem would be to randomly “sample” various architectures and analyze their performance distribution. The top-performing architectures found in the sampling can form the base for future exploration while the variance in performance can assist in determining the number of architectures that need to be sampled. We explore a meta-learning approach that may improve the efficiency of this process by ranking the architectures based on their expected performance. Our approach models the topology of the DNN as well as the changes in weights, biases and activation function layers throughout the initial training steps and uses this information to rank the architectures by their relative performance. Preliminary results are encouraging.\nWhile we consider this study to be an important first step, we feel obliged to point out that work is done in a limited setting. To enable the generation of multiple DNN architectures with diverse topologies, we applied uniform and fixed parameters such as layer sizes and learning rates. As a result, the architecture space we explore is limited. Validating our results on a more diverse set of architectures with multiple hyperparameter configuration will require additional experimentation. We plan to address these issues in future work.\nOur contributions are as follows:\n• We explore DNNs across multiple datasets, evaluate their effectiveness and analyze if some perform best across datasets.\n• We systematically evaluate a large number of architectures over multiple supervisedclassification datasets and derive insights regarding the design and application of DNNs with parallel layers for general classification problems.\n• We present a novel meta learning-based ranking method that utilizes both topological features as well as weights, biases and activation function layers of the various components of the DNN architecture during the initial training phase. To the best of our knowledge, this is the first time these characteristics have been used in a meta-learning scheme. Preliminary results of this approach are promising."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "We review two areas of research whose aim is to better understand and improve the performance of DNN architectures. The first is area of research focuses on the exploration and analysis of DNN architectures. The second area of research is automatic parameter tuning."
    }, {
      "heading" : "2.1 EXPLORATION AND ANALYSIS OF DNN ARCHITECTURES",
      "text" : "Despite their remarkable success in various domains, the inner-workings of DNNs remain to some degree a “black box”. Multiple studies attempted to provide insight into this matter. In Jarrett et al. (2009), the authors analyze convolutional neural networks (CNNs) and derive insights regarding the architecture design and the contribution of its different components. Another work aimed at better understanding CNNs is presented in Shang et al. (2016). The authors analyze widely used CNN architectures and derive insights into their possible shortcomings. To address these shortcomings, they propose a new version of the popular ReLU activation scheme.\nThe exploration of DNN architectures has also taken place for recurrent neural networks (RNNs). In Zaremba (2015), the authors explore various modifications to LSTM architectures to improve their performance, and propose several enhancements to the architecture. Another study Wu & King (2016) aims to determine the reasons for the effectiveness of LSTMs and identify the contribution of its different elements. Based on their conclusions, the authors proposed a simplified version of LSTM."
    }, {
      "heading" : "2.2 AUTOMATIC DNN PARAMETER TUNING",
      "text" : "The ability to automatically tune the hyperparameters of a DNN architecture is important not only because of its ability to improve performance, but also due to the considerable time it can potentially save. In Maclaurin et al. (2015) the authors demonstrate how information extracted from the stochastic gradient descent can efficiently tune multiple parameters in the architecture. An additional work that analyzes the gradient is presented in Duvenaud et al. (2016), where the information is used to determine when to terminate the training of the architecture to avoid over-fitting. A different optimization approach is presented in Mendoza et al., where the authors define a large set of hyperparameters (batch size, learning rate, activation types, etc.) and apply Bayesian optimization on top-performing configurations. The approach is only applied to feed-forward networks and outperforms human experts by 10%, using the AUC measure.\nAdditional types of optimization have also been proposed in recent years. In Jin et al. (2016), the authors focus on setting the size of hidden layers in RNNs. They accomplish this by converting the optimization problem into a subset selection problem. An important aspect of this approach is\nthat it takes time constraints into account, thus enabling solutions that are feasible given available resources. Another approach, in which one long-short term memory network (LSTM) is used to optimize another, was proposed by Andrychowicz et al. (2016). The two networks have shared parameters but separate hidden states and the optimizer network is both modifying its own weights and those of the optimized simultaneously. Finally, an approach that automatically adjusts the learning rates of the neural net was presented in Schaul et al. (2013). The approach has been shown to be effective both on convex and non-convex learning tasks.\nRecent work by Li et al. (2016) proposes an exploration/exploitation scheme for hyperparameter tuning. The authors apply a multi-arm bandits algorithm, with each arm representing a parameter configuration. A process of successive halving (Jamieson & Talwalkar, 2015), in which a certain percentage of the lowest-performing configurations are dropped every n steps enables the framework to explore promising directions. We consider this approach complementary to our proposed metalearning approach, as the former enables exploration of a large number of configurations while the latter can reduce time required to assess their performance."
    }, {
      "heading" : "3 PROBLEM DEFINITION",
      "text" : "As mentioned in Section 1, one of the challenges in applying deep learning to a new field is the need to design and test multiple DNN architectures. Only by iterative testing can practitioners discover the capabilities and limitations of deep learning in the domain. Even with ever-increasing computing power, the high computational cost of this process currently presents a significant barrier for most practitioners.\nThis limitation leads us to explore the following questions:\n1. Would DNN architectures that perform well on one general supervised classification problem also be effective when applied to dataset in other domains?\n2. What types of architectures are effective for general supervised learning problems? Should practitioners consider other types architectures besides “deep”?\n3. Can DNN architectures outperform “conventional” machine learning classifiers in general supervised problems?\n4. Is it possible to identify top-performing networks in the early stages of the training? if possible, such a technique could preserve valuable computing resources.\nWe attempt to begin addressing these questions in the subsequent sections of this study. We iteratively evaluate a large number of DNN architectures on a set of supervised classification problems. These datasets differ from those of image and speech classification in that they consist of tabular data with both numeric and discrete features. These differences make it unclear what types of architectures are likely to perform well on these domains. The datasets we analyze were selected because of their diversity in terms of size and feature number and composition. These traits also enable us to better understand the difficulties in applying DNN architectures across multiple domains.\nIn order to provide meaningful results, the set of architectures we evaluate is also diverse. We therefore automatically generate a diverse set of architecture with various topological traits. Because little information is available on the application of deep learning to general supervised classification problems, we choose to explore not only architectures that are linear but also architectures with parallel layers. While the generate set is diverse, additional work is required in order to model additional types of architectures. We elaborate on these points further in the subsequent section."
    }, {
      "heading" : "4 GENERATING MULTIPLE DNN ARCHITECTURES",
      "text" : "In order to effectively explore the architecture space, we require a large and diverse set. We create this set by automatically generating a large number of architectures and training each of them on all training set datasets. Our generation algorithm, presented in Algorithm 1, generates both “deep” and “wide” architectures with parallel layers (see Figure 1(b)). Next we describe the generation process.\nWe consider DNN architectures to consist of components. We define a component as any part of an architecture, be it a layer, normalization or activation function. In this study we consider\nthe following components: fully-connected layers, softmax, batch normalization, dropout and the ReLU, sigmoid and tanh activation functions.\nWe begin the generation process with a “basic” architecture consisting only of two components: a fully-connected input layer and an output softmax layer. We then expand the set of possible architectures by iteratively applying the following steps:\n1. For each pair of components in the architecture, identify all component that could be inserted between them (Figure 1(a)).\n2. For each pair of components in the architecture, identify all component that could be inserted in parallel to one of them (Figure 1(b)).\n3. For each of the components identified in the previous steps, generate a new copy of the architecture and perform the corresponding insertion.\nOur proposed architecture generation approach enables us to generate the topological representation of every possible neural networks that consist of the predefined components. However, we do not generate multiple hyperparameter configurations for each topology and use fixed parameters for each component. We plan to address this limitation in future work, possibly by using an approach similar to the one presented in Li et al. (2016). It is also important to point out that we currently do not support weight-sharing and therefore do not consider CNN and RNN architectures. Given the characteristics of the analyzed data, we do not consider these architecture types likely to produce meaningful results.\nAnother important aspect of the our architecture generation approach is that we generate architectures with connections between layers of various depths. An example of this is shown in Figure 1(b), where we connect layers of depths 1 and 2. This setting enables us to systematically explore more complex designs than those commonly used. We analyze these architectures further in Section 6.\nAs the number of possible architectures grows exponentially, we limit the total number of architectures that we generate by constraining the maximal number of components in a architecture and the number of parallel layers an architecture may contain. The specific settings used in our experiments are presented in Section 6.1. These settings were chosen in order to ensure a diverse set of both deep and wide architectures given the time and computing-power constraints, and we plan to change them in future work to further diversify the set of generated architectures. To select the architectures from which additional ones will be generated, we apply a priority queue. We first sort the architectures by the number of their activation layers (in a descending order) with a secondary sorting based on the total number of components (in an ascending order). This setting prioritizes the creation of deeper architectures with multiple activation layers. For each architecture in the final set, we generate the\nmeta-features described in Section 5. The algorithm for the architecture generation is presented in Algorithm 1.\nAlgorithm 1 Automatic architecture generation 1: procedure ARCHITECTUREGENERATION(arcQueue, initArc) 2: architecturesSet← initArc 3: architecturesQueue← initArc 4: while (architecturesQueue 6= ∅) do 5: newarchitectures← ∅ 6: architecture← arcQueue.pop() 7: for each P (ci, cj)i 6= j ∈ {c1, c2, ..., cn} do 8: candidateComponents← proposeInsertBetweenCandidates(P (ci, cj)) 9: for each candidate ∈ candidateComponents do 10: newarchitecture← insertBetween(architecture, P (ci, cj), candidate) 11: newarchitectures← newarchitectures ∪ newarchitecture 12: candidateComponents← proposeInsertAsideCandidates(P (ci, cj)) 13: for each candidate ∈ candidateComponents do 14: newarchitecture← insertAside(architecture, P (ci, cj), candidate) 15: newarchitectures← newarchitectures ∪ newarchitecture 16: newarchitectures← filter(newarchitectures) 17: arcQueue← arcQueue ∪ newarchitectures 18: architecturesSet← architecturesSet ∪ newarchitectures 19: return architecturesSet"
    }, {
      "heading" : "5 META-LEARNING FOR ARCHITECTURE RANKING",
      "text" : "Our goal is to determine whether by analyzing the topology of DNN architecture as well as the transformations it undergoes in its early training iterations could be used to predict its performance. To this end we develop a novel machine learning-based approach that generates a set of features for each analyzed architecture. Once the features are generated, we use a ranking classifier to assign a score to each architecture. The classifier is trained on a large corpus of datasets (additional information is provided in Section 6.1).\nWe apply meta-learning (Vilalta & Drissi, 2002) to predict the performance of the DNN architectures. Meta-learning is a branch of machine learning in which an algorithm “learns how to learn” by extracting information on the learning process of another algorithm. The features extracted in this process are called meta-features. We generate three types of meta-features: dataset-based, topologybased and training-based. We hypothesize that these groups represent the elements that affect the performance of the DNN architecture - the data on which it is trained, the structure of the network and the changes in its weights, biases and activation functions during throughout the training process. We provide a full overview of the meta-features groups below and detailed information in Appendix A .\nDataset-based meta-features. As explained in Section 3, the datasets we use in the evaluation vary significantly in size and feature composition. These meta-features attempt to represent the multiple characteristics that may affect the performance of deep learning algorithms. We generate three types of meta-features:\n1. General information: general statistics on the analyzed dataset: number of instances and classes, number and type of features and statistics on the correlations among various features.\n2. Entropy-based measures: we partition the dataset’s features based on their type (discrete, numeric, etc.) and calculate statistics on the Information Gain (IG) of the features in each group.\n3. Feature diversity: we partition the dataset into type-based groups and use the chi-squared and paired-t test to calculate the similarity of each pair in each group. We then generate meta-features using the tests’ statistic values.\nTopology-based meta-features. Our generated architectures vary significantly in size, depth and width. Since these traits are likely to affect their performance, we use the meta-features of this group to quantify and model them. The meta-features can be partitioned into two groups:\n1. Architecture composition: general statistics on the number and types of layers and functions that make up the architecture, statistics on layer composition as a function of depth etc.\n2. Connectivity-based measures: for each layer in the architectures, we calculate various measures that are frequently used for graph-analysis. These measures include statistics on the number and ratio of incoming and outgoing edges (overall, per depth and per type) and node-centrality evaluation measures.\nTraining-based meta-features. The goal of these meta-features is to model the transformations undergone by the DNN during the course of its training. These meta-features consist of statistics on the weights, biases and activation function layers of the various components in the architecture. These meta-features can be partitioned into two groups:\n1. Static evaluation: general statistics on the distribution of the various values across different depths and layer types. These features provide “snapshot” information on the training status of the architecture in multiple training steps.\n2. Time series-based evaluation: We compare the values obtained in the various training iterations to those obtained earlier, calculate ratios and modeling the changes in values distribution over time.\nA full description of all meta-features is provided in Appendix A."
    }, {
      "heading" : "6 EXPERIMENTS AND ANALYSIS",
      "text" : ""
    }, {
      "heading" : "6.1 EXPERIMENTAL SETUP",
      "text" : "We conduct our experiments on 13 supervised classification datasets in a tabular form. We selected these datasets since they represent common supervised-learning problems that are not often addressed by deep learning. In addition, their feature composition consists of both numeric and discrete features, a trait that makes them different from image and speech classification datasets. The datasets vary significantly in size, number and type of features (some contain only numerical features while others also contain discrete features) and class imbalance - traits we hypothesize will make learning across domains more challenging. All datasets are available on the OpenML repository and their properties are represented in Appendix B.\nWe use the following settings:\n• For each dataset, we train the same set of 11,170 architectures, generated as described in Section 4. The maximal width (number of parallel layers) allowed for an architecture was set to 4, and we terminated the generation process upon reaching the predefined number of architectures. This deepest architectures generated by this approach have 8 activation layers and 14 components overall.\n• For architectures training, all datasets were randomly partitioned into training, validation and test sets. 80% of the data points was used by the training and the remaining two sets assigned 10% each. The same split was used for all the architectures explored for each dataset. Original class ratios were maintained in all sets.\n• All generated architectures were trained until convergence, with the time of termination determined by performance on the validation set.\n• The training-based meta-features were only extracted for the following steps: 20, 40, 60, 80 and 100.\n• We used a leave-one-out (LOO) cross-validation approach to train the ranking classifier: for each evaluated dataset di, we train the ranking classifier using the meta-features from dj ∈ D where i 6= j. This setting enables to test whether a meta-model trained on one dataset could be effectively applied on another.\n• We randomly split the generated architectures into two groups. The first group, consisting of 70% of the architectures, is used for training. We use the remaining 30% to evaluate the performance of our approach on each dataset."
    }, {
      "heading" : "6.2 ANALYSIS",
      "text" : "We begin by analyzing the accuracy distribution of the generated architectures across the datasets. We found that the distribution of accuracies varies significantly across the different datasets, with some datasets with ranges of [45%-90%] accuracy while others are in the range [89%-95%]. This difference has significant impact on one’s ability to apply architectures that are effective in one domain to another, as we confirm with the next experiment. An example of accuracies distributions is presented in figures 2 and 3 and plots for all datasets are presented in Appendix D.\nAnalyzing the performance differences of “parent–child” architectures. In order to determine whether our architecture generation method is effective, we analyzed the differences in accuracy between every architecture and its descendant. Our reason for performing this analysis is as follows: if making incremental additions to an existing architecture does not significantly change its performance, then we simply generate a large number of architecture which are nearly identical in performance.\nThe results of our analysis are presented in Table 1. For every ”parent–child“ pair we calculate the difference in accuracy on the test set. We then calculate the maximal and average changes in accuracy for each dataset. It is clear from the results that the changes in accuracy are significant, especially given the fact that changes are accumulated over time (deeper architectures are a result of multiple modifications).\nNext we analyze the “parent–child” architectures with the maximal differences in order to determine whether the addition of particular component is most likely to induce large changes in accuracy. Our results, presented in Table 2, show that no one component type can be consistently attributed with inducing large changes.\nApplying architectures across datasets. We attempt to determine whether it is possible to find architectures that perform well across multiple datasets. For each of the generated architectures, we calculate its performance-based ranking (i.e. position in a list ordered by the accuracy measure) on each of the datasets. Then, for each dataset we test the performance of the architecture with the best average ranking on the remaining datasets. We compare the performance of this architecture to that of the best evaluated architecture and to that of the best architecture found by our meta-learning model (described in the following section). The results, presented in Table 3, show significant differences in performance and lead us to conclude that in most cases DNN architectures do not perform well across multiple datasets.\nComparing the performance of DNN architectures to those of “conventional classifiers”. As a point of reference to “classical” machine learning approaches for classifying tabular data, in Table 3 we also presents the performance of the Random Forest algorithm (using the Weka Hall et al. (2009) implementation with the default parameters ). It is clear that neither Random Forest nor the DNN architectures consistently outperform the other. We intend to explore the factors that cause these differences in performance in future work.\nAnalyzing the performance of architectures with parallel layers. Next we explore whether architectures with parallel layers outperform similar non-parallel architectures. We analyze the 100 top-performing architectures of each dataset and calculate the percentage of architectures with parallel layers. The results, presented in Appendix C, show that this type of architecture consists on average of 62% of the top-performing architectures.\nTo determine whether the benefit of applying parallel layers is significant, we randomly choose one of our datasets (Ailerons) and identify the 100 top-performing architectures with parallel layers. From this set we randomly sample 10 architectures and compare the performance of each of them to those of all of their possible serial counterparts, created by iteratively removing all but one of the different parallel layers. Our results, presented in Table 4, show that architectures with parallel layers significantly outperform all of their serial counterparts.\nConsidering the same sample of parallel architectures, we analyze whether architectures performance can be improved by adding a batch normalization before, after or before and after each activation function. As shown by the results in Table 4, we did not find evidence that the addition of batch normalization improves the performance of architectures with parallel layers. We find this fact surprising and intend to explore this further in future work. An example of one of the parallel architectures is presented in Figure 4 in Appendix C.\nFinally, we also analyze the component composition of the 100 top-performing architectures for each dataset. The most interesting conclusion found in this analysis is the fact that a relatively shallow architectures ( 4 fully-connected layers) seem to yield the best performance on average for all datasets. The full analysis of the architecture components is presented in Table 12 in Appendix C."
    }, {
      "heading" : "6.3 EVALUATING THE META-LEARNING APPRAOCH",
      "text" : "We analyze the performance of our meta-learning model as a classifier to rank architectures based on their performance. For these experiments, we use the following settings:\n• We define the 5% of the top-performing architectures of each dataset as “good” and label the remaining as “bad”. We use this setting due to the large variance in the performance of the DNN architectures on the different datasets (see Appendix D for full details). We also intend to experiment with other labeling methods in future work.\n• We use the precision@X measure as the evaluation metric. We calculate it by ranking all architectures according with the confidence of the meta-classifier (i.e. the classifier trained on the meta-features) in them being “good”. Then, for the X top-ranking architectures we calculate the actual percentage of “good” architectures is X .\n• We conduct a separate evaluation on the training-based meta-features and the dataset-based and topological meta-features. Since the training-based features are more computationally expensive to compute, we find it interesting to compare their performance to the other types of meta-features. In our experiments we denote the full set as MLfull, the trainingbased meta-features as MLtrain and the topological and dataset-based meta-features as MLdata+top.\n• We use the Random Forest algorithm for the training of the meta-model.\nThe results of our evaluation are presented in Table 5. We show that we are able to identify multiple architectures in the top-ranking spots in a much higher ratio than their share of the population. It is also clear that the joint set of all meta-features outperforms both of the examined subsets.\nNext we conduct a random sampling over architectures, and compare the performance of the sampled architectures to those obtained by ranking all architectures using the proposed meta-classifier. Our goal is to determine the probability that N randomly-sampled architectures will consist of at least one architecture that outperforms all the top M items ranked by the meta-classifier. We conduct the experiment as follows: for each dataset, we randomly sample a fixed number of architectures and identify the one with the highest performance among those sampled. We then check if this\narchitecture outperforms all those in the ranked list provided by the meta-learning model. We repeat this process 50,000 for each dataset and calculate the probability of this scenario. The results, presented in Table 6, show that our model outperforms random sampling for all datasets, often by a large margin. However, further experimentation is required to fully determine the effectiveness of the meta-learning approach.\nFinally, we analyze the results in order to determine the effectiveness of the different meta-features used by our model. The analysis was carried out by running LASSO logistic regression and analyzing the weights assigned to the various meta-features. Based on this analysis we reach the following conclusions:\n• The dataset-based meta-features had the smallest contribution to the performance. While it is somewhat surprising given the fact that DNNs perform very differently on dataset with different characteristics, we conclude that the model is focused on the way in the architecture is trained on the data (i.e. weights and activations).\n• The topological meta-features that had the largest contribution were those modeling the depth of the network, the number of parallel layers and those counting the number of various components.\n• The ranking model uses a large number of training-based meta-features and from all types described in Appendix A. However, the model includes only weight and activation-based meta-features among the training-based meta-features. The biases-based meta-features are almost never used."
    }, {
      "heading" : "7 CONCLUSIONS AND FUTURE WORK",
      "text" : "In this study we have explored several aspects of applying DNNs to supervised classification problems. Our results demonstrate the difficulty in using DNN architectures that are effective in one domain to another. We also systematically compare the performance of architectures with parallel layers to those of similar linear architectures and demonstrate that the former outperforms the latter in many cases. We present a novel approach for predicting the performance of a DNN architecture by analyzing its topology and the changes in its weights, biases and activation function values during early phases of training. Our aim is that this work can lay the foundation for a better understanding of the DNN architectures space.\nFor future work we consider several directions. First, we plan to add additional components to the ones currently used in our automatic architecture generation method in order to enable further exploration. In addition, we will seek to enhance our approach adding automatic parameter tuning methods. This will enable us to efficiently explore multiple configurations and possibly identify higher-performing architectures. We are also considering the use of an exploration/exploitation\nscheme along the lines presented in Li et al. (2016) to enable us to efficiently explore larger architecture spaces.\nAnother approach we plan to explore is to make the search over network architectures a fullydifferentiable problem, by encoding the problem only using mechanisms that enable such a search. As an example, let us imagine that we want to decide the best number of internal hidden layers to use in a multi-layer fully-connected neural net. For this, we could create multiple parallel stacks of layers with the same input at the bottom (e.g. the features for each data point) and the same kind of output at the end (e.g. probabilities over the possible classes) and then use a softmax to take a weighted sum of the outputs from each of the parallel stacks. By using a penalty on the negative entropy of this weighted sum, and increasing the penalty over time, the network should learn to produce the output using only one of the parallel stacks which we can then use at inference time. We can also train multiple models simultaneously using this method, and introduce additional penalties to ensure that the multiple models explore different architectures during training, to enable a more diverse search."
    }, {
      "heading" : "A THE META-FEATURES USED BY OUR APPROACH",
      "text" : "We extract three types of meta-features for the training: dataset-based, topological and trainingbased. We now provide a complete description of the features used. Note that because calculate the meta-features in Table 9 three times: for the weights, biases and activation functions."
    }, {
      "heading" : "B FULL INFORMATION ON THE DATASETS USED IN THE EVALUATION",
      "text" : ""
    }, {
      "heading" : "C ANALYSIS OF THE PERFORMANCE PARALLEL LAYERS",
      "text" : "For each dataset, we analyze the 100 top-performing architectures and determine the percentage of architectures with parallel layers. The results, presented in Table 11, show that the percentage is significant. In table 12 we analyze the component composition of these architectures. The most interesting point (in our view) is that the number of fully-connected layers is about half of the possible maximum. We take this as an indication that the creation of very deep DNNs may not be required for tabular datasets of the type analyzed in this work. In Figure 4 we present an example of an architecture with parallel layers that was among the 100 top-performing on the Ailerons dataset.\nTable 12: The average number of component types per architecture for 100 top-performing architectures of each dataset.\nDataset Concat FC BatchnormDropout ReLU Sigmoid Tanh Softmax Contraceptive 0.53 3.54 2.57 0.91 1.71 0.51 0.43 1 Seismic bumps 0.47 3.53 1.75 1.62 1.68 0.64 0.48 1 Page Blocks 0.67 3.46 1.59 0.35 1.22 0.56 0.6 1 Wind 0.57 3.65 2.47 0.3 1.67 0.34 0.52 1 Puma 32 0.55 3.56 1.84 0.94 1.6 0.46 0.57 1 CPU act 0.65 3.95 3.23 0.2 1.91 0.3 0.63 1 Delta elevators 0.64 3.4 1.81 0.49 1.34 0.38 0.59 1 Mammography 0.69 3.6 3 0.22 1.63 0.36 0.49 1 Ailerons 0.62 3.68 2.34 0.69 1.58 0.39 0.55 1 Bank marketing 0.52 3.41 2.23 1.19 1.82 0.53 0.35 1 German Credit 0.5 3.66 2.39 1.1 2 0.42 0.31 1 Space 0.63 3.8 3.32 0.28 2.29 0.35 0.35 1 Cardiography 0.55 3.61 3.29 0.1 2.34 0.26 0.35 1\nInput\nBatchnorm\nFully Connected\nOutput\nReLU\nFully Connected\nSigmoid\nReLU\nConcat\nDropout\nFully Connected\nFigure 4: An example of an architecture with parallel layers."
    }, {
      "heading" : "D ACCURACY DISTRIBUTION OF THE GENERATED ARCHITECTURES ACROSS THE EVALUATED DATASETS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Learning to learn by gradient descent by gradient descent",
      "author" : [ "Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas" ],
      "venue" : "arXiv preprint arXiv:1606.04474,",
      "citeRegEx" : "Andrychowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andrychowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition",
      "author" : [ "George E Dahl", "Dong Yu", "Li Deng", "Alex Acero" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "Dahl et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dahl et al\\.",
      "year" : 2012
    }, {
      "title" : "Early stopping as nonparametric variational inference",
      "author" : [ "David Duvenaud", "Dougal Maclaurin", "Ryan P Adams" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Duvenaud et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duvenaud et al\\.",
      "year" : 2016
    }, {
      "title" : "The weka data mining software: an update",
      "author" : [ "Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten" ],
      "venue" : "ACM SIGKDD explorations newsletter,",
      "citeRegEx" : "Hall et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Non-stochastic best arm identification and hyperparameter optimization",
      "author" : [ "Kevin Jamieson", "Ameet Talwalkar" ],
      "venue" : "Preprint available a t,",
      "citeRegEx" : "Jamieson and Talwalkar.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jamieson and Talwalkar.",
      "year" : 2015
    }, {
      "title" : "What is the best multi-stage architecture for object recognition",
      "author" : [ "Kevin Jarrett", "Koray Kavukcuoglu", "Yann Lecun" ],
      "venue" : "IEEE 12th International Conference on Computer Vision,",
      "citeRegEx" : "Jarrett et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jarrett et al\\.",
      "year" : 2009
    }, {
      "title" : "Optimizing recurrent neural networks architectures under time constraints",
      "author" : [ "Junqi Jin", "Ziang Yan", "Kun Fu", "Nan Jiang", "Changshui Zhang" ],
      "venue" : "arXiv preprint arXiv:1608.07892,",
      "citeRegEx" : "Jin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient hyperparameter optimization and infinitely many armed bandits",
      "author" : [ "Lisha Li", "Kevin Jamieson", "Giulia DeSalvo", "Afshin Rostamizadeh", "Ameet Talwalkar" ],
      "venue" : "arXiv preprint arXiv:1603.06560,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradient-based hyperparameter optimization through reversible learning",
      "author" : [ "Dougal Maclaurin", "David Duvenaud", "Ryan P Adams" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Maclaurin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maclaurin et al\\.",
      "year" : 2015
    }, {
      "title" : "No more pesky learning rates",
      "author" : [ "Tom Schaul", "Sixin Zhang", "Yann LeCun" ],
      "venue" : "ICML (3),",
      "citeRegEx" : "Schaul et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2013
    }, {
      "title" : "Understanding and improving convolutional neural networks via concatenated rectified linear units",
      "author" : [ "Wenling Shang", "Kihyuk Sohn", "Diogo Almeida", "Honglak Lee" ],
      "venue" : "arXiv preprint arXiv:1603.05201,",
      "citeRegEx" : "Shang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "A perspective view and survey of meta-learning",
      "author" : [ "Ricardo Vilalta", "Youssef Drissi" ],
      "venue" : "Artificial Intelligence Review,",
      "citeRegEx" : "Vilalta and Drissi.,? \\Q2002\\E",
      "shortCiteRegEx" : "Vilalta and Drissi.",
      "year" : 2002
    }, {
      "title" : "Roles of pre-training and fine-tuning in context-dependent",
      "author" : [ "Dong Yu", "Li Deng", "George Dahl" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Recent advances in deep neural networks (DNNs) have led to breakthroughs in fields such as image classification (He et al., 2015; Krizhevsky et al., 2012) and speech recognition (Yu et al.",
      "startOffset" : 112,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "Recent advances in deep neural networks (DNNs) have led to breakthroughs in fields such as image classification (He et al., 2015; Krizhevsky et al., 2012) and speech recognition (Yu et al.",
      "startOffset" : 112,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : ", 2012) and speech recognition (Yu et al., 2010; Dahl et al., 2012).",
      "startOffset" : 31,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "While recent work such as (Simonyan & Zisserman, 2014) suggests that in many cases the depth of the architecture is crucial, the emergence of more complex architectures (He et al., 2015; Szegedy et al., 2015) demonstrates that depth alone often does not suffice.",
      "startOffset" : 169,
      "endOffset" : 208
    }, {
      "referenceID" : 14,
      "context" : "While recent work such as (Simonyan & Zisserman, 2014) suggests that in many cases the depth of the architecture is crucial, the emergence of more complex architectures (He et al., 2015; Szegedy et al., 2015) demonstrates that depth alone often does not suffice.",
      "startOffset" : 169,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "In Jarrett et al. (2009), the authors analyze convolutional neural networks (CNNs) and derive insights regarding the architecture design and the contribution of its different components.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "In Jarrett et al. (2009), the authors analyze convolutional neural networks (CNNs) and derive insights regarding the architecture design and the contribution of its different components. Another work aimed at better understanding CNNs is presented in Shang et al. (2016). The authors analyze widely used CNN architectures and derive insights into their possible shortcomings.",
      "startOffset" : 3,
      "endOffset" : 271
    }, {
      "referenceID" : 9,
      "context" : "In Maclaurin et al. (2015) the authors demonstrate how information extracted from the stochastic gradient descent can efficiently tune multiple parameters in the architecture.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "An additional work that analyzes the gradient is presented in Duvenaud et al. (2016), where the information is used to determine when to terminate the training of the architecture to avoid over-fitting.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "In Jin et al. (2016), the authors focus on setting the size of hidden layers in RNNs.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Another approach, in which one long-short term memory network (LSTM) is used to optimize another, was proposed by Andrychowicz et al. (2016). The two networks have shared parameters but separate hidden states and the optimizer network is both modifying its own weights and those of the optimized simultaneously.",
      "startOffset" : 114,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "Another approach, in which one long-short term memory network (LSTM) is used to optimize another, was proposed by Andrychowicz et al. (2016). The two networks have shared parameters but separate hidden states and the optimizer network is both modifying its own weights and those of the optimized simultaneously. Finally, an approach that automatically adjusts the learning rates of the neural net was presented in Schaul et al. (2013). The approach has been shown to be effective both on convex and non-convex learning tasks.",
      "startOffset" : 114,
      "endOffset" : 435
    }, {
      "referenceID" : 9,
      "context" : "Recent work by Li et al. (2016) proposes an exploration/exploitation scheme for hyperparameter tuning.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "We plan to address this limitation in future work, possibly by using an approach similar to the one presented in Li et al. (2016). It is also important to point out that we currently do not support weight-sharing and therefore do not consider CNN and RNN architectures.",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "As a point of reference to “classical” machine learning approaches for classifying tabular data, in Table 3 we also presents the performance of the Random Forest algorithm (using the Weka Hall et al. (2009) implementation with the default parameters ).",
      "startOffset" : 188,
      "endOffset" : 207
    }, {
      "referenceID" : 9,
      "context" : "scheme along the lines presented in Li et al. (2016) to enable us to efficiently explore larger architecture spaces.",
      "startOffset" : 36,
      "endOffset" : 53
    } ],
    "year" : 2016,
    "abstractText" : "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures.",
    "creator" : "LaTeX with hyperref package"
  }
}