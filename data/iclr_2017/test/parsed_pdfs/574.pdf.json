{
  "name" : "574.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "FASHION SEARCH", "Se-Yeoung Kim", "Sang-Il Na", "Ha-Yoon Kim", "Moon-Ki Kim", "Byoung-Ki Jeon", "Taewan Kim" ],
    "emails" : [ "seyeong@sk.com", "sang.il.na@sk.com", "hayoon@sk.com", "moonki@sk.com", "standard@sk.com", "{taey.16@navercorp.com}" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Online commerce has been a great impact on our life over the past decade. We focus on an online market for fashion related items1. Finding similar fashion-product images for a given image query is a classical problem in an application to computer vision, however, still challenging due to the absence of an absolute definition of the similarity between arbitrary fashion items.\nDeep learning technology has given great success in computer vision tasks such as efficient feature representation (Razavian et al., 2014; Babenko et al., 2014), classification (He et al., 2016a; Szegedy et al., 2016b), detection (Ren et al., 2015; Zhang et al., 2016), and segmentation (Long et al., 2015). Furthermore, image to caption generation (Vinyals et al., 2015; Xu et al., 2015) and visual question answering (VQA) (Antol et al., 2015) are emerging research fields combining vision, language (Mikolov et al., 2010), sequence to sequence (Sutskever et al., 2014), long-term memory (Xiong et al., 2016) based modelling technologies.\nThese computer vision researches mainly concern about general object recognition. However, in our fashion-product search domain, we need to build a very specialised model which can mimic human's perception of fashion-product similarity. To this end, we start by brainstorming about what makes two fashion items are similar or dissimilar. Fashion-specialist and merchandisers are also involved. We then compose fashion-attribute dataset for our fashion-product images. Table 1 explains a part of our fashion-attributes. Conventionally, each of the columns in Table 1 can be modelled as a multi-class classification. Therefore, our fashion-attributes naturally is modelled as a multi-label classification.\n∗This work was done by the author at SK Planet. 1In our e-commerce platform, 11st (http://english.11st.co.kr/html/en/main.html), al-\nmost a half of user-queries are related to the fashion styles, and clothes.\nMulti-label classification has a long history in the machine learning field. To address this problem, a straightforward idea is to split such multi-labels into a set of multi-class classification problems. In our fashion-attributes, there are more than 90 attributes. Consequently, we need to build more than 90 classifiers for each attribute. It is worth noting that, for example, collar attribute can represent the upper-garments, but it is absent to represent bottom-garments such as skirts or pants, which means some attributes are conditioned on other attributes. This is the reason that the learning tree structure of the attributes dependency can be more efficient (Zhang & Zhang, 2010; Fu et al., 2012; Gibaja & Ventura, 2015).\nRecently, recurrent neural networks (RNN) are very commonly used in automatic speech recognition (ASR) (Graves et al., 2013; Graves & Jaitly, 2014), language modelling (Mikolov et al., 2010), word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al., 2014), and dialog modelling (Henderson et al., 2014; Serban et al., 2016). To preserve long-term dependency in hidden context, Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and its variants (Zaremba et al., 2014; Cooijmans et al., 2016) are breakthroughs in such fields. We use this LSTM to learn fashion-attribute dependency structure implicitly. By using the LSTM, our attribute recognition problem is regarded to as a sequence classification. There is a similar work in Wang et al. (2016), however, we do not use the VGG16 network (Simonyan & Zisserman, 2014) as an image encoder but use our own encoder. To the best of our knowledge, it is the first work applying LSTM into a multi-label classification task in the commercial fashion-product search domain.\nThe remaining of this paper is organized as follows. In Sec. 2, We describe details about our fashion-attribute dataset. Sec. 3 describes the proposed fashion-product search system in detail. Sec. 4 explains empirical results given image queries. Finally, we draw our conclusion in Sec. 5."
    }, {
      "heading" : "2 BUILDING THE FASHION-ATTRIBUTE DATASET",
      "text" : "We start by building large-scale fashion-attribute dataset in the last year. We employ maximum 100 man-months and take almost one year for completion. There are 19 fashion-categories and more than 90 attributes for representing a specific fashion-style. For example, top garments have the Tshirts, blouse, bag etc. The T-shirts category has the collar, sleeve-length, gender, etc. The gender attribute has binary classes (i.e. female and male). Sleeve-length attribute has multiple classes (i.e. long, a half, sleeveless etc.). Theoretically, the combination of our attributes can represent thousands of unique fashion-styles. A part of our attributes are in Table 1. ROIs for each fashion item in an image are also included in this dataset. Finally, we collect 1 million images in total. This internal dataset is to be used for training our fashion-attribute recognition model and fashion-product ROI detector respectively."
    }, {
      "heading" : "3 FASHION-PRODUCT SEARCH SYSTEM",
      "text" : "In this section, we describe the details of our system. The whole pipeline is illustrated in Fig. 3. As a conventional information retrieval system, our system has offline and online phase. In offline process, we take both an image and its textual meta-information as the inputs. The reason we take additional textual meta-information is that, for example, in Fig. 1a dominant fashion item in the image is a white dress however, our merchandiser enrolled it to sell the brown cardigan as described\nin its meta-information. In Fig. 1b, there is no way of finding which fashion item is to be sold without referring the textual meta-information seller typed manually. Therefore, knowing intension (i.e. what to sell) for our merchandisers is very important in practice. To catch up with these intension, we extract fashion-category information from the textual meta. The extracted fashion-category information is fed to the fashion-attribute recognition model. The fashion-attribute recognition model predicts a set of fashion-attributes for the given image. (see Fig. 2) These fashion-attributes are used as keys in the inverted indexing scheme. On the next stage, our fashion-product ROI detector finds where the fashion-category item is in the image. (see Fig. 8) We extract colour and appearance features for the detected ROI. These visual features are stored in a postings list. In these processes, it is worth noting that, as shown in Fig. 8, our system can generate different results in the fashion-attribute recognition and the ROI detection for the same image by guiding the fashioncategory information. In online process, there is two options for processing a user-query. We can\ntake a guided information, what the user wants to find, or the fashion-attribute recognition model automatically finds what fashion-category item is the most likely to be queried. This is up to the user's choice. For the given image by the user, the fashion-attribute recognition model generates fashion-attributes, and the results are fed into the fashion-product ROI detector. We extract colour and appearance features in the ROI resulting from the detector. We access to the inverted index addressed by the generated a set of fashion-attributes, and then get a postings list for each fashionattribute. We perform nearest-neighbor retrieval in the postings lists so that the search complexity is reduced drastically while preserving the semantic similarity. To reduce memory capacity and speed up this nearest-neighbor retrieval process once more, our features are binarized and CPU depen-\ndent intrinsic instruction (i.e. assembly popcnt instruction2) is used for computing the hamming distance."
    }, {
      "heading" : "3.1 VISION ENCODER NETWORK",
      "text" : "We build our own vision encoder network (ResCeption) which is based on inception-v3 architecture (Szegedy et al., 2016b). To improve both speed of convergence and generalization, we introduce a shortcut path (He et al., 2016a;b) for each data-flow stream (except streams containing one convolutional layer at most) in all inception-v3 modules. Denote input of l-th layer , xl ∈ R , output of the l-th layer, xl+1, a l-th layer is a function, H : xl 7→ xl+1 and a loss function, L(θ;xL). Then forward and back(ward)propagation is derived such that\nxl+1 = H(xl) + xl (1) ∂xl+1\n∂xl = ∂H(xl) ∂xl + 1 (2)\nImposing gradients from the loss function to l-th layer to Eq. (2),\n∂L ∂xl := ∂L ∂xL . . . ∂xl+2 ∂xl+1 ∂xl+1 ∂xl\n= ∂L ∂xL\n( 1+ · · ·+ ∂H(x L−2)\n∂xl + ∂H(xL−1) ∂xl ) =\n∂L ∂xL\n( 1+ l∑ i=L−1 ∂H(xi) ∂xl ) . (3)\nAs in the Eq. (3), the error signal, ∂L ∂xL\n, goes down to the l-th layer directly through the shortcut path, and then the gradient signals from (L − 1)-th layer to l-th layer are added consecutively (i.e.∑l i=L−1 ∂H(xi) ∂xl\n). Consequently, all of terms in Eq. (3) are aggregated by the additive operation instead of the multiplicative operation except initial error from the loss (i.e. ∂L\n∂xL ). It prevents\nfrom vanishing or exploding gradient problem. Fig. 4 depicts network architecture for shortcut\n2http://www.gregbugaj.com/?tag=assembly (accessed at Aug. 2016)\npaths in an inception-v3 module. We use projection shortcuts throughout the original inception-v3 modules due to the dimension constraint.3 To demonstrate the effectiveness of the shortcut paths in the inception modules, we reproduce ILSVRC2012 classification benchmark (Russakovsky et al., 2015) for inception-v3 and our ResCeption network. As in Fig. 5a, we verify that residual shortcut paths are beneficial for fast training and slight better generalization.4 The whole of the training curve is shown in Fig. 5b. The best validation error is reached at 23.37% and 6.17% at top-1 and top-5, respectively. That is a competitive result.5 To demonstrate the representation power of our ResCeption, we employ the transfer learning strategy for applying the pre-trained ResCeption as an image encoder to generate captions. In this experiment, we verify our ResCeption encoder outperforms the existing VGG16 network6 on MS-COCO challenge benchmark (Chen et al., 2015). The best validation CIDEr-D score (Vedantam et al., 2015) for c5 is 0.923 (see Fig. 5c) and test CIDEr-D score for c40 is 0.937.7"
    }, {
      "heading" : "3.2 MULTI-LABEL LEARNING AS SEQUENCE PREDICTION BY USING THE RNN",
      "text" : "The traditional multi-class classification associates an instance x with a single label a from previously defined a finite set of labels A. The multi-label classification task associates several finite sets of labels An ⊂ A. The most well known method in the multi-label literature are the binary relevance method (BM) and the label combination method (CM). There are drawbacks in both BM\n3If the input and output dimension of the main-branch is not the same, projection shortcut should be used instead of identity shortcut.\n4This is almost the same finding from Szegedy et al. (2016a) but our work was done independently. 5http://image-net.org/challenges/LSVRC/2015/results 6https://github.com/torch/torch7/wiki/ModelZoo 7We submitted our final result with beam search on MS-COCO evaluation server and found out the beam\nsearch improves final CIDEr-D for c40 score by 0.02.\nand CM. The BM ignores label correlations that exist in the training data. The CM directly takes into account label correlations, however, a disadvantage is its worst-case time complexity (Read et al., 2009). To tackle these drawbacks, we introduce to use the RNN. Suppose we have random variables a ∈ An, An ⊂ A. The objective of the RNN is to maximise the joint probability, p(at, at−1, at−2, . . . a0), where t is a sequence (time) index. This joint probability is factorized as a product of conditional probabilities recursively,\np(at, at−1, . . . a0) = p(a0)p(a1|a0)︸ ︷︷ ︸ p(a0,a1) p(a2|a1, a0)\n︸ ︷︷ ︸ p(a0,a1,a2)\n· · ·\n︸ ︷︷ ︸ p(a0,a1,a2,... )\n(4)\n= p(a0) ∏T t=1 p(at|at−1, . . . , a0).\nFollowing the Eq. 4, we can handle multi-label classification as sequence classification which is illustrated in Fig. 6. There are many label dependencies among our fashion-attributes. Direct modelling of such label dependencies in the training data using the RNN is our key idea. We use the ResCeption as a vision encoder θI , LSTM and softmax regression as our sequence classifier θseq, and negative log-likelihood (NLL) as the loss function. We backpropagage gradient signal from the sequence classifier to vision encoder.8 Empirical results of our ResCeption-LSTM based attribute recognition are in Fig. 2. Many fashion-category dependent attributes such as sweetpants, fading, zipper-lock, mini, and tailored-collar are recognized quite well. Fashion-category independent attributes (e.g., male, female) are also recognizable. It is worth noting we do not model the fashionattribute dependance tree at all. We demonstrate the RNN learns attribute dependency structure implicitly. We evaluate our attribute recognition model on the fashion-attribute dataset. We split this dataset into 721544, 40000, and 40000 images for training, validating, and testing. We employ the early-stopping strategy to preventing over-fitting using the validation set. We measure precision and recall for a set of ground-truth attributes and a set of predicted attributes for each image. The quantitative results are in Table 2.\n8Our attribute recognition model is parameterized as θ = [θI ; θseq]. In our case, updating θI as well as θseq in the gradient descent step helps for much better performance.\n3.3 Guided ATTRIBUTE-SEQUENCE GENERATION\nOur prediction model of the fashion-attribute recognition is based on the sequence generation process in the RNN (Graves, 2013). The attribute-sequence generation process is illustrated in Fig. 7. First, we predict a probability of the first attribute for a given internal representation of the image i.e. pθseq(a0|gθI (I)), and then sample from the estimated probability of the attribute, a0 ∼ pθseq(a0|gθI (I)). The sampled symbol is fed to as the next input to compute pθseq(a1|a0, gθI (I)). This sequential process is repeated recursively until a sampled result is reached at the special endof-sequence (EOS) symbol. In case that we generate a set of attributes for a guided fashion-category, we do not sample from the previously estimated probability, but select the guided fashion-category, and then we feed into it as the next input deterministically. It is the key to considering for each seller's intention. Results for the guided attribute-sequence generation is shown in Fig. 8.\n3.4 Guided ROI DETECTION\nOur fashion-product ROI detection is based on the Faster R-CNN (Ren et al., 2015). In the conventional multi-class Faster R-CNN detection pipeline, one takes an image and outputs a tuple of (ROI coordinate, object-class, class-score). In our ROI detection pipeline, we take additional information, guided fashion-category from the ResCeption-LSTM based attribute-sequence generator. Our fashion-product ROI detector finds where the guided fashion-category item is in a given image. Jing et al. (2015) also uses a similar idea, but they train several detectors for each category independently so that their works do not scale well. We train a detector for all fashion-categories jointly. Our detector produces ROIs for all of the fashion-categories at once. In post-processing, we reject ROIs that their object-classes are not matched to the guided fashion-category. We demonstrate that the guided fashion-category information contributes to higher performance in terms of mean average precision (mAP) on the fashion-attribute dataset. We measure the mAP for the intersection-of-union (IoU) between ground-truth ROIs and predicted ROIs. (see Table 3) That is due to the fact that our guided fashion-category information reduces the false positive rate. In our fashion-product search pipeline, the colour and appearance features are extracted in the detected ROIs."
    }, {
      "heading" : "3.5 VISUAL FEATURE EXTRACTION",
      "text" : "To extract appearance feature for a given ROI, we use pre-trained GoogleNet (Szegedy et al., 2015). In this network, both inception4 and inception5 layer's activation maps are used. We evaluate this feature on two similar image retrieval benchmarks, i.e. Holidays (Jegou et al., 2008) and UK-benchmark (UKB) (Nistér & Stewénius, 2006). In this experiment, we do not use any postprocessing method or fine-tuning at all. The mAP on Holidays is 0.783, and the precision@4 and recall@4 on UKB is 0.907 and 0.908 respectively. These scores are competitive against several deep feature representation methods (Razavian et al., 2014; Babenko et al., 2014). Examples of queries and resulting nearest-neighbors are in Fig. 9. On the next step, we binarize this appearance feature by simply thresholding at 0. The reason we take this simple thresholding to generate the hash code is twofold. The neural activation feature map at a higher layer is a sparse and distributed code in nature. Furthermore, the bias term in a linear layer (e.g., convolutional layer) compensates for\naligning zero-centering of the output feature space weakly. Therefore, we believe that a code from a well-trained neural model, itself, can be a good feature even to be binarized. In our experiment, such simple thresholding degrades mAP by 0.02 on the Holidays dataset, but this method makes it possible to scaling up in the retrieval. In addition to the appearance feature, we extract colour feature using the simple (bins) colour histogram in HSV space, and distance between a query and a reference image is computed by using the weighted combination of the two distances from the colour and the appearance feature."
    }, {
      "heading" : "4 EMPIRICAL RESULTS",
      "text" : "To evaluate empirical results of the proposed fashion-product search system, we select 3 million fashion-product images in our e-commerce platform at random. These images are mutually exclusive to the fashion-attribute dataset. We have again selected images from the web used for the queries. All of the reference images pass through the offline process as described in Sec. 3, and resulting inverted indexing database is loaded into main-memory (RAM) by our daemon system. We send the pre-selected queries to the daemon system with the RESTful API. The daemon system then performs the online process and returns nearest-neighbor images correspond to the queries. In this scenario, there are three options to get similar fashion-product images. Option 1 is that the fashion-attribute recognition model automatically selects fashion-category, the most likely to be queried in the given image. Option 2 is that a user manually selects a fashion-category given a query image. (see Fig. 10) Option 3 is that a user draw a rectangle to be queried by hand like Jing et al. (2015). (see Fig. 11) By the recognized fashion-attributes, the retrieved results reflect the user's main needs, e.g. gender, season, utility as well as the fashion-style, that could be lacking when using visual feature representation only."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "Today's deep learning technology has given great impact on various research fields. Such a success story is about to be applied to many industries. Following this trend, we traced the start-of-the art computer vision and language modelling research and then, used these technologies to create value for our customers especially in the e-commerce platform. We expect active discussion on that how to apply many existing research works into the e-commerce industry."
    } ],
    "references" : [ {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In The International Conference on Computer Vision,",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural codes for image retrieval",
      "author" : [ "Artem Babenko", "Anton Slesarev", "Alexander Chigorin", "Victor S. Lempitsky" ],
      "venue" : "CoRR, abs/1404.1777,",
      "citeRegEx" : "Babenko et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Babenko et al\\.",
      "year" : 2014
    }, {
      "title" : "Microsoft COCO captions: Data collection and evaluation",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C. Lawrence Zitnick" ],
      "venue" : "server. CoRR,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1409.1259,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning tree structure of label dependency for multi-label learning",
      "author" : [ "Bin Fu", "Zhihai Wang", "Rong Pan", "Guandong Xu", "Peter Dolog" ],
      "venue" : "Advances in Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Fu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2012
    }, {
      "title" : "A tutorial on multilabel learning",
      "author" : [ "Eva Gibaja", "Sebastián Ventura" ],
      "venue" : "The ACM Computing Surveys,",
      "citeRegEx" : "Gibaja and Ventura.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gibaja and Ventura.",
      "year" : 2015
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "CoRR, abs/1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "In The International Conference on Machine Learning. JMLR Workshop and Conference Proceedings,",
      "citeRegEx" : "Graves and Jaitly.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves and Jaitly.",
      "year" : 2014
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-Rahman Mohamed", "Geoffrey E. Hinton" ],
      "venue" : "In The IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Word-based Dialog State Tracking with Recurrent Neural Networks",
      "author" : [ "M. Henderson", "B. Thomson", "S.J. Young" ],
      "venue" : "In The Annual SIGdial Meeting on Discourse and Dialogue,",
      "citeRegEx" : "Henderson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Hamming embedding and weak geometric consistency for large scale image search",
      "author" : [ "Herve Jegou", "Matthijs Douze", "Cordelia Schmid" ],
      "venue" : "In The European Conference on Computer Vision,",
      "citeRegEx" : "Jegou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jegou et al\\.",
      "year" : 2008
    }, {
      "title" : "Visual search at pinterest",
      "author" : [ "Yushi Jing", "David Liu", "Dmitry Kislyuk", "Andrew Zhai", "Jiajing Xu", "Jeff Donahue", "Sarah Tavel" ],
      "venue" : "In ACM International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Jing et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukás Burget", "Jan Cernocký", "Sanjeev Khudanpur" ],
      "venue" : "In The Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Dependency recurrent neural language models for sentence completion",
      "author" : [ "Piotr Mirowski", "Andreas Vlachos" ],
      "venue" : "CoRR, abs/1507.01193,",
      "citeRegEx" : "Mirowski and Vlachos.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mirowski and Vlachos.",
      "year" : 2015
    }, {
      "title" : "Scalable recognition with a vocabulary tree",
      "author" : [ "D. Nistér", "H. Stewénius" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Nistér and Stewénius.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nistér and Stewénius.",
      "year" : 2006
    }, {
      "title" : "Cnn features offthe-shelf: An astounding baseline for recognition",
      "author" : [ "Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "Razavian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Razavian et al\\.",
      "year" : 2014
    }, {
      "title" : "Classifier chains for multi-label classification",
      "author" : [ "Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank" ],
      "venue" : "In The European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Read et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Read et al\\.",
      "year" : 2009
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei" ],
      "venue" : "The International Journal of Computer Vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau" ],
      "venue" : "In The AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "author" : [ "Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke" ],
      "venue" : "In The International Conference on Learning Representation Workshop,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vedantam et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "CNN-RNN: A unified framework for multi-label image classification",
      "author" : [ "Jiang Wang", "Yi Yang", "Junhua Mao", "Zhiheng Huang", "Chang Huang", "Wei Xu" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : "In The International Conference on Machine Learning,",
      "citeRegEx" : "Xiong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio" ],
      "venue" : "In The International Conference on Machine Learning,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals" ],
      "venue" : "CoRR, abs/1409.2329,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Is faster R-CNN doing well for pedestrian detection",
      "author" : [ "Liliang Zhang", "Liang Lin", "Xiaodan Liang", "Kaiming He" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-label learning by exploiting label dependency",
      "author" : [ "Min-Ling Zhang", "Kun Zhang" ],
      "venue" : "In The ACM International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Zhang and Zhang.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and Zhang.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Deep learning technology has given great success in computer vision tasks such as efficient feature representation (Razavian et al., 2014; Babenko et al., 2014), classification (He et al.",
      "startOffset" : 115,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "Deep learning technology has given great success in computer vision tasks such as efficient feature representation (Razavian et al., 2014; Babenko et al., 2014), classification (He et al.",
      "startOffset" : 115,
      "endOffset" : 160
    }, {
      "referenceID" : 21,
      "context" : ", 2016b), detection (Ren et al., 2015; Zhang et al., 2016), and segmentation (Long et al.",
      "startOffset" : 20,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : ", 2016b), detection (Ren et al., 2015; Zhang et al., 2016), and segmentation (Long et al.",
      "startOffset" : 20,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : ", 2016), and segmentation (Long et al., 2015).",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : "Furthermore, image to caption generation (Vinyals et al., 2015; Xu et al., 2015) and visual question answering (VQA) (Antol et al.",
      "startOffset" : 41,
      "endOffset" : 80
    }, {
      "referenceID" : 33,
      "context" : "Furthermore, image to caption generation (Vinyals et al., 2015; Xu et al., 2015) and visual question answering (VQA) (Antol et al.",
      "startOffset" : 41,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and visual question answering (VQA) (Antol et al., 2015) are emerging research fields combining vision, language (Mikolov et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : ", 2015) are emerging research fields combining vision, language (Mikolov et al., 2010), sequence to sequence (Sutskever et al.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 25,
      "context" : ", 2010), sequence to sequence (Sutskever et al., 2014), long-term memory (Xiong et al.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 32,
      "context" : ", 2014), long-term memory (Xiong et al., 2016) based modelling technologies.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "This is the reason that the learning tree structure of the attributes dependency can be more efficient (Zhang & Zhang, 2010; Fu et al., 2012; Gibaja & Ventura, 2015).",
      "startOffset" : 103,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "Recently, recurrent neural networks (RNN) are very commonly used in automatic speech recognition (ASR) (Graves et al., 2013; Graves & Jaitly, 2014), language modelling (Mikolov et al.",
      "startOffset" : 103,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : ", 2013; Graves & Jaitly, 2014), language modelling (Mikolov et al., 2010), word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : ", 2010), word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al., 2014), and dialog modelling (Henderson et al.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : ", 2014), and dialog modelling (Henderson et al., 2014; Serban et al., 2016).",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : ", 2014), and dialog modelling (Henderson et al., 2014; Serban et al., 2016).",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "To preserve long-term dependency in hidden context, Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and its variants (Zaremba et al., 2014; Cooijmans et al., 2016) are breakthroughs in such fields.",
      "startOffset" : 132,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : ", 2010), word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al., 2014), and dialog modelling (Henderson et al., 2014; Serban et al., 2016). To preserve long-term dependency in hidden context, Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and its variants (Zaremba et al., 2014; Cooijmans et al., 2016) are breakthroughs in such fields. We use this LSTM to learn fashion-attribute dependency structure implicitly. By using the LSTM, our attribute recognition problem is regarded to as a sequence classification. There is a similar work in Wang et al. (2016), however, we do not use the VGG16 network (Simonyan & Zisserman, 2014) as an image encoder but use our own encoder.",
      "startOffset" : 82,
      "endOffset" : 603
    }, {
      "referenceID" : 22,
      "context" : "3 To demonstrate the effectiveness of the shortcut paths in the inception modules, we reproduce ILSVRC2012 classification benchmark (Russakovsky et al., 2015) for inception-v3 and our ResCeption network.",
      "startOffset" : 132,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "In this experiment, we verify our ResCeption encoder outperforms the existing VGG16 network6 on MS-COCO challenge benchmark (Chen et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "The best validation CIDEr-D score (Vedantam et al., 2015) for c5 is 0.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 26,
      "context" : "This is almost the same finding from Szegedy et al. (2016a) but our work was done independently.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "The CM directly takes into account label correlations, however, a disadvantage is its worst-case time complexity (Read et al., 2009).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "Our prediction model of the fashion-attribute recognition is based on the sequence generation process in the RNN (Graves, 2013).",
      "startOffset" : 113,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "Our fashion-product ROI detection is based on the Faster R-CNN (Ren et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "Jing et al. (2015) also uses a similar idea, but they train several detectors for each category independently so that their works do not scale well.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "To extract appearance feature for a given ROI, we use pre-trained GoogleNet (Szegedy et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "Holidays (Jegou et al., 2008) and UK-benchmark (UKB) (Nistér & Stewénius, 2006).",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 19,
      "context" : "These scores are competitive against several deep feature representation methods (Razavian et al., 2014; Babenko et al., 2014).",
      "startOffset" : 81,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "These scores are competitive against several deep feature representation methods (Razavian et al., 2014; Babenko et al., 2014).",
      "startOffset" : 81,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "10) Option 3 is that a user draw a rectangle to be queried by hand like Jing et al. (2015). (see Fig.",
      "startOffset" : 72,
      "endOffset" : 91
    } ],
    "year" : 2016,
    "abstractText" : "We build a large-scale visual search system which finds similar product images given a fashion item. Defining similarity among arbitrary fashion-products is still remains a challenging problem, even there is no exact ground-truth. To resolve this problem, we define more than 90 fashion-related attributes, and combination of these attributes can represent thousands of unique fashion-styles. We then introduce to use the recurrent neural networks (RNNs) recognising multiple fashion-attributes with the end-to-end manner. To build our system at scale, these fashion-attributes are again used to build an inverted indexing scheme. In addition to these fashion-attributes for semantic similarity, we extract colour and appearance features in a region-of-interest (ROI) of a fashion item for visual similarity. By sharing our approach, we expect active discussion on that how to apply current deep learning researches into the e-commerce industry.",
    "creator" : "LaTeX with hyperref package"
  }
}