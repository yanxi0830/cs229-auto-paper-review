{
  "name" : "678.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "READING COMPREHENSION", "Rudolf Kadlec", "Ondrej Bajgar", "Peter Hrincar", "Jan Kleindienst" ],
    "emails" : [ "kadlec@cz.ibm.com", "obajgar@cz.ibm.com", "phrincar@cz.ibm.com", "jankle@cz.ibm.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Machine intelligence has had some notable successes, however often in narrow domains which are sometimes of little practical use to humans – for instance games like chess (Campbell et al., 2002) or Go (Silver et al., 2016). If we aimed to build a general AI that would be able to efficiently assist humans in a wide range of settings, we would want it to have a much larger set of skills – among them would be an ability to understand human language, to perform common-sense reasoning and to be able to generalize its abilities to new situations like humans do.\nIf we want to achieve this goal through Machine Learning, we need data to learn from. A lot of data if the task at hand is complex – which is the case for many useful tasks. One way to achieve wide applicability would be to provide training data for each specific task we would like the machine to perform. However it is unrealistic to obtain a sufficient amount of training data for some domains – it may for instance require expensive human annotation or all domains of application may be difficult to predict in advance – while the amount of training data in other domains is practically unlimited, (e.g. in language modelling or Cloze-style question answering).\nThe way to bridge this gap – and to achieve the aforementioned adaptability – is transfer learning (Pan & Yang, 2010) and closely related semi-supervised learning (Zhu & Goldberg, 2009) which allow the system to acquire a set of skills on domains where data are abundant and then use these skills to succeed on previously unseen domains. Despite how important generalization is for general AI, a lot of research keeps focusing on solving narrow tasks.\nIn this paper we would like to examine transfer of learnt skills and knowledge within the domain of text comprehension, a field that has lately attracted a lot of attention within the NLP community (Hermann et al., 2015; Hill et al., 2015; Kobayashi et al., 2016; Kadlec et al., 2016b; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016; Weissenborn, 2016; Cui et al., 2016b;a;\n∗These authors contributed equally to this work.\nLi et al., 2016; Shen et al., 2016). Specifically, we would like to address the following research questions:\n1. Whether we could train models on natural-language tasks where data are abundant and transfer the learnt skills to tasks where in-domain training data may be difficult to obtain. We will first look into what reasoning abilities a model learns from two large-scale readingcomprehension datasets using artificial tasks, and then check whether it can transfer its skills to real world tasks. Spoiler: both these transfers are very poor if we allow no training at all on the target task.\n2. Whether pre-training on large-scale datasets does help if we allow the model to train on a small sample of examples from the target tasks. Here the results are much more positive.\n3. Finally we examine whether the benefits of pre-training are concentrated in any particular part of the model - namely the word-embedding part or the context encoder (the reasoning part). It turns out that pre-training is useful for both components.\nAlthough our results do not improve current state of the art in any of the studied tasks, they show a clear positive effect of large-dataset pre-training on the performance of our baseline machine-learning model. Previous studies of transfer learning and semi-supervised learning in NLP focused on text classification (Dai & Le, 2015; Mou et al., 2016) and various parsing tasks (Collobert et al., 2011; Hashimoto et al., 2016). To our knowledge this work is the first study of transfer learning in reading comprehension, and we hope it will stimulate further work in this important area.\nWe will first briefly introduce the datasets we will be using on the pre-training and target sides, then our baseline model and afterwards in turn describe the method and results of each of the three experiments."
    }, {
      "heading" : "2 DATASETS",
      "text" : ""
    }, {
      "heading" : "2.1 PRE-TRAINING DATASETS",
      "text" : "We have mentioned that for the model pre-training we would want to use a task where training data are abundant. An example of such task is context-dependent cloze-style-question answering since the training data for this task can be generated automatically from a suitable corpus. We will use two such pre-training datasets in our experiments: the BookTest (Bajgar et al., 2016) and the CNN/Daily Mail (CNN/DM) news dataset (Hermann et al., 2015).\nThe task associated with both datasets is to answer a cloze-style question (i.e. fill in a blank in a sentence) the answer to which needs to be inferred from a context document provided with the question."
    }, {
      "heading" : "2.1.1 BOOKTEST",
      "text" : "In the BookTest dataset, the context document is formed from 20 consecutive sentences from a book. The question is then formed by omitting a common noun or a named entity from the subsequent 21st sentence. Among datasets of this kind, the BookTest is among the largest with more than 14 million training examples coming from 3555 copyright-free books avalable thanks to Project Gutenberg."
    }, {
      "heading" : "2.1.2 CNN/DAILY MAIL",
      "text" : "In the CNN/DM dataset the context document is formed from a news article while the cloze-style question is formed by removing a named entity from one of the short summary sentences which often appear at the top of the article.\nTo stop the model from using world knowledge from outside the context article (and hence truly test the comprehension of the article), all named entities were replaced by anonymous tags, which are further shuffled for each example. This may make the comprehension more difficult; however, since the answer is always one of the anonymized entities, it also reduces the number of possible answers making guessing easier."
    }, {
      "heading" : "2.2 TARGET DATASETS",
      "text" : ""
    }, {
      "heading" : "2.2.1 BABI",
      "text" : "The first target dataset are the bAbI tasks (Weston et al., 2016) – a set of artificial tasks each of which is designed to test a specific kind of reasoning. This toy dataset will allow us to observe what particular skills the model may be learning from each of the three training datasets.\nFor our experiments we will be using an architecture designed to select one word from the context document as the answer. Hence we have selected Tasks 1,2,3,4,5,11,12,13,14 and 16 which fulfill this requirement and added task 15 which required a slight modification. Furthermore because both pre-training datasets are cloze-style we converted also the bAbI task questions into cloze style (e.g. ”Where is John?” to ”John is in the XXXXX.”).\nFor the models pre-trained on CNN/DM we also anonymized the tasks in a way similar to the pre-training dataset - i.e. we replaced all names of characters and also all words that can appear as answers for the given task by anonymous tags in the style of CNN/DM. This gives even models that have not seen any training examples from the target domain a chance to answer the questions.\nFull details about these alterations can be found in Appendix A."
    }, {
      "heading" : "2.2.2 SQUAD",
      "text" : "Secondly, we will look on transfer to the SQuAD dataset (Rajpurkar et al., 2016); here the associated task may be already useful in the real world. Although cloze-style questions have the huge advantage in the possibility of being automatically generated from a suitable corpus – the path taken by CNN/DM and the BookTest – in practice humans would use a proper question, not its cloze-style substitute. This brings us to the need of transfer from the data-rich cloze-style training to the domain of proper questions where data are much scarcer due to the necessary human annotation.\nThe SQuAD dataset is a great target dataset to use for this. As opposed to the bAbI tasks, the goal of this dataset is actually a problem whose solving would be useful to humans - answering natural questions based on an natural language encyclopedic knowledge base.\nFor our experiments we selected only a subset of the SQuAD training and development examples where the answer is only a single word, since this is an inherent assumption of our machine learning model. This way we extracted 28,346 training examples out of the original 100,000 examples and 3,233 development examples out of 10,570."
    }, {
      "heading" : "3 MACHINE LEARNING MODEL: AS READER",
      "text" : "We perform our experiments using the Attention Sum Reader (AS Reader) (Kadlec et al., 2016b) model. The AS Reader is simple to implement while it achieves strong performance on several text comprehension tasks (Kadlec et al., 2016b; Bajgar et al., 2016; Chu et al., 2016). Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.\nA high level structure of the AS Reader is shown in Figure 1. The words from the document and the question are first converted into vector embeddings using a look-up matrix. The document is then read by a bidirectional Gated Recurrent Unit (GRU) network (Cho et al., 2014). A concatenation of the hidden states of the forward and backward GRUs at each word is then used as a contextual embedding of this word, intuitively representing the context in which the word is appearing. We can also understand it as representing the set of questions to which this word may be an answer.\nSimilarly the question is read by a bidirectional GRU but in this case only the final hidden states are concatenated to form the question embedding.\nThe attention over each word in the context is then calculated as the dot product of its contextual embedding with the question embedding. This attention is then normalized by the softmax function and summed across all occurrences of each answer candidate. The candidate with most accumulated attention is selected as the final answer.\nFor a more detailed description of the model including equations check Kadlec et al. (2016b)."
    }, {
      "heading" : "4 EXPERIMENTS: TRANSFER LEARNING IN TEXT COMPREHENSION",
      "text" : "Now let us turn in more detail to the three kinds of experiments that we performed."
    }, {
      "heading" : "4.1 PRE-TRAINED WITHOUT TARGET ADJUSTMENT",
      "text" : "In the first experiment we tested how a model trained on one of the large-scale pre-training datasets performs on the bAbI tasks without any opportunity to train on bAbI. Since the BookTest and CNN/DM tasks involve only cloze-style questions, we can’t expect a model trained on them to answer natural ?-style questions. Hence we did not study the transfer to SQuAD in this case, only the transfer to the (cloze-converted) bAbI tasks."
    }, {
      "heading" : "4.1.1 METHOD",
      "text" : "First we tested how the AS Reader architecture (Kadlec et al., 2016b) can handle the tasks if trained directly on the bAbI training data for each task. Then we tested the degree of transfer from the BookTest and CNN/DM data to the 11 selected bAbI tasks.\nIn the first part of the experiment we trained a separate instance of the AS Reader on the 10,000- example version of the bAbI training data for each of the 11 tasks (for more details see Appendix B.1). On 8 of them the architecture was able to learn the task with accuracy at least 95% 1 (results for each task can be found in Table 4 in Appendix C). Hence if given appropriate training the AS Reader is capable of the reasoning needed to solve most of the selected bAbI tasks. Now when we know that the AS Reader is powerful enough to learn the target tasks we can turn to transfer from the two large-scale datasets.\nThe main part of this first experiment was then straightforward: we pre-trained multiple models on the BookTest and CNN/DM datasets and then simply evaluated them on the test datasets of the 11 selected bAbI tasks."
    }, {
      "heading" : "4.1.2 RESULTS",
      "text" : "Table 1 summarizes the results of this experiment. Both the models trained on the BookTest and those trained on the CNN/DM dataset perform quite poorly on bAbI and achieve much lower accuracy than\n1It should be noted that there are several machine learning models that perform better than the AS Reader in the 10k weakly supervised setting, e.g. (Sukhbaatar et al., 2015; Xiong et al., 2016; Graves et al., 2016), however they often need significant fine-tuning. On the other hand we trained plain AS Reader model without any modifications. Hyperparameter and feature fine-tuning could probably further increase its performance on individual tasks however it goes directly against the idea of generality that is at the heart of this work. For comparison with state of the art we include results of DMN+ (Xiong et al., 2016) in Table 1 which had the best average performance over the original 20 tasks.\nthe models trained directly on each individual bAbI task. However there is some transfer between the tasks since the AS Reader trained on either the BookTest or CNN/DM outperforms a random baseline2 and even an improved baseline which selects the most frequent word from the context that also appears as an answer in the training data for this task.\nThe results also show that the models trained on CNN/DM perform somewhat better on most tasks than the BookTest models. This may be due to the fact that bAbI tasks generally require the model to summarize information from the context document, which is also what the CNN/DM dataset is testing. On the other hand, the BookTest requires prediction of a possible continuation of a story, where the required kind of reasoning is much less clear but certainly different from pure summarization. Another explanation for better performance of CNN/DM models might be that they solve slightly simpler task since the candidate answers were already pre-selected in the entity anonymization step.\nReaders interested in how the training-dataset size affects this kind of transfer can check (Kadlec et al., 2016a) where we show that the target-task performance is a bit better if we use the large BookTest as opposed to its smaller subset, the Children’s Book Test (CBT) (Hill et al., 2015).\nConclusions from this experiment are that the skills learned from two large-scale datasets generalize surprisingly poorly to even simple toy tasks. This may make us ask whether most teams’ focus on solving narrow tasks is truly beneficial if the skills learnt on these tasks are hard to apply elsewhere. However it also brings us to our next experiment, where we try to provide some help to the struggling pre-trained models."
    }, {
      "heading" : "4.2 PRE-TRAINED WITH TARGET ADJUSTMENT",
      "text" : "After showing that the skills learnt from the BookTest and CNN/DM datasets are by themselves insufficient for solving the toy tasks, the next natural question is whether they are useful if helped by training on a small sample of examples from the target task. We call this additional phase of training target adjustment. For this experiment we again use the bAbI tasks, however we also test transfer to a subset of the SQuAD dataset, which is much closer to real-world natural-language question answering.\nThe results presented in this and the following section are based on training 3701 model instances."
    }, {
      "heading" : "4.2.1 METHOD",
      "text" : "Common to bAbI and SQuAD datasets. In this experiment we started with a pre-trained model which we used in the previous experiment. However, after it finished training on one of the large pre-training datasets, we allowed it to train on a subset of training examples from the target dataset. We tried subsets of various sizes ranging from a single example to thousands. We tried training four different pre-trained models and also, for comparison, four randomly-initialized models with the same hyperparameters (see Appendix B.2 for details). The experiment with each task-model couple was run on 4 different data samples of each size which were randomly drawn from the training dataset\n2The random baseline selects randomly uniformly between all unique words contained in the context document.\nof the task to account for variations between these random samples – which may be substantial given the small sample size.3\nbAbI. For each of these models we observed the test accuracy at the best-validation epoch and compared this number between the randomly initialized and pre-trained models. Validation was done using 100 examples which were set aside from the task’s original 10k training data.4 We perform the experiment with models pre-trained on the BookTest and also on CNN/DM.\nSQuAD subset. In the SQuAD experiment, we trained the model on a subset of the original training dataset where answers were only single words and its sub-subsets. We report the best-validation accuracy on a development set filtered in the same way. This experiment was performed only with the models pre-trained on BookTest."
    }, {
      "heading" : "4.2.2 RESULTS",
      "text" : "The results of these experiments are summarized in Figures 2 and 3.\n3We are planning to release the split training datasets soon. 4The other models trained on the full 10k dataset usually use 1000 validation examples (Sukhbaatar et al., 2015; Xiong et al., 2016), however we wanted to focus on low data regime thus we used 10 times less examples.\nbAbI. Sub-figure 2a shows mean test accuracy of the models that achieved the best validation result for each single task. The results for both BookTest and CNN/DM experiments confirm positive effect of pre-training compared to randomly initialized baseline. Figure 3 shows performance on selected bAbI tasks where pre-training has clearly positive effect, such plot for each of the target tasks is provided in Appendix C.2 (Figure 4).\nNote that the CNN/DM models cannot be directly compared to BookTest results due to entity anonymization that seems to simplify the task when the model is trained on smaller datasets.\nSince our evaluation methodology with different training set sizes is novel, we can compare our result only to MemN2N (Sukhbaatar et al., 2015) trained on a 1k dataset. MemN2N is the only weakly supervised model that reports accuracy when trained on less than 10k examples. MemN2N achieves average accuracy 93.2%5 on the eleven selected tasks. This is substantially better than both our random baseline (78.0%) and the BookTest-pre-trained model (79.5%), however our model is not tuned in any way towards this particular task. One important conceptual difference is that the AS Reader processes the whole context as one sequence of words, whereas MemN2N receives the context split into single sentences, which simplifies the task for the network.\nSQuAD subset. The results of SQuAD experiment also confirm positive effect of pre-training, see Sub-figure 2b, for now compare just lines showing performance of the fully pre-trained model and the randomly initialized model – the meaning of the remaining two lines shall become clear in the next section.\nMore detailed statistics about the results of this experiment can be found in Appendix D.\nWe should note that performance of our model is not competitive with the state of the art models on this dataset. For instance the DCR model (Yu et al., 2016) trained on our SQuAD subset achieves validation accuracy 74.9% in this task which is better than our randomly initialized (35.4%) and pre-trained (51.6%) models6. However, the DCR model is designed specifically for the SQuAD task, for instance it utilizes features that are not used by our model."
    }, {
      "heading" : "4.3 PARTIALLY PRE-TRAINED MODEL",
      "text" : "Since our previous experiment confirmed positive effect of pre-training if followed by target-domain adjustment, we wondered which part of the model contains the knowledge transferable to new domains. To examine this we performed the following experiment."
    }, {
      "heading" : "4.3.1 METHOD",
      "text" : "Our machine learning model, the AS Reader, consists of two main parts: the word-embedding look-up and the bidirectional GRUs used to encode the document and question (see Figure 1). Therefore a natural question was what the contribution of each of these parts is.\nTo test this we created two models out of each pre-trained model used in the previous experiment. The first model variant uses the pre-trained word embeddings from the original model while the GRU encoders are randomly initialized. We say that this model has pre-trained embeddings. The second model variant uses the opposite setting where the word embeddings are randomly initialized while the encoders are taken form a pre-trained model. We call this pre-trained encoders.\nbAbI. For this experiment we selected only a subset of tasks with training set of 100 examples where there was significant difference in accuracy between randomly-initialized and pre-trained models. For evaluation we use the same methodology as in the previous experiment, that is, we report accuracy of the best-validation model averaged over 4 training splits.\nSQuAD subset. We evaluated both model variants on all training sets from the previous SQuAD experiment using the same methodology.\n5MemN2N trained on each single task with PE LS RN features, see (Sukhbaatar et al., 2015) for details. 6We would like to thank Yu et al. (2016) for training their system on our dataset."
    }, {
      "heading" : "4.3.2 RESULTS",
      "text" : "bAbI. Table 2 shows improvement of pre-trained models over a randomly initialized baseline. In most cases (all except Task 5) the fully pre-trained model achieved the best accuracy.\nSQuAD subset. The accuracies of the four model variants are plotted in Figure 2b together with results of the previous SQuAD experiment. The graph shows that both pre-trained embeddings and pre-trained encoders alone improve performance over the randomly initialized baseline, however the fully pre-trained model is always the best.\nThe overall result of this experiment is that both pre-training of the word embeddings and pre-training of the encoder parameters are important since the fully pre-trained model outperforms both partially pre-trained variants."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "Our experiments show that transfer from two large cloze-style question-answering datasets to our two target tasks is suprisingly poor, if the models aren’t provided with any examples from the target domain. However we show that models that pre-trained models perform significantly better than a randomly initialized model if they are shown at least a few training examples from the target domain. The usefulness of pre-trained word embeddings is well known in the NLP community however we show that the power of our pre-trained model does not lie just in the embeddings. This suggests that once the text-comprehension community agrees on sufficiently versatile model, much larger parts of the model could start being reused than just the word-embeddings.\nThe generalization of skills from a training domain to new tasks is an important ingredient of any system we would want to call intelligent. This work is an early step to explore this direction."
    }, {
      "heading" : "A CLOZE STYLE BABI DATASET",
      "text" : "Since our AS Reader architecture is designed to select a single word from the context document as an answer (the task of CBT and BookTest), we selected 10 bAbI tasks that fulfill this requirement out of the original 20. These tasks are: 1. single supporting fact, 2. two supporting facts, 3. three supporting facts, 4. two argument relations, 5. three argument relations, 11. basic coreference, 12. conjunction, 13. compound coreference, 14. time reasoning and 16. basic induction.\nTask 15 needed a slight modification to satisfy this requirement: we converted the answers into plural (e.g. ”Q: What is Gertrude afraid of? A: wolf.” was converted into ”A: wolves” which also seems to be the more natural way to formulate the answer to such a question.).\nAlso since CBT and BookTest train the model for Cloze-style question answering, we modify the original bAbI dataset by reformulating the questions into Cloze-style. For example we translate a question ”Where is John ?” to ”John is in the XXXXX .”\nFor the models pre-trained on CNN/DM we also replace two kinds of words by anonymized tags (e.g. ”@entity56”) in a style similar to the pre-training dataset. Specifically we replace two (largely overlapping) categories of words:\n1. Proper names of story characters (e.g. John, Sandra) 2. Any word that can appear as an answer for the particular task (e.g. kitchen, garden if the\ntask is asking about locations)."
    }, {
      "heading" : "B METHOD DETAILS",
      "text" : "B.1 DIRECT TRAINING ON BABI – METHOD\nHere we give a more detailed description of the method we used to arrive to our results. We highlight only facts particular to this experiment. A more detailed general description of training the AS Reader is given in (Kadlec et al., 2016b).\nThe results given for AS Reader trained on bAbI are each for a single model with 64 hidden units in each direction of the GRU context encoder and embedding dimension 32 trained on the 10k training data provided with that particular task.\nThe results for AS Reader trained on the BookTest and the CNN/DM are for a greedy ensemble consisting of 4 models whose predictions were simply averaged. The models and ensemble were all validated on the validation set corresponding to the training dataset. The performance on the bAbI tasks oscillated notably during training however the ensemble averaging does somewhat mitigate this to get more representative numbers.\nB.2 HYPERPARAMETERS FOR THE TARGET-ADJUSTMENT EXPERIMENTS\nTable 3 lists hyperparameters of the pre-trained AS Reader instances used in our experiments with target adjustment."
    }, {
      "heading" : "C DETAILED RESULTS",
      "text" : "C.1 EXPERIMENTS WITHOUT TARGET ADJUSTMENT\nTable 4 shows detailed results for the experiments on models which were just pre-trained on one of the pre-training datasets without any target-adjustment. It also shows several baselines and results of a state-of-the-art model.\nC.2 TARGET-ADJUSTMENT EXPERIMENTS\nC.2.1 RESULTS FOR ALL BABI TASKS\nFigure 4 shows the test accuracies of all models that we trained in the target-adjustment experiments as well as lines joining the accuracies of the best-validation models.\nTa bl\ne 4:\nPe rf\nor m\nan ce\nof th\ne A\nS R\nea de\nr w\nhe n\ntr ai\nne d\non th\ne bA\nbI 10\nk, B\noo kT\nes ta\nnd C\nN N\n/D M\nda ta\nse ts\nan d\nth en\nev al\nua te\nd on\nbA bI\nte st\nda ta\n. T\nhe D\nyn am ic M em or y N et w or k (D M N +) is th e st at eof -t he -a rt m od el in a w ea kl y su pe rv is ed se tti ng on th e bA bI 10 k da ta se t. It s re su lts ar e ta ke n fr om (X io ng et al ., 20 16 ). M em N 2N (S uk hb aa ta re ta l., 20 15 )i s th e st at eof -t he -a rt m od el on th e 1k tr ai ni ng da ta se t; fo rc om pl et en es s w e al so in cl ud e its re su lts w ith th e 10 k tr ai ni ng .\nM od\nel :\nR an\ndo m\nR nd\nca nd\n. M\nem N 2N (s in gl e) (P E L S R\nN )\nM em\nN 2N\n(s in\ngl e)\n(P E\nL S\nLW R\nN )\nD M\nN +\n(s in\ngl e)\nA SR\nea de\nr\na a\na a\na a a\na a a\nTe st\nda ta\nse t Tr ai\nn da\nta se\nt no\nt tr ai ne\nd bA\nbI 10\nk bA\nbI 1k\nbA bI 10 k\nbA bI 10 k\nbA bI 10 k\nB oo\nkT es t 14\nM D\nM +C\nN N\n1. 2M\n1 Si\nng le\nsu pp\nor tin\ng fa\nct 7.\n80 31\n.2 0\n10 0.\n00 10\n0. 00\n10 0.\n00 10\n0. 00\n37 .3\n0 51\n.5 0\n2 Tw\no su\npp or\ntin g\nfa ct\ns 4.\n40 26\n.9 6\n91 .7\n0 99\n.7 0\n99 .7\n0 91\n.9 0\n25 .8\n0 28\n.9 0\n3 T\nhr ee\nsu pp\nor tin\ng fa\nct s\n3. 40\n19 .1\n4 59\n.7 0\n97 .9\n0 98\n.9 0\n86 .0\n0 22\n.2 0\n27 .4 0 4 Tw oar gu m en tr el at io ns 10 .5 0 33 .5 8 97 .2 0 10 0. 00 10 0. 00 10 0. 00 50 .3 0 54 .9 0 5 T hr ee -a rg um en tr el at io ns 4. 40 21 .4 2 86 .9 0 99 .2 0 99 .5 0 99 .8 0 67 .6 0 68 .1 0 11 B as ic co re fe re nc e 6. 20 30 .4 2 99 .1 0 99 .9 0 10 0. 00 10 0. 00 33 .0 0 20 .8 0 12 C on ju nc tio n 6. 70 27 .2 5 99 .8 0 10 0. 00 10 0. 00 10 0. 00 30 .4 0 37 .7 0 13 C om po un d co re fe re nc e 5. 60 27 .7 3 99 .6 0 10 0. 00 10 0. 00 10 0. 00 33 .8 0 14 .0 0 14 Ti m e re as on in g 5. 00 27 .8 2 98 .3 0 99 .9 0 99 .8 0 95 .0 0 27 .6 0 50 .5 0 15 B as ic de du ct io n 5. 20 37 .2 0 10 0. 00 10 0. 00 10 0. 00 96 .7 0 39 .9 0 17 .6 0 16 B as ic in du ct io n 7. 50 45 .6 5 98 .7 0 48 .2 0 54 .7 0 50 .3 0 15 .1 0 48 .0 0 bA bI m ea n (1 1 ta sk s) 6. 06 29 .8 5 93 .7 3 94 .9 8 95 .6 9 92 .7 0 34 .8 2 38 .1 3\nC.2.2 AVERAGE OVER ALL MODELS TRAINED ON BABI TASKS\nFigure 5 plots mean accuracy of all models trained in our experiments. This suggests that pre-training helped all models, not only the top performing ones selected by validation as already shown in Figure 2a."
    }, {
      "heading" : "D MEANS, STANDARD DEVIATIONS AND P-VALUES BY EXPERIMENT",
      "text" : "Table 5 shows the mean accuracy across all models trained for each combination of task, pre-training dataset and target-adjustment dataset size. Table 6 shows the corresponding standard deviations.\nTable 7 then shows the p-value that whether the expected accuracy of pre-trained models is greater than the expected accuracy of randomly initialized models. This shows that the pre-trained models are statistically significantly better for all target-adjustment set sizes on the SQuAD dataset. On bAbI the BookTest pre-trained models perform convincingly better especially for target-adjustment dataset sizes 100, 500 and 1000, with Task 16 being the main exception to this because the AS Reader struggles to learn it in any setting. For the CNN+DM pre-training the results are not conclusive.\nTa sk\nPr et\nra in\nin g\nTa rg\net -a\ndj us\ntm en\nts et\nsi ze\n0 1\n10 10\n0 50\n0 10\n00 50\n00 10\n00 0\n28 17 4 SQ uA D B oo kT es t 1. 01 e45 4. 07 e05 7. 40 e05 7. 82 e08 N A 5. 17 e08 N A 3. 93 e08 8.\n52 e03 Ta sk 1 B oo kT es t 3. 34 e83 1. 81 e03 1. 33 e01 2. 35 e19 9. 41 e04 1. 67 e02 1. 32 e01 N A N A Ta sk 2 B oo kT es t 1. 24 e34 3. 86 e07 7. 29 e03 2. 59 e01 1. 39 e08 2. 63 e06 7. 54 e09 2. 04 e01 N A Ta sk 3 B oo kT es t 9. 84 e55 1. 27 e05 7. 66 e03 1. 48 e03 3. 18 e04 2. 18 e03 2. 16 e04 1. 03 e01 N A Ta sk 4 B oo kT es t 7. 25 e78 9. 50 e01 9. 71 e01 1. 04 e05 6. 38 e03 1. 70 e02 1. 81 e02 N A N A Ta sk 5 B oo kT es t 6. 55 e11 5 9. 88 e22 8. 87 e19 5. 25 e05 3. 66 e03 8. 61 e02 5. 65 e03 N A N A Ta sk 11 B oo kT es t 6. 78 e15 2 1. 00 e+ 00 9. 94 e01 4. 07 e09 2. 50 e04 2. 28 e02 6. 37 e02 N A N A Ta sk 12 B oo kT es t 2. 27 e90 9. 10 e01 6. 46 e01 1. 89 e05 2. 78 e04 1. 43 e02 2. 36 e02 N A N A Ta sk 13 B oo kT es t 5. 30 e91 9. 75 e01 9. 99 e01 2. 88 e02 2. 74 e02 1. 03 e01 7. 06 e02 N A N A Ta sk 14 B oo kT es t 1. 97 e20 0 1. 01 e03 6. 79 e01 2. 22 e14 3. 40 e05 2. 93 e03 3. 66 e06 3. 97 e01 N A Ta sk 15 B oo kT es t 3. 64 e09 4. 75 e01 4. 12 e01 6. 70 e01 1. 68 e03 3. 70 e03 1. 03 e05 4. 54 e01 N A Ta sk 16 B oo kT es t 1. 81 e05 8. 28 e04 4. 38 e01 2. 72 e01 4. 89 e01 5. 71 e01 7. 40 e03 N A N A Ta sk 1 C N N +D M 9. 43 e09 2. 99 e01 1. 11 e01 1. 05 e01 9. 54 e02 1. 45 e01 3. 97 e03 N A N A Ta sk 2 C N N +D M 9. 38 e17 6. 93 e01 9. 02 e01 9. 15 e01 1. 05 e03 4. 20 e01 2. 64 e03 8. 49 e02 N A Ta sk 3 C N N +D M 2. 42 e16 4. 95 e02 6. 30 e01 1. 75 e01 2. 13 e03 6. 59 e04 4. 68 e02 1. 24 e01 N A Ta sk 4 C N N +D M 5. 84 e03 9. 70 e01 1. 37 e01 4. 83 e03 3. 33 e01 8. 84 e01 1. 08 e01 N A N A Ta sk 5 C N N +D M 1. 17 e10 7. 00 e03 7. 93 e04 5. 20 e01 9. 70 e01 5. 66 e01 1. 83 e01 N A N A Ta sk 11 C N N +D M 1. 00 e+ 00 9. 84 e01 9. 73 e01 2. 58 e01 7. 17 e01 1. 45 e01 6. 95 e01 N A N A Ta sk 12 C N N +D M 1. 93 e14 9. 32 e01 9. 92 e01 2. 57 e02 4. 06 e01 6. 65 e02 2. 09 e01 N A N A Ta sk 13 C N N +D M 8. 69 e02 9. 61 e01 9. 72 e01 9. 89 e01 6. 22 e01 9. 44 e01 2. 83 e01 N A N A Ta sk 14 C N N +D M 2. 17 e12 6. 64 e02 1. 11 e01 2. 05 e02 3. 66 e02 4. 52 e01 9. 10 e01 8. 24 e01 N A Ta sk 15 C N N +D M 1. 36 e52 5. 30 e03 3. 48 e02 7. 21 e02 8. 36 e01 3. 09 e01 8. 47 e01 9. 84 e01 N A Ta sk 16 C N N +D M 6. 39 e35 4. 56 e02 9. 66 e01 5. 95 e01 7. 19 e01 4. 09 e02 2. 51 e02 2. 22 e03 N A\nTa bl\ne 7:\nO ne\n-s id\ned p-\nva lu\ne w\nhe th\ner th\ne m\nea n\nac cu\nra cy\nof pr\netr\nai ne\nd m\nod el\ns is\ngr ea\nte rt\nha n\nth e\nac cu\nra cy\nof th\ne ra\nnd om\nly in\niti al\niz ed\non es\nfo re\nac h\nco m\nbi na\ntio n of ta sk pr etr ai ni ng da ta se t. pva lu es be lo w 0. 05 ar e m ar ke d in gr ee n."
    } ],
    "references" : [ {
      "title" : "Embracing data abundance: BookTest Dataset for Reading Comprehension",
      "author" : [ "Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst" ],
      "venue" : "arXiv preprint arXiv:1610.00956,",
      "citeRegEx" : "Bajgar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bajgar et al\\.",
      "year" : 2016
    }, {
      "title" : "A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning" ],
      "venue" : "In Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Broad Context Language Modeling as Reading Comprehension",
      "author" : [ "Zewei Chu", "Hai Wang", "Kevin Gimpel", "David Mcallester" ],
      "venue" : null,
      "citeRegEx" : "Chu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural Language Processing ( Almost ) from Scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "Journal ofMachine Learning Research",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Attention-overAttention Neural Networks for Reading Comprehension",
      "author" : [ "Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu" ],
      "venue" : "2016a. URL http://arxiv.org/ abs/1607.04423",
      "citeRegEx" : "Cui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2016
    }, {
      "title" : "Consensus Attention-based Neural Networks for Chinese Reading Comprehension",
      "author" : [ "Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu" ],
      "venue" : null,
      "citeRegEx" : "Cui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2016
    }, {
      "title" : "Semi-supervised Sequence Learning",
      "author" : [ "Andrew M. Dai", "Quoc V. Le" ],
      "venue" : "ISSN 10495258",
      "citeRegEx" : "Dai and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "Gated-Attention Readers for Text Comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Dhingra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Hybrid Computing Using a Neural Network with Dynamic External Memory. Nature, 2016",
      "author" : [ "Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwińska", "Sergio Gómez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adrià Puigdomènech Badia", "Karl Moritz Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis" ],
      "venue" : null,
      "citeRegEx" : "Graves et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2016
    }, {
      "title" : "A JOINT MANYTASK MODEL: GROWING A NEURAL NETWORK FOR MULTIPLE NLP TASKS",
      "author" : [ "Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Hashimoto et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2017
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1511.02301,",
      "citeRegEx" : "Hill et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "From Particular to General : A Preliminary Case Study of Transfer Learning in Reading Comprehension",
      "author" : [ "Rudolf Kadlec", "Ondrej Bajgar", "Jan Kleindienst" ],
      "venue" : "MAIN Workshop at NIPS,",
      "citeRegEx" : "Kadlec et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural Text Understanding with Attention Sum Reader",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Ondej Bajgar", "Jan Kleindienst" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "Kadlec et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic Entity Representation with Max-pooling Improves Machine Reading",
      "author" : [ "Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui" ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT),",
      "citeRegEx" : "Kobayashi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2016
    }, {
      "title" : "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering",
      "author" : [ "Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean" ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR 2013),",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "How Transferable are Neural Networks in NLP Applications",
      "author" : [ "Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin" ],
      "venue" : null,
      "citeRegEx" : "Mou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Reasoning with Memory Augmented Neural Networks for Language Comprehension",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu" ],
      "venue" : null,
      "citeRegEx" : "Munkhdalai and Yu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2016
    }, {
      "title" : "A Survey on Transfer Learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Pan and Yang.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : "URL http://arxiv.org/abs/1606.05250",
      "citeRegEx" : "Rajpurkar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "ReasoNet: Learning to Stop Reading in Machine Comprehension",
      "author" : [ "Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Iterative Alternating Neural Attention for Machine Reading",
      "author" : [ "Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Sordoni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2016
    }, {
      "title" : "End-To-End Memory Networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "pp. 1–11,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural Language Comprehension with the EpiReader",
      "author" : [ "Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman" ],
      "venue" : null,
      "citeRegEx" : "Trischler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Separating Answers from Queries for Neural Reading Comprehension",
      "author" : [ "Dirk Weissenborn" ],
      "venue" : null,
      "citeRegEx" : "Weissenborn.,? \\Q2016\\E",
      "shortCiteRegEx" : "Weissenborn.",
      "year" : 2016
    }, {
      "title" : "Towards AI-complete Question Answering: A Set of Prerequisite",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart Van Merri", "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "Toy Tasks",
      "citeRegEx" : "Weston et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic Memory Networks for Visual and Textual Question Answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Xiong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking",
      "author" : [ "Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to semi-supervised learning",
      "author" : [ "Xiaojin Zhu", "Andrew B Goldberg" ],
      "venue" : "Synthesis lectures on artificial intelligence and machine learning,",
      "citeRegEx" : "Zhu and Goldberg.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu and Goldberg.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : ", 2002) or Go (Silver et al., 2016).",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "Previous studies of transfer learning and semi-supervised learning in NLP focused on text classification (Dai & Le, 2015; Mou et al., 2016) and various parsing tasks (Collobert et al.",
      "startOffset" : 105,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : ", 2016) and various parsing tasks (Collobert et al., 2011; Hashimoto et al., 2016).",
      "startOffset" : 34,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "We will use two such pre-training datasets in our experiments: the BookTest (Bajgar et al., 2016) and the CNN/Daily Mail (CNN/DM) news dataset (Hermann et al.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : ", 2016) and the CNN/Daily Mail (CNN/DM) news dataset (Hermann et al., 2015).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "1 BABI The first target dataset are the bAbI tasks (Weston et al., 2016) – a set of artificial tasks each of which is designed to test a specific kind of reasoning.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "2 SQUAD Secondly, we will look on transfer to the SQuAD dataset (Rajpurkar et al., 2016); here the associated task may be already useful in the real world.",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "The AS Reader is simple to implement while it achieves strong performance on several text comprehension tasks (Kadlec et al., 2016b; Bajgar et al., 2016; Chu et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "The AS Reader is simple to implement while it achieves strong performance on several text comprehension tasks (Kadlec et al., 2016b; Bajgar et al., 2016; Chu et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 171
    }, {
      "referenceID" : 25,
      "context" : "Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.",
      "startOffset" : 81,
      "endOffset" : 212
    }, {
      "referenceID" : 23,
      "context" : "Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.",
      "startOffset" : 81,
      "endOffset" : 212
    }, {
      "referenceID" : 7,
      "context" : "Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.",
      "startOffset" : 81,
      "endOffset" : 212
    }, {
      "referenceID" : 21,
      "context" : "Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.",
      "startOffset" : 81,
      "endOffset" : 212
    }, {
      "referenceID" : 12,
      "context" : "Under review as a conference paper at ICLR 2017 For a more detailed description of the model including equations check Kadlec et al. (2016b). Question encoder Document encoder Document Question P Obama question,document",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "(Sukhbaatar et al., 2015; Xiong et al., 2016; Graves et al., 2016), however they often need significant fine-tuning.",
      "startOffset" : 0,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "(Sukhbaatar et al., 2015; Xiong et al., 2016; Graves et al., 2016), however they often need significant fine-tuning.",
      "startOffset" : 0,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "(Sukhbaatar et al., 2015; Xiong et al., 2016; Graves et al., 2016), however they often need significant fine-tuning.",
      "startOffset" : 0,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "For comparison with state of the art we include results of DMN+ (Xiong et al., 2016) in Table 1 which had the best average performance over the original 20 tasks.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 28,
      "context" : "The following three columns show performance of the AS Reader trained on different datasets, the last column shows the results of DMN+ (Xiong et al., 2016), the state-of-the-art-model on the bAbI 10k dataset.",
      "startOffset" : 135,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : ", 2016a) where we show that the target-task performance is a bit better if we use the large BookTest as opposed to its smaller subset, the Children’s Book Test (CBT) (Hill et al., 2015).",
      "startOffset" : 166,
      "endOffset" : 185
    }, {
      "referenceID" : 24,
      "context" : "The other models trained on the full 10k dataset usually use 1000 validation examples (Sukhbaatar et al., 2015; Xiong et al., 2016), however we wanted to focus on low data regime thus we used 10 times less examples.",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 28,
      "context" : "The other models trained on the full 10k dataset usually use 1000 validation examples (Sukhbaatar et al., 2015; Xiong et al., 2016), however we wanted to focus on low data regime thus we used 10 times less examples.",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "Since our evaluation methodology with different training set sizes is novel, we can compare our result only to MemN2N (Sukhbaatar et al., 2015) trained on a 1k dataset.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "For instance the DCR model (Yu et al., 2016) trained on our SQuAD subset achieves validation accuracy 74.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "MemN2N trained on each single task with PE LS RN features, see (Sukhbaatar et al., 2015) for details.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "Since our evaluation methodology with different training set sizes is novel, we can compare our result only to MemN2N (Sukhbaatar et al., 2015) trained on a 1k dataset. MemN2N is the only weakly supervised model that reports accuracy when trained on less than 10k examples. MemN2N achieves average accuracy 93.2%5 on the eleven selected tasks. This is substantially better than both our random baseline (78.0%) and the BookTest-pre-trained model (79.5%), however our model is not tuned in any way towards this particular task. One important conceptual difference is that the AS Reader processes the whole context as one sequence of words, whereas MemN2N receives the context split into single sentences, which simplifies the task for the network. SQuAD subset. The results of SQuAD experiment also confirm positive effect of pre-training, see Sub-figure 2b, for now compare just lines showing performance of the fully pre-trained model and the randomly initialized model – the meaning of the remaining two lines shall become clear in the next section. More detailed statistics about the results of this experiment can be found in Appendix D. We should note that performance of our model is not competitive with the state of the art models on this dataset. For instance the DCR model (Yu et al., 2016) trained on our SQuAD subset achieves validation accuracy 74.9% in this task which is better than our randomly initialized (35.4%) and pre-trained (51.6%) models6. However, the DCR model is designed specifically for the SQuAD task, for instance it utilizes features that are not used by our model. 4.3 PARTIALLY PRE-TRAINED MODEL Since our previous experiment confirmed positive effect of pre-training if followed by target-domain adjustment, we wondered which part of the model contains the knowledge transferable to new domains. To examine this we performed the following experiment. 4.3.1 METHOD Our machine learning model, the AS Reader, consists of two main parts: the word-embedding look-up and the bidirectional GRUs used to encode the document and question (see Figure 1). Therefore a natural question was what the contribution of each of these parts is. To test this we created two models out of each pre-trained model used in the previous experiment. The first model variant uses the pre-trained word embeddings from the original model while the GRU encoders are randomly initialized. We say that this model has pre-trained embeddings. The second model variant uses the opposite setting where the word embeddings are randomly initialized while the encoders are taken form a pre-trained model. We call this pre-trained encoders. bAbI. For this experiment we selected only a subset of tasks with training set of 100 examples where there was significant difference in accuracy between randomly-initialized and pre-trained models. For evaluation we use the same methodology as in the previous experiment, that is, we report accuracy of the best-validation model averaged over 4 training splits. SQuAD subset. We evaluated both model variants on all training sets from the previous SQuAD experiment using the same methodology. MemN2N trained on each single task with PE LS RN features, see (Sukhbaatar et al., 2015) for details. We would like to thank Yu et al. (2016) for training their system on our dataset.",
      "startOffset" : 119,
      "endOffset" : 3274
    }, {
      "referenceID" : 16,
      "context" : "The last line shows results for models initialized with Google News word2vec word embeddings (Mikolov et al., 2013).",
      "startOffset" : 93,
      "endOffset" : 115
    } ],
    "year" : 2017,
    "abstractText" : "Deep learning has proven useful on many NLP tasks including reading comprehension. However, it requires large amounts of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed to test specific reasoning abilities, and of SQuAD, a question-answering dataset which is much closer to real-world applications. Our experiments show very limited transfer if the model is not shown any training examples from the target domain however the results are encouraging if the model is shown at least a few target-domain examples. Furthermore we show that the effect of pre-training is not limited to word embeddings.",
    "creator" : "LaTeX with hyperref package"
  }
}