{
  "name" : "444.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "W. James Murdoch" ],
    "emails" : [ "jmurdoch@berkeley.edu", "aszlam@fb.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Neural network language models, especially recurrent neural networks (RNN), are now standard tools for natural language processing. Amongst other things, they are used for translation Sutskever et al. (2014), language modelling Jozefowicz et al. (2016), and question answering Hewlett et al. (2016). In particular, the Long Short Term Memory (LSTM) Hochreiter & Schmidhuber (1997) architecture has become a basic building block of neural NLP. Although LSTM’s are regularly used in state of the art systems, their operation is not well understood. Besides the basic desire from a scientific viewpoint to clarify their workings, it is often the case that it is important to understand why a machine learning algorithm made a particular choice. Moreover, LSTM’s are computationally intensive compared to discrete models with lookup tables and pattern matching.\nIn this work, we describe a novel method for visualizing the importance of specific inputs for determining the output of an LSTM. We then demonstrate that, by searching for phrases which are consistently important, the importance scores can be used to extract simple phrase patterns consisting of one to five words from a trained LSTM. The phrase extraction is first done in a general document classification framework on two different sentiment analysis datasets. We then demonstrate that it can also be specialized to more complex models by applying it to WikiMovies, a recently introduced question answer dataset. To concretely validate the extracted patterns, we use them as input to a rules-based classifier which approximates the performance of the original LSTM."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "There are two lines of related work on visualizing LSTMs. First, Hendrik et al. (2016) and Karpathy et al. (2016) analyse the movement of the raw gate activations over a sequence. Karpathy et al. (2016) is able to identify co-ordinates of ct that correspond to semantically meaningful attributes such as whether the text is in quotes and how far along the sentence a word is. However, most of the cell co-ordinates are harder to interpret, and in particular, it is often not obvious from their activations which inputs are important for specific outputs.\n∗Work started during an internship at Facebook AI Research\nAnother approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting.\nA recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al. (2016) Miller et al. (2016) has focused on neural network techniques for extracting answers directly from documents. Previous work had focused on Knowledge Bases (KBs), and techniques to map questions to logical forms suitable for querying them. Although they are effective within their domain, KBs are inevitably incomplete, and are thus an unsatisfactory solution to the general problem of question-answering. Wikipedia, in contrast, has enough information to answer a far broader array of questions, but is not as easy to query. Originally introduced in Miller et al. (2016), the WikiMovies dataset consists of questions about movies paired with Wikipedia articles."
    }, {
      "heading" : "3 WORD IMPORTANCE SCORES IN LSTMS",
      "text" : "We present a novel decomposition of the output of an LSTM into a product of factors, where each term in the product can be interpreted as the contribution of a particular word. Thus, we can assign importance scores to words according to their contribution to the LSTM’s prediction"
    }, {
      "heading" : "3.1 LONG SHORT TERM MEMORY NETWORKS",
      "text" : "Over the past few years, LSTMs have become an important part of neural NLP systems. Given a sequence of word embeddings x1, ..., xT ∈ Rd, an LSTM processes one word at a time, keeping track of cell and state vectors (c1, h1), ..., (cT , hT ) which contain information in the sentence up to word i. ht and ct are computed as a function of xt, ct−1 using the below updates\nft = σ(Wfxt + Vfht−1 + bf ) (1) it = σ(Wixt + Viht−1 + bi) (2) ot = σ(Woxt + Voht−1 + bo) (3) c̃t = tanh(Wcxt + Vcht−1 + bc) (4) ct = ftct−1 + itc̃t (5) ht = ot tanh(ct) (6)\nAs initial values, we define c0 = h0 = 0. After processing the full sequence, a probability distribution over C classes is specified by p, with\npi = SoftMax(WhT ) = eWihT∑C j=1 e Wjht (7)\nwhere Wi is the i’th row of the matrix W"
    }, {
      "heading" : "3.2 DECOMPOSING THE OUTPUT OF A LSTM",
      "text" : "We now show that we can decompose the numerator of pi in Equation 7 into a product of factors, and interpret those factors as the contribution of individual words to the predicted probability of class i. Define βi,j = exp (Wi(oT (tanh(cj)− tanh(cj−1))) , (8) so that\nexp(WihT ) = exp  T∑ j=1 Wi(oT (tanh(cj)− tanh(cj−1))  = T∏ j=1 βi,j .\nAs tanh(cj)− tanh(cj−1) can be viewed as the update resulting from word j, so βi,j can be interpreted as the multiplicative contribution to pi by word j."
    }, {
      "heading" : "3.3 AN ADDITIVE DECOMPOSITION OF THE LSTM CELL",
      "text" : "We will show below that the βi,j capture some notion of the importance of a word to the LSTM’s output. However, these terms fail to account for how the information contributed by word j is affected by the LSTM’s forget gates between words j and T . Consequently, we empirically found that the importance scores from this approach often yield a considerable amount of false positives. A more nuanced approach is obtained by considering the additive decomposition of cT in equation (9), where each term ej can be interpreted as the contribution to the cell state cT by word j. By iterating the equation ct = ftct−1 + itc̃t, we get that\ncT = T∑ i=1 ( T∏ j=i+1 fj)iic̃i = T∑ i=1 ei,T (9)\nThis suggests a natural definition of an alternative score to the βi,j , corresponding to augmenting the cj terms with products of forget gates to reflect the upstream changes made to cj after initially processing word j.\nexp(WihT ) = T∏ j=1 exp\n( Wi(oT (tanh(\nj∑ k=1 ek,T )− tanh( j−1∑ k=1 ek,T )))\n) (10)\n= T∏ j=1 exp Wi(oT (tanh(( t∏ k=j+1 fk)cj)− tanh(( t∏ k=j fk)cj−1)))  (11) =\nT∏ j=1 γi,j (12)"
    }, {
      "heading" : "4 PHRASE EXTRACTION FOR DOCUMENT CLASSIFICATION",
      "text" : "We now introduce a technique for using our variable importance scores to extract phrases from a trained LSTM. To do so, we search for phrases which consistently provide a large contribution to the prediction of a particular class relative to other classes. The utility of these patterns is validated by using them as input for a rules based classifier. For simplicity, we focus on the binary classification case."
    }, {
      "heading" : "4.1 PHRASE EXTRACTION",
      "text" : "A phrase can be reasonably described as predictive if, whenever it occurs, it causes a document to both be labelled as a particular class, and not be labelled as any other. As our importance scores introduced above correspond to the contribution of particular words to class predictions, they can be used to score potential patterns by looking at a pattern’s average contribution to the prediction of a given class relative to other classes. More precisely, given a collection of D documents {{xi,j}Ndi=1}Dj=1, for a given phrase w1, ..., wk we can compute scores S1, S2 for classes 1 and 2, as well as a combined score S and class C as\nS1(w1, ..., wk) = Averagej,b\n{∏k l=1 β1,b+l,j |xb+i,j = wi, i = 1, ..., k } Averagej,b {∏k l=1 β2,b+l,j |xb+i,j = wi, i = 1, ..., k\n} (13) S2(w1, .., wk) = 1\nS1(w1, ..., wk) (14)\nS(w1, ..., wk) = max i (Si(w1, ..., wk)) (15)\nC(w1, ..., wk) = argmaxi(Si(w1, ..., wk)) (16)\nwhere βi,j,k denotes βi,j applied to document k.\nThe numerator of S1 denotes the average contribution of the phrase to the prediction of class 1 across all occurrences of the phrase. The denominator denotes the same statistic, but for class 2. Thus, if\nS1 is high, then w1, ..., wk is a strong signal for class 1, and likewise for S2. We propose to use S as a score function in order to search for high scoring, representative, phrases which provide insight into the trained LSTM, and C to denote the class corresponding to a phrase.\nIn practice, the number of phrases is too large to feasibly compute the score of them all. Thus, we approximate a brute force search through a two step procedure. First, we construct a list of candidate phrases by searching for strings of consecutive words j with importance scores βi,j > c for any i and some threshold c; in the experiments below we use c = 1.1. Then, we score and rank the set of candidate phrases, which is much smaller than the set of all phrases."
    }, {
      "heading" : "4.2 RULES BASED CLASSIFIER",
      "text" : "The extracted patterns from Section 4.1 can be used to construct a simple, rules-based classifier which approximates the output of the original LSTM. Given a document and a list of patterns sorted by descending score given by S, the classifier sequentially searches for each pattern within the document using simple string matching. Once it finds a pattern, the classifier returns the associated class given by C, ignoring the lower ranked patterns. The resulting classifier is interpretable, and despite its simplicity, retains much of the accuracy of the LSTM used to build it."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We now present the results of our experiments."
    }, {
      "heading" : "5.1 TRAINING DETAILS",
      "text" : "We implemented all models in Torch using default hyperparameters for weight initializations. For WikiMovies, all documents and questions were pre-processed so that multiple word entities were concatenated into a single word. For a given question, relevant articles were found by first extracting from the question the rarest entity, then returning a list of Wikipedia articles containing any of those words. We use the pre-defined splits into train, validation and test sets, containing 96k, 10k and 10k questions, respectively. The word and hidden representations of the LSTM were both set to dimension 200 for WikiMovies, 300 and 512 for Yelp, and 300 and 150 for Stanford Sentiment Treebank. All models were optimized using Adam Kingma & Ba (2015) with the default learning rate of 0.001 using early stopping on the validation set. For rule extraction using gradient scores, the product in the reward function is replaced by a sum. In both datasets, we found that normalizing the gradient scores by the largest gradient improved results."
    }, {
      "heading" : "5.2 SENTIMENT ANALYSIS",
      "text" : "We first applied the document classification framework to two different sentiment analysis datasets. Originally introduced in Zhang et al. (2015), the Yelp review polarity dataset was obtained from the Yelp Dataset Challenge and has train and test sets of size 560,000 and 38,000. The task is binary prediction for whether the review is positive (four or five stars) or negative (one or two stars). The reviews are relatively long, with an average length of 160.1 words. We also used the binary classification task from the Stanford Sentiment Treebank (SST) Socher et al. (2013), which has less data with train/dev/test sizes of 6920/872/1821, and is done at a sentence level, so has much shorter document lengths.\nWe report results in Table 1 for seven different models. We report state of the art results from prior work using convolutional neural networks; Kim (2014) for SST and Zhang et al. (2015) for Yelp. We also report our LSTM baselines, which are competitive with state of the art, along with the three different pattern matching models described above. For SST, we also report prior results using bag of words features with Naive Bayes.\nThe additive cell decomposition pattern equals or outperforms the cell-difference patterns, which handily beat the gradient results. This coincides with our empirical observations regarding the information contained within the importance measures, and validates our introduced measure. The differences between measures become more pronounced in Yelp, as the longer document sizes provide more opportunities for false positives.\nAlthough our pattern matching algorithms underperform other methods, we emphasize that pure performance is not our goal, nor would we expect more from such a simple model. Rather, the fact that our method provides reasonable accuracy is one piece of evidence, in addition to the qualitative evidence given later, that our word importance scores and extracted patterns contain useful information for understanding the actions of a LSTM."
    }, {
      "heading" : "5.3 WIKIMOVIES",
      "text" : "Although document classification comprises a sizeable portion of current research in natural language processing, much recent work focuses on more complex problems and models. In this section, we examine WikiMovies, a recently introduced question answer dataset, and show that with some simple modifications our approach can be adapted to this problem."
    }, {
      "heading" : "5.3.1 DATASET",
      "text" : "WikiMovies is a dataset consisting of more than 100,000 questions about movies, paired with relevant Wikipedia articles. It was constructed using the pre-existing dataset MovieLens, paired with templates extracted from the SimpleQuestions dataset Bordes et al. (2015), a open-domain question answering dataset based on Freebase. They then selected a set of Wikipedia articles about movies by identifying a set of movies from OMDb that had an associated article by title match, and kept the title and first section for each article.\nFor a given question, the task is to read through the relevant articles and extract the answer, which is contained somewhere within the text. The dataset also provides a list of 43k entities containing all possible answers."
    }, {
      "heading" : "5.3.2 LSTMS FOR WIKIMOVIES",
      "text" : "We propose a simplified version of recent work Li et al. (2016). Given a pair of question xq1, ..., x q N and document xd1, ..., x d T , we first compute an embedding for the question using a LSTM. Then, for each word t in the document, we augment the word embedding xt with the computed question embedding. This is equivalent to adding an additional term which is linear in the question embedding into the gate equations 3-6, allowing the patterns an LSTM absorbs to be directly conditioned upon the question at hand.\nhqt = LSTM(x q t ) (17)\nht = LSTM(xdt ‖h q N ) (18)\nHaving run the above model over the document while conditioning on a question, we are given contextual representations h1, ..., hT of the words in the document. For each entity t in the document\nwe use pt to conduct a binary prediction for whether or not the entity is the answer. At test time, we return the entity with the highest probability as the answer.\npt = SoftMax(Wht) (19)"
    }, {
      "heading" : "5.3.3 PHRASE EXTRACTION",
      "text" : "We now introduce some simple modifications that were useful in adapting our pattern extraction framework to this specific task. First, in order to define the set of classifications problems to search over, we treat each entity t within each document as a separate binary classification task with corresponding predictor pt. Given this set of classification problems, rather than search over the space of all possible phrases, we restrict ourselves to those ending at the entity in question. We also distinguish patterns starting at the beginning of the document with those that do not and introduce an entity character into our pattern vocabulary, which can be matched by any entity. Template examples can be seen below, in Table 4. Once we have extracted a list of patterns, in the rules-based classifier we only search for positive examples, and return as the answer the entity matched to the highest ranked positive pattern."
    }, {
      "heading" : "5.3.4 RESULTS",
      "text" : "We report results on six different models in Tables 2 and 3. We show the results from Miller et al. (2016), which fit a key-value memory network (KV-MemNN) on representations from information extraction (IE) and raw text (Doc). Next, we report the results of the LSTM described in Section 5.3.2. Finally, we show the results of using three variants of the pattern matching algorithm described in Section 5.3.3: using patterns extracted using the additive decomposition (cell decomposition), difference in cells approaches (cell-difference) and gradient importance scores (gradient), as discussed in Section 2. Performance is reported using the accuracy of the top hit over all possible answers (all entities), i.e. the hits@1 metric.\nAs shown in Table 2, our LSTM model surpasses the prior state of the art by nearly 4%. Moreover, our automatic pattern matching model approximates the LSTM with less than 6% error, which is surprisingly small for such a simple model, and falls within 2% of the prior state of the art. Similarly to sentiment analysis, we observe a clear ordering of the results across question categories, with our cell decomposition scores providing the best performance, followed by the cell difference and gradient scores."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : ""
    }, {
      "heading" : "6.1 LEARNED PATTERNS",
      "text" : "We present extracted patterns for both sentiment tasks, and some WikiMovies question categories in Table 4. These patterns are qualitatively sensible, providing further validation of our approach. The increased size of the Yelp dataset allowed for longer phrases to be extracted relative to SST.\nCategory Top Patterns Yelp Polarity Positive definitely come back again., love love love this\nplace, great food and great service., highly recommended!, will definitely be coming back, overall great experience, love everything about, hidden gem.\nYelp Polarity Negative worst customer service ever, horrible horrible horrible, won’t be back, disappointed in this place, never go back there, not worth the money, not recommend this place SST Positive riveting documentary, is a real charmer, funny and touching, well worth your time, journey of the heart, emotional wallop, pleasure to watch, the whole family, cast is uniformly superb, comes from the heart, best films of the year, surprisingly funny, deeply satisfying SST Negative pretentious mess ..., plain bad, worst film of the year, disappointingly generic, fart jokes, banal dialogue, poorly executed, waste of time, a weak script, dullard, how bad it is, platitudes, never catches fire, tries too hard to be, bad acting, untalented artistes, derivative horror film, lackluster WikiMovies movie to writer film adaptation of Charles Dickens’, film adapted from ENT, by journalist ENT, written by ENT WikiMovies movie to actor western film starring ENT, starring Ben Affleck, . The movie stars ENT, that stars ENT WikiMovies movie to language is a 2014 french, icelandic, finnish, russian, danish, bengali, dutch, original german, zulu,czech, estonian, mandarin, filipino, hungarian\nTable 4: Selected top patterns using cell decomposition scores, ENT denotes an entity placeholder"
    }, {
      "heading" : "6.2 APPROXIMATION ERROR BETWEEN LSTM AND PATTERN MATCHING",
      "text" : "Although our approach is able to extract sensible patterns and achieve reasonable performance, there is still an approximation gap between our algorithm and the LSTM. In Table 5 we present some examples of instances where the LSTM was able to correctly classify a sentence, and our algorithm was not, along with the pattern used by our algorithm. At first glance, the extracted patterns are sensible, as ”gets the job done” or ”witty dialogue” are phrases you’d expect to see in a positive review of a movie. However, when placed in the broader context of these particular reviews, they cease to be predictive. This demonstrates that, although our work is useful as a firstorder approximation, there are still additional relationships that an LSTM is able to learn from data."
    }, {
      "heading" : "6.3 COMPARISON BETWEEN WORD IMPORTANCE MEASURES",
      "text" : "While the prediction accuracy of our rules-based classifier provides quantitative validation of the relative merits of our visualizations, the qualitative differences are also insightful. In Table 6, we provide a side-by-side comparison between the different measures. As discussed before, the difference in cells technique fails to account for how the updates resulting from word j are affected by the LSTM’s forget gates between when the word is initially processed and the answer. Consequently, we empirically found that without the interluding forget gates to dampen cell movements, the variable importance scores were far noisier than in additive cell decomposition approach. Under the additive cell decomposition, it identifies the phrase ’it stars’, as well as the actor’s name Aqib Khan as being important, a sensible conclusion. Moreover, the vast majority of words are labelled with an importance score of 1, corresponding to irrelevant. On the other hand, the difference in cells approach yields widely changing importance scores, which are challenging to interpret. In terms of noise, the gradient measures seem to lie somewhere in the middle. These patterns are broadly consistent with what we have observed, and provide qualitative validation of our metrics."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "In this paper, we introduced a novel method for visualizing the importance of specific inputs in determining the output of an LSTM. By searching for phrases which consistently provide large contributions, we are able to distill trained, state of the art, LSTMs into an ordered set of representative phrases. We quantitatively validate the extracted phrases through their performance in a simple, rules-based classifier. Results are shown in a general document classification framework, then specialized to a more complex, recently introduced, question answer dataset. Our introduced measures provide superior predictive ability and cleaner visualizations relative to prior work. We believe that this represents an exciting new paradigm for analysing the behaviour of LSTM’s.\nAdditive cell decomposition Difference in cell values Gradient\nwest is west is a 2010 british comedy - drama film , which is a sequel to the 1999 comedy ”\neast is east ” . it stars aqib khan\nwest is west is a 2010 british comedy -\ndrama film , which is a sequel to the 1999 comedy ” east is east ” . it starsaqib khan\nwest is west is a 2010 british comedy - drama film , which is a sequel to the 1999 comedy ” east is east ”. itstars aqib khan\nTable 6: Comparison of importance scores acquired by three different approaches, conditioning on the question ”the film west is west starred which actors?”. Bigger and darker means more important."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "This research was partially funded by Air Force grant FA9550-14-1-0016. It was also supported by the Center for Science of Information (CSoI), an US NSF Science and Technology Center, under grant agreement CCF-0939370."
    }, {
      "heading" : "8 APPENDIX - HEAT MAPS",
      "text" : "We provide an example heat map using the cell decomposition metric for each class in both sentiment analysis datasets, and selected WikiMovie question categories\nDataset Category Heat Map\nYelp Polarity Positive we went here twice for breakfast . had the bananas foster waffles with fresh whipped cream , they were amazing ! ! perfect seat out side on the terrace\nYelp Polarity Negative call me spoiled ...this sushi is gross and the orange chicken , well it was so thin i don ’t think it had chicken in it. gosomewhereelse\nStanford Sentiment Positive Whether or not you ’re enlightened by any of Derrida ’s lectures on “ the other ” and “ the self ,\n” Derrida is an undeniablyfascinatingandplayfulfellow\nStanford Sentiment Negative ... begins with promise , but runs aground after being snared in its own tangled plot\nPattern Question Heat Map\nMovie to Year What was the release year of another 48 hours?\nanother 48 hrs is a 1990\nMovie to Writer Which person wrote the movie last of the dogmen? last of the dogmen is a 1995 western adventure film written and directed by tab murphy\nMovie to Actor Who acted in the movie thunderbolt? thunderbolt ( ) ( ” piklik foh ” ) is a 1995 hong kong action filmstarring jackie chan\nMovie to Director Who directed bloody bloody bible camp? bloody bloody bible cam p is a 2012 american horror - comedy /s platter film . the film\nwas directed by vito trabucco\nMovie to Genre What genre is trespass in? trespassisa 1992 action Movie to Votes How would people rate the pool? though filmed in hindi , a language smith didn ’t know , the film earned\ngood∗ Movie to Rating How popular was les miserables? les mis rables is a 1935 american drama film starring fredric march and charles laughton\nbased upon thefamous Movie to Tags Describe rough magic?\nrough magic is a 1995 comedy film directed by clare peploe and starring bridget fonda , russell crowe\nMovie to Language What is the main language in fate?\nfate ( ) is a 2001 turkish"
    } ],
    "references" : [ {
      "title" : "Automatic text scoring using neural networks",
      "author" : [ "Dimitrios Alikaniotis", "Helen Yannakoudakis", "Marek Rei" ],
      "venue" : "In Assocation for Computational Lingustics,",
      "citeRegEx" : "Alikaniotis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alikaniotis et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask the gru: Multi-task learning for deep text recommendations",
      "author" : [ "Trapit Bansal", "David Belanger", "Andrew McCallum" ],
      "venue" : "In ACM international conference on Recommender Systems (RecSys),",
      "citeRegEx" : "Bansal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2016
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1506.02075,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Extraction of salient sentences from labelled documents",
      "author" : [ "Misha Denil", "Alban Demiraj", "Nando de Freitas" ],
      "venue" : "In arXiv preprint: https://arxiv.org/abs/1412.6815,",
      "citeRegEx" : "Denil et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual analysis of hidden state dynamics in recurrent neural networks",
      "author" : [ "Strobelt Hendrik", "Sebastian Gehrmann", "Bernd Huber", "Hanspeter Pfister", "Alexander M. Rush" ],
      "venue" : "In arXiv,",
      "citeRegEx" : "Hendrik et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hendrik et al\\.",
      "year" : 2016
    }, {
      "title" : "Wikireading: A novel large-scale language understanding task over wikipedia",
      "author" : [ "Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Hewlett et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hewlett et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : "arXiv preprint arXiv:1602.02410,",
      "citeRegEx" : "Jozefowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualizing and understanding recurrent neural networks",
      "author" : [ "Andrej Karpathy", "Justin Johnson", "Fei-Fei Li" ],
      "venue" : "In ICLR Workshop,",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "arXiv preprint arXiv:1408.5882,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederick P. Kingma", "Jimmy Lei Ba" ],
      "venue" : "In International Conference for Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering",
      "author" : [ "Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu" ],
      "venue" : "In arXiv,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Key-value memory networks for directly reading documents",
      "author" : [ "Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston" ],
      "venue" : "In Empirical Methods for Natural Language Processing,",
      "citeRegEx" : "Miller et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2016
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : "In Empirical Methods for Natural Language Processing,",
      "citeRegEx" : "Rajpurkar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts Potts" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc Le" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Amongst other things, they are used for translation Sutskever et al. (2014), language modelling Jozefowicz et al.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "(2014), language modelling Jozefowicz et al. (2016), and question answering Hewlett et al.",
      "startOffset" : 27,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "(2016), and question answering Hewlett et al. (2016). In particular, the Long Short Term Memory (LSTM) Hochreiter & Schmidhuber (1997) architecture has become a basic building block of neural NLP.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "(2016), and question answering Hewlett et al. (2016). In particular, the Long Short Term Memory (LSTM) Hochreiter & Schmidhuber (1997) architecture has become a basic building block of neural NLP.",
      "startOffset" : 31,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "First, Hendrik et al. (2016) and Karpathy et al.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "First, Hendrik et al. (2016) and Karpathy et al. (2016) analyse the movement of the raw gate activations over a sequence.",
      "startOffset" : 7,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "First, Hendrik et al. (2016) and Karpathy et al. (2016) analyse the movement of the raw gate activations over a sequence. Karpathy et al. (2016) is able to identify co-ordinates of ct that correspond to semantically meaningful attributes such as whether the text is in quotes and how far along the sentence a word is.",
      "startOffset" : 7,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al.",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al.",
      "startOffset" : 52,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word.",
      "startOffset" : 52,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting. A recent line of work Li et al. (2016) Hewlett et al.",
      "startOffset" : 52,
      "endOffset" : 703
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting. A recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al.",
      "startOffset" : 52,
      "endOffset" : 725
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting. A recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al. (2016) Miller et al.",
      "startOffset" : 52,
      "endOffset" : 749
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting. A recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al. (2016) Miller et al. (2016) has focused on neural network techniques for extracting answers directly from documents.",
      "startOffset" : 52,
      "endOffset" : 770
    }, {
      "referenceID" : 0,
      "context" : "Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting. A recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al. (2016) Miller et al. (2016) has focused on neural network techniques for extracting answers directly from documents. Previous work had focused on Knowledge Bases (KBs), and techniques to map questions to logical forms suitable for querying them. Although they are effective within their domain, KBs are inevitably incomplete, and are thus an unsatisfactory solution to the general problem of question-answering. Wikipedia, in contrast, has enough information to answer a far broader array of questions, but is not as easy to query. Originally introduced in Miller et al. (2016), the WikiMovies dataset consists of questions about movies paired with Wikipedia articles.",
      "startOffset" : 52,
      "endOffset" : 1320
    }, {
      "referenceID" : 14,
      "context" : "Originally introduced in Zhang et al. (2015), the Yelp review polarity dataset was obtained from the Yelp Dataset Challenge and has train and test sets of size 560,000 and 38,000.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "We also used the binary classification task from the Stanford Sentiment Treebank (SST) Socher et al. (2013), which has less data with train/dev/test sizes of 6920/872/1821, and is done at a sentence level, so has much shorter document lengths.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "We report state of the art results from prior work using convolutional neural networks; Kim (2014) for SST and Zhang et al.",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "We report state of the art results from prior work using convolutional neural networks; Kim (2014) for SST and Zhang et al. (2015) for Yelp.",
      "startOffset" : 88,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Model Yelp Polarity Stanford Sentiment Treebank Large word2vec CNN Zhang et al. (2015) 95.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "CNN-multichannel Kim (2014) 88.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "Naive Bayes Socher et al. (2013) 82.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "It was constructed using the pre-existing dataset MovieLens, paired with templates extracted from the SimpleQuestions dataset Bordes et al. (2015), a open-domain question answering dataset based on Freebase.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "2 LSTMS FOR WIKIMOVIES We propose a simplified version of recent work Li et al. (2016). Given a pair of question xq1, .",
      "startOffset" : 70,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "We show the results from Miller et al. (2016), which fit a key-value memory network (KV-MemNN) on representations from information extraction (IE) and raw text (Doc).",
      "startOffset" : 25,
      "endOffset" : 46
    } ],
    "year" : 2017,
    "abstractText" : "Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
    "creator" : "LaTeX with hyperref package"
  }
}