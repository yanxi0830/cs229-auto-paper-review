{
  "name" : "498.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION",
    "authors" : [ "Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng" ],
    "emails" : [ "yingkaig}@cs.cmu.edu", "yaoliang}@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep neural networks (DNNs, e.g., LeCun et al., 2015; Schmidhuber, 2015), if trained properly, have been demonstrated to significantly improve the benchmark performances in a wide range of application domains. As neural networks go deeper and deeper, naturally, its model complexity also increases quickly, hence the pressing need to reduce overfitting in training DNNs. A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness. In a nutshell, dropout randomly “drops” neural units during training as a means to prevent feature co-adaptation—a sign of overfitting (Hinton et al., 2012). Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012; Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective.\nIn their pioneering work, Hinton et al. (2012) and Srivastava et al. (2014) interpreted dropout as an extreme form of model combination (aka. model ensemble) with extensive parameter/weight sharing, and they proposed to learn the combination through minimizing an appropriate expected loss. Interestingly, they also pointed out that for a single logistic neural unit, the output of dropout is in fact the geometric mean of the outputs of the model ensemble with shared parameters. Subsequently, many theoretical justifications of dropout have been explored, and we can only mention a few here due to space limits. Building on the weight sharing perspective, Baldi & Sadowski (2013; 2014) analyzed the ensemble averaging property of dropout in deep non-linear logistic networks, and supported the view that dropout is equivalent to applying stochastic gradient descent on some regularized\nloss function. Wager et al. (2013) treated dropout as an adaptive regularizer for generalized linear models (GLMs). Helmbold & Long (2016) discussed the differences between dropout and traditional weight decay regularization. In terms of statistical learning theory, Gao & Zhou (2014) studied the Rademacher complexity of different types of dropout, showing that dropout is able to reduce the Rademacher complexity polynomially for shallow neural networks (with one or no hidden layers) and exponentially for deep neural networks. This latter work (Gao & Zhou, 2014) formally demonstrated that dropout, due to its regularizing effect, contributes to reducing the inherent model complexity, in particular the variance component in the generalization error.\nSeen as a model combination technique, it is intuitive that dropout contributes to reducing the variance of the model performance. Surprisingly, dropout has also been shown to play some role in reducing the model bias. For instance, Jain et al. (2015) studied the ability of dropout training to escape local minima, hence leading to reduced model bias. Other studies (Chen et al., 2014; Helmbold & Long, 2014; Wager et al., 2014) focus on the effect of the dropout noise on models with shallow architectures. We noted in passing that there are also some work (Kingma et al., 2015; Gal & Ghahramani, 2015; 2016) trying to understand dropout from the Bayesian perspective.\nIn this work, we first formulate dropout as a tractable approximation of a latent variable model, and give a clean view of weight sharing (§3). Then, we focus on an inference gap in dropout that has somehow gotten under-appreciated: In the inference phase, for computational tractability considerations, the model ensemble generated by dropout is approximated by a single model with scaled weights, resulting in a gap between training and inference, and rendering the many previous theoretical findings inapplicable. In general, this inference gap can be very large and no attempt (to our best knowledge) has been made to control it. We make three contributions in bridging this gap: Theoretically, we introduce expectation-linear dropout neural networks, through which we are able to explicitly quantify the inference gap (§4). In particular, our theoretical results explain why the max-norm constraint on the network weights, a standard practice in training DNNs, can lead to a small inference gap hence potentially improve performance. Algorithmically, we propose to add a sampled version of the inference gap to regularize the standard dropout training objective (expectationlinearization), hence allowing explicit control of the inference gap, and analyze the interaction between expectation-linearization and the model accuracy (§5). Experimentally, through three benchmark datasets we show that our regularized dropout is not only as simple and efficient as standard dropout but also consistently leads to improved performance (§6)."
    }, {
      "heading" : "2 DROPOUT NEURAL NETWORKS",
      "text" : "In this section we set up the notations, review the dropout neural network model, and discuss the inference gap in standard dropout training that we will attempt to study in the rest of the paper."
    }, {
      "heading" : "2.1 DNNS AND NOTATIONS",
      "text" : "Throughout we use uppercase letters for random variables (and occasionally for matrices as well), and lowercase letters for realizations of the corresponding random variables. Let X ∈ X be the input of the neural network, Y ∈ Y be the desired output, and D = {(x1, y1), . . . , (xN , yN )} be our training sample, where xi, i = 1, . . . , N, (resp. yi) are usually i.i.d. samples of X (resp. Y ).\nLet M denote a deep neural network with L hidden layers, indexed by l ∈ {1, . . . , L}. Let h(l) denote the output vector from layer l. As usual, h(0) = x is the input, and h(L) is the output of the neural network. Denote θ = {θl : l = 1, . . . , L} as the set of parameters in the network M, where θl assembles the parameters in layer l. With dropout, we need to introduce a set of dropout random variables S = {Γ(l) : l = 1, . . . , L}, where Γ(l) is the dropout random variable for layer l. Then the deep neural network M can be described as:\nh(l) = fl(h (l−1) γ(l); θl), l = 1, . . . , L, (1)\nwhere is the element-wise product and fl is the transformation function of layer l. For example, if layer l is a fully connected layer with weight matrix W , bias vector b, and sigmoid activation function σ(x) = 11+exp(−x) , then fl(x) = σ(Wx+ b)). We will also use h\n(l)(x, s; θ) to denote the output of layer l with input x and dropout value s, under parameter θ.\nIn the simplest form of dropout, which is also called standard dropout, Γ(l) is a vector of independent Bernoulli random variables, each of which has probability pl of being 1 and 1− pl of being 0. This corresponds to dropping each of the weights independently with probability pl."
    }, {
      "heading" : "2.2 DROPOUT TRAINING",
      "text" : "The standard dropout neural networks can be trained using stochastic gradient decent (SGD), with a sub-network sampled by dropping neural units for each training instance in a mini-batch. Forward and backward pass for that training instance are done only on the sampled sub-network. Intuitively, dropout aims at, simultaneously and jointly, training an ensemble of exponentially many neural networks (one for each configuration of dropped units) while sharing the same weights/parameters.\nThe goal of the stochastic training procedure of dropout can be understood as minimizing an expected loss function, after marginalizing out the dropout variables (Srivastava, 2013; Wang & Manning, 2013). In the context of maximal likelihood estimation, dropout training can be formulated as:\nθ∗ = argmin θ ESD [−l(D,SD; θ)] = argmin θ ESD\n[ − N∑ i=1 log p(yi|xi, Si; θ) ] , (2)\nwhere recall that D is the training sample, SD = {S1, . . . , SN} is the dropout variable (one for each training instance), and l(D,SD; θ) is the (conditional) log-likelihood function defined by the conditional distribution p(y|x, s; θ) of output y given input x, under parameter θ and dropout variable s. Throughout we use the notation EZ to denote the conditional expectation where all random variables except Z are conditioned on.\nDropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al., 2004), among which the max-norm regularization — that constrains the norm of the incoming weight matrix to be bounded by some constant — was found to be especially useful for dropout (Srivastava, 2013; Srivastava et al., 2014)."
    }, {
      "heading" : "2.3 DROPOUT INFERENCE AND GAP",
      "text" : "As mentioned before, dropout is effectively training an ensemble of neural networks with weight sharing. Consequently, at test time, the output of each network in the ensemble should be averaged to deliver the final prediction. This averaging over exponentially many sub-networks is, however, intractable, and standard dropout typically implements an approximation by introducing a deterministic scaling factor for each layer to replace the random dropout variable:\nES [H (L)(x, S; θ)] ? ≈ h(L)(x,E[S]; θ), (3)\nwhere the right-hand side is the output of a single deterministic neural network whose weights are scaled to match the expected number of active hidden units on the left-hand side. Importantly, the right-hand side can be easily computed since it only involves a single deterministic network.\nBulò et al. (2016) combined dropout with knowledge distillation methods (Hinton et al., 2015) to better approximate the averaging processing of the left-hand side. However, the quality of the approximation in (3) is largely unknown, and to our best knowledge, no attempt has been made to explicitly control this inference gap. The main goal of this work is to explicitly quantify, algorithmically control, and experimentally demonstrate the inference gap in (3), in the hope of improving the generalization performance of DNNs eventually. To this end, in the next section we first present a latent variable model interpretation of dropout, which will greatly facilitate our later theoretical analysis."
    }, {
      "heading" : "3 DROPOUT AS LATENT VARIABLE MODELS",
      "text" : "With the end goal of studying the inference gap in (3) in mind, in this section, we first formulate dropout neural networks as a latent variable model (LVM) in § 3.1. Then, we point out the relation between the training procedure of LVM and that of standard dropout in § 3.2. The advantage of formulating dropout as a LVM is that we need only deal with a single model (with latent structure), instead of an ensemble of exponentially many different models (with weight sharing). This much\nsimplified view of dropout enables us to understand and analyze the model parameter θ in a much more straightforward and intuitive way."
    }, {
      "heading" : "3.1 AN LVM FORMULATION OF DROPOUT",
      "text" : "A latent variable model consists of two types of variables: the observed variables that represent the empirical (observed) data and the latent variables that characterize the hidden (unobserved) structure. To formulate dropout as a latent variable model, the input x and output y are regarded as observed variables, while the dropout variable s, representing the sub-network structure, is hidden. Then, upon fixing the input space X , the output space Y , and the latent space S for dropout variables, the conditional probability of y given x under parameter θ can be written as\np(y|x; θ) = ∫ S p(y|x, s; θ)p(s)dµ(s), (4)\nwhere p(y|x, s; θ) is the conditional distribution modeled by the neutral network with configuration s (same as in Eq. (2)), p(s) is the distribution of dropout variable S (e.g. Bernoulli), here assumed to be independent of the input x, and µ(s) is the base measure on the space S."
    }, {
      "heading" : "3.2 LVM DROPOUT TRAINING VS. STANDARD DROPOUT TRAINING",
      "text" : "Building on the above latent variable model formulation (4) of dropout, we are now ready to point out a simple relation between the training procedure of LVM and that of standard dropout. Given an i.i.d. training sample D, the maximum likelihood estimate for the LVM formulation of dropout in (4) is equivalent to minimizing the following negative log-likelihood function:\nθ∗ = argmin θ −l(D; θ) = argmin θ − N∑ i=1 log p(yi|xi; θ), (5)\nwhere p(y|x; θ) is given in Eq. (4). Recall the dropout training objective ESD [−l(D,SD; θ)] in Eq. (2). We have the following theorem as a simple consequence of Jensen’s inequality (details in Appendix A): Theorem 1. The expected loss function of standard dropout (Eq. (2)) is an upper bound of the negative log-likelihood of LVM dropout (Eq. (5)):\n−l(D; θ) ≤ ESD [−l(D,SD; θ)]. (6)\nTheorem 1, in a rigorous sense, justifies dropout training as a convenient and tractable approximation of the LVM formulation in (4). Indeed, since directly minimizing the marginalized negative loglikelihood in (5) may not be easy, a standard practice is to replace the marginalized (conditional) likelihood p(y|x; θ) in (4) with its empirical Monte carlo average through drawing samples from the dropout variable S. The dropout training objective in (2) corresponds exactly to this Monte carlo approximation when a single sample Si is drawn for each training instance (xi, yi). Importantly, we note that the above LVM formulation involves only a single network parameter θ, which largely simplifies the picture and facilitates our subsequent analysis."
    }, {
      "heading" : "4 EXPECTATION-LINEAR DROPOUT NEURAL NETWORKS",
      "text" : "Building on the latent variable model formulation in § 3, we introduce in this section the notion of expectation-linearity that essentially measures the inference gap in (3). We then characterize a general class of neural networks that exhibit expectation-linearity, either exactly or approximately over a distribution p(x) on the input space.\nWe start with defining expectation-linearity in the simplest single-layer neural network, then we extend the notion into general deep networks in a natural way. Definition 1 (Expectation-linear Layer). A network layer h = f(x γ; θ) is expectation-linear with respect to a set X ′ ⊆ X , if for all x ∈ X ′ we have∥∥E[f(x Γ; θ)]− f(x E[Γ]; θ)∥∥\n2 = 0. (7)\nIn this case we say that X ′ is expectation-linearizable, and θ is expectation-linearizing w.r.t X ′.\nObviously, the condition in (7) will guarantee no gap in the dropout inference approximation (3)—an admittedly strong condition that we will relax below. Clearly, if f is an affine function, then we can choose X ′ = X and expectation-linearity is trivial. Note that expectation-linearity depends on the network parameter θ and the dropout distribution Γ.\nExpectation-linearity, as defined in (7), is overly strong: under standard regularity conditions, essentially the transformation function f has to be affine over the set X ′, ruling out for instance the popular sigmoid or tanh activation functions. Moreover, in practice, downstream use of DNNs are usually robust to small errors resulting from approximate expectation-linearity (hence the empirical success of dropout), so it makes sense to define an inexact extension. We note also that the definition in (7) is uniform over the set X ′, while in a statistical setting it is perhaps more meaningful to have expectation-linearity “on average,” since inputs from lower density regions are not going to play a significant role anyway. Taking into account the aforementioned motivations, we arrive at the following inexact extension: Definition 2 (Approximately Expectation-linear Layer). A network layer h = f(x γ; θ) is δ-approximately expectation-linear with respect to a distribution p(x) over X if\nEX [∥∥EΓ[f(X Γ; θ)|X]− f(X E[Γ]; θ)∥∥2] < δ. (8) In this case we say that p(x) is δ-approximately expectation-linearizable, and θ is δ-approximately expectation-linearizing.\nTo appreciate the power of cutting some slack from exact expectation-linearity, we remark that even non-affine activation functions often have approximately linear regions. For example, the logistic function, a commonly used non-linear activation function in DNNs, is approximately linear around the origin. Naturally, we can ask whether it is sufficient for a target distribution p(x) to be well-approximated by an approximately expectation-linearizable one. We begin by providing an appropriate measurement of the quality of this approximation. Definition 3 (Closeness, (Andreas et al., 2015)). A distribution p(x) is C-close to a set X ′ ⊆ X if\nE [\ninf x∗∈X ′ sup γ∈S ‖X γ − x∗ γ‖2\n] ≤ C, (9)\nwhere recall that S is the (bounded) space that the dropout variable lives in.\nIntuitively, p(x) is C-close to a set X ′ if a random sample from p is no more than a distance C from X ′ in expectation and under the worst “dropout perturbation”. For example, a standard normal distribution is close to an interval centering at origin ([−α, α]) with some constant C. Our definition of closeness is similar to that in Andreas et al. (2015), who used this notion to analyze self-normalized log-linear models.\nWe are now ready to state our first major result that quantifies approximate expectation-linearity of a single-layered network (proof in Appendix B.1): Theorem 2. Given a network layer h = f(x γ; θ), where θ is expectation-linearizing w.r.t. X ′ ⊆ X . Suppose p(x) is C-close to X ′ and for all x ∈ X , ‖∇xf(x)‖op ≤ B, where ‖ · ‖op is the usual operator norm. Then, p(x) is 2BC-approximately expectation-linearizable.\nRoughly, Theorem 2 states that the input distribution p(x) that place most of its mass on regions close to expectation-linearizable sets are approximately expectation-linearizable on a similar scale. The bounded operator norm assumption on the derivative ∇f is satisfied in most commonly used layers. For example, for a fully connected layer with weight matrix W , bias vector b, and activation function σ, ‖∇f(·)‖op = |σ′(·)| · ‖W‖op is bounded by ‖W‖op and the supremum of |σ′(·)| (1/4 when σ is sigmoid and 1 when σ is tanh).\nNext, we extend the notion of approximate expectation-linearity to deep dropout neural networks. Definition 4 (Approximately Expectation-linear Network). A deep neural network with L layers (cf. Eq. (1)) is δ-approximately expectation-linear with respect to p(x) over X if\nEX [∥∥ES[H(L)(X,S; θ)|X]− h(L)(X,E[S]; θ)∥∥2] < δ. (10) where h(L)(X,E[S]; θ) is the output of the deterministic neural network in standard dropout.\nLastly, we relate the level of approximate expectation-linearity of a deep neural network to the level of approximate expectation-linearity of each of its layers: Theorem 3. Given an L-layer neural network as in Eq. (1), and suppose that each layer l ∈ {1, . . . , L} is δ-approximately expectation-linear w.r.t. p(h(l)), E[Γ(l)] ≤ γ, supx ‖∇fl(x)‖op ≤ B, and E [ Var[H(l)|X] ] ≤ σ2. Then the network is ∆-approximately expectation-linear with\n∆ = (Bγ)L−1δ + (δ +Bγσ)\n( 1− (Bγ)L−1\n1−Bγ\n) . (11)\nFrom Theorem 3 (proof in Appendix B.2) we observe that the level of approximate expectationlinearity of the network mainly depends on four factors: the level of approximate expecatationlinearity of each layer (δ), the expected variance of each layer (σ), the operator norm of the derivative of each layer’s transformation function (B), and the mean of each layer’s dropout variable (γ). In practice, γ is often a constant less than or equal to 1. For example, if Γ ∼ Bernoulli(p), then γ = p. According to the theorem, the operator norm of the derivative of each layer’s transformation function is an important factor in the level of approximate expectation-linearity: the smaller the operator norm is, the better the approximation. Interestingly, the operator norm of a layer often depends on the norm of the layer’s weight (e.g. for fully connected layers). Therefore, adding max-norm constraints to regularize dropout neural networks can lead to better approximate expectation-linearity hence smaller inference gap and the often improved model performance.\nIt should also be noted that when Bγ < 1, the approximation error ∆ tends to be a constant when the network becomes deeper. When Bγ = 1, ∆ grows linearly with L, and when Bγ > 1, the growth of ∆ becomes exponential. Thus, it is essential to keep Bγ < 1 to achieve good approximation, particularly for deep neural networks."
    }, {
      "heading" : "5 EXPECTATION-LINEAR REGULARIZED DROPOUT",
      "text" : "In the previous section we have managed to bound the approximate expectation-linearity, hence the inference gap in (3), of dropout neural networks. In this section, we first prove a uniform deviation bound of the sampled approximate expectation-linearity measure from its mean, which motivates adding the sampled (hence computable) expectation-linearity measure as a regularization scheme to standard dropout, with the goal of explicitly controlling the inference gap of the learned parameter, hence potentially improving the performance. Then we give the upper bounds on the loss in accuracy due to expectation-linearization, and describe classes of distributions that expectation-linearize easily."
    }, {
      "heading" : "5.1 A UNIFORM DEVIATION BOUND FOR THE SAMPLED EXPECTATION-LINEAR MEASURE",
      "text" : "We now show that an expectation-linear network can be found by expectation-linearizing the network on the training sample. To this end, we prove a uniform deviation bound between the empirical expectation-linearization measure using i.i.d. samples (Eq. (12)) and its mean (Eq. (13)).\nTheorem 4. Let H = { h(L)(x, s; θ) : θ ∈ Θ } denote a space of L-layer dropout neural networks\nindexed with θ, where h(L) : X × S → R and Θ is the space that θ lives in. Suppose that the neural networks inH satisfy the constraints: 1) ∀x ∈ X , ‖x‖2 ≤ α; 2) ∀l ∈ {1, . . . , L},E(Γ(l)) ≤ γ and ‖∇fl‖op ≤ B; 3) ‖h(L)‖ ≤ β. Denote empirical expectation-linearization measure and its mean as:\n∆̂ = 1\nn n∑ i=1 ∥∥ESi[H(L)(Xi, Si; θ)]− h(L)(Xi,E[Si]; θ)∥∥2, (12) ∆ = EX\n[∥∥ES[H(L)(X,S; θ)]− h(L)(X,E[S]; θ)∥∥2]. (13) Then, with probability at least 1− ν, we have\nsup θ∈Θ |∆− ∆̂| < 2αB L(γL/2 + 1)√ n + β\n√ log(1/ν)\nn . (14)\nFrom Theorem 4 (proof in Appendix C.1) we observe that the deviation bound decreases exponentially with the number of layers L when the operator norm of the derivative of each layer’s transformation\nfunction (B) is less than 1 (and the contrary if B ≥ 1). Importantly, the square root dependence on the number of samples (n) is standard and cannot be improved without significantly stronger assumptions.\nIt should be noted that Theorem 4 per se does not imply anything between expectation-linearization and the model accuracy (i.e. how well the expectation-linearized neural network actually achieves on modeling the data). Formally studying this relation is provided in § 5.3. In addition, we provide some experimental evidences in § 6 on how improved approximate expectation-linearity (equivalently smaller inference gap) does lead to better empirical performances."
    }, {
      "heading" : "5.2 EXPECTATION-LINEARIZATION AS REGULARIZATION",
      "text" : "The uniform deviation bound in Theorem 4 motivates the possibility of obtaining an approximately expectation-linear dropout neural networks through adding the empirical measure (12) as a regularization scheme to the standard dropout training objective, as follows:\nloss(D; θ) = −l(D; θ) + λV (D; θ), (15) where −l(D; θ) is the negative log-likelihood defined in Eq. (5), λ > 0 is a regularization constant, and V (D; θ) measures the level of approximate expectation-linearity:\nV (D; θ) = 1\nN N∑ i=1 ∥∥ESi[H(L)(xi, Si; θ)]− h(L)(xi,E[Si]; θ)∥∥22. (16) To solve (15), we can minimize loss(D; θ) via stochastic gradient descent as in standard dropout, and approximate V (D; θ) using Monte carlo:\nV (D; θ) ≈ 1 N N∑ i=1 ∥∥h(L)(xi, si; θ)− h(L)(xi,E[Si]; θ)∥∥22, (17) where si is the same dropout sample as in l(D; θ) for each training instance in a mini-batch. Thus, the only additional computational cost comes from the deterministic term h(L)(xi,E[Si]; θ). Overall, our regularized dropout (15), in its Monte carlo approximate form, is as simple and efficient as the standard dropout."
    }, {
      "heading" : "5.3 ON THE ACCURACY OF EXPECTATION-LINEARIZED MODELS",
      "text" : "So far our discussion has concentrated on the problem of finding expectation-linear neural network models, without any concerns on how well they actually perform at modeling the data. In this section, we characterize the trade-off between maximizing “data likelihood” and satisfying an expectationlinearization constraint.\nTo achieve the characterization, we measure the likelihood gap between the classical maximum likelihood estimator (MLE) and the MLE subject to a expectation-linearization constraint. Formally, given training data D = {(x1, y1), . . . , (xn, yn)}, we define\nθ̂ = argmin θ∈Θ\n−l(D; θ) (18)\nθ̂δ = argmin θ∈Θ,V (D;θ)≤δ\n−l(D; θ) (19)\nwhere −l(D; θ) is the negative log-likelihood defined in Eq. (5), and V (D; θ) is the level of approximate expectation-linearity in Eq. (16).\nWe would like to control the loss of model accuracy by obtaining a bound on the likelihood gap defined as:\n∆l(θ̂, θ̂δ) = 1\nn (l(D; θ̂)− l(D; θ̂δ)) (20)\nIn the following, we focus on neural networks with softmax output layer for classification tasks.\np(y|x, s; θ) = h(L)y (x, s; θ) = fL(h(L−1)(x, s); η) = eη\nT y h (L−1)(x,s)∑ y′∈Y e ηT y′h (L−1)(x,s) (21)\nwhere θ = {θ1, . . . , θL−1, η}, Y = {1, . . . , k} and η = {ηy : y ∈ Y}. We claim:\nTheorem 5. Given an L-layer neural network h(L)(x, s; θ) with softmax output layer in (21), where parameter θ ∈ Θ, dropout variable s ∈ S, input x ∈ X and target y ∈ Y . Suppose that for every x and s, p(y|x, s; θ̂) makes a unique best prediction—that is, for each x ∈ X , s ∈ S, there exists a unique y∗ ∈ Y such that ∀y 6= y∗, η̂Ty h(L−1)(x, s) < η̂Ty∗h(L−1)(x, s). Suppose additionally that ∀x, s, ‖h(L−1)(x, s; θ̂)‖ ≤ β, and ∀y, p(y|x; θ̂) > 0. Then\n∆l(θ̂, θ̂δ) ≤ c1β2 ( ‖η̂‖2 − δ\n4β\n)2 e−c2δ/4β (22)\nwhere c1 and c2 are distribution-dependent constants.\nFrom Theorem 5 (proof in Appendix C.2) we observe that, at one extreme, distributions closed to deterministic can be expectation-linearized with little loss of likelihood.\nWhat about the other extreme — distributions “as close to uniform distribution as possible”? With suitable assumptions about the form of p(y|x, s; θ̂) and p(y|x; θ̂), we can achieve an accuracy loss bound for distributions that are close to uniform: Theorem 6. Suppose that ∀x, s, ‖h(L−1)(x, s; θ̂)‖ ≤ β. Additionally, for each (xi, yi) ∈ D, s ∈ S , log 1k ≤ log p(yi|xi, s; θ̂) ≤ 1 k ∑ y∈Y log p(y|xi, s; θ̂). Then asymptotically as n→∞:\n∆l(θ̂, θ̂δ) ≤ (\n1− δ 4β‖η̂‖2\n) E [KL (p(·|X; θ)‖Unif(Y))] (23)\nTheorem 6 (proof in Appendix C.3) indicates that uniform distributions are also an easy class for expectation-linearization.\nThe next question is whether there exist any classes of conditional distributions p(y|x) for which all distributions are provably hard to expectation-linearize. It remains an open problem and might be an interesting direction for future work."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "In this section, we evaluate the empirical performance of the proposed regularized dropout in (15) on a variety of network architectures for the classification task on three benchmark datasets—MNIST, CIFAR-10 and CIFAR-100. We applied the same data preprocessing procedure as in Srivastava et al. (2014). To make a thorough comparison and provide experimental evidence on how the expectationlinearization interacts with the predictive power of the learned model, we perform experiments of Monte Carlo (MC) dropout, which approximately computes the final prediction (left-hand side of (3)) via Monte Carlo sampling, w/o the proposed regularizer. In the case of MC dropout, we average m = 100 predictions using randomly sampled configurations. In addition, the network architectures and hyper-parameters for each experiment setup are the same as those in Srivastava et al. (2014), unless we explicitly claim to use different ones. Following previous works, for each data set We held out 10,000 random training images for validation to tune the hyper-parameters, including λ in Eq. (15). When the hyper-parameters are fixed, we train the final models with all the training data, including the validation data. A more detailed description of the conducted experiments can be provided in Appendix D. For each experiment, we report the mean test errors with corresponding standard deviations over 5 repetitions."
    }, {
      "heading" : "6.1 MNIST",
      "text" : "The MNIST dataset (LeCun et al., 1998) consists of 70,000 handwritten digit images of size 28×28, where 60,000 images are used for training and the rest for testing. This task is to classify the images into 10 digit classes. For the purpose of comparison, we train 6 neural networks with different architectures. The experimental results are shown in Table 1."
    }, {
      "heading" : "6.2 CIFAR-10 AND CIFAR-100",
      "text" : "The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 60,000 color images of size 32× 32, drawn from 10 and 100 categories, respectively. 50,000 images are used for training and the\nrest for testing. The neural network architecture we used for these two datasets has 3 convolutional layers, followed by two fully-connected (dense) hidden layers (again, same as that in Srivastava et al. (2014)). The experimental results are recorded in Table 1, too.\nFrom Table 1 we can see that on MNIST data, dropout network training with expectation-linearization outperforms standard dropout on all 6 neural architectures. On CIFAR data, expectation-linearization reduces error rate from 12.82% to 12.20% for CIFAR-10, achieving 0.62% improvement. For CIFAR-100, the improvement in terms of error rate is 0.97% with reduction from 37.22% to 36.25%.\nFrom the results we see that with or without expectation-linearization, the MC dropout networks achieve similar results. It illustrates that by achieving expectation-linear neural networks, the predictive power of the learned models has not degraded significantly. Moreover, it is interesting to see that with the regularization, on MNIST dataset, standard dropout networks achieve even better accuracy than MC dropout. It may be because that with expectation-linearization, standard dropout inference achieves better approximation of the final prediction than MC dropout with (only) 100 samples. On CIFAR datasets, MC dropout networks achieve better accuracy than the ones with the regularization. But, obviously, MC dropout requires much more inference time than standard dropout (MC dropout with m samples requires about m times the inference time of standard dropout).\n6.3 EFFECT OF REGULARIZATION CONSTANT λ\nIn this section, we explore the effect of varying the hyper-parameter for the expectation-linearization rate λ. We train the network architectures in Table 1 with the λ value ranging from 0.1 to 10.0. Figure 1 shows the test errors obtained as a function of λ on three datasets. In addition, Figure 1, middle and right panels, also measures the empirical expectation-linearization risk ∆̂ of Eq. (12) with varying λ on CIFAR-10 and CIFAR-100, where ∆̂ is computed using Monte carlo with 100 independent samples.\nFrom Figure 1 we can see that when λ increases, better expectation-linearity is achieved (i.e. ∆̂ decreases). The model accuracy, however, has not kept growing with increasing λ, showing that in practice considerations on the trade-off between model expectation-linearity and accuracy are needed."
    }, {
      "heading" : "6.4 COMPARISON WITH DROPOUT DISTILLATION",
      "text" : "To make a thorough empirical comparison with the recently proposed Dropout Distillation method (Bulò et al., 2016), we also evaluate our regularization method on CIFAR-10 and CIFAR-100 datasets with the All Convolutional Network (Springenberg et al., 2014) (AllConv). To facilitate comparison, we adopt the originally reported hyper-parameters and the same setup for training.\nTable 2 gives the results comparison the classification error percentages on test data under AllConv using standard dropout, Monte Carlo dropout, standard dropout with our proposed expectationlinearization, and recently proposed dropout distillation on CIFAR-10 and CIFAR-100 1. According to Table 2, our proposed expectation-linear regularization method achieves comparable performance to dropout distillation."
    }, {
      "heading" : "7 CONCLUSIONS",
      "text" : "In this work, we attempted to establish a theoretical basis for the understanding of dropout, motivated by controlling the gap between dropout’s training and inference phases. Through formulating dropout as a latent variable model and introducing the notion of (approximate) expectation-linearity, we have formally studied the inference gap of dropout, and introduced an empirical measure as a regularization scheme to explicitly control the gap. Experiments on three benchmark datasets demonstrate that reducing the inference gap can indeed improve the end performance. In the future, we intend to formally relate the inference gap to the generalization error of the underlying network, hence providing further justification of regularized dropout."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA."
    }, {
      "heading" : "APPENDIX: DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION",
      "text" : ""
    }, {
      "heading" : "A LVM DROPOUT TRAINING VS. STANDARD DROPOUT TRAINING",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : ""
    }, {
      "heading" : "Proof.",
      "text" : "ESD [l(D,SD; θ)] = ∫ S N∏ i=1 p(si) ( N∑ i=1 log p(yi|xi, si; θ) ) dµ(s1) . . . dµ(sN )\n= N∑ i=1 ∫ S p(si) log p(yi|xi, si; θ)dµ(si)\nBecause log(·) is a concave function, from Jensen’s Inequality,∫ S p(s) log p(y|x, s; θ)dµ(s) ≤ log ∫ S p(s)p(y|x, s; θ)dµ(s)\nThus\nESD [−l(D,SD; θ)] ≥ N∑ i=1 log ∫ S p(si)p(yi|xi, si; θ)dµ(si) = −l(D; θ)."
    }, {
      "heading" : "B EXPECTATION-LINEAR DROPOUT NEURAL NETWORKS",
      "text" : ""
    }, {
      "heading" : "B.1 PROOF OF THEOREM 2",
      "text" : "Proof. Let γ∗ = E[Γ], and\nA ∆ = {x : ‖E[f(x Γ; θ)]− f(x γ∗; θ)‖2 = 0}\nLet X∗ = argmin x∈A sup γ∈S ‖X γ − x γ‖2, and X− = X −X∗. Then,\nX γ = X∗ γ +X− γ\nIn the following, we omit the parameter θ for convenience. Moreover, we denote EΓ [ f(X Γ; θ) ] ∆ = E [ f(X Γ; θ)|X ] From Taylor Series, there exit some X ′, X ′′ ∈ X satisfy that\nf(X Γ) = f(X∗ Γ) + f ′(X ′ Γ)(X− Γ) f(X γ∗) = f(X∗ γ∗) + f ′(X ′′ γ∗)(X− γ∗)\nwhere we denote f ′(x) = (∇xf(x))T . Then,\nEΓ[f(X Γ)− f(X γ∗)] = EΓ[f(X\n∗ Γ +X− Γ)− f(X∗ γ∗ +X− γ∗)] = EΓ[f(X\n∗ Γ)− f(X∗ γ∗) + f ′(X ′ Γ)(X− Γ)− f ′(X ′′ γ∗)(X− γ∗)] = EΓ[f(X ∗ Γ)− f(X∗ γ∗)] + EΓ[f ′(X ′ Γ)(X− Γ)− f ′(X ′′ γ∗)(X− γ∗)]\nSince X∗ ∈ A, we have EΓ[f(X\n∗ Γ)− f(X∗ γ∗)] = 0. Then,\nEΓ[f(X Γ)− f(X γ∗)] = EΓ[f\n′(X ′ Γ)(X− Γ)− f ′(X ′′ γ∗)(X− γ∗)] = EΓ[(f\n′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)] + EΓ[f ′(X ′′ γ∗)(X− Γ−X− γ∗)] = EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]\nThen, ‖EΓ[f(X Γ)]− f(X γ∗)‖2 = ‖EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]‖2 Since ‖X− γ′‖2 ≤ sup\nγ∈S ‖X− γ‖2 = inf x∈A sup γ∈S ‖X γ − x γ‖2, and from Jensen’s inequality\nand property of operator norm,\n‖EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]‖2 ≤ EΓ [ ‖f ′(X ′ Γ)− f ′(X ′′ γ∗)‖op‖X− Γ‖2 ] ≤ 2BEΓ [ ‖X− Γ‖2\n] ≤ 2B inf\nx∈A sup γ∈S ‖X γ − x γ‖2\nFinally we have,\nEX [ ‖EΓ[(f ′(X ′ Γ)− f ′(X ′′ γ∗))(X− Γ)]‖2 ] ≤ 2BE [ inf x∈A sup γ∈S ‖X γ − x γ‖2 ] ≤ 2BC"
    }, {
      "heading" : "B.2 PROOF OF THEOREM 3",
      "text" : "Proof. Induction on the number of the layers L. As before, we omit the parameter θ. Initial step: when L = 1, the statement is obviously true. Induction on L: Suppose that the statement is true for neural networks with L layers. Now we prove the case L+ 1. From the inductive assumption, we have,\nEX [∥∥ESL[H(L)(X,SL)]− h(L)(X,E[SL])∥∥2] ≤ ∆L (1) where SL = {Γ(1), . . . ,Γ(L)} is the dropout random variables for the first L layers, and\n∆L = (Bγ) L−1δ + (δ +Bγσ)\n( 1− (Bγ)L−1\n1−Bγ ) In addition, the L+ 1 layer is δ-approximately expectation-linear, we have:\nEH(L) [∥∥EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(H(L) γ(L+1))∥∥2] ≤ δ (2)\nLet E[Γ(l)] = γ(l),∀l ∈ {1, . . . , L + 1}, and let H(l) and h(l) be short for H(l)(X,Sl) and h(l)(X,E(Sl)), respectively, when there is no ambiguity. Moreover, we denote\nES [ H(L)(X,S; θ) ] = ES [ H(L)(X,S; θ) ∣∣X] for convenience. Then,\nEX [∥∥ESL+1[H(L+1)]− h(L+1)∥∥2] = EX\n[∥∥∥ESL[EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))] +ESL [ fL+1(H (L) γ(L+1)) ] − fL+1(h(L) γ(L+1)) ∥∥∥ 2\n] ≤ EX [∥∥∥ESL[EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))]∥∥∥ 2\n] +EX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(h(L) γ(L+1))∥∥∥ 2\n] From Eq. 2 and Jensen’s inequality, we have\nEX [∥∥∥ESL[EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))]∥∥∥ 2 ] ≤ EH(L) [∥∥∥EΓ(L+1)[fL+1(H(L) Γ(L+1))]− fL+1(h(L) γ(L+1))∥∥∥ 2 ] ≤ δ\n(3)\nand\nEX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(h(L) γ(L+1))∥∥∥ 2 ] = EX\n[∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(ESL[H(L)] γ(L+1)) +fL+1(ESL [ H(L) ] γ(L+1))− fL+1(h(L) γ(L+1)) ∥∥∥ 2\n] ≤ EX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(ESL[H(L)] γ(L+1))∥∥∥ 2\n] +EX [∥∥∥fL+1(ESL[H(L)] γ(L+1))− fL+1(h(L) γ(L+1))∥∥∥ 2 ] (4)\nUsing Jensen’s inequality, property of operator norm and E [ Var[H(l)|X] ] ≤ σ2, we have\nEX [∥∥∥ESL[fL+1(H(L) γ(L+1))]− fL+1(ESL[H(L)] γ(L+1))∥∥∥ 2 ] ≤ EH(L) [∥∥∥fL+1(H(L) γ(L+1))− fL+1(ESL[H(L)] γ(L+1))∥∥∥ 2\n] ≤ BγEH(L)\n[∥∥H(L) − ESL[H(L)]∥∥2] ≤ Bγ ( EH(L) [∥∥H(L) − ESL[H(L)]∥∥22]) 12 ≤ Bγσ (5)\nFrom Eq. 1\nEX [∥∥∥fL+1(ESL[H(L)] γ(L+1))− fL+1(h(L) γ(L+1))∥∥∥ 2 ] = BγEX\n[∥∥ESL[H(L)]− h(L)∥∥2] ≤ Bγ∆L (6) Finally, to sum up with Eq. 3, Eq. 4, , Eq. 5, , Eq. 6, we have\nEX [∥∥ESL+1[H(L+1)]− h(L+1)∥∥2] ≤ δ +Bγσ +Bγ∆L = (Bγ)Lδ + (δ +Bγσ) ( 1−(Bγ)L\n1−Bγ\n) = ∆L+1"
    }, {
      "heading" : "C EXPECTATION-LINEARIZATION",
      "text" : ""
    }, {
      "heading" : "C.1 PROOF OF THEOREM 4: UNIFORM DEVIATION BOUND",
      "text" : "Before proving Theorem 4, we first define the notations.\nLet Xn = {X1, . . . , Xn} be a set of n samples of input X . For a function space F : X → R, we use Radn(F , Xn) to denote the empirical Rademacher complexity of F ,\nRadn(F , Xn) = Eσ [\nsup f∈F ( 1 n n∑ i=1 σif(Xi) )]\nand the Rademacher complexity is defined as Radn(F) = EXn [ Radn(F , Xn) ] In addition, we import the definition of dropout Rademacher complexity from Gao & Zhou (2014):\nRn(H, Xn, Sn) = Eσ [\nsup h∈H\n( 1 n n∑ i=1 σih(Xi, Si) )]\nRn(H) = EXn,Sn [ Radn(H, Xn, Sn) ]\nwhere H : X × S → R is a function space defined on input space X and dropout variable space S. Rn(H, Xn, Sn) and Rn(H) are the empirical dropout Rademacher complexity and dropout Rademacher complexity, respectively. We further denoteRn(H, Xn) ∆ = ESn [ Radn(H, Xn, Sn) ] .\nNow, we define the following function spaces: F = { f(x; θ) : f(x; θ) = ES [ H(L)(x, S; θ) ] , θ ∈ Θ } G = { g(x; θ) : g(x; θ) = h(L)(x,E[S]; θ), θ ∈ Θ\n} H = { h(x, s; θ) : h(x, s; θ) = h(L)(x, s; θ), θ ∈ Θ\n} Then, the function space of v(x) = f(x)− g(x) is V = {f(x)− g(x) : f ∈ F , g ∈ G}. Lemma 7.\nRadn(F , Xn) ≤ Rn(H, Xn)\nProof. Rn(H, Xn) = ESn [ Radn(H, Xn, Sn) ] = ESn [ Eσ [ sup h∈H ( 1 n n∑ i=1 σih(Xi, Si) )]]\n= Eσ [ ESn [ sup h∈H ( 1 n n∑ i=1 σih(Xi, Si) )]]\n≥ Eσ [\nsup h∈H\nESn [( 1 n n∑ i=1 σih(Xi, Si) )]]\n= Eσ [ sup h∈H ( 1 n n∑ i=1 σiESi [ h(Xi, Si) ])] = Eσ [ sup h∈H ( 1 n n∑ i=1 σiESi [ H(L)(Xi, Si; θ) ])] = Radn(F , Xn)\nFrom Lemma 7, we have Radn(F) ≤ Rn(H). Lemma 8.\nRn(H) ≤ αB LγL/2√ n\nRadn(G) ≤ αB L\n√ n\nProof. See Theorem 4 in Gao & Zhou (2014).\nNow, we can prove Theorem 4."
    }, {
      "heading" : "Proof of Theorem 4",
      "text" : "Proof. From Rademacher-based uniform bounds theorem, with probability ≥ 1− δ,\nsup v∈V |∆− ∆̂| < 2Radn(V) + β\n√ log(1/δ)\nn\nSince V = F − G, we have\nRadn(V) = Radn(F − G) ≤ Radn(F) +Radn(G) ≤ αBL(γL/2 + 1)√\nn\nThen, finally, we have that with probability ≥ 1− δ,\nsup θ∈Θ |∆− ∆̂| < 2αB L(γL/2 + 1)√ n + β\n√ log(1/δ)\nn"
    }, {
      "heading" : "C.2 PROOF OF THEOREM 5: NON-UNIFORM BOUND OF MODEL ACCURACY",
      "text" : "For convenience, we denote λ = {θ1, . . . , θL−1}. Then θ = {λ, η}, and MLE θ̂ = {λ̂, η̂} Lemma 9.\n‖∇fL(·; η)T ‖op ≤ 2‖η‖2 (7)\nProof. denote\nA = ∇fL(·; η)T = [ py(ηy − η)T ] ∣∣∣k y=1\nwhere py = p(y|x, s; θ), η = E [ηY ] = k∑ y=1 pyηy .\nFor each v such that ‖v‖2 = 1,\n‖Av‖22 = ∑ y∈Y ( py (ηy − η)T v )2 ≤ ∑ y∈Y ‖py (ηy − η) ‖22‖v‖22 = ∑ y∈Y ‖py (ηy − η) ‖22\n≤ ∑ y∈Y py‖ηy − η‖22 ≤ ∑ y∈Y 2py ( ‖η‖22 + ∑ y′∈Y py′‖ηy′‖22 ) = 4\n∑ y∈Y py‖ηy‖22 ≤ 4‖η‖22\nSo we have ‖A‖op ≤ 2‖η‖2.\nLemma 10. If parameter θ̃ = {λ̂, η} satisfies that ‖η‖2 ≤ δ4β , then V (D; θ̃) ≤ δ, where V (D; θ) is defined in Eq. (16).\nProof. Let SL = {Γ(1), . . . ,Γ(L)}, and let H(l) and h(l) be short for H(l)(X,Sl; θ̃) and h(l)(X,E(Sl); θ̃), respectively.\nFrom lemma 9, we have ‖fL(x; η)− fL(y; η)‖2 ≤ 2‖η‖2‖x− y‖2. Then,∥∥ESL [HL]− hL∥∥2 = ∥∥ESL−1 [fL(H(L−1); η)]− fL(h(L−1); η)∥∥2 ≤ ESL−1\n∥∥fL(H(L−1); η)− fL(h(L−1); η)∥∥2 ≤ 2‖η‖2 ∥∥H(L−1) − h(L−1)∥∥ 2 ≤ 4β‖η‖2 ≤ δ\nLemma 10 says that we can get θ satisfying the expectation-linearization constrain by explicitly scaling down η̂ while keeping λ̂.\nIn order to prove Theorem 5, we make the following assumptions:\n• The dimension of h(L−1) is d, i.e. h(L−1) ∈ Rd. • Since ∀y ∈ Y, p(y|x; θ̂) > 0, we assume p(y|x; θ̂) ≥ 1/b, where b ≥ |Y| = k. • As in the body text, let p(y|x, s; θ̂) be nonuniform, and in particular let η̂Ty∗h (L−1)(x, s; λ̂)− η̂Ty h(L−1)(x, s; λ̂) > c‖η̂‖2,∀y 6= y∗.\nFor convenience, we denote ηTh(L−1)(x, s;λ) = ηTuy(x, s;λ), where uTy (x, s;λ) = (v T 0 , . . . , v T k ) and\nvi = { h(L−1)(x, s;λ) if i = y 0 otherwise\nTo prove Theorem 5, we first prove the following lemmas.\nLemma 11. If p(y|x; θ̂) ≥ 1/b, then ∀α ∈ [0, 1], for parameter θ̃ = {λ̂, αη̂}, we have\np(y|x; θ̃) ≥ 1 b\nProof. We define\nf(α) ∆ = (y|x, s; θ̃) = e αηTy h (L−1)(x,s;λ̂)∑\ny′∈Y e αηT y′h (L−1)(x,s;λ̂)\n=\n( eη T y h (L−1)(x,s;λ̂) )α\n∑ y′∈Y ( e ηT y′h (L−1)(x,s;λ̂) )α\nSince Y = {1, . . . , k}, for fixed x ∈ X , s ∈ S, log f(α) is a concave function w.r.t α. Since b ≥ k, we have\nlog f(α) ≥ (1− α) log f(0) + α log f(1) ≥ − log b\nSo we have ∀x, s, p(y|x, s; θ̃) ≥ 1/b. Then\np(y|x; θ̃) = ES [ p(y|x, S; θ̂) ] ≥ 1 b\nLemma 12. if y is not the majority class, i.e. y 6= y∗, then for parameter θ̃ = {λ̂, αη̂}\np(y|x, s, θ̃) ≤ e−cα‖η̂‖2\nProof.\np(y|x, s, θ̃) = e αη̂Tuy∑\ny′∈Y eαη̂\nTuy′ ≤ e\nαη̂Tuy\neαη̂ Tuy∗\n≤ e−cα‖η̂‖2\nLemma 13. For a fixed x and s, the absolute value of the entry of the vector under the parameter θ̃ = {λ̂, αη̂}:\n|p(y|x, s; θ̃)(uy − EY [uY ])|i ≤ β(k − 1)e−cα‖η̂‖2\nProof. Suppose y is the majority class of p(y|x, s; θ̃). Then,\nuy − Ey[uY ] = (vy′)ky′=1\nwhere\nvy = { (1− p(y|x, s; θ̃)h(L−1) if y = y∗ −p(y|x, s; θ̃)h(L−1) otherwise\nFrom Lemma 12, we have\n|p(y|x, s; θ̃)(uy − EY [uY ])|i ≤ |(uy − EY [uY ])|i ≤ β(k − 1)e−cα‖η̂‖2\nNow, we suppose y is not the majority class of p(y|x, s; θ̃). Then,\n|p(y|x, s; θ̃)(uy − EY [uY ])|i ≤ p(y|x, s; θ̃)β ≤ βe−cα‖η̂‖2\nOverall, the lemma follows.\nLemma 14. We denote the matrix\nA ∆ = ES [ p(y|x,s;θ̃) p(y|x;θ̃) (uy − EY [uY ])(uy − EY [uY ]) T ]\n−ES [ p(y|x,s;θ̃) p(y|x;θ̃) (uy − EY [uY ]) ] ES [ p(y|x,s;θ̃) p(y|x;θ̃) (uy − EY [uY ]) ]T Then the absolute value of the entry of A under the parameter θ̃ = {λ̂, αη̂}:\n|Aij | ≤ 2b(k − 1)β2e−cα‖η̂‖2\nProof. From Lemma 11, we have p(y|x; θ̃) ≥ 1/b. Additionally, the absolute value of the entry of uy − EY [uY ] is bounded by β. We have for each i∣∣∣∣∣ES [ p(y|x, s; θ̃) p(y|x; θ̃) (uy − EY [uY ]) ]∣∣∣∣∣ i ≤ ES [ p(y|x, s; θ̃) p(y|x; θ̃) β ] = β\nThen from Lemma 13 |Aij | ≤ 2b(k − 1)β2e−cα‖η̂‖2\nLemma 15. We denote the matrix\nB ∆ = ES [ p(y|x, s; θ̃) p(y|x; θ̃) ( EY [ uY u T Y ] − EY [uY ]EY [uY ]T )]\nThen the absolute value of the entry of B under the parameter θ̃ = {λ̂, αη̂}:\n|Bij | ≤ 2(k − 1)β2e−cα‖η̂‖2\nProof. We only need to prove that for fixed x and s, for each i, j:∣∣EY [uY uTY ]− EY [uY ]EY [uY ]T ∣∣ij ≤ 2(k − 1)β2e−cα‖η̂‖2 Since ∣∣EY [uY uTY ]− EY [uY ]EY [uY ]T ∣∣ij = |CovY [(uY )i, (uY )j ]| ≤ β2 k∑ y=1 p(y|x, s; θ̃)− p(y|x, s; θ̃)2\nSuppose y is the majority class. Then from Lemma 12,\np(y|x, s; θ̃)− p(y|x, s; θ̃)2 ≤ 1− p(y|x, s; θ̃) ≤ (k − 1)e−cα‖η̂‖2\nIf y is not the majority class. Then,\np(y|x, s; θ̃)− p(y|x, s; θ̃)2 ≤ p(y|x, s; θ̃) ≤ e−cα‖η̂‖2\nSo we have k∑ y=1 p(y|x, s; θ̃)− p(y|x, s; θ̃)2 ≤ 2(k − 1)e−cα‖η̂‖2\nThe lemma follows.\nLemma 16. Under the parameter θ̃ = {λ̂, αη̂}, the largest eigenvalue of the matrix\n1\nn n∑ i=1 (A(xi, yi)−B(xi, yi)) (8)\nis at most 2dk(k − 1)(b+ 1)β2e−cα‖η̂‖2\nProof. From Lemma 14 and Lemma 15, each entry of the matrix in (8) is at most 2(k − 1)(b + 1)β2e−cα‖η̂‖2 . Thus, by Gershgorin’s theorem, the maximum eigenvalue of the matrix in (8) is at most 2dk(k − 1)(b+ 1)β2e−cα‖η̂‖2 .\nNow, we can prove Theorem 5 by constructing a scaled version of θ̂ that satisfies the expectationlinearization constraint."
    }, {
      "heading" : "Proof of Theorem 5",
      "text" : "Proof. Consider the likelihood evaluated at θ̃ = {λ̂, αη̂}, where α = δ4β‖η̂‖2 . If α > 1, then ‖η‖2 > δ4β . We know the MLE θ̂ already satisfies the expectation-linearization constraint. So we can assume that 0 ≤ α ≤ 1, and we know that θ̃ satisfies V (D; θ̃) ≤ δ. Then,\n∆l(θ̂, θ̂δ) ≤ ∆l(θ̂, θ̃) = 1\nn (l(D; θ̂)− l(D; θ̃)) = g(λ̂, η̂)− g(λ̂, αη̂)\nwhere g(λ, η) = 1n l(D; (λ, η)). Taking the second-order Taylor expansion about η, we have\ng(λ̂, αη̂) = g(λ̂, η̂) +∇Tη g(λ̂, η̂)(αη̂ − η̂) + (αη̂ − η̂)T∇2ηg(λ̂, η̂)(αη̂ − η̂)\nSince θ̂ is the MLE, the first-order term ∇Tη g(λ̂, η̂)(αη̂ − η̂) = 0. The Hessian in the second-order term is just Eq.(8). Thus, from Lemma 16 we have\ng(λ̂, αη̂) ≤ g(λ̂, η̂)− (1− α)2‖η̂‖222dk(k − 1)(b+ 1)β2e−cα‖η̂‖2 = g(λ̂, η̂)− 2dk(k − 1)(b+ 1)β2 ( ‖η̂‖2 − δ4β )2 e−cδ/4β\n= g(λ̂, η̂)− c1β2 ( ‖η̂‖2 − δ4β )2 e−c2δ/4β\nwith setting c1 = 2dk(k − 1)(b+ 1) and c2 = c. Then the theorem follows."
    }, {
      "heading" : "C.3 PROOF OF THEOREM 6: UNIFORM BOUND OF MODEL ACCURACY",
      "text" : "In the following, we denote θ̃ = {λ̂, αη̂}.\nLemma 17. For each y ∈ Y , if p(y|x, s; θ̂) ≥ 1/k, then ∀α ∈ [0, 1]\np(y|x, s; θ̃) ≥ 1 k\nProof. This lemma can be regarded as a corollary of Lemma 11.\nLemma 18. For a fixed x and s, we denote eη̂ T y h (L−1)(x,s;λ̂) = wy . Then we have\np(y|x, s, θ̃) = e αη̂Ty h (L−1)(x,s;λ̂)∑ y′∈Y e αη̂T y′h (L−1)(x,s;λ̂) = (wy) α∑ y′∈Y (wy′)α\nAdditionally, we denote gs(α) = ∑ y′∈Y p(y′|x, s; θ̃) logwy′ − logwy . We assume gs(0) ≥ 0. Then we have ∀α ≥ 0 gs(α) ≥ 0\nProof.\n∂gs(α) ∂α = ∑ y′∈Y logwy′ ∂p(y′|x, s; θ̃) ∂α = VarY [logwY |X − x, S = s] ≥ 0\nSo gs(α) is non-decreasing. Since gs(0) ≥ 0, we have gs(α) ≥ 0 when α ≥ 0.\nFrom above lemma, we have for each training instance (xi, yi) ∈ D, and ∀α ∈ [0, 1],\nEY [ log p(Y |xi, s; θ̃) ] ≥ log p(yi|xi, s; θ̃) (9)\nFor convenience, we define m(s, y) = log p(y|x, s; θ̃)− EY [ log p(Y |x, s; θ̃) ]\nLemma 19. If y satisfies Lemma 17 and gs(α) ≥ 0, then\nVarY [m(s, Y )] ≥ m(s, y)2\nProof. First we have m(s, y) = log p(y|x, s; θ̃)− log 1/k −KL ( p(·|x, s; θ̃)|Unif(Y) ) ≤ 0\nSo we have\n(VarY [m(s, Y )]) 1/2 = √ EY [( log p(Y |x, s; θ̃)− EY [ log p(Y |x, s; θ̃) ])2] ≥ EY\n[∣∣∣log p(Y |x, s; θ̃)− EY [log p(Y |x, s; θ̃)]∣∣∣] = EY\n[∣∣∣KL(p(·|x, s; θ̃)|Unif(Y))+ log 1/k − log p(Y |x, s; θ̃)∣∣∣] = EY [ KL ( p(·|x, s; θ̃)|Unif(Y) ) + ∣∣∣log 1/k − log p(Y |x, s; θ̃)∣∣∣]\n≥ KL ( p(·|x, s; θ̃)|Unif(Y) ) + EY [ log p(Y |x, s; θ̃)− log 1/k ] = 2KL ( p(·|x, s; θ̃)|Unif(Y)\n) As KL ( p(·|x, s; θ̃)|Unif(Y) ) ≥ 0 and log p(y|x, s; θ̃) ≥ log 1/k. So we have 2KL ( p(·|x, s; θ̃)|Unif(Y) ) ≥ KL ( p(·|x, s; θ̃)|Unif(Y) ) +log 1/k−log p(y|x, s; θ̃) = −m(s, y)\nThen the lemma follows.\nFrom Lemma 19 and Eq. (9), we have for each training instance (xi, yi) ∈ D, and ∀α ∈ [0, 1],\nVarY [m(s, Y )] ≥ m(s, yi)2 (10)\nLemma 20. For each training instance (xi, yi) ∈ D, and ∀α ∈ [0, 1], we have\nlog p(yi|xi; {λ̂, αη̂}) ≥ (1− α) log p(yi|xi; {λ̂, 0}) + α log p(yi|xi; {λ̂, η̂})\nProof. We define\nf(α) = log p(yi|xi; {λ̂, αη̂})− (1− α) log p(yi|xi; {λ̂, 0})− α log p(yi|xi; {λ̂, η̂})\nBecause f(0) = f(1) = 0, we only need to prove that f(α) is concave on [0, 1]. We have\n∇2f(α) = −ES|Y=yi [VarY [m(S, Y )]] + VarS|Y=yi [m(S, yi)]\nwhere S|Y = yi is under the probability distribution p(s|Y = yi, xi; θ̃) = p(yi|xi,S;θ̃)p(s)p(yi|xi;θ̃) From Eq. (10), we have\nES|Y=yi [VarY [m(S, Y )]] ≥ ES|Y=yi [ m(S, yi) 2 ] ≥ VarS|Y=yi [m(S, yi)]\nSo we have ∇2f(α) ≤ 0. The lemma follows.\nNow, we can prove Theorem 6 by using the same construction of an expectation-linearizing parameter as in Theorem 5."
    }, {
      "heading" : "Proof of Theorem 6",
      "text" : "Proof. Consider the same parameter θ̃ = {λ̂, αη̂}, where α = δ4β‖η̂‖2 ≤ 1. we know that θ̃ satisfies V (D; θ̃) ≤ δ. Then,\n∆l(θ̂, θ̂δ) ≤ ∆l(θ̂, θ̃) = 1\nn (l(D; θ̂)− l(D; θ̃))\nFrom Lemma 20 we have:\nl(D; θ̃) = l(D; {λ̂, αη̂}) ≥ (1− α)l(D; {λ̂, 0}) + αl(D; {λ̂, η̂})\nSo ∆l(θ̂, θ̂δ) ≤ (1− α) 1n ( l(D; θ̂)− l(D; {λ̂, 0}) ) = (1− α) 1n n∑ i=1 log p(yi|xi; θ̂)− log Unif(Y)\n(1− α)E [KL (p(·|X; θ)‖Unif(Y))] ≤ ( 1− δ4β‖η̂‖2 ) E [KL (p(·|X; θ)‖Unif(Y))]"
    }, {
      "heading" : "D DETAILED DESCRIPTION OF EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "D.1 NEURAL NETWORK ARCHITECTURES",
      "text" : "MNIST For MNIST, we train 6 different fully-connected (dense) neural networks with 2 or 3 layers (see Table 1). For all architectures, we used dropout rate p = 0.5 for all hidden layers and p = 0.2 for the input layer.\nCIFAR-10 and CIFAR-100 For the two CIFAR datasets, we used the same architecture in Srivastava et al. (2014) — three convolutional layers followed by two fully-connected hidden layers. The convolutional layers have 96, 128, 265 filters respectively, with a 5× 5 receptive field applied with a stride of 1. Each convolutional layer is followed by a max pooling layer pools 3× 3 regions at strides of 2. The fully-connected layers have 2048 units each. All units use the rectified linear activation function. Dropout was applied to all the layers with dropout rate p = (0.1, 0.25, 0.25, 0.5, 0.5, 0.5) for the layers going from input to convolutional layers to fully-connected layers."
    }, {
      "heading" : "D.2 NEURAL NETWORK TRAINING",
      "text" : "Neural network training in all the experiments is performed with mini-batch stochastic gradient descent (SGD) with momentum. We choose an initial learning rate of η0, and the learning rate is updated on each epoch of training as ηt = η0/(1 + ρt), where ρ is the decay rate and t is the number of epoch completed. We run each experiment with 2,000 epochs and choose the parameters achieving the best performance on validation sets.\nTable 3 summarizes the chosen hyper-parameters for all experiments. Most of the hyper-parameters are chosen from Srivastava et al. (2014). But for some experiments, we cannot reproduce the performance reported in Srivastava et al. (2014) (We guess one of the possible reasons is that we used different library for implementation.). For these experiments, we tune the hyper-parameters on the validation sets by random search. Due to time constrains it is infeasible to do a random search across the full hyper-parameter space. Thus, we try to use as many hyper-parameters reported in Srivastava et al. (2014) as possible.\nD.3 EFFECT OF EXPECTATION-LINEARIZATION RATE λ\nTable 4 illustrates the detailed results of the experiments on the effect of λ. For MNIST, it lists the error rates under different λ values for six different network architectures. For two datasets of CIFAR, it gives the error rates under different λ values, among with the empirical expectation-linearization risk ∆̂."
    } ],
    "references" : [ {
      "title" : "On the accuracy of selfnormalized log-linear models",
      "author" : [ "Jacob Andreas", "Maxim Rabinovich", "Michael I Jordan", "Dan Klein" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Andreas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2015
    }, {
      "title" : "The dropout learning algorithm",
      "author" : [ "Pierre Baldi", "Peter Sadowski" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Baldi and Sadowski.,? \\Q2014\\E",
      "shortCiteRegEx" : "Baldi and Sadowski.",
      "year" : 2014
    }, {
      "title" : "Understanding dropout",
      "author" : [ "Pierre Baldi", "Peter J Sadowski" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Baldi and Sadowski.,? \\Q2013\\E",
      "shortCiteRegEx" : "Baldi and Sadowski.",
      "year" : 2013
    }, {
      "title" : "Differential sparse coding",
      "author" : [ "David M Bradley", "J Andrew Bagnell" ],
      "venue" : null,
      "citeRegEx" : "Bradley and Bagnell.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bradley and Bagnell.",
      "year" : 2008
    }, {
      "title" : "Dropout distillation",
      "author" : [ "Samuel Rota Bulò", "Lorenzo Porzi", "Peter Kontschieder" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Bulò et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bulò et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout training for support vector machines",
      "author" : [ "Ning Chen", "Jun Zhu", "Jianfei Chen", "Bo Zhang" ],
      "venue" : "In Proceedings Twenty-Eighth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout as a bayesian approximation: Insights and applications",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani" ],
      "venue" : "In Deep Learning Workshop,",
      "citeRegEx" : "Gal and Ghahramani.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2015
    }, {
      "title" : "A theoretically grounded application of dropout in recurrent neural networks",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gal and Ghahramani.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Dropout rademacher complexity of deep neural networks",
      "author" : [ "Wei Gao", "Zhi-Hua Zhou" ],
      "venue" : "arXiv preprint arXiv:1402.3811,",
      "citeRegEx" : "Gao and Zhou.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gao and Zhou.",
      "year" : 2014
    }, {
      "title" : "On the inductive bias of dropout",
      "author" : [ "David P Helmbold", "Philip M Long" ],
      "venue" : "arXiv preprint arXiv:1412.4736,",
      "citeRegEx" : "Helmbold and Long.,? \\Q2014\\E",
      "shortCiteRegEx" : "Helmbold and Long.",
      "year" : 2014
    }, {
      "title" : "Fundamental differences between dropout and weight decay in deep networks",
      "author" : [ "David P Helmbold", "Philip M Long" ],
      "venue" : "arXiv preprint arXiv:1602.04484,",
      "citeRegEx" : "Helmbold and Long.,? \\Q2016\\E",
      "shortCiteRegEx" : "Helmbold and Long.",
      "year" : 2016
    }, {
      "title" : "A practical guide to training restricted boltzmann machines. Momentum",
      "author" : [ "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hinton.",
      "year" : 2010
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "To drop or not to drop: Robustness, consistency and differential privacy properties of dropout",
      "author" : [ "Prateek Jain", "Vivek Kulkarni", "Abhradeep Thakurta", "Oliver Williams" ],
      "venue" : "arXiv preprint arXiv:1503.02031,",
      "citeRegEx" : "Jain et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational dropout and the local reparameterization trick",
      "author" : [ "Diederik P Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy" ],
      "venue" : "In Proceedings of ACL-2016,",
      "citeRegEx" : "Ma and Hovy.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Schmidhuber.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2015
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1412.6806,",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2004
    }, {
      "title" : "Improving neural networks with dropout",
      "author" : [ "Nitish Srivastava" ],
      "venue" : "PhD thesis, University of Toronto,",
      "citeRegEx" : "Srivastava.,? \\Q2013\\E",
      "shortCiteRegEx" : "Srivastava.",
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological), pp",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "On the stability of inverse problems",
      "author" : [ "Andrey Nikolayevich Tikhonov" ],
      "venue" : "In Dokl. Akad. Nauk SSSR,",
      "citeRegEx" : "Tikhonov.,? \\Q1943\\E",
      "shortCiteRegEx" : "Tikhonov.",
      "year" : 1943
    }, {
      "title" : "Dropout training as adaptive regularization",
      "author" : [ "Stefan Wager", "Sida Wang", "Percy S Liang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Wager et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2013
    }, {
      "title" : "Altitude training: Strong bounds for single-layer dropout",
      "author" : [ "Stefan Wager", "William Fithian", "Sida Wang", "Percy S Liang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wager et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast dropout training",
      "author" : [ "Sida Wang", "Christopher Manning" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Wang and Manning.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang and Manning.",
      "year" : 2013
    }, {
      "title" : "We guess one of the possible reasons is that we used different library for implementation.)",
      "author" : [ "Srivastava" ],
      "venue" : null,
      "citeRegEx" : "Srivastava,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava",
      "year" : 2014
    }, {
      "title" : "EFFECT OF EXPECTATION-LINEARIZATION RATE λ Table 4 illustrates the detailed results of the experiments on the effect of λ. For MNIST, it lists the error rates under different λ values for six different network architectures. For two datasets of CIFAR",
      "author" : [ "Srivastava" ],
      "venue" : null,
      "citeRegEx" : "Srivastava,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Deep neural networks (DNNs, e.g., LeCun et al., 2015; Schmidhuber, 2015), if trained properly, have been demonstrated to significantly improve the benchmark performances in a wide range of application domains.",
      "startOffset" : 21,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness.",
      "startOffset" : 98,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness.",
      "startOffset" : 98,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "In a nutshell, dropout randomly “drops” neural units during training as a means to prevent feature co-adaptation—a sign of overfitting (Hinton et al., 2012).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 13,
      "context" : "Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012; Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective.",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness. In a nutshell, dropout randomly “drops” neural units during training as a means to prevent feature co-adaptation—a sign of overfitting (Hinton et al., 2012). Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012; Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective. In their pioneering work, Hinton et al. (2012) and Srivastava et al.",
      "startOffset" : 99,
      "endOffset" : 718
    }, {
      "referenceID" : 11,
      "context" : "A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness. In a nutshell, dropout randomly “drops” neural units during training as a means to prevent feature co-adaptation—a sign of overfitting (Hinton et al., 2012). Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012; Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective. In their pioneering work, Hinton et al. (2012) and Srivastava et al. (2014) interpreted dropout as an extreme form of model combination (aka.",
      "startOffset" : 99,
      "endOffset" : 747
    }, {
      "referenceID" : 5,
      "context" : "Other studies (Chen et al., 2014; Helmbold & Long, 2014; Wager et al., 2014) focus on the effect of the dropout noise on models with shallow architectures.",
      "startOffset" : 14,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "Other studies (Chen et al., 2014; Helmbold & Long, 2014; Wager et al., 2014) focus on the effect of the dropout noise on models with shallow architectures.",
      "startOffset" : 14,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "We noted in passing that there are also some work (Kingma et al., 2015; Gal & Ghahramani, 2015; 2016) trying to understand dropout from the Bayesian perspective.",
      "startOffset" : 50,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Wager et al. (2013) treated dropout as an adaptive regularizer for generalized linear models (GLMs).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "Wager et al. (2013) treated dropout as an adaptive regularizer for generalized linear models (GLMs). Helmbold & Long (2016) discussed the differences between dropout and traditional weight decay regularization.",
      "startOffset" : 0,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "Wager et al. (2013) treated dropout as an adaptive regularizer for generalized linear models (GLMs). Helmbold & Long (2016) discussed the differences between dropout and traditional weight decay regularization. In terms of statistical learning theory, Gao & Zhou (2014) studied the Rademacher complexity of different types of dropout, showing that dropout is able to reduce the Rademacher complexity polynomially for shallow neural networks (with one or no hidden layers) and exponentially for deep neural networks.",
      "startOffset" : 0,
      "endOffset" : 270
    }, {
      "referenceID" : 13,
      "context" : "For instance, Jain et al. (2015) studied the ability of dropout training to escape local minima, hence leading to reduced model bias.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "The goal of the stochastic training procedure of dropout can be understood as minimizing an expected loss function, after marginalizing out the dropout variables (Srivastava, 2013; Wang & Manning, 2013).",
      "startOffset" : 162,
      "endOffset" : 202
    }, {
      "referenceID" : 25,
      "context" : "Dropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al.",
      "startOffset" : 86,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "Dropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "Dropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al.",
      "startOffset" : 141,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "Dropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al., 2004), among which the max-norm regularization — that constrains the norm of the incoming weight matrix to be bounded by some constant — was found to be especially useful for dropout (Srivastava, 2013; Srivastava et al.",
      "startOffset" : 210,
      "endOffset" : 231
    }, {
      "referenceID" : 22,
      "context" : ", 2004), among which the max-norm regularization — that constrains the norm of the incoming weight matrix to be bounded by some constant — was found to be especially useful for dropout (Srivastava, 2013; Srivastava et al., 2014).",
      "startOffset" : 185,
      "endOffset" : 228
    }, {
      "referenceID" : 12,
      "context" : "(2016) combined dropout with knowledge distillation methods (Hinton et al., 2015) to better approximate the averaging processing of the left-hand side.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Bulò et al. (2016) combined dropout with knowledge distillation methods (Hinton et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Definition 3 (Closeness, (Andreas et al., 2015)).",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Our definition of closeness is similar to that in Andreas et al. (2015), who used this notion to analyze self-normalized log-linear models.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "We applied the same data preprocessing procedure as in Srivastava et al. (2014). To make a thorough comparison and provide experimental evidence on how the expectationlinearization interacts with the predictive power of the learned model, we perform experiments of Monte Carlo (MC) dropout, which approximately computes the final prediction (left-hand side of (3)) via Monte Carlo sampling, w/o the proposed regularizer.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 22,
      "context" : "We applied the same data preprocessing procedure as in Srivastava et al. (2014). To make a thorough comparison and provide experimental evidence on how the expectationlinearization interacts with the predictive power of the learned model, we perform experiments of Monte Carlo (MC) dropout, which approximately computes the final prediction (left-hand side of (3)) via Monte Carlo sampling, w/o the proposed regularizer. In the case of MC dropout, we average m = 100 predictions using randomly sampled configurations. In addition, the network architectures and hyper-parameters for each experiment setup are the same as those in Srivastava et al. (2014), unless we explicitly claim to use different ones.",
      "startOffset" : 55,
      "endOffset" : 654
    }, {
      "referenceID" : 17,
      "context" : "1 MNIST The MNIST dataset (LeCun et al., 1998) consists of 70,000 handwritten digit images of size 28×28, where 60,000 images are used for training and the rest for testing.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "2 CIFAR-10 AND CIFAR-100 The CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 60,000 color images of size 32× 32, drawn from 10 and 100 categories, respectively.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "The neural network architecture we used for these two datasets has 3 convolutional layers, followed by two fully-connected (dense) hidden layers (again, same as that in Srivastava et al. (2014)).",
      "startOffset" : 169,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "4 COMPARISON WITH DROPOUT DISTILLATION To make a thorough empirical comparison with the recently proposed Dropout Distillation method (Bulò et al., 2016), we also evaluate our regularization method on CIFAR-10 and CIFAR-100 datasets with the All Convolutional Network (Springenberg et al.",
      "startOffset" : 134,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : ", 2016), we also evaluate our regularization method on CIFAR-10 and CIFAR-100 datasets with the All Convolutional Network (Springenberg et al., 2014) (AllConv).",
      "startOffset" : 122,
      "endOffset" : 149
    } ],
    "year" : 2017,
    "abstractText" : "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout’s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.",
    "creator" : "TeX"
  }
}