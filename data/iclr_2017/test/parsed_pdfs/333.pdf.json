{
  "name" : "333.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "WHAT DOES IT TAKE TO GENERATE NATURAL TEXTURES?",
    "authors" : [ "Ivan Ustyuzhaninov", "Wieland Brendel", "Leon Gatys", "Matthias Bethge" ],
    "emails" : [ "first.last@bethgelab.org" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "During the last two years several different approaches towards natural image generation have been suggested, among them generative adversarial networks (Goodfellow et al., 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al., 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016). The latter approach has been particularly groundbreaking for artistic style transfer and natural texture generation (e.g. Gatys et al., 2015a;b) and has the potential to uncover the regularities that supervisedly trained deep neural networks infer from natural images.\nFor the sake of clarity and concreteness, this paper will focus on natural texture synthesis. Parametric texture models aim to uniquely describe each texture by a set of statistical measurements that are taken over the spatial extent of the image. Each image with the same spatial summary statistics should be perceived as the same texture. Consequently, synthesizing a texture corresponds to finding a new image that reproduces the summary statistics inferred from the reference texture. Starting from Nth-order joint histograms of the pixels by Julesz (1962), many different statistical measures have been proposed (see e.g. Heeger & Bergen, 1995; Portilla & Simoncelli, 2000). The quality of the synthesized textures is usually determined by human inspection; the synthesis is successful if a human observer cannot tell the reference texture from the synthesized ones.\nThe current state of the art in parametric texture modeling (Gatys et al., 2015a) employs the hierarchical image representation in a deep 19-layer convolutional network (Simonyan & Zisserman (2014); in the following referred to as VGG network) that was trained on object recognition in natural images(Russakovsky et al. (2015)). In this model textures are described by the raw correlations between feature activations in response to the texture image from a collection of network layers (see section 5 for details). Since its initial reception several papers explored which additional elements or constraints can further increase the perceptual quality of the generated textures (Berger & Memisevic, 2016; Liu et al., 2016; Aittala et al., 2016). In this work we go the opposite way and ask which elements of the original texture synthesis algorithm (Gatys et al., 2015a) are absolutely indispensable.\nIn particular two aspects have been deemed critical for natural texture synthesis: the hierarchical multi-layer representation of the textures, and the supervised training of the feature spaces. Here we show that neither aspect is imperative for texture modeling and that in fact a single convolutional layer with random features can synthesize textures that often rival the perceptual quality of Gatys et al. (2015a). This is in contrast to earlier reports (Gatys et al., 2015a) that suggested that networks with random weights fail to generate perceptually interesting images. We suggest that this discrepancy originates from a more elaborate tuning of the optimization procedure (see section 4).\nOur main contributions are:\n• We present a strong minimal baseline for parametric texture synthesis that solely relies on a single-layer network and random, data-independent filters. • We show that textures synthesized from the baseline are of high quality and often rival\nstate-of-the-art approaches, suggesting that the depth and the pre-training of multi-layer image representations are not as indispensable for natural image generation as has previously been thought. • We test and compare a wide range of single-layer architectures with different filter-sizes and\ndifferent types of filters (random, hand-crafted and unsupervisedly learnt filters) against the state-of-the-art texture model by Gatys et al. (2015a). • We utilize a quantitative texture quality measure based on the synthesis loss in the VGG-\nbased model (Gatys et al., 2015a) to replace the common-place evaluation of texture models through qualitative human inspection. • We discuss a formal generalization of maximum entropy models to account for the natural\nvariability of textures with limited spatial extent."
    }, {
      "heading" : "2 CONVOLUTIONAL NEURAL NETWORK",
      "text" : "If not mentioned otherwise, all our models employ single-layer CNNs with standard rectified linear units (ReLUs) and convolutions with stride one, no bias and padding (f − 1)/2 where f is the filter-size. This choice ensures that the spatial dimension of the output feature maps is the same as the input. All networks except the last one employ filters of size 11× 11× 3 (filter width × filter height × no. of input channels), but the number of feature maps as well as the selection of the filters differ:\n• Fourier-363: Each color channel (R, G, B) is filtered separately by each element Bi ∈ R11×11 of the 2D Fourier basis (11×11 = 121 feature maps/channel), yielding 3·121 = 363 feature maps in total. More concretely, each filter can be described as the tensor product Bi ⊗ ek where the elements of the unit-norm ek ∈ R3 are all zero except one. • Fourier-3267: All color channels (R, G, B) are filtered simultaneously by each element Bi of the 2D Fourier basis but with different weighting terms wR, wG, wB ∈ [1, 0,−1], yielding 3 · 3 · 3 · 121 = 3267 feature maps in total. More concretely, each filter can be described by the tensor product Bi ⊗ [wR, wG, wB ]. • Kmeans-363: We randomly sample and whiten 1e7 patches of size 11 × 11 from the\nImagenet dataset (Russakovsky et al., 2015), partition the patches into 363 clusters using k-means (Rubinstein et al., 2009), and use the cluster means as convolutional filters. • Kmeans-3267: Same as Kmeans-363 but with 3267 clusters. • Kmeans-NonWhite-363/3267: Same as Kmeans-363/3267 but without whitening of the\npatches. • Kmeans-Sample-363/3267: Same as Kmeans-363/3267, but patches are only sampled\nfrom the target texture. • PCA-363: We randomly sample 1e7 patches of size 11 × 11 from the Imagenet dataset\n(Russakovsky et al., 2015), vectorize each patch, perform PCA and use the set of principal axes as convolutional filters. • Random-363: Filters are drawn from a uniform distribution according to (Glorot & Bengio,\n2010), 363 feature maps in total. • Random-3267: Same as Random-363 but with 3267 feature maps.\nOriginal Fourier-363, 11x11 K-means-363, 11x11 K-samples-363, 11x11 K-NW-363, 11x11 Random-363, 11x11 PCA-363, 11x11\nGatys et al. Fourier-3267, 11x11 K-means-3267, 11x11 K-samples-3267, 11x11 K-NW-3267, 11x11 Random-3267, 11x11 Random multi-scale\nThe networks were implemented in Lasagne (Dieleman et al., 2015; Theano Development Team, 2016). We remove the DC component of the inputs by subtracting the mean intensity in each color channel (estimated over the Imagenet dataset (Russakovsky et al., 2015))."
    }, {
      "heading" : "3 TEXTURE MODEL",
      "text" : "The texture model closely follows (Gatys et al., 2015a). In essence, to characterise a given vectorised texture x ∈ RM , we first pass x through the convolutional layer and compute the output activations. The output can be understood as a non-linear filter bank, and thus its activations form a set of filtered images (so-called feature maps). For N distinct feature maps, the rectified output activations can be\ndescribed by a matrix F ∈ RN×M . To capture the stationary structure of the textures, we compute the covariances (or, more precisely, the Gramian matrix) G ∈ RN×N between the feature activations F by averaging the outer product of the point-wise feature vectors,\nGij = 1\nM M∑ m=1 FimFjm. (1)\nWe will denote G(x) as the Gram matrix of the feature activations for the input x. To determine the relative distance between two textures x and y we compute the euclidean distance of the normalized Gram matrices,\nd(x,y) = 1√∑\nm,n Gmn(x)2 √∑ m,n Gmn(y)2 N∑ i,j=1 (Gij(x)−Gij(y))2 . (2)\nTo compare with the distance in the raw pixel values, we compute\ndp(x,y) = 1√∑ m x2m √∑ m y2m N∑ i=1 (xi − yi)2 . (3)"
    }, {
      "heading" : "4 TEXTURE SYNTHESIS",
      "text" : "To generate a new texture we start from a uniform noise image (in the range [0, 1]) and iteratively optimize it to match the Gram matrix of the reference texture. More precisely, let G(x) be the Gram matrix of the reference texture. The goal is to find a synthesised image x̃ such that the squared distance between G(x) and the Gram matrix G(x̃) of the synthesized image is minimized, i.e.\nx̃ = argmin y∈RM E(y), (4)\nE(y) = 1∑N\ni,j=1Gij(x) 2 N∑ i,j=1 ( Gij(x)−Gij(y) )2 . (5)\nThe gradient ∂E(y)/∂y of the reconstruction error with respect to the image can readily be computed using standard backpropagation, which we then use in conjunction with the L-BFGS-B algorithm (Jones et al., 2001–) to solve (4). We leave all parameters of the optimization algorithm at their default value except for the maximum number of iterations (2000), and add a box constraints with range [0, 1]. In addition, we scale the loss and the gradients by a factor of 107 in order to avoid early stopping of the optimization algorithm."
    }, {
      "heading" : "5 TEXTURE EVALUATION",
      "text" : "Evaluating the quality of the synthesized textures is traditionally performed by human inspection. Optimal texture synthesis should generate samples that humans perceive as being the same texture as the reference. The high quality of the synthesized textures by (Gatys et al., 2015a) suggests that the summary statistics from multiple layers of VGG can approximate the perceptual metric of humans. Even though the VGG texture representation is not perfect, this allows us to utilize these statistics as a more objective quantification of texture quality.\nFor all details of the VGG-based texture model see (Gatys et al., 2015a). Here we use the standard 19-layer VGG network (Simonyan & Zisserman, 2014) with pretrained weights and average- instead of max-pooling1. We compute a Gram matrix on the output of each convolutional layer that follows a pooling layer. Let G`(.) be the Gram matrix on the activations of the `-th layer and\nE`(y) = 1∑N\ni,j=1G ` ij(x) 2 N∑ i,j=1 ( G`ij(x)−G`ij(y) )2 . (6)\nthe corresponding relative reconstruction cost. The total reconstruction cost is then defined as the average distance between the reference Gram matrices and the synthesized ones, i.e.\nE(y) = 1\n5 5∑ `=1 E`(y). (7)\nThis cost is reported on top of each synthesised texture in Figures 4. To visually evaluate samples from our single- and multi-scale model against the VGG-based model (Gatys et al., 2015a), we additionally synthesize textures from VGG by minimizing (7) using L-BFGS-B as in section 4."
    }, {
      "heading" : "6 RESULTS",
      "text" : "In Fig. 1 we show textures synthesised from two random single- and multi-scale models, as well as eight other non-random single-layer models for three different source images (top left). For\n1https://github.com/Lasagne/Recipes/blob/master/modelzoo/vgg19.py as accessed on 12.05.2016.\ncomparison, we also plot samples generated from the VGG model by Gatys et al. (Gatys et al., 2015a) (bottom left). There are roughly two groups of models: those with a small number of feature maps (363, top row), and those with a large number of feature maps (3267, bottom row). Only the multi-scale model employs 1024 feature maps. Within each group, we can differentiate models for which the filters are unsupervisedly trained on natural images (e.g. sparse coding filters from k-means), principally devised filter banks (e.g. 2D Fourier basis) and completely random filters (see sec. 2 for all details). All single-layer networks, except for multi-scale, feature 11× 11 filters. Remarkably, despite the small spatial size of the filters, all models capture much of the small- and mid-scale structure of the textures, in particular if the number of feature maps is large. Notably, the scale of these structures extends far beyond the receptive fields of the single units (see e.g. the pebble texture). We further observe that a larger number of feature maps generally increases the perceptual quality of the generated textures. Surprisingly, however, completely random filters perform on par or better then filters that have been trained on the statistics of natural images. This is particularly true for the multi-scale model that clearly outperforms the single-scale models on all textures. The captured structures in the multi-scale model are generally much larger and often reach the full size of the texture (see e.g. the wall).\nWhile the above results show that for natural texture synthesis one neither needs a hierarchical deep network architecture with spatial pooling nor filters that are adapted to the statistics of natural images, we now focus on the aspects that are crucial for high quality texture synthesis. First, we evaluate whether the success of the random multi-scale network arises from the combination of filters on multiple scales or whether it is simply the increased size of its largest receptive fields (55× 55 vs. 11× 11) that leads to the improvement compared to the single-scale model. Thus, to investigate the influence of the spatial extend of the filters and the importance of combining multiple filter sizes in one model, we generate textures from multiple single-scale models, where each model has the same number of random filters as the multi-scale model (1024) but only uses filters from a single scale of the multi-scale model (Fig. 2). We find that while 3× 3 filters mainly capture the marginal distribution of the color channels, larger filters like 11 × 11 model small- to mid-scale structures (like small stones) but miss more long-range structures (larger stones are not well separated). Very large filters like 55× 55, on the other hand, are capable of modeling long-range structures but then miss much of the small- to midscale statistics (like the texture of the stone). Therefore we conclude that the combination of different scales in the multi-scale network is important for good texture synthesis since it allows to simultaneously model small-, mid- and long-range correlations of the textures. Finally we note that a further indispensable component for good texture models are the non-linearities: textures synthesised the multi-scale model without ReLU (Fig. 2, right column) are unable to capture the statistical dependencies of the texture.\nThe perceptual quality of the textures generated from models with only a single layer and random filters is quite remarkable and surpasses parametric methods like Portilla & Simoncelli (2000) that have been state-of-the-art two years ago (before the use of DNNs). The multi-scale model often rivals the current state of the art (Gatys et al., 2015a) as we show in Fig. 4 where we compare samples synthesized from 20 different textures for the random single- and multi-scale model, as well as VGG. The multi-scale model generates very competitive samples in particular for textures with extremely regular structures across the whole image (e.g. for the brick wall, the grids or the scales). In part, this effect can be attributed to the more robust optimization of the single-layer model that is less prone to local minima then the optimization in deeper models. This can be seen by initializing the VGG-based synthesis with textures from the single-layer model, which consistently yields superior synthesis results (see Appendix A, Fig. 5). In addition, for a few textures such as the grid structures, the VGG-based loss is paradoxically lower for samples from the multi-scale model then for the VGG-based model (which directly optimized the VGG-based loss). This suggests that the naive synthesis performed here favors images that are perceptually similar to the reference texture and thus looses variability (see sec. 7 for further discussion). Nonetheless, samples from the single-layer model still exhibit large perceptual differences, see Fig. 3. The VGG-based loss (7) appears to generally be an acceptable approximation of the perceptual differences between the reference and the synthesized texture. Only for a few textures, especially those with very regular men-made structures (e.g. the wall or the grids), the VGG-based loss fails to capture the perceptual advantage of the multi-scale synthesis."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "We proposed a generative model of natural textures based on a single-layer convolutional neural network with completely random filters and showed that the model is able to qualitatively capture the perceptual differences between natural textures. Samples from the model often rival the current state-of-the-art (Gatys et al., 2015a) (Fig. 4, third vs fourth row), even though the latter relies on a high-performance deep neural network with features that are tuned to the statistics of natural images. Seen more broadly, this finding suggests that natural image generation does not necessarily depend on deep hierarchical representations or on the training of the feature maps. Instead, for texture synthesis, both aspects rather seem to serve as fine-tuning of the image representation.\nOne concern about the proposed single-layer multi-scale model is its computational inefficiency since it involves convolutions with spatially large filters (up to 55× 55). A more efficient way to achieve receptive fields of similar size would be to use a hierarchical multi-layer net. We conducted extensive experiments with various hierarchical architectures and while the synthesis is indeed significantly faster, the quality of the synthesized textures does not improve compared to a single-layer model. Thus for a minimal model of natural textures, deep hierarchical representations are not necessary but they can improve the efficiency of the texture synthesis.\nOur results clearly demonstrate that Gram matrices computed from the feature maps of convolutional neural networks generically lead to useful summary statistics for texture synthesis. The Gram matrix on the feature maps transforms the representations from the convolutional neural network into a stationary feature space that captures the pairwise correlations between different features. If the number of feature maps is large, then the local structures in the image are well preserved in the projected space and the overlaps of the convolutional filtering add additional constraints. At the same time, averaging out the spatial dimensions yields sufficient flexibility to generate entirely new textures that differ from the reference on a patch by patch level, but still share much of the small- and long-range statistics.\nThe success of shallow convolutional networks with random filters in reproducing the structure of the reference texture is remarkable and indicates that they can be useful for parametric texture synthesis. Besides reproducing the stationary correlation structure of the reference image (\"perceptual similarity\") another desideratum of a texture synthesis is to exhibit a large variety between different samples generated from the same given image (\"variability\"). Hence, synthesis algorithms need to balance perceptual similarity and variability. This balance is determined by a complex interplay between the choice of summary statistics and the optimization algorithm used. For example the stopping criterion of the optimization algorithm can be adjusted to trade perceptual similarity for larger variability.\nFinding the right balance between perceptual similarity and variability is challenging because we are currently lacking robust measures of these quantities. In this work we introduced VGG-loss as a measure of perceptual similarity, and, even though, it works much better than other common measures such as Structural Similarity Index (SSIM, Wang et al., 2004, see Appendix A, Figure 6) or Euclidean distance in the pixel space (not shown), it is still not perfect (Figure 4). Measuring variability is probably even more difficult: in principle it requires measuring the entropy of generated samples, which is intractable in a high-dimensional space. A different approach could be based on a psychophysical assessment of generated samples. For example, we could use an inpainting task (illustrated in Appendix A, Figure 7) to make human observers decide between actual texture patches and inpaited ones. Performance close to a chance-level would indicate that the texture model produces variable enough samples to capture the diversity of actual patches. The further exploration of variability measures lies, however, beyond the scope of this work.\nIn this paper we focused on maximizing perceptual similarity only, and it is worth pointing out that additional efforts will be necessary to find an optimal trade-off between perceptual similarity and variability. For the synthesis of textures from the random models considered here, the trade-off leans more towards perceptual similarity in comparison to Gatys et al. (2015a)(due to the simpler optimization) which also explains the superior performance on some samples. In fact, we found some anecdotal evidence (not shown) in deeper multi-layer random CNNs where the reference texture was exactly reconstructed during the synthesis. From a theoretical point of view this is likely a finite size effect which does not necessarily constitute a failure of the chosen summary statistics: for finite size images it is well possible that only the reference image can exactly reproduce all the summary\nstatistics. Therefore, in practice, the Gram matrices are not treated as hard constraints but as soft constraints only. More generally, we do not expect a perceptual distance metric to assign exactly zero to a random pair of patches from the same texture. Instead, we expect it to assign small values for pairs from the same texture, and large values for patches from different textures. Therefore, the selection of constraints is not sufficient to characterize a texture synthesis model but only determines the exact minima of the objective function (which are sought for by the synthesis). If we additionally consider images with small but non-zero distance to the reference statistics, then the set of equivalent textures increases substantially, and the precise composition of this set becomes critically dependent on the perceptual distance metric.\nMathematically, parametric texture synthesis models are described as ergodic random fields that have maximum entropy subject to certain constraints Zhu et al. (1997); Bruna & Mallat (2013); Zhu et al. (2000) (MaxEnt framework). Practical texture synthesis algorithms, however, always deal with finite size images. As discussed above, two finite-size patches from the same ergodic random field will almost never feature the exact same summary statistics. This additional uncertainty in estimating the constraints on finite length processes is not thoroughly accounted for by the MaxEnt framework (see discussion on its “ad hockeries” by Jaynes (Jaynes (1982))). Thus, a critical difference of practical implementations of texture synthesis algorithms from the conceptual MaxEnt texture modeling framework is that they genuinely allow a small mismatch in the constraints. Accordingly, specifying the summary statistics is not sufficient but a comprehensive definition of a texture synthesis model should specify:\n1. A metric d(x,y) that determines the distance between any two arbitrary textures x,y.\n2. A bipartition Px of the image space that determines which images are considered perceptually equivalent to a reference texture x. A simple example for such a partition is the -environment U (y) := {y : d(y,x) < } and its complement.\nThis definition is relevant for both under- as well as over-constrained models, but its importance becomes particularly obvious for the latter. According to the Minimax entropy principle for texture modeling suggested by Zhu et al Zhu et al. (1997), as many constraints as possible should be used to reduce the (Kullback-Leibler) divergence between the true texture model and its estimate. However, for finite spatial size, the synthetic samples become exactly equivalent to the reference texture (up to shifts) in the limit of sufficiently many independent constraints. In contrast, if we explicitly allow for a small mismatch between the summary statistics of the reference image and the synthesized textures, then the set of possible textures does not constitute a low-dimensional manifold but rather a small volume within the pixel space. Alternatively, instead of introducing an -environment it is also possible to extent the MaxEnt framework to allow for variability in the summary statistics (Joan Bruna, personal communication). It will be interesting to compare in the future to what extent the difference between the two approaches can lead to differences in the perceptual appearance of the textures.\nTaken together we have shown that simple single-layer CNNs with random filters can serve as the basis for excellent texture synthesis models that outperform previous hand-crafted synthesis models and sometimes even rivals the current state-of-the-art. This finding repeals previous observations that suggested a critical role for the multi-layer representations in trained deep networks for natural texture generation. On the other hand, it is not enough to just use sufficiently many constraints as one would predict from the MaxEnt framework. Instead, for the design of good texture synthesis algorithms it will be crucial to find distance measures for which the -environment around the reference texture leads to perceptually satisfying results. In this way, building better texture synthesis models is inherently related to better quantitative models of human perception."
    }, {
      "heading" : "A APPENDIX",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Reflectance modeling by neural texture synthesis",
      "author" : [ "M. Aittala", "T. Aila", "J. Lehtinen" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "Aittala et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Aittala et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating long-range consistency in cnn-based texture generation",
      "author" : [ "G. Berger", "R. Memisevic" ],
      "venue" : "Jun 2016",
      "citeRegEx" : "Berger and Memisevic.,? \\Q2016\\E",
      "shortCiteRegEx" : "Berger and Memisevic.",
      "year" : 2016
    }, {
      "title" : "Audio texture synthesis with scattering moments",
      "author" : [ "Joan Bruna", "Stéphane Mallat" ],
      "venue" : "CoRR, abs/1311.0407,",
      "citeRegEx" : "Bruna and Mallat.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bruna and Mallat.",
      "year" : 2013
    }, {
      "title" : "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
      "author" : [ "X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Texture synthesis using convolutional neural networks",
      "author" : [ "L.A. Gatys", "A.S. Ecker", "M. Bethge" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural algorithm of artistic style",
      "author" : [ "L.A. Gatys", "A.S. Ecker", "M. Bethge" ],
      "venue" : "Aug 2015b. URL http://arxiv.org/abs/1508.06576",
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10).,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Generative Adversarial Networks",
      "author" : [ "I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Pyramid-based texture analysis/synthesis",
      "author" : [ "David J. Heeger", "James R. Bergen" ],
      "venue" : "In Proceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Techniques,",
      "citeRegEx" : "Heeger and Bergen.,? \\Q1995\\E",
      "shortCiteRegEx" : "Heeger and Bergen.",
      "year" : 1995
    }, {
      "title" : "On the rationale of maximum-entropy methods",
      "author" : [ "E.T. Jaynes" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Jaynes.,? \\Q1982\\E",
      "shortCiteRegEx" : "Jaynes.",
      "year" : 1982
    }, {
      "title" : "Perceptual losses for real-time style transfer and super-resolution",
      "author" : [ "Justin Johnson", "Alexandre Alahi", "Li Fei-Fei" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "SciPy: Open source scientific tools for Python, 2001",
      "author" : [ "Eric Jones", "Travis Oliphant", "Pearu Peterson" ],
      "venue" : "URL http://www.scipy.org/. [Online;",
      "citeRegEx" : "Jones et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2016
    }, {
      "title" : "Visual pattern discrimination",
      "author" : [ "B. Julesz" ],
      "venue" : "IRE Transactions on Information Theory,",
      "citeRegEx" : "Julesz.,? \\Q1962\\E",
      "shortCiteRegEx" : "Julesz.",
      "year" : 1962
    }, {
      "title" : "Texture synthesis through convolutional neural networks and spectrum constraints",
      "author" : [ "G. Liu", "Y. Gousseau", "G. Xia" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "A parametric texture model based on joint statistics of complex wavelet coefficients",
      "author" : [ "Javier Portilla", "Eero P. Simoncelli" ],
      "venue" : "Int. J. Comput. Vision,",
      "citeRegEx" : "Portilla and Simoncelli.,? \\Q2000\\E",
      "shortCiteRegEx" : "Portilla and Simoncelli.",
      "year" : 2000
    }, {
      "title" : "Efficient implementation of the k-svd algorithm using batch orthogonal matching pursuit",
      "author" : [ "Ron Rubinstein", "Michael Zibulevsky", "Michael Elad" ],
      "venue" : null,
      "citeRegEx" : "Rubinstein et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rubinstein et al\\.",
      "year" : 2009
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Texture Networks: Feedforward Synthesis of Textures and Stylized Images",
      "author" : [ "Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky" ],
      "venue" : "[cs],",
      "citeRegEx" : "Ulyanov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ulyanov et al\\.",
      "year" : 2016
    }, {
      "title" : "Pixel Recurrent Neural Networks. ArXiv e-prints, January 2016a",
      "author" : [ "A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional Image Generation with PixelCNN Decoders",
      "author" : [ "A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Image quality assessment: from error visibility to structural similarity",
      "author" : [ "Zhou Wang", "Alan C Bovik", "Hamid R Sheikh", "Eero P Simoncelli" ],
      "venue" : "IEEE transactions on image processing,",
      "citeRegEx" : "Wang et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2004
    }, {
      "title" : "Minimax entropy principle and its application to texture modeling",
      "author" : [ "Song Chun Zhu", "Ying Nian Wu", "David Mumford" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Zhu et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 1997
    }, {
      "title" : "Exploring texture ensembles by efficient markov chain monte carlo-toward a ’trichromacy’ theory of texture",
      "author" : [ "Song Chun Zhu", "Xiuwen Liu", "Ying Nian Wu" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "1 INTRODUCTION During the last two years several different approaches towards natural image generation have been suggested, among them generative adversarial networks (Goodfellow et al., 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al.",
      "startOffset" : 167,
      "endOffset" : 211
    }, {
      "referenceID" : 3,
      "context" : "1 INTRODUCTION During the last two years several different approaches towards natural image generation have been suggested, among them generative adversarial networks (Goodfellow et al., 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al.",
      "startOffset" : 167,
      "endOffset" : 211
    }, {
      "referenceID" : 10,
      "context" : ", 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 164
    }, {
      "referenceID" : 17,
      "context" : ", 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "Since its initial reception several papers explored which additional elements or constraints can further increase the perceptual quality of the generated textures (Berger & Memisevic, 2016; Liu et al., 2016; Aittala et al., 2016).",
      "startOffset" : 163,
      "endOffset" : 229
    }, {
      "referenceID" : 0,
      "context" : "Since its initial reception several papers explored which additional elements or constraints can further increase the perceptual quality of the generated textures (Berger & Memisevic, 2016; Liu et al., 2016; Aittala et al., 2016).",
      "startOffset" : 163,
      "endOffset" : 229
    }, {
      "referenceID" : 2,
      "context" : ", 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al., 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016). The latter approach has been particularly groundbreaking for artistic style transfer and natural texture generation (e.g. Gatys et al., 2015a;b) and has the potential to uncover the regularities that supervisedly trained deep neural networks infer from natural images. For the sake of clarity and concreteness, this paper will focus on natural texture synthesis. Parametric texture models aim to uniquely describe each texture by a set of statistical measurements that are taken over the spatial extent of the image. Each image with the same spatial summary statistics should be perceived as the same texture. Consequently, synthesizing a texture corresponds to finding a new image that reproduces the summary statistics inferred from the reference texture. Starting from Nth-order joint histograms of the pixels by Julesz (1962), many different statistical measures have been proposed (see e.",
      "startOffset" : 8,
      "endOffset" : 1106
    }, {
      "referenceID" : 2,
      "context" : ", 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al., 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016). The latter approach has been particularly groundbreaking for artistic style transfer and natural texture generation (e.g. Gatys et al., 2015a;b) and has the potential to uncover the regularities that supervisedly trained deep neural networks infer from natural images. For the sake of clarity and concreteness, this paper will focus on natural texture synthesis. Parametric texture models aim to uniquely describe each texture by a set of statistical measurements that are taken over the spatial extent of the image. Each image with the same spatial summary statistics should be perceived as the same texture. Consequently, synthesizing a texture corresponds to finding a new image that reproduces the summary statistics inferred from the reference texture. Starting from Nth-order joint histograms of the pixels by Julesz (1962), many different statistical measures have been proposed (see e.g. Heeger & Bergen, 1995; Portilla & Simoncelli, 2000). The quality of the synthesized textures is usually determined by human inspection; the synthesis is successful if a human observer cannot tell the reference texture from the synthesized ones. The current state of the art in parametric texture modeling (Gatys et al., 2015a) employs the hierarchical image representation in a deep 19-layer convolutional network (Simonyan & Zisserman (2014); in the following referred to as VGG network) that was trained on object recognition in natural images(Russakovsky et al.",
      "startOffset" : 8,
      "endOffset" : 1615
    }, {
      "referenceID" : 2,
      "context" : ", 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al., 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016). The latter approach has been particularly groundbreaking for artistic style transfer and natural texture generation (e.g. Gatys et al., 2015a;b) and has the potential to uncover the regularities that supervisedly trained deep neural networks infer from natural images. For the sake of clarity and concreteness, this paper will focus on natural texture synthesis. Parametric texture models aim to uniquely describe each texture by a set of statistical measurements that are taken over the spatial extent of the image. Each image with the same spatial summary statistics should be perceived as the same texture. Consequently, synthesizing a texture corresponds to finding a new image that reproduces the summary statistics inferred from the reference texture. Starting from Nth-order joint histograms of the pixels by Julesz (1962), many different statistical measures have been proposed (see e.g. Heeger & Bergen, 1995; Portilla & Simoncelli, 2000). The quality of the synthesized textures is usually determined by human inspection; the synthesis is successful if a human observer cannot tell the reference texture from the synthesized ones. The current state of the art in parametric texture modeling (Gatys et al., 2015a) employs the hierarchical image representation in a deep 19-layer convolutional network (Simonyan & Zisserman (2014); in the following referred to as VGG network) that was trained on object recognition in natural images(Russakovsky et al. (2015)).",
      "startOffset" : 8,
      "endOffset" : 1744
    }, {
      "referenceID" : 4,
      "context" : "Here we show that neither aspect is imperative for texture modeling and that in fact a single convolutional layer with random features can synthesize textures that often rival the perceptual quality of Gatys et al. (2015a). This is in contrast to earlier reports (Gatys et al.",
      "startOffset" : 202,
      "endOffset" : 223
    }, {
      "referenceID" : 4,
      "context" : "Here we show that neither aspect is imperative for texture modeling and that in fact a single convolutional layer with random features can synthesize textures that often rival the perceptual quality of Gatys et al. (2015a). This is in contrast to earlier reports (Gatys et al., 2015a) that suggested that networks with random weights fail to generate perceptually interesting images. We suggest that this discrepancy originates from a more elaborate tuning of the optimization procedure (see section 4). Our main contributions are: • We present a strong minimal baseline for parametric texture synthesis that solely relies on a single-layer network and random, data-independent filters. • We show that textures synthesized from the baseline are of high quality and often rival state-of-the-art approaches, suggesting that the depth and the pre-training of multi-layer image representations are not as indispensable for natural image generation as has previously been thought. • We test and compare a wide range of single-layer architectures with different filter-sizes and different types of filters (random, hand-crafted and unsupervisedly learnt filters) against the state-of-the-art texture model by Gatys et al. (2015a). • We utilize a quantitative texture quality measure based on the synthesis loss in the VGGbased model (Gatys et al.",
      "startOffset" : 202,
      "endOffset" : 1224
    }, {
      "referenceID" : 15,
      "context" : ", 2015), partition the patches into 363 clusters using k-means (Rubinstein et al., 2009), and use the cluster means as convolutional filters.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "The gradient ∂E(y)/∂y of the reconstruction error with respect to the image can readily be computed using standard backpropagation, which we then use in conjunction with the L-BFGS-B algorithm (Jones et al., 2001–) to solve (4). We leave all parameters of the optimization algorithm at their default value except for the maximum number of iterations (2000), and add a box constraints with range [0, 1].",
      "startOffset" : 194,
      "endOffset" : 357
    }, {
      "referenceID" : 4,
      "context" : "comparison, we also plot samples generated from the VGG model by Gatys et al. (Gatys et al., 2015a) (bottom left). There are roughly two groups of models: those with a small number of feature maps (363, top row), and those with a large number of feature maps (3267, bottom row). Only the multi-scale model employs 1024 feature maps. Within each group, we can differentiate models for which the filters are unsupervisedly trained on natural images (e.g. sparse coding filters from k-means), principally devised filter banks (e.g. 2D Fourier basis) and completely random filters (see sec. 2 for all details). All single-layer networks, except for multi-scale, feature 11× 11 filters. Remarkably, despite the small spatial size of the filters, all models capture much of the small- and mid-scale structure of the textures, in particular if the number of feature maps is large. Notably, the scale of these structures extends far beyond the receptive fields of the single units (see e.g. the pebble texture). We further observe that a larger number of feature maps generally increases the perceptual quality of the generated textures. Surprisingly, however, completely random filters perform on par or better then filters that have been trained on the statistics of natural images. This is particularly true for the multi-scale model that clearly outperforms the single-scale models on all textures. The captured structures in the multi-scale model are generally much larger and often reach the full size of the texture (see e.g. the wall). While the above results show that for natural texture synthesis one neither needs a hierarchical deep network architecture with spatial pooling nor filters that are adapted to the statistics of natural images, we now focus on the aspects that are crucial for high quality texture synthesis. First, we evaluate whether the success of the random multi-scale network arises from the combination of filters on multiple scales or whether it is simply the increased size of its largest receptive fields (55× 55 vs. 11× 11) that leads to the improvement compared to the single-scale model. Thus, to investigate the influence of the spatial extend of the filters and the importance of combining multiple filter sizes in one model, we generate textures from multiple single-scale models, where each model has the same number of random filters as the multi-scale model (1024) but only uses filters from a single scale of the multi-scale model (Fig.",
      "startOffset" : 65,
      "endOffset" : 2402
    }, {
      "referenceID" : 4,
      "context" : "comparison, we also plot samples generated from the VGG model by Gatys et al. (Gatys et al., 2015a) (bottom left). There are roughly two groups of models: those with a small number of feature maps (363, top row), and those with a large number of feature maps (3267, bottom row). Only the multi-scale model employs 1024 feature maps. Within each group, we can differentiate models for which the filters are unsupervisedly trained on natural images (e.g. sparse coding filters from k-means), principally devised filter banks (e.g. 2D Fourier basis) and completely random filters (see sec. 2 for all details). All single-layer networks, except for multi-scale, feature 11× 11 filters. Remarkably, despite the small spatial size of the filters, all models capture much of the small- and mid-scale structure of the textures, in particular if the number of feature maps is large. Notably, the scale of these structures extends far beyond the receptive fields of the single units (see e.g. the pebble texture). We further observe that a larger number of feature maps generally increases the perceptual quality of the generated textures. Surprisingly, however, completely random filters perform on par or better then filters that have been trained on the statistics of natural images. This is particularly true for the multi-scale model that clearly outperforms the single-scale models on all textures. The captured structures in the multi-scale model are generally much larger and often reach the full size of the texture (see e.g. the wall). While the above results show that for natural texture synthesis one neither needs a hierarchical deep network architecture with spatial pooling nor filters that are adapted to the statistics of natural images, we now focus on the aspects that are crucial for high quality texture synthesis. First, we evaluate whether the success of the random multi-scale network arises from the combination of filters on multiple scales or whether it is simply the increased size of its largest receptive fields (55× 55 vs. 11× 11) that leads to the improvement compared to the single-scale model. Thus, to investigate the influence of the spatial extend of the filters and the importance of combining multiple filter sizes in one model, we generate textures from multiple single-scale models, where each model has the same number of random filters as the multi-scale model (1024) but only uses filters from a single scale of the multi-scale model (Fig. 2). We find that while 3× 3 filters mainly capture the marginal distribution of the color channels, larger filters like 11 × 11 model small- to mid-scale structures (like small stones) but miss more long-range structures (larger stones are not well separated). Very large filters like 55× 55, on the other hand, are capable of modeling long-range structures but then miss much of the small- to midscale statistics (like the texture of the stone). Therefore we conclude that the combination of different scales in the multi-scale network is important for good texture synthesis since it allows to simultaneously model small-, mid- and long-range correlations of the textures. Finally we note that a further indispensable component for good texture models are the non-linearities: textures synthesised the multi-scale model without ReLU (Fig. 2, right column) are unable to capture the statistical dependencies of the texture. The perceptual quality of the textures generated from models with only a single layer and random filters is quite remarkable and surpasses parametric methods like Portilla & Simoncelli (2000) that have been state-of-the-art two years ago (before the use of DNNs).",
      "startOffset" : 65,
      "endOffset" : 3592
    }, {
      "referenceID" : 4,
      "context" : "Samples from the model often rival the current state-of-the-art (Gatys et al., 2015a) (Fig. 4, third vs fourth row), even though the latter relies on a high-performance deep neural network with features that are tuned to the statistics of natural images. Seen more broadly, this finding suggests that natural image generation does not necessarily depend on deep hierarchical representations or on the training of the feature maps. Instead, for texture synthesis, both aspects rather seem to serve as fine-tuning of the image representation. One concern about the proposed single-layer multi-scale model is its computational inefficiency since it involves convolutions with spatially large filters (up to 55× 55). A more efficient way to achieve receptive fields of similar size would be to use a hierarchical multi-layer net. We conducted extensive experiments with various hierarchical architectures and while the synthesis is indeed significantly faster, the quality of the synthesized textures does not improve compared to a single-layer model. Thus for a minimal model of natural textures, deep hierarchical representations are not necessary but they can improve the efficiency of the texture synthesis. Our results clearly demonstrate that Gram matrices computed from the feature maps of convolutional neural networks generically lead to useful summary statistics for texture synthesis. The Gram matrix on the feature maps transforms the representations from the convolutional neural network into a stationary feature space that captures the pairwise correlations between different features. If the number of feature maps is large, then the local structures in the image are well preserved in the projected space and the overlaps of the convolutional filtering add additional constraints. At the same time, averaging out the spatial dimensions yields sufficient flexibility to generate entirely new textures that differ from the reference on a patch by patch level, but still share much of the small- and long-range statistics. The success of shallow convolutional networks with random filters in reproducing the structure of the reference texture is remarkable and indicates that they can be useful for parametric texture synthesis. Besides reproducing the stationary correlation structure of the reference image (\"perceptual similarity\") another desideratum of a texture synthesis is to exhibit a large variety between different samples generated from the same given image (\"variability\"). Hence, synthesis algorithms need to balance perceptual similarity and variability. This balance is determined by a complex interplay between the choice of summary statistics and the optimization algorithm used. For example the stopping criterion of the optimization algorithm can be adjusted to trade perceptual similarity for larger variability. Finding the right balance between perceptual similarity and variability is challenging because we are currently lacking robust measures of these quantities. In this work we introduced VGG-loss as a measure of perceptual similarity, and, even though, it works much better than other common measures such as Structural Similarity Index (SSIM, Wang et al., 2004, see Appendix A, Figure 6) or Euclidean distance in the pixel space (not shown), it is still not perfect (Figure 4). Measuring variability is probably even more difficult: in principle it requires measuring the entropy of generated samples, which is intractable in a high-dimensional space. A different approach could be based on a psychophysical assessment of generated samples. For example, we could use an inpainting task (illustrated in Appendix A, Figure 7) to make human observers decide between actual texture patches and inpaited ones. Performance close to a chance-level would indicate that the texture model produces variable enough samples to capture the diversity of actual patches. The further exploration of variability measures lies, however, beyond the scope of this work. In this paper we focused on maximizing perceptual similarity only, and it is worth pointing out that additional efforts will be necessary to find an optimal trade-off between perceptual similarity and variability. For the synthesis of textures from the random models considered here, the trade-off leans more towards perceptual similarity in comparison to Gatys et al. (2015a)(due to the simpler optimization) which also explains the superior performance on some samples.",
      "startOffset" : 65,
      "endOffset" : 4369
    } ],
    "year" : 2017,
    "abstractText" : "Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters.",
    "creator" : "LaTeX with hyperref package"
  }
}