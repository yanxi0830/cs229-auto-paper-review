{
  "name" : "363.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES",
    "authors" : [ "Shuohang Wang", "Jing Jiang" ],
    "emails" : [ "shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Many natural language processing problems involve matching two or more sequences to make a decision. For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing.\nWith recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015).\nA common trait of a number of these recent studies on sequence matching problems is the use of a “compare-aggregate” framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words. It then combines the results of these word interactions using a similarity focus layer followed by a multi-layer CNN. Parikh et al. (2016) proposed a decomposable attention model for textual entailment, in which words from each sequence are compared with an\nattention-weighted version of the other sequence to produce a series of comparison vectors. The comparison vectors are then aggregated and fed into a feed forward network for final classification.\nAlthough these studies have shown the effectiveness of such a “compare-aggregate” framework for sequence matching, there are at least two limitations with these previous studies: (1) Each of the models proposed in these studies is tested on one or two tasks only, but we hypothesize that this general framework is effective on many sequence matching problems. There has not been any study that empirically verifies this. (2) More importantly, these studies did not pay much attention to the comparison function that is used to compare two small textual units. Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.g., two words. However, based on the nature of these sequence matching problems, we essentially need to measure how semantically similar the two sequences are. Presumably, this property of these sequence matching problems should guide us in choosing more appropriate comparison functions. Indeed He & Lin (2016) used cosine similarity, Euclidean distance and dot product to define the comparison function, which seem to be better justifiable. But they did not systematically evaluate these similarity or distance functions or compare them with a standard feedforward network.\nIn this paper, we argue that the general “compare-aggregate” framework is effective for a wide range of sequence matching problems. We present a model that follows this general framework and test it on four different datasets, namely, MovieQA, InsuranceQA, WikiQA and SNLI. The first three datasets are for Question Answering, but the setups of the tasks are quite different. The last dataset is for textual entailment. More importantly, we systematically present and test six different comparison functions. We find that overall a comparison function based on element-wise subtraction and multiplication works the best on the four datasets.\nThe contributions of this work are twofold: (1) Using four different datasets, we show that our model following the “compare-aggregate” framework is very effective when compared with the state-ofthe-art performance on these datasets. (2) We conduct systematic evaluation of different comparison functions and show that a comparison function based on element-wise operations, which is not widely used for word-level matching, works the best across the different datasets. We believe that these findings will be useful for future research on sequence matching problems. We have also made our code available online.1"
    }, {
      "heading" : "2 METHOD",
      "text" : "In this section, we propose a general model following the “compare-aggregate” framework for matching two sequences. This general model can be applied to different tasks. We focus our discussion on six different comparison functions that can be plugged into this general “compare-aggregate” model. In particular, we hypothesize that two comparison functions based on element-wise operations, SUB and MULT, are good middle ground between highly flexible functions using standard neural network models and highly restrictive functions based on cosine similarity and/or Euclidean\n1https://github.com/shuohangwang/SeqMatchSeq\ndistance. As we will show in the experiment section, these comparison functions based on elementwise operations can indeed perform very well on a number of sequence matching problems."
    }, {
      "heading" : "2.1 PROBLEM DEFINITION AND MODEL OVERVIEW",
      "text" : "The general setup of the sequence matching problem we consider is the following. We assume there are two sequences to be matched. We use two matrices Q ∈ Rd×Q and A ∈ Rd×A to represent the word embeddings of the two sequences, where Q and A are the lengths of the two sequences, respectively, and d is the dimensionality of the word embeddings. In other words, each column vector of Q or A is an embedding vector representing a single word. Given a pair of Q and A, the goal is to predict a label y. For example, in textual entailment, Q may represent a premise and A a hypothesis, and y indicates whether Q entails A or contradicts A. In question answering, Q may be a question and A a candidate answer, and y indicates whether A is the correct answer to Q.\nWe treat the problem as a supervised learning task. We assume that a set of training examples in the form of (Q,A, y) is given and we aim to learn a model that maps any pair of (Q,A) to a y.\nAn overview of our model is shown in Figure 1. The model can be divided into the following four layers:\n1. Preprocessing: We use a preprocessing layer (not shown in the figure) to process Q and A to obtain two new matrices Q ∈ Rl×Q and A ∈ Rl×A. The purpose here is to use some gate values to control the importance of different words in making the predictions on the sequence pair. For example, qi ∈ Rl, which is the ith column vector of Q, encodes the ith word in Q.\n2. Attention: We apply a standard attention mechanism on Q and A to obtain attention weights over the column vectors in Q for each column vector in A. With these attention weights, for each column vector aj in A, we obtain a corresponding vector hj , which is an attention-weighted sum of the column vectors of Q.\n3. Comparison: We use a comparison function f to combine each pair of aj and hj into a vector tj .\n4. Aggregation: We use a CNN layer to aggregate the sequence of vectors tj for the final classification.\nAlthough this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences. First, we will pay much attention to the comparison function f and compare a number of options, including some uncommon ones based on elementwise operations. Second, we apply our model to four different datasets representing four different tasks to evaluate its general effectiveness for sequence matching problems. There are also some other differences from the work by Parikh et al. (2016). For example, we use a CNN layer instead of summation and concatenation for aggregation. Our attention mechanism is one-directional instead of two-directional.\nIn the rest of this section we will present the model in detail. We will focus mostly on the comparison functions we consider."
    }, {
      "heading" : "2.2 PREPROCESSING AND ATTENTION",
      "text" : "Inspired by the use of gates in LSTM and GRU, we preprocess Q and A with the following formulas:\nQ = σ(WiQ+ bi ⊗ eQ) tanh(WuQ+ bu ⊗ eQ), A = σ(WiA+ bi ⊗ eA) tanh(WuA+ bu ⊗ eA), (1)\nwhere is element-wise multiplication, and Wi,Wu ∈ Rl×d and bi,bu ∈ Rl are parameters to be learned. The outer product (· ⊗ eX) produces a matrix or row vector by repeating the vector or scalar on the left for X times. Here σ(WiQ + bi ⊗ eQ) and σ(WiA + bi ⊗ eA) act as gate values to control the degree to which the original values of Q and A are preserved in Q and A. For example, for stop words, their gate values would likely be low for tasks where stop words make little difference to the final predictions.\nIn this preprocessing step, the word order does not matter. Although a better way would be to use RNN such as LSTM and GRU to chain up the words such that we can capture some contextual information, this could be computationally expensive for long sequences. In our experiments, we only incorporated LSTM into the formulas above for the SNLI task.\nThe general attention (Luong et al., 2015) layer is built on top of the resulting Q and A as follows: G = softmax ( (WgQ+ bg ⊗ eQ)TA ) ,\nH = QG, (2)\nwhere Wg ∈ Rl×l and bg ∈ Rl are parameters to be learned, G ∈ RQ×A is the attention weight matrix, and H ∈ Rl×A are the attention-weighted vectors. Specifically, hj , which is the jth column vector of H, is a weighted sum of the column vectors of Q and represents the part of Q that best matches the jth word in A. Next we will combine hj and aj using a comparison function."
    }, {
      "heading" : "2.3 COMPARISON",
      "text" : "The goal of the comparison layer is to match each aj , which represents the jth word and its context in A, with hj , which represents a weighted version of Q that best matches aj . Let f denote a comparison function that transforms aj and hj into a vector tj to represent the comparison result.\nA natural choice of f is a standard neural network layer that consists of a linear transformation followed by a non-linear activation function. For example, we can consider the following choice:\nNEURALNET (NN): tj = f(aj ,hj) = ReLU(W [ aj hj ] + b), (3)\nwhere matrix W ∈ Rl×2l and vector b ∈ Rl are parameters to be learned. Alternatively, another natural choice is a neural tensor network (Socher et al., 2013) as follows:\nNEURALTENSORNET (NTN): tj = f(aj ,hj) = ReLU(aTjT [1...l]hj + b), (4)\nwhere tensor T[1...l] ∈ Rl×l×l and vector b ∈ Rl are parameters to be learned.\nHowever, we note that for many sequence matching problems, we intend to measure the semantic similarity or relatedness of the two sequences. So at the word level, we also intend to check how similar or related aj is to hj . For this reason, a more natural choice used in some previous work is Euclidean distance or cosine similarity between aj and hj . We therefore consider the following definition of f :\nEUCLIDEAN+COSINE (EUCCOS): tj = f(aj ,hj) = [ ‖aj − hj‖2 cos(aj ,hj) ] . (5)\nNote that with EUCCOS, the resulting vector tj is only a 2-dimensional vector. Although EUCCOS is a well-justified comparison function, we suspect that it may lose some useful information from the original vectors aj and hj . On the other hand, NN and NTN are too general and thus do not capture the intuition that we care mostly about the similarity between aj and hj .\nTo use something that is a good compromise between the two extreme cases, we consider the following two new comparison functions, which operate on the two vectors in an element-wise manner. These functions have been used previously by Mou et al. (2016).\nSUBTRACTION (SUB): tj = f(aj ,hj) = (aj − hj) (aj − hj), (6) MULTIPLICATION (MULT): tj = f(aj ,hj) = aj hj . (7)\nNote that the operator is element-wise multiplication. For both comparison functions, the resulting vector tj has the same dimensionality as aj and hj .\nWe can see that SUB is closely related to Euclidean distance in that Euclidean distance is the sum of all the entries of the vector tj produced by SUB. But by not summing up these entries, SUB preserves some information about the different dimensions of the original two vectors. Similarly, MULT is closely related to cosine similarity but preserves some information about the original two vectors.\nFinally, we consider combining SUB and MULT followed by an NN layer as follows: SUBMULT+NN: tj = f(aj ,hj) = ReLU(W [ (aj − hj) (aj − hj)\naj hj\n] + b). (8)\nIn summary, we consider six different comparison functions: NN, NTN, EUCCOS, SUB, MULT and SUBMULT+NN. Among these functions, the last three (SUB, MULT and SUBMULT+NN) have not been widely used in previous work for word-level matching."
    }, {
      "heading" : "2.4 AGGREGATION",
      "text" : "After we apply the comparison function to each pair of aj and hj to obtain a series of vectors tj , finally we aggregate these vectors using a one-layer CNN (Kim, 2014):\nr = CNN([t1, . . . , tA]). (9)\nr ∈ Rnl is then used for the final classification, where n is the number of windows in CNN."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "In this section, we evaluate our model on four different datasets representing different tasks. The first three datasets are question answering tasks while the last one is on textual entailment. The statistics of the four datasets are shown in Table 2. We will fist introduce the task settings and the way we customize the “compare-aggregate” structure to each task. Then we will show the baselines for the different datasets. Finally, we discuss the experiment results shown in Table 3 and the ablation study shown in Table 4."
    }, {
      "heading" : "3.1 TASK-SPECIFIC MODEL STRUCTURES",
      "text" : "In all these tasks, we use matrix Q ∈ Rd×Q to represent the question or premise and matrix Ak ∈ Rd×Ak (k ∈ [1,K]) to represent the kth answer or the hypothesis. For the machine comprehension task MovieQA (Tapaswi et al., 2016), there is also a matrix P ∈ Rd×P that represents the plot of a movie. Here Q is the length of the question or premise, Ak the length of the kth answer, and P the length of the plot.\nFor the SNLI (Bowman et al., 2015) dataset, the task is text entailment, which identifies the relationship (entailment, contradiction or neutral) between a premise sentence and a hypothesis sentence. Here K = 1, and there are exactly two sequences to match. The actual model structure is what we have described before.\nFor the InsuranceQA (Feng et al., 2015) dataset, the task is an answer selection task which needs to select the correct answer for a question from a candidate pool. For the WikiQA (Yang et al., 2015) datasets, we need to rank the candidate answers according to a question. For both tasks,\nthere are K candidate answers for each question. Let us use rk to represent the resulting vector produced by Eqn. 9 for the kth answer. In order to select one of the K answers, we first define R = [r1, r2, . . . , rK ]. We then compute the probability of the kth answer to be the correct one as follows:\np(k|R) = softmax(wT tanh(WsR+ bs ⊗ eK) + b⊗ eK), (10) where Ws ∈ Rl×nl, w ∈ Rl, bs ∈ Rl, b ∈ R are parameters to be learned. For the machine comprehension task MovieQA, each question is related to Plot Synopses written by fans after watching the movie and each question has five candidate answers. So for each candidate answer there are three sequences to be matched: the plot P, the question Q and the answer Ak. For each k, we first match Q and P and refer to the matching result at position j as tqj , as generated by one of the comparison functions f . Similarly, we also match Ak with P and refer to the matching result at position j as tak,j . We then define\ntk,j = [ tqj tak,j ] ,\nand\nrk = CNN([tk,1, . . . , tk,P ]).\nTo select an answer from the K candidate answers, again we use Eqn. 10 to compute the probabilities.\nThe implementation details of the modes are as follows. The word embeddings are initialized from GloVe (Pennington et al., 2014). During training, they are not updated. The word embeddings not found in GloVe are initialized with zero.\nThe dimensionality l of the hidden layers is set to be 150. We use ADAMAX (Kingma & Ba, 2015) with the coefficients β1 = 0.9 and β2 = 0.999 to optimize the model. We do not use L2regularization. The main parameter we tuned is the dropout on the embedding layer. For WikiQA, which is a relatively small dataset, we also tune the learning rate and the batch size. For the others, we set the batch size to be 30 and the learning rate 0.002."
    }, {
      "heading" : "3.2 BASELINES",
      "text" : "Here, we will introduce the baselines for each dataset. We did not re-implement these models but simply took the reported performance for the purpose of comparison.\nSNLI: •W-by-W Attention: The model by Rocktäschel et al. (2015), who first introduced attention mechanism into text entailment. • match-LSTM: The model by Wang & Jiang (2016b), which concatenates the matched words as the inputs of an LSTM. • LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). • Decomp Attention: Another “compare-aggregate” model proposed by Parikh et al. (2016). • EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al. (2016) on the SNLI dataset.\nInsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer.\nWikiQA: • IARNN-Occam and IARNN-Gate as introduced before. • CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. • ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). • CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.\nMovieQA: All the baselines we consider come from Tapaswi et al. (2016)’s work: • Cosine Word2Vec: A sliding window is used to select the answer according to the similarities computed\nthrough Word2Vec between the sentences in plot and the question/answer. • Cosine TFIDF: This model is similar to the previous method but uses bag-of-word with tf-idf scores to compute similarity. • SSCB TFIDF: Instead of using the sliding window method, a convolutional neural network is built on the sentence level similarities."
    }, {
      "heading" : "3.3 ANALYSIS OF RESULTS",
      "text" : "We use accuracy as the evaluation metric for the datasets MovieQA, InsuranceQA and SNLI, as there is only one correct answer or one label for each instance. For WikiQA, there may be multiple correct answers, so evaluation metrics we use are Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).\nWe observe the following from the results. (1) Overall, we can find that our general “compareaggregate” structure achieves the best performance on MovieQA, InsuranceQA, WikiQA datasets and very competitive performance on the SNLI dataset. Especially for the InsuranceQA dataset, with any comparison function we use, our model can outperform all the previous models. (2) The comparison method SUBMULT+NN is the best in general. (3) Some simple comparison functions can achieve better performance than the neural networks or neural tensor network comparison functions. For example, the simplest comparison function EUCCOS achieves nearly the best performance in the MovieQA dataset, and the element-wise comparison functions, which do not need parameters can achieve the best performance on the WikiQA dataset. (4) We find the preprocessing layer and the attention layer for word selection to be important in the “compare-aggregate” structure through the experiments of removing these two layers separately. We also see that for sequence matching with big difference in length, such as the MovieQA and InsuranceQA tasks, the attention layer plays a more important role. For sequence matching with smaller difference in length, such as the WikiQA and SNLI tasks, the pre-processing layer plays a more important role. (5) For the MovieQA, InsuranceQA and WikiQA tasks, our preprocessing layer is order-insensitive so that it will not take the context information into consideration during the comparison, but our model can still outperform the previous work with order-sensitive preprocessing layer. With this finding, we believe the word-by-word comparison part plays a very important role in these tasks. We will further explore the preprocessing layer in the future."
    }, {
      "heading" : "3.4 FURTHER ANALYSES",
      "text" : "To further explain how our model works, we visualize the max values in each dimension of the convolutional layer. We use two examples shown in Table 1 from MovieQA and InsuranceQA datasets respectively. In the top of Figure 2, we can see that the plot words that also appear in either the question or the answer will draw more attention by the CNN. We hypothesize that if the nearby words in the plot can match both the words in question and the words in one answer, then this answer is more likely to be the correct one. Similarly, the bottom one of Figure 2 also shows that the CNN will focus more on the matched word representations. If the words in one answer continuously match the words in the question, this answer is more likely to be the correct one."
    }, {
      "heading" : "4 RELATED WORK",
      "text" : "We review related work in three types of general structures for matching sequences.\nSiamense network: These kinds of models use the same structure, such as RNN or CNN, to build the representations for the sequences separately and then use them for classification. Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al. (2015) are used for sequence matching.\nAttentive network: Soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been widely used for sequence matching in machine comprehension (Hermann et al., 2015), text entailment (Rocktäschel et al., 2015) and question answering (Tan et al., 2016). Instead of using the final state of RNN to represent a sequence, these studies use weighted sum of all the states for the sequence representation.\nCompare-Aggregate network: This kind of framework is to perform the word level matching (Wang & Jiang, 2016a; Parikh et al., 2016; He & Lin, 2016; Trischler et al., 2016; Wan et al.,\n2016). Our work is under this framework. But our structure is different from previous models and our model can be applied on different tasks. Besides, we analyzed different word-level comparison functions separately."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "In this paper, we systematically analyzed the effectiveness of a “compare-aggregate” model on four different datasets representing different tasks. Moreover, we compared and tested different kinds of word-level comparison functions and found that some element-wise comparison functions can outperform the others. According to our experiment results, many different tasks can share the same “compare-aggregate” structure. In the future work, we would like to test its effectiveness on multi-task learning."
    }, {
      "heading" : "6 ACKNOWLEDGMENTS",
      "text" : "This research is supported by the National Research Foundation, Prime Ministers Office, Singapore under its International Research Centres in Singapore Funding Initiative."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning concept importance using a weighted dependence model",
      "author" : [ "Michael Bendersky", "Donald Metzler", "W Bruce Croft" ],
      "venue" : "In Proceedings of the third ACM International Conference on Web Search and Data Mining. ACM,",
      "citeRegEx" : "Bendersky et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bendersky et al\\.",
      "year" : 2010
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Enhancing and combining sequential and tree LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang" ],
      "venue" : "arXiv preprint arXiv:1609.06038,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory-networks for machine reading",
      "author" : [ "Jianpeng Cheng", "Li Dong", "Mirella Lapata" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Cheng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Applying deep learning to answer selection: A study and an open task",
      "author" : [ "Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou" ],
      "venue" : "In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),",
      "citeRegEx" : "Feng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2015
    }, {
      "title" : "Pairwise word interaction modeling with deep neural networks for semantic similarity measurement",
      "author" : [ "Hua He", "Jimmy Lin" ],
      "venue" : "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "He and Lin.,? \\Q2016\\E",
      "shortCiteRegEx" : "He and Lin.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Proceedings of the Conference on Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The Goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Hill et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning" ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language inference by tree-based convolution and heuristic matching",
      "author" : [ "Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Mou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "A decomposable attention model for natural language inference",
      "author" : [ "Ankur P Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Parikh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "MCTest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher JC Burges", "Erin Renshaw" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Richardson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Phil Blunsom" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Rocktäschel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Conference on Association for Computational Linguistics,",
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved representation learning for question answer matching",
      "author" : [ "Ming Tan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "In Proceedings of the Conference on Association for Computational Linguistics,",
      "citeRegEx" : "Tan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2016
    }, {
      "title" : "MovieQA: Understanding stories in movies through question-answering",
      "author" : [ "Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Tapaswi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tapaswi et al\\.",
      "year" : 2016
    }, {
      "title" : "A parallel-hierarchical model for machine comprehension on sparse data",
      "author" : [ "Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman" ],
      "venue" : "In Proceedings of the Conference on Association for Computational Linguistics,",
      "citeRegEx" : "Trischler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Match-srnn: Modeling the recursive matching structure with spatial RNN",
      "author" : [ "Shengxian Wan", "Yanyan Lan", "Jun Xu", "Jiafeng Guo", "Liang Pang", "Xueqi Cheng" ],
      "venue" : "International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Wan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2016
    }, {
      "title" : "Inner attention based recurrent neural networks for answer selection",
      "author" : [ "Bingning Wang", "Kang Liu", "Jun Zhao" ],
      "venue" : "In Proceedings of the Conference on Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine comprehension using match-LSTM and answer pointer",
      "author" : [ "Shuohang Wang", "Jing Jiang" ],
      "venue" : "arXiv preprint arXiv:1608.07905,",
      "citeRegEx" : "Wang and Jiang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2016
    }, {
      "title" : "Learning natural language inference with LSTM",
      "author" : [ "Shuohang Wang", "Jing Jiang" ],
      "venue" : "In Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang and Jiang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2016
    }, {
      "title" : "WikiQA: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "ABCNN: Attention-based convolutional neural network for modeling sentence pairs",
      "author" : [ "Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "arXiv preprint arXiv:1512.05193,",
      "citeRegEx" : "Yin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016).",
      "startOffset" : 121,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016).",
      "startOffset" : 121,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 185
    }, {
      "referenceID" : 5,
      "context" : "To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015).",
      "startOffset" : 292,
      "endOffset" : 359
    }, {
      "referenceID" : 8,
      "context" : "However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015).",
      "startOffset" : 292,
      "endOffset" : 359
    }, {
      "referenceID" : 17,
      "context" : "However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015).",
      "startOffset" : 292,
      "endOffset" : 359
    }, {
      "referenceID" : 14,
      "context" : "A common trait of a number of these recent studies on sequence matching problems is the use of a “compare-aggregate” framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing. With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015). A common trait of a number of these recent studies on sequence matching problems is the use of a “compare-aggregate” framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise.",
      "startOffset" : 130,
      "endOffset" : 2266
    }, {
      "referenceID" : 2,
      "context" : "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing. With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015). A common trait of a number of these recent studies on sequence matching problems is the use of a “compare-aggregate” framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words.",
      "startOffset" : 130,
      "endOffset" : 2459
    }, {
      "referenceID" : 2,
      "context" : "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing. With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rocktäschel et al., 2015). A common trait of a number of these recent studies on sequence matching problems is the use of a “compare-aggregate” framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words. It then combines the results of these word interactions using a similarity focus layer followed by a multi-layer CNN. Parikh et al. (2016) proposed a decomposable attention model for textual entailment, in which words from each sequence are compared with an",
      "startOffset" : 130,
      "endOffset" : 2744
    }, {
      "referenceID" : 9,
      "context" : "Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.",
      "startOffset" : 47,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.g., two words. However, based on the nature of these sequence matching problems, we essentially need to measure how semantically similar the two sequences are. Presumably, this property of these sequence matching problems should guide us in choosing more appropriate comparison functions. Indeed He & Lin (2016) used cosine similarity, Euclidean distance and dot product to define the comparison function, which seem to be better justifiable.",
      "startOffset" : 48,
      "endOffset" : 472
    }, {
      "referenceID" : 14,
      "context" : "Although this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Although this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences. First, we will pay much attention to the comparison function f and compare a number of options, including some uncommon ones based on elementwise operations. Second, we apply our model to four different datasets representing four different tasks to evaluate its general effectiveness for sequence matching problems. There are also some other differences from the work by Parikh et al. (2016). For example, we use a CNN layer instead of summation and concatenation for aggregation.",
      "startOffset" : 85,
      "endOffset" : 538
    }, {
      "referenceID" : 12,
      "context" : "The general attention (Luong et al., 2015) layer is built on top of the resulting Q and A as follows:",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Alternatively, another natural choice is a neural tensor network (Socher et al., 2013) as follows:",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "These functions have been used previously by Mou et al. (2016). SUBTRACTION (SUB): tj = f(aj ,hj) = (aj − hj) (aj − hj), (6) MULTIPLICATION (MULT): tj = f(aj ,hj) = aj hj .",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "After we apply the comparison function to each pair of aj and hj to obtain a series of vectors tj , finally we aggregate these vectors using a one-layer CNN (Kim, 2014):",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "For the machine comprehension task MovieQA (Tapaswi et al., 2016), there is also a matrix P ∈ Rd×P that represents the plot of a movie.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "For the SNLI (Bowman et al., 2015) dataset, the task is text entailment, which identifies the relationship (entailment, contradiction or neutral) between a premise sentence and a hypothesis sentence.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "For the InsuranceQA (Feng et al., 2015) dataset, the task is an answer selection task which needs to select the correct answer for a question from a candidate pool.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "For the WikiQA (Yang et al., 2015) datasets, we need to rank the candidate answers according to a question.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "The word embeddings are initialized from GloVe (Pennington et al., 2014).",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "SNLI: •W-by-W Attention: The model by Rocktäschel et al. (2015), who first introduced attention mechanism into text entailment.",
      "startOffset" : 38,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "SNLI: •W-by-W Attention: The model by Rocktäschel et al. (2015), who first introduced attention mechanism into text entailment. • match-LSTM: The model by Wang & Jiang (2016b), which concatenates the matched words as the inputs of an LSTM.",
      "startOffset" : 38,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "• LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). • Decomp Attention: Another “compare-aggregate” model proposed by Parikh et al.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "• LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). • Decomp Attention: Another “compare-aggregate” model proposed by Parikh et al. (2016). • EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al.",
      "startOffset" : 52,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "• EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al. (2016) on the SNLI dataset.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer.",
      "startOffset" : 39,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions.",
      "startOffset" : 39,
      "endOffset" : 348
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights.",
      "startOffset" : 39,
      "endOffset" : 537
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer.",
      "startOffset" : 39,
      "endOffset" : 630
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: • IARNN-Occam and IARNN-Gate as introduced before. • CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression.",
      "startOffset" : 39,
      "endOffset" : 823
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: • IARNN-Occam and IARNN-Gate as introduced before. • CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. • ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). • CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.",
      "startOffset" : 39,
      "endOffset" : 1021
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: • IARNN-Occam and IARNN-Gate as introduced before. • CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. • ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). • CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.",
      "startOffset" : 39,
      "endOffset" : 1060
    }, {
      "referenceID" : 1,
      "context" : "InsuranceQA: • IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. • CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. • Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. • IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. • IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: • IARNN-Occam and IARNN-Gate as introduced before. • CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. • ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). • CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity. MovieQA: All the baselines we consider come from Tapaswi et al. (2016)’s work: • Cosine Word2Vec: A sliding window is used to select the answer according to the similarities computed",
      "startOffset" : 39,
      "endOffset" : 1177
    }, {
      "referenceID" : 5,
      "context" : "Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : ", 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al.",
      "startOffset" : 32,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : ", 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al.",
      "startOffset" : 32,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Attentive network: Soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been widely used for sequence matching in machine comprehension (Hermann et al.",
      "startOffset" : 44,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Attentive network: Soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been widely used for sequence matching in machine comprehension (Hermann et al.",
      "startOffset" : 44,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : ", 2015) has been widely used for sequence matching in machine comprehension (Hermann et al., 2015), text entailment (Rocktäschel et al.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : ", 2015), text entailment (Rocktäschel et al., 2015) and question answering (Tan et al.",
      "startOffset" : 25,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : ", 2015) and question answering (Tan et al., 2016).",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : ", 2016) or neural network-based combination Bowman et al. (2015) are used for sequence matching.",
      "startOffset" : 44,
      "endOffset" : 65
    } ],
    "year" : 2017,
    "abstractText" : "Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general “compare-aggregate” framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.",
    "creator" : "LaTeX with hyperref package"
  }
}