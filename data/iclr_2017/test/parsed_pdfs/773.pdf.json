{
  "name" : "773.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "RECTIFIED FACTOR NETWORKS FOR BICLUSTERING",
    "authors" : [ "Djork-Arné Clevert", "Thomas Unterthiner" ],
    "emails" : [ "okko@bioinf.jku.at", "unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Biclustering is widely-used in statistics (A. Kasim & Talloen, 2016), and recently it also became popular in the machine learning community (O´ Connor & Feizi, 2014; Lee et al., 2015; Kolar et al., 2011), e.g., for analyzing large dyadic data given in matrix form, where one dimension are the samples and the other the features. A matrix entry is a feature value for the according sample. A bicluster is a pair of a sample set and a feature set for which the samples are similar to each other on the features and vice versa. Biclustering simultaneously clusters rows and columns of a matrix. In particular, it clusters row elements that are similar to each other on a subset of column elements. In contrast to standard clustering, the samples of a bicluster are only similar to each other on a subset of features. Furthermore, a sample may belong to different biclusters or to no bicluster at all. Thus, biclusters can overlap in both dimensions. For example, in drug design biclusters are compounds which activate the same gene module and thereby indicate a side effect. In this example different chemical compounds are added to a cell line and the gene expression is measured (Verbist et al., 2015). If multiple pathways are active in a sample, it belongs to different biclusters and may\nhave different side effects. In e-commerce often matrices of costumers times products are available, where an entry indicates whether a customer bought the product or not. Biclusters are costumers which buy the same subset of products. In a collaboration with the internet retailer Zalando the biclusters revealed outfits which were created by customers which selected certain clothes for a particular outfit.\nFABIA (factor analysis for bicluster acquisition, (Hochreiter et al., 2010)) evolved into one of the most successful biclustering methods. A detailed comparison has shown FABIA’s superiority over existing biclustering methods both on simulated data and real-world gene expression data (Hochreiter et al., 2010). In particular FABIA outperformed non-negative matrix factorization with sparseness constraints and state-of-the-art biclustering methods. It has been applied to genomics, where it identified in gene expression data task-relevant biological modules (Xiong et al., 2014). In the large drug design project QSTAR, FABIA was used to extract biclusters from a data matrix that contains bioactivity measurements across compounds (Verbist et al., 2015). Due to its successes, FABIA has become part of the standard microarray data processing pipeline at the pharmaceutical company Janssen Pharmaceuticals. FABIA has been applied to genetics, where it has been used to identify DNA regions that are identical by descent in different individuals. These individuals inherited an IBD region from a common ancestor (Hochreiter, 2013; Povysil & Hochreiter, 2014). FABIA is a generative model that enforces sparse codes (Hochreiter et al., 2010) and, thereby, detects biclusters. Sparseness of code units and parameters is essential for FABIA to find biclusters, since only few samples and few features belong to a bicluster. Each FABIA bicluster is represented by two membership vectors: one for the samples and one for the features. These membership vectors are both sparse since only few samples and only few features belong to the bicluster.\nHowever, FABIA has shortcomings, too. A disadvantage of FABIA is that it is only feasible with about 20 code units (the biclusters) because of the high computational complexity which depends cubically on the number of biclusters, i.e. the code units. If less code units were used, only the large and common input structures would be detected, thereby, occluding the small and rare ones. Another shortcoming of FABIA is that units are insufficiently decorrelated and, therefore, multiple units may encode the same event or part of it. A third shortcoming of FABIA is that the membership vectors do not have exact zero entries, that is the membership is continuous and a threshold have to be determined. This threshold is difficult to adjust. A forth shortcoming is that biclusters can have large positive but also large negative members of samples (that is positive or negative code values). In this case it is not clear whether the positive pattern or the negative pattern has been recognized.\nRectified Factor Networks (RFNs; (Clevert et al., 2015)) RFNs overcome the shortcomings of FABIA. The first shortcoming of only few code units is avoided by extending FABIA to thousands of code units. RFNs introduce rectified units to FABIA’s posterior distribution and, thereby, allow for fast computations on GPUs. They are the first methods which apply rectification to the posterior distribution of factor analysis and matrix factorization, though rectification it is well established in Deep Learning by rectified linear units (ReLUs). RFNs transfer the methods for rectification from the neural network field to latent variable models. Addressing the second shortcoming of FABIA, RFNs achieve decorrelation by increasing the sparsity of the code units using dropout from field of Deep Learning. RFNs also address the third FABIA shortcoming, since the rectified posterior means yield exact zero values. Therefore, memberships to biclusters are readily obtained by values that are not zero. Since RFNs only have non-negative code units, the problem of separating the negative from the positive pattern disappears."
    }, {
      "heading" : "2 IDENTIFYING BICLUSTERS BY RECTIFIED FACTOR NETWORKS",
      "text" : ""
    }, {
      "heading" : "2.1 RECTIFIED FACTOR NETWORKS",
      "text" : "We propose to use the recently introduced Rectified Factor Networks (RFNs; (Clevert et al., 2015)) for biclustering to overcome the drawbacks of the FABIA model. The factor analysis model and the construction of a bicluster matrix are depicted in Fig. 1. RFNs efficiently construct very sparse, nonlinear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure.\nRFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. These posterior means are the code of the input data. The RFN code can be computed very efficiently. For nonGaussian priors, the computation of the posterior mean of a new input requires either to numerically solve an integral or to iteratively update variational parameters. In contrast, for Gaussian priors the posterior mean is the product between the input and a matrix that is independent of the input. RFNs use a rectified Gaussian posterior, therefore, they have the speed of Gaussian posteriors but lead to sparse codes via rectification. RFNs are implemented on GPUs.\nThe RFN model is a factor analysis model\nv = Wh + ✏ , (1)\nwhich extracts the covariance structure of the data. The prior h ⇠ N (0, I) of the hidden units (factors) h 2 Rl and the noise ✏ ⇠ N (0, ) of visible units (observations) v 2 Rm are independent. The model parameters are the weight (factor loading) matrix W 2 Rm⇥l and the noise covariance matrix 2 Rm⇥m. RFN models are selected via the posterior regularization method (Ganchev et al., 2010). For data {v} = {v1, . . . ,vn}, it maximizes the objective F :\nF = 1 n\nnX\ni=1\nlog p(vi) 1\nn\nnX\ni=1\nDKL(Q(hi | vi) k p(hi | vi)), (2)\nwhere DKL is the Kullback-Leibler distance. Maximizing F achieves two goals simultaneously: (1) extracting desired structures and information from the data as imposed by the generative model and (2) ensuring sparse codes via Q from the set of rectified Gaussians.\nFor Gaussian posterior distributions, and mean-centered data {v} = {v1, . . . ,vn}, the posterior p(hi | vi) is Gaussian with mean vector (µp)i and covariance matrix ⌃p:\n(µp)i = I + W T 1W 1 W T 1 vi , ⌃p = I + W T 1W 1 . (3)\nFor rectified Gaussian posterior distributions, ⌃p remains as in the Gaussian case, but minimizing the second DKL of Eq. (2) leads to constrained optimization problem (see Clevert et al. (2015))\nmin\nµi\n1\nn\nnX\ni=1\n(µi (µp)i)T ⌃ 1p (µi (µp)i)\ns.t. 8i : µi 0 , 8j : 1\nn\nnX\ni=1\nµ2ij = 1 , (4)\nwhere “ ” is component-wise. In the E-step of the generalized alternating minimization algorithm (Ganchev et al., 2010), which is used for RFN model selection, we only perform a step of the gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton method for solving Eq. (4) (Clevert et al., 2015). Therefore, RFN model selection is extremely efficient but still guarantees the correct solution."
    }, {
      "heading" : "2.2 RFN BICLUSTERING",
      "text" : "For a RFN model, each code unit represents a bicluster, where samples, for which the code unit is active, belong to the bicluster. On the other hand features that activates the code unit belong to the bicluster, too. The vector of activations of a unit across all samples is the sample membership vector. The weight vector which activates the unit is the feature membership vector. The un-constraint posterior mean vector is computed by multiplying the input with a matrix according to Eq. (3). The constraint posterior of a code unit is obtained by multiplying the input by a vector and subsequently rectifying and normalizing the code unit (Clevert et al., 2015).\nTo keep feature membership vector sparse, we introduce a Laplace prior on the parameters. Therefore only few features contribute to activating a code unit, that is, only few features belong to a bicluster. Sparse weights Wi are achieved by a component-wise independent Laplace prior for the weights:\np(Wi) = ⇣\n1p 2\n⌘n nY\nk=1\ne p 2 |Wki| (5)\nThe weight update for RFN (Laplace prior on the weights) is\nW = W + ⌘ U S 1 W\n↵ sign(W ) . (6)\nWhereby the sparseness of the weight matrix can be controlled by the hyper-parameter ↵ and U and S are defined as U = 1n Pn i=1 viµ T i and S = 1 n Pn i=1 µiµ T i +⌃, respectively. In order to enforce more sparseness of the sample membership vectors, we introduce dropout of code units. Dropout means that during training some code units are set to zero at the same time as they get rectified. Dropout avoids co-adaptation of code units and reduces correlation of code units — a problem of FABIA which is solved.\nRFN biclustering does not require a threshold for determining sample memberships to a bicluster since rectification sets code units to zero. Further crosstalk between biclusters via mixing up negative and positive memberships is avoided, therefore spurious biclusters do less often appear."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "In this section, we will present numerical results on multiple synthetic and real data sets to verify the performance of our RFN biclustering algorithm, and compare it with various other biclustering methods."
    }, {
      "heading" : "3.1 METHODS COMPARED",
      "text" : "To assess the performance of rectified factor networks (RFNs) as unsupervised biclustering methods, we compare the following 14 biclustering methods:\n(1) RFN: rectified factor networks (Clevert et al., 2015), (2) FABIA: factor analysis with Laplace prior on the hidden units (Hochreiter et al., 2010; Hochreiter, 2013), (3) FABIAS: factor analysis with sparseness projection (Hochreiter et al., 2010), (4) MFSC: matrix factorization with sparseness constraints (Hoyer, 2004), (5) plaid: plaid model (Lazzeroni & Owen, 2002; T. Chekouo & Raffelsberger, 2015), (6) ISA: iterative signature algorithm (Ihmels et al., 2004), (7) OPSM: orderpreserving sub-matrices (Ben-Dor et al., 2003), (8) SAMBA: statistical-algorithmic method for bicluster analysis (Tanay et al., 2002), (9) xMOTIF: conserved motifs (Murali & Kasif, 2003), (10) Bimax: divide-and-conquer algorithm (Prelic et al., 2006), (11) CC: Cheng-Church -biclusters\n(Cheng & Church, 2000), (12) plaid t: improved plaid model (Turner et al., 2003), (13) FLOC: flexible overlapped biclustering, a generalization of CC (Yang et al., 2005), and (14) spec: spectral biclustering (Kluger et al., 2003).\nFor a fair comparison, the parameters of the methods were optimized on auxiliary toy data sets. If more than one setting was close to the optimum, all near optimal parameter settings were tested. In the following, these variants are denoted as method variant (e.g. plaid ss). For RFN we used the following parameter setting: 13 hidden units, a dropout rate of 0.1, 500 iterations with a learning rate of 0.1, and set the parameter ↵ (controlling the sparseness on the weights) to 0.01."
    }, {
      "heading" : "3.2 SIMULATED DATA SETS WITH KNOWN BICLUSTERS",
      "text" : "In the following subsections, we describe the data generation process and results for synthetically generated data according to either a multiplicative or additive model structure."
    }, {
      "heading" : "3.2.1 DATA WITH MULTIPLICATIVE BICLUSTERS",
      "text" : "We assumed n = 1000 genes and l = 100 samples and implanted p = 10 multiplicative biclusters. The bicluster datasets with p biclusters are generated by following model:\nX = pX\ni=1\ni z T i + ⌥ , (7)\nwhere ⌥ 2 Rn⇥l is additive noise; i 2 Rn and zi 2 Rl are the bicluster membership vectors for the i-th bicluster. The i’s are generated by (i) randomly choosing the number N i of genes in bicluster i from {10, . . . , 210}, (ii) choosing N i genes randomly from {1, . . . , 1000}, (iii) setting i components not in bicluster i to N (0, 0.22) random values, and (iv) setting i components that are in bicluster i to N (±3, 1) random values, where the sign is chosen randomly for each gene. The zi’s are generated by (i) randomly choosing the number Nzi of samples in bicluster i from {5, . . . , 25}, (ii) choosing Nzi samples randomly from {1, . . . , 100}, (iii) setting zi components not in bicluster i to N (0, 0.22) random values, and (iv) setting zi components that are in bicluster i to N (2, 1) random values. Finally, we draw the ⌥ entries (additive noise on all entries) according to N (0, 32) and compute the data X according to Eq. (7). Using these settings, noisy biclusters of random sizes between 10⇥5 and 210⇥25 (genes⇥samples) are generated. In all experiments, rows (genes) were standardized to mean 0 and variance 1."
    }, {
      "heading" : "3.2.2 DATA WITH ADDITIVE BICLUSTERS",
      "text" : "In this experiment we generated biclustering data where biclusters stem from an additive two-way ANOVA model:\nX = pX\ni=1\n✓i ( i zTi ) + ⌥ , ✓ikj = µi + ↵ik + ij , (8)\nwhere is the element-wise product of matrices and both i and zi are binary indicator vectors which indicate the rows and columns belonging to bicluster i. The i-th bicluster is described by an ANOVA model with mean µi, k-th row effect ↵ik (first factor of the ANOVA model), and jth column effect ij (second factor of the ANOVA model). The ANOVA model does not have interaction effects. While the ANOVA model is described for the whole data matrix, only the effects on rows and columns belonging to the bicluster are used in data generation. Noise and bicluster sizes are generated as in previous Subsection 3.2.1.\nData was generated for three different signal-to-noise ratios which are determined by distribution from which µi is chosen: A1 (low signal) N (0, 22), A2 (moderate signal) N (±2, 0.52), and A3 (high signal) N (±4, 0.52), where the sign of the mean is randomly chosen. The row effects ↵ki are chosen from N (0.5, 0.22) and the column effects ij are chosen from N (1, 0.52)."
    }, {
      "heading" : "3.2.3 RESULTS ON SIMULATED DATA SETS",
      "text" : "For method evaluation, we use the previously introduced biclustering consensus score for two sets of biclusters (Hochreiter et al., 2010), which is computed as follows:\nStep (3) penalizes different numbers of biclusters in the sets. The highest consensus score is 1 and only obtained for identical sets of biclusters.\nTable 1 shows the biclustering results for these data sets. RFN significantly outperformed all other methods (t-test and McNemar test of correct elements in biclusters)."
    }, {
      "heading" : "3.3 GENE EXPRESSION DATA SETS",
      "text" : "In this experiment, we test the biclustering methods on gene expression data sets, where the biclusters are gene modules. The genes that are in a particular gene module belong to the according bicluster and samples for which the gene module is activated belong to the bicluster. We consider three gene expression data sets which have been provided by the Broad Institute and were previously clustered by Hoshida et al. (2007) using additional data sets. Our goal was to study how well biclustering methods are able to recover these clusters without any additional information. (A) The “breast cancer” data set (van’t Veer et al., 2002) was aimed at a predictive gene signature for the outcome of a breast cancer therapy. We removed the outlier array S54 which leads to a data set with 97 samples and 1213 genes. In Hoshida et al. (2007), three biologically meaningful subclasses were found that should be re-identified. (B) The “multiple tissue types” data set (Su et al., 2002) are gene expression profiles from human cancer samples from diverse tissues and cell lines. The data set contains 102 samples with 5565 genes. Biclustering should be able to re-identify the tissue types. (C) The “diffuse large-B-cell lymphoma (DLBCL)” data set (Rosenwald et al., 2002) was aimed at predicting the survival after chemotherapy. It contains 180 samples and 661 genes. The three classes found by Hoshida et al. (2007) should be re-identified.\nFor methods assuming a fixed number of biclusters, we chose five biclusters — slightly higher than the number of known clusters to avoid biases towards prior knowledge about the number of actual clusters. Besides the number of hidden units (biclusters) we used the same parameters as described in Sec. 3.1. The performance was assessed by comparing known classes of samples in the data sets with the sample sets identified by biclustering using the consensus score defined in Subsection 3.2.3 — here the score is evaluated for sample clusters instead of biclusters. The biclustering results are summarized in Table 2. RFN biclustering yielded in two out of three datasets significantly better results than all other methods and was on second place for the third dataset (significantly according to a McNemar test of correct samples in clusters)."
    }, {
      "heading" : "3.4 1000 GENOMES DATA SETS",
      "text" : "In this experiment, we used RFN for detecting DNA segments that are identical by descent (IBD). A DNA segment is IBD in two or more individuals, if they have inherited it from a common ancestor, that is, the segment has the same ancestral origin in these individuals. Biclustering is well-suited to detect such IBD segments in a genotype matrix (Hochreiter, 2013; Povysil & Hochreiter, 2014), which has individuals as row elements and genomic structural variations (SNVs) as column elements. Entries in the genotype matrix usually count how often the minor allele of a particular SNV is present in a particular individual. Individuals that share an IBD segment are similar to each other because they also share minor alleles of SNVs (tagSNVs) within the IBD segment. Individuals that share an IBD segment represent a bicluster.\nFor our IBD-analysis we used the next generation sequencing data from the 1000 Genomes Phase 3. This data set consists of low-coverage whole genome sequences from 2,504 individuals of the main continental population groups (Africans (AFR), Asians (ASN), Europeans (EUR), and Admixed Americans (AMR)). Individuals that showed cryptic first degree relatedness to others were removed, so that the final data set consisted of 2,493 individuals. Furthermore, we also included archaic human and human ancestor genomes, in order to gain insights into the genetic relationships between humans, Neandertals and Denisovans. The common ancestor genome was reconstructed from human, chimpanzee, gorilla, orang-utan, macaque, and marmoset genomes. RFN IBD detec-\ntion is based on low frequency and rare variants, therefore we removed common and private variants prior to the analysis. Afterwards, all chromosomes were divided into intervals of 10,000 variants with adjacent intervals overlapping by 5,000 variants\nIn the data of the 1000 Genomes Project, we found IBD-based indications of interbreeding between ancestors of humans and other ancient hominins within Africa (see Fig. 2 as an example of an IBD segment that matches the Neandertal genome)."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "We have introduced rectified factor networks (RFNs) for biclustering and benchmarked it with 13 other biclustering methods on artificial and real-world data sets.\nOn 400 benchmark data sets with artificially implanted biclusters, RFN significantly outperformed all other biclustering competitors including FABIA. On three gene expression data sets with previously verified ground-truth, RFN biclustering yielded twice significantly better results than all other methods and was once the second best performing method. On data of the 1000 Genomes Project, RFN could identify IBD segments which support the hypothesis that interbreeding between ancestors of humans and other ancient hominins already have taken place in Africa.\nRFN biclustering is geared to large data sets, sparse coding, many coding units, and distinct membership assignment. Thereby RFN biclustering overcomes the shortcomings of FABIA and has the potential to become the new state of the art biclustering algorithm.\nAcknowledgment. We thank the NVIDIA Corporation for supporting this research with several Titan X GPUs."
    } ],
    "references" : [ {
      "title" : "Applied Biclustering Methods for Big and High-Dimensional Data Using R",
      "author" : [ "S. Kaiser S. Hochreiter A. Kasim", "Z. Shkedy", "W. Talloen" ],
      "venue" : null,
      "citeRegEx" : "Kasim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kasim et al\\.",
      "year" : 2016
    }, {
      "title" : "Discovering local structure in gene expression data: the order-preserving submatrix problem",
      "author" : [ "A. Ben-Dor", "B. Chor", "R. Karp", "Z. Yakhini" ],
      "venue" : "J. Comput. Biol.,",
      "citeRegEx" : "Ben.Dor et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ben.Dor et al\\.",
      "year" : 2003
    }, {
      "title" : "On the Goldstein-Levitin-Polyak gradient projection method",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "IEEE Trans. Automat. Control,",
      "citeRegEx" : "Bertsekas.,? \\Q1976\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1976
    }, {
      "title" : "Biclustering of expression data",
      "author" : [ "Y. Cheng", "G.M. Church" ],
      "venue" : "In Proc. Int. Conf. on Intelligent Systems for Molecular Biology,",
      "citeRegEx" : "Cheng and Church.,? \\Q2000\\E",
      "shortCiteRegEx" : "Cheng and Church.",
      "year" : 2000
    }, {
      "title" : "Rectified factor networks",
      "author" : [ "D.-A. Clevert", "T. Unterthiner", "A. Mayr", "S. Hochreiter" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Clevert et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2015
    }, {
      "title" : "Posterior regularization for structured latent variable models",
      "author" : [ "K. Ganchev", "J. Graca", "J. Gillenwater", "B. Taskar" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ganchev et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ganchev et al\\.",
      "year" : 2010
    }, {
      "title" : "HapFABIA: Identification of very short segments of identity by descent characterized by rare variants in large sequencing data",
      "author" : [ "S. Hochreiter" ],
      "venue" : "Nucleic Acids Res.,",
      "citeRegEx" : "Hochreiter.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 2013
    }, {
      "title" : "FABIA: factor analysis for bicluster",
      "author" : [ "S. Hochreiter", "U. Bodenhofer", "M. Heusel", "A. Mayr", "A. Mitterecker", "A. Kasim", "S. VanSanden", "D. Lin", "W. Talloen", "L. Bijnens", "H.W.H. Göhlmann", "Z. Shkedy", "D.-A. Clevert" ],
      "venue" : "acquisition. Bioinformatics,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2010
    }, {
      "title" : "Subclass mapping: Identifying common subtypes in independent disease data sets",
      "author" : [ "Y. Hoshida", "J.-P. Brunet", "P. Tamayo", "T.R. Golub", "J.P. Mesirov" ],
      "venue" : "PLoS ONE,",
      "citeRegEx" : "Hoshida et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hoshida et al\\.",
      "year" : 2007
    }, {
      "title" : "Non-negative matrix factorization with sparseness constraints",
      "author" : [ "P.O. Hoyer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Hoyer.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hoyer.",
      "year" : 2004
    }, {
      "title" : "Defining transcription modules using large-scale gene expression data",
      "author" : [ "J. Ihmels", "S. Bergmann", "N. Barkai" ],
      "venue" : null,
      "citeRegEx" : "Ihmels et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Ihmels et al\\.",
      "year" : 1993
    }, {
      "title" : "Iterative Methods for Optimization",
      "author" : [ "C.T. Kelley" ],
      "venue" : "Society for Industrial and Applied Mathematics (SIAM), Philadelphia,",
      "citeRegEx" : "Kelley.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kelley.",
      "year" : 1999
    }, {
      "title" : "Spectral biclustering of microarray data: Coclustering genes and conditions",
      "author" : [ "Y. Kluger", "R. Basri", "J.T. Chang", "M.B. Gerstein" ],
      "venue" : "Genome Res.,",
      "citeRegEx" : "Kluger et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kluger et al\\.",
      "year" : 2003
    }, {
      "title" : "Minimax localization of structural information in large noisy matrices",
      "author" : [ "Mladen Kolar", "Sivaraman Balakrishnan", "Alessandro Rinaldo", "Aarti Singh" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kolar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolar et al\\.",
      "year" : 2011
    }, {
      "title" : "Plaid models for gene expression data",
      "author" : [ "L. Lazzeroni", "A. Owen" ],
      "venue" : "Stat. Sinica,",
      "citeRegEx" : "Lazzeroni and Owen.,? \\Q2002\\E",
      "shortCiteRegEx" : "Lazzeroni and Owen.",
      "year" : 2002
    }, {
      "title" : "Evaluating the statistical significance of biclusters",
      "author" : [ "Jason D Lee", "Yuekai Sun", "Jonathan E Taylor" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Lee et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2015
    }, {
      "title" : "Extracting conserved gene expression motifs from gene expression data",
      "author" : [ "T.M. Murali", "S. Kasif" ],
      "venue" : "In Pac. Symp. Biocomputing,",
      "citeRegEx" : "Murali and Kasif.,? \\Q2003\\E",
      "shortCiteRegEx" : "Murali and Kasif.",
      "year" : 2003
    }, {
      "title" : "Biclustering using message passing",
      "author" : [ "Luke O ́ Connor", "Soheil Feizi" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Connor and Feizi.,? \\Q2014\\E",
      "shortCiteRegEx" : "Connor and Feizi.",
      "year" : 2014
    }, {
      "title" : "Sharing of very short IBD segments between humans, neandertals, and denisovans",
      "author" : [ "G. Povysil", "S. Hochreiter" ],
      "venue" : "In bioRxiv. WWW,",
      "citeRegEx" : "Povysil and Hochreiter.,? \\Q2014\\E",
      "shortCiteRegEx" : "Povysil and Hochreiter.",
      "year" : 2014
    }, {
      "title" : "A systematic comparison and evaluation of biclustering methods for gene expression data",
      "author" : [ "A. Prelic", "S. Bleuler", "P. Zimmermann", "A. Wille", "P. Bühlmann", "W. Gruissem", "L. Hennig", "L. Thiele", "E. Zitzler" ],
      "venue" : null,
      "citeRegEx" : "Prelic et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Prelic et al\\.",
      "year" : 2006
    }, {
      "title" : "The use of molecular profiling to predict survival after chemotherapy for diffuse large-B-cell lymphoma",
      "author" : [ "A. Rosenwald" ],
      "venue" : "New Engl. J. Med.,",
      "citeRegEx" : "Rosenwald,? \\Q2002\\E",
      "shortCiteRegEx" : "Rosenwald",
      "year" : 2002
    }, {
      "title" : "Large-scale analysis of the human and mouse transcriptomes",
      "author" : [ "A.I. Su" ],
      "venue" : "P. Natl. Acad. Sci. USA,",
      "citeRegEx" : "Su,? \\Q2002\\E",
      "shortCiteRegEx" : "Su",
      "year" : 2002
    }, {
      "title" : "The gibbs-plaid biclustering model",
      "author" : [ "A. Murua T. Chekouo", "W. Raffelsberger" ],
      "venue" : "Ann. Appl. Stat.,",
      "citeRegEx" : "Chekouo and Raffelsberger.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chekouo and Raffelsberger.",
      "year" : 2015
    }, {
      "title" : "Discovering statistically significant biclusters in gene expression data",
      "author" : [ "A. Tanay", "R. Sharan", "R. Shamir" ],
      "venue" : "Bioinformatics, 18(Suppl",
      "citeRegEx" : "Tanay et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Tanay et al\\.",
      "year" : 2002
    }, {
      "title" : "Improved biclustering of microarray data demonstrated through systematic performance tests",
      "author" : [ "H. Turner", "T. Bailey", "W. Krzanowski" ],
      "venue" : "Comput. Stat. Data An.,",
      "citeRegEx" : "Turner et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Turner et al\\.",
      "year" : 2003
    }, {
      "title" : "Gene expression profiling predicts clinical outcome of breast cancer",
      "author" : [ "L.J. van’t Veer" ],
      "venue" : "Nature, 415:530–536,",
      "citeRegEx" : "Veer,? \\Q2002\\E",
      "shortCiteRegEx" : "Veer",
      "year" : 2002
    }, {
      "title" : "Using transcriptomics to guide lead optimization in drug discovery projects: Lessons learned from the QSTAR project",
      "author" : [ "B. Verbist", "G. Klambauer", "L. Vervoort", "W. Talloen", "Z. Shkedy", "O. Thas", "A. Bender", "H.W.H. Göhlmann", "S. Hochreiter" ],
      "venue" : "Drug Discovery Today,",
      "citeRegEx" : "Verbist et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Verbist et al\\.",
      "year" : 2015
    }, {
      "title" : "Identification of transcription factors for drug-associated gene modules and biomedical implications",
      "author" : [ "M. Xiong", "B. Li", "Q. Zhu", "Y.-X. Wang", "H.-Y. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Xiong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2014
    }, {
      "title" : "An improved biclustering method for analyzing gene expression profiles",
      "author" : [ "J. Yang", "H. Wang", "W. Wang", "P.S. Yu" ],
      "venue" : "Int. J. Artif. Intell. T.,",
      "citeRegEx" : "Yang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Kasim & Talloen, 2016), and recently it also became popular in the machine learning community (O ́ Connor & Feizi, 2014; Lee et al., 2015; Kolar et al., 2011), e.",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "Kasim & Talloen, 2016), and recently it also became popular in the machine learning community (O ́ Connor & Feizi, 2014; Lee et al., 2015; Kolar et al., 2011), e.",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 26,
      "context" : "In this example different chemical compounds are added to a cell line and the gene expression is measured (Verbist et al., 2015).",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "FABIA (factor analysis for bicluster acquisition, (Hochreiter et al., 2010)) evolved into one of the most successful biclustering methods.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "A detailed comparison has shown FABIA’s superiority over existing biclustering methods both on simulated data and real-world gene expression data (Hochreiter et al., 2010).",
      "startOffset" : 146,
      "endOffset" : 171
    }, {
      "referenceID" : 27,
      "context" : "It has been applied to genomics, where it identified in gene expression data task-relevant biological modules (Xiong et al., 2014).",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 26,
      "context" : "In the large drug design project QSTAR, FABIA was used to extract biclusters from a data matrix that contains bioactivity measurements across compounds (Verbist et al., 2015).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "These individuals inherited an IBD region from a common ancestor (Hochreiter, 2013; Povysil & Hochreiter, 2014).",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "FABIA is a generative model that enforces sparse codes (Hochreiter et al., 2010) and, thereby, detects biclusters.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Rectified Factor Networks (RFNs; (Clevert et al., 2015)) RFNs overcome the shortcomings of FABIA.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "We propose to use the recently introduced Rectified Factor Networks (RFNs; (Clevert et al., 2015)) for biclustering to overcome the drawbacks of the FABIA model.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "RFN models are selected via the posterior regularization method (Ganchev et al., 2010).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "(2) leads to constrained optimization problem (see Clevert et al. (2015))",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "In the E-step of the generalized alternating minimization algorithm (Ganchev et al., 2010), which is used for RFN model selection, we only perform a step of the gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton method for solving Eq.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : ", 2010), which is used for RFN model selection, we only perform a step of the gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton method for solving Eq.",
      "startOffset" : 108,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : ", 2010), which is used for RFN model selection, we only perform a step of the gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton method for solving Eq.",
      "startOffset" : 108,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "(4) (Clevert et al., 2015).",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "The constraint posterior of a code unit is obtained by multiplying the input by a vector and subsequently rectifying and normalizing the code unit (Clevert et al., 2015).",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "To assess the performance of rectified factor networks (RFNs) as unsupervised biclustering methods, we compare the following 14 biclustering methods: (1) RFN: rectified factor networks (Clevert et al., 2015), (2) FABIA: factor analysis with Laplace prior on the hidden units (Hochreiter et al.",
      "startOffset" : 185,
      "endOffset" : 207
    }, {
      "referenceID" : 7,
      "context" : ", 2015), (2) FABIA: factor analysis with Laplace prior on the hidden units (Hochreiter et al., 2010; Hochreiter, 2013), (3) FABIAS: factor analysis with sparseness projection (Hochreiter et al.",
      "startOffset" : 75,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : ", 2015), (2) FABIA: factor analysis with Laplace prior on the hidden units (Hochreiter et al., 2010; Hochreiter, 2013), (3) FABIAS: factor analysis with sparseness projection (Hochreiter et al.",
      "startOffset" : 75,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : ", 2010; Hochreiter, 2013), (3) FABIAS: factor analysis with sparseness projection (Hochreiter et al., 2010), (4) MFSC: matrix factorization with sparseness constraints (Hoyer, 2004), (5) plaid: plaid model (Lazzeroni & Owen, 2002; T.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : ", 2010), (4) MFSC: matrix factorization with sparseness constraints (Hoyer, 2004), (5) plaid: plaid model (Lazzeroni & Owen, 2002; T.",
      "startOffset" : 68,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : ", 2004), (7) OPSM: orderpreserving sub-matrices (Ben-Dor et al., 2003), (8) SAMBA: statistical-algorithmic method for bicluster analysis (Tanay et al.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : ", 2003), (8) SAMBA: statistical-algorithmic method for bicluster analysis (Tanay et al., 2002), (9) xMOTIF: conserved motifs (Murali & Kasif, 2003), (10) Bimax: divide-and-conquer algorithm (Prelic et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : ", 2002), (9) xMOTIF: conserved motifs (Murali & Kasif, 2003), (10) Bimax: divide-and-conquer algorithm (Prelic et al., 2006), (11) CC: Cheng-Church -biclusters",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "(Cheng & Church, 2000), (12) plaid t: improved plaid model (Turner et al., 2003), (13) FLOC: flexible overlapped biclustering, a generalization of CC (Yang et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 28,
      "context" : ", 2003), (13) FLOC: flexible overlapped biclustering, a generalization of CC (Yang et al., 2005), and (14) spec: spectral biclustering (Kluger et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : ", 2005), and (14) spec: spectral biclustering (Kluger et al., 2003).",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "For method evaluation, we use the previously introduced biclustering consensus score for two sets of biclusters (Hochreiter et al., 2010), which is computed as follows:",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "We consider three gene expression data sets which have been provided by the Broad Institute and were previously clustered by Hoshida et al. (2007) using additional data sets.",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "We consider three gene expression data sets which have been provided by the Broad Institute and were previously clustered by Hoshida et al. (2007) using additional data sets. Our goal was to study how well biclustering methods are able to recover these clusters without any additional information. (A) The “breast cancer” data set (van’t Veer et al., 2002) was aimed at a predictive gene signature for the outcome of a breast cancer therapy. We removed the outlier array S54 which leads to a data set with 97 samples and 1213 genes. In Hoshida et al. (2007), three biologically meaningful subclasses were found that should be re-identified.",
      "startOffset" : 125,
      "endOffset" : 558
    }, {
      "referenceID" : 8,
      "context" : "We consider three gene expression data sets which have been provided by the Broad Institute and were previously clustered by Hoshida et al. (2007) using additional data sets. Our goal was to study how well biclustering methods are able to recover these clusters without any additional information. (A) The “breast cancer” data set (van’t Veer et al., 2002) was aimed at a predictive gene signature for the outcome of a breast cancer therapy. We removed the outlier array S54 which leads to a data set with 97 samples and 1213 genes. In Hoshida et al. (2007), three biologically meaningful subclasses were found that should be re-identified. (B) The “multiple tissue types” data set (Su et al., 2002) are gene expression profiles from human cancer samples from diverse tissues and cell lines. The data set contains 102 samples with 5565 genes. Biclustering should be able to re-identify the tissue types. (C) The “diffuse large-B-cell lymphoma (DLBCL)” data set (Rosenwald et al., 2002) was aimed at predicting the survival after chemotherapy. It contains 180 samples and 661 genes. The three classes found by Hoshida et al. (2007) should be re-identified.",
      "startOffset" : 125,
      "endOffset" : 1131
    }, {
      "referenceID" : 6,
      "context" : "Biclustering is well-suited to detect such IBD segments in a genotype matrix (Hochreiter, 2013; Povysil & Hochreiter, 2014), which has individuals as row elements and genomic structural variations (SNVs) as column elements.",
      "startOffset" : 77,
      "endOffset" : 123
    } ],
    "year" : 2016,
    "abstractText" : "Biclustering is evolving into one of the major tools for analyzing large datasets given as matrix of samples times features. Biclustering has several noteworthy applications and has been successfully applied in life sciences and e-commerce for drug design and recommender systems, respectively. FABIA is one of the most successful biclustering methods and is used by companies like Bayer, Janssen, or Zalando. FABIA is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated. Sample membership is difficult to determine because vectors do not have exact zero entries and can have both large positive and large negative values. We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of existing biclustering methods. RFNs efficiently construct very sparse, non-linear, highdimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster. On 400 benchmark datasets with artificially implanted biclusters, RFN significantly outperformed 13 other biclustering competitors including FABIA. In biclustering experiments on three gene expression datasets with known clusters that were determined by separate measurements, RFN biclustering was two times significantly better than the other 13 methods and once on second place. On data of the 1000 Genomes Project, RFN could identify DNA segments which indicate, that interbreeding with other hominins starting already before ancestors of modern humans left Africa.",
    "creator" : "LaTeX with hyperref package"
  }
}