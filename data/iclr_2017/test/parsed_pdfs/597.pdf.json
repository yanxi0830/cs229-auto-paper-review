{
  "name" : "597.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "REINFORCEMENT LEARNING", "Irwan Bello", "Hieu Pham", "Quoc V. Le", "Mohammad Norouzi", "Samy Bengio" ],
    "emails" : [ "ibello@google.com", "hyhieu@google.com", "qvl@google.com", "mnorouzi@google.com", "bengio@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Combinatorial optimization is a fundamental problem in computer science. A canonical example is the traveling salesman problem (TSP), where given a graph, one needs to search the space of permutations to find an optimal sequence of nodes with minimal total edge weights (tour length). The TSP and its variants have myriad applications in planning, manufacturing, genetics, etc. (see (Applegate et al., 2011) for an overview).\nFinding the optimal TSP solution is NP-hard, even in the two-dimensional Euclidean case (Papadimitriou, 1977), where the nodes are 2D points and edge weights are Euclidean distances between pairs of points. In practice, TSP solvers rely on handcrafted heuristics that guide their search procedures to find competitive (and in many cases optimal) tours efficiently. Even though these heuristics work well on TSP, once the problem statement changes slightly, they need to be revised. In contrast, machine learning methods have the potential to be applicable across many optimization tasks by automatically discovering their own heuristics based on the training data, thus requiring less handengineering than solvers that are optimized for one task only.\nWhile most successful machine learning techniques fall into the family of supervised learning, where a mapping from training inputs to outputs is learned, supervised learning is not applicable to most combinatorial optimization problems because one does not have access to optimal labels. However, one can compare the quality of a set of solutions using a verifier, and provide some reward feedbacks to a learning algorithm. Hence, we follow the reinforcement learning (RL) paradigm to tackle combinatorial optimization. We empirically demonstrate that, even when using optimal solutions as labeled data to optimize a supervised mapping, the generalization is rather poor compared to an RL agent that explores different tours and observes their corresponding rewards.\nWe propose Neural Combinatorial Optimization, a framework to tackle combinatorial optimization problems using reinforcement learning and neural networks. We consider two approaches based on policy gradients (Williams, 1992). The first approach, called RL pretraining, uses a training set to optimize a recurrent neural network (RNN) that parameterizes a stochastic policy over solutions, using the expected reward as objective. At test time, the policy is fixed, and one performs inference ∗Equal contributions. Members of the Google Brain Residency program (g.co/brainresidency).\nby greedy decoding or sampling. The second approach, called active search, involves no pretraining. It starts from a random policy and iteratively optimizes the RNN parameters on a single test instance, again using the expected reward objective, while keeping track of the best solution sampled during the search. We find that combining RL pretraining and active search works best in practice.\nOn 2D Euclidean graphs with up to 100 nodes, Neural Combinatorial Optimization significantly outperforms the supervised learning approach to the TSP (Vinyals et al., 2015b) and obtains close to optimal results when allowed more computation time (see Figure 1). We illustrate the flexibility of the method by also applying it to the KnapSack problem, for which we get optimal results for instances with up to 200 items. Our results, while still inferior to the state-of-the-art in many dimensions (such as speed, scale and performance), give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems, especially those that are difficult to design heuristics for."
    }, {
      "heading" : "2 PREVIOUS WORK",
      "text" : "The Traveling Salesman Problem is a well studied combinatorial optimization problem and many exact or approximate algorithms have been proposed for both Euclidean and non-Euclidean graphs. Christofides (1976) proposes a heuristic algorithm that involves computing a minimum-spanning tree and a minimum-weight perfect matching. The algorithm has polynomial running time and returns solutions that are guaranteed to be within a factor of 1.5× to optimality in the metric instance of the TSP.\nThe best known exact dynamic programming algorithm for TSP has a complexity of Θ(2nn2), making it infeasible to scale up to large instances, say with 40 points. Nevertheless, state of the art TSP solvers, thanks to carefully handcrafted heuristics that describe how to navigate the space of feasible solutions in an efficient manner, can solve symmetric TSP instances with thousands of nodes. Concorde (Applegate et al., 2006), widely accepted as one of the best exact TSP solvers, makes use of cutting plane algorithms (Dantzig et al., 1954; Padberg & Rinaldi, 1990; Applegate et al., 2003), iteratively solving linear programming relaxations of the TSP, in conjunction with a branch-and-bound approach that prunes parts of the search space that provably will not contain an optimal solution. Similarly, the Lin-Kernighan-Helsgaun heuristic (Helsgaun, 2000), inspired from the Lin-Kernighan heuristic (Lin & Kernighan, 1973), is a state of the art approximate search heuristic for the symmetric TSP and has been shown to solve instances with hundreds of nodes to optimality.\nMore generic solvers, such as Google’s vehicle routing problem solver (Google, 2016) that tackles a superset of the TSP, typically rely on a combination of local search algorithms and metaheuristics. Local search algorithms apply a specified set of local move operators on candidate solutions, based\non hand-engineered heuristics such as 2-opt (Johnson, 1990), to navigate from solution to solution in the search space. A metaheuristic is then applied to propose uphill moves and escape local optima. A popular choice of metaheuristic for the TSP and its variants is guided local search (Voudouris & Tsang, 1999), which moves out of a local minimum by penalizing particular solution features that it considers should not occur in a good solution.\nThe difficulty in applying existing search heuristics to newly encountered problems - or even new instances of a similar problem - is a well-known challenge that stems from the No Free Lunch theorem (Wolpert & Macready, 1997). Because all search algorithms have the same performance when averaged over all problems, one must appropriately rely on a prior over problems when selecting a search algorithm to guarantee performance. This challenge has fostered interest in raising the level of generality at which optimization systems operate (Burke et al., 2003) and is the underlying motivation behind hyper-heuristics, defined as ”search method[s] or learning mechanism[s] for selecting or generating heuristics to solve computation search problems”. Hyper-heuristics aim to be easier to use than problem specific methods by partially abstracting away the knowledge intensive process of selecting heuristics given a combinatorial problem and have been shown to successfully combine human-defined heuristics in superior ways across many tasks (see (Burke et al., 2013) for a survey). However, hyper-heuristics operate on the search space of heuristics, rather than the search space of solutions, therefore still initially relying on human created heuristics.\nThe application of neural networks to combinatorial optimization has a distinguished history, where the majority of research focuses on the Traveling Salesman Problem (Smith, 1999). One of the earliest proposals is the use of Hopfield networks (Hopfield & Tank, 1985) for the TSP. The authors modify the network’s energy function to make it equivalent to TSP objective and use Lagrange multipliers to penalize the violations of the problem’s constraints. A limitation of this approach is that it is sensitive to hyperparameters and parameter initialization as analyzed by (Wilson & Pawley, 1988). Overcoming this limitation is central to the subsequent work in the field, especially by (Aiyer et al., 1990; Gee, 1993). Parallel to the development of Hopfield networks is the work on using deformable template models to solve TSP. Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al., 1988; Kohonen, 1990). Addressing the limitations of deformable template models is central to the following work in this area (Burke, 1994; Favata & Walker, 1991; Vakhutinsky & Golden, 1995). Even though these neural networks have many appealing properties, they are still limited as research work. When being carefully benchmarked, they have not yielded satisfying results compared to algorithmic methods (Sarwar & Bhatti, 2012; La Maire & Mladenov, 2012). Perhaps due to the negative results, this research direction is largely overlooked since the turn of the century.\nMotivated by the recent advancements in sequence-to-sequence learning (Sutskever et al., 2014), neural networks are again the subject of study for optimization in various domains (Yutian et al., 2016), including discrete ones (Zoph & Le, 2016). In particular, the TSP is revisited in the introduction of Pointer Networks (Vinyals et al., 2015b), where a recurrent network with non-parametric softmaxes is trained in a supervised manner to predict the sequence of visited cities. Despite architecural improvements, their models were trained using supervised signals given by an approximate solver."
    }, {
      "heading" : "3 NEURAL NETWORK ARCHITECTURE FOR TSP",
      "text" : "We focus on the 2D Euclidean TSP in this paper. Given an input graph, represented as a sequence of n cities in a two dimensional space s = {xi}ni=1 where each xi ∈ R2, we are concerned with finding a permutation of the points π, termed a tour, that visits each city once and has the minimum total length. We define the length of a tour defined by a permutation π as\nL(π | s) = ∥∥xπ(n) − xπ(1)∥∥2 + n−1∑\ni=1 ∥∥xπ(i) − xπ(i+1)∥∥2 , (1) where ‖·‖2 denotes `2 norm. We aim to learn the parameters of a stochastic policy p(π | s) that given an input set of points s, assigns high probabilities to short tours and low probabilities to long tours. Our neural network\narchitecture uses the chain rule to factorize the probability of a tour as\np(π | s) = n∏ i=1 p (π(i) | π(< i), s) , (2)\nand then uses individual softmax modules to represent each term on the RHS of (2).\nWe are inspired by previous work (Sutskever et al., 2014) that makes use of the same factorization based on the chain rule to address sequence to sequence problems like machine translation. One can use a vanilla sequence to sequence model to address the TSP where the output vocabulary is {1, 2, . . . , n}. However, there are two major issues with this approach: (1) networks trained in this fashion cannot generalize to inputs with more than n cities. (2) one needs to have access to groundtruth output permutations to optimize the parameters with conditional log-likelihood. We address both isssues in this paper.\nFor generalization beyond a pre-specified graph size, we follow the approach of (Vinyals et al., 2015b), which makes use of a set of non-parameteric softmax modules, resembling the attention mechanism from (Bahdanau et al., 2015). This approach, named pointer network, allows the model to effectively point to a specific position in the input sequence rather than predicting an index value from a fixed-size vocabulary. We employ the pointer network architecture, depicted in Figure 2, as our policy model to parameterize p(π | s)."
    }, {
      "heading" : "3.1 ARCHITECTURE DETAILS",
      "text" : "Our pointer network comprises two recurrent neural network (RNN) modules, encoder and decoder, both of which consist of Long Short-Term Memory (LSTM) cells (Hochreiter & Schmidhuber, 1997). The encoder network reads the input sequence s, one city at a time, and transforms it into a sequence of latent memory states {enci}ni=1 where enci ∈ Rd. The input to the encoder network at time step i is a d-dimensional embedding of a 2D point xi, which is obtained via a linear transformation of xi shared across all input steps. The decoder network also maintains its latent memory states {deci}ni=1 where deci ∈ Rd and, at each step i, uses a pointing mechanism to produce a distribution over the next city to visit in the tour. Once the next city is selected, it is passed as the input to the next decoder step. The input of the first decoder step (denoted by 〈g〉 in Figure 2) is a d-dimensional vector treated as a trainable parameter of our neural network.\nOur attention function, formally defined in Appendix A.1, takes as input a query vector q = deci ∈ Rd and a set of reference vectors ref = {enc1, . . . , enck} where enci ∈ Rd, and predicts a distribution A(ref, q) over the set of k references. This probability distribution represents the degree to which the model is pointing to reference ri upon seeing query q.\nVinyals et al. (2015a) also suggest including some additional computation steps, named glimpses, to aggregate the contributions of different parts of the input sequence, very much like (Bahdanau et al., 2015). We discuss this approach in details in Appendix A.1. In our experiments, we find that utilizing one glimpse in the pointing mechanism yields performance gains at an insignificant cost latency.\nAlgorithm 1 Actor-critic training 1: procedure TRAIN(training set S, number of training steps T , batch size B) 2: Initialize pointer network params θ 3: Initialize critic network params θv 4: for t = 1 to T do 5: si ∼ SAMPLEINPUT(S) for i ∈ {1, . . . , B} 6: πi ∼ SAMPLESOLUTION(pθ(.|si)) for i ∈ {1, . . . , B} 7: bi ← bθv (si) for i ∈ {1, . . . , B} 8: gθ ← 1B ∑B i=1(L(πi|si)− bi)∇θ log pθ(πi|si)\n9: Lv ← 1B ∑B i=1 ‖bi − L(πi)‖ 2 2\n10: θ ← ADAM(θ, gθ) 11: θv ← ADAM(θv,∇θvLv) 12: end for 13: return θ 14: end procedure"
    }, {
      "heading" : "4 OPTIMIZATION WITH POLICY GRADIENTS",
      "text" : "Vinyals et al. (2015b) proposes training a pointer network using a supervised loss function comprising conditional log-likelihood, which factors into a cross entropy objective between the network’s output probabilities and the targets provided by a TSP solver. Learning from examples in such a way is undesirable for NP-hard problems because (1) the performance of the model is tied to the quality of the supervised labels, (2) getting high-quality labeled data is expensive and may be infeasible for new problem statements, (3) one cares about finding a competitive solution more than replicating the results of another algorithm.\nBy contrast, we believe Reinforcement Learning (RL) provides an appropriate paradigm for training neural networks for combinatorial optimization, especially because these problems have relatively simple reward mechanisms that could be even used at test time. We hence propose to use model-free policy-based Reinforcement Learning to optimize the parameters of a pointer network denoted θ. Our training objective is the expected tour length which, given an input graph s, is defined as\nJ(θ | s) = Eπ∼pθ(.|s) L(π | s) . (3) During training, our graphs are drawn from a distribution S, and the total training objective involves sampling from the distribution of graphs, i.e. J(θ) = Es∼S J(θ | s) . We resort to policy gradient methods and stochastic gradient descent to optimize the parameters. The gradient of (3) is formulated using the well-known REINFORCE algorithm (Williams, 1992):\n∇θJ(θ | s) = Eπ∼pθ(.|s) [( L(π | s)− b(s) ) ∇θ log pθ(π | s) ] , (4)\nwhere b(s) denotes a baseline function that does not depend on π and estimates the expected tour length to reduce the variance of the gradients.\nBy drawing B i.i.d. sample graphs s1, s2, . . . , sB ∼ S and sampling a single tour per graph, i.e. πi ∼ pθ(. | si), the gradient in (4) is approximated with Monte Carlo sampling as follows:\n∇θJ(θ) ≈ 1\nB B∑ i=1 ( L(πi|si)− b(si) ) ∇θ log pθ(πi | si) . (5)\nA simple and popular choice of the baseline b(s) is an exponential moving average of the rewards obtained by the network over time to account for the fact that the policy improves with training. While this choice of baseline proved sufficient to improve upon the Christofides algorithm, it suffers from not being able to differentiate between different input graphs. In particular, the optimal tour π∗ for a difficult graph s may be still discouraged if L(π∗|s) > b because b is shared across all instances in the batch.\nUsing a parametric baseline to estimate the expected tour length Eπ∼pθ(.|s)L(π | s) typically improves learning. Therefore, we introduce an auxiliary network, called a critic and parameterized\nAlgorithm 2 Active Search 1: procedure ACTIVESEARCH(input s, θ, number of candidates K, B, α) 2: π ← RANDOMSOLUTION() 3: Lπ ← L(π | s) 4: n← dK\nB e\n5: for t = 1 . . . n do 6: πi ∼ SAMPLESOLUTION(pθ(. | s)) for i ∈ {1, . . . , B} 7: j ← ARGMIN(L(π1 | s) . . . L(πB | s)) 8: Lj ← L(πj | s) 9: if Lj < Lπ then\n10: π ← πj 11: Lπ ← Lj 12: end if 13: gθ ← 1B ∑B i=1(L(πi | s)− b)∇θ log pθ(πi | s) 14: θ ← ADAM(θ, gθ) 15: b← α× b+ (1− α)× ( 1\nB ∑B i=1 bi)\n16: end for 17: return π 18: end procedure\nby θv , to learn the expected tour length found by our current policy pθ given an input sequence s. The critic is trained with stochastic gradient descent on a mean squared error objective between its predictions bθv (s) and the actual tour lengths sampled by the most recent policy. The additional objective is formulated as\nL(θv) = 1\nB B∑ i=1 ∥∥ bθv (si)− L(πi | si)∥∥22 . (6) Critic’s architecture for TSP. We now explain how our critic maps an input sequence s into a baseline prediction bθv (s). Our critic comprises three neural network modules: 1) an LSTM encoder, 2) an LSTM process block and 3) a 2-layer ReLU neural network decoder. Its encoder has the same architecture as that of our pointer network’s encoder and encodes an input sequence s into a sequence of latent memory states and a hidden state h. The process block, similarly to (Vinyals et al., 2015a), then performs P steps of computation over the hidden state h. Each processing step updates this hidden state by glimpsing at the memory states as described in Appendix A.1 and feeds the output of the glimpse function as input to the next processing step. At the end of the process block, the obtained hidden state is then decoded into a baseline prediction (i.e a single scalar) by two fully connected layers with respectively d and 1 unit(s).\nOur training algorithm, described in Algorithm 1, is closely related to the asynchronous advantage actor-critic (A3C) proposed in (Mnih et al., 2016), as the difference between the sampled tour lengths and the critic’s predictions is an unbiased estimate of the advantage function. We perform our updates asynchronously across multiple workers, but each worker also handles a mini-batch of graphs for better gradient estimates."
    }, {
      "heading" : "4.1 SEARCH STRATEGIES",
      "text" : "As evaluating a tour length is inexpensive, our TSP agent can easily simulate a search procedure at inference time by considering multiple candidate solutions per graph and selecting the best. This inference process resembles how solvers search over a large set of feasible solutions. In this paper, we consider two search strategies detailed below, which we refer to as sampling and active search.\nSampling. Our first approach is simply to sample multiple candidate tours from our stochastic policy pθ(.|s) and select the shortest one. In contrast to heuristic solvers, we do not enforce our model to sample different tours during the process. However, we can control the diversity of the sampled tours with a temperature hyperparameter when sampling from our non-parametric softmax (see Appendix A.2). This sampling process yields significant improvements over greedy decoding, which always selects the index with the largest probability. We also considered perturbing the pointing\nmechanism with random noise and greedily decoding from the obtained modified policy, similarly to (Cho, 2016), but this proves less effective than sampling in our experiments.\nActive Search. Rather than sampling with a fixed model and ignoring the reward information obtained from the sampled solutions, one can refine the parameters of the stochastic policy pθ during inference to minimize Eπ∼pθ(.|s)L(π | s) on a single test input s. This approach proves especially competitive when starting from a trained model. Remarkably, it also produces satisfying solutions when starting from an untrained model. We refer to these two approaches as RL pretraining-Active Search and Active Search because the model actively updates its parameters while searching for candidate solutions on a single test instance.\nActive Search applies policy gradients similarly to Algorithm 1 but draws Monte Carlo samples over candidate solutions π1 . . . πB ∼ pθ(·|s) for a single test input. It resorts to an exponential moving average baseline, rather than a critic, as there is no need to differentiate between inputs. Our Active Search training algorithm is presented in Algorithm 2. We note that while RL training does not require supervision, it still requires training data and hence generalization depends on the training data distribution. In contrast, Active Search is distribution independent. Finally, since we encode a set of cities as a sequence, we randomly shuffle the input sequence before feeding it to our pointer network. This increases the stochasticity of the sampling procedure and leads to large improvements in Active Search."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We conduct experiments to investigate the behavior of the proposed Neural Combinatorial Optimization methods. We consider three benchmark tasks, Euclidean TSP20, 50 and 100, for which we generate a test set of 1, 000 graphs. Points are drawn uniformly at random in the unit square [0, 1]2."
    }, {
      "heading" : "5.1 EXPERIMENTAL DETAILS",
      "text" : "Across all experiments, we use mini-batches of 128 sequences, LSTM cells with 128 hidden units, and embed the two coordinates of each point in a 128-dimensional space. We train our models with the Adam optimizer (Kingma & Ba, 2014) and use an initial learning rate of 10−3 for TSP20 and TSP50 and 10−4 for TSP100 that we decay every 5000 steps by a factor of 0.96. We initialize our parameters uniformly at random within [−0.08, 0.08] and clip the L2 norm of our gradients to 1.0. We use up to one attention glimpse. When searching, the mini-batches either consist of replications of the test sequence or its permutations. The baseline decay is set to α = 0.99 in Active Search. Our model and training code in Tensorflow (Abadi et al., 2016) will be made availabe soon. Table 1 summarizes the configurations and different search strategies used in the experiments. The variations of our method, experimental procedure and results are as follows.\nSupervised Learning. In addition to the described baselines, we implement and train a pointer network with supervised learning, similarly to (Vinyals et al., 2015b). While our supervised data consists of one million optimal tours, we find that our supervised learning results are not as good as those reported in by (Vinyals et al., 2015b). We suspect that learning from optimal tours is harder for supervised pointer networks due to subtle features that the model cannot figure out only by looking at given supervised targets. We thus refer to the results in (Vinyals et al., 2015b) for TSP20 and TSP50 and report our results on TSP100, all of which are suboptimal compared to other approaches.\nRL pretraining. For the RL experiments, we generate training mini-batches of inputs on the fly and update the model parameters with the Actor Critic Algorithm 1. We use a validation set of 10, 000 randomly generated instances for hyper-parameters tuning. Our critic consists of an encoder network which has the same architecture as that of the policy network, but followed by 3 processing steps and 2 fully connected layers. We find that clipping the logits to [−10, 10] with a tanh(·) activation function, as described in Appendix A.2, helps with exploration and yields marginal performance gains. The simplest search strategy using an RL pretrained model is greedy decoding, i.e. selecting the city with the largest probability at each decoding step. We also experiment with decoding greedily from a set of 16 pretrained models at inference time. For each graph, the tour found by each individual model is collected and the shortest tour is chosen. We refer to those approaches as RL pretraining-greedy and RL pretraining-greedy@16.\nRL pretraining-Sampling. For each test instance, we sample 1, 280, 000 candidate solutions from a pretrained model and keep track of the shortest tour. A grid search over the temperature hyperparameter found respective temperatures of 2.0, 2.2 and 1.5 to yield the best results for TSP20, TSP50 and TSP100. We refer to the tuned temperature hyperparameter as T ∗. Since sampling does not require parameter udpates and is entirely parallelizable, we use a larger batch size for speed purposes.\nRL pretraining-Active Search. For each test instance, we initialize the model parameters from a pretrained RL model and run Active Search for up to 10, 000 training steps with a batch size of 128, sampling a total of 1, 280, 000 candidate solutions. We set the learning rate to a hundredth of the initial learning rate the TSP agent was trained on (i.e. 10−5 for TSP20/TSP50 and 10−6 for TSP100).\nActive Search. We allow the model to train much longer to account for the fact that it starts from scratch. For each test graph, we run Active Search for 100, 000 training steps on TSP20/TSP50 and 200, 000 training steps on TSP100."
    }, {
      "heading" : "5.2 RESULTS AND ANALYSES",
      "text" : "We compare our methods against 3 different baselines of increasing performance and complexity: 1) Christofides, 2) the vehicle routing solver from OR-Tools (Google, 2016) and 3) optimality. Christofides solutions are obtained in polynomial time and guaranteed to be within a 1.5 ratio of optimality. OR-Tools improves over Christofides’ solutions with simple local search operators, including 2-opt (Johnson, 1990) and a version of the Lin-Kernighan heuristic (Lin & Kernighan, 1973), stopping when it reaches a local minimum. In order to escape poor local optima, ORTools’ local search can also be run in conjunction with different metaheuristics, such as simulated annealing (Kirkpatrick et al., 1983), tabu search (Glover & Laguna, 2013) or guided local search (Voudouris & Tsang, 1999). OR-Tools’ vehicle routing solver can tackle a superset of the TSP and operates at a higher level of generality than solvers that are highly specific to the TSP. While not state-of-the art for the TSP, it is a common choice for general routing problems and provides a reasonable baseline between the simplicity of the most basic local search operators and the sophistication of the strongest solvers. Optimal solutions are obtained via Concorde (Applegate et al., 2006) and LK-H’s local search (Helsgaun, 2012; 2000). While only Concorde provably solves instances to optimality, we empirically find that LK-H also achieves optimal solutions on all of our test sets after 50 trials per graph (which is the default parameter setting).\nWe report the average tour lengths of our approaches on TSP20, TSP50, and TSP100 in Table 2. Notably, results demonstrate that training with RL significantly improves over supervised learning\n(Vinyals et al., 2015b). All our methods comfortably surpass Christofides’ heuristic, including RL pretraining-Greedy which also does not rely on search. Table 3 compares the running times of our greedy methods to the aforementioned baselines, with our methods running on a single Nvidia Tesla K80 GPU, Concorde and LK-H running on an Intel Xeon CPU E5-1650 v3 3.50GHz CPU and ORTool on an Intel Haswell CPU. We find that both greedy approaches are time-efficient but still quite far from optimality.\nSearching at inference time proves crucial to get closer to optimality but comes at the expense of longer running times. Fortunately, the search from RL pretraining-Sampling and RL pretrainingActive Search can be stopped early with a small performance tradeoff in terms of the final objective. This can be seen in Table 4, where we show their performances and corresponding running times as a function of how many solutions they consider.\nWe also find that many of our RL pretraining methods outperform OR-Tools’ local search, including RL pretraining-Greedy@16 which runs similarly fast. Table 6 in Appendix A.3 presents the performance of the metaheuristics as they consider more solutions and the corresponding running times. In our experiments, Neural Combinatorial proves superior than Simulated Annealing but is slightly less competitive that Tabu Search and much less so than Guided Local Search.\nWe present a more detailed comparison of our methods in Figure 3, where we sort the ratios to optimality of our different learning configurations. RL pretraining-Sampling and RL pretrainingActive Search are the most competitive Neural Combinatorial Optimization methods and recover the optimal solution in a significant number of our test cases. We find that for small solution spaces, RL pretraining-Sampling, with a finetuned softmax temperature, outperforms RL pretraining-Active Search with the latter sometimes orienting the search towards suboptimal regions of the solution space (see TSP50 results in Table 4 and Figure 3). Furthermore, RL pretraining-Sampling benefits from being fully parallelizable and runs faster than RL pretraining-Active Search. However, for larger solution spaces, RL-pretraining Active Search proves superior both when controlling for the number of sampled solutions or the running time. Interestingly, Active Search - which starts from an untrained model - also produces competitive tours but requires a considerable amount of time (respectively 7 and 25 hours per instance of TSP50/TSP100). Finally, we show randomly picked example tours found by our methods in Figure 4 in Appendix A.4."
    }, {
      "heading" : "6 GENERALIZATION TO OTHER PROBLEMS",
      "text" : "In this section, we discuss how to apply Neural Combinatorial Optimization to other problems than the TSP. In Neural Combinatorial Optimization, the model architecture is tied to the given combinatorial optimization problem. Examples of useful networks include the pointer network, when the output is a permutation or a truncated permutation or a subset of the input, and the classical seq2seq model for other kinds of structured outputs. For combinatorial problems that require to assign labels to elements of the input, such as graph coloring, it is also possible to combine a pointer module and a softmax module to simultaneously point and assign at decoding time. Given a model that encodes an instance of a given combinatorial optimization task and repeatedly branches into subtrees to construct a solution, the training procedures described in Section 4 can then be applied by adapting the reward function depending on the optimization problem being considered.\nAdditionally, one also needs to ensure the feasibility of the obtained solutions. For certain combinatorial problems, it is straightforward to know exactly which branches do not lead to any feasible solutions at decoding time. We can then simply manually assign them a zero probability when decoding, similarly to how we enforce our model to not point at the same city twice in our pointing mechanism (see Appendix A.1). However, for many combinatorial problems, coming up with a feasible solution can be a challenge in itself. Consider, for example, the Travelling Salesman Problem with Time Windows, where the travelling salesman has the additional constraint of visiting each city during a specific time window. It might be that most branches being considered early in the tour do not lead to any solution that respects all time windows. In such cases, knowing exactly which branches are feasible requires searching their subtrees, a time-consuming process that is not much easier than directly searching for the optimal solution unless using problem-specific heuristics.\nRather than explicitly constraining the model to only sample feasible solutions, one can also let the model learn to respect the problem’s constraints. A simple approach, to be verified experimentally in future work, consists in augmenting the objective function with a term that penalizes solutions for violating the problem’s constraints, similarly to penalty methods in constrained optimization. While this does not guarantee that the model consistently samples feasible solutions at inference time, this is not necessarily problematic as we can simply ignore infeasible solutions and resample from the model (for RL pretraining-Sampling and RL-pretraining Active Search). It is also conceivable to combine both approaches by assigning zero probabilities to branches that are easily identifiable as infeasible while still penalizing infeasible solutions once they are entirely constructed."
    }, {
      "heading" : "6.1 KNAPSACK EXAMPLE",
      "text" : "As an example of the flexibility of Neural Combinatorial Optimization, we consider the KnapSack problem, another intensively studied problem in computer science. Given a set of n items i = 1...n, each with weight wi and value vi and a maximum weight capacity ofW , the 0-1 KnapSack problem consists in maximizing the sum of the values of items present in the knapsack so that the sum of the\nweights is less than or equal to the knapsack capacity:\nmax S⊆{1,2,...,n} ∑ i∈S vi\nsubject to ∑ i∈S wi ≤W (7)\nWith wi, vi and W taking real values, the problem is NP-hard (Kellerer et al., 2004). A naive heuristic is to take the items ordered by their weight-to-value ratios until they fill up the weight capacity. Two simple heuristics are ExpKnap, which employs branch-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which uses dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be obtained by quantizing the weights to high precisions and then performing dynamic programming with pseudo-polynomial complexity (Bertsimas & Demir, 2002).\nWe apply the pointer network and encode each KnapSack instance as a sequence of 2D vectors (wi, vi). At decoding time, the pointer network points to items to include in the knapsack and stops when the total weight of the items collected so far exceeds the weight capacity. We generate three datasets, KNAP50, KNAP100 and KNAP200, of a thousand instances with items’ weights and values drawn uniformly at random in [0, 1]. Without loss of generality (since we can scale the items’ weights), we set the capacities to 12.5 for KNAP50 and 25 for KNAP100 and KNAP200. We present the performances of RL pretraining-Greedy and Active Search (which we run for 5, 000 training steps) in Table 5 and compare them to the following baselines: 1) random search (which we let sample as many feasible solutions seen by Active Search), 2) the greedy value-to-weight ratio heuristic, 3) MinKnap, 4) ExpKnap, 5) OR-Tools’ KnapSack solver (Google, 2016) and 6) optimality (which we obtained by quantizing the weights to high precisions and using dynamic programming)."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "This paper presents Neural Combinatorial Optimization, a framework to tackle combinatorial optimization with reinforcement learning and neural networks. We focus on the traveling salesman problem (TSP) and present a set of results for each variation of the framework. Experiments demonstrate that Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Our results, while still far from the strongest solvers (especially those which are optimized for one problem), provide an interesting research avenue for using neural networks as a general tool for tackling combinatorial optimization problems."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank Vincent Furnon, Mustafa Ispir, Lukasz Kaiser, Oriol Vinyals, Barret Zoph, the Google Brain team and the anonymous ICLR reviewers for insightful comments and discussion."
    }, {
      "heading" : "A APPENDIX",
      "text" : "A.1 POINTING AND ATTENDING\nPointing mechanism: Its computations are parameterized by two attention matrices Wref ,Wq ∈ Rd×d and an attention vector v ∈ Rd as follows:\nui = { v> · tanh (Wref · ri +Wq · q) if i 6= π(j) for all j < i −∞ otherwise for i = 1, 2, ..., k (8)\nA(ref, q;Wref ,Wq, v) def = softmax(u). (9)\nOur pointer network, at decoder step j, then assigns the probability of visiting the next point π(j) of the tour as follows:\np(π(j)|π(< j), s) def= A(enc1:n, decj). (10)\nSetting the logits of cities that already appeared in the tour to −∞, as shown in Equation 8, ensures that our model only points at cities that have yet to be visited and hence outputs valid TSP tours.\nAttending mechanism: Specifically, our glimpse function G(ref, q) takes the same inputs as the attention function A and is parameterized by W gref ,W g q ∈ Rd×d and vg ∈ Rd. It performs the following computations:\np = A(ref, q;W gref ,W g q , v g) (11)\nG(ref, q;W gref ,W g q , v g) def = k∑ i=1 ripi. (12)\nThe glimpse functionG essentially computes a linear combination of the reference vectors weighted by the attention probabilities. It can also be applied multiple times on the same reference set ref :\ng0 def = q (13)\ngl def = G(ref, gl−1;W g ref ,W g q , v g) (14)\nFinally, the ultimate gl vector is passed to the attention function A(ref, gl;Wref ,Wq, v) to produce the probabilities of the pointing mechanism. We observed empirically that glimpsing more than once with the same parameters made the model less likely to learn and barely improved the results.\nA.2 IMPROVING EXPLORATION\nSoftmax temperature: We modify Equation 9 as follows:\nA(ref, q, T ;Wref ,Wq, v) def = softmax(u/T ), (15)\nwhere T is a temperature hyperparameter set to T = 1 during training. When T > 1, the distribution represented by A(ref, q) becomes less steep, hence preventing the model from being overconfident.\nLogit clipping: We modify Equation 9 as follows:\nA(ref, q;Wref ,Wq, v) def = softmax(C tanh(u)), (16)\nwhereC is a hyperparameter that controls the range of the logits and hence the entropy ofA(ref, q).\nA.3 OR TOOL’S METAHEURISTICS BASELINES FOR TSP\nA.4 SAMPLE TOURS\n(5.934)\nRL pretraining -Greedy\n(5.734)\nRL pretraining -Sampling\n(5.688)\nRL pretraining -Active Search\n(5.827)\nActive Search\n(5.688)\nOptimal\nRL pretraining -Greedy\nRL pretraining -Sampling\nRL pretraining -Active Search\nActive Search\nOptimal"
    } ],
    "references" : [ {
      "title" : "Tensorflow: A system for largescale machine learning",
      "author" : [ "Martı́n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard" ],
      "venue" : "arXiv preprint arXiv:1605.08695,",
      "citeRegEx" : "Abadi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2016
    }, {
      "title" : "A theoretical investigation into the performance of the Hopfield model",
      "author" : [ "Sreeram V.B. Aiyer", "Mahesan Niranjan", "Frank Fallside" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Aiyer et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Aiyer et al\\.",
      "year" : 1990
    }, {
      "title" : "Self-organizing feature maps and the Travelling Salesman Problem",
      "author" : [ "Bernard Angeniol", "Gael De La Croix Vaubois", "Jean-Yves Le Texier" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Angeniol et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Angeniol et al\\.",
      "year" : 1988
    }, {
      "title" : "Implementing the dantzigfulkerson-johnson algorithm for large traveling salesman problems",
      "author" : [ "David Applegate", "Robert Bixby", "Vašek Chvátal", "William Cook" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Applegate et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Applegate et al\\.",
      "year" : 2003
    }, {
      "title" : "The traveling salesman problem: a computational study",
      "author" : [ "David L Applegate", "Robert E Bixby", "Vasek Chvatal", "William J Cook" ],
      "venue" : "Princeton university press,",
      "citeRegEx" : "Applegate et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Applegate et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "An approximate dynamic programming approach to multidimensional knapsack problems",
      "author" : [ "Dimitris Bertsimas", "Ramazan Demir" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Bertsimas and Demir.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bertsimas and Demir.",
      "year" : 2002
    }, {
      "title" : "Hyperheuristics: An emerging direction in modern search technology",
      "author" : [ "Edmund Burke", "Graham Kendall", "Jim Newall", "Emma Hart", "Peter Ross", "Sonia Schulenburg" ],
      "venue" : null,
      "citeRegEx" : "Burke et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Burke et al\\.",
      "year" : 2003
    }, {
      "title" : "Hyper-heuristics: a survey of the state of the art",
      "author" : [ "Edmund K. Burke", "Michel Gendreau", "Matthew R. Hyde", "Graham Kendall", "Gabriela Ochoa", "Ender zcan", "Rong Qu" ],
      "venue" : "JORS, 64(12):1695–1724,",
      "citeRegEx" : "Burke et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Burke et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural methods for the Traveling Salesman Problem: insights from operations research",
      "author" : [ "Laura I. Burke" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Burke.,? \\Q1994\\E",
      "shortCiteRegEx" : "Burke.",
      "year" : 1994
    }, {
      "title" : "Noisy parallel approximate decoding for conditional recurrent language model",
      "author" : [ "Kyunghyun Cho" ],
      "venue" : "arXiv preprint arXiv:1605.03835,",
      "citeRegEx" : "Cho.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cho.",
      "year" : 2016
    }, {
      "title" : "Worst-case analysis of a new heuristic for the Travelling Salesman Problem",
      "author" : [ "Nicos Christofides" ],
      "venue" : "In Report 388. Graduate School of Industrial Administration,",
      "citeRegEx" : "Christofides.,? \\Q1976\\E",
      "shortCiteRegEx" : "Christofides.",
      "year" : 1976
    }, {
      "title" : "Solution of a large-scale traveling-salesman problem",
      "author" : [ "George Dantzig", "Ray Fulkerson", "Selmer Johnson" ],
      "venue" : "Journal of the operations research society of America,",
      "citeRegEx" : "Dantzig et al\\.,? \\Q1954\\E",
      "shortCiteRegEx" : "Dantzig et al\\.",
      "year" : 1954
    }, {
      "title" : "An analogue approach to the Travelling Salesman",
      "author" : [ "Richard Durbin" ],
      "venue" : "Nature, 326:16,",
      "citeRegEx" : "Durbin.,? \\Q1987\\E",
      "shortCiteRegEx" : "Durbin.",
      "year" : 1987
    }, {
      "title" : "A study of the application of Kohonen-type neural networks to the travelling salesman problem",
      "author" : [ "Favio Favata", "Richard Walker" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Favata and Walker.,? \\Q1991\\E",
      "shortCiteRegEx" : "Favata and Walker.",
      "year" : 1991
    }, {
      "title" : "Solving a combinatorial problem via self-organizing process: an application of the Kohonen algorithm to the traveling salesman problem",
      "author" : [ "J.C. Fort" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Fort.,? \\Q1988\\E",
      "shortCiteRegEx" : "Fort.",
      "year" : 1988
    }, {
      "title" : "Problem solving with optimization networks",
      "author" : [ "Andrew Howard Gee" ],
      "venue" : "PhD thesis, Citeseer,",
      "citeRegEx" : "Gee.,? \\Q1993\\E",
      "shortCiteRegEx" : "Gee.",
      "year" : 1993
    }, {
      "title" : "An effective implementation of the Lin-Kernighan traveling salesman",
      "author" : [ "Keld Helsgaun" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "Helsgaun.,? \\Q2000\\E",
      "shortCiteRegEx" : "Helsgaun.",
      "year" : 2000
    }, {
      "title" : "URL http://akira.ruc.dk/ ̃keld/research/LKH",
      "author" : [ "Keld Helsgaun" ],
      "venue" : "LK-H,",
      "citeRegEx" : "Helsgaun.,? \\Q2012\\E",
      "shortCiteRegEx" : "Helsgaun.",
      "year" : 2012
    }, {
      "title" : "Neural” computation of decisions in optimization problems",
      "author" : [ "John J. Hopfield", "David W. Tank" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Hopfield and Tank.,? \\Q1985\\E",
      "shortCiteRegEx" : "Hopfield and Tank.",
      "year" : 1985
    }, {
      "title" : "Local search and the traveling salesman problem",
      "author" : [ "DS Johnson" ],
      "venue" : "In Proceedings of 17th International Colloquium on Automata Languages and Programming, Lecture Notes in Computer Science,(Springer-Verlag,",
      "citeRegEx" : "Johnson.,? \\Q1990\\E",
      "shortCiteRegEx" : "Johnson.",
      "year" : 1990
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Optimization by simulated annealing",
      "author" : [ "S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 1983
    }, {
      "title" : "The self-organizing map",
      "author" : [ "Teuvo Kohonen" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Kohonen.,? \\Q1990\\E",
      "shortCiteRegEx" : "Kohonen.",
      "year" : 1990
    }, {
      "title" : "Comparison of neural networks for solving the Travelling Salesman Problem",
      "author" : [ "Bert F.J. La Maire", "Valeri M. Mladenov" ],
      "venue" : "In NEUREL,",
      "citeRegEx" : "Maire and Mladenov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Maire and Mladenov.",
      "year" : 2012
    }, {
      "title" : "An effective heuristic algorithm for the traveling-salesman problem",
      "author" : [ "S. Lin", "B.W. Kernighan" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Lin and Kernighan.,? \\Q1973\\E",
      "shortCiteRegEx" : "Lin and Kernighan.",
      "year" : 1973
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adri Puigdomnech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1605.03835,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "A branch-and-cut algorithm for the resolution of largescale symmetric traveling salesman problems",
      "author" : [ "Manfred Padberg", "Giovanni Rinaldi" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Padberg and Rinaldi.,? \\Q1990\\E",
      "shortCiteRegEx" : "Padberg and Rinaldi.",
      "year" : 1990
    }, {
      "title" : "The Euclidean Travelling Salesman Problem is NP-complete",
      "author" : [ "Christos H. Papadimitriou" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Papadimitriou.,? \\Q1977\\E",
      "shortCiteRegEx" : "Papadimitriou.",
      "year" : 1977
    }, {
      "title" : "An expanding-core algorithm for the exact 0-1 knapsack problem european journal of operational research",
      "author" : [ "David Pisinger" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "Pisinger.,? \\Q1995\\E",
      "shortCiteRegEx" : "Pisinger.",
      "year" : 1995
    }, {
      "title" : "A minimal algorithm for the 0-1 knapsack problem",
      "author" : [ "David Pisinger" ],
      "venue" : "Operations Research, pp",
      "citeRegEx" : "Pisinger.,? \\Q1997\\E",
      "shortCiteRegEx" : "Pisinger.",
      "year" : 1997
    }, {
      "title" : "Critical analysis of Hopfield’s neural network model for TSP and its comparison with heuristic algorithm for shortest path computation",
      "author" : [ "Farah Sarwar", "Abdul Aziz Bhatti" ],
      "venue" : "IBCAST,",
      "citeRegEx" : "Sarwar and Bhatti.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sarwar and Bhatti.",
      "year" : 2012
    }, {
      "title" : "Neural networks for combinatorial optimization: a review of more than a decade of research",
      "author" : [ "Kate A. Smith" ],
      "venue" : "INFORMS Journal on Computing,",
      "citeRegEx" : "Smith.,? \\Q1999\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 1999
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A hierarchical strategy for solving traveling salesman problems using elastic nets",
      "author" : [ "Andrew I. Vakhutinsky", "Bruce L. Golden" ],
      "venue" : "Journal of Heuristics,",
      "citeRegEx" : "Vakhutinsky and Golden.,? \\Q1995\\E",
      "shortCiteRegEx" : "Vakhutinsky and Golden.",
      "year" : 1995
    }, {
      "title" : "Order matters: Sequence to sequence for sets",
      "author" : [ "Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur" ],
      "venue" : "arXiv preprint arXiv:1511.06391,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Guided local search and its application to the traveling salesman problem",
      "author" : [ "Christos Voudouris", "Edward Tsang" ],
      "venue" : "European journal of operational research,",
      "citeRegEx" : "Voudouris and Tsang.,? \\Q1999\\E",
      "shortCiteRegEx" : "Voudouris and Tsang.",
      "year" : 1999
    }, {
      "title" : "Simple statistical gradient following algorithms for connectionnist reinforcement learning",
      "author" : [ "Ronald Williams" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "On the stability of the travelling salesman problem algorithm of hopfield and tank",
      "author" : [ "G.V. Wilson", "G.S. Pawley" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Wilson and Pawley.,? \\Q1988\\E",
      "shortCiteRegEx" : "Wilson and Pawley.",
      "year" : 1988
    }, {
      "title" : "No free lunch theorems for optimization",
      "author" : [ "D.H. Wolpert", "W.G. Macready" ],
      "venue" : "Transactions on Evolutionary Computation,",
      "citeRegEx" : "Wolpert and Macready.,? \\Q1997\\E",
      "shortCiteRegEx" : "Wolpert and Macready.",
      "year" : 1997
    }, {
      "title" : "Learning to learn for global optimization of black box functions",
      "author" : [ "Chen Yutian", "Hoffman Matthew W", "Colmenarejo Sergio Gomez", "Denil Misha", "Lillicrap Timothy P", "de Freitas Nando" ],
      "venue" : "arXiv preprint arXiv:1611.03824,",
      "citeRegEx" : "Yutian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yutian et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural architecture search with reinforcement learning",
      "author" : [ "Barret Zoph", "Quoc Le" ],
      "venue" : "arXiv preprint arXiv:1611.01578,",
      "citeRegEx" : "Zoph and Le.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zoph and Le.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "(see (Applegate et al., 2011) for an overview).",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 28,
      "context" : "Finding the optimal TSP solution is NP-hard, even in the two-dimensional Euclidean case (Papadimitriou, 1977), where the nodes are 2D points and edge weights are Euclidean distances between pairs of points.",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 37,
      "context" : "We consider two approaches based on policy gradients (Williams, 1992).",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "Figure 1: Tour length ratios of LK-H (Helsgaun, 2000) local search and our best method (RL pretraining-Active Search) against optimality, guaranteed by Concorde (Applegate et al.",
      "startOffset" : 37,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "Generic local search, obtained via Googles vehicle routing problem solver (Google, 2016), applies a set of heuristics starting from the (Christofides, 1976) solution.",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 12,
      "context" : ", 2006), widely accepted as one of the best exact TSP solvers, makes use of cutting plane algorithms (Dantzig et al., 1954; Padberg & Rinaldi, 1990; Applegate et al., 2003), iteratively solving linear programming relaxations of the TSP, in conjunction with a branch-and-bound approach that prunes parts of the search space that provably will not contain an optimal solution.",
      "startOffset" : 101,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : ", 2006), widely accepted as one of the best exact TSP solvers, makes use of cutting plane algorithms (Dantzig et al., 1954; Padberg & Rinaldi, 1990; Applegate et al., 2003), iteratively solving linear programming relaxations of the TSP, in conjunction with a branch-and-bound approach that prunes parts of the search space that provably will not contain an optimal solution.",
      "startOffset" : 101,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "Similarly, the Lin-Kernighan-Helsgaun heuristic (Helsgaun, 2000), inspired from the Lin-Kernighan heuristic (Lin & Kernighan, 1973), is a state of the art approximate search heuristic for the symmetric TSP and has been shown to solve instances with hundreds of nodes to optimality.",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "Christofides (1976) proposes a heuristic algorithm that involves computing a minimum-spanning tree and a minimum-weight perfect matching.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "on hand-engineered heuristics such as 2-opt (Johnson, 1990), to navigate from solution to solution in the search space.",
      "startOffset" : 44,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "This challenge has fostered interest in raising the level of generality at which optimization systems operate (Burke et al., 2003) and is the underlying motivation behind hyper-heuristics, defined as ”search method[s] or learning mechanism[s] for selecting or generating heuristics to solve computation search problems”.",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "Hyper-heuristics aim to be easier to use than problem specific methods by partially abstracting away the knowledge intensive process of selecting heuristics given a combinatorial problem and have been shown to successfully combine human-defined heuristics in superior ways across many tasks (see (Burke et al., 2013) for a survey).",
      "startOffset" : 296,
      "endOffset" : 316
    }, {
      "referenceID" : 32,
      "context" : "The application of neural networks to combinatorial optimization has a distinguished history, where the majority of research focuses on the Traveling Salesman Problem (Smith, 1999).",
      "startOffset" : 167,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "Overcoming this limitation is central to the subsequent work in the field, especially by (Aiyer et al., 1990; Gee, 1993).",
      "startOffset" : 89,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "Overcoming this limitation is central to the subsequent work in the field, especially by (Aiyer et al., 1990; Gee, 1993).",
      "startOffset" : 89,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al.",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al., 1988; Kohonen, 1990).",
      "startOffset" : 146,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al., 1988; Kohonen, 1990).",
      "startOffset" : 146,
      "endOffset" : 196
    }, {
      "referenceID" : 23,
      "context" : "Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al., 1988; Kohonen, 1990).",
      "startOffset" : 146,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Addressing the limitations of deformable template models is central to the following work in this area (Burke, 1994; Favata & Walker, 1991; Vakhutinsky & Golden, 1995).",
      "startOffset" : 103,
      "endOffset" : 167
    }, {
      "referenceID" : 33,
      "context" : "Motivated by the recent advancements in sequence-to-sequence learning (Sutskever et al., 2014), neural networks are again the subject of study for optimization in various domains (Yutian et al.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 40,
      "context" : ", 2014), neural networks are again the subject of study for optimization in various domains (Yutian et al., 2016), including discrete ones (Zoph & Le, 2016).",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 33,
      "context" : "We are inspired by previous work (Sutskever et al., 2014) that makes use of the same factorization based on the chain rule to address sequence to sequence problems like machine translation.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : ", 2015b), which makes use of a set of non-parameteric softmax modules, resembling the attention mechanism from (Bahdanau et al., 2015).",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "(2015a) also suggest including some additional computation steps, named glimpses, to aggregate the contributions of different parts of the input sequence, very much like (Bahdanau et al., 2015).",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 34,
      "context" : "Vinyals et al. (2015a) also suggest including some additional computation steps, named glimpses, to aggregate the contributions of different parts of the input sequence, very much like (Bahdanau et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 37,
      "context" : "The gradient of (3) is formulated using the well-known REINFORCE algorithm (Williams, 1992):",
      "startOffset" : 75,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "Our training algorithm, described in Algorithm 1, is closely related to the asynchronous advantage actor-critic (A3C) proposed in (Mnih et al., 2016), as the difference between the sampled tour lengths and the critic’s predictions is an unbiased estimate of the advantage function.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "mechanism with random noise and greedily decoding from the obtained modified policy, similarly to (Cho, 2016), but this proves less effective than sampling in our experiments.",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "Our model and training code in Tensorflow (Abadi et al., 2016) will be made availabe soon.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : "OR-Tools improves over Christofides’ solutions with simple local search operators, including 2-opt (Johnson, 1990) and a version of the Lin-Kernighan heuristic (Lin & Kernighan, 1973), stopping when it reaches a local minimum.",
      "startOffset" : 99,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "In order to escape poor local optima, ORTools’ local search can also be run in conjunction with different metaheuristics, such as simulated annealing (Kirkpatrick et al., 1983), tabu search (Glover & Laguna, 2013) or guided local search (Voudouris & Tsang, 1999).",
      "startOffset" : 150,
      "endOffset" : 176
    }, {
      "referenceID" : 18,
      "context" : ", 2006) and LK-H’s local search (Helsgaun, 2012; 2000).",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 29,
      "context" : "Two simple heuristics are ExpKnap, which employs branch-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which uses dynamic programming with enumerative bounds (Pisinger, 1997).",
      "startOffset" : 97,
      "endOffset" : 113
    }, {
      "referenceID" : 30,
      "context" : "Two simple heuristics are ExpKnap, which employs branch-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which uses dynamic programming with enumerative bounds (Pisinger, 1997).",
      "startOffset" : 183,
      "endOffset" : 199
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items. These results, albeit still far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.",
    "creator" : "LaTeX with hyperref package"
  }
}