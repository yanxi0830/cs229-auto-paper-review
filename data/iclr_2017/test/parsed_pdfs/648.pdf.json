{
  "name" : "648.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "authors" : [ "Emily Denton" ],
    "emails" : [ "denton@cs.nyu.edu", "sgross@fb.com", "robfergus@fb.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep neural networks have yielded dramatic performance gains in recent years on tasks such as object classification (Krizhevsky et al., 2012), text classification (Zhang et al., 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). These successes are heavily dependent on large training sets of manually annotated data. In many settings however, such large collections of labels may not be readily available, motivating the need for methods that can learn from data where labels are rare.\nWe propose a method for harnessing unlabeled image data based on image in-painting. A generative model is trained to generate pixels within a missing hole, based on the context provided by surrounding parts of the image. These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image). The realistic looking fake examples provided by the generative model cause the discriminator to learn features that generalize to the related task of classifying objects. Thus adversarial training for the in-painting task can be used to regularize large discriminative models during supervised training on a handful of labeled images."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "Learning From Context: The closest work to ours is the independently developed context-encoder approach of Pathak et al. (2016). This introduces an encoder-decoder framework, shown in Fig. 1(a), that is used to in-paint images where a patch has been randomly removed. After using this as a pre-training task, a classifier is added to the encoder and the model is fine-tuned using the labeled examples. Although both approaches use the concept of in-painting, they differ in several important ways. First, the architectures are different (see Fig. 1): in Pathak et al. (2016), the features for the classifier are taken from the encoder, whereas ours come from the discriminator network. In practice this makes an important difference as we are able to directly train large models such as VGG (Simonyan & Zisserman, 2015) using adversarial loss alone. By contrast, Pathak et al. (2016) report difficulties in training an AlexNet encoder with this loss. This leads to the second difference, namely that on account of these issues, they instead employ an `2 loss when training models for classification and detection (however they do use a joint `2 and adversarial loss to achieve impressive in-painting results). Finally, the unsupervised learning task differs between the two models. The context-encoder learns a feature representation suitable for in-painting whereas our model learns a feature representation suitable for differentiating real/fake in-paintings. Notably, while we also use a neural network to generate the in-paintings, this model is only used as an adversary for the\ndiscriminator, rather than as a feature extractor. In section 4, we compare the performance of our model to the context-encoder on the PASCAL dataset.\nOther forms of spatial context within images have recently been utilized for representation learning. Doersch et al. (2015) propose training a CNN to predict the spatial location of one image patch relative to another. Noroozi & Favaro (2016) propose a model that learns by unscrambling image patches, essentially solving a jigsaw puzzle to learn visual representations. In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015).\nDeep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task. More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014).\nDosovitskiy et al. (2014) achieved state-of-the-art results by training a CNN with a different class for each training example and introducing a set of transformations to provide multiple examples per class. The pseudo-label approach (Lee, 2013) is a simple semi-supervised method that trains using the maximumly predicted class as a label when labels are unavailable. Springenberg (2015) propose a categorical generative adversarial network (CatGAN) which can be used for unsupervised and semi-supervised learning. The discriminator in a CatGAN outputs a distribution over classes and is trained to minimize the predicted entropy for real data and maximize the predicted entropy for fake data. Similar to our model, CatGANs use the feature space learned by the discriminator for the final supervised learning task. Salimans et al. (2016) recently proposed a semi-supervised GAN model in which the discriminator outputs a softmax over classes rather than a probability of real vs. fake. An additional ‘generated’ class is used as the target for generated samples. This method differs from our work in that it does not utilize context information and has only been applied to datasets of small resolution. However, the discriminator loss is similar to the one we propose and could be combined with our context-conditional approach.\nMore traditional semi-supervised methods include graph-based approaches (Zhou et al., 2004; Zhu, 2006) that show impressive performance when good image representations are available. However, the focus of our work is on learning such representations.\nGenerative models of images: Restricted Boltzmann machines (Salakhutdinov, 2015), de-noising autoencoders (Vincent et al., 2008) and variational autoencoders (Kingma & Welling, 2014) optimize a maximum likelihood criterion and thus learn decoders that map from latent space to image space. More recently, generative adversarial networks (Goodfellow et al., 2014) and generative mo-\nment matching networks (Li et al., 2015; Dziugaite et al., 2015) have been proposed. These methods ignore data likelihoods and instead directly train a generative model to produce realistic samples. Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks.\nOther models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color. Sohl-Dickstein et al. (2015) propose a generative model based on a reverse diffusion process. While our model does involve image generation, it differs from these approaches in that the main focus is on learning a good representation for classification tasks.\nPredictive generative models of videos aim to extrapolate from current frames to future ones and in doing so learn a feature representation that is useful for other tasks. In this vein, Ranzato et al. (2014) used an `2-loss in pixel-space. Mathieu et al. (2015) combined an adversarial loss with `2, giving models that generate crisper images. While our model is also predictive, it only considers interpolation within an image, rather than extrapolation in time."
    }, {
      "heading" : "2 APPROACH",
      "text" : "We present a semi-supervised learning framework built on generative adversarial networks (GANs) of Goodfellow et al. (2014). We first review the generative adversarial network framework and then introduce context conditional generative adversarial networks (CC-GANs). Finally, we show how combining a classification objective and a CC-GAN objective provides a unified framework for semi-supervised learning."
    }, {
      "heading" : "2.1 GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "The generative adversarial network approach (Goodfellow et al., 2014) is a framework for training generative models, which we briefly review. It consists of two networks pitted against one another in a two player game: A generative model, G, is trained to synthesize images resembling the data distribution and a discriminative model, D, is trained to distinguish between samples drawn from G and images drawn from the training data.\nMore formally, let X = {x1, ...,xn} be a dataset of images of dimensionality d. Let D denote a discriminative function that takes as input an image x ∈ Rd and outputs a scalar representing the probability of input x being a real sample. Let G denote the generative function that takes as input a random vector z ∈ Rz sampled from a prior noise distribution pNoise and outputs a synthesized image x̃ = G(z) ∈ Rd. Ideally, D(x) = 1 when x ∈ X and D(x) = 0 when x was generated from G. The GAN objective is given by:\nmin G max D Ex∼X [logD(x)] + Ez∼pNoise(z)[log(1−D(G(z)))] (1)\nThe conditional generative adversarial network (Mirza & Osindero, 2014) is an extension of the GAN in which bothD andG receive an additional vector of information y as input. The conditional GAN objective is given by:\nmin G max D Ex,y∼X [logD(x,y)] + Ez∼pNoise(z)[log(1−D(G(z,y),x))] (2)"
    }, {
      "heading" : "2.2 CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "We propose context-conditional generative adversarial networks (CC-GANs) which are conditional GANs where the generator is trained to fill in a missing image patch and the generator and discriminator are conditioned on the surrounding pixels.\nIn particular, the generator G receives as input an image with a randomly masked out patch. The generator outputs an entire image. We fill in the missing patch from the generated output and then pass the completed image into D. We pass the completed image into D rather than the context and the patch as two separate inputs so as to prevent D from simply learning to identify discontinuities along the edge of the missing patch.\nMore formally, let m ∈ Rd denote to a binary mask that will be used to drop out a specified portion of an image. The generator receives as input m x where denotes element-wise multiplication. The generator outputs xG = G(m x, z) ∈ Rd and the in-painted image xI is given by:\nxI = (1−m) xG +m x (3)\nThe CC-GAN objective is given by:\nmin G max D\nEx∼X [logD(x)] + Ex∼X ,m∼M[log(1−D(xI))] (4)"
    }, {
      "heading" : "2.3 COMBINED GAN AND CC-GAN",
      "text" : "While the generator of the CC-GAN outputs a full image, only a portion of it (corresponding to the missing hole) is seen by the discriminator. In the combined model, which we denote by CC-GAN2, the fake examples for the discriminator include both the in-painted image xI and the full image xG produced by the generator (i.e. not just the missing patch). By combining the GAN and CC-GAN approaches, we introduce a wider array of negative examples to the discriminator. The CC-GAN2 objective given by:\nmin G max D\nEx∼X [logD(x)] (5)\n+ Ex∼X ,m∼M[log(1−D(xI))] (6) + Ex∼X ,m∼M[log(1−D(xG))] (7)"
    }, {
      "heading" : "2.4 SEMI-SUPERVISED LEARNING WITH CC-GANS",
      "text" : "A common approach to semi-supervised learning is to combine a supervised and unsupervised objective during training. As a result unlabeled data can be leveraged to aid the supervised task.\nIntuitively, a GAN discriminator must learn something about the structure of natural images in order to effectively distinguish real from generated images. Recently, Radford et al. (2016) showed that a GAN discriminator learns a hierarchical image representation that is useful for object classification. Such results suggest that combining an unsupervised GAN objective with a supervised classification objective would produce a simple and effective semi-supervised learning method. This approach, denoted by SSL-GAN, is illustrated in Fig. 1(b). The discriminator network receives a gradient from the real/fake loss for every real and generated image. The discriminator also receives a gradient from the classification loss on the subset of (real) images for which labels are available.\nGenerative adversarial networks have shown impressive performance on many diverse datasets. However, samples are most coherent when the set of images the network is trained on comes from a limited domain (eg. churches or faces). Additionally, it is difficult to train GANs on very large images. Both these issues suggest semi-supervised learning with vanilla GANs may not scale well to datasets of large diverse images. Rather than determining if a full image is real or fake, context conditional GANs address a different task: determining if a part of an image is real or fake given the surrounding context.\nFormally, let XL = {(x1, y1), ..., (xn, yn)} denote a dataset of labeled images. Let Dc(x) denote the output of the classifier head on the discriminator (see Fig. 1(c) for details). Then the semisupervised CC-GAN objective is:\nmin G max D Ex∼X [logD(x)] + Ex∼X ,m∼M[log(1−D(xI))] + λcEx,y∼XL [log(Dc(y|x))] (8)\nThe hyperparameter λc balances the classification and adversarial losses. We only consider the CCGAN in the semi-supervised setting and thus drop the SSL notation when referring to this model."
    }, {
      "heading" : "2.5 MODEL ARCHITECTURE AND TRAINING DETAILS",
      "text" : "The architecture of our generative model,G, is inspired by the generator architecture of the DCGAN (Radford et al., 2016). The model consists of a sequence of convolutional layers with subsampling (but no pooling) followed by a sequence of fractionally-strided convolutional layers. For the discriminator, D, we used the VGG-A network (Simonyan & Zisserman, 2015) without the fully connected layers (which we call the VGG-A’ architecture). Details of the generator and discriminator are given\nin Fig. 2. The input to the generator is an image with a patch zeroed out. In preliminary experiments we also tried passing in a separate mask to the generator to make the missing area more explicit but found this did not effect performance.\nEven with the context conditioning it is difficult to generate large image patches that look realistic, making it problematic to scale our approach to high resolution images. To address this, we propose conditioning the generator on both the high resolution image with a missing patch and a low resolution version of the whole image (with no missing region). In this setting, the generators task becomes one of super-resolution on a portion of an image. However, the discriminator does not receive the low resolution image and thus is still faced with the same problem of determining if a given in-painting is viable or not. Where indicated, we used this approach in our PASCAL VOC 2007 experiments, with the original image being downsampled by a factor of 4. This provided enough information for the generator to fill in larger holes but not so much that it made the task trivial. This optional low resolution image is illustrated in Fig. 2(left) with the dotted line.\nWe followed the training procedures of Radford et al. (2016). We used the Adam optimizer (Kingma & Ba, 2015) in all our experiments with learning rate of 0.0002, momentum term β1 of 0.5, and the remaining Adam hyperparameters set to their default values. We set λc = 1 for all experiments."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 STL-10 CLASSIFICATION",
      "text" : "STL-10 is a dataset of 96×96 color images with a 1:100 ratio of labeled to unlabeled examples, making it an ideal fit for our semi-supervised learning framework. The training set consists of 5000 labeled images, mapped to 10 pre-defined folds of 1000 images each, and 100,000 unlabeled images. The labeled images belong to 10 classes and were extracted from the ImageNet dataset and the unlabeled images come from a broader distribution of classes. We follow the standard testing protocol and train 10 different models on each of the 10 predefined folds of data. We then evaluate classification accuracy of each model on the test set and report the mean and standard deviation.\nWe trained our CC-GAN and CC-GAN2 models on 64×64 crops of the 96×96 image. The hole was 32×32 pixels and the location of the hole varied randomly (see Fig. 3(top)). We trained for 100 epochs and then fine-tuned the discriminator on the 96x96 labeled images, stopping when training accuracy reached 100%. As shown in Table 1, the CC-GAN model performs comparably to current state of the art (Dosovitskiy et al., 2014) and the CC-GAN2 model improves upon it.\nWe also trained two baseline models in an attempt to tease apart the contributions of adversarial training and context conditional adversarial training. The first is a purely supervised training of the VGG-A’ model (the same architecture as the discriminator in the CC-GAN framework). This was trained using a dropout of 0.5 on the final layer and weight decay of 0.001. The performance of this model is significantly worse than the CC-GAN model.\nWe also trained a semi-supervised GAN (SSL-GAN, see Fig. 1(b)) on STL-10. This consisted of the same discriminator as the CC-GAN (VGG-A’ architecture) and generator from the DCGAN model (Radford et al., 2016). The training setup in this case is identical to the CC-GAN model. The SSLGAN performs almost as well as the CC-GAN, confirming our hypothesis that the GAN objective is a useful unsupervised criterion."
    }, {
      "heading" : "3.2 PASCAL VOC CLASSIFICATION",
      "text" : "In order to compare against other methods that utilize spatial context we ran the CC-GAN model on PASCAL VOC 2007 dataset. This dataset consists of natural images coming from 20 classes. The dataset contains a large amount of variability with objects varying in size, pose, and position. The training and validation sets combined contain 5,011 images, and the test set contains 4,952 images. The standard measure of performance is mean average precision (mAP).\nWe trained each model on the combined training and validation set for ∼5000 epochs and evaluated on the test set once1. Following Pathak et al. (2016), we train using random cropping, and then evaluate using the average prediction from 10 random crops.\nOur best performing model was trained on images of resolution 128×128 with a hole size of 64×64 and a low resolution input of size 32×32. Table 2 compares our CC-GAN method to other feature learning approaches on the PASCAL test set. It outperforms them, beating the current state of the art (Wang & Gupta, 2015) by 3.8%. It is important to note that our feature extractor is the VGGA’ model which is larger than the AlexNet architecture (Krizhevsky et al., 2012) used by other approaches in Table 2. However, purely supervised training of the two models reveals that VGG-A’\n1Hyperparameters were determined by initially training on the training set alone and measuring performance on the validation set.\nis less than 2% better than AlexNet. Furthermore, our model outperforms the supervised VGG-A’ baseline by a 7% margin (62.2% vs. 55.2%). This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\nTable 3 shows the effect of training on different resolutions. The CC-GAN improves over the baseline CNN consistently regardless of image size. We found that conditioning on the low resolution image began to help when the hole size was largest (64×64). We hypothesize that the low resolution conditioning would be more important for larger images, potentially allowing the method to scale to larger image sizes than we explored in this work."
    }, {
      "heading" : "3.3 INPAINTING",
      "text" : "We now show some sample in-paintings produced by our CC-GAN generators. In our semisupervised learning experiments on STL-10 we remove a single fixed size hole from the image. The top row of Fig. 3 shows in-paintings produced by this model. We can also explored different masking schemes as illustrated in the remaining rows of Fig. 3 (however these did not improve classification results). In all cases we see that training the generator with the adversarial loss produces sharp semantically plausible in-painting results.\nFig. 4 shows generated images and in-painted images from a model trained with the CC-GAN2 criterion. The output of a CC-GAN generator tends to be corrupted outside the patch used to inpaint the image (since gradients only flow back to the missing patch). However, in the CC-GAN2 model, we see that both the in-painted image and the generated image are coherent and semantically consistent with the masked input image.\nFig. 5 shows in-painted images from a generator trained on 128×128 PASCAL images. Fig. 6 shows the effect of adding a low resolution (32×32) image as input to the generator. For comparison we also show the result of in-painting by filling in with a bi-linearly upsampled image. Here we see the generator produces high-frequency structure rather than simply learning to copy the low resolution patch."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "We have presented a simple semi-supervised learning framework based on in-painting with an adversarial loss. The generator in our CC-GAN model is capable of producing semantically meaningful\nin-paintings and the discriminator performs comparable to or better than existing semi-supervised methods on two classification benchmarks.\nSince discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification. Furthermore, since our model operates on images half the resolution as those used by other approaches (128×128 vs. 224×244), there is potential for further gains if improvements in the generator resolution can be made. Our models and code are available at https://github.com/edenton/cc-gan.\nAcknowledgements: Emily Denton is supported by a Google Fellowship. Rob Fergus is grateful for the support of CIFAR."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised visual representation learning by context prediction",
      "author" : [ "Carl Doersch", "Abhinav Gupta", "Alexei A. Efros" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "Doersch et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Doersch et al\\.",
      "year" : 2015
    }, {
      "title" : "Discriminative unsupervised feature learning with convolutional neural networks",
      "author" : [ "Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to generate chairs, tables and cars with convolutional networks",
      "author" : [ "Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2015
    }, {
      "title" : "Training generative neural networks via maximum mean discrepancy optimization",
      "author" : [ "Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Dziugaite et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dziugaite et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y. Teh" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kiros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "author" : [ "D.H. Lee" ],
      "venue" : "In Workshop on Challenges in Representation Learning,",
      "citeRegEx" : "Lee.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lee.",
      "year" : 2013
    }, {
      "title" : "Generative moment matching networks",
      "author" : [ "Yujia Li", "Kevin Swersky", "Richard Zemel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating images from captions with attention",
      "author" : [ "Elman Mansimov", "Emilio Parisotto", "Jimmy Ba", "Ruslan Salakhutdinov" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Mansimov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mansimov et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep multi-scale video prediction beyond mean square error",
      "author" : [ "Michaël Mathieu", "Camille Couprie", "Yann LeCun" ],
      "venue" : "arXiv 1511.05440,",
      "citeRegEx" : "Mathieu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mathieu et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional generative adversarial nets",
      "author" : [ "Mehdi Mirza", "Simon Osindero" ],
      "venue" : "CoRR, abs/1411.1784,",
      "citeRegEx" : "Mirza and Osindero.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza and Osindero.",
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of visual representations by solving jigsaw puzzles",
      "author" : [ "Mehdi Noroozi", "Paolo Favaro" ],
      "venue" : null,
      "citeRegEx" : "Noroozi and Favaro.,? \\Q2016\\E",
      "shortCiteRegEx" : "Noroozi and Favaro.",
      "year" : 2016
    }, {
      "title" : "Context encoders: Feature learning by inpainting",
      "author" : [ "Deepak Pathak", "Philipp Krahenbuhl", "Jeff Donahue", "Trevor Darrell", "Alexei A. Efros" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Pathak et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Radford et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2016
    }, {
      "title" : "Video (language) modeling: a baseline for generative models of natural videos",
      "author" : [ "Marc’Aurelio Ranzato", "Arthur Szlam", "Joan Bruna", "Michaël Mathieu", "Ronan Collobert", "Sumit Chopra" ],
      "venue" : "arXiv 1412.6604,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with ladder network",
      "author" : [ "Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning deep generative models",
      "author" : [ "Ruslan Salakhutdinov" ],
      "venue" : "Annual Review of Statistics and Its Application,",
      "citeRegEx" : "Salakhutdinov.,? \\Q2015\\E",
      "shortCiteRegEx" : "Salakhutdinov.",
      "year" : 2015
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "In The International Conference on Learning Representations,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Deep unsupervised learning using nonequilibrium thermodynamics",
      "author" : [ "Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli" ],
      "venue" : "CoRR, abs/1503.03585,",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised and semi-supervised learning with categorical generative adversarial networks",
      "author" : [ "Jost Tobias Springenberg" ],
      "venue" : "arXiv 1511.06390,",
      "citeRegEx" : "Springenberg.,? \\Q2015\\E",
      "shortCiteRegEx" : "Springenberg.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc Le" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-task bayesian optimization",
      "author" : [ "Kevin Swersky", "Jasper Snoek", "Ryan P. Adams" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Swersky et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Swersky et al\\.",
      "year" : 2013
    }, {
      "title" : "Generative image modeling using spatial lstms",
      "author" : [ "L. Theis", "M. Bethge" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Theis and Bethge.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis and Bethge.",
      "year" : 2015
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Unsupervised learning of visual representations using videos",
      "author" : [ "Xiaolong Wang", "Abhinav Gupta" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Wang and Gupta.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang and Gupta.",
      "year" : 2015
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Stacked what-where auto-encoders",
      "author" : [ "Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann LeCun" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Schoelkopf" ],
      "venue" : "In Advances in Neural Information Processing Systemsi",
      "citeRegEx" : "Zhou et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2004
    }, {
      "title" : "Semi-supervised learning literature",
      "author" : [ "Xiaojin Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhu.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Deep neural networks have yielded dramatic performance gains in recent years on tasks such as object classification (Krizhevsky et al., 2012), text classification (Zhang et al.",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 37,
      "context" : ", 2012), text classification (Zhang et al., 2015) and machine translation (Sutskever et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 31,
      "context" : ", 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 32,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 32,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image).",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2015). These successes are heavily dependent on large training sets of manually annotated data. In many settings however, such large collections of labels may not be readily available, motivating the need for methods that can learn from data where labels are rare. We propose a method for harnessing unlabeled image data based on image in-painting. A generative model is trained to generate pixels within a missing hole, based on the context provided by surrounding parts of the image. These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image). The realistic looking fake examples provided by the generative model cause the discriminator to learn features that generalize to the related task of classifying objects. Thus adversarial training for the in-painting task can be used to regularize large discriminative models during supervised training on a handful of labeled images. 1.1 RELATED WORK Learning From Context: The closest work to ours is the independently developed context-encoder approach of Pathak et al. (2016). This introduces an encoder-decoder framework, shown in Fig.",
      "startOffset" : 8,
      "endOffset" : 1232
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2015). These successes are heavily dependent on large training sets of manually annotated data. In many settings however, such large collections of labels may not be readily available, motivating the need for methods that can learn from data where labels are rare. We propose a method for harnessing unlabeled image data based on image in-painting. A generative model is trained to generate pixels within a missing hole, based on the context provided by surrounding parts of the image. These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image). The realistic looking fake examples provided by the generative model cause the discriminator to learn features that generalize to the related task of classifying objects. Thus adversarial training for the in-painting task can be used to regularize large discriminative models during supervised training on a handful of labeled images. 1.1 RELATED WORK Learning From Context: The closest work to ours is the independently developed context-encoder approach of Pathak et al. (2016). This introduces an encoder-decoder framework, shown in Fig. 1(a), that is used to in-paint images where a patch has been randomly removed. After using this as a pre-training task, a classifier is added to the encoder and the model is fine-tuned using the labeled examples. Although both approaches use the concept of in-painting, they differ in several important ways. First, the architectures are different (see Fig. 1): in Pathak et al. (2016), the features for the classifier are taken from the encoder, whereas ours come from the discriminator network.",
      "startOffset" : 8,
      "endOffset" : 1679
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2015). These successes are heavily dependent on large training sets of manually annotated data. In many settings however, such large collections of labels may not be readily available, motivating the need for methods that can learn from data where labels are rare. We propose a method for harnessing unlabeled image data based on image in-painting. A generative model is trained to generate pixels within a missing hole, based on the context provided by surrounding parts of the image. These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image). The realistic looking fake examples provided by the generative model cause the discriminator to learn features that generalize to the related task of classifying objects. Thus adversarial training for the in-painting task can be used to regularize large discriminative models during supervised training on a handful of labeled images. 1.1 RELATED WORK Learning From Context: The closest work to ours is the independently developed context-encoder approach of Pathak et al. (2016). This introduces an encoder-decoder framework, shown in Fig. 1(a), that is used to in-paint images where a patch has been randomly removed. After using this as a pre-training task, a classifier is added to the encoder and the model is fine-tuned using the labeled examples. Although both approaches use the concept of in-painting, they differ in several important ways. First, the architectures are different (see Fig. 1): in Pathak et al. (2016), the features for the classifier are taken from the encoder, whereas ours come from the discriminator network. In practice this makes an important difference as we are able to directly train large models such as VGG (Simonyan & Zisserman, 2015) using adversarial loss alone. By contrast, Pathak et al. (2016) report difficulties in training an AlexNet encoder with this loss.",
      "startOffset" : 8,
      "endOffset" : 1988
    }, {
      "referenceID" : 22,
      "context" : "Figure 1: (a) Context-encoder of Pathak et al. (2016), configured for object classification task.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 215
    }, {
      "referenceID" : 13,
      "context" : "In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 215
    }, {
      "referenceID" : 9,
      "context" : "Deep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task.",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 25,
      "context" : "More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al.",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 38,
      "context" : ", 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : ", 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014).",
      "startOffset" : 37,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "The pseudo-label approach (Lee, 2013) is a simple semi-supervised method that trains using the maximumly predicted class as a label when labels are unavailable.",
      "startOffset" : 26,
      "endOffset" : 37
    }, {
      "referenceID" : 39,
      "context" : "More traditional semi-supervised methods include graph-based approaches (Zhou et al., 2004; Zhu, 2006) that show impressive performance when good image representations are available.",
      "startOffset" : 72,
      "endOffset" : 102
    }, {
      "referenceID" : 40,
      "context" : "More traditional semi-supervised methods include graph-based approaches (Zhou et al., 2004; Zhu, 2006) that show impressive performance when good image representations are available.",
      "startOffset" : 72,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "Generative models of images: Restricted Boltzmann machines (Salakhutdinov, 2015), de-noising autoencoders (Vincent et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 35,
      "context" : "Generative models of images: Restricted Boltzmann machines (Salakhutdinov, 2015), de-noising autoencoders (Vincent et al., 2008) and variational autoencoders (Kingma & Welling, 2014) optimize a maximum likelihood criterion and thus learn decoders that map from latent space to image space.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "More recently, generative adversarial networks (Goodfellow et al., 2014) and generative mo-",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "Doersch et al. (2015) propose training a CNN to predict the spatial location of one image patch relative to another.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "Doersch et al. (2015) propose training a CNN to predict the spatial location of one image patch relative to another. Noroozi & Favaro (2016) propose a model that learns by unscrambling image patches, essentially solving a jigsaw puzzle to learn visual representations.",
      "startOffset" : 0,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015). Deep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task. More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014). Dosovitskiy et al. (2014) achieved state-of-the-art results by training a CNN with a different class for each training example and introducing a set of transformations to provide multiple examples per class.",
      "startOffset" : 150,
      "endOffset" : 778
    }, {
      "referenceID" : 1,
      "context" : "In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015). Deep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task. More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014). Dosovitskiy et al. (2014) achieved state-of-the-art results by training a CNN with a different class for each training example and introducing a set of transformations to provide multiple examples per class. The pseudo-label approach (Lee, 2013) is a simple semi-supervised method that trains using the maximumly predicted class as a label when labels are unavailable. Springenberg (2015) propose a categorical generative adversarial network (CatGAN) which can be used for unsupervised and semi-supervised learning.",
      "startOffset" : 150,
      "endOffset" : 1141
    }, {
      "referenceID" : 1,
      "context" : "In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015). Deep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task. More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014). Dosovitskiy et al. (2014) achieved state-of-the-art results by training a CNN with a different class for each training example and introducing a set of transformations to provide multiple examples per class. The pseudo-label approach (Lee, 2013) is a simple semi-supervised method that trains using the maximumly predicted class as a label when labels are unavailable. Springenberg (2015) propose a categorical generative adversarial network (CatGAN) which can be used for unsupervised and semi-supervised learning. The discriminator in a CatGAN outputs a distribution over classes and is trained to minimize the predicted entropy for real data and maximize the predicted entropy for fake data. Similar to our model, CatGANs use the feature space learned by the discriminator for the final supervised learning task. Salimans et al. (2016) recently proposed a semi-supervised GAN model in which the discriminator outputs a softmax over classes rather than a probability of real vs.",
      "startOffset" : 150,
      "endOffset" : 1591
    }, {
      "referenceID" : 16,
      "context" : "ment matching networks (Li et al., 2015; Dziugaite et al., 2015) have been proposed.",
      "startOffset" : 23,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "ment matching networks (Li et al., 2015; Dziugaite et al., 2015) have been proposed.",
      "startOffset" : 23,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016).",
      "startOffset" : 125,
      "endOffset" : 191
    }, {
      "referenceID" : 23,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016).",
      "startOffset" : 125,
      "endOffset" : 191
    }, {
      "referenceID" : 27,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016).",
      "startOffset" : 125,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks.",
      "startOffset" : 126,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks. Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color.",
      "startOffset" : 126,
      "endOffset" : 612
    }, {
      "referenceID" : 2,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks. Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color. Sohl-Dickstein et al. (2015) propose a generative model based on a reverse diffusion process.",
      "startOffset" : 126,
      "endOffset" : 720
    }, {
      "referenceID" : 2,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks. Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color. Sohl-Dickstein et al. (2015) propose a generative model based on a reverse diffusion process. While our model does involve image generation, it differs from these approaches in that the main focus is on learning a good representation for classification tasks. Predictive generative models of videos aim to extrapolate from current frames to future ones and in doing so learn a feature representation that is useful for other tasks. In this vein, Ranzato et al. (2014) used an `2-loss in pixel-space.",
      "startOffset" : 126,
      "endOffset" : 1159
    }, {
      "referenceID" : 2,
      "context" : "Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks. Other models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color. Sohl-Dickstein et al. (2015) propose a generative model based on a reverse diffusion process. While our model does involve image generation, it differs from these approaches in that the main focus is on learning a good representation for classification tasks. Predictive generative models of videos aim to extrapolate from current frames to future ones and in doing so learn a feature representation that is useful for other tasks. In this vein, Ranzato et al. (2014) used an `2-loss in pixel-space. Mathieu et al. (2015) combined an adversarial loss with `2, giving models that generate crisper images.",
      "startOffset" : 126,
      "endOffset" : 1213
    }, {
      "referenceID" : 7,
      "context" : "1 GENERATIVE ADVERSARIAL NETWORKS The generative adversarial network approach (Goodfellow et al., 2014) is a framework for training generative models, which we briefly review.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "We present a semi-supervised learning framework built on generative adversarial networks (GANs) of Goodfellow et al. (2014). We first review the generative adversarial network framework and then introduce context conditional generative adversarial networks (CC-GANs).",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "Recently, Radford et al. (2016) showed that a GAN discriminator learns a hierarchical image representation that is useful for object classification.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "5 MODEL ARCHITECTURE AND TRAINING DETAILS The architecture of our generative model,G, is inspired by the generator architecture of the DCGAN (Radford et al., 2016).",
      "startOffset" : 141,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "We followed the training procedures of Radford et al. (2016). We used the Adam optimizer (Kingma & Ba, 2015) in all our experiments with learning rate of 0.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 32,
      "context" : "Method Accuracy Multi-task Bayesian Optimization (Swersky et al., 2013) 70.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "6 Exemplar CNN (Dosovitskiy et al., 2014) 75.",
      "startOffset" : 15,
      "endOffset" : 41
    }, {
      "referenceID" : 38,
      "context" : "3 Stacked What-Where Autoencoder (Zhao et al., 2016) 74.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "As shown in Table 1, the CC-GAN model performs comparably to current state of the art (Dosovitskiy et al., 2014) and the CC-GAN model improves upon it.",
      "startOffset" : 86,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "This consisted of the same discriminator as the CC-GAN (VGG-A’ architecture) and generator from the DCGAN model (Radford et al., 2016).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "It is important to note that our feature extractor is the VGGA’ model which is larger than the AlexNet architecture (Krizhevsky et al., 2012) used by other approaches in Table 2.",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "As shown in Table 1, the CC-GAN model performs comparably to current state of the art (Dosovitskiy et al., 2014) and the CC-GAN model improves upon it. We also trained two baseline models in an attempt to tease apart the contributions of adversarial training and context conditional adversarial training. The first is a purely supervised training of the VGG-A’ model (the same architecture as the discriminator in the CC-GAN framework). This was trained using a dropout of 0.5 on the final layer and weight decay of 0.001. The performance of this model is significantly worse than the CC-GAN model. We also trained a semi-supervised GAN (SSL-GAN, see Fig. 1(b)) on STL-10. This consisted of the same discriminator as the CC-GAN (VGG-A’ architecture) and generator from the DCGAN model (Radford et al., 2016). The training setup in this case is identical to the CC-GAN model. The SSLGAN performs almost as well as the CC-GAN, confirming our hypothesis that the GAN objective is a useful unsupervised criterion. 3.2 PASCAL VOC CLASSIFICATION In order to compare against other methods that utilize spatial context we ran the CC-GAN model on PASCAL VOC 2007 dataset. This dataset consists of natural images coming from 20 classes. The dataset contains a large amount of variability with objects varying in size, pose, and position. The training and validation sets combined contain 5,011 images, and the test set contains 4,952 images. The standard measure of performance is mean average precision (mAP). We trained each model on the combined training and validation set for ∼5000 epochs and evaluated on the test set once1. Following Pathak et al. (2016), we train using random cropping, and then evaluate using the average prediction from 10 random crops.",
      "startOffset" : 87,
      "endOffset" : 1652
    }, {
      "referenceID" : 3,
      "context" : "4% Context prediction (Doersch et al., 2015) 55.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "3% Context encoders (Pathak et al., 2016) 56.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "Table 2: Comparison of CC-GAN and other methods (as reported by Pathak et al. (2016)) on PASCAL VOC 2007.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.",
      "startOffset" : 248,
      "endOffset" : 269
    } ],
    "year" : 2016,
    "abstractText" : "We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.",
    "creator" : "LaTeX with hyperref package"
  }
}