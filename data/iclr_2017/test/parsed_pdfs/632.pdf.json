{
  "name" : "632.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "QUESTION ANSWERING", "Liwen Zhang" ],
    "emails" : [ "liwenz@cs.uchicago.edu", "ryoto}@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "There is a growing interest in incorporating external memory into neural networks. For example, memory networks (Weston et al., 2014; Sukhbaatar et al., 2015) are equipped with static memory slots that are content or location addressable. Neural Turing machines (Graves et al., 2014) implement memory slots that can be read and written as in Turing machines (Turing, 1938) but through differentiable attention mechanism.\nEach memory slot in these models stores a vector corresponding to a continuous representation of the memory content. In order to recall a piece of information stored in memory, attention is typically employed. Attention mechanism introduced by Bahdanau et al. (2014) uses a network that outputs a discrete probability mass over memory items. A memory read can be implemented as a weighted sum of the memory vectors in which the weights are given by the attention network. Reading out a single item can be realized as a special case in which the output of the attention network is peaked at the desired item. The attention network may depend on the current context as well as the memory item itself. The attention model is called location-based and content-based, if it depends on the location in the memory and the stored memory vector, respectively.\nKnowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011; Nickel et al., 2011; Socher et al., 2013) or through a feedforward network (Bordes et al., 2015).\nWhen we embed entities from a knowledge base in a continuous vector space, if the capacity of the embedding model is appropriately controlled, we expect semantically similar entities to be close to each other, which will allow the model to generalize to unseen facts. However the notion of proximity may strongly depend on the type of a relation. For example, Benjamin Franklin was an engineer but also a politician. We would need different metrics to capture his proximity to other engineers and politicians of his time.\nIn this paper, we propose a new attention model for content-based addressing. Our model scores each item vitem in the memory by the (logarithm of) multivariate Gaussian likelihood as follows:\nscore(vitem) = log φ(vitem|µcontext,Σcontext)\n= −1 2 (vitem − µcontext)Σ −1 context(vitem − µcontext) + const. (1)\nwhere context denotes all the variables that the attention depends on. For example, “American engineers in the 18th century” or “American politicians in the 18th century” would be two contexts that include Benjamin Franklin but the two attentions would have very different shapes.\nCompared to the (normalized) inner product used in previous work (Sukhbaatar et al., 2015; Graves et al., 2014) for content-based addressing, the Gaussian model has the additional control of the spread of the attention over items in the memory. As we show in Figure 1, we can view the conventional inner-product-based attention and the proposed Gaussian attention as addressing by an affine energy function and a quadratic energy function, respectively. By making the addressing mechanism more complex, we may represent many entities in a relatively low dimensional embedding space. Since knowledge bases are typically extremely sparse, it is more likely that we can afford to have a more complex attention model than a large embedding dimension.\nWe apply the proposed Gaussian attention model to question answering based on knowledge bases. At the high-level, the goal of the task is to learn the mapping from a question about objects in the knowledge base in natural language to a probability distribution over the entities. We use the scoring function (1) for both embedding the entities as vectors, and extracting the conditions mentioned in the question and taking a conjunction of them to score each candidate answer to the question.\nThe ability to compactly represent a set of objects makes the Gaussian attention model well suited for representing the uncertainty in a multiple-answer question (e.g., “who are the children of Abraham Lincoln?”). Moreover, traversal over the knowledge graph (see Guu et al., 2015) can be naturally handled by a series of Gaussian convolutions, which generalizes the addition of vectors. In fact, we model each relation as a Gaussian with mean and variance parameters. Thus a traversal on a relation corresponds to a translation in the mean and addition of the variances.\nThe proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries. An example flow of how our model deals with a question “Who plays forward for Borussia Dortmund?” is shown in Figure 2 in Section 3.\nThis paper is structured as follows. In Section 2, we describe how the Gaussian scoring function (1) can be used to embed the entities in a knowledge base into a continuous vector space. We call our model TransGaussian because of its similarity to the TransE model proposed by Bordes et al. (2013). Then in Section 3, we describe our question answering model. In Section 4, we carry out experiments on WorldCup2014 dataset we collected. The dataset is relatively small but it allows us to evaluate not only simple questions but also path queries and conjunction of queries. The proposed TransGaussian embedding with the question answering model achieves significantly higher accuracy than the vanilla TransE embedding or TransE trained with compositional relations Guu et al. (2015) combined with the same question answering model."
    }, {
      "heading" : "2 KNOWLEDGE BASE EMBEDDING",
      "text" : "In this section, we describe the proposed TransGaussian model based on the Gaussian attention model (1). While it is possible to train a network that computes the embedding in a single pass (Bordes et al., 2015) or over multiple passes (Li et al., 2015), it is more efficient to offload the embedding as a separate step for question answering based on a large static knowledge base."
    }, {
      "heading" : "2.1 THE TRANSGAUSSIAN MODEL",
      "text" : "Let E be the set of entities and R be the set of relations. A knowledge base is a collection of triplets (s, r, o), where we call s ∈ E , r ∈ R, and o ∈ E , the subject, the relation, and the object of the triplet, respectively. Each triplet encodes a fact. For example, (Albert Einstein,has profession,theoretical physicist). All the triplets given in a knowledge base are assumed to be true. However generally speaking a triplet may be true or false. Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).\nIn this paper, we associate a vector vs ∈ Rd with each entity s ∈ E , and we associate each relation r ∈ R with two parameters, δr ∈ Rd and a positive definite symmetric matrix Σr ∈ Rd×d++ . Given subject s and relation r, we can compute the score of an object o to be in triplet (s, r, o) using the Gaussian attention model as (1) with\nscore(s, r, o) = log φ(vo|µcontext,Σcontext), (2) where µcontext = vs + δr, Σcontext = Σr. Note that if Σr is fixed to the identity matrix, we are modeling the relation of subject vs and object vo as a translation δr, which is equivalent to the TransE model (Bordes et al., 2013). We allow the covariance Σr to depend on the relation to handle one-to-many relations (e.g., profession has person relation) and capture the shape of the distribution of the set of objects that can be in the triplet. We call our model TransGaussian because of its similarity to TransE (Bordes et al., 2013).\nParameterization For computational efficiency, we will restrict the covariance matrix Σr to be diagonal in this paper. Furthermore, in order to ensure that Σr is strictly positive definite, we employ the exponential linear unit (ELU, Clevert et al., 2015) and parameterize Σr as follows:\nΣr = diag\n( ELU(mr,1)+1+\n. . . ELU(mr,d)+1+ ) where mr,j (j = 1, . . . , d) are the unconstrained parameters that are optimized during training and is a small positive value that ensure the positivity of the variance during numerical computation. The ELU is defined as\nELU(x) = { x, x ≥ 0, exp (x)− 1, x < 0.\nRanking loss Suppose we have a set of triplets T = {(si, ri, oi)}Ni=1 from the knowledge base. Let N (s, r) be the set of incorrect objects to be in the triplet (s, r, ·). Our objective function uses the ranking loss to measure the margin between the scores of true answers and those of false answers and it can be written as follows:\nmin {ve:e∈E},\n{δr,Mr,:r∈R̄}\n1\nN ∑ (s,r,o)∈T Et′∼N (s,r) [ [µ− score(s, r, o) + score(s, r, t′)]+ ]\n+ λ ∑ e∈E ‖ve‖22 + ∑ r∈R̄ ( ‖δr‖22 + ‖M r‖2F ) , (3) where, N = |T |, µ is the margin parameter and M r denotes the diagonal matrix with mr,j , j = 1, . . . , d on the diagonal; the function [·]+ is defined as [x]+ = max(0, x). Here, we treat an inverse\nrelation as a separate relation and denote by R̄ = R∪R−1 the set of all the relations including both relations in R and their inverse relations; a relation r̃ is the inverse relation of r if (s, r̃, o) implies (o, r, s) and vice versa. Moreover, Et′∼N (s,r) denotes the expectation with respect to the uniform distribution over the set of incorrect objects, which we approximate with 10 random samples in the experiments. Finally, the last terms are `2 regularization terms for the embedding parameters."
    }, {
      "heading" : "2.2 COMPOSITIONAL RELATIONS",
      "text" : "Guu et al. (2015) has recently shown that training TransE with compositional relations can make it competitive to more complex models, although TransE is much simpler compared to for example, neural tensor networks (NTN, Socher et al. (2013)) and TransH Wang et al. (2014). Here, a compositional relation is a relation that is composed as a series of relations in R, for example, grand father of can be composed as first applying the parent of relation and then the father of relation, which can be seen as a traversal over a path on the knowledge graph.\nTransGaussian model can naturally handle and propagate the uncertainty over such a chain of relations by convolving the Gaussian distributions along the path. That is, the score of an entity o to be in the τ -step relation r1/r2/ · · · /rτ with subject s, which we denote by the triplet (s, r1/r2/ · · · /rτ , o), is given as\nscore(s, r1/r2/ · · · /rτ , o) = log φ(vo|µcontext,Σcontext), (4) with µcontext = vs + ∑τ t=1 δrt , Σcontext = ∑τ t=1 Σrt , where the covariance associated with each relation is parameterized in the same way as in the previous subsection. Training with compositional relations Let P = {( si, ri1/ri2/ · · · /rili , oi )}N ′ i=1\nbe a set of randomly sampled paths from the knowledge graph. Here relation rik in a path can be a relation in R or an inverse relation in R−1. With the scoring function (4), the generalized training objective for compositional relations can be written identically to (3) except for replacing T with T ∪ P and replacing N with N ′ = |T ∪ P|."
    }, {
      "heading" : "3 QUESTION ANSWERING",
      "text" : "Given a set of question-answer pairs, in which the question is phrased in natural language and the answer is an entity in the knowledge base, our goal is to train a model that learns the mapping from the question to the correct entity. Our question answering model consists of three steps, entity recognition, relation composition, and conjunction. We first identify a list of entities mentioned in the question (which is assumed to be provided by an oracle in this paper). If the question is “Who plays Forward for Borussia Dortmund?” then the list would be [Forward, Borussia Dortmund]. The next step is to predict the path of relations on the knowledgegraph starting from each entity in the list extracted in the first step. In the above example, this will be (smooth versions of) /Forward/position played by/ and /Borussia Dortmund/has player/ predicted as series of Gaussian convolutions. In general, we can have multiple relations appearing in each path. Finally, we take a product of all the Gaussian attentions and renormalize it, which is equivalent to Bayes’ rule with independent observations (paths) and a noninformative prior."
    }, {
      "heading" : "3.1 ENTITY RECOGNITION",
      "text" : "We assume that there is an oracle that provides a list containing all the entities mentioned in the question, because (1) a domain specific entity recognizer can be developed efficiently (Williams et al., 2015) and (2) generally entity recognition is a challenging task and it is beyond the scope of this paper to show whether there is any benefit in training our question answering model jointly with a entity recognizer. We assume that the number of extracted entities can be different for each question."
    }, {
      "heading" : "3.2 RELATION COMPOSITION",
      "text" : "We train a long short-term memory (LSTM, Hochreiter & Schmidhuber, 1997) network that emits an output ht for each token in the input sequence. Then we compute the attention over the hidden\nstates for each recognized entity e as\npt,e = softmax (f(ve,ht)) (t = 1, . . . , T ),\nwhere ve is the vector associated with the entity e. We use a two-layer perceptron for f in our experiments, which can be written as follows:\nf(ve,ht) = u > f ReLU (W f,vve +W f,hht + b1) + b2,\nwhereW f,v∈RL×d,W f,h∈RL×H , b1 ∈ RL, uf ∈ RL, b2 ∈ R are parameters. Here ReLU(x)= max(0, x) is the rectified linear unit. Finally, softmax denotes softmax over the T tokens.\nNext, we use the weights pt,e to compute the weighted sum over the hidden states ht as oe = ∑T\nt=1 pt,eht. (5)\nThen we compute the weights αr,e over all the relations as αr,e = ReLU ( w>r oe ) (∀r ∈ R ∪ R−1). Here the rectified linear unit is used to ensure the positivity of the weights. Note however that the weights should not be normalized, because we may want to use the same relation more than once in the same path. Making the weights positive also has the effect of making the attention sparse and interpretable because there is no cancellation.\nFor each extracted entity e, we view the extracted entity and the answer of the question to be the subject and the object in some triplet (e, p, o), respectively, where the path p is inferred from the question as the weights αr,e as we described above. Accordingly, the score for each candidate answer o can be expressed using (1) as:\nscoree(vo) = log φ(vo|µe,α,KB,Σe,α,KB) (6) with µe,α,KB = ve + ∑ r∈R̄ αr,eδr, Σe,α,KB = ∑ r∈R̄ α 2 r,eΣr, where ve is the vector associated with entity e and R̄ = R∪R−1 denotes the set of relations including the inverse relations."
    }, {
      "heading" : "3.3 CONJUNCTION",
      "text" : "Let E(q) be the set of entities recognized in the question q. The final step of our model is to take the conjunction of the Gaussian attentions derived in the previous step. This step is simply carried out by multiplying the Gaussian attentions as follows:\nscore(vo|E(q),Θ) = log ∏\ne∈E(q)\nφ(vo|µe,α,KB,Σe,α,KB)\n= −1 2 ∑ e∈E(q) ( vo − µe,α,KB )> Σ−1e,α,KB(vo − µe,α,KB) + const., (7)\nwhich is again a (logarithm of) Gaussian scoring function, where µe,α,KB and Σe,α,KB are the mean and the covariance of the Gaussian attention given in (6). Here Θ denotes all the parameters of the question-answering model."
    }, {
      "heading" : "3.4 TRAINING THE QUESTION ANSWERING MODEL",
      "text" : "Suppose we have a knowledge base (E ,R, T ) and a trained TransGaussian model( {ve}e∈E , {(δr,Σr)}r∈R̄ ) , where R̄ is the set of all relations including the inverse relations. During training time, we assume the training set is a supervised question-answer pairs {(qi, E(qi), ai) : i = 1, 2, . . . ,m}. Here, qi is a question formulated in natural language, E(qi) ⊂ E is a set of knowledge base entities that appears in the question, and ai ∈ E is the answer to the question. For example, on a knowledge base of soccer players, a valid training sample could be\n(“Who plays forward for Borussia Dortmund?”,[Forward,Borussia Dortmund], Marco Reus).\nNote that the answer to a question is not necessarily unique and we allow ai to be any of the true answers in the knowledge base. During test time, our model is shown (qi, E(qi)) and the task is to find ai. We denote the set of answers to qi by A(qi).\nTo train our question-answering model, we minimize the objective function\n1\nm m∑ i=1 ( E t′∼N (qi) [ [µ− score(vai |E(qi),Θ) + score(vt′ |E(qi),Θ)]+ ] + ν ∑ e∈E(qi) ∑ r∈R̄ |αr,e| ) + λ‖Θ‖22\nwhere Et′∼N (qi) is expectation with respect to a uniform distribution over of all incorrect answers to qi, which we approximate with 10 random samples. We assume that the number of relations implied in a question is small compared to the total number of relations in the knowledge base. Hence the coefficients αr,e computed for each question qi are regularized by their `1 norms."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "As a demonstration of the proposed framework, we perform question and answering on a dataset of soccer players. In this work, we consider two types of questions. A path query is a question that contains only one named entity from the knowledge base and its answer can be found from the knowledge graph by walking down a path consisting of a few relations. A conjunctive query is a question that contains more than one entities and the answer is given as the conjunction of all path queries starting from each entity. Furthermore, we experimented on a knowledge base completion task with TransGaussian embeddings to test its capability of generalization to unseen fact. Since knowledge base completion is not the main focus of this work, we include the results in the Appendix."
    }, {
      "heading" : "4.1 WORLDCUP2014 DATASET",
      "text" : "We build a knowledge base of football players that participated in FIFA World Cup 2014 1. The original dataset consists of players’ information such as nationality, positions on the field and ages etc. We picked a few attributes and constructed 1127 entities and 6 atomic relations. The entities include 736 players, 297 professional soccer clubs, 51 countries, 39 numbers and 4 positions. And the six atomic relations are\n1The original dataset can be found at https://datahub.io/dataset/fifa-world-cup-2014-all-players.\nplays in club: PLAYER→ CLUB, plays position: PLAYER→ POSITION, is aged: PLAYER→ NUMBER, wears number 2: PLAYER→ NUMBER, plays for country: PLAYER→ COUNTRY, is in country: CLUB→ COUNTRY,\nwhere PLAYER, CLUB, NUMBER, etc, denote the type of entities that can appear as the left or right argument for each relation. Some relations share the same type as the right argument, e.g., plays for country and is in country.\nGiven the entities and relations, we transformed the dataset into a set of 3977 triplets. A list of sample triplets can be found in the Appendix. Based on these triplets, we created two sets of question answering tasks which we call path query and conjunctive query respectively. The answer of every question is always an entity in the knowledge base and a question can involve one or two triplets. The questions are generated as follows.\nPath queries. Among the paths on the knowledge graph, there are some natural composition of relations, e.g., plays in country (PLAYER → COUNTRY) can be decomposed as the composition of plays in club (PLAYER→ CLUB) and is in country (CLUB→ COUNTRY). In addition to the atomic relations, we manually picked a few meaningful compositions of relations and formed query templates, which takes the form “find e ∈ E , such that (s, p, e) is true”, where s is the subject and p can be an atomic relation or a path of relations. To formulate a set of path-based question-answer pairs, we manually created one or more question templates for every query template (see Table 5) Then, for a particular instantiation of a query template with subject and object entities, we randomly select a question template to generate a question given the subject; the object entity becomes the answer of the question. See Table 6 for the list of composed relations, sample questions, and answers. Note that all atomic relations in this dataset are many-to-one while these composed relations can be one-to-many or many-to-many as well.\nConjunctive queries. To generate question-and-answer pairs of conjunctive queries, we first picked three pairs of relations and used them to create query templates of the form “Find e ∈ E , such that both (s1, r1, e) and (s2, r2, e) are true.” (see Table 5). For a pair of relations r1 and r2, we enumerated all pairs of entities s1, s2 that can be their subjects and formulated the corresponding query in natural language using question templates as in the same way as path queries. See Table 7 for a list of sample questions and answers.\nAs a result, we created 8003 question-and-answer pairs of path queries and 2208 pairs of conjunctive queries which are partitioned into train / validation / test subsets. We refer to Table 1 for more statistics about the dataset. Templates for generating the questions are list in Table 5."
    }, {
      "heading" : "4.2 EXPERIMENTAL SETUP",
      "text" : "To perform question and answering under our proposed framework, we first train the TransGaussian model on WorldCup2014 dataset. In addition to the atomic triplets, we randomly sampled 50000 paths with length 1 or 2 from the knowledge graph and trained a TransGaussian model compositionally as described in Set 2.2. An inverse relation is treated as a separate relation. Following the naming convention from Guu et al. (2015), we denote this trained embedding by TransGaussian (COMP). We found that the learned embedding possess some interesting properties. Some dimensions of the embedding space dedicate to represent a particular relation. Players are clustered by their attributes when entities’ embeddings are projected to the corresponding lower dimensional subspaces. We elaborate and illustrate such properties in the Appendix.\nBaseline methods We also trained a TransGaussian model only on the atomic triplets and denote such a model by TransGaussian (SINGLE). Since no inverse relation was involved when TransGaussian (SINGLE) was trained, to use this embedding in question answering tasks, we represent the inverse relations as follows: for each relation r with mean δr and variance Σr, we model its inverse r−1 as a Gaussian attention with mean −δr and variance equal to Σr. We also trained TransE models on WorldCup2014 dataset by using the code released by the authors of Guu et al. (2015). Likewise, we use TransE (SINGLE) to denote the model trained with atomic triplets only and use TransE (COMP) to denote the model trained with the union of triplets and paths. Note that TransE can be considered as a special case of TransGaussian where the variance matrix is the identity and hence, the scoring formula Eq. (7) is applicable to TransE as well.\nTraining configurations For all models, dimension of entity embeddings was set to 30. The hidden size of LSTM was set to 80. Word embeddings were trained jointly with the question answering model and dimension of word embedding was set to 40. We employed Adam (Kingma & Ba, 2014) as the optimizer. All parameters were tuned on the validation set. Under the same setting, we experimented with two cases: first, we trained models for path queries and conjunctive queries separately; Furthermore, we trained a single model that addresses both types queries. We present the results of the latter case in the next subsection while the results of the former are included in the Appendix.\nEvaluation metrics During test time, our model receives a question in natural language and a list of knowledge base entities contained in the question. Then it predicts the mean and variance of a Gaussian attention formulated in Eq. (7) which is expected to capture the distribution of all positive answers. We rank all entities in the knowledge base by their scores under this Gaussian attention. Next, for each entity which is a correct answer, we check its rank relative to all incorrect answers and call this rank the filtered rank. For example, if a correct entity is ranked above all negative answers except for one, it has filtered rank two. We compute this rank for all true answers and report mean filtered rank and H@1 which is the percentage of true answers that have filtered rank 1."
    }, {
      "heading" : "4.3 EXPERIMENTAL RESULTS",
      "text" : "We present the results of joint learning in Table 2. These results show that TransGaussian works better than TransE in general. In fact, TransGaussian (COMP) achieved the best performance in almost all aspects. Most notably, it achieved the highest H@1 rates on challenging questions such as “where is the club that edin dzeko plays for?” (#11, composition of two relations) and “who are the defenders on german national team?” (#14, conjunction of two queries).\nThe same table shows that TransGaussian benefits remarkably from compositional training. For example, compositional training improved TransGaussian’s H@1 rate by near 60% in queries on players from a given countries (#8) and queries on players who play a particular position (#9). It also boosted TransGaussian’s performance on all conjunctive quries (#13–#15) significantly.\nTo understand TransGaussian (COMP)’s weak performance on answering queries on the professional football club located in a given country (#10) and queries on professional football club that has players from a particular country (#12), we tested its capability of modeling the composed relation by feeding the correct relations and subjects during test time. It turns out that these two relations were not modeled well by TransGaussian (COMP) embedding, which limits its performance in question answering. (See Table 8 in the Appendix for quantitative evaluations.) The same limit was found in the other three embeddings as well.\nNote that all the models compared in Table 2 uses the proposed Gaussian attention model because TransE is the special case of TransGaussian where the variance is fixed to one. Thus the main differences are whether the variance is learned and whether the embedding was trained compositionally. Finally, we refer to Table 9 and 10 in the Appendix for experimental results of models trained on path and conjunctive queries separately."
    }, {
      "heading" : "5 RELATED WORK",
      "text" : "The work of Vilnis & McCallum (2014) is similar to our Gaussian attention model. They discuss many advantages of the Gaussian embedding; for example, it is arguably a better way of handling asymmetric relations and entailment. However the work was presented in the word2vec (Mikolov et al., 2013)-style word embedding setting and the Gaussian embedding was used to capture the diversity in the meaning of a word. Our Gaussian attention model extends their work to a more general setting in which any memory item can be addressed through a concept represented as a Gaussian distribution over the memory items.\nBordes et al. (2014; 2015) proposed a question-answering model that embeds both questions and their answers to a common continuous vector space. Their method in Bordes et al. (2015) can combine multiple knowledge bases and even generalize to a knowledge base that was not used during training. However their method is limited to the simple question answering setting in which the answer of each question associated with a triplet in the knowledge base. In contrast, our method can handle both composition of relations and conjunction of conditions, which are both naturally enabled by the proposed Gaussian attention model.\nNeelakantan et al. (2015a) proposed a method that combines relations to deal with compositional relations for knowledge base completion. Their key technical contribution is to use recurrent neural networks (RNNs) to encode a chain of relations. When we restrict ourselves to path queries, question answering can be seen as a sequence transduction task (Graves, 2012; Sutskever et al., 2014) in which the input is text and the output is a series of relations. If we use RNNs as a decoder, our model would be able to handle non-commutative composition of relations, which the current weighted convolution cannot handle well. Another interesting connection to our work is that they take the maximum of the inner-product scores (see also Weston et al., 2013; Neelakantan et al., 2015b), which are computed along multiple paths connecting a pair of entities. Representing a set as a collection of vectors and taking the maximum over the inner-product scores is a natural way to represent a set of memory items. The Gaussian attention model we propose in this paper, however, has the advantage of differentiability and composability."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we have proposed the Gaussian attention model which can be used in a variety of contexts where we can assume that the distance between the memory items in the latent space is compatible with some notion of semantics. We have shown that the proposed Gaussian scoring function can be used for knowledge base embedding achieving competitive accuracy. We have also shown that our embedding model can naturally propagate uncertainty when we compose relations together. Our embedding model also benefits from compositional training proposed by Guu et al. (2015). Furthermore, we have demonstrated the power of the Gaussian attention model in a challenging question answering problem which involves both composition of relations and conjunction of queries. Future work includes experiments on natural question answering datasets and end-to-end training including the entity extractor."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank Daniel Tarlow, Nate Kushman, and Kevin Gimpel for valuable discussions."
    }, {
      "heading" : "A WORDCUP2014 DATASET",
      "text" : ""
    }, {
      "heading" : "B TRANSGAUSSIAN EMBEDDING OF WORLDCUP2014",
      "text" : "We trained our TransGaussian model on triplets and paths from WorldCup2014 dataset and illustrated the embeddings in Fig 3 and 4. Recall that we modeled every relation as a Gaussian with diagonal covariance matrix. Fig 3 shows the learned variance parameters of different relations. Each row corresponds to the variances of one relation. Columns are permuted to reveal the block structure. From this figure, we can see that every relation has a small variance in two or more dimensions. This implies that the coordinates of the embedding space are partitioned into semantically coherent clusters each of which represent a particular attribute of a player (or a football club). To verify this further, we picked the two coordinates in which a relation (e.g. plays position) has the least variance and projected the embedding of all valid subjects and objects (e.g. players and positions) of the relation to this 2 dimensional subspace. See Fig. 4. The relation between the subjects and the objects are simply translation in the projection when the corresponding subspace is two dimensional (e.g., plays position relation in Fig. 4 (a)). The same is true for other relations that requires larger dimension but it is more challenging to visualize in two dimensions. For relations that have a large number of unique objects, we only plotted for the eight objects with the most subjects for clarity of illustration.\nFurthermore, in order to elucidate whether we are limited by the capacity of the TransGaussian embedding or the ability to decode question expressed in natural language, we evaluated the test question-answer pairs using the TransGaussian embedding composed according to the ground-truth relations and entities. The results were evaluated with the same metrics as in Sec. 4.3. This estimation is conducted for TransE embeddings as well. See Table 8 for the results. Compared to Table 2, the accuracy of TransGaussian (COMP) is higher on the atomic relations and path queries but lower on conjunctive queries. This is natural because when the query is simple there is not much room for the question-answering network to improve upon just combining the relations according to the ground truth relations, whereas when the query is complex the network could combine the embedding in a more creative way to overcome its limitation. In fact, the two queries (#10 and #12) that TransGaussian (COMP) did not perform well in Table 2 pertain to a single relation is in country−1 (#10) and a composition of two relations plays for country−1 / plays in club (#12). The performance of the two queries were low even when the ground truth\nrelations were given, which indicates that the TransGaussian embedding rather than the questionanswering network is the limiting factor."
    }, {
      "heading" : "C KNOWLEDGE BASE COMPLETION",
      "text" : "Knowledge base completion has been a common task for testing knowledge base models on their ability of generalizing to unseen facts. Here, we apply our TransGaussian model to a knowledge completion task and show that it has competitive performance.\nWe tested on the subset of WordNet released by Guu et al. (2015). The atomic triplets in this dataset was originally created by Socher et al. (2013) and Guu et al. (2015) added path queries that were randomly sampled from the knowledge graph. We build our TransGaussian model by training on these triplets and paths and tested our model on the same link prediction task as done by Socher et al. (2013); Guu et al. (2015).\nAs done by Guu et al. (2015), we trained TransGaussian (SINGLE) with atomic triplets only and trained TransGaussian (COMP) with the union of atomic triplets and paths. We did not incorporate\nword embedding in this task and each entity is assigned its individual vector. Without getting parameters tuned too much, TransGaussian (COMP) obtained accuracy comparable to TransE (COMP). See Table 11."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio" ],
      "venue" : "In Conference on Artificial Intelligence, number EPFL-CONF-192344,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Open question answering with weakly supervised embedding models",
      "author" : [ "Antoine Bordes", "Jason Weston", "Nicolas Usunier" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1506.02075,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (elus)",
      "author" : [ "Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter" ],
      "venue" : "arXiv preprint arXiv:1511.07289,",
      "citeRegEx" : "Clevert et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence transduction with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1211.3711,",
      "citeRegEx" : "Graves.,? \\Q2012\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2012
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Traversing knowledge graphs in vector space",
      "author" : [ "Kelvin Guu", "John Miller", "Percy Liang" ],
      "venue" : "In EMNLP 2015,",
      "citeRegEx" : "Guu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Gated graph sequence neural networks",
      "author" : [ "Yujia Li", "Daniel Tarlow", "Marc Brockschmidt", "Richard Zemel" ],
      "venue" : "arXiv preprint arXiv:1511.05493,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Compositional vector space models for knowledge base completion",
      "author" : [ "Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum" ],
      "venue" : "arXiv preprint arXiv:1504.06662,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum" ],
      "venue" : "arXiv preprint arXiv:1504.06654,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "A three-way model for collective learning on multi-relational data",
      "author" : [ "Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel" ],
      "venue" : "In Proceedings of the 28th international conference on machine learning",
      "citeRegEx" : "Nickel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2011
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "On computable numbers, with an application to the entscheidungsproblem: A correction",
      "author" : [ "Alan Mathison Turing" ],
      "venue" : "Proceedings of the London Mathematical Society,",
      "citeRegEx" : "Turing.,? \\Q1938\\E",
      "shortCiteRegEx" : "Turing.",
      "year" : 1938
    }, {
      "title" : "Word representations via gaussian embedding",
      "author" : [ "Luke Vilnis", "Andrew McCallum" ],
      "venue" : "arXiv preprint arXiv:1412.6623,",
      "citeRegEx" : "Vilnis and McCallum.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vilnis and McCallum.",
      "year" : 2014
    }, {
      "title" : "Knowledge graph embedding by translating on hyperplanes",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Nonlinear latent factorization by embedding multiple user interests",
      "author" : [ "Jason Weston", "Ron J Weiss", "Hector Yee" ],
      "venue" : "In Proceedings of the 7th ACM conference on Recommender systems,",
      "citeRegEx" : "Weston et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast and easy language understanding for dialog systems with microsoft language understanding intelligent service (LUIS)",
      "author" : [ "Jason D Williams", "Eslam Kamal", "Hani Amr Mokhtar Ashour", "Jessica Miller", "Geoff Zweig" ],
      "venue" : "In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
      "citeRegEx" : "Williams et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2015
    }, {
      "title" : "TRANSGAUSSIAN EMBEDDING OF WORLDCUP2014 We trained our TransGaussian model on triplets and paths from WorldCup2014 dataset and illustrated the embeddings in Fig 3 and 4. Recall that we modeled every relation as a Gaussian with diagonal covariance matrix. Fig 3 shows the learned variance parameters of different relations",
      "author" : [ "B ha daesung" ],
      "venue" : null,
      "citeRegEx" : "daesung,? \\Q2014\\E",
      "shortCiteRegEx" : "daesung",
      "year" : 2014
    }, {
      "title" : "The atomic triplets in this dataset was originally created by Socher et al. (2013) and Guu et al. (2015) added path queries that were randomly sampled from the knowledge graph. We build our TransGaussian model by training on these triplets and paths and tested our model on the same link prediction task",
      "author" : [ "Guu" ],
      "venue" : "We tested on the subset of WordNet",
      "citeRegEx" : "Guu,? \\Q2015\\E",
      "shortCiteRegEx" : "Guu",
      "year" : 2015
    }, {
      "title" : "2015), we trained TransGaussian (SINGLE) with atomic triplets only and trained TransGaussian (COMP) with the union of atomic triplets and paths",
      "author" : [ "Guu" ],
      "venue" : null,
      "citeRegEx" : "Guu,? \\Q2015\\E",
      "shortCiteRegEx" : "Guu",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "For example, memory networks (Weston et al., 2014; Sukhbaatar et al., 2015) are equipped with static memory slots that are content or location addressable.",
      "startOffset" : 29,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "Neural Turing machines (Graves et al., 2014) implement memory slots that can be read and written as in Turing machines (Turing, 1938) but through differentiable attention mechanism.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : ", 2014) implement memory slots that can be read and written as in Turing machines (Turing, 1938) but through differentiable attention mechanism.",
      "startOffset" : 82,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "Knowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011; Nickel et al., 2011; Socher et al., 2013) or through a feedforward network (Bordes et al.",
      "startOffset" : 128,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "Knowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011; Nickel et al., 2011; Socher et al., 2013) or through a feedforward network (Bordes et al.",
      "startOffset" : 128,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "Knowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011; Nickel et al., 2011; Socher et al., 2013) or through a feedforward network (Bordes et al.",
      "startOffset" : 128,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : ", 2013) or through a feedforward network (Bordes et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Attention mechanism introduced by Bahdanau et al. (2014) uses a network that outputs a discrete probability mass over memory items.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "Compared to the (normalized) inner product used in previous work (Sukhbaatar et al., 2015; Graves et al., 2014) for content-based addressing, the Gaussian model has the additional control of the spread of the attention over items in the memory.",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Compared to the (normalized) inner product used in previous work (Sukhbaatar et al., 2015; Graves et al., 2014) for content-based addressing, the Gaussian model has the additional control of the spread of the attention over items in the memory.",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "The proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al.",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "The proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries.",
      "startOffset" : 168,
      "endOffset" : 282
    }, {
      "referenceID" : 1,
      "context" : "The proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries. An example flow of how our model deals with a question “Who plays forward for Borussia Dortmund?” is shown in Figure 2 in Section 3. This paper is structured as follows. In Section 2, we describe how the Gaussian scoring function (1) can be used to embed the entities in a knowledge base into a continuous vector space. We call our model TransGaussian because of its similarity to the TransE model proposed by Bordes et al. (2013). Then in Section 3, we describe our question answering model.",
      "startOffset" : 168,
      "endOffset" : 742
    }, {
      "referenceID" : 1,
      "context" : "The proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries. An example flow of how our model deals with a question “Who plays forward for Borussia Dortmund?” is shown in Figure 2 in Section 3. This paper is structured as follows. In Section 2, we describe how the Gaussian scoring function (1) can be used to embed the entities in a knowledge base into a continuous vector space. We call our model TransGaussian because of its similarity to the TransE model proposed by Bordes et al. (2013). Then in Section 3, we describe our question answering model. In Section 4, we carry out experiments on WorldCup2014 dataset we collected. The dataset is relatively small but it allows us to evaluate not only simple questions but also path queries and conjunction of queries. The proposed TransGaussian embedding with the question answering model achieves significantly higher accuracy than the vanilla TransE embedding or TransE trained with compositional relations Guu et al. (2015) combined with the same question answering model.",
      "startOffset" : 168,
      "endOffset" : 1227
    }, {
      "referenceID" : 4,
      "context" : "While it is possible to train a network that computes the embedding in a single pass (Bordes et al., 2015) or over multiple passes (Li et al.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : ", 2015) or over multiple passes (Li et al., 2015), it is more efficient to offload the embedding as a separate step for question answering based on a large static knowledge base.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 15,
      "context" : "Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 16,
      "context" : "Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 21,
      "context" : "Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 2,
      "context" : "Note that if Σr is fixed to the identity matrix, we are modeling the relation of subject vs and object vo as a translation δr, which is equivalent to the TransE model (Bordes et al., 2013).",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "We call our model TransGaussian because of its similarity to TransE (Bordes et al., 2013).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : "We assume that there is an oracle that provides a list containing all the entities mentioned in the question, because (1) a domain specific entity recognizer can be developed efficiently (Williams et al., 2015) and (2) generally entity recognition is a challenging task and it is beyond the scope of this paper to show whether there is any benefit in training our question answering model jointly with a entity recognizer.",
      "startOffset" : 187,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "Following the naming convention from Guu et al. (2015), we denote this trained embedding by TransGaussian (COMP).",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "We also trained TransE models on WorldCup2014 dataset by using the code released by the authors of Guu et al. (2015). Likewise, we use TransE (SINGLE) to denote the model trained with atomic triplets only and use TransE (COMP) to denote the model trained with the union of triplets and paths.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "However the work was presented in the word2vec (Mikolov et al., 2013)-style word embedding setting and the Gaussian embedding was used to capture the diversity in the meaning of a word.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "When we restrict ourselves to path queries, question answering can be seen as a sequence transduction task (Graves, 2012; Sutskever et al., 2014) in which the input is text and the output is a series of relations.",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "When we restrict ourselves to path queries, question answering can be seen as a sequence transduction task (Graves, 2012; Sutskever et al., 2014) in which the input is text and the output is a series of relations.",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Our embedding model also benefits from compositional training proposed by Guu et al. (2015). Furthermore, we have demonstrated the power of the Gaussian attention model in a challenging question answering problem which involves both composition of relations and conjunction of queries.",
      "startOffset" : 74,
      "endOffset" : 92
    } ],
    "year" : 2016,
    "abstractText" : "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.",
    "creator" : "LaTeX with hyperref package"
  }
}