{"conference": "ICLR 2017 conference submission", "title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.", "histories": [], "reviews": [{"DATE": "18 May 2017", "TITLE": "Better word embeddings", "IS_META_REVIEW": false, "comments": "Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. ", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "22 Mar 2017", "TITLE": "About Table 3: Words with embeddings closest to 0 learned by different algorithms.", "IS_META_REVIEW": false, "comments": "I'm using ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.\n \n Pros:\n + interesting and simple algorithm\n + strong performance\n + efficient\n \n Cons:\n + individual ideas are not so novel\n \n This is a paper that will be well received at a poster presentation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Dec 2016", "TITLE": "new dataset and baselines added", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "21 Dec 2016", "TITLE": "revisions", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nThank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission. ", "OTHER_KEYS": "Minmin Chen"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Simple idea, nicely composed", "comments": "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.\n\nThe main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?\n\nWhile I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 04 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "Need more recent baselines", "IS_META_REVIEW": false, "comments": "Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Interesting corruption mechanism for document representation", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Effect of document length", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"TITLE": "pre review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 5}, {"DATE": "18 May 2017", "TITLE": "Better word embeddings", "IS_META_REVIEW": false, "comments": "Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. ", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "22 Mar 2017", "TITLE": "About Table 3: Words with embeddings closest to 0 learned by different algorithms.", "IS_META_REVIEW": false, "comments": "I'm using ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.\n \n Pros:\n + interesting and simple algorithm\n + strong performance\n + efficient\n \n Cons:\n + individual ideas are not so novel\n \n This is a paper that will be well received at a poster presentation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Dec 2016", "TITLE": "new dataset and baselines added", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "21 Dec 2016", "TITLE": "revisions", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nThank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission. ", "OTHER_KEYS": "Minmin Chen"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Simple idea, nicely composed", "comments": "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.\n\nThe main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?\n\nWhile I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 04 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "Need more recent baselines", "IS_META_REVIEW": false, "comments": "Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Interesting corruption mechanism for document representation", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Effect of document length", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"TITLE": "pre review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 5}], "authors": "Minmin Chen", "accepted": true, "id": "330"}