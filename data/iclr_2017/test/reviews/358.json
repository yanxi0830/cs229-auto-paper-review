{"conference": "ICLR 2017 conference submission", "title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Solid Contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a \"double\" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  \n\nHierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  \n\nPros:\n  -The few-shot learning results look good, but I'm not an expert in this area.  \n  -The idea of using a \"double\" variational bound in a hierarchical generative model is well presented and seems widely applicable.  \n\nQuestions: \n  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  \n  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  \n\nSuggestions: \n  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.\n\nThis paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to \"learn to learn\" by acquiring the ability to learn distributions from small numbers of examples.\n\nOverall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.\n\nThe name of the paper is overly grandiose relative to what was done; the proposed method doesn\u2019t seem to have much in common with a statistician, unless one means by that \"someone who thinks up statistics\". \n\nThe experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.\n\nThe spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn\u2019t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don\u2019t seem to correspond to meaningful points as was claimed in the text.) \n\nWill the authors release the code?\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "a nice addition to the one-/few-shot learning literature", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting paper that starts to expand the repertoire of variational autoencoders", "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "one-shot task for generative models", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "05 Dec 2016", "CLARITY": 3}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "MNIST classification performance?", "IS_META_REVIEW": false, "DATE": "29 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Solid Contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a \"double\" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  \n\nHierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  \n\nPros:\n  -The few-shot learning results look good, but I'm not an expert in this area.  \n  -The idea of using a \"double\" variational bound in a hierarchical generative model is well presented and seems widely applicable.  \n\nQuestions: \n  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  \n  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  \n\nSuggestions: \n  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.\n\nThis paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to \"learn to learn\" by acquiring the ability to learn distributions from small numbers of examples.\n\nOverall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.\n\nThe name of the paper is overly grandiose relative to what was done; the proposed method doesn\u2019t seem to have much in common with a statistician, unless one means by that \"someone who thinks up statistics\". \n\nThe experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.\n\nThe spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn\u2019t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don\u2019t seem to correspond to meaningful points as was claimed in the text.) \n\nWill the authors release the code?\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "a nice addition to the one-/few-shot learning literature", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting paper that starts to expand the repertoire of variational autoencoders", "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "one-shot task for generative models", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "05 Dec 2016", "CLARITY": 3}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "MNIST classification performance?", "IS_META_REVIEW": false, "DATE": "29 Nov 2016"}], "authors": "Harrison Edwards, Amos Storkey", "accepted": true, "id": "358"}