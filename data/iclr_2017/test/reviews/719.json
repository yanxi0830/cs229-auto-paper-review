{"conference": "ICLR 2017 conference submission", "title": "Classless Association using Neural Networks", "abstract": "The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class.  This scenario is inspired by the Symbol Grounding Problem and the association learning in infants.  We propose a novel model called Classless Association.  It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa.  In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution.  We generate four classless datasets based on MNIST, where the input is two different instances of the same digit.  In addition,  the digits have a uniform distribution.  Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised.  In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint.  In the second scenario, our model reaches better results against two clustering algorithms.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "New revision", "IS_META_REVIEW": false, "comments": "We have updated our paper.  The changes are\n* We have improved the clarity and motivation of our model\n* We have evaluated our model to three more classless datasets (Rotated-90 MNIST, Inverted MNIST, and Random Rotation MNIST).\n* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.\n* We have added two more examples and demo as supplemental material ", "OTHER_KEYS": "Federico Raue"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.\n\n\nThe presentation of the paper is not very clear, the writing can be improved.\nSome design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  \n\nAlso, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. \n\nOverall, I think this work should be clarified and improved to be a good fit for this venue.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.\n\nThe basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. \n\nWhat would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. \n\nAt the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.\n\nFew more points:\nTypo: Figure1. second line in the caption \"that\" -> \"than\"\nNecessity of Equation 2 is not clear\nBatch size M is enormous compared to classical models, there is no explanation for this\nWhy uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)\nTypo: Page 6, second paragraph line 3: \"that\" -> \"than\"", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Classes association", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Any prior info?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "New revision", "IS_META_REVIEW": false, "comments": "We have updated our paper.  The changes are\n* We have improved the clarity and motivation of our model\n* We have evaluated our model to three more classless datasets (Rotated-90 MNIST, Inverted MNIST, and Random Rotation MNIST).\n* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.\n* We have added two more examples and demo as supplemental material ", "OTHER_KEYS": "Federico Raue"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.\n\n\nThe presentation of the paper is not very clear, the writing can be improved.\nSome design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  \n\nAlso, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. \n\nOverall, I think this work should be clarified and improved to be a good fit for this venue.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.\n\nThe basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. \n\nWhat would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. \n\nAt the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.\n\nFew more points:\nTypo: Figure1. second line in the caption \"that\" -> \"than\"\nNecessity of Equation 2 is not clear\nBatch size M is enormous compared to classical models, there is no explanation for this\nWhy uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)\nTypo: Page 6, second paragraph line 3: \"that\" -> \"than\"", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Classes association", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Any prior info?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Federico Raue, Sebastian Palacio, Andreas Dengel, Marcus Liwicki", "accepted": false, "id": "719"}