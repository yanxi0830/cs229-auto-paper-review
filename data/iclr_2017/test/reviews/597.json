{"conference": "ICLR 2017 conference submission", "title": "Neural Combinatorial Optimization with Reinforcement Learning", "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items. These results, albeit still far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. \n \n Pros:\n - All agree that the work is extremely clear, going as far as saying the work is \"very well written\" and \"easy to understand\". \n - Generally there was a predisposition to support the work for its originality particularly due to its \"methodological contributions\", and even going so far as a saying it would generally be a natural accept.\n \n Cons:\n - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an \"excellent example of hype-generation far before having state-of-the-art results\" and that it was \"doing a disservice to our community since it builds up an expectation that the field cannot live up to\" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting \"the toy-ness of the evaluation metric\" and the way the comparisons were carried out.\n - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting \"operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality\". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "25 Jan 2017", "TITLE": "Summary of new changes to the paper", "IS_META_REVIEW": false, "comments": "We ask reviewers to have a look at the new version of the paper again given the changes outlined below:\n\n- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).\n\n- We include the original KnapSack baselines back into the paper.\n\n- We explain in details how the running time of the LKH baseline is obtained.\n\n- We modify the statement on the performance of greedy approaches: instead of stating that they are \u201cjust a few percents from optimality\u201d, we express that they are \u201cstill quite far from optimality\u201d.\n\nWe thank reviewers for their help in improving the quality of the paper.", "OTHER_KEYS": "Irwan Bello"}, {"DATE": "06 Jan 2017", "TITLE": "Question", "IS_META_REVIEW": false, "comments": "I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.\n\nThanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being \"highly optimized\" to particular problems, but if every worthwhile problem has \"highly optimized\" solutions, what good is your work? \n\nAlso, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with \"thousands of cities\" when it has solved a 85k problem. \n\nI'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.\n\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "05 Jan 2017", "TITLE": "Summary of paper's revision", "IS_META_REVIEW": false, "comments": "We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.\n\n1) Previous Figure 1, which was problematic due to different possible interpretations of \u201clocal search\u201d was removed.\n\n2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.\n\n3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.\n\n4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.\n\n5) We added a more detailed description of the critic network (see Section 4 - Critic\u2019s architecture for TSP).\n\nPlease take a look and let us know your thoughts.", "OTHER_KEYS": "Irwan Bello"}, {"TITLE": "Promising approach to combinatorial optimization", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper applies the pointer network architecture\u2014wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements\u2014in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.\n\nThe paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference.\n\nI have a few comments and some important reservations with the paper:\n\n1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems \u2014 for practical applications \u2014 lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just \u00ab striking off \u00bb previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.\n\n2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Promising method, but biased presentation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. \n\nSeeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for \"local search\" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being \"RNNs now also clearly perform better than local search\". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. \n\nThe right course of action upon realizing the real strength of local search with LK-H would've been to make \"local search\" the same line as \"Optimal\", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. \n\n------------------------\n\nUpdate after rebuttal and changes:\n\nI'm torn about this paper. \n\nOn the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.\n\nOn the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as \"We find that both greedy approaches are time-efficient and just a few percents worse than optimality.\"\nThat statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. \n(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).\n\nNevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:\n\n\"Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).\" That version then went on to show that these simple heuristics were already optimal, just like their own method.\n\nIn a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "About Concorde", "IS_META_REVIEW": false, "comments": "This is very interesting to me! Thank you for this.\n\nAfter reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.\nBut error can be small if multiply the distance by a large constant.\n\nI want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "09 Dec 2016", "TITLE": "Control for computation cost?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Code availability", "IS_META_REVIEW": false, "comments": "I am very glad to read \"Our model and training code will be made available soon.\" Thanks for that! My question is: how soon is soon? During the review period? In time for the conference? ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Is RL pretraining Sampling T=T* actually better than RL pretraining AS when compared with the same number of 10.000 batches?", "IS_META_REVIEW": false, "comments": "In Table 3, what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? \n\nSince performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g., 5.79->5.71 vs 5.74->5.71 for TSP50), I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying \"we sample 1000 batches from a pretrained model, afer which we do not see significant improvement\", but seeing the much larger \"gradient\" from 50, 100, and 1000 batches than for RL pretraining AS, and seeing how key the result is to the final take-away from the paper, I would be far more convinced by just seeing the numbers for 10.000 batches.\n\nAlso, what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.)\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Christofides algorithm wrong baseline", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "07 Nov 2016", "TITLE": "Related work incomplete", "IS_META_REVIEW": false, "comments": "There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.\n\n", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. \n \n Pros:\n - All agree that the work is extremely clear, going as far as saying the work is \"very well written\" and \"easy to understand\". \n - Generally there was a predisposition to support the work for its originality particularly due to its \"methodological contributions\", and even going so far as a saying it would generally be a natural accept.\n \n Cons:\n - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an \"excellent example of hype-generation far before having state-of-the-art results\" and that it was \"doing a disservice to our community since it builds up an expectation that the field cannot live up to\" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting \"the toy-ness of the evaluation metric\" and the way the comparisons were carried out.\n - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting \"operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality\". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "25 Jan 2017", "TITLE": "Summary of new changes to the paper", "IS_META_REVIEW": false, "comments": "We ask reviewers to have a look at the new version of the paper again given the changes outlined below:\n\n- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).\n\n- We include the original KnapSack baselines back into the paper.\n\n- We explain in details how the running time of the LKH baseline is obtained.\n\n- We modify the statement on the performance of greedy approaches: instead of stating that they are \u201cjust a few percents from optimality\u201d, we express that they are \u201cstill quite far from optimality\u201d.\n\nWe thank reviewers for their help in improving the quality of the paper.", "OTHER_KEYS": "Irwan Bello"}, {"DATE": "06 Jan 2017", "TITLE": "Question", "IS_META_REVIEW": false, "comments": "I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.\n\nThanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being \"highly optimized\" to particular problems, but if every worthwhile problem has \"highly optimized\" solutions, what good is your work? \n\nAlso, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with \"thousands of cities\" when it has solved a 85k problem. \n\nI'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.\n\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "05 Jan 2017", "TITLE": "Summary of paper's revision", "IS_META_REVIEW": false, "comments": "We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.\n\n1) Previous Figure 1, which was problematic due to different possible interpretations of \u201clocal search\u201d was removed.\n\n2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.\n\n3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.\n\n4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.\n\n5) We added a more detailed description of the critic network (see Section 4 - Critic\u2019s architecture for TSP).\n\nPlease take a look and let us know your thoughts.", "OTHER_KEYS": "Irwan Bello"}, {"TITLE": "Promising approach to combinatorial optimization", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper applies the pointer network architecture\u2014wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements\u2014in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.\n\nThe paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference.\n\nI have a few comments and some important reservations with the paper:\n\n1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems \u2014 for practical applications \u2014 lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just \u00ab striking off \u00bb previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.\n\n2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Promising method, but biased presentation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. \n\nSeeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for \"local search\" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being \"RNNs now also clearly perform better than local search\". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. \n\nThe right course of action upon realizing the real strength of local search with LK-H would've been to make \"local search\" the same line as \"Optimal\", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. \n\n------------------------\n\nUpdate after rebuttal and changes:\n\nI'm torn about this paper. \n\nOn the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.\n\nOn the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as \"We find that both greedy approaches are time-efficient and just a few percents worse than optimality.\"\nThat statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. \n(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).\n\nNevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:\n\n\"Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).\" That version then went on to show that these simple heuristics were already optimal, just like their own method.\n\nIn a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "About Concorde", "IS_META_REVIEW": false, "comments": "This is very interesting to me! Thank you for this.\n\nAfter reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.\nBut error can be small if multiply the distance by a large constant.\n\nI want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "09 Dec 2016", "TITLE": "Control for computation cost?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Code availability", "IS_META_REVIEW": false, "comments": "I am very glad to read \"Our model and training code will be made available soon.\" Thanks for that! My question is: how soon is soon? During the review period? In time for the conference? ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Is RL pretraining Sampling T=T* actually better than RL pretraining AS when compared with the same number of 10.000 batches?", "IS_META_REVIEW": false, "comments": "In Table 3, what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? \n\nSince performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g., 5.79->5.71 vs 5.74->5.71 for TSP50), I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying \"we sample 1000 batches from a pretrained model, afer which we do not see significant improvement\", but seeing the much larger \"gradient\" from 50, 100, and 1000 batches than for RL pretraining AS, and seeing how key the result is to the final take-away from the paper, I would be far more convinced by just seeing the numbers for 10.000 batches.\n\nAlso, what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.)\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Christofides algorithm wrong baseline", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "07 Nov 2016", "TITLE": "Related work incomplete", "IS_META_REVIEW": false, "comments": "There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.\n\n", "OTHER_KEYS": "(anonymous)"}], "authors": "Irwan Bello*, Hieu Pham*, Quoc V. Le, Mohammad Norouzi, Samy Bengio", "accepted": false, "id": "597"}