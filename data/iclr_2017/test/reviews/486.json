{"conference": "ICLR 2017 conference submission", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "General response", "IS_META_REVIEW": false, "comments": "Dear Reviewers,\n\nThanks a lot for reviewing our paper and for your valuable comments. \n\nTo incorporate your feedback, we have uploaded a revision of our paper with the following changes:\n\n1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github: ", "OTHER_KEYS": "Thomas N. Kipf"}, {"TITLE": "Simple and reasonable approach", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. \n\nExperiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "29 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496\u2013503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955\u2013984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593\u2013598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853\u2013\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96\u2013103.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Solid results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Why is GPU not much faster than CPU", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "General response", "IS_META_REVIEW": false, "comments": "Dear Reviewers,\n\nThanks a lot for reviewing our paper and for your valuable comments. \n\nTo incorporate your feedback, we have uploaded a revision of our paper with the following changes:\n\n1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github: ", "OTHER_KEYS": "Thomas N. Kipf"}, {"TITLE": "Simple and reasonable approach", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. \n\nExperiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "29 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496\u2013503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955\u2013984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593\u2013598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853\u2013\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96\u2013103.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Solid results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Why is GPU not much faster than CPU", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Thomas N. Kipf, Max Welling", "accepted": true, "id": "486"}