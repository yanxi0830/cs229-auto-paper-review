{"conference": "ICLR 2017 conference submission", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "01 Jan 2017", "TITLE": "Revision of the paper", "IS_META_REVIEW": false, "comments": "We made the following revisions:\n\n1. We switched the section 6.3 and 6.4 to make the paper more clear.\n\n2. We added the definition of MC dropout on page 8.\n\n3. We fixed all the typos in the three reviewers' comments.", "OTHER_KEYS": "Xuezhe Ma"}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "summary\n\nThe paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n\nThe paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).\nThis framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.\n\nFinally a new regularisation term is introduced to account for minimisation of the inference gap during learning.\n\nExperiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)\n\n\nThe study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.\n\nThe framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.\n\nThe proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.\n\np6 line 8 typo: expecatation", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.\n\nOne relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.\n\nMC dropout on page 8 is not defined, please define.\n\nOn page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?\n\nFrom Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.\n\nCouple of typos:\n- Pg. 2 \u201c \u2026 x is he input \u2026\u201d -> \u201c \u2026 x is the input \u2026\u201d\n- Pg. 5 \u201c \u2026 as defined in (1), is \u2026\u201d -> ref. to (1) is not right at two places in this paragraph\n\nOverall it is a good paper, I think should be accepted and discussed at the conference.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.\n ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "precisions on the details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "01 Jan 2017", "TITLE": "Revision of the paper", "IS_META_REVIEW": false, "comments": "We made the following revisions:\n\n1. We switched the section 6.3 and 6.4 to make the paper more clear.\n\n2. We added the definition of MC dropout on page 8.\n\n3. We fixed all the typos in the three reviewers' comments.", "OTHER_KEYS": "Xuezhe Ma"}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "summary\n\nThe paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n\nThe paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).\nThis framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.\n\nFinally a new regularisation term is introduced to account for minimisation of the inference gap during learning.\n\nExperiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)\n\n\nThe study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.\n\nThe framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.\n\nThe proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.\n\np6 line 8 typo: expecatation", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.\n\nOne relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.\n\nMC dropout on page 8 is not defined, please define.\n\nOn page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?\n\nFrom Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.\n\nCouple of typos:\n- Pg. 2 \u201c \u2026 x is he input \u2026\u201d -> \u201c \u2026 x is the input \u2026\u201d\n- Pg. 5 \u201c \u2026 as defined in (1), is \u2026\u201d -> ref. to (1) is not right at two places in this paragraph\n\nOverall it is a good paper, I think should be accepted and discussed at the conference.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.\n ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "precisions on the details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, Eduard Hovy", "accepted": true, "id": "498"}