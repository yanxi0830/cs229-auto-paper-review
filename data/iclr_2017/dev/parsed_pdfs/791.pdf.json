{
  "name" : "791.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEEP UNSUPERVISED LEARNING THROUGH SPATIAL CONTRASTING",
    "authors" : [ "Elad Hoffer", "Itay Hubara" ],
    "emails" : [ "ehoffer@tx.technion.ac.il", "itayh@tx.technion.ac.il", "nailon@cs.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "For the past few years convolutional networks (ConvNets, CNNs) LeCun et al. (1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015) Pinheiro et al. (2015) Razavian et al. (2014). A convolutional network is composed of multiple convolutional and pooling layers, followed by a fully-connected affine transformations. As with other neural network models, each layer is typically followed by a non-linearity transformation such as a rectified-linear unit (ReLU). A convolutional layer is applied by cross correlating an image with a trainable weight filter. This stems from the assumption of stationarity in natural images, which means that parameters learned for one local region in an image can be shared for other regions and images.\nDeep learning models, including convolutional networks, are usually trained in a supervised manner, requiring large amounts of labeled data (ranging between thousands to millions of examples per-class for classification tasks) in almost all modern applications. These models are optimized using a variant of stochastic-gradient-descent (SGD) over batches of images sampled from the whole training dataset and their ground truth-labels. Gradient estimation for each one of the optimized parameters is done by back propagating the objective error from the final layer towards the input. This is commonly known as ”backpropagation” Rumelhart et al..\nIn early works, unsupervised training was used as a part of pre-training procedure to obtain an effective initial state of the model. The network was later fine-tuned in a supervised manner as displayed by Hinton (2007). Such unsupervised pre-training procedures were later abandoned, since they provided no apparent benefit over other initialization heuristics in more careful fully supervised training regimes. This led to the de-facto almost exclusive usage of neural networks in supervised environments.\nIn this work we will present a novel unsupervised learning criterion for convolutional network based on comparison of features extracted from regions within images. Our experiments indicate that by\nusing this criterion to pre-train networks we can improve their performance and achieve state-ofthe-art results."
    }, {
      "heading" : "2 PREVIOUS WORKS",
      "text" : "Using unsupervised methods to improve performance have been the holy grail of deep learning for the last couple of years and vast research efforts have been focused on that. We hereby give a short overview of the most popular and recent methods that tried to tackle this problem.\nAutoEncoders and reconstruction loss These are probably the most popular models for unsupervised learning using neural networks, and ConvNets in particular. Autoencoders are NNs which aim to transform inputs into outputs with the least possible amount of distortion. An Autoencoder is constructed using an encoder G(x;w1) that maps an input to a hidden compressed representation, followed by a decoder F (y;w2), that maps the representation back into the input space. Mathematically, this can be written in the following general form:\nx̂ = F (G(x;w1);w2)\nThe underlying encoder and decoder contain a set of trainable parameters that can be tied together and optimized for a predefined criterion. The encoder and decoder can have different architectures, including fully-connected neural networks, ConvNets and others. The criterion used for training is the reconstruction loss, usually the mean squared error (MSE) between the original input and its reconstruction Zeiler et al. (2010)\nmin‖x− x̂‖2\nThis allows an efficient training procedure using the aforementioned backpropagation and SGD techniques. Over the years autoencoders gained fundamental role in unsupervised learning and many modification to the classic architecture were made. Ng (2011) regularized the latent representation to be sparse, Vincent et al. (2008) substituted the input with a noisy version thereof, requiring the model to denoise while reconstructing. Kingma et al. (2014) obtained very promising results with variational autoencoders (VAE). A variational autoencoder model inherits typical autoencoder architecture, but makes strong assumptions concerning the distribution of latent variables. They use variational approach for latent representation learning, which results in an additional loss component which required a new training algorithm called Stochastic Gradient Variational Bayes (SGVB). VAE assumes that the data is generated by a directed graphical model p(x|z) and require the encoder to learn an approximation qw1(z|x) to the posterior distribution pw2(z|x) where w1 and w2 denote the parameters of the encoder and decoder. The objective of the variational autoencoder in that case has the following form:\nL(w1, w2, x) = −DKL(qw1(z|x)||pw2(z)) + Eqw1 (z|x) ( log pw2(x|z) ) Recently, a stacked set of denoising autoencoders architectures showed promising results in both semi-supervised and unsupervised tasks. A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping. Ladder networks by Rasmus et al. (2015) - use lateral connections and layer-wise cost functions to allow the higher levels of an autoencoder to focus on invariant abstract features.\nExemplar Networks: The unsupervised method introduced byDosovitskiy et al. (2014) takes a different approach to this task and trains the network to discriminate between a set of pseudo-classes. Each pseudo-class is formed by applying multiple transformations to a randomly sampled image patch. The number of pseudo-classes can be as big as the size of the input samples. This criterion ensures that different input samples would be distinguished while providing robustness to the applied transformations. In this work we will explore an alternative method with a similar motivation.\nContext prediction Another method for unsupervised learning by context was introduced by Doersch et al. (2015). This method uses an auxiliary criterion of predicting the location of an image patch given another from the same image. This is done by classification to 1 of 9 possible locations. Although the work of Doersch et al. (2015) and ours both use patches from an image to perform unsupervised learning, the methods are quite different. Whereas the former used a classification criterion over the spatial location of each patch within a single image, our work is concerned with comparing patches from several images to each other. We claim that this encourages discriminability between images (which we feel to be important aspect of feature learning), and was not an explicit goal in previous work.\nAdversarial Generative Models: This a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models uses a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015),this model can create useful latent representations for subsequent classification tasks.\nSampling Methods: Methods for training models to discriminate between a very large number of classes often use a noise contrasting criterion. In these methods, roughly speaking, the posterior probability P (t|yt) of the ground-truth target t given the model output on an input sampled from the true distribution yt = F (x) is maximized, while the probability P (t|yn) given a noise measurement y = F (n) is minimized. This was successfully used in a language domain to learn unsupervised representation of words. The most noteworthy case is the word2vec model introduced by Mikolov et al. (2013). When using this setting in language applications, a natural contrasting noise is a smooth approximation of the Unigram distribution. A suitable contrasting distribution is less obvious when data points are sampled from a high dimensional continuous space, such as the case of image patches."
    }, {
      "heading" : "2.1 PROBLEMS WITH CURRENT APPROACHES",
      "text" : "Only recently the potential of ConvNets in an unsupervised environment began to bear fruit, still we believe it is not fully uncovered.\nThe majority of unsupervised optimization criteria currently used are based on variations of reconstruction losses. One limitation of this fact is that a pixel level reconstruction is non-compliant with the idea of a discriminative objective, which is expected to be agnostic to low level information in the input. In addition, it is evident that MSE is not best suited as a measurement to compare images, for example, viewing the possibly large square-error between an image and a single pixel shifted copy of it. Another problem with recent approaches such as Rasmus et al. (2015); Zeiler et al. (2010) is their need to extensively modify the original convolutional network model. This leads to a gap between unsupervised method and the state-of-the-art, supervised, models for classification - which can hurt future attempt to reconcile them in a unified framework, as well as efficiently leverage unlabeled data with otherwise supervised regimes."
    }, {
      "heading" : "3 LEARNING BY COMPARISONS",
      "text" : "The most common way to train NN is by defining a loss function between the target values and the network output. Learning by comparison approaches the supervised task from a different angle. The main idea is to use distance comparisons between samples to learn useful representations. For example, we consider relative and qualitative examples of the form X1 is closer to X2 than X1 is to X3. Using a comparative measure with neural network to learn embedding space was introduced in the “Siamese network” framework by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. There, one image served as an anchor x, and an additional pair of images served as a positive example x+ (containing an instance\nof the face of the same person) together with a negative example x−, containing a face of a different person. The training objective was on the embedded distance of the input faces, where the distance between the anchor and positive example is adjusted to be smaller by at least some constant α from the negative distance. More precisely, the loss function used in this case was defined as\nL(x, x+, x−) = max {‖F (x)− F (x+)‖2 − ‖F (x)− F (x−)‖2 + α, 0} (1)\nwhere F (x) is the embedding (the output of a convolutional neural network), and α is a predefined margin constant. Another similar model used by Hoffer & Ailon (2015) with triplets comparisons for classification, where examples from the same class were trained to have a lower embedded distance than that of two images from distinct classes. This work introduced a concept of a distance ratio loss, where the defined measure amounted to:\nL(x, x+, x−) = e−‖F (x)−F (x+)‖2\ne−‖F (x)−F (x+)‖2 + e−‖F (x)−F (x−)‖2 (2)\nThis loss has a flavor of a probability of a biased coin flip. By ‘pushing’ this probability to zero, we express the objective that pairs of samples coming from distinct classes should be less similar to each other, compared to pairs of samples coming from the same class. It was shown empirical by Balntas et al. (2016) to provide better feature embeddings than the margin based distance loss 1"
    }, {
      "heading" : "4 OUR CONTRIBUTION: SPATIAL CONTRASTING",
      "text" : "One implicit assumption in convolutional networks, is that features are gradually learned hierarchically, each level in the hierarchy corresponding to a layer in the network. Each spatial location within a layer corresponds to a region in the original image. It is empirically observed that deeper layers tend to contain more ‘abstract’ information from the image. Intuitively, features describing different regions within the same image are likely to be semantically similar (e.g. different parts of an animal), and indeed the corresponding deep representations tend to be similar. Conversely, regions from two probably unrelated images (say, two images chosen at random) tend to be far from each other in the deep representation. This logic is commonly used in modern deep networks such as Szegedy et al. (2015) Lin et al. (2013) He et al. (2015), where a global average pooling is used to aggregate spatial features in the final layer used for classification.\nOur suggestion is that this property, often observed as a side effect of supervised applications, can be used as a desired objective when learning deep representations in an unsupervised task. Later, the resulting representation can be used, as typically done, as a starting point or a supervised learning task. We call this idea which we formalize below Spatial contrasting. The spatial contrasting criterion is similar to noise contrasting estimation Gutmann & Hyvärinen (2010) Mnih & Kavukcuoglu (2013), in trying to train a model by maximizing the expected probability on desired inputs, while minimizing it on contrasting sampled measurements."
    }, {
      "heading" : "4.1 FORMULATION",
      "text" : "We will concern ourselves with samples of images patches x̃(m) taken from an image x. Our convolutional network model, denoted by F (x), extracts spatial features f so that f (m) = F (x̃(m)) for an image patch x̃(m). We will also define P (f1|f2) as the probability for two features f1, f2 to occur together in the same image. We wish to optimize our model such that for two features representing patches taken from the same image x̃(1)i , x̃ (2) i ∈ xi for which f (1) i = F (x̃ (1) i ) and f (2) i = F (x̃ (2) i ), P (f (1) i |f (2) i ) will be maximized. This means that features from a patch taken from a specific image can effectively predict, under our model, features extracted from other patches in the same image. Conversely, we want our model to minimize P (fi|fj) for i, j being two patches taken from distinct images. Following the logic presented before, we will need to sample contrasting patch x̃(1)j from a different image xj such that P (f (1) i |f (2) i ) > P (f (1) j |f (2) i ), where f (1) j = F (x̃ (1) j ). In order to obtain contrasting samples, we use regions from two random images in the training set. We will use a distance ratio, described earlier\nin Eq. (2) for the supervised case, to represent the probability two feature vectors were taken from the same image. The resulting training loss for a pair of images will be defined as\nLSC(x1, x2) = − log e−‖f (1) 1 −f (2) 1 ‖2\ne−‖f (1) 1 −f (2) 1 ‖2 + e−‖f (1) 1 −f (1) 2 ‖2\n(3)\nEffectively minimizing a log-probability under the SoftMax measure. This formulation is portrayed in figure 4.1. Since we sample our contrasting sample from the same underlying distribution, we can evaluate this loss considering the image patch as both patch compared (anchor) and contrast symmetrically. The final loss will be the average between these estimations:\nL̂SC(x1, x2) = 1\n2 [LSC(x1, x2) + LSC(x2, x1)]"
    }, {
      "heading" : "4.2 METHOD",
      "text" : "Convolutional network are usually trained using SGD over mini-batch of samples, therefore we can extract patches and contrasting patches without changing the network architecture. Each image serves as both anchor and positive patches, for which the corresponding features should be closer, as well as contrasting samples for other images in that batch. For a batch of N images, two samples from each image are taken, and N2 different distance comparisons are made. The final loss is defined as the average distance ratio for all images in the batch:\nLSC({x}Ni=1) = 1\nN N∑ i=1 LSC(xi, {x}j 6=i) = − 1 N N∑ i=1 log e−‖f (1) i −f (2) i ‖2∑N j=1 e −‖f(1)i −f (2) j ‖2\n(4)\nSince the criterion is differentiable with respect to its inputs, it is fully compliant with standard methods for training convolutional network and specifically using backpropagation and gradient descent. Furthermore, SC can be applied to any layer in the network hierarchy. In fact, SC can be used at multiple layers within the same convolutional network. The spatial properties of the\nfeatures means that we can sample directly from feature space f̃ (m) ∈ f instead of from the original image. Therefore SC has a simple implementation which doesn’t require substation amount of computation. The complete algorithm for batch training is described in Algorithm (1). Similar to the batch normalization (BN) layer Ioffe & Szegedy (2015), a recent usage for batch statistics in neural networks, SC also uses the batch statistics. While BN normalize the input based on the batch statistics, SC sample from it. This can be viewed as a simple sampling from the space of possible features describing a patch of image.\nAlgorithm 1 Calculation the spatial contrasting loss Require: X = {x}Ni=1 # Training on batches of images\n# Get the spatial features for the whole batch of images # Size: N ×Wf ×Hf × C {f}Ni=1 ← ConvNet(X)\n# Sample spatial features and calculate embedded distance between all pairs of images for i = 1 to N do f̃ (1) i ← sample(fi)\nfor j = 1 to N do f̃ (2) j ← sample(fj) Dist(i, j)← ‖f̃ (1)i − f̃ (2) j ‖2\nend for end for\n# Calculate log SoftMax normalized distances di ← − log e\n−Dist(i,i)∑N k=1 e −Dist(i,k)\n# Spatial contrasting loss is the mean of distance ratios return 1N ∑N i=1 di"
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In this section we report empirical results showing that using SC loss as an unsupervised pretraining procedure can improve state-of-the-art performance on subsequent classification. We experimented with MNIST, CIFAR-10 and STL10 datasets. We used modified versions of well studied networks such as those of Lin et al. (2013) and Rasmus et al. (2015). A detailed description of our architecture can be found in 4.\nIn each one of the experiments, we used the spatial contrasting criterion to train the network on the unlabeled images. In each usage of SC criterion, patch features were sampled from the preceding layer in uniform. We note that spatial size of sampled patches ranged between datasets, where on STL10 and Cifar10 it covered about 30% of the image, MNIST required the use of larger patches covering almost the entire image.Training was done by using SGD with an initial learning rate of 0.1 that was decreased by a factor of 10 whenever the measured loss stopped decreasing. After convergence, we used the trained model as an initialization for a supervised training on the complete labeled dataset. The supervised training was done following the same regime, only starting with a lower initial learning rate of 0.01. We used mild data augmentations, such as small translations and horizontal mirroring. The datasets we used are:\n• STL10 (Coates et al. (2011)). This dataset consists of 100, 000 96× 96 colored, unlabeled images, together with another set of 5, 000 labeled training images and 8, 000 test images . The label space consists of 10 object classes.\n• Cifar10 (Krizhevsky & Hinton (2009)). The well known CIFAR-10 is an image classification benchmark dataset containing 50, 000 training images and 10, 000 test images. The\nAll experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.com/eladhoffer/ SpatialContrasting."
    }, {
      "heading" : "5.1 RESULTS ON STL10",
      "text" : "Since STL10 dataset is comprised of mostly unlabeled data, it is most suitable to highlight the benefits of the spatial contrasting criterion. The initial training was unsupervised, as described earlier, using the entire set of 105, 000 samples (union of the original unlabeled set and labeled training set). The representation outputted by the training, was used to initialize supervised training on the 5, 000 labeled images. Evaluation was done on a separate test set of 8, 000 samples. Comparing with state of the art results, we see an improvement of 7% in test accuracy over the best model by Zhao et al. (2015), setting the SC as best model at 81.3% test classification accuracy (see Table (1)). We note that the results of Dosovitskiy et al. (2014) are achieved with no fine-tuning over labeled examples, which may be unfair to this work. We also compare with the same network, but without SC initialization, which achieves a lower classification of 72.6%. This is an indication that indeed SC managed to leverage unlabeled examples to provide a better initialization point for the supervised model."
    }, {
      "heading" : "5.2 RESULTS ON CIFAR10",
      "text" : "For Cifar10 dataset, we use the same setting as Coates & Ng (2012) and Hui (2013) to test a model’s ability to learn from unlabeled images. Here, only 4, 000 samples out of 50, 000 are used with their label annotation, and the rest of the samples can be used only in an unsupervised manner. The final test accuracy is measured on the entire 10, 000 test set. In our experiments, we trained our model using SC criterion on the entire dataset, and then used only 400 labeled samples per class (for a total of 4000) in a supervised regime over the initialized network. The results are compared with previous efforts in Table (2). Using the SC criterion allowed an improvement of 6.8% over a non-initialized model, and achieved a final test accuracy of 79.2%. This is a competitive result with current state-of-the-art models."
    }, {
      "heading" : "5.3 RESULTS ON MNIST",
      "text" : "The MNIST dataset is very different in nature from the Cifar10 and STL10 datasets, we experimented earlier. The biggest difference, relevant to this work, is that spatial regions sampled from MNIST images usually provide very little, or no information. Thus, SC is much less suited for MNIST dataset, and was conjured to have little benefit. We still, however, experimented with initializing a model with SC criterion and continuing with a fully-supervised regime over all labeled\nexamples. We found again that this provided benefit over training the same network without preinitialization, improving results from 0.63% to 0.34% error on test set. As mentioned previously, the effective compared patches of MNIST covered almost the entire image area. This can be attributed to the fact that MNIST requires global features to differentiate between digits. The results, compared with previous attempts are included in Table (3)."
    }, {
      "heading" : "6 CONCLUSIONS AND FUTURE WORK",
      "text" : "In this work we presented spatial contrasting - a novel unsupervised criterion for training convolutional networks on unlabeled data. Its is based on comparison between spatial features sampled from a number of images. We’ve shown empirically that using spatial contrasting as a pretraining technique to initialize a ConvNet, can improve its performance on a subsequent supervised training. In cases where a lot of unlabeled data is available, such as the STL10 dataset, this translates to state-of-the-art classification accuracy in the final model.\nSince the spatial contrasting loss is a differentiable estimation that can be computed within a network parallel to supervised losses, in future work we plan to embed it as a semi-supervised model. This usage will allow to create models that can leverage both labeled an unlabeled data, and can be compared to similar semi-supervised models such as the ladder network Rasmus et al. (2015). It is is also apparent that contrasting can occur in dimensions other than the spatial, the most straightforward is the temporal dimension. This suggests that similar training procedure can be applied on segments of sequences to learn useful representation without explicit supervision."
    }, {
      "heading" : "7 APPENDIX",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Pn-net: Conjoined triple deep network for learning local image descriptors",
      "author" : [ "Vassileios Balntas", "Edward Johns", "Lilian Tang", "Krystian Mikolajczyk" ],
      "venue" : "arXiv preprint arXiv:1601.05030,",
      "citeRegEx" : "Balntas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Balntas et al\\.",
      "year" : 2016
    }, {
      "title" : "Signature verification using a siamese time delay neural network",
      "author" : [ "Jane Bromley", "James W Bentz", "Léon Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard Säckinger", "Roopak Shah" ],
      "venue" : "International Journal of Pattern Recognition and Artificial Intelligence,",
      "citeRegEx" : "Bromley et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1993
    }, {
      "title" : "Learning a similarity metric discriminatively, with application to face verification",
      "author" : [ "Sumit Chopra", "Raia Hadsell", "Yann LeCun" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Chopra et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning feature representations with k-means",
      "author" : [ "Adam Coates", "Andrew Y Ng" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "Coates and Ng.,? \\Q2012\\E",
      "shortCiteRegEx" : "Coates and Ng.",
      "year" : 2012
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "Adam Coates", "Andrew Y Ng", "Honglak Lee" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Coates et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2011
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clément Farabet" ],
      "venue" : "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Unsupervised visual representation learning by context prediction",
      "author" : [ "Carl Doersch", "Abhinav Gupta", "Alexei A Efros" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Doersch et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Doersch et al\\.",
      "year" : 2015
    }, {
      "title" : "Discriminative unsupervised feature learning with convolutional neural networks",
      "author" : [ "Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2014
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Michael Gutmann", "Aapo Hyvärinen" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Gutmann and Hyvärinen.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gutmann and Hyvärinen.",
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "To recognize shapes, first learn to generate images",
      "author" : [ "Geoffrey E Hinton" ],
      "venue" : "Progress in brain research,",
      "citeRegEx" : "Hinton.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hinton.",
      "year" : 2007
    }, {
      "title" : "Deep metric learning using triplet network",
      "author" : [ "Elad Hoffer", "Nir Ailon" ],
      "venue" : "In Similarity-Based Pattern Recognition,",
      "citeRegEx" : "Hoffer and Ailon.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hoffer and Ailon.",
      "year" : 2015
    }, {
      "title" : "Direct modeling of complex invariances for visual object features",
      "author" : [ "Ka Y Hui" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Hui.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hui.",
      "year" : 2013
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "What is the best multistage architecture for object recognition",
      "author" : [ "Kevin Jarrett", "Koray Kavukcuoglu", "Marc’Aurelio Ranzato", "Yann LeCun" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "Jarrett et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jarrett et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : "Computer Science Department,",
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning word embeddings efficiently with noise-contrastive estimation",
      "author" : [ "Andriy Mnih", "Koray Kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih and Kavukcuoglu.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih and Kavukcuoglu.",
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "An analysis of unsupervised pre-training in light of recent advances",
      "author" : [ "Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S Huang" ],
      "venue" : "arXiv preprint arXiv:1412.6597,",
      "citeRegEx" : "Paine et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Paine et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to segment object candidates",
      "author" : [ "Pedro O Pinheiro", "Ronan Collobert", "Piotr Dollar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Pinheiro et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Pinheiro et al\\.",
      "year" : 1981
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Semisupervised learning with ladder networks",
      "author" : [ "Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Cnn features off-theshelf: an astounding baseline for recognition",
      "author" : [ "Ali Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "Razavian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Razavian et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "arXiv preprint arXiv:1606.03498,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Facenet: A unified embedding for face recognition and clustering",
      "author" : [ "Florian Schroff", "Dmitry Kalenichenko", "James Philbin" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Schroff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schroff et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised and semi-supervised learning with categorical generative adversarial networks",
      "author" : [ "Jost Tobias Springenberg" ],
      "venue" : "In International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Springenberg.,? \\Q2016\\E",
      "shortCiteRegEx" : "Springenberg.",
      "year" : 2016
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Wan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep representation learning with target coding",
      "author" : [ "Shuo Yang", "Ping Luo", "Chen Change Loy", "Kenneth W Shum", "Xiaoou Tang" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Deconvolutional networks",
      "author" : [ "Matthew D Zeiler", "Dilip Krishnan", "Graham W Taylor", "Rob Fergus" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Zeiler et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zeiler et al\\.",
      "year" : 2010
    }, {
      "title" : "Stacked what-where auto-encoders",
      "author" : [ "Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun" ],
      "venue" : "arXiv preprint arXiv:1506.02351,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "For the past few years convolutional networks (ConvNets, CNNs) LeCun et al. (1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "(1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al.",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "(1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015) Pinheiro et al.",
      "startOffset" : 77,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "(1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015) Pinheiro et al. (2015) Razavian et al.",
      "startOffset" : 77,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "(1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015) Pinheiro et al. (2015) Razavian et al. (2014). A convolutional network is composed of multiple convolutional and pooling layers, followed by a fully-connected affine transformations.",
      "startOffset" : 77,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "The network was later fine-tuned in a supervised manner as displayed by Hinton (2007). Such unsupervised pre-training procedures were later abandoned, since they provided no apparent benefit over other initialization heuristics in more careful fully supervised training regimes.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 34,
      "context" : "The criterion used for training is the reconstruction loss, usually the mean squared error (MSE) between the original input and its reconstruction Zeiler et al. (2010)",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 31,
      "context" : "Ng (2011) regularized the latent representation to be sparse, Vincent et al. (2008) substituted the input with a noisy version thereof, requiring the model to denoise while reconstructing.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 31,
      "context" : "Ng (2011) regularized the latent representation to be sparse, Vincent et al. (2008) substituted the input with a noisy version thereof, requiring the model to denoise while reconstructing. Kingma et al. (2014) obtained very promising results with variational autoencoders (VAE).",
      "startOffset" : 62,
      "endOffset" : 210
    }, {
      "referenceID" : 34,
      "context" : "A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "Ladder networks by Rasmus et al. (2015) - use lateral connections and layer-wise cost functions to allow the higher levels of an autoencoder to focus on invariant abstract features.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "Exemplar Networks: The unsupervised method introduced byDosovitskiy et al. (2014) takes a different approach to this task and trains the network to discriminate between a set of pseudo-classes.",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "Context prediction Another method for unsupervised learning by context was introduced by Doersch et al. (2015). This method uses an auxiliary criterion of predicting the location of an image patch given another from the same image.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "Context prediction Another method for unsupervised learning by context was introduced by Doersch et al. (2015). This method uses an auxiliary criterion of predicting the location of an image patch given another from the same image. This is done by classification to 1 of 9 possible locations. Although the work of Doersch et al. (2015) and ours both use patches from an image to perform unsupervised learning, the methods are quite different.",
      "startOffset" : 89,
      "endOffset" : 336
    }, {
      "referenceID" : 8,
      "context" : "Adversarial Generative Models: This a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models uses a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "Adversarial Generative Models: This a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models uses a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015),this model can create useful latent representations for subsequent classification tasks.",
      "startOffset" : 108,
      "endOffset" : 577
    }, {
      "referenceID" : 19,
      "context" : "The most noteworthy case is the word2vec model introduced by Mikolov et al. (2013). When using this setting in language applications, a natural contrasting noise is a smooth approximation of the Unigram distribution.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "Another problem with recent approaches such as Rasmus et al. (2015); Zeiler et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : "Another problem with recent approaches such as Rasmus et al. (2015); Zeiler et al. (2010) is their need to extensively modify the original convolutional network model.",
      "startOffset" : 47,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "Using a comparative measure with neural network to learn embedding space was introduced in the “Siamese network” framework by Bromley et al. (1993) and later used in the works of Chopra et al.",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "Using a comparative measure with neural network to learn embedding space was introduced in the “Siamese network” framework by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face.",
      "startOffset" : 126,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "Using a comparative measure with neural network to learn embedding space was introduced in the “Siamese network” framework by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples.",
      "startOffset" : 126,
      "endOffset" : 480
    }, {
      "referenceID" : 0,
      "context" : "It was shown empirical by Balntas et al. (2016) to provide better feature embeddings than the margin based distance loss 1",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "This logic is commonly used in modern deep networks such as Szegedy et al. (2015) Lin et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : "This logic is commonly used in modern deep networks such as Szegedy et al. (2015) Lin et al. (2013) He et al.",
      "startOffset" : 60,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "(2013) He et al. (2015), where a global average pooling is used to aggregate spatial features in the final layer used for classification.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "(2013) He et al. (2015), where a global average pooling is used to aggregate spatial features in the final layer used for classification. Our suggestion is that this property, often observed as a side effect of supervised applications, can be used as a desired objective when learning deep representations in an unsupervised task. Later, the resulting representation can be used, as typically done, as a starting point or a supervised learning task. We call this idea which we formalize below Spatial contrasting. The spatial contrasting criterion is similar to noise contrasting estimation Gutmann & Hyvärinen (2010) Mnih & Kavukcuoglu (2013), in trying to train a model by maximizing the expected probability on desired inputs, while minimizing it on contrasting sampled measurements.",
      "startOffset" : 7,
      "endOffset" : 618
    }, {
      "referenceID" : 10,
      "context" : "(2013) He et al. (2015), where a global average pooling is used to aggregate spatial features in the final layer used for classification. Our suggestion is that this property, often observed as a side effect of supervised applications, can be used as a desired objective when learning deep representations in an unsupervised task. Later, the resulting representation can be used, as typically done, as a starting point or a supervised learning task. We call this idea which we formalize below Spatial contrasting. The spatial contrasting criterion is similar to noise contrasting estimation Gutmann & Hyvärinen (2010) Mnih & Kavukcuoglu (2013), in trying to train a model by maximizing the expected probability on desired inputs, while minimizing it on contrasting sampled measurements.",
      "startOffset" : 7,
      "endOffset" : 644
    }, {
      "referenceID" : 25,
      "context" : "(2013) and Rasmus et al. (2015). A detailed description of our architecture can be found in 4.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "• STL10 (Coates et al. (2011)).",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "• STL10 (Coates et al. (2011)). This dataset consists of 100, 000 96× 96 colored, unlabeled images, together with another set of 5, 000 labeled training images and 8, 000 test images . The label space consists of 10 object classes. • Cifar10 (Krizhevsky & Hinton (2009)).",
      "startOffset" : 9,
      "endOffset" : 270
    }, {
      "referenceID" : 21,
      "context" : "Table 1: State of the art results on STL-10 dataset Model STL-10 test accuracy Zero-bias Convnets - Paine et al. (2014) 70.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "Table 1: State of the art results on STL-10 dataset Model STL-10 test accuracy Zero-bias Convnets - Paine et al. (2014) 70.2% Triplet network - Hoffer & Ailon (2015) 70.",
      "startOffset" : 100,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "7% Exemplar Convnets - Dosovitskiy et al. (2014) 72.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "7% Exemplar Convnets - Dosovitskiy et al. (2014) 72.8% Target Coding - Yang et al. (2015) 73.",
      "startOffset" : 23,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "7% Exemplar Convnets - Dosovitskiy et al. (2014) 72.8% Target Coding - Yang et al. (2015) 73.15% Stacked what-where AE - Zhao et al. (2015) 74.",
      "startOffset" : 23,
      "endOffset" : 140
    }, {
      "referenceID" : 18,
      "context" : "• MNIST (LeCun et al. (1998)).",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : "Comparing with state of the art results, we see an improvement of 7% in test accuracy over the best model by Zhao et al. (2015), setting the SC as best model at 81.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "We note that the results of Dosovitskiy et al. (2014) are achieved with no fine-tuning over labeled examples, which may be unfair to this work.",
      "startOffset" : 28,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "For Cifar10 dataset, we use the same setting as Coates & Ng (2012) and Hui (2013) to test a model’s ability to learn from unlabeled images.",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "7% View-Invariant K-means - Hui (2013) 72.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "7% View-Invariant K-means - Hui (2013) 72.6% DCGAN - Radford et al. (2015) 73.",
      "startOffset" : 28,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "8% Exemplar Convnets - Dosovitskiy et al. (2014) 76.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "8% Exemplar Convnets - Dosovitskiy et al. (2014) 76.6% Ladder networks - Rasmus et al. (2015) 79.",
      "startOffset" : 23,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "8% Exemplar Convnets - Dosovitskiy et al. (2014) 76.6% Ladder networks - Rasmus et al. (2015) 79.6% Conv-CatGan Springenberg (2016) 80.",
      "startOffset" : 23,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "8% Exemplar Convnets - Dosovitskiy et al. (2014) 76.6% Ladder networks - Rasmus et al. (2015) 79.6% Conv-CatGan Springenberg (2016) 80.42% (± 0.58) ImprovedGan Salimans et al. (2016) 81.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 32,
      "context" : "Table 3: results on MNIST dataset Model MNIST test error Stacked what-where AE - Zhao et al. (2015) 0.",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 32,
      "context" : "Table 3: results on MNIST dataset Model MNIST test error Stacked what-where AE - Zhao et al. (2015) 0.71% Triplet network - Hoffer & Ailon (2015) 0.",
      "startOffset" : 81,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "56% Jarrett et al. (2009) 0.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "56% Jarrett et al. (2009) 0.53% Ladder networks - Rasmus et al. (2015) 0.",
      "startOffset" : 4,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "56% Jarrett et al. (2009) 0.53% Ladder networks - Rasmus et al. (2015) 0.36% DropConnect - Wan et al. (2013) 0.",
      "startOffset" : 4,
      "endOffset" : 109
    } ],
    "year" : 2017,
    "abstractText" : "Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and optimized using standard techniques such as SGD and backpropagation, thus complementing supervised methods.",
    "creator" : "LaTeX with hyperref package"
  }
}