{
  "name" : "448.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEEP INFORMATION PROPAGATION",
    "authors" : [ "Samuel S. Schoenholz", "Justin Gilmer", "Surya Ganguli" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep neural network architectures have become ubiquitous in machine learning. The success of deep networks is due to the fact that they are highly expressive (Montufar et al., 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al., 2015). Consequently, developments in machine learning often accompany improvements in our ability to train increasingly deep networks. Despite this, designing novel network architectures is frequently equal parts art and science. This is, in part, because a general theory for neural networks that might inform design decisions has lagged behind the feverish pace of design.\nA pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth. Central to their approach was the consideration of networks after random initialization, whose weights and biases were i.i.d. Gaussian distributed. In particular the paper by Poole et al. (2016) developed a “mean field” formalism for treating wide, untrained, neural networks. They showed that these mean field networks exhibit an order-to-chaos transition as a function of the weight and bias variances. Notably the mean field formalism is not closely tied to a specific choice of activation function or loss.\nIn this paper, we demonstrate the existence of several characteristic “depth” scales that emerge naturally and control signal propagation in these random networks. We then show that one of these depth scales, ξc, diverges at the boundary between order and chaos. This result is insensitive to many architectural decisions (such as choice of activation function) and will generically be true at any order-to-chaos transition. We then extend these results to include dropout and we show that even small amounts of dropout destroys the order-to-chaos critical point and consequently removes the divergence in ξc. Together these results bound the depth to which signal may propagate through random neural networks.\nWe then develop a corresponding mean field model for gradients and we show that a duality exists between the forward propagation of signals and the backpropagation of gradients. The ordered and chaotic phases that Poole et al. (2016) identified correspond to regions of vanishing and exploding gradients, respectively. We demonstrate the validity of this mean field theory by computing gradients of random networks on MNIST. This provides a formal explanation of the ‘vanishing gradients’\n∗Work done as a member of the Google Brain Residency program (g.co/brainresidency)\nphenomenon that has long been observed in neural networks (Bengio et al., 1993). We continue to show that the covariance between two gradients is controlled by the same depth scale that limits correlated signal propagation in the forward direction.\nFinally, we hypothesize that a necessary condition for a random neural network to be trainable is that information should be able to pass through it. Thus, the depth-scales identified here bound the set of hyperparameters that will lead to successful training. To test this ansatz we train ensembles of deep, fully connected, feed-forward neural networks of varying depth on MNIST and CIFAR10, with and without dropout. Our results confirm that neural networks are trainable precisely when their depth is not much larger than ξc. This result is dataset independent and is, therefore, a universal function of network architecture.\nA corollary of these result is that asymptotically deep neural networks should be trainable provided they are initialized sufficiently close to the order-to-chaos transition. The notion of “edge of chaos” initialization has been explored previously. Such investigations have been both direct as in Bertschinger et al. (2005); Glorot & Bengio (2010) or indirect, through initialization schemes that favor deep signal propagation such as batch normalization (Ioffe & Szegedy, 2015), orthogonal matrix initialization (Saxe et al., 2014), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al., 2015). The novelty of the work presented here is two-fold. First, our framework predicts the depth at which networks may be trained even far from the order-to-chaos transition. While a skeptic might ask when it would be profitable to initialize a network far from criticality, we respond by noting that there are architectures (such as neural networks with dropout) where no critical point exists and so this more general framework is needed. Second, our work provides a formal, as opposed to intuitive, explanation for why very deep networks can only be trained near the edge of chaos."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "We begin by recapitulating the mean-field formalism developed in Poole et al. (2016). Consider a fully-connected, untrained, feed-forward, neural network of depth L with layer width Nl and some nonlinearity φ : R → R. Since this is an untrained neural network we suppose that its weights and biases are respectively i.i.d. as W lij ∼ N(0, σ2w/Nl) and bli ∼ N(0, σ2b ). Notationally we set zli to be the pre-activations of the lth layer and yl+1i to be the activations of that layer. Finally, we take the input to the network to be y0i = xi. The propagation of a signal through the network is described by the pair of equations,\nzli = ∑ j W lijy l j + b l i y l+1 i = φ(z l i). (1)\nSince the weights and biases are randomly distributed, these equations define a probability distribution on the activations and pre-activations over an ensemble of untrained neural networks. The “mean-field” approximation is then to replace zli by a Gaussian whose first two moments match those of zli. For the remainder of the paper we will take the mean field approximation as given.\nConsider first the evolution of a single input, xi;a, as it evolves through the network (as quantified by yli;a and z l i;a). Since the weights and biases are independent with zero mean, the first two moments of the pre-activations in the same layer will be,\nE[zli;a] = 0 E[zli;azlj;a] = qlaaδij (2)\nwhere δij is the Kronecker delta. Here qlaa is the variance of the pre-activations in the lth layer due to an input xi;a and it is described by the recursion relation,\nqlaa = σ 2 w\n∫ Dzφ2 (√ ql−1aa z ) + σ2b (3)\nwhere ∫ Dz = 1√\n2π\n∫ dze− 1 2 z 2\nis the measure for a standard Gaussian distribution. Together these equations completely describe the evolution of a single input through a mean field neural network. For any choice of σ2w and σ 2 b with bounded φ, eq. 3 has a fixed point at q ∗ = liml→∞ qlaa.\nThe propagation of a pair of signals, x0i;a and x 0 i;b, through this network can be understood similarly. Here the mean pre-activations are trivially the same as in the single-input case. The independence\nof the weights and biases implies that the covariance between different pre-activations in the same layer will be given by, E[zli;azlj;b] = qlabδij . The covariance, qlab, will be given by the recurrence relation,\nqlab = σ 2 w ∫ Dz1Dz2φ(u1)φ(u2) + σ2b (4)\nwhere u1 = √ ql−1aa z1 and u2 = √ ql−1bb ( cl−1ab z1 + √ 1− (cl−1ab )2z2 ) , with clab = q l ab/ √ qlaaq l bb,\nare Gaussian approximations to the pre-activations in the preceding layer with the correct covariance matrix. Moreover clab is the correlation between the two inputs after l layers.\nExamining eq. 4 it is clear that c∗ = 1 is a fixed point of the recurrence relation. To determine whether or not the c∗ = 1 is an attractive fixed point the quantity,\nχ1 = ∂clab ∂cl−1ab = σ2w\n∫ Dz [ φ′ (√ q∗z )]2\n(5)\nis introduced. Poole et al. (2016) note that the c∗ = 1 fixed point is stable if χ1 < 1 and is unstable otherwise. Thus, χ1 = 1 represents a critical line separating an ordered phase (in which c∗ = 1 and all inputs end up asymptotically correlated) and a chaotic phase (in which c∗ < 1 and all inputs end up asymptotically decorrelated). For the case of φ = tanh, the phase diagram in fig. 1 (a) is observed."
    }, {
      "heading" : "3 ASYMPTOTIC EXPANSIONS AND DEPTH SCALES",
      "text" : "Our first contribution is to demonstrate the existence of two depth-scales that arise naturally within the framework of mean field neural networks. Motivating the existence of these depth-scales, we iterate eq. 3 and 4 until convergence for many values of σ2w between 0.1 and 3.0 and with σ 2 b = 0.05 starting with q0aa = q 0 bb = 0.8 and c 0 ab = 0.6. We see, in fig. 1 (b) and (c), that the manner in which both qlaa approaches q ∗ and clab approaches c\n∗ is exponential over many orders of magnitude. We therefore anticipate that asymptotically |qlaa − q∗| ∼ e−l/ξq and |clab − c∗| ∼ e−l/ξc for sufficiently large l. Here, ξq and ξc define depth-scales over which information may propagate about the magnitude of a single input and the correlation between two inputs respectively.\nWe will presently prove that qlaa and c l ab are asymptotically exponential. In both cases we will use the same fundamental strategy wherein we expand one of the recurrence relations (either eq. 3 or eq. 4) about its fixed point to get an approximate “asymptotic” recurrence relation. We find that this asymptotic recurrence relation in turn implies exponential decay towards the fixed point over a depth-scale, ξx.\nWe first analyze eq. 3 and identify a depth-scale at which information about a single input may propagate. Let qlaa = q ∗ + l. By construction so long as liml→∞ qlaa = q ∗ exists it follows that\nl → 0 as l→∞. Eq. 3 may be expanded to lowest order in l to arrive at an asymptotic recurrence relation (see Appendix 7.1),\nl+1 = l [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O ( ( l)2 ) . (6)\nNotably, the term multiplying l is a constant. It follows that for large l the asymptotic recurrence relation has an exponential solution, l ∼ e−l/ξq , with ξq given by\nξ−1q = − log [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] . (7)\nThis establishes ξq as a depth scale that controls how deep information from a single input may penetrate into a random neural network.\nNext, we consider eq. 4. Using a similar argument (detailed in Appendix 7.2) we can expand about clab = c ∗ + l to find an asymptotic recurrence relation,\nl+1 = l [ σ2w ∫ Dz1Dz2φ′(u∗1)φ′(u∗2) ] +O(( l)2). (8)\nHere u∗1 = √ q∗z1 and u∗2 = √ q∗(c∗z1 + √ 1− (c∗)2z2). Thus, once again, we expect that for large l this recurrence will have an exponential solution, l ∼ e−l/ξc , with ξc given by\nξ−1c = − log [ σ2w ∫ Dz1Dz2φ′(u∗1)φ′(u∗2) ] . (9)\nIn the ordered phase c∗ = 1 and so ξ−1c = − logχ1. Since the transition between order and chaos occurs when χ1 = 1 it follows that ξc diverges at any order-to-chaos transition so long as q∗ and c∗ exist.\nThese results can be investigated intuitively by plotting cl+1ab vs c l ab in fig. 2 (a). In the ordered phase there is only a single fixed point, clab = 1. In the chaotic regime we see that a second fixed point develops and the clab = 1 point becomes unstable. We see that the linearization about the fixed points becomes significantly closer to the trivial map near the order-to-chaos transition.\nTo test these claims we measure ξq and ξc directly by iterating the recurrence relations for qlaa and clab as before with q 0 aa = q 0 bb = 0.8 and c 0 ab = 0.6. In this case we consider values of σ 2 w between\n0.1 and 3.0 and σ2b between 0.01 and 0.3. For each hyperparameter settings we fit the resulting residuals, |qlaa − q∗| and |clab − c∗|, to exponential functions and infer the depth-scale. We then compare this measured depth-scale to that predicted by the asymptotic expansion. The result of this measurement is shown in fig. 2. In general we see that the agreement is quite good. As expected we see that ξc diverges at the critical point.\nAs observed in Poole et al. (2016) we see that the depth scale for the propagation of information in a single input, ξq , is consistently finite and significantly shorter than ξc. To understand why this is the case consider eq. 6 and note that for tanh nonlinearities the second term is always negative. Thus, even as χ1 approaches 1 we expect χ1 + σ2w ∫ Dzφ′′(√q∗z)φ(√q∗z) to be substantially smaller than 1."
    }, {
      "heading" : "3.1 DROPOUT",
      "text" : "The mean field formalism can be extended to include dropout. The main contribution here will be to argue that even infinitesimal amounts of dropout destroys the mean field critical point, and therefore limits the trainable network depth. In the presence of dropout the propagation equation, eq. 1, becomes,\nzli = 1\nρ ∑ j W lijp l jy l j + b l i (10)\nwhere pj ∼ Bernoulli(ρ) and ρ is the dropout rate. As is typically the case we have re-scaled the sum by ρ−1 so that the mean of the pre-activation is invariant with respect to our choice of dropout rate.\nFollowing a similar procedure to the original mean field calculation consider the fate of two inputs, x0i;a and x 0 i;b, as they are propagated through such a random network. We take the dropout masks to be chosen independently for the two inputs mimicking the manner in which dropout is employed in practice. With dropout the diagonal term in the covariance matrix will be (see Appendix 7.3),\nq̄laa = σ2w ρ\n∫ Dzφ2 (√ q̄l−1aa z ) + σ2b . (11)\nThe variance of a single input with dropout will therefore propagate in an identical fashion to the vanilla case with a re-scaling σ2w → σ2w/ρ. Intuitively, this result implies that, for the case of a single input, the presence of dropout simply increases the effective variance of the weights.\nComputing the off-diagonal term of the covariance matrix similarly (see Appendix 7.4),\nq̄lab = σ 2 w ∫ Dz1Dz2φ(ū1)φ(ū2) + σ2b (12)\nwith ū1, ū2, and c̄lab defined by analogy to the mean field equations without dropout. Here, unlike in the case of a single input, the recurrence relation is identical to the recurrence relation without dropout. To see that c̄∗ = 1 is no longer a fixed point of these dynamics consider what happens to eq. 12 when we input c̄l = 1. For simplicity, we leverage the short range of ξq to replace q̄laa = q̄ l bb = q̄ ∗. We find (see Appendix 7.5),\nc̄l+1ab = 1− 1− ρ ρq̄∗ σ2w\n∫ Dzφ2 (√ q̄∗z ) . (13)\nThe second term is positive for any ρ < 1. This implies that if c̄lab = 1 for any l then c̄ l+1 ab < 1. Thus, c∗ = 1 is not a fixed point of eq. 12 for any ρ < 1. Since eq. 12 is identical in form to eq. 4 it follows that the depth scale for signal propagation with dropout will likewise be given by eq. 9 with the substitutions q∗ → q̄∗ and c∗ → c̄∗ computed using eq. 11 and eq. 12 respectively. Importantly, since there is no longer a sharp critical point with dropout we do not expect a diverging depth scale.\nAs in networks without dropout we plot, in fig. 3 (a), the iterative map c̄l+1ab as a function of c̄ l ab. Most significantly, we see that the c̄lab = 1 is no longer a fixed point of the dynamics. Instead, as the dropout rate increases c̄lab gets mapped to decreasing values and the fixed point monotonically decreases.\nTo test these results we plot in fig. 3 (b) the asymptotic correlation, c∗, as a function of σ2w for different values of dropout from ρ = 0.8 to ρ = 1.0. As expected, we see that for all ρ < 1 there is no sharp transition between c∗ = 1 and c∗ < 1. Moreover as the dropout rate increases the correlation c∗ monotonically decreases. Intuitively this makes sense. Identical inputs passed through two different dropout masks will become increasingly dissimilar as the dropout rate increases. In fig. 3 (c) we show the depth scale, ξc, as a function of σ2w for the same range of dropout probabilities. We find that, as predicted, the depth of signal propagation with dropout is drastically reduced and, importantly, there is no longer a divergence in ξc. Increasing the dropout rate continues to decrease the correlation depth for constant σ2w."
    }, {
      "heading" : "4 GRADIENT BACKPROPAGATION",
      "text" : "There is a duality between the forward propagation of signals and the backpropagation of gradients. To elucidate this connection consider the backpropagation equations given a loss E,\n∂E\n∂W lij = δliφ(z l−1 j ) δ l i = φ ′(zli) ∑ j δl+1j W l+1 ji (14)\nwith the identification δli = ∂E/∂z l i. Within mean field theory, it is clear that the scale of fluctuations of the gradient of weights in a layer will be proportional to E[(δli)2] (see appendix 7.6). In contrast to the pre-activations in forward propagation (eq. 1), the δli will typically not be Gaussian distributed even in the large layer width limit.\nNonetheless, we can work out a recurrence relation for the variance of the error, q̃ laa = E[(δli)2], leveraging the Gaussian ansatz on the pre-activations. In order to do this, however, we must first make an additional approximation that the weights used during forward propagation are drawn independently from the weights used in backpropagation. This approximation is similar in spirit to the vanilla mean field approximation and is reminiscent of work on feedback alignment (Lillicrap et al., 2014). With this in mind we arrive at the recurrence (see appendix 7.7),\nq̃ laa = q̃ l+1 aa Nl+1 Nl χ1. (15)\nThe presence of χ1 in the above equation should perhaps not be surprising. In Poole et al. (2016) they show that χ1 is intimately related to the tangent space of a given layer in mean field neural\nnetworks. We note that the backpropagation recurrence features an explicit dependence on the ratio of widths of adjacent layers of the network, Nl+1/Nl. Here we will consider exclusively constant width networks where this factor is unity. For a discussion of the case of unequal layer widths see Glorot & Bengio (2010).\nSince χ1 depends only on the asymptotic q∗ it follows that for constant width networks we expect eq. 15 to again have an exponential solution with,\nq̃ laa = q̃ L aae −(L−l)/ξ∇ ξ−1 ∇ = − logχ1. (16)\nNote that here ξ−1 ∇ = − logχ1 both above and below the transition. It follows that ξ∇ can be both positive and negative. We conclude that there should be three distinct regimes for the gradients.\n1. In the ordered phase, χ1 < 1 and so ξ∇ > 0. We therefore expect gradients to vanish over a depth |ξ∇ |. 2. At criticality, χ1 → 1 and so ξ∇ →∞. Here gradients should be stable regardless of depth. 3. In the chaotic phase, χ1 > 1 and so ξ∇ < 0. It follows that in this regime gradients should\nexplode over a depth |ξ∇ |.\nIntuitively these three regimes make sense. To see this, recall that perturbations to a weight in layer l can alternatively be viewed as perturbations to the pre-activations in the same layer. In the ordered phase both the perturbed signal and the unperturbed signal will be asymptotically mapped to the same point and the derivative will be small. In the chaotic phase the perturbed and unperturbed signals will become asymptotically decorrelated and the gradient will be large.\nTo investigate these predictions we construct deep random networks of depth L = 240 and layerwidth Nl = 300. We then consider the cross-entropy loss of these networks on MNIST. In fig. 4 (a) we plot the layer-by-layer 2-norm of the gradient, ||∇W labE|| 2 2, as a function of layer, l, for different values of σ2w. We see that ||∇W labE|| 2 2 behaves exponentially over many orders of magnitude. Moreover, we see that the gradient vanishes in the ordered phase and explodes in the chaotic phase. We test the quantitative predictions of eq. 16 in fig. 4 (b) where we compare |ξ∇ | as predicted from theory with the measured depth-scale constructed from exponential fits to the gradient data. Here we see good quantitative agreement between the theoretical predictions from mean field random networks and experimentally realized networks. Together these results suggest that the approximations on the backpropagation equations were representative of deep, wide, random networks.\nFinally, we show that the depth scale for correlated signal propagation likewise controls the depth at which information stored in the covariance between gradients can survive. The existence of\nconsistent gradients across similar samples from a training set ought to be especially important for determining whether or not a given neural network architecture can be trained. To establish this depth-scale first note (see Appendix 7.8) that the covariance between gradients of two different inputs, xi;1 and xi;2, will be proportional to (∇W lijEa) · (∇W lijEb) ∼ E[δ l i;aδ l i;b] = q̃ l ab where Ea is the loss evaluated on xi;a and δi;a = ∂Ea/∂zli;a are appropriately defined errors.\nIt can be shown (see Appendix 7.9) that q̃ lab features the recurrence relation,\nq̃ lab = q̃ l+1 ab Nl+1 Nl+2 σ2w\n∫ Dz1Dz2φ′(u1)φ′(u2) (17)\nwhere u1 and u2 are defined similarly as for the forward pass. Expanding asymptotically it is clear that to zeroth order in l, q̃lab will have an exponential solution with q̃ l ab = q̃ L abe −(L−l)/ξc with ξc as defined in the forward pass."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS",
      "text" : "Taken together, the results of this paper lead us to the following hypothesis: a necessary condition for a random network to be trained is that information about the inputs should be able to propagate forward through the network, and information about the gradients should be able to propagate backwards through the network. The preceding analysis shows that networks will have this property precisely when the network depth, L, is not much larger than the depth-scale ξc. This criterion is data independent and therefore offers a “universal” constraint on the hyperparameters that depends on network architecture alone. We now explore this relationship between depth of signal propagation and network trainability empirically.\nTo investigate this prediction, we consider random networks of depth 10 ≤ L ≤ 300 and 1 ≤ σ2w ≤ 4 with σ2b = 0.05. We train these networks using Stochastic Gradient Descent (SGD) and RMSProp\non MNIST and CIFAR10. We use a learning rate of 10−3 for SGD when L . 200, 10−4 for larger L, and 10−5 for RMSProp. These learning rates were selected by grid search between 10−6 and 10−2 in exponentially spaced steps of size 10. We note that the depth dependence of learning rate was explored in detail in Saxe et al. (2014). In fig. 5 (a)-(d) we color in red the training accuracy that neural networks achieved as a function of σ2w and L for different datasets, training time, and choice of minimizer (see Appendix 7.10 for more comparisons). In all cases the neural networks over-fit the data to give a training accuracy of 100% and test accuracies of 98% on MNIST and 55% on CIFAR10. We emphasize that the purpose of this study is to demonstrate trainability as opposed to optimizing test accuracy.\nWe now make the connection between the depth scale, ξc, and the maximum trainable depth more precise. Given the arguments in the preceding sections we note that if L = nξc then signal through the network will be attenuated by a factor of en. To understand how much signal can be lost while still allowing for training, we overlay in fig. 5 (a) curves corresponding to nξc from n = 1 to 6. We find that networks appear to be trainable when L . 6ξc. It would be interesting to understand why this is the case.\nMotivated by this argument in fig. 5 (b)-(d) in white, dashed, overlay we plot twice the predicted depth scale, 6ξc. There is clearly a relationship between the depth of correlated signal propagation and whether or not these networks are trainable. Networks closer to their critical point appear to train more quickly than those further away. Moreover, this relationship has no obvious dependence on dataset, duration of training, or minimizer. We therefore conclude that these bounds on trainable hyperparameters are universal. This in turn implies that to train increasingly deep networks, one must generically be ever closer to criticality.\nNext we consider the effect of dropout. As we showed earlier, even infinitesimal amounts of dropout disrupt the order-to-chaos phase transition and cause the depth scale to become finite. However, since the effect of a single dropout mask is to simply re-scale the weight variance by σ2w → σ2w/ρ, the gradient magnitude will be stable near criticality, while the input and gradient correlations will not be. This therefore offers a unique opportunity to test whether the relevant depth-scale is |1/ logχ1| or ξc.\nIn fig. 6 we repeat the same experimental setup as above on MNIST with dropout rates ρ = 0.99, 0.98, and 0.94. We observe, first and foremost, that even extremely modest amounts of dropout limit the maximum trainable depth to about L = 100. We additionally notice that the depth-scale, ξc, predicts the trainable region accurately for varying amounts of dropout."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "In this paper we have elucidated the existence of several depth-scales that control signal propagation in random neural networks. Furthermore, we have shown that the degree to which a neural network can be trained depends crucially on its ability to propagate information about inputs and gradients\nthrough its full depth. At the transition between order and chaos, information stored in the correlation between inputs can propagate infinitely far through these random networks. This in turn implies that extremely deep neural networks may be trained sufficiently close to criticality. However, our contribution goes beyond advocating for hyperparameter selection that brings random networks to be nearly critical. Instead, we offer a general purpose framework that predicts, at the level of mean field theory, which hyperparameters should allow a network to be trained. This is especially relevant when analyzing schemes like dropout where there is no critical point and which therefore imply an upper bound on trainable network depth.\nAn alternative perspective as to why information stored in the covariance between inputs is crucial for training can be understood by appealing to the correspondence between infinitely wide Bayesian neural networks and Gaussian Processes (Neal, 2012). In particular the covariance, qlab, is intimately related to the kernel of the induced Gaussian Process. It follows that cases in which signal stored in the covariance between inputs may propagate through the network correspond precisely to situations in which the associated Gaussian Process is well defined.\nOur work suggests that it may be fruitful to investigate pre-training schemes that attempt to perturb the weights of a neural network to favor information flow through the network. In principle this could be accomplished through a layer-by-layer local criterion for information flow or by selecting the mean and variance in schemes like batch normalization to maximize the covariance depth-scale.\nThese results suggest that theoretical work on random neural networks can be used to inform practical architectural decisions. However, there is still much work to be done. For instance, the framework developed here does not apply to unbounded activations, such as rectified linear units, where it can be shown that there are phases in which eq. 3 does not have a fixed point. Additionally, the analysis here applies directly only to fully connected feed-forward networks, and will need to be extended to architectures with structured weight matrices such as convolutional networks.\nWe close by noting that in physics it has long been known that, through renormalization, the behavior of systems near critical points can control their behavior even far from the idealized critical case. We therefore make the somewhat bold hypothesis that a broad class of neural network topologies will be controlled by the fully-connected mean field critical point."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Ben Poole, Jeffrey Pennington, Maithra Raghu, and George Dahl for useful discussions. We are additionally grateful to RocketAI for introducing us to Temporally Recurrent Online Learning and two-dimensional time."
    }, {
      "heading" : "7 APPENDIX",
      "text" : "Here we present derivations of results from throughout the paper."
    }, {
      "heading" : "7.1 SINGLE INPUT DEPTH-SCALE",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "Consider the recurrence relation for the variance of a single input,\nqlaa = σ 2 w\n∫ Dzφ2 (√ ql−1aa z ) + σ2b (18)\nand a fixed point of the dynamics, q∗. qlaa can be expanded about the fixed point to yield the asymptotic recurrence relation,\nl+1 = l [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O ( ( l)2 ) . (19)\nDerivation:\nWe begin by first expanding to order l,\nq∗ + l+1 = σ2w\n∫ Dz [ φ (√ q∗ + lz )]2\n+ σ2b (20)\n≈ σ2w ∫ Dz [ φ (√ q∗z + 1\n2 lz√ q∗\n)]2 + σ2b (21)\n≈ σ2w ∫ Dz [ φ (√ q∗z ) + 1\n2 lz√ q∗ φ′ (√ q∗z )]2 + σ2b +O(( l)2) (22)\n≈ σ2w ∫ Dzφ2 (√ q∗z ) + σ2b + l σ\n2 w√ q∗\n∫ Dzzφ (√ q∗z ) φ′ (√ q∗z ) +O(( l)2) (23)\n≈ q∗ + l σ 2 w√ q∗\n∫ Dzzφ(√q∗z)φ′ (√ q∗z ) +O(( l)2). (24)\nWe therefore arrive at the approximate reccurence relation,\nl+1 = l σ2w√ q∗\n∫ Dzzφ(√q∗z)φ′ (√ q∗z ) +O(( l)2). (25)\nUsing the identity, ∫ Dzzf(z) = ∫ Dzf ′(z) we can rewrite this asymptotic recurrence relation as,\nl+1 = l [ σ2w ∫ Dz [ φ′ (√ q∗z )]2 + σ2w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O(( l)2) (26)\n= l [ χ1 + σ 2 w ∫ Dzφ′′ (√ q∗z ) φ (√ q∗z )] +O(( l)2) (27)\nas required."
    }, {
      "heading" : "7.2 TWO INPUT DEPTH-SCALE",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "Consider the recurrence relation for the co-variance of two input,\nqlab = σ 2 w ∫ Dz1Dz2φ(u1)φ(u2) + σ2b , (28)\na correlation between the inputs, clab = q l ab/ √ qlaaq l bb, and a fixed point of the dynamics, c\n∗. clab can be expanded about the fixed point to yield the asymptotic recurrence relation,\nl+1 = l [ σ2w ∫ Dz1Dz2φ′(u1)φ′(u2) ] +O ( ( l)2 ) . (29)"
    }, {
      "heading" : "Derivation:",
      "text" : "Since the relaxation of qlaa and q l bb to q ∗ occurs much more quickly than the convergence of qlab we approximate qlaa = q l bb = q\n∗ as in Poole et al. (2016). We therefore consider the perturbation qlab/q ∗ = clab = c ∗ + l. It follows that we may make the approximation,\nul2 = √ q∗ ( clabz1 + √ 1− (clab)2z2 ) (30)\n≈ √q∗ ( c∗z1 + √ 1− (c∗)2 − 2c∗ lz2 ) + √ q∗ lz1 +O( 2) (31)\n(32)\nWe now consider the case where c∗ < 1 and c∗ = 1 separately; we will later show that these two results agree with one another. First we consider the case where c∗ < 1 in which case we may safely expand the above equation to get,\nul2 = √ q∗ ( c∗z1 + √ 1− (c∗)2z2 ) + √ q∗ l ( z1 −\nc∗√ 1− (c∗)2 z2\n) +O( 2). (33)\nThis allows us to in turn approximate the recurrence relation,\ncl+1ab = σ2w q∗\n∫ Dz1Dz2φ(u∗1)φ(ul2) + σ2b (34)\n≈ σ 2 w\nq∗\n∫ Dz1Dz2φ(u∗1) [ φ(u∗2) + √ q∗ l ( z1 −\nc∗√ 1− (c∗)2 z2\n) φ′(u∗2) ] + σ2b +O( 2)\n(35) = c∗ + σ2w√ q∗ l ∫ Dz1Dz2 ( z1 − c∗√ 1− (c∗)2 z2 ) φ(u∗1)φ ′(u∗2) (36)\n= c∗ + σ2w√ q∗ l\n[∫ Dz1Dz2z1φ(u∗1)φ′(u∗2)−\nc∗√ 1− (c∗)2\n∫ Dz1Dz2z2φ(u∗1)φ′(u∗2) ] (37)\n= c∗ + σ2w l [∫ Dz1Dz2(φ′(u∗1)φ′(u∗2) + c∗φ(u∗1)φ′′(u∗2))− c∗ ∫ Dz1Dz2φ(u∗1)φ′′(u∗2) ] (38)\n= c∗ + σ2w l ∫ Dz1Dz2φ′(u∗1)φ′(u∗2). (39)\nwhere u∗1 and u ∗ 2 are appropriately defined asymptotic random variables. This leads to the asymptotic recurrence relation,\nl+1 = σ2w l ∫ Dz1Dz2φ′(u∗1)φ′(u∗2) (40)\nas required.\nWe now consider the case where c∗ = 1 and clab = 1 − l. In this case the expansion of ul2 will become,\nul2 = √ q∗z1 + √ 2q∗ lz2 − √ q∗ lz1 +O( 3/2) (41)\nand so the lowest order correction is of orderO( √ l) as opposed toO( l). As usual we now expand the recurrence relation, noting that u∗2 = u ∗ 1 is independent of z2 when c ∗ = 1 to find,\ncl+1ab = σ2w q∗\n∫ Dz1Dz2φ(u∗1)φ(ul2) + σ2b (42)\n≈ σ 2 w\nq∗\n∫ Dz1Dz2φ(u∗1) [ φ(u∗2) + (√ 2q∗ lz2 − √ q∗ lz1 ) φ′(u∗2) + q ∗ lz22φ ′′(u∗2) ] + σ2b\n(43)\n= c∗ + σ2w l\n∫ Dzφ(√q∗z) [ φ′′( √ q∗z)− 1√\nq∗ zφ′( √ q∗z)\n] (44)\n= c∗ + σ2w l\n[∫ Dzφ(√q∗z)φ′′(√q∗z)− 1√\nq∗\n∫ Dzzφ(√q∗z)φ′(√q∗z) ] (45)\n= c∗ − σ2w l ∫ Dz [ φ′( √ q∗z) ]2 (46)\nIt follows that the asymptotic recurrence relation in this case will be, l+1 = − lσ2w ∫ Dz [ φ′( √ q∗z) ]2 = − lχ1. (47)\nwhere χ1 is the stability condition for the ordered phase. We note that although the approximations were somewhat different the asymptotic recurrence relation for c∗ < 1 reduces eq. 47 result for c∗ = 1. We may therefore use 4 for all c∗."
    }, {
      "heading" : "7.3 VARIANCE OF AN INPUT WITH DROPOUT",
      "text" : "Result:\nIn the presence of dropout with rate ρ, the variance of a single input as it is passed through the network is described by the recurrence relation,\nq̄laa = σ2w ρ\n∫ Dzφ2 (√ q̄l−1aa z ) + σ2b . (48)"
    }, {
      "heading" : "Derivation:",
      "text" : "Recall that the recurrence relation for the pre-activations is given by,\nzli = 1\nρ ∑ j W lijp l jy l j + b l i (49)\nwhere plj ∼ Bernoulli(ρ). It follows that the variance will be given by,\nq̄laa = E[(zli)2] (50)\n= 1\nρ2 ∑ j E[(W lij)2]E[(ρlj)2]E[(ylj)2] + E[(bli)2] (51)\n= σ2w ρ\n∫ Dzφ2 (√ q̄l−1aa z ) + σ2b . (52)\nwhere we have used the fact that E[(plj)2] = ρ."
    }, {
      "heading" : "7.4 COVARIANCE OF TWO INPUTS WITH DROPOUT",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "The co-variance between two signals, zli;a and z l i;b, with separate i.i.d. dropout masks p l i;a and p l i;b is given by,\nq̄lab = σ 2 w ∫ Dz1Dz2φ(ū1)φ(ū2) + σ2b . (53)\nwhere, in analogy to eq. 4, ū1 = √ q̄laaz1 and ū2 = √ q̄lbb ( c̄labz1 + √ 1− (c̄lab)2z2 ) ."
    }, {
      "heading" : "Derivation:",
      "text" : "Proceeding directly we find that,\nE[zli;azli;b] = 1\nρ2 ∑ j E[(W lij)2]E[plj;a]E[plj;b]E[ylj;aylj;b] + E[bli] (54)\n= σ2w ∫ Dz1Dz2φ(ū1)φ(ū2) + σ2b (55)\nwhere we have used the fact that E[pli;a] = E[pli;b] = ρ. We have also used the same substitution for E[ylj;aylj;b] used in the original mean field calculation with the appropriate substitution.\n7.5 THE LACK OF A c∗ = 1 FIXED POINT WITH DROPOUT"
    }, {
      "heading" : "Result:",
      "text" : "If clab = 1 then it follows that,\nc̄l+1ab = 1− 1− ρ ρq̄∗ σ2w\n∫ Dzφ2 (√ q̄∗z )\n(56)\nsubject to the approximation, qlaa ≈ qlbb ≈ q∗. This implies that cl+1ab < 1. Derivation:\nPlugging in clab = 1 with q l aa ≈ qlbb ≈ q∗ we find that ū1 = ū2 = √ q∗z1. It follows that,\ncl+1ab = ql+1ab q∗\n(57)\n= 1\nq∗\n[ σ2w ∫ Dzφ2 (√ q∗z ) + σ2b ] (58)\n= 1\nq∗\n[ σ2w(1− ρ−1 + ρ−1) ∫ Dzφ2 (√ q∗z ) + σ2b ] (59)\n= 1\nq∗ [ σ2w ρ ∫ Dzφ2 (√ q∗z ) + σ2b ] + σ2w q∗ (1− ρ−1) ∫ Dzφ2 (√ q∗z )\n(60)\n= 1− 1− ρ ρq̄∗ σ2w\n∫ Dzφ2 (√ q̄∗z )\n(61)\nas required. Here we have integrated out z2 since nether ū1 nor ū2 depend on it."
    }, {
      "heading" : "7.6 MEAN FIELD GRADIENT SCALING",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "In mean field theory the expected magnitude of the gradient ||∇W lijE|| 2 will be proportional to E[(δli)2]."
    }, {
      "heading" : "Derivation:",
      "text" : "We first note that since the W lij are i.i.d. it follows that,\n||∇W lijE|| 2 = ∑ ij\n( ∂E\n∂W lij\n)2 (62)\n≈ NlNl+1E ( ∂E ∂W lij )2 (63) where we have used the fact that the first line is related to the sample expectation over the different realizations of theW lij to approximate it by the analytic expectation in the second line. In mean field theory since the pre-activations in each layer are assumed to be i.i.d. Gaussian it follows that,\nE ( ∂E ∂W lij )2 = E[(δli)2]E[φ2(zl−1j )] (64) and the result follows."
    }, {
      "heading" : "7.7 MEAN FIELD BACKPROPAGATION",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "In mean field theory the recursion relation for the variance of the errors, q̃ l = E[(δli)2] is given by,\nq̃ laa = q̃ l+1 aa Nl+1 Nl+2 χ1(q l aa). (65)\nDerivation:\nComputing the variance directly and using mean field approximation, q̃ laa = E[(δli;a)2] = E[(φ′(zli;a))2] ∑ j E[(δl+1j;a ) 2]E[(W l+1ji ) 2] (66)\n= E[(φ′(zli;a))2] σ2w Nl+1 ∑ j E[(δl+1j;a ) 2] (67)\n= E[(φ′(zli;a))2] Nl+1 Nl+2 σ2w q̃ l+1 aa (68)\n= σ2w q̃ l+1 aa Nl+1 Nl+2\n∫ Dz [ φ′ (√\nqlaaz\n)]2 (69)\n≈ q̃ l+1aa Nl+1 Nl+2 χ1 (70)\nas required. In the last step we have made the approximation that qlaa ≈ q∗ since the depth scale for the variance is short ranged."
    }, {
      "heading" : "7.8 MEAN FIELD GRADIENT COVARIANCE SCALING",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "In mean field theory we expect the covariance between the gradients of two different inputs to scale as,\n(∇W lijEa) · (∇W lijEb) ∼ E[δi;aδi;b]. (71)"
    }, {
      "heading" : "Derivation:",
      "text" : "We proceed in a manner analogous to Appendix 7.6. Note that in mean field theory since the weights are i.i.d. it follows that\n(∇W lijEa) · (∇W lijEb) = ∑ ij ∂Ea ∂W lij ∂Eb ∂W lij\n(72)\n≈ NlNl+1E [ ∂Ea ∂W lij ∂Eb ∂W lij ] (73)\nwhere, as before, the final term is approximating the sample expectation. Since the weights in the forward and backwards passes are chosen independently it follows that we can factor the expectation as,\nE [ ∂Ea ∂W lij ∂Eb ∂W lij ] = E[δli;aδli;b]E[φ(zli;a)φ(zli;b)] (74)\nand the result follows."
    }, {
      "heading" : "7.9 MEAN FIELD BACKPROPAGATION OF COVARIANCE",
      "text" : ""
    }, {
      "heading" : "Result:",
      "text" : "The covariance between the gradients due to two inputs scales as,\nq̃ lab = q̃ l+1 ab Nl+1 Nl+2 σ2w\n∫ Dz1Dz2φ′(u1)φ′(u2) (75)\nunder backpropagation."
    }, {
      "heading" : "Derivation",
      "text" : "As in the analogous derivation for the variance, we compute directly, q̃ lab = E[δli;aδli;b] = E [φ′(zi;a)φ′(zi;b)] ∑ j E[δl+1j;a δ l+1 j;b ]E[(W l+1 ji ) 2] (76)\n= q̃ l+1ab Nl+1 Nl+2 σ2w\n∫ Dz1Dz2φ′(u1)φ′(u2) (77)\nas required."
    }, {
      "heading" : "7.10 FURTHER EXPERIMENTAL RESULTS",
      "text" : "Here we include some more experimental figures that investigate the effects of training time, minimizer, and dataset more closely."
    } ],
    "references" : [ {
      "title" : "The problem of learning long-term dependencies in recurrent networks",
      "author" : [ "Y Bengio", "Paolo Frasconi", "P Simard" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1993
    }, {
      "title" : "At the edge of chaos: Real-time computations and self-organized criticality in recurrent neural networks",
      "author" : [ "Nils Bertschinger", "Thomas Natschläger", "Robert A. Legenstein" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bertschinger et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bertschinger et al\\.",
      "year" : 2005
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity",
      "author" : [ "A. Daniely", "R. Frostig", "Y. Singer" ],
      "venue" : null,
      "citeRegEx" : "Daniely et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In Aistats,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Qualitatively characterizing neural network optimization problems",
      "author" : [ "Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Random feedback weights support learning in deep neural networks",
      "author" : [ "Timothy P Lillicrap", "Daniel Cownden", "Douglas B Tweed", "Colin J Akerman" ],
      "venue" : null,
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2014
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Montufar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Montufar et al\\.",
      "year" : 2014
    }, {
      "title" : "Bayesian learning for neural networks, volume 118",
      "author" : [ "Radford M Neal" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Neal.,? \\Q2012\\E",
      "shortCiteRegEx" : "Neal.",
      "year" : 2012
    }, {
      "title" : "Exponential expressivity in deep neural networks through transient chaos",
      "author" : [ "B. Poole", "S. Lahiri", "M. Raghu", "J. Sohl-Dickstein", "S. Ganguli" ],
      "venue" : null,
      "citeRegEx" : "Poole et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Poole et al\\.",
      "year" : 2016
    }, {
      "title" : "On the expressive power of deep neural networks",
      "author" : [ "M. Raghu", "B. Poole", "J. Kleinberg", "S. Ganguli", "J. Sohl-Dickstein" ],
      "venue" : null,
      "citeRegEx" : "Raghu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Raghu et al\\.",
      "year" : 2016
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "Benjamin Recht", "Moritz Hardt", "Yoram Singer" ],
      "venue" : null,
      "citeRegEx" : "Recht et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2015
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "A.M. Saxe", "J.L. McClelland", "S. Ganguli" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2014
    }, {
      "title" : "Random walks: Training very deep nonlinear feed-forward networks with smart initialization",
      "author" : [ "David Sussillo", "LF Abbott" ],
      "venue" : "CoRR, vol. abs/1412.6558,",
      "citeRegEx" : "Sussillo and Abbott.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sussillo and Abbott.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "The success of deep networks is due to the fact that they are highly expressive (Montufar et al., 2014) while simultaneously being relatively easy to optimize (Choromanska et al.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : ", 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al.",
      "startOffset" : 63,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : ", 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al.",
      "startOffset" : 63,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : ", 2014) with strong generalization properties (Recht et al., 2015).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "A pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth.",
      "startOffset" : 24,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "A pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth.",
      "startOffset" : 24,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : ", 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al., 2015). Consequently, developments in machine learning often accompany improvements in our ability to train increasingly deep networks. Despite this, designing novel network architectures is frequently equal parts art and science. This is, in part, because a general theory for neural networks that might inform design decisions has lagged behind the feverish pace of design. A pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth. Central to their approach was the consideration of networks after random initialization, whose weights and biases were i.i.d. Gaussian distributed. In particular the paper by Poole et al. (2016) developed a “mean field” formalism for treating wide, untrained, neural networks.",
      "startOffset" : 64,
      "endOffset" : 889
    }, {
      "referenceID" : 2,
      "context" : ", 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al., 2015). Consequently, developments in machine learning often accompany improvements in our ability to train increasingly deep networks. Despite this, designing novel network architectures is frequently equal parts art and science. This is, in part, because a general theory for neural networks that might inform design decisions has lagged behind the feverish pace of design. A pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth. Central to their approach was the consideration of networks after random initialization, whose weights and biases were i.i.d. Gaussian distributed. In particular the paper by Poole et al. (2016) developed a “mean field” formalism for treating wide, untrained, neural networks. They showed that these mean field networks exhibit an order-to-chaos transition as a function of the weight and bias variances. Notably the mean field formalism is not closely tied to a specific choice of activation function or loss. In this paper, we demonstrate the existence of several characteristic “depth” scales that emerge naturally and control signal propagation in these random networks. We then show that one of these depth scales, ξc, diverges at the boundary between order and chaos. This result is insensitive to many architectural decisions (such as choice of activation function) and will generically be true at any order-to-chaos transition. We then extend these results to include dropout and we show that even small amounts of dropout destroys the order-to-chaos critical point and consequently removes the divergence in ξc. Together these results bound the depth to which signal may propagate through random neural networks. We then develop a corresponding mean field model for gradients and we show that a duality exists between the forward propagation of signals and the backpropagation of gradients. The ordered and chaotic phases that Poole et al. (2016) identified correspond to regions of vanishing and exploding gradients, respectively.",
      "startOffset" : 64,
      "endOffset" : 2150
    }, {
      "referenceID" : 0,
      "context" : "phenomenon that has long been observed in neural networks (Bengio et al., 1993).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "(2005); Glorot & Bengio (2010) or indirect, through initialization schemes that favor deep signal propagation such as batch normalization (Ioffe & Szegedy, 2015), orthogonal matrix initialization (Saxe et al., 2014), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al.",
      "startOffset" : 196,
      "endOffset" : 215
    }, {
      "referenceID" : 3,
      "context" : ", 2014), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : ", 2016), or residual network architectures (He et al., 2015).",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "phenomenon that has long been observed in neural networks (Bengio et al., 1993). We continue to show that the covariance between two gradients is controlled by the same depth scale that limits correlated signal propagation in the forward direction. Finally, we hypothesize that a necessary condition for a random neural network to be trainable is that information should be able to pass through it. Thus, the depth-scales identified here bound the set of hyperparameters that will lead to successful training. To test this ansatz we train ensembles of deep, fully connected, feed-forward neural networks of varying depth on MNIST and CIFAR10, with and without dropout. Our results confirm that neural networks are trainable precisely when their depth is not much larger than ξc. This result is dataset independent and is, therefore, a universal function of network architecture. A corollary of these result is that asymptotically deep neural networks should be trainable provided they are initialized sufficiently close to the order-to-chaos transition. The notion of “edge of chaos” initialization has been explored previously. Such investigations have been both direct as in Bertschinger et al. (2005); Glorot & Bengio (2010) or indirect, through initialization schemes that favor deep signal propagation such as batch normalization (Ioffe & Szegedy, 2015), orthogonal matrix initialization (Saxe et al.",
      "startOffset" : 59,
      "endOffset" : 1204
    }, {
      "referenceID" : 0,
      "context" : "phenomenon that has long been observed in neural networks (Bengio et al., 1993). We continue to show that the covariance between two gradients is controlled by the same depth scale that limits correlated signal propagation in the forward direction. Finally, we hypothesize that a necessary condition for a random neural network to be trainable is that information should be able to pass through it. Thus, the depth-scales identified here bound the set of hyperparameters that will lead to successful training. To test this ansatz we train ensembles of deep, fully connected, feed-forward neural networks of varying depth on MNIST and CIFAR10, with and without dropout. Our results confirm that neural networks are trainable precisely when their depth is not much larger than ξc. This result is dataset independent and is, therefore, a universal function of network architecture. A corollary of these result is that asymptotically deep neural networks should be trainable provided they are initialized sufficiently close to the order-to-chaos transition. The notion of “edge of chaos” initialization has been explored previously. Such investigations have been both direct as in Bertschinger et al. (2005); Glorot & Bengio (2010) or indirect, through initialization schemes that favor deep signal propagation such as batch normalization (Ioffe & Szegedy, 2015), orthogonal matrix initialization (Saxe et al.",
      "startOffset" : 59,
      "endOffset" : 1228
    }, {
      "referenceID" : 11,
      "context" : "We begin by recapitulating the mean-field formalism developed in Poole et al. (2016). Consider a fully-connected, untrained, feed-forward, neural network of depth L with layer width Nl and some nonlinearity φ : R → R.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "Poole et al. (2016) note that the c∗ = 1 fixed point is stable if χ1 < 1 and is unstable otherwise.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "As observed in Poole et al. (2016) we see that the depth scale for the propagation of information in a single input, ξq , is consistently finite and significantly shorter than ξc.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "This approximation is similar in spirit to the vanilla mean field approximation and is reminiscent of work on feedback alignment (Lillicrap et al., 2014).",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "In Poole et al. (2016) they show that χ1 is intimately related to the tangent space of a given layer in mean field neural",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "We note that the depth dependence of learning rate was explored in detail in Saxe et al. (2014). In fig.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "An alternative perspective as to why information stored in the covariance between inputs is crucial for training can be understood by appealing to the correspondence between infinitely wide Bayesian neural networks and Gaussian Processes (Neal, 2012).",
      "startOffset" : 238,
      "endOffset" : 250
    } ],
    "year" : 2017,
    "abstractText" : "We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}