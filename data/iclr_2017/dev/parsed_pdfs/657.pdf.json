{
  "name" : "657.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Hervé Jégou" ],
    "emails" : [ "ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Text classification is an important problem in Natural Language Processing (NLP). Real world usecases include spam filtering or e-mail categorization. It is a core component in more complex systems such as search and ranking. Recently, deep learning techniques based on neural networks have achieved state of the art results in various NLP applications. One of the main successes of deep learning is due to the effectiveness of recurrent networks for language modeling and their application to speech recognition and machine translation (Mikolov, 2012). However, in other cases including several text classification problems, it has been shown that deep networks do not convincingly beat the prior state of the art techniques (Wang & Manning, 2012; Joulin et al., 2016).\nIn spite of being (typically) orders of magnitude slower to train than traditional techniques based on n-grams, neural networks are often regarded as a promising alternative due to compact model sizes, in particular for character based models. This is important for applications that need to run on systems with limited memory such as smartphones.\nThis paper specifically addresses the compromise between classification accuracy and the model size. We extend our previous work implemented in the fastText library1. It is based on n-gram features, dimensionality reduction, and a fast approximation of the softmax classifier (Joulin et al., 2016). We show that a few key ingredients, namely feature pruning, quantization, hashing, and retraining, allow us to produce text classification models with tiny size, often less than 100kB when trained on several popular datasets, without noticeably sacrificing accuracy or speed.\nWe plan to publish the code and scripts required to reproduce our results as an extension of the fastText library, thereby providing strong reproducible baselines for text classifiers that optimize the compromise between the model size and accuracy. We hope that this will help the engineering community to improve existing applications by using more efficient models.\nThis paper is organized as follows. Section 2 introduces related work, Section 3 describes our text classification model and explains how we drastically reduce the model size. Section 4 shows the effectiveness of our approach in experiments on multiple text classification benchmarks.\n1https://github.com/facebookresearch/fastText"
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Models for text classification. Text classification is a problem that has its roots in many applications such as web search, information retrieval and document classification (Deerwester et al., 1990; Pang & Lee, 2008). Linear classifiers often obtain state-of-the-art performance while being scalable (Agarwal et al., 2014; Joachims, 1998; Joulin et al., 2016; McCallum & Nigam, 1998). They are particularly interesting when associated with the right features (Wang & Manning, 2012). They usually require storing embeddings for words and n-grams, which makes them memory inefficient.\nCompression of language models. Our work is related to compression of statistical language models. Classical approaches include feature pruning based on entropy (Stolcke, 2000) and quantization. Pruning aims to keep only the most important n-grams in the model, leaving out those with probability lower than a specified threshold. Further, the individual n-grams can be compressed by quantizing the probability value, and by storing the n-gram itself more efficiently than as a sequence of characters. Various strategies have been developed, for example using tree structures or hash functions, and are discussed in (Talbot & Brants, 2008).\nCompression for similarity estimation and search. There is a large body of literature on how to compress a set of vectors into compact codes, such that the comparison of two codes approximates a target similarity in the original space. The typical use-case of these methods considers an indexed dataset of compressed vectors, and a query for which we want to find the nearest neighbors in the indexed set. One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature.\nBeyond these binarization strategies, more general quantization techniques derived from Jegou et al. (2011) offer better trade-offs between memory and the approximation of a distance estimator. The Product Quantization (PQ) method approximates the distances by calculating, in the compressed domain, the distance between their quantized approximations. This method is statistically guaranteed to preserve the Euclidean distance between the vectors within an error bound directly related to the quantization error. The original PQ has been concurrently improved by Ge et al. (2013) and Norouzi & Fleet (2013), who learn an orthogonal transform minimizing the overall quantization loss. In our paper, we will consider the Optimized Product Quantization (OPQ) variant (Ge et al., 2013).\nSoftmax approximation The aforementioned works approximate either the Euclidean distance or the cosine similarity (both being equivalent in the case of unit-norm vectors). However, in the context of fastText, we are specifically interested in approximating the maximum inner product involved in a softmax layer. Several approaches derived from LSH have been recently proposed to achieve this goal, such as Asymmetric LSH by Shrivastava & Li (2014), subsequently discussed by Neyshabur & Srebro (2015). In our work, since we are not constrained to purely binary codes, we resort a more traditional encoding by employing a magnitude/direction parametrization of our vectors. Therefore we only need to encode/compress an unitary d-dimensional vector, which fits the aforementioned LSH and PQ methods well.\nNeural network compression models. Recently, several research efforts have been conducted to compress the parameters of architectures involved in computer vision, namely for state-of-theart Convolutional Neural Networks (CNNs) (Han et al., 2016; Lin et al., 2015). Some use vector quantization (Gong et al., 2014) while others binarize the network (Courbariaux et al., 2016). Denil et al. (2013) show that such classification models are easily compressed because they are overparametrized, which concurs with early observations by LeCun et al. (1990).\n2In the literature, LSH refers to multiple distinct strategies related to the Johnson-Lindenstrauss lemma. For instance, LSH sometimes refers to a partitioning technique with random projections allowing for sublinear search via cell probes, see for instance the E2LSH variant of Datar et al. (2004).\nSome of these works both aim at reducing the model size and the speed. In our case, since the fastText classifier on which our proposal is built upon is already very efficient, we are primilarly interested in reducing the size of the model while keeping a comparable classification efficiency."
    }, {
      "heading" : "3 PROPOSED APPROACH",
      "text" : ""
    }, {
      "heading" : "3.1 TEXT CLASSIFICATION",
      "text" : "In the context of text classification, linear classifiers (Joulin et al., 2016) remain competitive with more sophisticated, deeper models, and are much faster to train. On top of standard tricks commonly used in linear text classification (Agarwal et al., 2014; Wang & Manning, 2012; Weinberger et al., 2009), Joulin et al. (2016) use a low rank constraint to reduce the computation burden while sharing information between different classes. This is especially useful in the case of a large output space, where rare classes may have only a few training examples. In this paper, we focus on a similar model, that is, which minimizes the softmax loss ` over N documents:\nN∑ n=1 `(yn, BAxn), (1)\nwhere xn is a bag of one-hot vectors and yn the label of the n-th document. In the case of a large vocabulary and a large output space, the matrices A and B are big and can require gigabytes of memory. Below, we describe how we reduce this memory usage."
    }, {
      "heading" : "3.2 BOTTOM-UP PRODUCT QUANTIZATION",
      "text" : "Product quantization is a popular method for compressed-domain approximate nearest neighbor search (Jegou et al., 2011). As a compression technique, it approximates a real-valued vector by finding the closest vector in a pre-defined structured set of centroids, referred to as a codebook. This codebook is not enumerated, since it is extremely large. Instead it is implicitly defined by its structure: a d-dimensional vector x ∈ Rd is approximated as\nx̂ = k∑ i=1 qi(x), (2)\nwhere the different subquantizers qi : x 7→ qi(x) are complementary in the sense that their respective centroids lie in distinct orthogonal subspaces, i.e., ∀i 6= j, ∀x, y, 〈qi(x)|qj(y)〉 = 0. In the original PQ, the subspaces are aligned with the natural axis, while OPQ learns a rotation, which amounts to alleviating this constraint and to not depend on the original coordinate system. Another way to see this is to consider that PQ splits a given vector x into k subvectors xi, i = 1 . . . k, each of dimension d/k: x = [x1 . . . xi . . . xk], and quantizes each sub-vector using a distinct k-means quantizer. Each subvector xi is thus mapped to the closest centroid amongst 2b centroids, where b is the number of bits required to store the quantization index of the subquantizer, typically b = 8. The reconstructed vector can take 2kb distinct reproduction values, and is stored in kb bits.\nPQ estimates the inner product in the compressed domain as\nx>y ≈ x̂>y = k∑\ni=1\nqi(x i)>yi. (3)\nThis is a straightforward extension of the square L2 distance estimation of Jegou et al. (2011). In practice, the vector estimate x̂ is trivially reconstructed from the codes, i.e., from the quantization indexes, by concatenating these centroids.\nThe two parameters involved in PQ, namely the number of subquantizers k and the number of bits b per quantization index, are typically set to k ∈ [2, d/2], and b = 8 to ensure byte-alignment.\nDiscussion. PQ offers several interesting properties in our context of text classification. Firstly, the training is very fast because the subquantizers have a small number of centroids, i.e., 256 centroids for b = 8. Secondly, at test time it allows the reconstruction of the vectors with almost no\ncomputational and memory overhead. Thirdly, it has been successfully applied in computer vision, offering much better performance than binary codes, which makes it a natural candidate to compress relatively shallow models. As observed by Sánchez & Perronnin (2011), using PQ just before the last layer incurs a very limited loss in accuracy when combined with a support vector machine.\nIn the context of text classification, the norms of the vectors are widely spread, typically with a ratio of 1000 between the max and the min. Therefore kmeans performs poorly because it optimizes an absolute error objective, so it maps all low-norm vectors to 0. A simple solution is to separate the norm and the angle of the vectors and to quantize them separately. This allows a quantization with no loss of performance, yet requires an extra b bits per vector.\nBottom-up strategy: re-training. The first works aiming at compressing CNN models like the one proposed by (Gong et al., 2014) used the reconstruction from off-the-shelf PQ, i.e., without any re-training. However, as observed in Sablayrolles et al. (2016), when using quantization methods like PQ, it is better to re-train the layers occurring after the quantization, so that the network can re-adjust itself to the quantization. There is a strong argument arguing for this re-training strategy: the square magnitude of vectors is reduced, on average, by the average quantization error for any quantizer satisfying the Lloyd conditions; see Jegou et al. (2011) for details.\nThis suggests a bottom-up learning strategy where we first quantize the input matrix, then retrain and quantize the output matrix (the input matrix being frozen). Experiments in section 4 show that it is worth adopting this strategy.\nMemory savings with PQ. In practice, the bottom-up PQ strategy offers a compression factor of 10 without any noticeable loss of performance. Without re-training, we notice a drop in accuracy between 0.1% and 0.5%, depending on the dataset and setting; see Section 4 and the appendix."
    }, {
      "heading" : "3.3 FURTHER TEXT SPECIFIC TRICKS",
      "text" : "The memory usage strongly depends on the size of the vocabulary, which can be large in many text classification tasks. While it is clear that a large part of the vocabulary is useless or redundant, directly reducing the vocabulary to the most frequent words is not satisfactory: most of the frequent words, like “the” or “is” are not discriminative, in contrast to some rare words, e.g., in the context of tag prediction. In this section, we discuss a few heuristics to reduce the space taken by the dictionary. They lead to major memory reduction, in extreme cases by a factor 100. We experimentally show that this drastic reduction is complementary with the PQ compression method, meaning that the combination of both strategies reduces the model size by a factor up to ×1000 for some datasets.\nPruning the vocabulary. Discovering which word or n-gram must be kept to preserve the overall performance is a feature selection problem. While many approaches have been proposed to select groups of variables during training (Bach et al., 2012; Meier et al., 2008), we are interested in selecting a fixed subset of K words and ngrams from a pre-trained model. This can be achieved by selecting the K embeddings that preserve as much of the model as possible, which can be reduced to selecting the K words and ngrams associated with the highest norms.\nWhile this approach offers major memory savings, it has one drawback occurring in some particular cases: some documents may not contained any of the K best features, leading to a significant drop in performance. It is thus important to keep the K best features under the condition that they cover the whole training set. More formally, the problem is to find a subset S in the feature set V that maximizes the sum of their norms ws under the constraint that all the documents in the training set D are covered:\nmax S⊆V ∑ s∈S ws s.t. |S| ≤ K, P1S ≥ 1D,\nwhere P is a matrix such that Pds = 1 if the s-th feature is in the d-th document, and 0 otherwise. This problem is directly related to set covering problems that are NP-hard (Feige, 1998). Standard greedy approaches require the storing of an inverted index or to do multiple passes over the dataset, which is prohibitive on very large dataset (Chierichetti et al., 2010). This problem can be cast as an instance of online submodular maximization with a rank constraint (Badanidiyuru et al., 2014;\nBateni et al., 2010). In our case, we use a simple online parallelizable greedy approach: For each document, we verify if it is already covered by a retained feature and, if not, we add the feature with the highest norm to our set of retained features. If the number of features is below k, we add the features with the highest norm that have not yet been picked.\nHashing trick & Bloom filter. On small models, the dictionary can take a significant portion of the memory. Instead of saving it, we extend the hashing trick used in Joulin et al. (2016) to both words and n-grams. This strategy is also used in Vowpal Wabbit (Agarwal et al., 2014) in the context of online training. This allows us to save around 1-2Mb with almost no overhead at test time (just the cost of computing the hashing function).\nPruning the vocabulary while using the hashing trick requires keeping a list of the indices of the K remaining buckets. At test time, a binary search over the list of indices is required. It has a complexity of O(log(K)) and a memory overhead of a few hundreds of kilobytes. Using Bloom filters instead reduces the complexityO(1) at test time and saves a few hundred kilobytes. However, in practice, it degrades performance."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "This section evaluates the quality of our model compression pipeline and compare it to other compression methods on different text classification problems, and to other compact text classifiers.\nEvaluation protocol and datasets. Our experimental pipeline is as follows: we train a model using fastText with the default setting unless specified otherwise. That is 2M buckets, a learning rate of 0.1 and 10 training epochs. The dimensionality d of the embeddings is set to powers of 2 to avoid border effects that could make the interpretation of the results more difficult. As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al., 2011) and OPQ (Ge et al., 2013) (the non-parametric variant). Note that we use an improved version of LSH where random orthogonal matrices are used instead of random matrix projection Jégou et al. (2008). In a first series of experiments, we use the 8 datasets and evaluation protocol of Zhang et al. (2015). These datasets contain few million documents and have at most 10 classes. We also explore the limit of quantization on a dataset with an extremely large output space, that is a tag dataset extracted from the YFCC100M collection (Thomee et al., 2016)3, referred to as FlickrTag in the rest of this paper."
    }, {
      "heading" : "4.1 SMALL DATASETS",
      "text" : "Compression techniques. We compare three popular methods used for similarity estimation with compact codes: LSH, PQ and OPQ on the datasets released by Zhang et al. (2015). Figure 1 shows the accuracy as a function of the number of bytes used per embedding, which corresponds to the number k of subvectors in the case of PQ and OPQ. See more results in the appendix. As discussed in Section 2, LSH reproduces the cosine similarity and is therefore not adapted to un-normalized data. Therefore we only report results with normalization. Once normalized, PQ and OPQ are almost lossless even when using only k = 4 subquantizers per embedding (equivalently, bytes). We observe in practice that using k = d/2, i.e., half of the components of the embeddings, works well in practice. In the rest of the paper and if not stated otherwise, we focus on this setting. The difference between the normalized versions of PQ and OPQ is limited and depends on the dataset. Therefore we adopt the normalized PQ (NPQ) for the rest of this study, since it is faster to train.\n3Data available at https://research.facebook.com/research/fasttext/\nPruning. Figure 2 shows the performance of our model with different sizes. We fix k = d/2 and use different pruning thresholds. NPQ offers a compression rate of×10 compared to the full model. As the pruning becomes more agressive, the overall compression can increase up up to ×1, 000 with little drop of performance and no additional overhead at test time. In fact, using a smaller dictionary makes the model faster at test time. We also compare with character-level Convolutional Neural Networks (CNN) (Zhang et al., 2015; Xiao & Cho, 2016). They are attractive models for text classification because they achieve similar performance with less memory usage than linear models (Xiao & Cho, 2016). Even though fastText with the default setting uses more memory, NPQ is already on par with CNNs’ memory usage. Note that CNNs are not quantized, and it would be worth seeing how much they can be quantized with no drop of performance. Such a study is beyond the scope of this paper. Our pruning is based on the norm of the embeddings according to the guidelines of Section 3.3. Table 1 compares the ranking obtained with norms to the ranking obtained using entropy, which is commonly used in unsupervised settings Stolcke (2000).\nExtreme compression. Finally, in Table 2, we explore the limit of quantized model by looking at the performance obtained for models under 64KiB. Surprisingly, even at 64KiB and 32KiB, the drop of performance is only around 0.8% and 1.7% despite a compression rate of ×1, 000− 4, 000."
    }, {
      "heading" : "4.2 LARGE DATASET: FLICKRTAG",
      "text" : "In this section, we explore the limit of compression algorithms on very large datasets. Similar to Joulin et al. (2016), we consider a hashtag prediction dataset containing 312, 116 labels. We set the minimum count for words at 10, leading to a dictionary of 1, 427, 667 words. We take 10M buckets for n-grams and a hierarchical softmax. We refer to this dataset as FlickrTag.\nOutput encoding. We are interested in understanding how the performance degrades if the classifier is also quantized (i.e., the matrix B in Eq. 1) and when the pruning is at the limit of the minimum number of features required to cover the full dataset.\nTable 3 shows that quantizing both the “input” matrix (i.e., A in Eq. 1) and the “output” matrix (i.e., B) does not degrade the performance compared to the full model. We use embeddings with d = 256 dimensions and use k = d/2 subquantizers. We do not use any text specific tricks, which leads to a compression factor of 8. Note that even if the output matrix is not retrained over the embeddings, the performance is only 0.2% away from the full model. As shown in the Appendix, using less subquantizers significantly decreases the performance for a small memory gain.\nPruning. Table 4 shows how the performance evolves with pruning. We measure this effect on top of a fully quantized model. The full model misses 11.6% of the test set because of missing words (some documents are either only composed of hashtags or have only rare words). There are 312, 116 labels and thus it seems reasonable to keep embeddings in the order of the million. A naive pruning with 1M features misses about 30−40% of the test set, leading to a significant drop of performance. On the other hand, even though the max-coverage pruning approach was set on the train set, it does not suffer from any coverage loss on the test set. This leads to a smaller drop of performance. If the pruning is too aggressive, however, the coverage decreases significantly."
    }, {
      "heading" : "5 FUTURE WORK",
      "text" : "It may be possible to obtain further reduction of the model size in the future. One idea is to condition the size of the vectors (both for the input features and the labels) based on their frequency (Chen et al., 2015; Grave et al., 2016). For example, it is probably not worth representing the rare labels by full 256-dimensional vectors in the case of the FlickrTag dataset. Thus, conditioning the vector size on the frequency and norm seems like an interesting direction to explore in the future.\nWe may also consider combining the entropy and norm pruning criteria: instead of keeping the features in the model based just on the frequency or the norm, we can use both to keep a good set of features. This could help to keep features that are both frequent and discriminative, and thereby to reduce the coverage problem that we have observed.\nAdditionally, instead of pruning out the less useful features, we can decompose them into smaller units (Mikolov et al., 2012). For example, this can be achieved by splitting every non-discriminative word into a sequence of character trigrams. This could help in cases where training and test examples are very short (for example just a single word)."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we have presented several simple techniques to reduce, by several orders of magnitude, the memory complexity of certain text classifiers without sacrificing accuracy nor speed. This is achieved by applying discriminative pruning which aims to keep only important features in the trained model, and by performing quantization of the weight matrices and hashing of the dictionary.\nWe will publish the code as an extension of the fastText library. We hope that our work will serve as a baseline to the research community, where there is an increasing interest for comparing the performance of various deep learning text classifiers for a given number of parameters. Overall, compared to recent work based on convolutional neural networks, fastText.zip is often more accurate, while requiring several orders of magnitude less time to train on common CPUs, and incurring a fraction of the memory complexity."
    } ],
    "references" : [ {
      "title" : "Streaming submodular maximization: Massive data summarization on the fly",
      "author" : [ "Ashwinkumar Badanidiyuru", "Baharan Mirzasoleiman", "Amin Karbasi", "Andreas Krause" ],
      "venue" : "In SIGKDD,",
      "citeRegEx" : "Badanidiyuru et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Badanidiyuru et al\\.",
      "year" : 2014
    }, {
      "title" : "Submodular secretary problem and extensions. In Approximation, Randomization, and Combinatorial Optimization",
      "author" : [ "Mohammad Hossein Bateni", "Mohammad Taghi Hajiaghayi", "Morteza Zadimoghaddam" ],
      "venue" : "Algorithms and Techniques,",
      "citeRegEx" : "Bateni et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bateni et al\\.",
      "year" : 2010
    }, {
      "title" : "Similarity estimation techniques from rounding algorithms",
      "author" : [ "Moses S. Charikar" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Charikar.,? \\Q2002\\E",
      "shortCiteRegEx" : "Charikar.",
      "year" : 2002
    }, {
      "title" : "Strategies for training large vocabulary neural language models",
      "author" : [ "Welin Chen", "David Grangier", "Michael Auli" ],
      "venue" : "arXiv preprint arXiv:1512.04906,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Max-cover in map-reduce",
      "author" : [ "Flavio Chierichetti", "Ravi Kumar", "Andrew Tomkins" ],
      "venue" : "In International Conference on World Wide Web,",
      "citeRegEx" : "Chierichetti et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chierichetti et al\\.",
      "year" : 2010
    }, {
      "title" : "Binarized neural networks: Training neural networks with weights and activations constrained to +1 or -1",
      "author" : [ "Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1602.02830,",
      "citeRegEx" : "Courbariaux et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Courbariaux et al\\.",
      "year" : 2016
    }, {
      "title" : "Locality-sensitive hashing scheme based on pstable distributions",
      "author" : [ "M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni" ],
      "venue" : "In Proceedings of the Symposium on Computational Geometry,",
      "citeRegEx" : "Datar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Datar et al\\.",
      "year" : 2004
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman" ],
      "venue" : "Journal of the American society for information science,",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc-Aurelio Ranzato", "Nando et all de Freitas" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Denil et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2013
    }, {
      "title" : "A threshold of ln n for approximating set cover",
      "author" : [ "Uriel Feige" ],
      "venue" : "JACM, 45(4):634–652,",
      "citeRegEx" : "Feige.,? \\Q1998\\E",
      "shortCiteRegEx" : "Feige.",
      "year" : 1998
    }, {
      "title" : "Optimized product quantization for approximate nearest neighbor search",
      "author" : [ "Tiezheng Ge", "Kaiming He", "Qifa Ke", "Jian Sun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Ge et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2013
    }, {
      "title" : "Iterative quantization: A procrustean approach to learning binary codes",
      "author" : [ "Yunchao Gong", "Svetlana Lazebnik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Gong and Lazebnik.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gong and Lazebnik.",
      "year" : 2011
    }, {
      "title" : "Compressing deep convolutional networks using vector quantization",
      "author" : [ "Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev" ],
      "venue" : "arXiv preprint arXiv:1412.6115,",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient softmax approximation for gpus",
      "author" : [ "Edouard Grave", "Armand Joulin", "Moustapha Cissé", "David Grangier", "Hervé Jégou" ],
      "venue" : "arXiv preprint arXiv:1609.04309,",
      "citeRegEx" : "Grave et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Han et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Hamming embedding and weak geometric consistency for large scale image search",
      "author" : [ "Hervé Jégou", "Matthijs Douze", "Cordelia Schmid" ],
      "venue" : null,
      "citeRegEx" : "Jégou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jégou et al\\.",
      "year" : 2008
    }, {
      "title" : "Product quantization for nearest neighbor search",
      "author" : [ "Hervé Jegou", "Matthijs Douze", "Cordelia Schmid" ],
      "venue" : "IEEE Trans. PAMI,",
      "citeRegEx" : "Jegou et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jegou et al\\.",
      "year" : 2011
    }, {
      "title" : "Text categorization with support vector machines: Learning with many relevant features",
      "author" : [ "Thorsten Joachims" ],
      "venue" : null,
      "citeRegEx" : "Joachims.,? \\Q1998\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 1998
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov" ],
      "venue" : "arXiv preprint arXiv:1607.01759,",
      "citeRegEx" : "Joulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Optimal brain damage",
      "author" : [ "Yann LeCun", "John S Denker", "Sara A Solla" ],
      "venue" : "NIPS, 2:598–605,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1990
    }, {
      "title" : "Neural networks with few multiplications",
      "author" : [ "Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1510.03009,",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "A comparison of event models for naive bayes text classification",
      "author" : [ "Andrew McCallum", "Kamal Nigam" ],
      "venue" : "In AAAI workshop on learning for text categorization,",
      "citeRegEx" : "McCallum and Nigam.,? \\Q1998\\E",
      "shortCiteRegEx" : "McCallum and Nigam.",
      "year" : 1998
    }, {
      "title" : "The group lasso for logistic regression",
      "author" : [ "Lukas Meier", "Sara Van De Geer", "Peter Bühlmann" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Meier et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Meier et al\\.",
      "year" : 2008
    }, {
      "title" : "Statistical language models based on neural networks",
      "author" : [ "Tomas Mikolov" ],
      "venue" : "In PhD thesis. VUT Brno,",
      "citeRegEx" : "Mikolov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov.",
      "year" : 2012
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "J Cernocky" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "On symmetric and asymmetric lshs for inner product search",
      "author" : [ "Behnam Neyshabur", "Nathan Srebro" ],
      "venue" : "In ICML, pp. 1926–1934,",
      "citeRegEx" : "Neyshabur and Srebro.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neyshabur and Srebro.",
      "year" : 2015
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "Foundations and trends in information retrieval,",
      "citeRegEx" : "Pang and Lee.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "How should we evaluate supervised hashing",
      "author" : [ "Alexandre Sablayrolles", "Matthijs Douze", "Hervé Jégou", "Nicolas Usunier" ],
      "venue" : "arXiv preprint arXiv:1609.06753,",
      "citeRegEx" : "Sablayrolles et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sablayrolles et al\\.",
      "year" : 2016
    }, {
      "title" : "High-dimensional signature compression for large-scale image classification",
      "author" : [ "Jorge Sánchez", "Florent Perronnin" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Sánchez and Perronnin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sánchez and Perronnin.",
      "year" : 2011
    }, {
      "title" : "Asymmetric LSH for sublinear time maximum inner product search",
      "author" : [ "Anshumali Shrivastava", "Ping Li" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Shrivastava and Li.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shrivastava and Li.",
      "year" : 2014
    }, {
      "title" : "Entropy-based pruning of backoff language models",
      "author" : [ "Andreas Stolcke" ],
      "venue" : "arXiv preprint cs/0006025,",
      "citeRegEx" : "Stolcke.,? \\Q2000\\E",
      "shortCiteRegEx" : "Stolcke.",
      "year" : 2000
    }, {
      "title" : "Randomized language models via perfect hash functions",
      "author" : [ "David Talbot", "Thorsten Brants" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Talbot and Brants.,? \\Q2008\\E",
      "shortCiteRegEx" : "Talbot and Brants.",
      "year" : 2008
    }, {
      "title" : "Yfcc100m: The new data in multimedia research",
      "author" : [ "Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li" ],
      "venue" : "In Communications of the ACM,",
      "citeRegEx" : "Thomee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Thomee et al\\.",
      "year" : 2016
    }, {
      "title" : "Hashing for similarity search: A survey",
      "author" : [ "Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji" ],
      "venue" : "arXiv preprint arXiv:1408.2927,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to hash for indexing big data - A survey",
      "author" : [ "Jun Wang", "Wei Liu", "Sanjiv Kumar", "Shih-Fu Chang" ],
      "venue" : "CoRR, abs/1509.05472,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Baselines and bigrams: Simple, good sentiment and topic classification",
      "author" : [ "Sida Wang", "Christopher D Manning" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Wang and Manning.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang and Manning.",
      "year" : 2012
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "Kilian Q Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Weinberger et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger et al\\.",
      "year" : 2009
    }, {
      "title" : "Spectral hashing",
      "author" : [ "Yair Weiss", "Antonio Torralba", "Rob Fergus" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Weiss et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient character-level document classification by combining convolution and recurrent layers",
      "author" : [ "Yijun Xiao", "Kyunghyun Cho" ],
      "venue" : "arXiv preprint arXiv:1602.00367,",
      "citeRegEx" : "Xiao and Cho.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiao and Cho.",
      "year" : 2016
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "One of the main successes of deep learning is due to the effectiveness of recurrent networks for language modeling and their application to speech recognition and machine translation (Mikolov, 2012).",
      "startOffset" : 183,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : "However, in other cases including several text classification problems, it has been shown that deep networks do not convincingly beat the prior state of the art techniques (Wang & Manning, 2012; Joulin et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 215
    }, {
      "referenceID" : 18,
      "context" : "It is based on n-gram features, dimensionality reduction, and a fast approximation of the softmax classifier (Joulin et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "Text classification is a problem that has its roots in many applications such as web search, information retrieval and document classification (Deerwester et al., 1990; Pang & Lee, 2008).",
      "startOffset" : 143,
      "endOffset" : 186
    }, {
      "referenceID" : 17,
      "context" : "Linear classifiers often obtain state-of-the-art performance while being scalable (Agarwal et al., 2014; Joachims, 1998; Joulin et al., 2016; McCallum & Nigam, 1998).",
      "startOffset" : 82,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "Linear classifiers often obtain state-of-the-art performance while being scalable (Agarwal et al., 2014; Joachims, 1998; Joulin et al., 2016; McCallum & Nigam, 1998).",
      "startOffset" : 82,
      "endOffset" : 165
    }, {
      "referenceID" : 30,
      "context" : "Classical approaches include feature pruning based on entropy (Stolcke, 2000) and quantization.",
      "startOffset" : 62,
      "endOffset" : 77
    }, {
      "referenceID" : 37,
      "context" : "Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization.",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "In our paper, we will consider the Optimized Product Quantization (OPQ) variant (Ge et al., 2013).",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes.",
      "startOffset" : 63,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al.",
      "startOffset" : 63,
      "endOffset" : 682
    }, {
      "referenceID" : 2,
      "context" : "One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature.",
      "startOffset" : 63,
      "endOffset" : 705
    }, {
      "referenceID" : 2,
      "context" : "One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature. Beyond these binarization strategies, more general quantization techniques derived from Jegou et al. (2011) offer better trade-offs between memory and the approximation of a distance estimator.",
      "startOffset" : 63,
      "endOffset" : 863
    }, {
      "referenceID" : 2,
      "context" : "One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature. Beyond these binarization strategies, more general quantization techniques derived from Jegou et al. (2011) offer better trade-offs between memory and the approximation of a distance estimator. The Product Quantization (PQ) method approximates the distances by calculating, in the compressed domain, the distance between their quantized approximations. This method is statistically guaranteed to preserve the Euclidean distance between the vectors within an error bound directly related to the quantization error. The original PQ has been concurrently improved by Ge et al. (2013) and Norouzi & Fleet (2013), who learn an orthogonal transform minimizing the overall quantization loss.",
      "startOffset" : 63,
      "endOffset" : 1336
    }, {
      "referenceID" : 2,
      "context" : "One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature. Beyond these binarization strategies, more general quantization techniques derived from Jegou et al. (2011) offer better trade-offs between memory and the approximation of a distance estimator. The Product Quantization (PQ) method approximates the distances by calculating, in the compressed domain, the distance between their quantized approximations. This method is statistically guaranteed to preserve the Euclidean distance between the vectors within an error bound directly related to the quantization error. The original PQ has been concurrently improved by Ge et al. (2013) and Norouzi & Fleet (2013), who learn an orthogonal transform minimizing the overall quantization loss.",
      "startOffset" : 63,
      "endOffset" : 1363
    }, {
      "referenceID" : 14,
      "context" : "Recently, several research efforts have been conducted to compress the parameters of architectures involved in computer vision, namely for state-of-theart Convolutional Neural Networks (CNNs) (Han et al., 2016; Lin et al., 2015).",
      "startOffset" : 192,
      "endOffset" : 228
    }, {
      "referenceID" : 20,
      "context" : "Recently, several research efforts have been conducted to compress the parameters of architectures involved in computer vision, namely for state-of-theart Convolutional Neural Networks (CNNs) (Han et al., 2016; Lin et al., 2015).",
      "startOffset" : 192,
      "endOffset" : 228
    }, {
      "referenceID" : 12,
      "context" : "Some use vector quantization (Gong et al., 2014) while others binarize the network (Courbariaux et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : ", 2014) while others binarize the network (Courbariaux et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : ", 2014) while others binarize the network (Courbariaux et al., 2016). Denil et al. (2013) show that such classification models are easily compressed because they are overparametrized, which concurs with early observations by LeCun et al.",
      "startOffset" : 43,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : ", 2014) while others binarize the network (Courbariaux et al., 2016). Denil et al. (2013) show that such classification models are easily compressed because they are overparametrized, which concurs with early observations by LeCun et al. (1990).",
      "startOffset" : 43,
      "endOffset" : 245
    }, {
      "referenceID" : 6,
      "context" : "For instance, LSH sometimes refers to a partitioning technique with random projections allowing for sublinear search via cell probes, see for instance the ELSH variant of Datar et al. (2004).",
      "startOffset" : 171,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "In the context of text classification, linear classifiers (Joulin et al., 2016) remain competitive with more sophisticated, deeper models, and are much faster to train.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 36,
      "context" : "On top of standard tricks commonly used in linear text classification (Agarwal et al., 2014; Wang & Manning, 2012; Weinberger et al., 2009), Joulin et al.",
      "startOffset" : 70,
      "endOffset" : 139
    }, {
      "referenceID" : 18,
      "context" : "In the context of text classification, linear classifiers (Joulin et al., 2016) remain competitive with more sophisticated, deeper models, and are much faster to train. On top of standard tricks commonly used in linear text classification (Agarwal et al., 2014; Wang & Manning, 2012; Weinberger et al., 2009), Joulin et al. (2016) use a low rank constraint to reduce the computation burden while sharing information between different classes.",
      "startOffset" : 59,
      "endOffset" : 331
    }, {
      "referenceID" : 16,
      "context" : "Product quantization is a popular method for compressed-domain approximate nearest neighbor search (Jegou et al., 2011).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "This is a straightforward extension of the square L2 distance estimation of Jegou et al. (2011). In practice, the vector estimate x̂ is trivially reconstructed from the codes, i.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "The first works aiming at compressing CNN models like the one proposed by (Gong et al., 2014) used the reconstruction from off-the-shelf PQ, i.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "The first works aiming at compressing CNN models like the one proposed by (Gong et al., 2014) used the reconstruction from off-the-shelf PQ, i.e., without any re-training. However, as observed in Sablayrolles et al. (2016), when using quantization methods like PQ, it is better to re-train the layers occurring after the quantization, so that the network can re-adjust itself to the quantization.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 12,
      "context" : "The first works aiming at compressing CNN models like the one proposed by (Gong et al., 2014) used the reconstruction from off-the-shelf PQ, i.e., without any re-training. However, as observed in Sablayrolles et al. (2016), when using quantization methods like PQ, it is better to re-train the layers occurring after the quantization, so that the network can re-adjust itself to the quantization. There is a strong argument arguing for this re-training strategy: the square magnitude of vectors is reduced, on average, by the average quantization error for any quantizer satisfying the Lloyd conditions; see Jegou et al. (2011) for details.",
      "startOffset" : 75,
      "endOffset" : 628
    }, {
      "referenceID" : 22,
      "context" : "While many approaches have been proposed to select groups of variables during training (Bach et al., 2012; Meier et al., 2008), we are interested in selecting a fixed subset of K words and ngrams from a pre-trained model.",
      "startOffset" : 87,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "This problem is directly related to set covering problems that are NP-hard (Feige, 1998).",
      "startOffset" : 75,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Standard greedy approaches require the storing of an inverted index or to do multiple passes over the dataset, which is prohibitive on very large dataset (Chierichetti et al., 2010).",
      "startOffset" : 154,
      "endOffset" : 181
    }, {
      "referenceID" : 39,
      "context" : "Figure 1: Accuracy as a function of the memory per vector/embedding on 3 datasets from Zhang et al. (2015). Note, an extra byte is required when we encode the norm explicitly (”norm”).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "Instead of saving it, we extend the hashing trick used in Joulin et al. (2016) to both words and n-grams.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al.",
      "startOffset" : 54,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al., 2011) and OPQ (Ge et al.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : ", 2011) and OPQ (Ge et al., 2013) (the non-parametric variant).",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 32,
      "context" : "We also explore the limit of quantization on a dataset with an extremely large output space, that is a tag dataset extracted from the YFCC100M collection (Thomee et al., 2016)3, referred to as FlickrTag in the rest of this paper.",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al., 2011) and OPQ (Ge et al., 2013) (the non-parametric variant). Note that we use an improved version of LSH where random orthogonal matrices are used instead of random matrix projection Jégou et al. (2008). In a first series of experiments, we use the 8 datasets and evaluation protocol of Zhang et al.",
      "startOffset" : 55,
      "endOffset" : 294
    }, {
      "referenceID" : 2,
      "context" : "As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al., 2011) and OPQ (Ge et al., 2013) (the non-parametric variant). Note that we use an improved version of LSH where random orthogonal matrices are used instead of random matrix projection Jégou et al. (2008). In a first series of experiments, we use the 8 datasets and evaluation protocol of Zhang et al. (2015). These datasets contain few million documents and have at most 10 classes.",
      "startOffset" : 55,
      "endOffset" : 398
    }, {
      "referenceID" : 39,
      "context" : "Full PQ Pruned Zhang et al. (2015) Xiao & Cho (2016)",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "Full PQ Pruned Zhang et al. (2015) Xiao & Cho (2016)",
      "startOffset" : 15,
      "endOffset" : 53
    }, {
      "referenceID" : 39,
      "context" : "We also compare with Zhang et al. (2015) and Xiao & Cho (2016).",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 39,
      "context" : "We also compare with Zhang et al. (2015) and Xiao & Cho (2016). Note that the size is in log scale.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 39,
      "context" : "We compare three popular methods used for similarity estimation with compact codes: LSH, PQ and OPQ on the datasets released by Zhang et al. (2015). Figure 1 shows the accuracy as a function of the number of bytes used per embedding, which corresponds to the number k of subvectors in the case of PQ and OPQ.",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 39,
      "context" : "We also compare with character-level Convolutional Neural Networks (CNN) (Zhang et al., 2015; Xiao & Cho, 2016).",
      "startOffset" : 73,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "Table 1 compares the ranking obtained with norms to the ranking obtained using entropy, which is commonly used in unsupervised settings Stolcke (2000).",
      "startOffset" : 136,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "Similar to Joulin et al. (2016), we consider a hashtag prediction dataset containing 312, 116 labels.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "One idea is to condition the size of the vectors (both for the input features and the labels) based on their frequency (Chen et al., 2015; Grave et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "One idea is to condition the size of the vectors (both for the input features and the labels) based on their frequency (Chen et al., 2015; Grave et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 158
    }, {
      "referenceID" : 24,
      "context" : "Additionally, instead of pruning out the less useful features, we can decompose them into smaller units (Mikolov et al., 2012).",
      "startOffset" : 104,
      "endOffset" : 126
    } ],
    "year" : 2016,
    "abstractText" : "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Combined with simple approaches specifically adapted to text classification, our approach derived from fastText requires, at test time, only a fraction of the memory compared to the original FastText, without noticeably sacrificing quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}