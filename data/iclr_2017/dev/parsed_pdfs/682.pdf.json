{
  "name" : "682.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "ANNEALING GAUSSIAN", "LEAKY-RELU RBM", "Chun-Liang Li", "Siamak Ravanbakhsh", "Barnabás Póczos" ],
    "emails" : [ "chunlial@cs.cmu.edu", "mravanba@cs.cmu.edu", "bapoczos@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In this paper, we are interested in deep generative models. One may naively classify these models into a family of directed deep generative models trainable by back-propagation (e.g., Kingma & Welling, 2013; Goodfellow et al., 2014), and deep energy-based models, such as deep belief network (Hinton et al., 2006) and deep Boltzmann machine (Salakhutdinov & Hinton, 2009). The building block of deep energy-based models is a bipartite graphical model called restricted Boltzmann machine (RBM). The RBM model consists of two layers, visible and hidden. The resulting graphical model which can account for higher-order interactions of the visible units (visible layer) using the hidden units (hidden layer). It also makes the inference easier that there are no interactions between the variables in each layer.\nThe conventional RBM uses Bernoulli units for both the hidden and visible units (Smolensky, 1986). One extension is using Gaussian visible units to model general natural images (Freund & Haussler, 1994). For hidden units, we can also generalize Bernoulli units to the exponential family (Welling et al., 2004; Ravanbakhsh et al., 2016).\nNair & Hinton (2010) propose a variation using Rectified Linear Unit (ReLU) for the hidden layer with a heuristic sampling procedure, which has promising performance in terms of reconstruction error and classification accuracy. Unfortunately, due to its lack of strict monotonicity, ReLU RBM does not fit within the framework of exponential family RBMs (Ravanbakhsh et al., 2016). Instead we study leaky-ReLU RBM (leaky RBM) in this work and address two important issues i) a better training (sampling) algorithm for ReLU RBM and; ii) a better quantification of leaky RBM –i.e., evaluation of its performance in terms of likelihood.\nWe study some of the fundamental properties of leaky RBM, including its joint and marginal distributions (Section 2). By analyzing these distributions, we show that the leaky RBM is a union of\ntruncated Gaussian distributions. In this paper, we show that training leaky RBM involves underlying positive definite constraints. Because of this, the training can diverge if these constrains are not satisfied. This is an issue that was previously ignored in ReLU RBM, as it was mainly used for pre-training rather than generative modeling.\nOur contribution in this paper is three-fold: I) we systematically identify and address model constraints in leaky RBM (Section 3); II) for the training of leaky RBM, we propose a meta algorithm for sampling, which anneals leakiness during the Gibbs sampling procedure (Section 3) and empirically show that it can boost contrastive divergence with faster mixing (Section 5); III) We demonstrate the power of the proposed sampling algorithm on estimating the partition function. In particular, comparison on several benchmark datasets shows that the proposed method outperforms the conventional AIS (Salakhutdinov & Murray, 2008) in terms of efficiency and accuracy (Section 4). Moreover, we provide an incentive for using leaky RBM by showing that the leaky ReLU hidden units perform better than the Bernoulli units in terms of the model log-likelihood (Section 4)."
    }, {
      "heading" : "2 RESTRICTED BOLTZMANN MACHINE AND RELU",
      "text" : "The Boltzmann distribution is defined as p(x) = e−E(x)/Z where Z = ∑ x e −E(x) is the partition function. Restricted Boltzmann Machine (RBM) is a Boltzmann distribution with a bipartite structure It is also the building block for many deep models (e.g., Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009), which are widely used in numerous applications (Bengio, 2009). The conventional Bernoulli RBM, models the joint probability p(v, h) for the visible units v ∈ [0, 1]I and the hidden units h ∈ [0, 1]J as p(v, h) ∝ exp(−E(v, h)), where E(v, h) = a>v − v>Wh + b>h. The parameters are a ∈ RI , b ∈ RJ and W ∈ RI×J . We can derive the conditional probabilities as\np(vi = 1|h) = σ  J∑ j=1 Wijhj + ai  and p(hj = 1|v) = σ( I∑ i=1 Wijvi + bj ) , (1)\nwhere σ(x) = (1 + e−x)−1 is the sigmoid function.\nOne extension of Bernoulli RBM is replacing the binary visible units by linear units v ∈ RI with independent Gaussian noise. The energy function in this case is given by\nE(v, h) = I∑ i=1 (vi − ai)2 2σ2i − I∑ i=1 J∑ j=1 vi σi Wijhj + b >h.\nTo simplify the notation, we assume a normalized data so that ai and σi is no longer required. The energy function is accordingly simplified to E(v, h) = ‖v‖ 2\n2 − v >Wh + b>h (Note that the\nelimination does not influence the discussion and one can easily extend all the results in this paper to the model that includes ai and σi.).\nThe conditional distributions are as follows:\np(vi|h) = N  J∑ j=1 Wijhj , 1  and p(hj = 1|v) = σ( I∑ i=1 Wijvi + bj ) , (2)\nwhere N (µ, V ) is a Gaussian distribution with mean µ and variance V . To simplify the notation, in the following we define ηj = ∑I i=1Wijvi + bj – that is ηj is the input to the j\nth hidden layer neuron and similarly define νi = ∑J j=1Wijhj + ai. Using this notation the conditionals in the (2) are p(vi|νi) = N (νi, 1) and p(hj = 1|ηj) = σ(ηj)."
    }, {
      "heading" : "2.1 RELU RBM WITH CONTINUOUS VISIBLE UNITS",
      "text" : "From (1) and (2), we can see that the mean of the p(hj |v) is the nonlinearity of the hidden unit at ηj = ∑I i=1Wijvi + bj – e.g., mean of the Bernoulli unit is the sigmoid function. From this perspective, we can extend the sigmoid function to other functions and thus allow RBM to have more expressive power (Ravanbakhsh et al., 2016). In particular, it would be interesting to use rectified linear unit (ReLU) nonlinearity, f(ηj) = max(0, ηj), for generative modeling.\nNair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining. They suggest sampling from max(0, ηj+N (0, σ(ηj)) for conditional sampling from the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods such as Annealed Importance Sampling that require access to this parametric form. In fact, only strictly monotonic activation functions can derive feasible joint and conditional distributions in the exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar activation functions that are monotonic are Softplus, f(ηj) = log(1 + eηj ) and leaky ReLU (Maas et al., 2013), defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) is the leakiness parameter. In contrast to the ReLU RBM the joint parametric form of these two distributions are available. However, the energy (logarithm of the joint probability) in the case of Softplus activation function contains a polylogarithmic term that requires evaluation of an infinite series; see Table 1 in Ravanbakhsh et al. (2016). For this reason, here we focus on Leaky-ReLU activation function.\nBy Ravanbakhsh et al. (2016), the conditional probability of the activation, assuming the nonlinearity f(ηj), is generally defined as p(hj |v) = exp (−Df (ηj‖hj) + g(hj)), where Df (ηj‖hj) is the Bregman Divergence associated with f, and g(hj) is the base (or carrier) measure in the exponential family which ensures the distribution is well-defined. The Bergman divergence, for strictly monotonic function f , is Df (ηj‖hj) = −ηjhj + F (ηj) + F ∗(hj), where F with ddηj F (ηj) = f(ηj) is the anti-derivative (integral) of f and F ∗ is the anti-derivative of f−1 (i.e., f−1(f(η)) = η); Note that due to the strict monotonicity of f , f−1 is well-defined, and F and F ∗ are commonly referred to as conjugate duals.\nConsidering the leaky ReLU activation function f(η) = max(cη, η), using this formalism, the conditional distributions of hidden units in the leaky RBM simplifies to (see Appendix A.1 for details)\np(hj |v) = { N (ηj , 1), if ηj > 0 N (cηj , c), if ηj ≤ 0.\n(3)\nSince the visible units uses the identity function, the corresponding conditional distribution is a Gaussian1\np(vi|h) = N  J∑ j=1 Wijhj , 1  , (4) Having these two conditional distributions is enough for training a leaky RBM model using contrastive divergence (Hinton, 2002) or some other alternatives (e.g., Tieleman, 2008; Tieleman & Hinton, 2009)."
    }, {
      "heading" : "3 TRAINING AND SAMPLING FROM LEAKY RBM",
      "text" : "Given the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model is (Yang et al., 2012; Ravanbakhsh et al., 2016)\np(v, h) ∝ exp v>Wh− I∑ i=1 (F̃ ∗(vi) + g(vi))− J∑ j=1 (F ∗(hj) + g(hj))  , (5) where F̃ ∗(vi) and F ∗(hj) are anti-derivatives of the inverses of the activation functions f̃(vi) and f(hj) for visible units vi and hidden units hj , respectively (see Section 2.1). Assuming f(ηj) = max(cηj , c) and f̃(νi) = νi in leaky-ReLU RBM, the joint distribution above becomes (see Appendix A.2 for details)\np(v, h) ∝ exp v>Wh− ‖v‖2 2 − ∑ ηj>0 ( h2j 2 + log √ 2π ) − ∑ ηj≤0 ( h2j 2c + log √ 2cπ ) + b>h  , 1which can also be written as p(vi|h) = exp ( −Df̃ (νi‖vi) + g(vi) ) , where νi = ∑ j=1Wijhj and\nf̃(νi) = νi and Df̃ (νi‖vi) = (νi − vi) 2 and g(vi) = − log\n√ 2π.\nand the corresponding visible marginal distribution is\np(v) ∝ exp −1 2 v> I −∑ ηj>0 WjW > j − c ∑ ηj≤0 WjW > j  v + ∑ ηj>0 bjW > j v + c ∑ ηj≤0 bjW > j v  . (6)\nwhere Wj is the j-th column of W ."
    }, {
      "heading" : "3.1 LEAKY RBM AS UNION OF TRUNCATED GAUSSIAN DISTRIBUTIONS",
      "text" : "From (6) we see that the marginal probability is determined by the affine constraints ηj > 0 or ηj ≤ 0 for all hidden units j. By combinatorics, these constraints divide RI (the visible domain) into at most M = ∑I i=1 ( J i ) convex regions R1, · · ·RM . An example with I = 2 and J = 3 is shown in Figure 1. If I > J , then we have at most 2J regions.\nWe discuss the two types of these regions. For bounded regions, such as R1 in Figure 1, the integration of (6) is also bounded, which results in a valid distribution. Before we discuss the unbounded cases, we define Ω = I − ∑J j=1 αjWjW > j , where αj = 1ηj>0 + c1ηj≤0. For the unbounded region, if Ω ∈ RI×I is a positive definite (PD) matrix, then the probability density is proportional to a multivariate Gaussian distribution with mean µ = Ω−1 (∑J j=1 αjbjWj ) and precision matrix Ω (covariance matrix Ω−1) but over an affine-constrained region. Therefore, the distribution of each unbounded region can be treated as a truncated Gaussian distribution. The marginal distrubution can be treated as a union of truncated Gaussain distribution. Note that leaky RBM is different from Su et al. (2017), which use single truncated Gaussian distribution to model joint (conditional) distributions and require approximated and more complicated sampling algorithms for truncated Gaussian distribution, while leaky RBM only requires to sample from Gaussian distributions.\nOn the other hand, if Ω is not PD, and the region Ri contains the eigenvectors with negative eigenvalues of Ω, the integration of (6) over Ri is divergent (infinite), which can not result in a valid probability distribution. In practice, with this type of parameter, when we do Gibbs sampling on the conditional distributions, the sampling will diverge. However, it is unfeasible to check exponentially many regions for each gradient update. Theorem 1. If I −WW> is positive definite, then I − ∑ j αjWjW > j is also positive definite, for all αj ∈ [0, 1].\nThe proof is shown in Appendix 1. From Theorem 1 we can see that if the constraint I −WW> is PD, then one can guarantee that the distribution of every region is a valid truncated Gaussian distribution. Therefore, we introduce the following projection step for each W after the gradient update.\nargmin W̃\n‖W − W̃‖2F\ns.t. I − W̃W̃> 0 (7)\nTheorem 2. The above projection step (7) can be done by shrinking the singular values to be less than 1.\nThe proof is shown in Appendix C. The training algorithm of the leaky RBM is shown in Algorithm 1. By using the projection step (7), we could treat the leaky RBM as the union of truncated Gaussian distributions, which uses weight vectors to divide the space of visible units into several regions and use a truncated Gaussian distribution to model each region. Note that the leaky RBM model is different from Su et al. (2016), which uses a truncated Gaussian distribution to model the conditional distribution p(h|v) instead of the marginal distribution. The empirical study about the divergent values and the necessity of the projection step is shown in Appendix D. Without the projection step, when we run Gibbs sampling for several iterations from the model, the sampled values will diverge because the model does not have a valid marginal distribution p(v). It also implies that we cannot train leaky RBM with larger CD steps without projection, which would result in divergent gradients. The detailed discussion is shown in Appendix D.\nAlgorithm 1 Training Leaky RBM for t = 1, . . . , T do\nEstimate gradient gθ by CD or other algorithms with (13) and (4), where θ = {W,a, b}. θ(t) ← θ(t−1) + ηgθ. Project W (t) by (7).\nend for"
    }, {
      "heading" : "3.2 SAMPLING FROM LEAKY-RELU RBM",
      "text" : "Gibbs sampling is the core procedure for RBM, including training, inference, and estimating the partition function (Fischer & Igel, 2012; Tieleman, 2008; Salakhutdinov & Murray, 2008). For every task, we start from randomly initializing v by an arbitrary distribution q, and iteratively sample from the conditional distributions. Gibbs sampling guarantees the procedure result in the stationary distribution in the long run for any initialized distribution q. However, if q is close to the target distribution p, it can significantly shorten the number of iterations to achieve the stationary distribution. If we set the leakiness c to be 1, then (6) becomes a simple multivariate Gaussian distribution N ( (I −WW>)−1Wb, (I −WW>)−1 ) , which can be easily sampled without Gibbs sampling. Also, the projection step (7) guarantees it is a valid Gaussian distribution. Then we decrease the leakiness with a small , and use samples from the multivariate Gaussian distribution when c = 1 as the initialization to do Gibbs sampling. Note that the distribution of each region is a truncated Gaussian distribution. When we only decrease the leakiness with a small amount, the resulted distribution is a “similar” truncated Gaussian distribution with more concentrated density. From this observation, we could expect the original multivariate Gaussian distribution serves as a good initialization. The one-dimensional example is shown in Figure 2. We then repeat this procedure until we reach the target leakiness. The algorithm can be seen as annealing the leakiness during the Gibbs sampling procedure. The meta algorithm is shown in Algorithm 2. Next, we show the proposed sampling algorithm can help both the partition function estimation and the training of leaky RBM.\nAlgorithm 2 Meta Algorithm for Sampling from Leaky RBM. Sample v from N ( (I −WW>)−1Wb, (I −WW>)−1 ) = (1− c)/T and c′ = 1 for t = 1, . . . , T do\nDecrease c′ = c′ − and perform Gibbs sampling by using (13) and (4) with leakiness c′ end for"
    }, {
      "heading" : "4 PARTITION FUNCTION ESTIMATION",
      "text" : "It is known that estimating the partition function of RBM is intractable (Salakhutdinov & Murray, 2008). Existing approaches, including Salakhutdinov & Murray (2008); Grosse et al. (2013); Liu et al. (2015); Carlson et al. (2016) focus on using sampling to approximate the partition function of the conventional Bernoulli RBM instead of the RBM with Gaussian visible units and non-Bernoulli hidden units. In this paper, we focus on extending the classic annealed importance sampling (AIS) algorithm (Salakhutdinov & Murray, 2008) to leaky RBM.\nAssuming that we want to estimate the partition function Z of p(v) with p(v) = p∗(v)/Z and p∗(v) ∝ ∑ h exp(−E(v, h)), Salakhutdinov & Murray (2008) start from a initial distribution\np0(v) ∝ ∑ h exp(−E0(v, h)), where computing the partition Z0 of p0(v) is tractable and we can draw samples from p0(v). They then use the “geometric path” to anneal the intermediate distribution as pk(v) ∝ p∗k(v) = ∑ h exp (−βkE0(v, h)− (1− βk)E(v, h)), where they grid βk from 1 to 0. If we let β0 = 1, we can draw samples vk from pk(v) by using samples vk−1 from pk−1(v) for k ≥ 1 via Gibbs sampling. The partition function is then estimated via Z = Z0M ∑M i=1 ω (i), where\nω(i) = p∗1(v (i) 0 )\np∗0(v (i) 0 )\np∗2(v (i) 1 ) p∗1(v (i) 1 ) · · · p∗K−1(v (i) K−2) p∗K−2(v (i) K−2) p∗K(v (i) K−1) p∗K−1(v (i) K−1) , and βK = 0.\nSalakhutdinov & Murray (2008) use the initial distribution with independent visible units and without hidden units. We consider application of AIS to the leaky-ReLU case with E0(v, h) = ‖v‖2 2 , which results in a multivariate Gaussian distribution p0(v). Compared with the meta algorithm shown in Algorithm 2 which anneals between leakiness, AIS anneals between energy functions."
    }, {
      "heading" : "4.1 STUDY ON TOY EXAMPLES",
      "text" : "As we discussed in Section 3.1, leaky RBM with J hidden units is a union of 2J truncated Gaussian distributions. Here we perform a study on the leaky RBM with a small number hidden units. Since in this example the number of hidden units is small, we can integrate out all possible configurations of h. However, integrating a truncated Gaussian distribution with general affine constraints does not have analytical solutions, and several approximations have been developed (e.g., Pakman & Paninski, 2014). To compare our results with the exact partition function, we consider a special case that has the following form:\np(v) ∝ exp −1 2 v> I −∑ ηj>0 WjW > j − c ∑ ηj≤0 WjW > j  v  . (8)\nCompared to (6), it is equivalent to the setting where b = 0. Geometrically, everyWj passes through the origin. We further put the additional constraint Wi ⊥ Wj ,∀i 6= j. Therefore. we divide the whole space into 2J equally-sized regions. A three dimensional example is shown in Figure 3. Then the partition function of this special case has the analytical form\nZ = 1\n2J ∑ αj∈{1,c},∀j (2π)− I 2 ∣∣∣∣∣∣∣ I − J∑ j=1 αjWjW > j − 12 ∣∣∣∣∣∣∣ .\nWe randomly initialize W and use SVD to make columns orthogonal. Also, we scale ‖Wj‖ to satisfy I −WW> 0. The leakiness parameter is set to be 0.01. For Salakhutdinov & Murray (2008) (AIS-Energy), we use 105 particles with 105 intermediate distributions. For the proposed method (AIS-Leaky), we use only 104 particles with 103 intermediate distributions. In this small problem we study the cases when the model has 5, 10, 20 and 30 hidden units and 3072 visible units. The true log partition function logZ is shown in Table 1 and the difference between logZ and the estimates given by the two algorithms are shown in Table 2.\nFrom Table 1, we observe that AIS-Leaky has significantly better and more stable estimations than AIS-Energy especially and this gap increases as we increase the number of hidden units. AIS-Leaky achieves this with orders magnitude reduced computation –e.g., here it uses ∼.1% of resources used by conventional AIS. For example, when we increase J from 5 to 30, the bias (difference) of AIS-Leaky only increases from 0.02 to 0.13; however, the bias of AIS-Energy increases from 1.76 to 9.6. We further study the implicit connection between the proposed AIS-Leaky and AIS-Energy in Appendix E, which shows AIS-Leaky is a special case of AIS-Energy under certain conditions."
    }, {
      "heading" : "4.2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM",
      "text" : "It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results).\nFortunately, our accurate estimate of the partition function for leaky RBM can produce a reliable quantitative estimate of the representation power of leaky RBM. We compare the BernoulliGaussian RBM2, which has Bernoulli hidden units and Gaussian visible units. We trained both models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized W by sampling from Unif(0, 0.01), a = 0, b = 0 and σ = 1. The momentum parameter was 0.9 and the batch size was set to 100. We tuned the learning rate between 10−1 and 10−6. We studied two benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3.\nFrom Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the performance of Bernoulli-Gaussian RBM can be improved by ∼ 300 nats for both datasets. Also, increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help. Although Nair & Hinton (2010) demonstrate the power of ReLU in terms of reconstruction error and classification accuracy, it does not imply its superior generative capability. Our study confirms leaky RBM could have much better generative performance compared to Bernoulli-Gaussian RBM."
    }, {
      "heading" : "5 BETTER MIXING BY ANNEALING LEAKINESS",
      "text" : "In this section, we show the idea of annealing between leakiness benefit the mixing in Gibbs sampling in other settings. A common procedure for comparison of sampling methods for RBM is through visualization. Here, we are interested in more quantitative metrics and the practical benefits of improved sampling. For this, we consider optimization performance as the evaluation metric.\nThe gradient of the log-likelihood function L(θ|vdata) of general RBM models is\n∂L(θ|vdata) ∂θ = Eh|vdata\n[ ∂E(v, h)\n∂θ\n] − Ev,h [ ∂E(v, h)\n∂θ\n] . (9)\nSince the second expectation in (9) is usually intractable, different approximation algorithms are used (Fischer & Igel, 2012).\n2Our GPU implementation with gnumpy and cudamat can reproduce the results of http://www.cs.toronto.edu/ tang/code/GaussianRBM.m\n3CD-n means that contrastive divergence was run for n steps\nIn this section, we compare two gradient approximation procedures. The baselines are the conventional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman, 2008) (PCD). The second method is using Algorithm 2 (Leaky) with the same number of mixing steps as CD. The experiment setup is the same as that of Section 4.\nThe results are shown in Figure 4. The proposed sampling procedure is slightly better than typical CD steps. The reason is we only anneals the leakiness for 20 steps. To get accurate estimation requires thousands of steps as shown in Section 4 when we estimate the partition function. Therefore, the estimated gradient is still inaccurate. However, it still outperforms the conventional CD algorithm. On the other hand, unlike the binary RBM case shown in Tieleman (2008), PCD does not outperform CD with 20 mixing steps for leaky RBM.\nThe drawback of Algorithm 2 is that sampling v from N ( (I −WW>)−1Wb, (I −WW>)−1 ) requires computing mean, covariance and the Cholesky decomposition of the covariance matrix in every iteration, which are computationally expensive. We study a mixture algorithm by combining CD and the idea of annealing leakiness. The mixture algorithm replaces the sampling from N ( (I −WW>)−1Wb, (I −WW>)−1 ) with sampling from the empirical data distribution. The resulted mix algorithm is almost the same as CD algorithm while it anneals the leakiness over the iterations as Algorithm 2. The results of the mix algorithm is also shown in Figure 4.\nThe mix algorithm is slightly worse than the original leaky algorithm, but it also outperforms the conventional CD algorithm without additional computation cost. The comparison in terms of CPU time is shown in Appendix F. Annealing the leakiness helps the mix algorithm explore different modes of the distribution, thereby improves the training. The idea could also be combined with more advanced algorithms (Tieleman, 2008; Tieleman & Hinton, 2009)4."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we study the properties of the exponential family distribution produced by leaky RBM. This study relates the leaky RBM model and truncated Gaussian distribution and reveals an underlying positive definite constraint of training leaky RBM. We further proposed a meta sampling algorithm, which anneals between leakiness during the Gibbs sampling procedure. We first demonstrate the proposed sampling algorithm is significantly more effective and efficient in estimating the partition function than the conventional AIS algorithm. Second, we show that the proposed sampling algorithm has comparatively better mixing properties (compared to CD). A few direction are worth further study; in particular we are investigating on speeding up the naive projection step; either using the barrier function as shown in Hsieh et al. (2011) or by eliminating the need for projection by artificially bounding the domain via additional constraints.\n4We studied the PCD extension of the proposed sampling algorithm. However, the performance is not as stable as CD."
    }, {
      "heading" : "A DERIVATION OF LEAKY (RELU) RBM",
      "text" : "A.1 CONDITIONAL DISTRIBUTIONS\nFor leaky RBM, the activation function of hidden units is defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) and ηj = ∑I i=1Wijvi + bj . The inverse function of f is f\n−1(hj) = min(hj , hj/c). Therefore, the anti-derivatives are\nF (ηj) =\n{ 1 2η 2 j , if ηj > 0\nc 2η 2 j , else,\n(10)\nand\nF ∗(hj) =\n{ 1 2h 2 j , if ηj > 0\n1 2ch 2 j , else.\n(11)\nThe activation function of Gaussian visible units can be treated as the linear unit f̃(νi) = νi, where νi = ∑J j=1Wijhj . Following the similar steps for deriving F and F\n∗, we get the anti-derivatives F̃ (νi) = 1 2ν 2 i and F̃ ∗(vi) = 1 2v 2 i .\nFrom Ravanbakhsh et al. (2016), the conditional distribution is defined as\np(hj |ηj) = exp (−ηjhj + F (ηj) + F ∗(hj)) (12)\nBy plugging F and F ∗ into (12), we get the conditional distribution for leaky RBM\np(hj |v) = { N (ηj , 1)with g(hj) = − log( √ 2π), if ηj > 0\nN (cηj , c)with g(hj) = − log( √ 2cπ), if ηj ≤ 0. (13)\nSimilarly, we have p(vi|νi) = N (νi, 1) with g(vi) = − log( √ 2π).\nA.2 JOINT AND MARGINAL DISTRIBUTIONS\nGiven the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model given by Yang et al. (2012) is\np(v, h) ∝ exp v>Wh− I∑ i=1 (F̃ ∗(vi) + g(vi))− J∑ j=1 (F ∗(hj) + g(hj))  , (14) By plugging F ∗, F̃ ∗ and g from Section A.1 into (14), we have\np(v, h) ∝ exp v>Wh− ‖v‖2 2 − ∑ ηj>0 ( h2j 2 + log √ 2π ) − ∑ ηj≤0 ( h2j 2c + log √ 2cπ ) + b>h  , Then the marginal distribution is\np(v) ∝ ∫ h p(v, h)dh\n∝ ∫ h exp ( −‖v‖ 2 2 ) ∏ ηj>0 exp ( − h2j 2 + ηjhj − log √ 2π ) ∏ ηj≤0 ( − h2j 2c + hjηj − log √ 2cπ ) dh\n∝ exp ( −‖v‖ 2\n2 ) ∏ ηj>0 exp ( η2j 2 ) ∏ ηj≤0 ( cη2j 2 )\n∝ exp −1 2 v> I −∑ ηj>0 WjW > j − c ∑ ηj≤0 WjW > j  v + ∑ ηj>0 bjW > j v + c ∑ ηj≤0 bjW > j v  ."
    }, {
      "heading" : "B PROOF OF THEOREM 1",
      "text" : "Proof. Since WW>− ∑ j αjWjWj = ∑ j(1−αj)WjW>j 0, we have WW> ∑ j αjWjWj .\nTherefore, I − ∑ j αjWjW > j I −WW> 0."
    }, {
      "heading" : "C PROOF OF THEOREM 2",
      "text" : "Proof. Let the SVD decomposition of W and W̃ as W = USV > and W̃ = Ũ S̃Ṽ >. Then we have\n‖W − W̃‖2F = ‖USV > − Ũ S̃Ṽ >‖2F ≥ I∑ i=1 (Sii − S̃ii)2, (15)\nand the constraint I − W̃W̃> 0 can be rewritten as 0 ≤ S̃ii ≤ 1,∀i. The transformed problem has a Lasso-like formulation and we can solve it by S̃ii = min(Sii, 1) (Parikh & Boyd, 2014). Also, the lower bound ∑I i=1(Sii − S̃ii)2 in (15) becomes a tight bound when we set Ũ = U and Ṽ = V , which completes the proof."
    }, {
      "heading" : "D NECESSITY OF THE PROJECTION STEP",
      "text" : "We conduct a short comparison to demonstrate the projection step is necessary for the leaky RBM on generative tasks. We train two leaky RBM as follows. The first model is trained by the same setting in Section 4. We use the convergence of log likelihood as the stopping criteria. The second model is trained by CD-1 with weight decay and without the projection step. We stop the training when the reconstruction error is less then 10−2. After we train these two models, we run Gibbs sampling with 1000 independent chains for several steps and output the average value of the visible units. Note that the visible units are normalized to zero mean. The results on SVHN and CIFAR10 are shown in Figure 5.\nFrom Figure 5, the model trained by weight decay without projection step is suffered by the problem of the diverged values. It confirms the study shown in Section 3.1. It also implies that we cannot\ntrain leaky RBM with larger CD steps when we do not do projection; otherwise, we would have the diverged gradients. Therefore, the projection is necessary for training leaky RBM for the generative purpose. However, we also oberseve that the projection step is not necessary for the classification and reconstruction tasks. he reason may be the independency of different evaluation criteria (Hinton, 2012; Theis et al., 2016) or other implicit reasons to be studied."
    }, {
      "heading" : "E EQUIVALENCE BETWEEN ANNEALING THE ENERGY AND LEAKINESS",
      "text" : "We analyze the performance gap between AIS-Leaky and AIS-Energy. One major difference is the initial distribution. The intermediate marginal distribution of AIS-Energy has the following form:\npk(v) ∝ exp −1 2 v> I − (1− βk) ∑ ηj>0 WjW > j − (1− βk)c ∑ ηj≤0 WjW > j  v  . (16)\nHere we eliminated the bias terms b for simplicity. Compared with Algorithm 2, (16) not only anneals the leakiness (1 − βk)c ∑ ηj≤0WjW > j when ηj ≤ 0, but also in the case (1 −\nβk) ∑ ηj>0 WjW > j when ηj > 0, which brings more bias to the estimation. In other words, AIS-Leaky is a one-sided leakiness annealing while AIS-Energy is a two-sided leakiness annealing method.\nTo address the higher bias problem of AIS-Energy, we replace the initial distribution with the one used in Algorithm 2. By elementary calculation, the marginal distribution becomes\npk(v) ∝ exp −1 2 v> I −∑ ηj>0 WjW > j − (βk + (1− βk)c) ∑ ηj≤0 WjW > j  v  , (17)\nwhich recovers the proposed Algorithm 2. From this analysis, we understand AIS-Leaky is a special case of conventional AIS-Energy with better initialization inspired by the study in Section 3. Also, by this connection between AIS-Energy and AIS-Leaky, we note that AIS-Leaky can be combined with other extensions of AIS (Grosse et al., 2013; Burda et al., 2015) as well."
    }, {
      "heading" : "F MORE EXPERIMENTAL RESULTS FOR SAMPLING",
      "text" : "F.1 SAMPLED IMAGES\nWe show the sampled images from leaky RBM train on CIFAR10 and SVHN datasets. We randomly initialize 20 chains and run Gibbs sampling for 1000 iterations. The sampled results are shown in Figure 6 The results shows that single layer RBM does not adequately model CIFAR10 and SVHN\nwhen compared to multilayer models. The similar results for single layer Bernoulli-Gaussian RBM from Ranzato & Hinton (2010) (in gray scale) is shown in Figure 7. Therefore, we instead focused on quantitative evaluation of the log-likelihood in Table 3.\nF.2 COMPUTATIONAL TIME BETWEEN DIFFERENT SAMPLING STRATEGIES\nThe comparison in terms of CPU time of different sampling algorithms discussed in Section 5 is shown in Figure 8. Please note that the complexity of CD and Mix are the almost the same. Mix only need a few more constant time steps which can be ignored compared with sampling steps. Leaky is more time-consuming because of computing and decomposing the covariance matrix as we discussed in Section 5. We also report the execution time of each step of algorithms in Table 4.\nF.3 STUDY ON RELU-BERNOULLI RBM\nWe study the idea of annealing leakiness on the RBM model with leaky ReLU hidden units and Bernoulli visible units. We create the toy dataset with 20, 25 and 30 visible units as shown in Figure 9. The small datasets allow exact computation of the partition function. For each dataset, we sample 60,000 images for training and 10,000 images for testing. We use 100 hidden units and PCD to train the model. The log likelihood results are shown in Table 5.\nCompared to the Gaussian visible units case we study in Section 3, where p(v) is a multi-variate Gaussian distribution when c = 1, the partition function of p(v) in ReLU-Bernoulli when c = 1 does not have the analytical form. Therefore, we do the following two-stage alternative. We first run the standard AIS algorithm, which anneals the energy, to the distribution with leakiness c = 1. We then change to anneals the leakiness from 1 to the target value. For the typical AIS algorithm (AIS-Energy), we use 104 chains with 2 × 104 intermediate distributions. For the proposed twostaged algorithm (AIS-Leaky), we use 104 chains with 104 intermediate distributions for annealing to c = 1 and the other 104 distributions for annealing the leakiness. The results are shown in Table 6.\nIn Table 6, the standard AIS algorithm (AIS-Energy) has unsatisfactory performance. We show the performance of AIS for estimating the partition function of models with different leakiness on Toy20. We use the 104 independent chains and 2 × 104 intermediate distributions. The results are shown in Table 7. From Table 7, we observe that the AIS performances worse when the leakiness is closer to 0. Although we observed that increasing chains and intermediate distributions could improve the performance, but the improvements are limited. The study demonstrates when the\n(a) I = 20\n(b) I = 25\nnon-linearity of the distribution increases (the leakiness value c decreases), the standard AIS cannot effectively estimate the partition function within feasible computational time. On the other hand, it also confirm the proposed idea, annealing the leakiness, can serve as an effective building block for algorithms without enhancing the algorithm complexity. Note that the unsatisfactory performance of AIS may be addressed by Grosse et al. (2013). From Appendix E, the two-stage algorithm used here can also be improved by applying Grosse et al. (2013).\nF.3.1 MNIST AND CALTECH DATASETS\nWe study MNIST and Caltech 101 Silhouettes datasets with 500 hidden units and train the model with CD-25. The results are shown in Table 8 and Table 9. The leaky RBM is better than conventional Bernoulli RBM and some deep models on MNIST data. Although leaky RBM deos not outperform Su et al. (2017), but it enjoys the advantage of the simpler sampling procedure (Gaussian distribution vs truncated Gaussian distribution) in the binary visible unit case."
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Y. Bengio" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "Bengio.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "Reweighted wake-sleep",
      "author" : [ "Jörg Bornschein", "Yoshua Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Bornschein and Bengio.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bornschein and Bengio.",
      "year" : 2015
    }, {
      "title" : "Accurate and conservative estimates of mrf loglikelihood using reverse annealing",
      "author" : [ "Y. Burda", "R.B. Grosse", "R. Salakhutdinov" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Burda et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2015
    }, {
      "title" : "Partition functions from rao-blackwellized tempered sampling",
      "author" : [ "D.E. Carlson", "P. Stinson", "A. Pakman", "L. Paninski" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Carlson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2016
    }, {
      "title" : "Enhanced gradient for training restricted boltzmann machines",
      "author" : [ "KyungHyun Cho", "Tapani Raiko", "Alexander Ilin" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Cho et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2013
    }, {
      "title" : "An introduction to restricted boltzmann machines",
      "author" : [ "A. Fischer", "C. Igel" ],
      "venue" : "In CIARP,",
      "citeRegEx" : "Fischer and Igel.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fischer and Igel.",
      "year" : 2012
    }, {
      "title" : "Unsupervised learning of distributions on binary vectors using two layer networks",
      "author" : [ "Y. Freund", "D. Haussler" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Freund and Haussler.,? \\Q1994\\E",
      "shortCiteRegEx" : "Freund and Haussler.",
      "year" : 1994
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "In ICML",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Annealing between distributions by averaging moments",
      "author" : [ "R.B. Grosse", "C.J. Maddison", "R. Salakhutdinov" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Grosse et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Grosse et al\\.",
      "year" : 2013
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hinton.",
      "year" : 2002
    }, {
      "title" : "A practical guide to training restricted boltzmann machines",
      "author" : [ "G.E. Hinton" ],
      "venue" : "In Neural Networks: Tricks of the Trade (2nd ed.)",
      "citeRegEx" : "Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton.",
      "year" : 2012
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Sparse inverse covariance matrix estimation using quadratic approximation",
      "author" : [ "C.-J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P. Ravikumar" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2011
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Estimating the partition function by discriminance sampling",
      "author" : [ "Q. Liu", "J. Peng", "A. Ihler", "J. Fisher III" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "A.L. Maas", "A.Y. Hannun", "A.Y. Ng" ],
      "venue" : "In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing,",
      "citeRegEx" : "Maas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2013
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Exact hamiltonian monte carlo for truncated multivariate gaussians",
      "author" : [ "A. Pakman", "L. Paninski" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Pakman and Paninski.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pakman and Paninski.",
      "year" : 2014
    }, {
      "title" : "Modeling pixel means and covariances using factorized third-order boltzmann machines",
      "author" : [ "M. Ranzato", "G.E. Hinton" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Ranzato and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ranzato and Hinton.",
      "year" : 2010
    }, {
      "title" : "Stochastic neural networks with monotonic activation functions",
      "author" : [ "S. Ravanbakhsh", "B. Póczos", "J.G. Schneider", "D. Schuurmans", "R. Greiner" ],
      "venue" : null,
      "citeRegEx" : "Ravanbakhsh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ravanbakhsh et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Boltzmann machines",
      "author" : [ "R. Salakhutdinov", "G. Hinton" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Salakhutdinov and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2009
    }, {
      "title" : "On the quantitative analysis of Deep Belief Networks",
      "author" : [ "R. Salakhutdinov", "I. Murray" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Salakhutdinov and Murray.,? \\Q2008\\E",
      "shortCiteRegEx" : "Salakhutdinov and Murray.",
      "year" : 2008
    }, {
      "title" : "Nonlinear statistical learning with truncated gaussian graphical models",
      "author" : [ "Q. Su", "X. Liao", "C. Chen", "L. Carin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Su et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised learning with truncated gaussian graphical models",
      "author" : [ "Qinliang Su", "Xuejun Liao", "Chunyuan Li", "Zhe Gan", "Lawrence Carin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Su et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2017
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "L. Theis", "A. van den Oord", "M. Bethge" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Theis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2016
    }, {
      "title" : "Training restricted boltzmann machines using approximations to the likelihood gradient",
      "author" : [ "T. Tieleman" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Tieleman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tieleman.",
      "year" : 2008
    }, {
      "title" : "Using Fast Weights to Improve Persistent Contrastive Divergence",
      "author" : [ "T. Tieleman", "G.E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2009
    }, {
      "title" : "Exponential family harmoniums with an application to information retrieval",
      "author" : [ "M. Welling", "M. Rosen-Zvi", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Welling et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Welling et al\\.",
      "year" : 2004
    }, {
      "title" : "Graphical models via generalized linear models",
      "author" : [ "E. Yang", "P. Ravikumar", "G.I. Allen", "Z. Liu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Yang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2012
    }, {
      "title" : "2016), the conditional distribution is defined as p(hj |ηj",
      "author" : [ "From Ravanbakhsh" ],
      "venue" : null,
      "citeRegEx" : "Ravanbakhsh,? \\Q2016\\E",
      "shortCiteRegEx" : "Ravanbakhsh",
      "year" : 2016
    }, {
      "title" : "Therefore, we instead focused on quantitative evaluation of the log-likelihood in Table 3. F.2 COMPUTATIONAL TIME BETWEEN DIFFERENT SAMPLING STRATEGIES The comparison in terms of CPU time of different sampling algorithms discussed in Section 5 is shown in Figure 8. Please note that the complexity of CD and Mix are the almost the same",
      "author" : [ "Ranzato", "Hinton" ],
      "venue" : null,
      "citeRegEx" : "Ranzato and Hinton,? \\Q2010\\E",
      "shortCiteRegEx" : "Ranzato and Hinton",
      "year" : 2010
    }, {
      "title" : "From Appendix E, the two-stage algorithm used here can also be improved by applying",
      "author" : [ "Grosse" ],
      "venue" : null,
      "citeRegEx" : "Grosse,? \\Q2013\\E",
      "shortCiteRegEx" : "Grosse",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "One may naively classify these models into a family of directed deep generative models trainable by back-propagation (e.g., Kingma & Welling, 2013; Goodfellow et al., 2014), and deep energy-based models, such as deep belief network (Hinton et al.",
      "startOffset" : 117,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : ", 2014), and deep energy-based models, such as deep belief network (Hinton et al., 2006) and deep Boltzmann machine (Salakhutdinov & Hinton, 2009).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "For hidden units, we can also generalize Bernoulli units to the exponential family (Welling et al., 2004; Ravanbakhsh et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "For hidden units, we can also generalize Bernoulli units to the exponential family (Welling et al., 2004; Ravanbakhsh et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "Unfortunately, due to its lack of strict monotonicity, ReLU RBM does not fit within the framework of exponential family RBMs (Ravanbakhsh et al., 2016).",
      "startOffset" : 125,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : ", Kingma & Welling, 2013; Goodfellow et al., 2014), and deep energy-based models, such as deep belief network (Hinton et al., 2006) and deep Boltzmann machine (Salakhutdinov & Hinton, 2009). The building block of deep energy-based models is a bipartite graphical model called restricted Boltzmann machine (RBM). The RBM model consists of two layers, visible and hidden. The resulting graphical model which can account for higher-order interactions of the visible units (visible layer) using the hidden units (hidden layer). It also makes the inference easier that there are no interactions between the variables in each layer. The conventional RBM uses Bernoulli units for both the hidden and visible units (Smolensky, 1986). One extension is using Gaussian visible units to model general natural images (Freund & Haussler, 1994). For hidden units, we can also generalize Bernoulli units to the exponential family (Welling et al., 2004; Ravanbakhsh et al., 2016). Nair & Hinton (2010) propose a variation using Rectified Linear Unit (ReLU) for the hidden layer with a heuristic sampling procedure, which has promising performance in terms of reconstruction error and classification accuracy.",
      "startOffset" : 26,
      "endOffset" : 985
    }, {
      "referenceID" : 14,
      "context" : "Restricted Boltzmann Machine (RBM) is a Boltzmann distribution with a bipartite structure It is also the building block for many deep models (e.g., Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009), which are widely used in numerous applications (Bengio, 2009).",
      "startOffset" : 141,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : ", 2009), which are widely used in numerous applications (Bengio, 2009).",
      "startOffset" : 56,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "From this perspective, we can extend the sigmoid function to other functions and thus allow RBM to have more expressive power (Ravanbakhsh et al., 2016).",
      "startOffset" : 126,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "Similar activation functions that are monotonic are Softplus, f(ηj) = log(1 + ej ) and leaky ReLU (Maas et al., 2013), defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) is the leakiness parameter.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "Nair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining.",
      "startOffset" : 7,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Nair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining. They suggest sampling from max(0, ηj+N (0, σ(ηj)) for conditional sampling from the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods such as Annealed Importance Sampling that require access to this parametric form. In fact, only strictly monotonic activation functions can derive feasible joint and conditional distributions in the exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar activation functions that are monotonic are Softplus, f(ηj) = log(1 + ej ) and leaky ReLU (Maas et al.",
      "startOffset" : 7,
      "endOffset" : 675
    }, {
      "referenceID" : 9,
      "context" : "Nair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining. They suggest sampling from max(0, ηj+N (0, σ(ηj)) for conditional sampling from the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods such as Annealed Importance Sampling that require access to this parametric form. In fact, only strictly monotonic activation functions can derive feasible joint and conditional distributions in the exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar activation functions that are monotonic are Softplus, f(ηj) = log(1 + ej ) and leaky ReLU (Maas et al., 2013), defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) is the leakiness parameter. In contrast to the ReLU RBM the joint parametric form of these two distributions are available. However, the energy (logarithm of the joint probability) in the case of Softplus activation function contains a polylogarithmic term that requires evaluation of an infinite series; see Table 1 in Ravanbakhsh et al. (2016). For this reason, here we focus on Leaky-ReLU activation function.",
      "startOffset" : 7,
      "endOffset" : 1192
    }, {
      "referenceID" : 9,
      "context" : "Nair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining. They suggest sampling from max(0, ηj+N (0, σ(ηj)) for conditional sampling from the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods such as Annealed Importance Sampling that require access to this parametric form. In fact, only strictly monotonic activation functions can derive feasible joint and conditional distributions in the exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar activation functions that are monotonic are Softplus, f(ηj) = log(1 + ej ) and leaky ReLU (Maas et al., 2013), defined as f(ηj) = max(cηj , ηj), where c ∈ (0, 1) is the leakiness parameter. In contrast to the ReLU RBM the joint parametric form of these two distributions are available. However, the energy (logarithm of the joint probability) in the case of Softplus activation function contains a polylogarithmic term that requires evaluation of an infinite series; see Table 1 in Ravanbakhsh et al. (2016). For this reason, here we focus on Leaky-ReLU activation function. By Ravanbakhsh et al. (2016), the conditional probability of the activation, assuming the nonlinearity f(ηj), is generally defined as p(hj |v) = exp (−Df (ηj‖hj) + g(hj)), where Df (ηj‖hj) is the Bregman Divergence associated with f, and g(hj) is the base (or carrier) measure in the exponential family which ensures the distribution is well-defined.",
      "startOffset" : 7,
      "endOffset" : 1288
    }, {
      "referenceID" : 9,
      "context" : "Having these two conditional distributions is enough for training a leaky RBM model using contrastive divergence (Hinton, 2002) or some other alternatives (e.",
      "startOffset" : 113,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "3 TRAINING AND SAMPLING FROM LEAKY RBM Given the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model is (Yang et al., 2012; Ravanbakhsh et al., 2016) p(v, h) ∝ exp v>Wh− I ∑",
      "startOffset" : 169,
      "endOffset" : 214
    }, {
      "referenceID" : 20,
      "context" : "3 TRAINING AND SAMPLING FROM LEAKY RBM Given the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model is (Yang et al., 2012; Ravanbakhsh et al., 2016) p(v, h) ∝ exp v>Wh− I ∑",
      "startOffset" : 169,
      "endOffset" : 214
    }, {
      "referenceID" : 23,
      "context" : "Note that leaky RBM is different from Su et al. (2017), which use single truncated Gaussian distribution to model joint (conditional) distributions and require approximated and more complicated sampling algorithms for truncated Gaussian distribution, while leaky RBM only requires to sample from Gaussian distributions.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 23,
      "context" : "Note that the leaky RBM model is different from Su et al. (2016), which uses a truncated Gaussian distribution to model the conditional distribution p(h|v) instead of the marginal distribution.",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 26,
      "context" : "2 SAMPLING FROM LEAKY-RELU RBM Gibbs sampling is the core procedure for RBM, including training, inference, and estimating the partition function (Fischer & Igel, 2012; Tieleman, 2008; Salakhutdinov & Murray, 2008).",
      "startOffset" : 146,
      "endOffset" : 214
    }, {
      "referenceID" : 7,
      "context" : "Existing approaches, including Salakhutdinov & Murray (2008); Grosse et al. (2013); Liu et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "Existing approaches, including Salakhutdinov & Murray (2008); Grosse et al. (2013); Liu et al. (2015); Carlson et al.",
      "startOffset" : 62,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "(2015); Carlson et al. (2016) focus on using sampling to approximate the partition function of the conventional Bernoulli RBM instead of the RBM with Gaussian visible units and non-Bernoulli hidden units.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012).",
      "startOffset" : 153,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood.",
      "startOffset" : 154,
      "endOffset" : 328
    }, {
      "referenceID" : 9,
      "context" : "2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results).",
      "startOffset" : 154,
      "endOffset" : 557
    }, {
      "referenceID" : 9,
      "context" : "2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results). Fortunately, our accurate estimate of the partition function for leaky RBM can produce a reliable quantitative estimate of the representation power of leaky RBM. We compare the BernoulliGaussian RBM2, which has Bernoulli hidden units and Gaussian visible units. We trained both models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized W by sampling from Unif(0, 0.01), a = 0, b = 0 and σ = 1. The momentum parameter was 0.9 and the batch size was set to 100. We tuned the learning rate between 10−1 and 10−6. We studied two benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3. From Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the performance of Bernoulli-Gaussian RBM can be improved by ∼ 300 nats for both datasets. Also, increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help.",
      "startOffset" : 154,
      "endOffset" : 1937
    }, {
      "referenceID" : 9,
      "context" : "2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results). Fortunately, our accurate estimate of the partition function for leaky RBM can produce a reliable quantitative estimate of the representation power of leaky RBM. We compare the BernoulliGaussian RBM2, which has Bernoulli hidden units and Gaussian visible units. We trained both models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized W by sampling from Unif(0, 0.01), a = 0, b = 0 and σ = 1. The momentum parameter was 0.9 and the batch size was set to 100. We tuned the learning rate between 10−1 and 10−6. We studied two benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3. From Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the performance of Bernoulli-Gaussian RBM can be improved by ∼ 300 nats for both datasets. Also, increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help.",
      "startOffset" : 154,
      "endOffset" : 1963
    }, {
      "referenceID" : 9,
      "context" : "2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results). Fortunately, our accurate estimate of the partition function for leaky RBM can produce a reliable quantitative estimate of the representation power of leaky RBM. We compare the BernoulliGaussian RBM2, which has Bernoulli hidden units and Gaussian visible units. We trained both models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized W by sampling from Unif(0, 0.01), a = 0, b = 0 and σ = 1. The momentum parameter was 0.9 and the batch size was set to 100. We tuned the learning rate between 10−1 and 10−6. We studied two benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3. From Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the performance of Bernoulli-Gaussian RBM can be improved by ∼ 300 nats for both datasets. Also, increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help. Although Nair & Hinton (2010) demonstrate the power of ReLU in terms of reconstruction error and classification accuracy, it does not imply its superior generative capability.",
      "startOffset" : 154,
      "endOffset" : 2003
    }, {
      "referenceID" : 9,
      "context" : "The baselines are the conventional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman, 2008) (PCD).",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "The baselines are the conventional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman, 2008) (PCD).",
      "startOffset" : 116,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "The idea could also be combined with more advanced algorithms (Tieleman, 2008; Tieleman & Hinton, 2009)4.",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "The baselines are the conventional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman, 2008) (PCD). The second method is using Algorithm 2 (Leaky) with the same number of mixing steps as CD. The experiment setup is the same as that of Section 4. The results are shown in Figure 4. The proposed sampling procedure is slightly better than typical CD steps. The reason is we only anneals the leakiness for 20 steps. To get accurate estimation requires thousands of steps as shown in Section 4 when we estimate the partition function. Therefore, the estimated gradient is still inaccurate. However, it still outperforms the conventional CD algorithm. On the other hand, unlike the binary RBM case shown in Tieleman (2008), PCD does not outperform CD with 20 mixing steps for leaky RBM.",
      "startOffset" : 64,
      "endOffset" : 758
    }, {
      "referenceID" : 12,
      "context" : "A few direction are worth further study; in particular we are investigating on speeding up the naive projection step; either using the barrier function as shown in Hsieh et al. (2011) or by eliminating the need for projection by artificially bounding the domain via additional constraints.",
      "startOffset" : 164,
      "endOffset" : 184
    } ],
    "year" : 2017,
    "abstractText" : "Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to its numerical stability and quantifiability of its likelihood, RBM is commonly used with Bernoulli units. Here, we consider an alternative member of the exponential family RBM with leaky rectified linear units – called leaky RBM. We first study the joint and marginal distributions of the leaky RBM under different leakiness, which leads to interesting interpretation of the leaky RBM model as truncated Gaussian distribution. We then propose a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; – i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations. This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and far more accurate than the commonly used annealed importance sampling (AIS). We further demonstrate that the proposed sampling algorithm enjoys relatively faster mixing than contrastive divergence algorithm, which improves the training procedure without any additional computational cost.",
    "creator" : "LaTeX with hyperref package"
  }
}