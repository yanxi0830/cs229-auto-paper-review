{
  "name" : "537.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "RENDERGAN: GENERATING REALISTIC LABELED DATA",
    "authors" : [ "Leon Sixt", "Benjamin Wild", "Tim Landgraf" ],
    "emails" : [ "tim.landgraf}@fu-berlin.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "When an image is taken from a real world scene, many factors determine the final appearance: background, lighting, object shape, position and orientation of the object, the noise of the camera sensor, and more. In computer vision, high-level information such as class, shape, or pose is reconstructed from raw image data. Most real-world applications require the reconstruction to be invariant to noise, background, and lighting changes.\nIn recent years, deep convolutional neural networks (DCNNs) advanced to the state of the art in many computer vision tasks (Krizhevsky et al., 2012; He et al., 2015; Razavian et al., 2014). More training data usually increases the performance of DCNNs. While image data is mostly abundant, labels for supervised training must often be created manually – a time-consuming and tedious activity. For complex annotations such as human joint angles, camera viewpoint or image segmentation, the costs of labeling can be prohibitive.\nIn this paper, we propose a method to drastically reduce the costs of labeling such that we can train a model to predict even complex sets of labels. We present a generative model that can sample from the joint distribution of labels and data. The training procedure of our model does not require any manual labeling. We show that the generated data is of high quality and can be used to train a model in a supervised setting, i.e. a model that maps from real samples to labels, without using any manually labeled samples.\nWe propose two modifications to the recently introduced GAN framework (Goodfellow et al., 2014). First, a simple 3D model is embedded into the generator network to produce samples from corresponding input labels. Second, the generator learns to add missing image characteristics to the model output using a number of parameterized augmentation functions. In the adversarial training we leverage large amounts of unlabeled image data to learn the particular form of blur, lighting, background and image detail. By constraining the augmentation functions we ensure that the resulting image still represents the given set of labels. The resulting images are hard to distinguish from real samples and can be used to train a DCNN to predict the labels from real input data.\nThe RenderGAN framework was developed to solve the scarcity of labeled data in the BeesBook project (Wario et al., 2015) in which we analyze the social behavior of honeybees. A barcode-like marker is attached to the honeybees’ backs for identification (see Fig. 1). Annotating this data is tedious, and therefore only a limited amount of labeled data exists. A 3D model (see the upper row of Fig. 2) generates a simple image of the tag based on position, orientation, and bit configuration. The RenderGAN then learns from unlabeled data to add lighting, background, and image details.\nTraining a DCNN on data generated by the RenderGAN yields considerably better performance compared to various baselines. We furthermore include a previously used computer vision pipeline in the evaluation. The networks’ detections are used as feature to track the honeybees over time. When we use detections from the DCNN instead of the computer vision pipeline, the accuracy of assigning the true id increases from 55% to 96%.\nOur contributions are as follows. We present an extension of the GAN framework that allows to sample from the joint distribution of data and labels. The generated samples are nearly indistinguishable from real data for a human observer and can be used to train a DCNN end-to-end to classify real samples. In a real-world use case, our approach significantly outperforms several baselines. Our approach requires no manual labeling. The simple 3D model is the only form of supervision."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "There exists multiple approaches to reduce the costs associated with labeling.\nA common approach to deal with limited amount of labels is data augmentation (Goodfellow et al., 2016, Chapter 7.4). Translation, noise, and other deformations can often be applied without changing the labels, thereby effectively increasing the number of training samples and reducing overfitting.\nDCNNs learn a hierarchy of features – many of which are applicable to related domains (Yosinski et al., 2014). Therefore, a common technique is to pre-train a model on a larger dataset such as ImageNet (Deng et al., 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014). This technique only works in cases where a large enough related dataset exists. Furthermore, labeling enough data for fine-tuning might still be costly.\nIf a basic model of the data exists (for example, a 3D model of the human body), it can be used to generate labeled data. Peng et al. (2015) generated training data for a DCNN with 3D-CAD models.\nSu et al. (2015) used 3D-CAD models from large online repositories to generate large amounts of training images for the viewpoint estimation task on the PASCAL 3D+ dataset (Xiang et al., 2014). Massa et al. (2015) are matching natural images to 3D-CAD models with features extracted from a DCNN. Richter et al. (2016) and Ros et al. (2016) used 3D game engines to collect labeled data for image segmentation. However, the explicit modeling of the image acquisition physics (scene lighting, reflections, lense distortions, sensor noise, etc.) is cumbersome and might still not be able to fully reproduce the particularities of the imaging process such as unstructured background or object specific noise. Training a DCNN on generated data that misses certain features will result in overfitting and poor performance on the real data.\nGenerative Adversarial Networks (GAN) (see Fig. 3) can learn to generate high-quality samples (Goodfellow et al., 2014), i.e. sample from the data distribution p(x). Denton et al. (2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images. While a GAN implicitly learns a meaningful latent embedding of the data (Radford et al., 2015), there is no simple relationship between the latent dimensions and the labels of interest. Therefore, high-level information can’t be inferred from generated samples. cGANs are an extension of GANs to sample from a conditional distribution given some labels, i.e. p(x|l). However, training cGANs requires a labeled dataset. Springenberg (2015) showed that GANs can be used in a semi-supervised setting but restricted their analysis to categorical labels. Wang & Gupta (2016) trained two separate GANs, one to model the object normals and another one for the texture conditioned on the normals. As they rely on conditional GANs, they need large amounts of labeled data. Chen et al. (2016) used an information theoretic to disentangle the representation. They decomposed the representation into a structured and unstructured part. And successfully related on a qualitative level the structured part to high-level concepts such as camera viewpoint or hair style. However, explicitly controlling the relationship between the latent space and generated samples without using labeled data is an open problem, i.e. sampling from p(x, l) without requiring labels for training."
    }, {
      "heading" : "3 RENDERGAN",
      "text" : "Most supervised learning tasks can be modeled as a regression problem, i.e. approximating a function f̂ : Rn 7→ L that maps from data space R to label space L. We consider f̂ to be the best available function on this particular task. Analogous to ground truth data, one could call f̂ the ground truth function.\nIn the RenderGAN framework, we aim to solve the inverse problem to this regression task: generate data given the labels. This is achieved by embedding a simple 3D model into the generator of a GAN. The samples generated by the simple model must correspond to the given labels but may lack many factors of the real data such as background or lightning. Through a cascade of augmentation functions, the generator can adapt the images from the 3D model to match the real data.\nWe formalize image augmentation as a function φ(x, d), which modifies the image x based on the augmentation parameter d (a tensor of any rank). The augmentation must preserve the labels of the image x. Therefore, it must hold for all images x and all augmentations parameters d:\nf̂ (φ(x, d)) = f̂(x) (1)\nThe augmentation function must furthermore be differentiable w.r.t. x and d as the gradient will be back-propagated through φ to the generator. Image augmentations such as lighting, surrounding, and noise do preserve the labels and fit this definition. We will provide appropriate definitions of φ for the mentioned augmentations in the following section.\nIf appropriate augmentation functions are found that can model the missing factors and are differentiable, we can use the GAN framework to find parameters that result in realistic output images. Multiple augmentation functions can be combined to perform a more complex augmentation. Here, we will consider multiple augmentation functions applied sequentially, i.e. we have k augmentation functions φi and k corresponding outputs Gi from the generator. The output of the previous augmentation function is the input to the next one. Thus, we can write the generator given some labels l as:\ng(z, l) = φk(φk−1(. . . φ0(M(l), G0(z)) . . . , Gk−1(z)), Gk(z)) (2)\nwhere M is the 3D model. We can furthermore learn the label distribution with the generator. As the discriminator loss must be backpropagated through the 3D model M, it must be differentiable. This can be achieved by emulating the 3D model with a neural network (Dosovitskiy et al., 2015). The resulting generator g(z) can be written as (see Fig. 4 for a visual interpretation):\ng(z) = φk(φk−1(. . . φ0(M(Gl(z)), G0(z)) . . . , Gk−1(z)), Gk(z)) (3)\nAs any differentiable function approximator can be employed in the GAN framework, the theoretical properties still hold. The training is carried out as in the conventional GAN framework. In a real application, the augmentation functions might restrict the generator from converging to the data distribution.\nIf the training converges, we can collect generated realistic data with g(z) and the high-level information captured in the 3D model with Gl(z). We can now train a supervised learning algorithm on the labeled generated data (Gl (z) , g (z)) and solve the regression task of approximating f̂ without depending on manual labels."
    }, {
      "heading" : "4 APPLICATION TO THE BEESBOOK PROJECT",
      "text" : "In the BeesBook project, we aim to understand the complex social behavior of honey bees. For identification, a tag with a binary code is attached to the back of the bees.\nThe orientations in space, position, and bits of the tags are required to track the bees over time. Decoding this information is not trivial: the bees are oriented in any direction in space. The tag might be partially occluded. Moreover, spotlights on the tag can sometimes even confuse humans. A previously used computer vision pipeline did not perform reliably. Although we invested a substantial amount of time on labeling, a DCNN trained on this data did not perform significantly better (see Sec. 3). We therefore wanted to synthesize labeled images which are realistic enough to train an improved decoder network.\nFollowing the idea outlined in section 3, we created a simple 3D model of a bee marker. The 3D model comprises a mesh which represents the structure of the marker and a simple camera model to project the mesh to the image plane. The model is parameterized by its position, its pitch, yaw and roll, and its ID. Given a parameter set, we obtain a marker image, a background segmentation mask\nand a depth map. The generated images lack many important factors: blur, lighting, background, and image detail (see Fig. 2). A DCNN trained on this data does not generalize well (see Sec. 5).\nOver the last years we collected a large amount of unlabeled image data. We successfully augmented the 3D model using this dataset, as described below.\nWe trained a neural network to emulate the 3D model. Its outputs are indistinguishable from the images of the 3D model. The discriminator error can now be backpropagated through the 3D model which allows the generator to also learn the distributions of positions and orientations of the bee marker. The IDs are sampled uniformly during training. The weights of the 3D model network are fixed during the RenderGAN training.\nWe apply different augmentation functions that account for blur, lighting, background, and image detail. The output of the 3D model and of each augmentation function is of shape (64, 64) and in the range [−1, 1]. In Fig. 5, the structure of the generator is shown. Blurriness: The 3D model produces hard edges, but the images of the real tags show a broad range of blur. The generator produces a scalar α ∈ [0, 1] per image that controls the blur.\nφblur(x, α) = (1− α) (x− bσ (x)) + bσ(x) (4)\nwhere bσ(x) = x ∗ kσ denotes convolving the image x with a Gaussian kernel kσ of scale σ. The implementation of the blur function is inspired by Laplacian pyramids (Burt & Adelson, 1983). As required for augmentation functions, the labels are preserved, because we limit the maximum amount of blur by picking σ = 2. φblur is also differentiable w.r.t the inputs α and x.\nLighting of the tag: The images from the 3D model are binary. In real images, tags exhibit different shades of gray. We model the lighting by a smooth scaling and shifting of the pixel intensities. The generator provides three outputs for the lighting: scaling of black parts sb, scaling of white parts sw and a shift t. All outputs have the same dimensions as the image x. An important invariant is that the black bits of the tag must stay darker than the white bits. Otherwise, a bit could flip, and the label would change. By restricting the scaling sw and sb to be between 0.10 and 1, we ensure that this invariant holds. The lighting is locally corrolated and should cause smooth changes in the image. Hence, Gaussian blur b(x) is applied to sb, sw, and t.\nφlighting(x, sw, sb, t) = x · b(sw) ·W (x) + x · b(sb) · (1−W (x)) + b(t) (5)\nThe segmentation mask W (x) is one for white parts and zero for the black part of the image. As the intensity of the input is distributed around -1 and 1, we can use thresholding to differentiate between black and white parts.\nBackground: The background augmentation can change the background pixels arbitrarily. A segmentation mask Bx marks the background pixels of the image x which are replaced by the pixels from the generated image d.\nφbg(x, d) = x · (1−Bx) + d ·Bx (6)\nThe 3D model provides the segmentation mask. As φbg can only change background pixels, the labels remain unchanged.\nDetails: In this stage, the generator can add small details to the whole image including the tag. The output of the generator d is passed through a high-pass filter to ensure that the added details are small enough not to flip a bit. Furthermore, d is restricted to be in [−2, 2] to make sure the generator cannot avoid the highpass filter by producing huge values. With the range [−2, 2], the generator has the possibility to change black pixels to white, which is needed to model spotlights.\nφdetail(x, d) = x+ highpass(d) (7) The high-pass is implemented by taking the difference between the image and a blurred version of the image (σ = 3.5). As the spotlights on the tags are only a little smaller than the bits, we increase its slope after the cutoff frequency by repeating the high-pass filter three times.\nThe image augmentations are applied in the order as listed above: φdetail ◦φbackground ◦φlighting ◦ φblur. Please note that there exist parameters to the augmentation functions that could change the labels. As long as it is guaranteed that such augmentations will result in unrealistic looking images, the generator network will learn to avoid them. For example, even though the detail augmentation could be used to add high-frequency noise to obscure the tag, this artifact would be detected by the discriminator.\nArchitecture of the generator: The generator network has to produce outputs for each augmentation function. We will outline only the most important parts. See our code available online for all the details of the networks1. The generator starts with a small network consisting of dense layers, which predicts the parameters for the 3D model (position, orientations). The output of another dense layer is reshaped and used as starting block for a chain of convolution and upsampling layers. We found it advantageous to merge a depth map of the 3D model into the generator as especially the lighting depends on the orientation of the tag in space. The input to the blur augmentation is predicted by reducing an intermediate convolutional feature map to a single scalar. An additional network is branched off to predict the input to the lighting augmentation. For the background generation, the output of the lighting network is merged back into the main generator network together with the actual image from the 3D model.\nFor the discriminator architecture, we mostly rely on the architecture given by Radford et al. (2015), but doubled the number of convolutional layers and added a final dense layer. This change improved the quality of the generated images.\nClip layer: Some of the augmentation parameters have to be restricted to a range of values to ensure that the labels remain valid. The training did not converge when using functions like tanh or sigmoid due to vanishing gradients. We are using a combination of clipping and activity regularization to keep the output in a given interval [a, b]. If the input x is out of bounds, it is clipped and a regularization loss r depending on the distance between x and the appropriate bound is added.\nr(x) =  γ||x− a||1 if x < a 0 if a ≤ x ≤ b γ||x− b||1 if x > b\n(8)\nf(x) = min(max(a, x), b) (9) With the scalar γ, the weight of the loss can be adapted. For us γ = 15 worked well. If γ is chosen too small, the regularization loss might not be big enough to move the output of the previous layer towards the interval [a, b]. During training, we observe that the loss decreases to a small value but never vanishes.\nTraining: We train generator and discriminator as in the normal GAN setting. We use 2.4M unlabeled images of tags to train the RenderGAN. We use Adam (Kingma & Ba, 2014) as an optimizer with a starting learning rate of 0.0002, which we reduce in epoch 200, 250, and 300 by a factor of 0.25. In Fig. 6b the training loss of the GAN is shown. The GAN does not converge to the point where the discriminator can no longer separate generated from real samples. The augmentation functions might restrict the generator too much such that it cannot model certain properties. Nevertheless, it is hard for a human to distinguish the generated from real images. In some cases, the\n1https://github.com/berleon/deepdecoder\ngenerator creates unrealistic high-frequencies artifacts. The discriminator unfailingly assigns a low score to theses images. We can therefore discard them for the training of the supervised algorithm. More generated images are shown in Appendix A. In Fig. 7, we show random points in the latent space, while fixing the tag parameters. The generator indeed learned to model the various lighting conditions, noise intensities, and backgrounds."
    }, {
      "heading" : "5 RESULTS",
      "text" : "We constructed the RenderGAN to generate labeled data. But does a DCNN trained with the RenderGAN data perform better than one trained on the limited amounts of real data? And are learned augmentations indeed needed or do simple hand-designed augmentation achieve the same result? The following paragraphs describe the different datasets used in the evaluation. We focus on the performance of a DCNN on the generated data. Thus, we do not compare our method to conventional GANs as those do not provide labels and are generally hard to evaluate.\nData from the RenderGAN: We generate 5 million tags with the RenderGAN framework. Due to the abundance, one training sample is only used twice during training. It is not further augmented.\nReal Data: The labels of the real data are extracted from ground truth data that was originally collected to evaluate bee trajectories. This ground truth data contains the path and id of each bee over multiple consecutive frames. Data from five different time spans was annotated – in total 66K tags. As the data is correlated in time (same ids, similar lighting conditions), we assign the data from one time span completely to either the train or test set. The data from three time spans forms the train set (40K). The test set (26K) contains data from the remaining two time spans. The ground truth data lacks the orientation of the tags, which is therefore omitted for this evaluation. Due to the smaller\nsize of the real training set, we augment it with random translation, rotation, shear transformation, histogram scaling, and noise (see Appendix C for exact parameters).\nRenderGAN + Real: We also train a DCNN on generated and real data which is mixed at a 50:50 ratio.\nHandmade augmentations: We tried to emulate the augmentations learned by the RenderGAN by hand. For example, we generate the background by an image pyramid where the pixel intensities are drawn randomly. We model all effects, i.e. blur, lighting, background, noise and spotlights (see Appendix B for details on their implementation). We apply the handmade augmentation to different learned representations of the RenderGAN, e.g. we use the learned lighting representation and add the remaining effects such as background and noise with handmade augmentations (HM LI). See Table 1 for the different combinations of learned representations and hand designed augmentations.\nComputer vision pipeline CV : The previously used computer vision pipeline (Wario et al., 2015) is based on manual feature extraction. For example, a modified Hough transformation to find ellipses. The MHD obtained by this model is only a rough estimate given that the computer vision pipeline had to be evaluated and fine-tuned on the same data set due to label scarcity.\nTraining setup: An epoch consists of 1000 batches á 128 samples. We use early stopping to select the best parameters of the networks. For the training with generated data, we use the real training set as the validation set. When training on real data, the test set is also used for validation. We could alternatively reduce the real training set further and form an extra validation set, but this would harm the performance of the DCNN trained on the real data. We use the 34-layer ResNet architecture (He et al., 2015) but start with 16 feature maps instead of 64. The DCNNs are evaluated on the mean Hamming distance (MHD) i.e. the expected value of bits decoded wrong. Human experts can decode tags with a MHD of around 0.23.\nResults: In Table 2, we present the results of the evaluation. The training losses of the networks are plotted in Fig. 8. The model trained with the data generated by the RenderGAN has an MHD of 0.424. The performance can furthermore be slightly improved by combining the generated with real data. The small gap in performance when adding real data is a further indicator of the quality of the generated samples.\nIf we use predictions from this DCNN instead of the computer vision pipeline, the accuracy of the tracking improves from 55% of the ids assigned correctly to 96%. At this quality, it is possible to analyze the social behavior of the honeybees reliably.\nCompared to the handmade augmentations (HM 3D), data from the RenderGAN leads to considerably better performance. The large gap in performance between the HM 3D and HM LI data highlights the importance of the learned lighting augmentation."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "We proposed a novel extension to the GAN framework that is capable of rendering samples from a basic 3D model more realistic. Compared to computer graphics pipelines, the RenderGAN can learn complex effects from unlabeled data that would be otherwise hard to model with explicit rules.\nContrary to conventional GANs, the generator provides explicit information about the synthesized images, which can be used as labels for a supervised algorithm. The training of the RenderGAN requires no labels.\nWe showed an application of the RenderGAN framework to the BeesBook project, in which the generator adds blur, lighting, background, and details to images from a basic 3D model. The generated data looks strikingly real and includes fine details such as spotlights, compression artifacts, and sensor noise.\nIn contrast to previous work that applied 3D models to produce training samples for DCNNs (Su et al., 2015; Richter et al., 2016; Ros et al., 2016), we were able to train a DCNN from scratch with only generated data that still generalizes to unseen real data.\nWhile some work is required to adapt the RenderGAN to a specific domain, once set up, arbitrary amounts of labeled data can be acquired cheaply, even if the data distribution changes. For example, if the tag design changes to include more bits, small adaptions to the 3D model’s source code and eventually the hyperparameters of the augmentation functions would be sufficient. However, if we had labeled the data by hand, then we would have to annotate data again.\nWhile the proposed augmentations represent common image characteristics, a disadvantage of the RenderGAN framework is that these augmentation functions must be carefully customized for the application at hand to ensure that high-level information is preserved. Furthermore, a suitable 3D model must be available."
    }, {
      "heading" : "7 FUTURE WORK",
      "text" : "For future work, it would be interesting to see the RenderGAN framework used on other tasks where basic 3D models exist e.g. human faces, pose estimation, or viewpoint prediction. In this context, one could come up with different augmentation functions e.g. colorization, affine transformations, or diffeomorphism. The RenderGAN could be especially valuable to domains where pre-trained models are not available or when the annotations are very complex. Another direction of future work might be to extend the RenderGAN framework to other fields. For example, in speech synthesis, one could use an existing software as a basic model and improve the realism of the output with a similar approach as in the RenderGAN framework."
    }, {
      "heading" : "B HANDMADE AUGMENTATIONS",
      "text" : "We constructed augmentations for blur, lighting, background, noise and spotlights manually. For synthesizing lighting, background and noise, we use image pyramids, i.e. a set of images L0, . . . , L6 of size (2i × 2i) for 0 ≤ i ≤ 6. Each level Li in the pyramid is weighted by a scalar ωi. Each pixel of the different level Li is drawn from N (0, 1). The generated image I6 is given by:\nI0 = ω0L0 (10) Ii = ωiLi + upscale(Ii−1) (11)\n, where upscale doubles the image dimensions. The pyramid enables us to generate random images while controlling their frequency domain by weighting the pyramid levels appropriately.\n• Blur: Gaussian blur with randomly sampled scale. • Lighting: Similar as in the RenderGAN. Here, the scaling of the white and black parts\nand shifting is constructed with image pyramids. • Background: image pyramids with the lower levels weight more. • Noise: image pyramids with only the last two layer. • Spotlights: overlay with possible multiple 2D Gauss function with a random position on\nthe tag and random covariance.\nWe selected all parameters manually by comparing the generated to real images. However, using slightly more unrealistic images resulted in better performance of the DCNN trained with the HM 3D data. The parameters of the handmade augmentations can be found online in our source code repository."
    }, {
      "heading" : "C AUGMENTATIONS OF THE REAL DATA",
      "text" : "We scale and shift the pixel intensities randomly, i.e. sI+t, where I is the image and s, t are scalars. The noise is sampled for each pixel fromN (0, ), where ∼ max(0, N (µn, σn)) is drawn for each image separately. Different affine transformations (rotation, scale, translation, and shear) are used."
    }, {
      "heading" : "D TRAINING SAMPLES",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "The laplacian pyramid as a compact image code",
      "author" : [ "Peter Burt", "Edward Adelson" ],
      "venue" : "IEEE Transactions on communications,",
      "citeRegEx" : "Burt and Adelson.,? \\Q1983\\E",
      "shortCiteRegEx" : "Burt and Adelson.",
      "year" : 1983
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1606.03657,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to generate chairs with convolutional neural networks",
      "author" : [ "Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik" ],
      "venue" : "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning. Book in preparation for MIT Press, 2016",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : "URL http://www.deeplearningbook.org",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Learning Multiple Layers of Features",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "Tiny Images,",
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Fully Convolutional Networks for Semantic Segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep exemplar 2d-3d detection by adapting from real to rendered views",
      "author" : [ "Francisco Massa", "Bryan Russell", "Mathieu Aubry" ],
      "venue" : "arXiv preprint arXiv:1512.02497,",
      "citeRegEx" : "Massa et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Massa et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Deep Object Detectors from 3D Models",
      "author" : [ "Xingchao Peng", "Baochen Sun", "Karim Ali", "Kate Saenko" ],
      "venue" : "Iccv,",
      "citeRegEx" : "Peng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Cnn features offthe-shelf: an astounding baseline for recognition",
      "author" : [ "Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "Razavian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Razavian et al\\.",
      "year" : 2014
    }, {
      "title" : "Playing for data: Ground truth from computer games",
      "author" : [ "Stephan R Richter", "Vibhav Vineet", "Stefan Roth", "Vladlen Koltun" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Richter et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Richter et al\\.",
      "year" : 2016
    }, {
      "title" : "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes",
      "author" : [ "German Ros", "Laura Sellart", "Joanna Materzynska", "David Vazquez", "Antonio M Lopez" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Ros et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ros et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised and semi-supervised learning with categorical generative adversarial networks",
      "author" : [ "Jost Tobias Springenberg" ],
      "venue" : "arXiv preprint arXiv:1511.06390,",
      "citeRegEx" : "Springenberg.,? \\Q2015\\E",
      "shortCiteRegEx" : "Springenberg.",
      "year" : 2015
    }, {
      "title" : "Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views",
      "author" : [ "Hao Su", "Charles R Qi", "Yangyan Li", "Leonidas J Guibas" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp. 2686–2694,",
      "citeRegEx" : "Su et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative image modeling using style and structure adversarial networks",
      "author" : [ "Xiaolong Wang", "Abhinav Gupta" ],
      "venue" : "arXiv preprint arXiv:1603.05631,",
      "citeRegEx" : "Wang and Gupta.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Gupta.",
      "year" : 2016
    }, {
      "title" : "Automatic methods for long-term tracking and the detection and decoding of communication dances in honeybees",
      "author" : [ "Fernando Wario", "Benjamin Wild", "Margaret Jane Couvillon", "Raúl Rojas", "Tim Landgraf" ],
      "venue" : "Frontiers in Ecology and Evolution,",
      "citeRegEx" : "Wario et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wario et al\\.",
      "year" : 2015
    }, {
      "title" : "Beyond pascal: A benchmark for 3d object detection in the wild",
      "author" : [ "Yu Xiang", "Roozbeh Mottaghi", "Silvio Savarese" ],
      "venue" : "In IEEE Winter Conference on Applications of Computer Vision,",
      "citeRegEx" : "Xiang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2014
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "In recent years, deep convolutional neural networks (DCNNs) advanced to the state of the art in many computer vision tasks (Krizhevsky et al., 2012; He et al., 2015; Razavian et al., 2014).",
      "startOffset" : 123,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "In recent years, deep convolutional neural networks (DCNNs) advanced to the state of the art in many computer vision tasks (Krizhevsky et al., 2012; He et al., 2015; Razavian et al., 2014).",
      "startOffset" : 123,
      "endOffset" : 188
    }, {
      "referenceID" : 16,
      "context" : "In recent years, deep convolutional neural networks (DCNNs) advanced to the state of the art in many computer vision tasks (Krizhevsky et al., 2012; He et al., 2015; Razavian et al., 2014).",
      "startOffset" : 123,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "We propose two modifications to the recently introduced GAN framework (Goodfellow et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 22,
      "context" : "The RenderGAN framework was developed to solve the scarcity of labeled data in the BeesBook project (Wario et al., 2015) in which we analyze the social behavior of honeybees.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 24,
      "context" : "DCNNs learn a hierarchy of features – many of which are applicable to related domains (Yosinski et al., 2014).",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "Therefore, a common technique is to pre-train a model on a larger dataset such as ImageNet (Deng et al., 2009) and then fine-tune its parameters to the task at hand (Girshick et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : ", 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014).",
      "startOffset" : 62,
      "endOffset" : 127
    }, {
      "referenceID" : 12,
      "context" : ", 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014).",
      "startOffset" : 62,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : ", 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014).",
      "startOffset" : 62,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "Therefore, a common technique is to pre-train a model on a larger dataset such as ImageNet (Deng et al., 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014). This technique only works in cases where a large enough related dataset exists. Furthermore, labeling enough data for fine-tuning might still be costly. If a basic model of the data exists (for example, a 3D model of the human body), it can be used to generate labeled data. Peng et al. (2015) generated training data for a DCNN with 3D-CAD models.",
      "startOffset" : 92,
      "endOffset" : 526
    }, {
      "referenceID" : 23,
      "context" : "(2015) used 3D-CAD models from large online repositories to generate large amounts of training images for the viewpoint estimation task on the PASCAL 3D+ dataset (Xiang et al., 2014).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "3) can learn to generate high-quality samples (Goodfellow et al., 2014), i.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "(2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "While a GAN implicitly learns a meaningful latent embedding of the data (Radford et al., 2015), there is no simple relationship between the latent dimensions and the labels of interest.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "Massa et al. (2015) are matching natural images to 3D-CAD models with features extracted from a DCNN.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "Massa et al. (2015) are matching natural images to 3D-CAD models with features extracted from a DCNN. Richter et al. (2016) and Ros et al.",
      "startOffset" : 0,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "Massa et al. (2015) are matching natural images to 3D-CAD models with features extracted from a DCNN. Richter et al. (2016) and Ros et al. (2016) used 3D game engines to collect labeled data for image segmentation.",
      "startOffset" : 0,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "Denton et al. (2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Denton et al. (2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images. While a GAN implicitly learns a meaningful latent embedding of the data (Radford et al., 2015), there is no simple relationship between the latent dimensions and the labels of interest. Therefore, high-level information can’t be inferred from generated samples. cGANs are an extension of GANs to sample from a conditional distribution given some labels, i.e. p(x|l). However, training cGANs requires a labeled dataset. Springenberg (2015) showed that GANs can be used in a semi-supervised setting but restricted their analysis to categorical labels.",
      "startOffset" : 0,
      "endOffset" : 591
    }, {
      "referenceID" : 2,
      "context" : "Denton et al. (2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images. While a GAN implicitly learns a meaningful latent embedding of the data (Radford et al., 2015), there is no simple relationship between the latent dimensions and the labels of interest. Therefore, high-level information can’t be inferred from generated samples. cGANs are an extension of GANs to sample from a conditional distribution given some labels, i.e. p(x|l). However, training cGANs requires a labeled dataset. Springenberg (2015) showed that GANs can be used in a semi-supervised setting but restricted their analysis to categorical labels. Wang & Gupta (2016) trained two separate GANs, one to model the object normals and another one for the texture conditioned on the normals.",
      "startOffset" : 0,
      "endOffset" : 722
    }, {
      "referenceID" : 1,
      "context" : "Chen et al. (2016) used an information theoretic to disentangle the representation.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "This can be achieved by emulating the 3D model with a neural network (Dosovitskiy et al., 2015).",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "For the discriminator architecture, we mostly rely on the architecture given by Radford et al. (2015), but doubled the number of convolutional layers and added a final dense layer.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "Computer vision pipeline CV : The previously used computer vision pipeline (Wario et al., 2015) is based on manual feature extraction.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "We use the 34-layer ResNet architecture (He et al., 2015) but start with 16 feature maps instead of 64.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : "In contrast to previous work that applied 3D models to produce training samples for DCNNs (Su et al., 2015; Richter et al., 2016; Ros et al., 2016), we were able to train a DCNN from scratch with only generated data that still generalizes to unseen real data.",
      "startOffset" : 90,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "In contrast to previous work that applied 3D models to produce training samples for DCNNs (Su et al., 2015; Richter et al., 2016; Ros et al., 2016), we were able to train a DCNN from scratch with only generated data that still generalizes to unseen real data.",
      "startOffset" : 90,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "In contrast to previous work that applied 3D models to produce training samples for DCNNs (Su et al., 2015; Richter et al., 2016; Ros et al., 2016), we were able to train a DCNN from scratch with only generated data that still generalizes to unseen real data.",
      "startOffset" : 90,
      "endOffset" : 147
    } ],
    "year" : 2017,
    "abstractText" : "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}