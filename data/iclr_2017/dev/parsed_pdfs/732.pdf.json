{
  "name" : "732.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GENERATIVE PARAGRAPH VECTOR",
    "authors" : [ "Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng" ],
    "emails" : [ "zhangruqing@software.ict.ac.cn,", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "A central problem in many text based applications, e.g., sentiment classification (Pang & Lee, 2008), question answering (Stefanie Tellex & Marton., 2003) and machine translation (I. Sutskever & Le, 2014), is how to capture the essential meaning of a piece of text in a fixed-length vector. Perhaps the most popular fixed-length vector representations for texts is the bag-of-words (or bag-of-ngrams) (Harris, 1954). Besides, probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003) are two widely adopted alternatives.\nA recent paradigm in this direction is to use a distributed representation for texts (T. Mikolov & Dean, 2013a). In particular, Le and Mikolov (Quoc Le, 2014; Andrew M.Dai, 2014) show that their method, Paragraph Vector (PV), can capture text semantics in dense vectors and outperform many existing representation models. Although PV is an efficient method for learning high-quality distributed text representations, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model (i.e., learned text and word vectors). Such a limitation largely restricts the usage of the PV model, especially in those prediction focused scenarios.\nInspired by the completion and improvement of LDA over PLSI, we first introduce the Generative Paragraph Vector (GPV) with a complete generation process for a corpus. Specifically, GPV can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector (PVDBOW), where the text vector is viewed as a hidden variable sampled from some prior distributions, and the words within the text are then sampled from the softmax distribution given the text and word vectors. With a complete generative process, we are able to infer the distributed representations of new texts based on the learned model. Meanwhile, the prior distribution over text vectors also acts as a regularization factor from the view of optimization, thus can lead to higher-quality text representations.\nMore importantly, with the ability to infer the distributed representations for unseen texts, we now can directly incorporate labels paired with the texts into the model to guide the representation learning, and turn the model into a supervised version, namely Supervised Generative Paragraph Vector (SGPV). Note that supervision cannot be directly leveraged in the original PV model since it has no\ngeneralization ability on new texts. By learning the SGPV model, we can directly employ SGPV to predict labels for new texts. As we know, when the goal is prediction, fitting a supervised model would be a better choice than learning a general purpose representations of texts in an unsupervised way. We further show that SGPV can be easily extended to accommodate n-grams so that we can take into account word order information, which is important in learning semantics of texts.\nWe evaluated our proposed models on five text classification benchmark datasets. For the unsupervised GPV, we show that its superiority over the existing counterparts, such as bag-of-words, LDA, PV and FastSent (Felix Hill, 2016). For the SGPV model, we take into comparison both traditional supervised representation models, e.g. MNB (S. Wang, 2012), and a variety of state-of-the-art deep neural models for text classification (Kim, 2014; N. Kalchbrenner, 2014; Socher & Potts, 2013; Irsoy & Cardie, 2014). Again we show that the proposed SGPV can outperform the baseline methods by a substantial margin, demonstrating it is a simple yet effective model.\nThe rest of the paper is organized as follows. We first review the related work in section 2 and briefly describe PV in section 3. We then introduce the unsupervised generative model GPV and supervised generative model SGPV in section 4 and section 5 respectively. Experimental results are shown in section 6 and conclusions are made in section 7."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Many text based applications require the text input to be represented as a fixed-length feature vector. The most common fixed-length representation is bag-of-words (BoW) (Harris, 1954). For example, in the popular TF-IDF scheme (Salton & McGill, 1983), each document is represented by tfidf values of a set of selected feature-words. However, the BoW representation often suffers from data sparsity and high dimension. Meanwhile, due to the independent assumption between words, BoW representation has very little sense about the semantics of the words.\nTo address this shortcoming, several dimensionality reduction methods have been proposed, such as latent semantic indexing (LSI) (S. Deerwester & Harshman, 1990), Probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003). Both PLSI and LDA have a good statistical foundation and proper generative model of the documents, as compared with LSI which relies on a singular value decomposition over the term-document cooccurrence matrix. In PLSI, each word is generated from a single topic, and different words in a document may be generated from different topics. While PLSI makes great effect on probabilistic modeling of documents, it is not clear how to assign probability to a document outside of the training set with the learned model. To address this issue, LDA is proposed by introducing a complete generative process over the documents, and demonstrated as a state-of-the-art document representation method. To further tackle the prediction task, Supervised LDA (David M.Blei, 2007) is developed by jointly modeling the documents and the labels.\nRecently, distributed models have been demonstrated as efficient methods to acquire semantic representations of texts. A representative method is Word2Vec (Tomas Mikolov & Dean, 2013b), which can learn meaningful word representations in an unsupervised way from large scale corpus. To represent sentences or documents, a simple approach is then using a weighted average of all the words. A more sophisticated approach is combing the word vectors in an order given by a parse tree (Richard Socher & Ng, 2012). Later, Paragraph Vector (PV) (Quoc Le, 2014) is introduced to directly learn the distributed representations of sentences and documents. There are two variants in PV, namely the Distributed Memory Model of Paragraph Vector (PV-DM) and the Distributed Bag of Words version of Paragraph Vector (PV-DBOW), based on two different model architectures. Although PV is a simple yet effective distributed model on sentences and documents, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model.\nBesides these unsupervised representation learning methods, there have been many supervised deep models with directly learn sentence or document representations for the prediction tasks. Recursive Neural Network (RecursiveNN) (Richard Socher & Ng, 2012) has been proven to be efficient in terms of constructing sentence representations. Recurrent Neural Network (RNN) (Ilya Sutskever & Hinton, 2011) can be viewed as an extremely deep neural network with weight sharing across time. Convolution Neural Network (CNN) (Kim, 2014) can fairly determine discriminative phrases in a\ntext with a max-pooling layer. However, these deep models are usually quite complex and thus the training would be time-consuming on large corpus."
    }, {
      "heading" : "3 PARAGRAPH VECTOR",
      "text" : "Since our model can be viewed as a probabilistic extension of the PV-DBOW model with a complete generative process, we first briefly review the PV-DBOW model for reference.\nIn PV-DBOW, each text is mapped to a unique paragraph vector and each word is mapped to a unique word vector in a continuous space. The paragraph vector is used to predict target words randomly sampled from the paragraph as shown in Figure 1. More formally, Let D={d1, . . . ,dN} denote a corpus of N texts, where each text dn = (wn1 , w n 2 , . . . , w n ln\n), n ∈ 1, 2, . . . , N is an lnlength word sequence over the word vocabulary V of size M . Each text d ∈ D and each word w ∈ V is associated with a vector ~d ∈ RK and ~w ∈ RK , respectively, where K is the embedding dimensionality. The predictive objective of the PV-DBOW for each word wnl ∈ dn is defined by the softmax function\np(wni |dn) = exp(~wni · ~dn)∑ w′∈V exp(~w ′ · ~dn) (1)\nThe PV-DBOW model can be efficiently trained using the stochastic gradient descent (Rumelhart & Williams, 1986) with negative sampling (T. Mikolov & Dean, 2013a).\nAs compared with traditional topic models, e.g. PLSI and LDA, PV-DBOW conveys the following merits. Firstly, PV-DBOW using negative sampling can be interpretated as a matrix factorization over the words-by-texts co-occurrence matrix with shifted-PMI values (Omer Levy & Ramat-Gan, 2015). In this way, more discriminative information (i.e., PMI) can be modeled in PV as compared with the generative topic models which learn over the words-by-texts co-occurrence matrix with raw frequency values. Secondly, PV-DBOW does not have the explicit “topic” layer and allows words automatically clustered according to their co-occurrence patterns during the learning process. In this way, PV-DBOW can potentially learn much finer topics than traditional topic models given the same hidden dimensionality of texts. However, a major problem with PV-DBOW is that it provides no model on text vectors: it is unclear how to infer the distributed representations for unseen texts."
    }, {
      "heading" : "4 GENERATIVE PARAGRAPH VECTOR",
      "text" : "In this section, we introduce the GPV model in detail. Overall, GPV is a generative probabilistic model for a corpus. We assume that for each text, a latent paragraph vector is first sampled from some prior distributions, and the words within the text are then generated from the normalized exponential (i.e. softmax) distribution given the paragraph vector and word vectors. In our work, multivariate normal distribution is employed as the prior distribution for paragraph vectors. It could\nbe replaced by other prior distributions and we will leave this as our future work. The specific generative process is as follows:\nFor each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :\nDraw word wni ∼ softmax(~dn ·W )i\nwhere W denotes a k ×M word embedding matrix with W∗j = ~wj , and softmax(~dn ·W )i is the softmax function defined the same as in Equation (1). Figure 2 (Left) provides the graphical model of this generative process. Note that GPV differs from PV-DBOW in that the paragraph vector is a hidden variable generated from some prior distribution, which allows us to infer the paragraph vector over future texts given the learned model. Based on the above generative process, the probability of the whole corpus can be written as follows:\np(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ∏ wni ∈dn p(wni |W, ~dn)d~dn\nTo learn the model, direct maximum likelihood estimation is not tractable due to non-closed form of the integral. We approximate this learning problem by using MAP estimates for ~dn, which can be formulated as follows:\n(µ∗,Σ∗,W ∗) = arg max µ,Σ,W\n∏ p(d̂n|µ,Σ) ∏ wni ∈dn p(wni |W, d̂n)\nwhere d̂n denotes the MAP estimate of ~dn for dn, (µ∗,Σ∗,W ∗) denotes the optimal solution. Note that for computational simplicity, in this work we fixed µ as a zero vector and Σ as a identity matrix. In this way, all the free parameters to be learned in our model are word embedding matrix W . By taking the logarithm and applying the negative sampling idea to approximate the softmax function, we obtain the final learning problem\nL= N∑ n=1 ( −1 2 ||d̂n||2+ ∑ wni ∈dn ( log σ(~wni ·d̂n)+k·Ew′∼Pnw log σ(− ~w′ · d̂n) )) where σ(x) = 1/(1 + exp(−x)), k is the number of “negative” samples, w′ denotes the sampled word and Pnw denotes the distribution of negative word samples. As we can see from the final objective function, the prior distribution over paragraph vectors actually act as a regularization term. From the view of optimization, such regularization term could constrain the learning space and usually produces better paragraph vectors.\nFor optimization, we use coordinate ascent, which first optimizes the word vectors W while leaving the MAP estimates (d̂) fixed. Then we find the new MAP estimate for each document while leaving the word vectors fixed, and continue this process until convergence. To accelerate the learning, we adopt a similar stochastic learning framework as in PV which iteratively updates W and estimates ~d by randomly sampling text and word pairs.\nAt prediction time, given a new text, we perform an inference step to compute the paragraph vector for the input text. In this step, we freeze the vector representations of each word, and apply the same MAP estimation process of ~d as in the learning phase. With the inferred paragraph vector of the test text, we can feed it to other prediction models for different applications."
    }, {
      "heading" : "5 SUPERVISED GENERATIVE PARAGRAPH VECTOR",
      "text" : "With the ability to infer the distributed representations for unseen texts, we now can incorporate the labels paired with the texts into the model to guide the representation learning, and turn the model into a more powerful supervised version directly towards prediction tasks. Specifically, we introduce an additional label generation process into GPV to accommodate text labels, and obtain the Supervised Generative Paragraph Vector (SGPV) model. Formally, in SGPV, the n-th text dn and the corresponding class label yn ∈ {1, 2, . . . , C} arise from the following generative process:\nFor each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :\nDraw word wni ∼ softmax(~dn ·W )i (c) Draw label yn|~dn, U, b ∼ softmax(U · ~dn+b)\nwhere U is a C ×K matrix for a dataset with C output labels, and b is a bias term. The graphical model of the above generative process is depicted in Figure 2 (Right). SGPV defines the probability of the whole corpus as follows\np(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ( ∏ wni ∈dn p(wni |W, ~dn) ) p(yn|~dn, U, b)d~dn\nWe adopt a similar learning process as GPV to estimate the model parameters. Since the SGPV includes the complete generative process of both paragraphs and labels, we can directly leverage it to predict the labels of new texts. Specifically, at prediction time, given all the learned model parameters, we conduct an inference step to infer the paragraph vector as well as the label using MAP estimate over the test text.\nThe above SGPV may have limited modeling ability on text representation since it mainly relies on uni-grams. As we know, word order information is often critical in capturing the meaning of texts. For example, “machine learning” and “learning machine” are totally different in meaning with the same words. There has been a variety of deep models using complex architectures such as convolution layers or recurrent structures to help capture such order information at the expense of large computational cost.\nHere we propose to extend SGPV by introducing an additional generative process for n-grams, so that we can incorporate the word order information into the model and meanwhile keep its simplicity in learning. We name this extension as SGPV-ngram. Here we take the generative process of SGPVbigram as an example.\nFor each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :\nDraw word wni ∼ softmax(~dn ·W )i\n(c) For each bigram gni ∈ dn, i = 1, 2, . . . , sn : Draw bigram gni ∼ softmax(~dn ·G)i\n(d) Draw label yn|~dn, U, b ∼ softmax(U · ~dn+b)\nwhere G denotes a K × S bigram embedding matrix with G∗j = ~gj , and S denotes the size of bigram vocabulary. The joint probability over the whole corpus is then defined as\np(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ( ∏ wni ∈dn p(wni |W, ~dn) )( ∏ gni ∈dn p(gni |G, ~dn) ) p(yn|~dn, U, b)d~dn"
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "In this section, we introduce the experimental settings and empirical results on a set of text classification tasks."
    }, {
      "heading" : "6.1 DATASET AND EXPERIMENTAL SETUP",
      "text" : "We made use of five publicly available benchmark datasets in comparison.\nTREC: The TREC Question Classification dataset (Li & Roth, 2002)1 which consists of 5, 452 train questions and 500 test questions. The goal is to classify a question into 6 different types depending on the answer they seek for.\nSubj: Subjectivity dataset (Pang & Lee, 2004) which contains 5, 000 subjective instances and 5, 000 objective instances. The task is to classify a sentence as being subjective or objective.\nMR: Movie reviews (Pang & Lee, 2005) 2 with one sentence per review. There are 5, 331 positive sentences and 5, 331 negative sentences. The objective is to classify each review into positive or negative category.\nSST-1: Stanford Sentiment Treebank (Socher & Potts, 2013) 3. SST-1 is provided with train/dev/test splits of size 8, 544/1, 101/2, 210. It is a fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive.\nSST-2: SST-2 is the same as SST-1 but with neutral reviews removed. We use the standard train/dev/test splits of size 6, 920/872/1, 821 for the binary classification task.\nPreprocessing steps were applied to all datasets: words were lowercased, non-English characters and stop words occurrence in the training set are removed. For fair comparison with other published results, we use the default train/test split for TREC, SST-1 and SST-2 datasets. Since explicit split of train/test is not provided by subj and MR datasets, we use 10-fold cross-validation instead.\nIn our model, text and word vectors are randomly initialized with values uniformly distributed in the range of [-0.5, +0.5]. Following the practice in (Tomas Mikolov & Dean, 2013b) , we set the noise distributions for context and words as pnw(w) ∝ #(w)0.75. We adopt the same linear learning rate strategy where the initial learning rate of our models is 0.025. For unsupervised methods, we use support vector machines (SVM) 4 as the classifier."
    }, {
      "heading" : "6.2 BASELINES",
      "text" : "We adopted both unsupervised and supervised methods on text representation as baselines."
    }, {
      "heading" : "6.2.1 UNSUPERVISED BASELINES",
      "text" : "Bag-of-word-TFIDF and Bag-of-bigram-TFIDF. In the bag-of-word-TFIDF scheme (Salton & McGill, 1983) , each text is represented as the tf-idf value of chosen feature-words. The bag-of-\n1http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2https://www.cs.cornell.edu/people/pabo/movie-review-data/ 3http://nlp.stanford.edu/sentiment/ 4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/\nbigram-TFIDF model is constructed by selecting the most frequent unigrams and bigrams from the training subset. We use the vanilla TFIDF in the gensim library5.\nLSI (S. Deerwester & Harshman, 1990) and LDA (Blei & Jordan, 2003). LSI maps both texts and words to lower-dimensional representations in a so-called latent semantic space using SVD decomposition. In LDA, each word within a text is modeled as a finite mixture over an underlying set of topics. We use the vanilla LSI and LDA in the gensim library with topic number set as 100.\ncBow (Tomas Mikolov & Dean, 2013b). Continuous Bag-Of-Words model. We use average pooling as the global pooling mechanism to compose a sentence vector from a set of word vectors.\nPV (Quoc Le, 2014). Paragraph Vector is an unsupervised model to learn distributed representations of words and paragraphs.\nFastSent (Felix Hill, 2016). In FastSent, given a simple representation of some sentence in context, the model attempts to predict adjacent sentences.\nNote that unlike LDA and GPV, LSI, cBow, and FastSent cannot infer the representations of unseen texts. Therefore, these four models need to fold-in all the test data to learn representations together with training data, which makes it not efficient in practice."
    }, {
      "heading" : "6.2.2 SUPERVISED BASELINES",
      "text" : "NBSVM and MNB (S. Wang, 2012). Naive Bayes SVM and Multinomial Naive Bayes with unigrams and bi-grams.\nDAN (Mohit Iyyer & III, 2015). Deep averaging network uses average word vectors as the input and applies multiple neural layers to learn text representation under supervision.\nCNN-multichannel (Kim, 2014). CNN-multichannel employs convolutional neural network for sentence modeling.\nDCNN (N. Kalchbrenner, 2014). DCNN uses a convolutional architecture that replaces wide convolutional layers with dynamic pooling layers.\nMV-RNN (Richard Socher & Ng, 2012). Matrix-Vector RNN represents every word and longer phrase in a parse tree as both a vector and a matrix.\nDRNN (Irsoy & Cardie, 2014). Deep Recursive Neural Networks is constructed by stacking multiple recursive layers.\nDependency Tree-LSTM (Kai Sheng Tai & Manning, 2015). The Dependency Tree-LSTM based on LSTM structure uses dependency parses of each sentence."
    }, {
      "heading" : "6.3 PERFORMANCE OF GENERATIVE PARAGRAPH VECTOR",
      "text" : "We first evaluate the GPV model by comparing with the unsupervised baselines on the TREC, Subj and MR datasets. As shown in table 1, GPV works better than PV over the three tasks. It demonstrates the benefits of introducing a prior distribution (i.e., regularization) over the paragraph vectors. Moreover, GPV can also outperform almost all the baselines on three tasks except Bow-TFIDF and Bigram-TFIDF on the TREC collection. The results show that for unsupervised text representation, bag-of-words representation is quite simple yet powerful which can beat many embedding models. Meanwhile, by using a complete generative process to infer the paragraph vectors, our model can achieve the state-of-the-art performance among the embedding based models."
    }, {
      "heading" : "6.4 PERFORMANCE OF SUPERVISED GENERATIVE PARAGRAPH VECTOR",
      "text" : "We compare SGPV model to supervised baselines on all the five classification tasks. Empirical results are shown in Table 2. We can see that SGPV achieves comparable performance against other deep learning models. Note that SGPV is much simpler than these deep models with significantly less parameters and no complex structures. Moreover, deep models with convolutional layers or recurrent structures can potentially capture compositional semantics (e.g., phrases), while SGPV only\n5http://radimrehurek.com/gensim/\nrelies on uni-gram. In this sense, SGPV is quite effective in learning text representation. Meanwhile, if we take Table 1 into consideration, it is not surprising to see that SGPV can consistently outperform GPV on all the three classification tasks. This also demonstrates that it is more effective to directly fit supervised representation models than to learn a general purpose representation in prediction scenarios.\nBy introducing bi-grams, SGPV-bigram can outperform all the other deep models on four tasks. In particular, the improvements of SGPV-bigram over other baselines are significant on SST-1 and SST-2. These results again demonstrated the effectiveness of our proposed SGPV model on text representations. It also shows the importance of word order information in modeling text semantics."
    }, {
      "heading" : "7 CONCLUSIONS",
      "text" : "In this paper, we introduce GPV and SGPV for learning distributed representations for pieces of texts. With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts. Our models keep as simple as PV models, and thus can be efficiently learned over large scale text corpus. Even with such simple structures, both GPV and SGPV can produce state-of-the-art results as compared with existing baselines, especially those complex deep models. For future work, we may consider other probabilistic distributions for both paragraph vectors and word vectors."
    } ],
    "references" : [ {
      "title" : "Document embedding with paragraph vectors",
      "author" : [ "Quoc V.Le Andrew M.Dai", "Christopher Olah" ],
      "venue" : "In Deep learning and Representation Learning Workshop",
      "citeRegEx" : "M.Dai and Olah.,? \\Q2014\\E",
      "shortCiteRegEx" : "M.Dai and Olah.",
      "year" : 2014
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Ng A. Blei", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei and Jordan.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei and Jordan.",
      "year" : 2003
    }, {
      "title" : "Supervised topic models",
      "author" : [ "Jon D.McAuliffe David M.Blei" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "M.Blei.,? \\Q2007\\E",
      "shortCiteRegEx" : "M.Blei.",
      "year" : 2007
    }, {
      "title" : "Learning distributed representations of sentences from unlabelled data",
      "author" : [ "Anna Korhone Felix Hill", "Kyunghyun Cho" ],
      "venue" : "arXiv preprint arXiv:1602.03483,",
      "citeRegEx" : "Hill and Cho.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hill and Cho.",
      "year" : 2016
    }, {
      "title" : "Self-adaptive hierarchical sentence model",
      "author" : [ "Zhengdong Lu Han Zhao", "Pascal Poupart" ],
      "venue" : "In IJCAI",
      "citeRegEx" : "Zhao and Poupart.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao and Poupart.",
      "year" : 2015
    }, {
      "title" : "Probabilistic latent semantic indexing",
      "author" : [ "T. Hofmann" ],
      "venue" : "In Proceedings of the Twenty-Second Annual International SIGIR Conference",
      "citeRegEx" : "Hofmann.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hofmann.",
      "year" : 1999
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "O. Vinyals I. Sutskever", "Q.V. Le" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutskever and Le.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever and Le.",
      "year" : 2014
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "James Martens Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Sutskever and Hinton.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever and Hinton.",
      "year" : 2011
    }, {
      "title" : "Deep recursive neural networks for compositionality in language",
      "author" : [ "Ozan Irsoy", "Claire Cardie" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems",
      "citeRegEx" : "Irsoy and Cardie.,? \\Q2014\\E",
      "shortCiteRegEx" : "Irsoy and Cardie.",
      "year" : 2014
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Richard Socher Kai Sheng Tai", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Association for Computational Linguistics",
      "citeRegEx" : "Tai and Manning.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai and Manning.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Li and Roth.,? \\Q2002\\E",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Jordan Boyd-Graber Mohit Iyyer", "Varun Manjunatha", "Hal Daume III" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "P. Blunsom N. Kalchbrenner", "E. Grefenstette" ],
      "venue" : "In Proceedings of the Association for Computational Linguistics",
      "citeRegEx" : "Kalchbrenner and Grefenstette.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner and Grefenstette.",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Ido Dagan Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Levy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Pang and Lee.,? \\Q2004\\E",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Pang and Lee.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "Foundations and trends in information retrieval,",
      "citeRegEx" : "Pang and Lee.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Tomas Mikolov Quoc Le" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Le.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le.",
      "year" : 2014
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Brody Huval", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning representations by back-propagating",
      "author" : [ "Hinton Geoffrey E Rumelhart", "David E", "Williams" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "S.T. Dumais", "R. Harshman" ],
      "venue" : "Journal of the American Society for Information Science,",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Baselines and bigrams: Simple, good sentiment and topic classification",
      "author" : [ "C. Manning S. Wang" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Wang.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2012
    }, {
      "title" : "Introduction to Modern Information Retrieval",
      "author" : [ "G. Salton", "M. McGill" ],
      "venue" : null,
      "citeRegEx" : "Salton and McGill.,? \\Q1983\\E",
      "shortCiteRegEx" : "Salton and McGill.",
      "year" : 1983
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard", "Christopher Potts" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Quantitative evaluation of passage retrieval algorithms for question answering",
      "author" : [ "Boris Katz", "Gregory Marton" ],
      "venue" : "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval",
      "citeRegEx" : "Tellex et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Tellex et al\\.",
      "year" : 2003
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "K. Chen G.S. Corrado T. Mikolov", "I. Sutskever", "J. Dean" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Greg Corrado Tomas Mikolov", "Kai Chen", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Besides, probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003) are two widely adopted alternatives.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "Wang, 2012), and a variety of state-of-the-art deep neural models for text classification (Kim, 2014; N. Kalchbrenner, 2014; Socher & Potts, 2013; Irsoy & Cardie, 2014).",
      "startOffset" : 90,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "Deerwester & Harshman, 1990), Probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003).",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "Convolution Neural Network (CNN) (Kim, 2014) can fairly determine discriminative phrases in a",
      "startOffset" : 33,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "CNN-multichannel (Kim, 2014).",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "3 - - CNN-multichannel (Kim, 2014) 47.",
      "startOffset" : 23,
      "endOffset" : 34
    } ],
    "year" : 2016,
    "abstractText" : "The recently introduced Paragraph Vector is an efficient method for learning highquality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.",
    "creator" : "LaTeX with hyperref package"
  }
}