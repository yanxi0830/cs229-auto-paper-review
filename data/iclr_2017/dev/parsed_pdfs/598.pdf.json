{
  "name" : "598.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SPEECH RECOGNITION SYSTEM",
    "authors" : [ "Ronan Collobert", "Christian Puhrsch" ],
    "emails" : [ "locronan@fb.com", "cpuhrsch@fb.com", "gab@fb.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "We present an end-to-end system to speech recognition, going from the speech signal (e.g. MelFrequency Cepstral Coefficients (MFCC), power spectrum, or raw waveform) to the transcription. The acoustic model is trained using letters (graphemes) directly, which take out the need for an intermediate (human or automatic) phonetic transcription. Indeed, the classical pipeline to build state of the art systems for speech recognition consists in first training an HMM/GMM model to force align the units on which the final acoustic model operates (most often context-dependent phone states). This approach takes its roots in HMM/GMM training (Woodland & Young, 1993). The improvements brought by deep neural networks (DNNs) (Mohamed et al., 2012; Hinton et al., 2012) and convolutional neural networks (CNNs) (Sercu et al., 2015; Soltau et al., 2014) for acoustic modeling only extend this training pipeline.\nThe current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too (Panayotov et al., 2015; Peddinti et al., 2015b), with an additional step of speaker adaptation (Saon et al., 2013; Peddinti et al., 2015a). Recently, Senior et al. (2014) proposed GMMfree training, but the approach still requires to generate a force alignment. An approach that cut ties with the HMM/GMM pipeline (and with force alignment) was to train with a recurrent neural network (RNN) (Graves et al., 2013) for phoneme transcription. There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al., 2006). However these models are computationally expensive, and thus take a long time to train.\nCompared to classical approaches that need phonetic annotation (often derived from a phonetic dictionary, rules, and generative training), we propose to train the model end-to-end, using graphemes directly. Compared to sequence criterion based approaches that train directly from speech signal to graphemes (Miao et al., 2015), we propose a simple(r) architecture (23 millions of parameters for our best model, vs. 100 millions of parameters in (Amodei et al., 2015)) based on convolutional networks\nfor the acoustic model, toppled with a graph transformer network (Bottou et al., 1997), trained with a simpler sequence criterion. Our word-error-rate on clean speech is slightly better than (Hannun et al., 2014), and slightly worse than (Amodei et al., 2015), in particular factoring that they train on 12,000 hours while we only train on the 960h available in LibriSpeech’s train set. Finally, some of our models are also trained on the raw waveform, as in (Palaz et al., 2013; 2015; Sainath et al., 2015). The rest of the paper is structured as follows: the next section presents the convolutional networks used for acoustic modeling, along with the automatic segmentation criterion. The following section shows experimental results comparing different features, the criterion, and our current best word error rates on LibriSpeech."
    }, {
      "heading" : "2 ARCHITECTURE",
      "text" : "Our speech recognition system is a standard convolutional neural network (LeCun & Bengio, 1995) fed with various different features, trained through an alternative to the Connectionist Temporal Classification (CTC) (Graves et al., 2006), and coupled with a simple beam search decoder. In the following sub-sections, we detail each of these components.\n2.1 FEATURES\nWe consider three types of input features for our model: MFCCs, power-spectrum, and raw wave. MFCCs are carefully designed speech-specific features, often found in classical HMM/GMM speech systems (Woodland & Young, 1993) because of their dimensionality compression (13 coefficients are often enough to span speech frequencies). Power-spectrum features are found in most recent deep learning acoustic modeling features (Amodei et al., 2015). Raw wave has been somewhat explored in few recent work (Palaz et al., 2013; 2015). ConvNets have the advantage to be flexible enough to be used with either of these input feature types. Our acoustic models output letter scores (one score per letter, given a dictionary L)."
    }, {
      "heading" : "2.2 CONVNET ACOUSTIC MODEL",
      "text" : "The acoustic models we considered in this paper are all based on standard 1D convolutional neural networks (ConvNets). ConvNets interleave convolution operations with pointwise non-linearity operations. Often ConvNets also embark pooling layers: these type of layers allow the network to “see” a larger context, without increasing the number of parameters, by locally aggregating the previous convolution operation output. Instead, our networks leverage striding convolutions. Given (xt)t=1...Tx an input sequence with Tx frames of dx dimensional vectors, a convolution with kernel width kw, stride dw and dy frame size output computes the following:\nyit = bi + dx∑ j=1 kw∑ k=1 wi,j,k x j dw×(t−1)+k ∀1 ≤ i ≤ dy, (1)\nwhere b ∈ Rdy and w ∈ Rdy×dx×kw are the parameters of the convolution (to be learned).\nPointwise non-linear layers are added after convolutional layers. In our experience, we surprisingly found that using hyperbolic tangents, their piecewise linear counterpart HardTanh (as in (Palaz et al., 2015)) or ReLU units lead to similar results.\nThere are some slight variations between the architectures, depending on the input features. MFCC-based networks need less striding, as standard MFCC filters are applied with large strides on the input\nraw sequence. With power spectrum-based and raw wave-based networks, we observed that the overall stride of the network was more important than where the convolution with strides were placed. We found thus preferrable to set the strided convolutions near the first input layers of the network, as it leads to the fastest architectures: with power spectrum features or raw wave, the input sequences are very long and the first convolutions are thus the most expensive ones.\nThe last layer of our convolutional network outputs one score per letter in the letter dictionary (dy = |L|). Our architecture for raw wave is shown in Figure 1 and is inspired by (Palaz et al., 2015). The architectures for both power spectrum and MFCC features do not include the first layer. The full network can be seen as a non-linear convolution, with a kernel width of size 31280 and stride equal to 320; given the sample rate of our data is 16KHz, label scores are produced using a window of 1955 ms, with steps of 20ms."
    }, {
      "heading" : "2.3 INFERRING SEGMENTATION WITH AUTOSEGCRITERION",
      "text" : "Most large labeled speech databases provide only a text transcription for each audio file. In a classification framework (and given our acoustic model produces letter predictions), one would need the segmentation of each letter in the transcription to train properly the model. Unfortunately, manually labeling the segmentation of each letter would be tedious. Several solutions have been explored in the speech community to alleviate this issue: HMM/GMM models use an iterative EM procedure: (i) during the Estimation step, the best segmentation is inferred, according to the current model, by maximizing the joint probability of the letter (or any sub-word unit) transcription and input sequence. (ii) During the Maximization step the model is optimized by minimizing a frame-level criterion, based on the (now fixed) inferred segmentation. This approach is also often used to boostrap the training of neural network-based acoustic models.\nOther alternatives have been explored in the context of hybrid HMM/NN systems, such as the MMI criterion (Bahl et al., 1986) which maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayse Risk (MBR) criterion (Gibson & Hain, 2006).\nMore recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription (Graves et al., 2006; Palaz et al., 2014). The most popular one is certainly the Connectionist Temporal Classification (CTC) criterion, which is at the core of Baidu’s Deep Speech architecture (Amodei et al., 2015). CTC assumes that the network output probability scores, normalized at the frame level. It considers all possible sequence of letters (or any sub-word units), which can lead to a to a given transcription. CTC also allow a special “blank” state to be optionally inserted between each letters. The rational behind the blank state is two-folds: (i) modeling “garbage” frames which might occur between each letter and (ii) identifying the separation between two identical consecutive letters in a transcription. Figure 2a shows an example of the sequences accepted by CTC for a given transcription. In practice, this graph is unfolded as shown in Figure 2b, over the available frames output by the acoustic model. We denote Gctc(θ, T ) an unfolded graph over T frames for a given transcription θ, and π = π1, . . . , πT ∈ Gctc(θ, T ) a path in this graph representing a (valid) sequence of letters for this transcription. At each time step t, each node of the graph is assigned with the corresponding log-probability letter (that we denote ft(·)) output by the acoustic model. CTC aims at maximizing the “overall” score of paths in Gctc(θ, T ); for that purpose, it minimizes the Forward score:\nCTC(θ, T ) = − logadd π∈Gctc(θ,T ) T∑ t=1 fπt(x) , (2)\nwhere the “logadd” operation, also often called “log-sum-exp” is defined as logadd(a, b) = exp(log(a) + log(b)). This overall score can be efficiently computed with the Forward algorithm. To put things in perspective, if one would replace the logadd(·) by a max(·) in (2) (which can be then efficiently computed by the Viterbi algorithm, the counterpart of the Forward algorithm), one would then maximize the score of the best path, according to the model belief. The logadd(·) can be seen as a smooth version of the max(·): paths with similar scores will be attributed the same weight in the overall score (and hence receive the same gradient), and paths with much larger score will have much more overall weight than paths with low scores. In practice, using the logadd(·) works much better than the max(·). It is also worth noting that maximizing (2) does not diverge, as the acoustic model is assumed to output normalized scores (log-probabilities) fi(·).\nIn this paper, we explore an alternative to CTC, with three differences: (i) there are no blank labels, (ii) un-normalized scores on the nodes (and possibly un-normalized transition scores on the edges) (iii) global normalization instead of per-frame normalization:\n• The advantage of (i) is that it produces a much simpler graph (see Figure 3a and Figure 3b). We found that in practice there was no advantage of having a blank class to model the possible “garbage” frames between letters. Modeling letter repetitions (which is also an important quality of the blank label in CTC) can be easily replaced by repetition character labels (we used two extra labels for two and three repetitions). For example “caterpillar” could be written as “caterpil2ar”, where “2” is a label to represent the repetition of the previous letter. Not having blank labels also simplifies the decoder.\n• With (ii) one can easily plug an external language model, which would insert transition scores on the edges of the graph. This could be particularly useful in future work, if one wanted to model representations more high-level than letters. In that respect, avoiding normalized transitions is important to alleviate the problem of “label bias” Bottou (1991); Lafferty et al. (2001). In this work, we limited ourselves to transition scalars, which are learned together with the acoustic model.\n• The normalization evoked in (iii) is necessary when using un-normalized scores on nodes or edges; it insures incorrect transcriptions will have a low confidence.\nIn the following, we name our criterion “Auto Segmentation Criterion” (ASG). Considering the same notations than for CTC in (2), and an unfolded graph Gasg(θ, T ) over T frames for a given transcription θ (as in Figure 3b), as well as a fully connected graph Gfull(θ, T ) over T frames (representing all possible sequence of letters, as in Figure 3c), ASG aims at minimizing:\nASG(θ, T ) = − logadd π∈Gasg(θ,T ) T∑ t=1 (fπt(x) + gπt−1,πt(x)) + logadd π∈Gfull(θ,T ) T∑ t=1 (fπt(x) + gπt−1,πt(x)) , (3) where gi,j(·) is a transition score model to jump from label i to label j. The left-hand part of 3 promotes sequences of letters leading to the right transcription, and the right-hand part demotes all sequences of letters. As for CTC, these two parts can be efficiently computed with the Forward algorithm. Derivatives with respect to fi(·) and gi,j(·) can be obtained (maths are a bit tedious) by applying the chain rule through the Forward recursion."
    }, {
      "heading" : "2.4 BEAM-SEARCH DECODER",
      "text" : "We wrote our own one-pass decoder, which performs a simple beam-search with beam threholding, histogram pruning and language model smearing Steinbiss et al. (1994). We kept the decoder as\nsimple as possible (under 1000 lines of C code). We did not implement any sort of model adaptation before decoding, nor any word graph rescoring. Our decoder relies on KenLM Heafield et al. (2013) for the language modeling part. It also accepts un-normalized acoustic scores (transitions and emissions from the acoustic model) as input. The decoder attempts to maximize the following:\nL(θ) = logadd π∈Gasg(θ,T ) T∑ t=1 (fπt(x) + gπt−1,πt(x)) + α logPlm(θ) + β|θ| , (4)\nwhere Plm(θ) is the probability of the language model given a transcription θ, α and β are two hyper-parameters which control the weight of the language model and the word insertion penalty respectively."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 SETUP",
      "text" : "We implemented everything using Torch71. The ASG criterion as well as the decoder were implemented in C (and then interfaced into Torch).\nWe consider as benchmark LibriSpeech, a large speech database freely available for download (Panayotov et al., 2015). LibriSpeech comes with its own train, validation and test sets. Except when specified, we used all the available data (about 1000h of audio files) for training and validating our models. We use the original 16 KHz sampling rate. The vocabulary L contains 30 graphemes: the standard English alphabet plus the apostrophe, silence, and two special “repetition” graphemes which encode the duplication (once or twice) of the previous letter (see Section 2.3).\nThe architecture hyper-parameters, as well the decoder ones were tuned using the validation set. In the following, we either report letter-error-rates (LERs) or word-error-rates (WERs). WERs have been obtained by using our own decoder (see Section 2.4), with the standard 4-gram language model provided with LibriSpeech2.\n1http://www.torch.ch. 2http://www.openslr.org/11.\nMFCC features are computed with 13 coefficients, a 25 ms sliding window and 10 ms stride. We included first and second order derivatives. Power spectrum features are computed with a 25 ms window, 10 ms stride, and have 257 components. All features are normalized (mean 0, std 1) per input sequence."
    }, {
      "heading" : "3.2 RESULTS",
      "text" : "Table 1 reports a comparison between CTC and ASG, in terms of LER and speed. Our ASG criterion is implemented in C (CPU only), leveraging SSE instructions when possible. Our batching is done with an OpenMP parallel for. We picked the CTC criterion implementation provided by Baidu3. Both criteria lead to the same LER. For comparing the speed, we report performance for sequence sizes as reported initially by Baidu, but also for longer sequence sizes, which corresponds to our average use case. ASG appears faster on long sequences, even though it is running on CPU only. Baidu’s GPU CTC implementation seems more aimed at larger vocabularies (e.g. 5000 Chinese characters).\nWe also investigated the impact of the training size on the dataset, as well as the effect of a simple data augmentation procedure, where shifts were introduced in the input frames, as well as stretching. For that purpose, we tuned the size of our architectures (given a particular size of the dataset), to avoid over-fitting. Figure 4a shows the augmentation helps for small training set size. However, with enough training data, the effect of data augmentation vanishes, and both type of features appear to perform similarly. Figure 4b reports the WER with respect to the available training data size. We observe that we compare very well against Deep Speech 1 & 2 which were trained with much more data Hannun et al. (2014); Amodei et al. (2015).\nFinally, we report in Table 2 the best results of our system so far, trained on 1000h of speech, for each type of features. The overall stride of architectures is 320 (see Figure 1), which produces a label every 20 ms. We found that one could squeeze out about 1% in performance by refining the precision of the output. This is efficiently achieved by shifting the input sequence, and feeding it to the network several times. Results in Table 2 were obtained by a single extra shift of 10 ms. Both power spectrum and raw features are performing slightly worse than MFCCs. One could expect, however, that with enough data (see Figure 4) the gap would vanish.\n3https://github.com/baidu-research/warp-ctc."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "We have introduced a simple end-to-end automatic speech recognition system, which combines a standard 1D convolutional neural network, a sequence criterion which can infer the segmentation, and a simple beam-search decoder. The decoding results are competitive on the LibriSpeech corpus with MFCC features (7.2% WER), and promising with power spectrum and raw speech (9.4% WER and 10.1% WER respectively). We showed that our AutoSegCriterion can be faster than CTC (Graves et al., 2006), and as accurate (table 1). Our approach breaks free from HMM/GMM pre-training and force-alignment, as well as not being as computationally intensive as RNN-based approaches (Amodei et al., 2015) (on average, one LibriSpeech sentence is processed in less than 60ms by our ConvNet, and the decoder runs at 8.6x on a single thread)."
    } ],
    "references" : [ {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos" ],
      "venue" : "arXiv preprint arXiv:1512.02595,",
      "citeRegEx" : "Amodei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Amodei et al\\.",
      "year" : 2015
    }, {
      "title" : "Maximum mutual information estimation of hidden markov model parameters for speech recognition",
      "author" : [ "L.R. Bahl", "P.F. Brown", "P.V. de Souza", "R.L. Mercer" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Bahl et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Bahl et al\\.",
      "year" : 1986
    }, {
      "title" : "Une approche theorique de l’apprentissage connexionniste et applications a la reconnaissance de la parole",
      "author" : [ "Leon Bottou" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Bottou.,? \\Q1991\\E",
      "shortCiteRegEx" : "Bottou.",
      "year" : 1991
    }, {
      "title" : "Global training of document processing systems using graph transformer networks",
      "author" : [ "Léon Bottou", "Yoshua Bengio", "Yann Le Cun" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Bottou et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 1997
    }, {
      "title" : "Hypothesis spaces for minimum bayes risk training in large vocabulary speech recognition",
      "author" : [ "M. Gibson", "T. Hain" ],
      "venue" : "In Proceedings of INTERSPEECH,",
      "citeRegEx" : "Gibson and Hain.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gibson and Hain.",
      "year" : 2006
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Graves et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep speech: Scaling up end-to-end speech recognition",
      "author" : [ "Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates" ],
      "venue" : "arXiv preprint arXiv:1412.5567,",
      "citeRegEx" : "Hannun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hannun et al\\.",
      "year" : 2014
    }, {
      "title" : "Scalable modified kneser-ney language model estimation",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H Clark", "Philipp Koehn" ],
      "venue" : "In ACL",
      "citeRegEx" : "Heafield et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F. Pereira" ],
      "venue" : "In Eighteenth International Conference on Machine Learning,",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "Yann LeCun", "Yoshua Bengio" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "LeCun and Bengio.,? \\Q1995\\E",
      "shortCiteRegEx" : "LeCun and Bengio.",
      "year" : 1995
    }, {
      "title" : "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Florian Metze" ],
      "venue" : "arXiv preprint arXiv:1507.08240,",
      "citeRegEx" : "Miao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2015
    }, {
      "title" : "Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing",
      "author" : [ "Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Mohamed et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohamed et al\\.",
      "year" : 2012
    }, {
      "title" : "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks",
      "author" : [ "Dimitri Palaz", "Ronan Collobert", "Mathew Magimai Doss" ],
      "venue" : "arXiv preprint arXiv:1304.1018,",
      "citeRegEx" : "Palaz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Palaz et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint phoneme segmentation inference and classification using crfs",
      "author" : [ "Dimitri Palaz", "Mathew Magimai-Doss", "Ronan Collobert" ],
      "venue" : "In Signal and Information Processing (GlobalSIP),",
      "citeRegEx" : "Palaz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Palaz et al\\.",
      "year" : 2014
    }, {
      "title" : "Analysis of cnn-based speech recognition system using raw speech as input",
      "author" : [ "Dimitri Palaz", "Ronan Collobert" ],
      "venue" : "In Proceedings of Interspeech, number EPFL-CONF-210029,",
      "citeRegEx" : "Palaz and Collobert,? \\Q2015\\E",
      "shortCiteRegEx" : "Palaz and Collobert",
      "year" : 2015
    }, {
      "title" : "Librispeech: an asr corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Panayotov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "Jhu aspire system: Robust lvcsr with tdnns, i-vector adaptation, and rnn-lms",
      "author" : [ "Vijayaditya Peddinti", "Guoguo Chen", "Vimal Manohar", "Tom Ko", "Daniel Povey", "Sanjeev Khudanpur" ],
      "venue" : "In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop,",
      "citeRegEx" : "Peddinti et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Peddinti et al\\.",
      "year" : 2015
    }, {
      "title" : "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "author" : [ "Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur" ],
      "venue" : "In Proceedings of INTERSPEECH,",
      "citeRegEx" : "Peddinti et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Peddinti et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning the speech front-end with raw waveform cldnns",
      "author" : [ "Tara N Sainath", "Ron J Weiss", "Andrew Senior", "Kevin W Wilson", "Oriol Vinyals" ],
      "venue" : "In Proc. Interspeech,",
      "citeRegEx" : "Sainath et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sainath et al\\.",
      "year" : 2015
    }, {
      "title" : "Speaker adaptation of neural network acoustic models using i-vectors",
      "author" : [ "George Saon", "Hagen Soltau", "David Nahamoo", "Michael Picheny" ],
      "venue" : "In ASRU, pp",
      "citeRegEx" : "Saon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saon et al\\.",
      "year" : 2013
    }, {
      "title" : "The ibm 2015 english conversational telephone speech recognition system",
      "author" : [ "George Saon", "Hong-Kwang J Kuo", "Steven Rennie", "Michael Picheny" ],
      "venue" : "arXiv preprint arXiv:1505.05899,",
      "citeRegEx" : "Saon et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Saon et al\\.",
      "year" : 2015
    }, {
      "title" : "Gmm-free dnn training",
      "author" : [ "Andrew Senior", "Georg Heigold", "Michiel Bacchiani", "Hank Liao" ],
      "venue" : "In Proceedings of ICASSP,",
      "citeRegEx" : "Senior et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Senior et al\\.",
      "year" : 2014
    }, {
      "title" : "Very deep multilingual convolutional neural networks for lvcsr",
      "author" : [ "Tom Sercu", "Christian Puhrsch", "Brian Kingsbury", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1509.08967,",
      "citeRegEx" : "Sercu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sercu et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint training of convolutional and non-convolutional neural networks",
      "author" : [ "Hagen Soltau", "George Saon", "Tara N Sainath" ],
      "venue" : "In ICASSP, pp",
      "citeRegEx" : "Soltau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Soltau et al\\.",
      "year" : 2014
    }, {
      "title" : "Improvements in beam search",
      "author" : [ "Volker Steinbiss", "Bach-Hiep Tran", "Hermann Ney" ],
      "venue" : "In ICSLP,",
      "citeRegEx" : "Steinbiss et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Steinbiss et al\\.",
      "year" : 1994
    }, {
      "title" : "The htk tied-state continuous speech recogniser",
      "author" : [ "Philip C Woodland", "Steve J Young" ],
      "venue" : "In Eurospeech,",
      "citeRegEx" : "Woodland and Young.,? \\Q1993\\E",
      "shortCiteRegEx" : "Woodland and Young.",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler.",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "The improvements brought by deep neural networks (DNNs) (Mohamed et al., 2012; Hinton et al., 2012) and convolutional neural networks (CNNs) (Sercu et al.",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "The improvements brought by deep neural networks (DNNs) (Mohamed et al., 2012; Hinton et al., 2012) and convolutional neural networks (CNNs) (Sercu et al.",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : ", 2012) and convolutional neural networks (CNNs) (Sercu et al., 2015; Soltau et al., 2014) for acoustic modeling only extend this training pipeline.",
      "startOffset" : 49,
      "endOffset" : 90
    }, {
      "referenceID" : 25,
      "context" : ", 2012) and convolutional neural networks (CNNs) (Sercu et al., 2015; Soltau et al., 2014) for acoustic modeling only extend this training pipeline.",
      "startOffset" : 49,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too (Panayotov et al., 2015; Peddinti et al., 2015b), with an additional step of speaker adaptation (Saon et al.",
      "startOffset" : 114,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : ", 2015b), with an additional step of speaker adaptation (Saon et al., 2013; Peddinti et al., 2015a).",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "An approach that cut ties with the HMM/GMM pipeline (and with force alignment) was to train with a recurrent neural network (RNN) (Graves et al., 2013) for phoneme transcription.",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al.",
      "startOffset" : 98,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al.",
      "startOffset" : 98,
      "endOffset" : 178
    }, {
      "referenceID" : 22,
      "context" : "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al.",
      "startOffset" : 98,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al.",
      "startOffset" : 98,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : ", 2015), trained with a sequence criterion (Graves et al., 2006).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Compared to sequence criterion based approaches that train directly from speech signal to graphemes (Miao et al., 2015), we propose a simple(r) architecture (23 millions of parameters for our best model, vs.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "100 millions of parameters in (Amodei et al., 2015)) based on convolutional networks",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : ", 2012; Hinton et al., 2012) and convolutional neural networks (CNNs) (Sercu et al., 2015; Soltau et al., 2014) for acoustic modeling only extend this training pipeline. The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too (Panayotov et al., 2015; Peddinti et al., 2015b), with an additional step of speaker adaptation (Saon et al., 2013; Peddinti et al., 2015a). Recently, Senior et al. (2014) proposed GMMfree training, but the approach still requires to generate a force alignment.",
      "startOffset" : 8,
      "endOffset" : 456
    }, {
      "referenceID" : 3,
      "context" : "for the acoustic model, toppled with a graph transformer network (Bottou et al., 1997), trained with a simpler sequence criterion.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "Our word-error-rate on clean speech is slightly better than (Hannun et al., 2014), and slightly worse than (Amodei et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : ", 2014), and slightly worse than (Amodei et al., 2015), in particular factoring that they train on 12,000 hours while we only train on the 960h available in LibriSpeech’s train set.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "Finally, some of our models are also trained on the raw waveform, as in (Palaz et al., 2013; 2015; Sainath et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 120
    }, {
      "referenceID" : 20,
      "context" : "Finally, some of our models are also trained on the raw waveform, as in (Palaz et al., 2013; 2015; Sainath et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "2 ARCHITECTURE Our speech recognition system is a standard convolutional neural network (LeCun & Bengio, 1995) fed with various different features, trained through an alternative to the Connectionist Temporal Classification (CTC) (Graves et al., 2006), and coupled with a simple beam search decoder.",
      "startOffset" : 230,
      "endOffset" : 251
    }, {
      "referenceID" : 0,
      "context" : "Power-spectrum features are found in most recent deep learning acoustic modeling features (Amodei et al., 2015).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "Raw wave has been somewhat explored in few recent work (Palaz et al., 2013; 2015).",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Other alternatives have been explored in the context of hybrid HMM/NN systems, such as the MMI criterion (Bahl et al., 1986) which maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayse Risk (MBR) criterion (Gibson & Hain, 2006).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "More recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription (Graves et al., 2006; Palaz et al., 2014).",
      "startOffset" : 208,
      "endOffset" : 249
    }, {
      "referenceID" : 15,
      "context" : "More recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription (Graves et al., 2006; Palaz et al., 2014).",
      "startOffset" : 208,
      "endOffset" : 249
    }, {
      "referenceID" : 0,
      "context" : "The most popular one is certainly the Connectionist Temporal Classification (CTC) criterion, which is at the core of Baidu’s Deep Speech architecture (Amodei et al., 2015).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "In that respect, avoiding normalized transitions is important to alleviate the problem of “label bias” Bottou (1991); Lafferty et al.",
      "startOffset" : 103,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "In that respect, avoiding normalized transitions is important to alleviate the problem of “label bias” Bottou (1991); Lafferty et al. (2001). In this work, we limited ourselves to transition scalars, which are learned together with the acoustic model.",
      "startOffset" : 103,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : "4 BEAM-SEARCH DECODER We wrote our own one-pass decoder, which performs a simple beam-search with beam threholding, histogram pruning and language model smearing Steinbiss et al. (1994). We kept the decoder as",
      "startOffset" : 162,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "Our decoder relies on KenLM Heafield et al. (2013) for the language modeling part.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "We consider as benchmark LibriSpeech, a large speech database freely available for download (Panayotov et al., 2015).",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "We observe that we compare very well against Deep Speech 1 & 2 which were trained with much more data Hannun et al. (2014); Amodei et al.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "(2014); Amodei et al. (2015). Finally, we report in Table 2 the best results of our system so far, trained on 1000h of speech, for each type of features.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "In (b) we provide Baidu Deep Speech 1 and 2 numbers on LibriSpeech, as a comparison Hannun et al. (2014); Amodei et al.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "(2014); Amodei et al. (2015).",
      "startOffset" : 8,
      "endOffset" : 29
    } ],
    "year" : 2016,
    "abstractText" : "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler. We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.",
    "creator" : "LaTeX with hyperref package"
  }
}