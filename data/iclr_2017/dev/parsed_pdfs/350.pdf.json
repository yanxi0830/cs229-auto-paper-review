{
  "name" : "350.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "UNSUPERVISED LEARNING", "William Lotter", "Gabriel Kreiman" ],
    "emails" : [ "lotter@fas.harvard.edu", "davidcox@fas.harvard.edu", "gabriel.kreiman@tch.harvard.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Many of the most successful current deep learning architectures for vision rely on supervised learning from large sets of labeled training images. While the performance of these networks is undoubtedly impressive, reliance on such large numbers of training examples limits the utility of deep learning in many domains where such datasets are not available. Furthermore, the need for large numbers of labeled examples stands at odds with human visual learning, where one or a few views of an object is often all that is needed to enable robust recognition of that object across a wide range of different views, lightings and contexts. The development of a representation that facilitates such abilities, especially in an unsupervised way, is a largely unsolved problem.\nIn addition, while computer vision models are typically trained using static images, in the real world, visual objects are rarely experienced as disjoint snapshots. Instead, the visual world is alive with movement, driven both by self-motion of the viewer and the movement of objects within the scene. Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016). For instance, Wiskott and Sejnowski proposed “slow feature analysis” as a framework for exploiting temporal structure in video streams (Wiskott & Sejnowski, 2002). Their approach attempts to build feature representations that extract\nCode and video examples can be found at: https://coxlab.github.io/prednet/\nslowly-varying parameters, such as object identity, from parameters that produce fast changes in the image, such as movement of the object. While approaches that rely on temporal coherence have arguably not yet yielded representations as powerful as those learned by supervised methods, they nonetheless point to the potential of learning useful representations from video (Mohabi et al., 2009; Sun et al., 2014; Goroshin et al., 2015a; Maltoni & Lomonaco, 2015; Wang & Gupta, 2015).\nHere, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016). A key insight here is that in order to be able to predict how the visual world will change over time, an agent must have at least some implicit model of object structure and the possible transformations objects can undergo. To this end, we have designed a neural network architecture, which we informally call a “PredNet,” that attempts to continually predict the appearance of future video frames, using a deep, recurrent convolutional network with both bottom-up and topdown connections. Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015). Predictive coding posits that the brain is continually making predictions of incoming sensory stimuli (Rao & Ballard, 1999; Friston, 2005). Top-down (and perhaps lateral) connections convey these predictions, which are compared against actual observations to generate an error signal. The error signal is then propagated back up the hierarchy, eventually leading to an update of the predictions.\nWe demonstrate the effectiveness of our model for both synthetic sequences, where we have access to the underlying generative model and can investigate what the model learns, as well as natural videos. Consistent with the idea that prediction requires knowledge of object structure, we find that these networks successfully learn internal representations that are well-suited to subsequent recognition and decoding of latent object parameters (e.g. identity, view, rotation speed, etc.). We also find that our architecture can scale effectively to natural image sequences, by training using car-mounted camera videos. The network is able to successfully learn to predict both the movement of the camera and the movement of objects in the camera’s view. Again supporting the notion of prediction as an unsupervised learning rule, the model’s learned representation in this setting supports decoding of the current steering angle."
    }, {
      "heading" : "2 THE PREDNET MODEL",
      "text" : "The PredNet architecture is diagrammed in Figure 1. The network consists of a series of repeating stacked modules that attempt to make local predictions of the input to the module, which is then subtracted from the actual input and passed along to the next layer. Briefly, each module of the network consists of four basic parts: an input convolutional layer (Al), a recurrent representation layer (Rl), a prediction layer (Âl), and an error representation (El). The representation layer, Rl, is a recurrent convolutional network that generates a prediction, Âl, of what the layer input, Al, will be on the next frame. The network takes the difference between Al and Âl and outputs an error representation, El, which is split into separate rectified positive and negative error populations. The error, El, is then passed forward through a convolutional layer to become the input to the next layer (Al+1). The recurrent prediction layerRl receives a copy of the error signalEl, along with top-down input from the representation layer of the next level of the network (Rl+1). The organization of the network is such that on the first time step of operation, the “right” side of the network (Al’s andEl’s) is equivalent to a standard deep convolutional network. Meanwhile, the “left” side of the network (the Rl’s) is equivalent to a generative deconvolutional network with local recurrence at each stage. The architecture described here is inspired by that originally proposed by (Rao & Ballard, 1999), but is formulated in a modern deep learning framework and trained end-to-end using gradient descent, with a loss function implicitly embedded in the network as the firing rates of the error neurons. Our work also shares motivation with the Deep Predictive Coding Networks of Chalasani & Principe (2013); however, their framework is based upon sparse coding and a linear dynamical system with greedy layer-wise training, whereas ours is rooted in convolutional and recurrent neural networks trained with backprop.\nWhile the architecture is general with respect to the kinds of data it models, here we focus on image sequence (video) data. Consider a sequence of images, xt. The target for the lowest layer is set to the the actual sequence itself, i.e. At0 = xt ∀t. The targets for higher layers, Atl for l > 0, are computed by a convolution over the error units from the layer below, Etl−1, followed by rectified linear unit (ReLU) activation and max-pooling. For the representation neurons, we specifically use convolutional LSTM units (Hochreiter & Schmidhuber, 1997; Shi et al., 2015). In our setting, the Rtl hidden state is updated according to R t−1 l , E t−1 l , as well as R t l+1, which is first spatially upsampled (nearest-neighbor), due to the pooling present in the feedforward path. The predictions, Âtl are made through a convolution of the R t l stack followed by a ReLU non-linearity. For the lowest layer, Âtl is also passed through a saturating non-linearity set at the maximum pixel value: SatLU(x; pmax) := min(pmax, x). Finally, the error response, Etl , is calculated from the difference between Âtl and A t l and is split into ReLU-activated positive and negative prediction errors, which are concatenated along the feature dimension. As discussed in (Rao & Ballard, 1999), although not explicit in their model, the separate error populations are analogous to the existence of on-center, off-surround and off-center, on-surround neurons early in the visual system.\nThe full set of update rules are listed in Equations (1) to (4). The model is trained to minimize the weighted sum of the activity of the error units. Explicitly, the training loss is formalized in Equation 5 with weighting factors by time, λt, and layer, λl, and where nl is the number of units in the lth layer. With error units consisting of subtraction followed by ReLU activation, the loss at each layer is equivalent to an L1 error. Although not explored here, other error unit implementations, potentially even probabilistic or adversarial (Goodfellow et al., 2014), could also be used.\nAtl = { xt if l = 0 MAXPOOL(RELU(CONV(Etl−1))) l > 0\n(1)\nÂtl = RELU(CONV(R t l)) (2)\nEtl = [RELU(A t l − Âtl); RELU(Âtl −Atl)] (3) Rtl = CONVLSTM(E t−1 l , R t−1 l ,UPSAMPLE(R t l+1)) (4)\nLtrain = ∑ t λt ∑ l λl nl ∑ nl Etl (5)\nAlgorithm 1 Calculation of PredNet states Require: xt\n1: At0 ← xt 2: E0l , R 0 l ← 0 3: for t = 1 to T do 4: for l = L to 0 do . Update Rtl states 5: if l = L then 6: RtL = CONVLSTM(E t−1 L , R t−1 L ) 7: else 8: Rtl = CONVLSTM(E t−1 l , R t−1 l ,UPSAMPLE(R t l+1))\n9: for l = 0 to L do . Update Âtl , Atl , Etl states 10: if l = 0 then 11: Ât0 = SATLU(RELU(CONV(R t 0))) 12: else 13: Âtl = RELU(CONV(R t l )) 14: Etl = [RELU(A t l − Âtl); RELU(Âtl −Alt)] 15: if l < L then 16: Atl+1 = MAXPOOL(CONV(E l t))\nThe order in which each unit in the model is updated must also be specified, and our implementation is described in Algorithm 1. Updating of states occurs through two passes: a top-down pass where the Rtl states are computed, and then a forward pass to calculate the predictions, errors, and higher level targets. A last detail of note is that Rl and El are initialized to zero, which, due to the convolutional nature of the network, means that the initial prediction is spatially uniform."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 RENDERED IMAGE SEQUENCES",
      "text" : "To gain an understanding of the representations learned in the proposed framework, we first trained PredNet models using synthetic images, for which we have access to the underlying generative stimulus model and all latent parameters. We created sequences of rendered faces rotating with two degrees of freedom, along the “pan” (out-of-plane) and “roll” (in-plane) axes. The faces start at a random orientation and rotate at a random constant velocity for a total of 10 frames. A different face was sampled for each sequence. The images were processed to be grayscale, with values normalized between 0 and 1, and 64x64 pixels in size. We used 16K sequences for training and 800 for both validation and testing.\nPredictions generated by a PredNet model are shown in Figure 2. The model is able to accumulate information over time to make accurate predictions of future frames. Since the representation neurons are initialized to zero, the prediction at the first time step is uniform. On the second time step, with no motion information yet, the prediction is a blurry reconstruction of the first time step. After further iterations, the model adapts to the underlying dynamics to generate predictions that closely match the incoming frame.\nFor choosing the hyperparameters of the model, we performed a random search and chose the model that had the lowest L1 error in frame prediction averaged over time steps 2-10 on a validation set. Given this selection criteria, the best performing models tended to have a loss solely concentrated at the lowest layer (i.e. λ0 = 1, λl>0 = 0), which is the case for the model shown. Using an equal loss at each layer considerably degraded predictions, but enforcing a moderate loss on upper layers that was one magnitude smaller than the lowest layer (i.e. λ0 = 1, λl>0 = 0.1) led to only slightly worse predictions, as illustrated in Figure 9 in the Appendix. In all cases, the time loss weight, λt, was set to zero for the first time step and then one for all time steps after. As for the remaining hyperparameters, the model shown has 5 layers with 3x3 filter sizes for all convolutions, max-pooling of stride 2, and number of channels per layer, for bothAl andRl units, of (1, 32, 64, 128, 256). Model weights were optimized using the Adam algorithm (Kingma & Ba, 2014).\nActual\nPredicted\nActual\nPredicted\nActual\nPredicted\nQuantitative evaluation of generative models is a difficult, unsolved problem (Theis et al., 2016), but here we report prediction error in terms of meansquared error (MSE) and the Structural Similarity Index Measure (SSIM) (Wang et al., 2004). SSIM is designed to be more correlated with perceptual judgments, and ranges from−1 and 1, with a larger score indicating greater similarity. We compare the PredNet to the trivial solution of copying the last\nframe, as well as a control model that shares the overall architecture and training scheme of the PredNet, but that sends forward the layer-wise activations (Al) rather than the errors (El). This model thus takes the form of a more traditional encoder-decoder pair, with a CNN encoder that has lateral skip connections to a convolutional LSTM decoder. The performance of all models on the rotating faces dataset is summarized in Table 1, where the scores were calculated as an average over all predictions after the first frame. We report results for the PredNet model trained with loss only on the lowest layer, denoted as PredNet L0, as well as the model trained with an 0.1 weight on upper layers, denoted as PredNet Lall. Both PredNet models outperformed the baselines on both measures, with the L0 model slightly outperforming Lall, as expected for evaluating the pixel-level predictions.\nSynthetic sequences were chosen as the initial training set in order to better understand what is learned in different layers of the model, specifically with respect to the underlying generative model (Kulkarni et al., 2015). The rotating faces were generated using the FaceGen software package (Singular Inversions, Inc.), which internally generates 3D face meshes by a principal component analysis in “face space”, derived from a corpus of 3D face scans. Thus, the latent parameters of the image sequences used here consist of the initial pan and roll angles, the pan and roll velocities, and the principal component (PC) values, which control the “identity” of the face. To understand the information contained in the trained models, we decoded the latent parameters from the representation neurons (Rl) in different layers, using a ridge regression. The Rl states were taken at the earliest possible informative time steps, which, in the our notation, are the second and third steps, respectively, for the static and dynamic parameters. The regression was trained using 4K sequences with 500 for validation and 1K for testing. For a baseline comparison of the information implicitly embedded in the network architecture, we compare to the decoding accuracies of an untrained network with random initial weights. Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks (Pinto et al., 2009; Jarrett et al., 2009; Saxe et al., 2010).\nLatent variable decoding accuracies of the pan and roll velocities, pan initial angle, and first PC are shown in the left panel of Figure 3. There are several interesting patterns. First, the trained models learn a representation that generally permits a better linear decoding of the underlying latent factors than the randomly initialized model, with the most striking difference in terms of the the pan rotation speed (αpan). Second, the most notable difference between the Lall and L0 versions occurs with the first principle component, where the model trained with loss on all layers has a higher decoding accuracy than the model trained with loss only on the lowest layer.\nThe latent variable decoding analysis suggests that the model learns a representation that may generalize well to other tasks for which it was not explicitly trained. To investigate this further, we assessed the models in a classification task from single, static images. We created a dataset of 25 previously unseen FaceGen faces at 7 pan angles, equally spaced between [−π2 , π 2 ], and 8 roll angles, equally spaced between [0, 2π). There were therefore 7 · 8 = 56 orientations per identity, which were tested in a cross-validated fashion. A linear SVM to decode face identity was fit on a model’s representation of a random subset of orientations and then tested on the remaining angles. For each size of the SVM training set, ranging from 1-40 orientations per face, 50 different random splits were generated, with results averaged over the splits.\nFor the static face classification task, we compare the PredNets to a standard autoencoder and a variant of the Ladder Network (Valpola, 2015; Rasmus et al., 2015). Both models were constructed to have the same number of layers and channel sizes as the PredNets, as well as a similar alternating convolution/max-pooling, then upsampling/convolution scheme. As both networks are autoencoders, they were trained with a reconstruction loss, with a dataset consisting of all of the individual frames from the sequences used to train the PredNets. For the Ladder Network, which is a denoising autoencoder with lateral skip connections, one must also choose a noise parameter, as well as the relative weights of each layer in the total cost. We tested noise levels ranging from 0 to 0.5 in increments of 0.1, with loss weights either evenly distributed across layers, solely concentrated at the pixel layer, or 1 at the bottom layer and 0.1 at upper layers (analogous to the PredNet Lall model). Shown is the model that performed best for classification, which consisted of 0.4 noise and only pixel weighting. Lastly, as in our architecture, the Ladder Network has lateral and top-down streams that are combined by a combinator function. Inspired by (Pezeshki et al., 2015), where a learnable MLP improved results, and to be consistent in comparing to the PredNet, we used a purely convolutional combinator. Given the distributed representation in both networks, we decoded from a concatenation of the feature representations at all layers, except the pixel layer. For the PredNets, the representation units were used and features were extracted after processing one input frame.\nFace classification accuracies using the representations learned by the L0 and Lall PredNets, a standard autoencoder, and a Ladder Network variant are shown in the right panel of Figure 3. Both PredNets compare favorably to the other models at all sizes of the training set, suggesting they learn a representation that is relatively tolerant to object transformations. Similar to the decoding accuracy of the first principle component, the PredNet Lall model actually outperformed the L0 variant. Altogether, these results suggest that predictive training with the PredNet can be a viable alternative to other models trained with a more traditional reconstructive or denoising loss, and that the relative layer loss weightings (λl’s) may be important for the particular task at hand."
    }, {
      "heading" : "3.2 NATURAL IMAGE SEQUENCES",
      "text" : "We next sought to test the PredNet architecture on complex, real-world sequences. As a testbed, we chose car-mounted camera videos, since these videos span across a wide range of settings and are characterized by rich temporal dynamics, including both self-motion of the vehicle and the motion of other objects in the scene (Agrawal et al., 2015). Models were trained using the raw videos from the KITTI dataset (Geiger et al., 2013), which were captured by a roof-mounted camera on a car driving around an urban environment in Germany. Sequences of 10 frames were sampled from the “City”, “Residential”, and “Road” categories, with 57 recording sessions used for training and 4 used for validation. Frames were center-cropped and downsampled to 128x160 pixels. In total, the training set consisted of roughly 41K frames.\nA random hyperparameter search, with model selection based on the validation set, resulted in a 4 layer model with 3x3 convolutions and layer channel sizes of (3, 48, 96, 192). Models were again trained with Adam (Kingma & Ba, 2014) using a loss either solely computed on the lowest layer (L0) or with a weight of 1 on the lowest layer and 0.1 on the upper layers (Lall). Adam parameters were initially set to their default values (α = 0.001, β1 = 0.9, β2 = 0.999) with the learning rate, α, decreasing by a factor of 10 halfway through training. To assess that the network had indeed learned a robust representation, we tested on the CalTech Pedestrian dataset (Dollár et al., 2009), which consists of videos from a dashboard-mounted camera on a vehicle driving around Los Angeles. Testing sequences were made to match the frame rate of the KITTI dataset and again cropped to 128x160 pixels. Quantitative evaluation was performed on the entire CalTech test partition, split into sequences of 10 frames.\nSample PredNet predictions (for the L0 model) on the CalTech Pedestrian dataset are shown in Figure 4, and example videos can be found at https://coxlab.github.io/prednet/. The model is able to make fairly accurate predictions in a wide range of scenarios. In the top sequence of Fig. 4, a car is passing in the opposite direction, and the model, while not perfect, is able to predict its trajectory, as well as fill in the ground it leaves behind. Similarly in Sequence 3, the model is able to predict the motion of a vehicle completing a left turn. Sequences 2 and 5 illustrate that the PredNet can judge its own movement, as it predicts the appearance of shadows and a stationary vehicle as they approach. The model makes reasonable predictions even in difficult scenarios, such as when the camera-mounted vehicle is turning. In Sequence 4, the model predicts the position of a tree, as the vehicle turns onto a road. The turning sequences also further illustrate the model’s ability to “fill-in”, as it is able to extrapolate sky and tree textures as unseen regions come into view. As an additional control, we show a sequence at the bottom of Fig. 4, where the input has been temporally scrambled. In this case, the model generates blurry frames, which mostly just resemble the previous frame. Finally, although the PredNet shown here was trained to predict one frame ahead, it is also possible to predict multiple frames into the future, by feeding back predictions as the inputs and recursively iterating. We explore this in Appendix 5.3.\nQuantitatively, the PredNet models again outperformed the CNN-LSTM EncoderDecoder. To ensure that the difference in performance was not simply because of the choice of hyperparameters, we trained models with four other sets of hyperparameters, which were sampled from the initial random search over the number of layers, fil-\nter sizes, and number of filters per layer. For each of the four additional sets, the PredNet L0 had the best performance, with an average error reduction of 14.7% and 14.9% for MSE and SSIM,\nrespectively, compared to the CNN-LSTM Encoder-Decoder. More details, as well as a thorough investigation of systematically simplified models on the continuum between the PredNet and the CNN-LSTM Encoder-Decoder can be found in Appendix 5.1. Briefly, the elementwise subtraction operation in the PredNet seems to be beneficial, and the nonlinearity of positive/negative splitting also adds modest improvements. Finally, while these experiments measure the benefits of each component of our model, we also directly compare against recent work in a similar car-cam setting, by reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. (2016). Our PredNet model outperforms the model by Brabandere et al. (2016) by 29%. Details can be found in Appendix 5.2. Also in Appendix 5.2, we present results for the Human3.6M (Ionescu et al., 2014) dataset, as reported by Finn et al. (2016). Without re-optimizing hyperparameters, our\nmodel underperforms the concurrently developed DNA model by Finn et al. (2016), but outperforms the model by Mathieu et al. (2016).\nTo test the implicit encoding of latent parameters in the car-cam setting, we used the internal representation in the PredNet to estimate the car’s steering angle (Bojarski et al., 2016; Biasini et al., 2016). We used a dataset released by Comma.ai (Biasini et al., 2016) consisting of 11 videos totaling about 7 hours of mostly highway driving. We first trained networks for next-frame prediction and then fit a linear fully-connected layer on the learned representation to estimate the steering angle, using a MSE loss. We again concatenate the Rl representation at all layers, but first spatially average pool lower layers to match the spatial size of the upper layer, in order to reduce dimensionality. Steering angle estimation results, using the representation on the 10th time step, are shown in Figure 5. Given just 1K labeled training examples, a simple linear readout on the PredNet L0 representation explains 74% of the variance in the steering angle and outperforms the CNN-LSTM Enc.-Dec. by 35%. With 25K labeled training examples, the PredNet L0 has a MSE (in degrees2) of 2.14. As a point of reference, a CNN model designed to predict the steering angle (Biasini et al., 2016), albeit from a single frame instead of multiple frames, achieve a MSE of ~4 when trained end-to-end using 396K labeled training examples. Details of this analysis can be found in Appendix 8. Interestingly, in this task, the PredNet Lall model actually underperformed the L0 model and slightly underperformed the CNN-LSTM Enc.-Dec, again suggesting that the λl parameter can affect the representation learned, and different values may be preferable in different end tasks. Nonetheless, the readout from the Lall model still explained a substantial proportion of the steering angle variance and strongly outperformed the random initial weights. Overall, this analysis again demonstrates that a representation learned through prediction, and particularly with the PredNet model with appropriate hyperparameters, can contain useful information about underlying latent parameters."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "Above, we have demonstrated a predictive coding inspired architecture that is able to predict future frames in both synthetic and natural image sequences. Importantly, we have shown that learning to predict how an object or scene will move in a future frame confers advantages in decoding latent parameters (such as viewing angle) that give rise to an object’s appearance, and can improve recognition performance. More generally, we argue that prediction can serve as a powerful unsupervised learning signal, since accurately predicting future frames requires at least an implicit model of the objects that make up the scene and how they are allowed to move. Developing a deeper understanding of the nature of the representations learned by the networks, and extending the architecture, by, for instance, allowing sampling, are important future directions."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Rasmus Berg Palm for fruitful discussions and early brainstorming. We would also like to thank the developers of Keras (Chollet, 2016). This work was supported by IARPA (contract D16PC00002), the National Science Foundation (NSF IIS 1409097), and the Center for Brains, Minds and Machines (CBMM, NSF STC award CCF-1231216)."
    }, {
      "heading" : "5 APPENDIX",
      "text" : ""
    }, {
      "heading" : "5.1 ADDITIONAL CONTROL MODELS",
      "text" : "Table 3 contains results for additional variations of the PredNet and CNN-LSTM Encoder-Decoder evaluated on the CalTech Pedestrian Dataset after being trained on KITTI. We evaluate the models in terms of pixel prediction, thus using the PredNet model trained with loss only on the lowest layer (PredNet L0) as the base model. In addition to mean-squared error (MSE) and the Structural Similarity Index Measure (SSIM), we include calculations of the Peak Signal-To-Noise Ratio (PSNR). For each model, we evaluate it with the original set of hyperparameters (controlling the number of layers, filter sizes, and number of filters per layer), as well as with the four additional sets of hyperparameters that were randomly sampled from the initial random search (see main text for more details). Below is an explanation of the additional control models:\nMSE (x 10−3) PSNR SSIM PredNet 3.13 (3.33) 25.8 (25.5) 0.884 (0.878) PredNet (no El split) 3.20 (3.37) 25.6 (25.4) 0.883 (0.878) CNN-LSTM Enc.-Dec. 3.67 (3.91) 25.0 (24.6) 0.865 (0.856) CNN-LSTM Enc.-Dec. (2x Al filts) 3.82 (3.97) 24.8 (24.6) 0.857 (0.853) CNN-LSTM Enc.-Dec. (except pass E0) 3.41 (3.61) 25.4 (25.1) 0.873 (0.866) CNN-LSTM Enc.-Dec. (+/- split) 3.71 (3.84) 24.9 (24.7) 0.861 (0.857) Copy Last Frame 7.95 20.0 0.762\nEqualizing the number of filters in the CNN-LSTM Encoder-Decoder (2x Al filts) cannot account for its performance difference with the PredNet, and actually leads to overfitting and a decrease in performance. Passing the error at the lowest layer (E0) in the CNN-LSTM Enc.-Dec. improves performance, but still does not match the PredNet, where errors are passed at all layers. Finally, splitting the activationsAl into positive and negative populations in the CNN-LSTM Enc.-Dec. does not help, but the PredNet with linear error activation (“no El split”) performs slightly worse than the original split version. Together, these results suggest that the PredNet’s error passing operation can lead to improvements in next-frame prediction performance."
    }, {
      "heading" : "5.2 COMPARING AGAINST OTHER MODELS",
      "text" : "While our main comparison in the text was a control model that isolates the effects of the more unique components in the PredNet, here we directly compare against other published models. We report results on a 64x64 pixel, grayscale car-cam dataset and the Human3.6M dataset (Ionescu et al., 2014) to compare against the two concurrently developed models by Brabandere et al. (2016)\nand Finn et al. (2016), respectively. For both comparisons, we use a model with the same hyperparameters (# of layers, # of filters, etc.) of the PredNet L0 model trained on KITTI, but train from scratch on the new datasets. The only modification we make is to train using an L2 loss instead of the effective L1 loss, since both models train with an L2 loss and report results using L2-based metrics (MSE for Brabandere et al. (2016) and PSNR for Finn et al. (2016)). That is, we keep the original PredNet model intact but directly optimize using MSE between actual and predicted frames. We measure next-frame prediction performance after inputting 3 frames and 10 frames, respectively, for the 64x64 car-cam and Human3.6M datasets, to be consistent with the published works. We also include the results using a feedforward multi-scale network, similar to the model of Mathieu et al. (2016), on Human3.6M, as reported by Finn et al. (2016).\nOn a dataset similar to KITTI, our model outperforms the model proposed by Brabandere et al. (2016). On Human3.6M, our model outperforms a model similar to (Mathieu et al., 2016), but underperforms Finn et al. (2016), although we note we did not perform any hyperparameter optimization."
    }, {
      "heading" : "5.3 MULTIPLE TIME STEP PREDICTION",
      "text" : "While the models presented here were originally trained to predict one frame ahead, they can be made to predict multiple frames by treating predictions as actual input and recursively iterating. Examples of this process are shown in Figure 6 for the PredNet L0 model. Although the next frame predictions are reasonably accurate, the model naturally breaks down when extrapolating further into the future. This is not surprising since the predictions will unavoidably have different statistics than the natural images for which the model was trained to handle (Bengio et al., 2015). If we additionally train the model to process its own predictions, the model is better able to extrapolate. The third row for every sequence shows the output of the original PredNet fine-tuned for extrapolation. Starting from the trained weights, the model was trained with a loss over 15 time steps, where the actual frame was inputted for the first 10 and then the model’s predictions were used as input to the network for the last 5. For the first 10 time steps, the training loss was calculated on the El activations as usual, and for the last 5, it was calculated directly as the mean absolute error with respect to the ground truth frames. Despite eventual blurriness (which might be expected to some extent due to uncertainty), the fine-tuned model captures some key structure in its extrapolations after the tenth time step. For instance, in the first sequence, the model estimates the general shape of an upcoming shadow, despite minimal information in the last seen frame. In the second sequence, the model is able to extrapolate the motion of a car moving to the right. The reader is again encouraged to visit https://coxlab.github.io/prednet/ to view the predictions in video form. Quantitatively, the MSE of the model’s predictions stay well below the trivial solution of copying the last seen frame, as illustrated in Fig 7. The MSE increases fairly linearly from time steps 2-10, even though the model was only trained for up to t+ 5 prediction."
    }, {
      "heading" : "5.4 ADDITIONAL STEERING ANGLE ANALYSIS",
      "text" : "In Figure 8, we show the steering angle estimation accuracy on the Comma.ai (Biasini et al., 2016) dataset using the representation learned by the PredNet L0 model, as a function of the number of frames inputted into the model. The PredNet’s representation at all layers was concatenated (after spatially pooling lower layers to a common spatial resolution) and a fully-connected readout was fit using MSE. For each level of the number of training examples, we average over 10 cross-validation splits. To serve as points of reference, we include results for two static models. The first model is an autoencoder trained on single frame reconstruction with appropriately matching hyperparameters. A fully-connected layer was fit on the autoencoder’s representation to estimate the steering angle in the same fashion as the PredNet. The second model is the default model in the posted Comma.ai code (Biasini et al., 2016), which is a five layer CNN. This model is trained end-to-end to estimate\nthe steering angle given the current frame as input, with a MSE loss. In addition to 25K examples, we trained a version using all of the frames in the Comma dataset (~396K). For all models, the final weights were chosen at the minimum validation error during training. Given the relatively small number of videos in the dataset compared to the average duration of each video, we used 5% of each video for validation and testing, chosen as a random continuous chunk, and discarded the 10 frames before and after the chosen segments from the training set.\nAs illustrated in Figure 8, the PredNet’s performance gets better over time, as one might expect, as the model is able to accumulate more information. Interestingly, it performs reasonably well after just one time step, in a regime that is orthogonal to the training procedure of the PredNet where there are no dynamics. Altogether, these results again point to the usefulness of the model in learning underlying latent parameters.\n5.5 PREDNET Lall NEXT-FRAME PREDICTIONS\nFigures 9 and 10 compare next-frame predictions by the PredNet Lall model, trained with a prediction loss on all layers (λ0 = 1, λl>0 = 0.1), and the PredNet L0 model, trained with a loss only on the lowest layer. At first glance, the difference in predictions seem fairly minor, and indeed, in terms of MSE, the Lall model only underperformed the L0 version by 3% and 6%, respectively, for the rotating faces and CalTech Pedestrian datasets. Upon careful inspection, however, it is apparent that the Lall predictions lack some of the finer details of the L0 predictions and are more blurry in regions of high variance. For instance, with the rotating faces, the facial features are less defined and with CalTech, details of approaching shadows and cars are less precise.\nActual\nPredNet \uD835\uDC3F0\nPredNet \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59\nError \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59 - \uD835\uDC3F0\nActual\nPredNet \uD835\uDC3F0\nPredNet \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59\nError \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59 - \uD835\uDC3F0\nActual\nPredNet \uD835\uDC3F0\nPredNet \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59\nError \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59 - \uD835\uDC3F0\nActual\nPredNet \uD835\uDC3F0\nPredNet \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59\nError \uD835\uDC3F\uD835\uDC4E\uD835\uDC59\uD835\uDC59 - \uD835\uDC3F0"
    } ],
    "references" : [ {
      "title" : "Learning to see by moving",
      "author" : [ "Pulkit Agrawal", "João Carreira", "Jitendra Malik" ],
      "venue" : null,
      "citeRegEx" : "Agrawal et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2015
    }, {
      "title" : "Canonical microcircuits for predictive coding",
      "author" : [ "Andre M. Bastos", "W. Martin Usrey", "Rick A. Adams", "George R. Mangun", "Pascal Fries", "Karl J. Friston" ],
      "venue" : null,
      "citeRegEx" : "Bastos et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastos et al\\.",
      "year" : 2012
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "How auto-encoders could provide credit assignment in deep networks via target propagation",
      "author" : [ "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bengio.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2014
    }, {
      "title" : "End to end learning for self-driving cars",
      "author" : [ "Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D. Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang", "Xin Zhang", "Jake Zhao", "Karol Zieba" ],
      "venue" : null,
      "citeRegEx" : "Bojarski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bojarski et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic filter networks",
      "author" : [ "Bert De Brabandere", "Xu Jia", "Tinne Tuytelaars", "Luc Van Gool" ],
      "venue" : null,
      "citeRegEx" : "Brabandere et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Brabandere et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep predictive coding",
      "author" : [ "Rakesh Chalasani", "Jose C. Principe" ],
      "venue" : null,
      "citeRegEx" : "Chalasani and Principe.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chalasani and Principe.",
      "year" : 2013
    }, {
      "title" : "Comma.ai, 2016. URL http://keras.io",
      "author" : [ "François Chollet" ],
      "venue" : null,
      "citeRegEx" : "Chollet.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2016
    }, {
      "title" : "Whatever next? predictive brains, situated agents, and the future of cognitive science",
      "author" : [ "Andy Clark" ],
      "venue" : "Behavioral and Brain Sciences,",
      "citeRegEx" : "Clark.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clark.",
      "year" : 2013
    }, {
      "title" : "Pedestrian detection: A benchmark",
      "author" : [ "Piotr Dollár", "Christian Wojek", "Bernt Schiele", "Pietro Perona" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Dollár et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dollár et al\\.",
      "year" : 2009
    }, {
      "title" : "Expectation and surprise determine neural population responses in the ventral visual stream",
      "author" : [ "Tobias Egner", "Jim M. Monti", "Christopher Summerfield" ],
      "venue" : "J Neurosci,",
      "citeRegEx" : "Egner et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Egner et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised learning for physical interaction through video prediction",
      "author" : [ "Chelsea Finn", "Ian J. Goodfellow", "Sergey Levine" ],
      "venue" : null,
      "citeRegEx" : "Finn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning invariance from transformation sequences",
      "author" : [ "Peter Földiák" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Földiák.,? \\Q1991\\E",
      "shortCiteRegEx" : "Földiák.",
      "year" : 1991
    }, {
      "title" : "A theory of cortical responses",
      "author" : [ "Karl Friston" ],
      "venue" : "Philos Trans R Soc Lond B Biol Sci,",
      "citeRegEx" : "Friston.,? \\Q2005\\E",
      "shortCiteRegEx" : "Friston.",
      "year" : 2005
    }, {
      "title" : "Vision meets robotics: The kitti dataset",
      "author" : [ "Andreas Geiger", "Philip Lenz", "Christoph Stiller", "Raquel Urtasun" ],
      "venue" : "International Journal of Robotics Research (IJRR),",
      "citeRegEx" : "Geiger et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2013
    }, {
      "title" : "A hierarchical bayesian model of invariant pattern recognition in the visual cortex",
      "author" : [ "Dileep George", "Jeff Hawkins" ],
      "venue" : "In Proceedings of the International Joint Conference on Neural Networks",
      "citeRegEx" : "George and Hawkins.,? \\Q2005\\E",
      "shortCiteRegEx" : "George and Hawkins.",
      "year" : 2005
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of spatiotemporally coherent metrics",
      "author" : [ "Ross Goroshin", "Joan Bruna", "Jonathan Tompson", "David Eigen", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Goroshin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goroshin et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to linearize under uncertainty",
      "author" : [ "Ross Goroshin", "Michaël Mathieu", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Goroshin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goroshin et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jurgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
      "author" : [ "Catalin Ionescu", "Dragos Papava", "Vlad Olaru", "Cristian Sminchisescu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Ionescu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ionescu et al\\.",
      "year" : 2014
    }, {
      "title" : "What is the best multistage architecture for object recognition",
      "author" : [ "Kevin Jarrett", "Koray Kavukcuoglu", "MarcAurelio Ranzato", "Yann LeCun" ],
      "venue" : "In ICCV",
      "citeRegEx" : "Jarrett et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jarrett et al\\.",
      "year" : 2009
    }, {
      "title" : "Cerebral hierarchies : predictive processing , precision and the pulvinar",
      "author" : [ "Ryota Kanai", "Yutaka Komura", "Stewart Shipp", "Karl Friston" ],
      "venue" : "Philos Trans R Soc Lond B Biol Sci,",
      "citeRegEx" : "Kanai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kanai et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Deep convolutional inverse graphics",
      "author" : [ "Tejas D. Kulkarni", "Will Whitney", "Pushmeet Kohli", "Joshua B. Tenenbaum" ],
      "venue" : null,
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical bayesian inference in the visual cortex",
      "author" : [ "Tai Sing Lee", "David Mumford" ],
      "venue" : "J Opt Soc Am A Opt Image Sci Vis,",
      "citeRegEx" : "Lee and Mumford.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lee and Mumford.",
      "year" : 2003
    }, {
      "title" : "Unsupervised learning of visual structure using predictive generative networks",
      "author" : [ "William Lotter", "Gabriel Kreiman", "David Cox" ],
      "venue" : null,
      "citeRegEx" : "Lotter et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lotter et al\\.",
      "year" : 2015
    }, {
      "title" : "Semi-supervised tuning from temporal coherence",
      "author" : [ "Davide Maltoni", "Vincenzo Lomonaco" ],
      "venue" : null,
      "citeRegEx" : "Maltoni and Lomonaco.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maltoni and Lomonaco.",
      "year" : 2015
    }, {
      "title" : "Deep multi-scale video prediction beyond mean square error",
      "author" : [ "Michaël Mathieu", "Camille Couprie", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Mathieu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mathieu et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling deep temporal dependencies with recurrent ”grammar cells",
      "author" : [ "Vincent Michalski", "Roland Memisevic", "Kishore Konda" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Michalski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Michalski et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning from temporal coherence in video",
      "author" : [ "Hossein Mohabi", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In ICML",
      "citeRegEx" : "Mohabi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mohabi et al\\.",
      "year" : 2009
    }, {
      "title" : "Actionconditional video prediction using deep networks in atari",
      "author" : [ "Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L. Lewis", "Satinder P. Singh" ],
      "venue" : null,
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning through time in the thalamocortical loops",
      "author" : [ "Randall C. O’Reilly", "Dean Wyatte", "John Rohrlich" ],
      "venue" : null,
      "citeRegEx" : "O.Reilly et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "O.Reilly et al\\.",
      "year" : 2014
    }, {
      "title" : "Prediction as a candidate for learning deep hierarchical models of data",
      "author" : [ "Rasmus Berg Palm" ],
      "venue" : "Technical University of Denmark,",
      "citeRegEx" : "Palm.,? \\Q2012\\E",
      "shortCiteRegEx" : "Palm.",
      "year" : 2012
    }, {
      "title" : "Spatio-temporal video autoencoder with differentiable memory",
      "author" : [ "Viorica Patraucean", "Ankur Handa", "Roberto Cipolla" ],
      "venue" : null,
      "citeRegEx" : "Patraucean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Patraucean et al\\.",
      "year" : 2015
    }, {
      "title" : "Deconstructing the ladder network",
      "author" : [ "Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron C. Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Pezeshki et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pezeshki et al\\.",
      "year" : 2015
    }, {
      "title" : "A high-throughput screening approach to discovering good forms of biologically inspired visual representation",
      "author" : [ "Nicolas Pinto", "David Doukhan", "James J. DiCarlo", "David D. Cox" ],
      "venue" : "PLoS Comput Biol,",
      "citeRegEx" : "Pinto et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pinto et al\\.",
      "year" : 2009
    }, {
      "title" : "Video (language) modeling: a baseline for generative models of natural videos",
      "author" : [ "Marc’Aurelio Ranzato", "Arthur Szlam", "Joan Bruna", "Michaël Mathieu", "Ronan Collobert", "Sumit Chopra" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2014
    }, {
      "title" : "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects",
      "author" : [ "Rajesh P.N. Rao", "Dana H. Ballard" ],
      "venue" : "Nature Neuroscience,",
      "citeRegEx" : "Rao and Ballard.,? \\Q1999\\E",
      "shortCiteRegEx" : "Rao and Ballard.",
      "year" : 1999
    }, {
      "title" : "Predictive sequence learning in recurrent neocortical circuits",
      "author" : [ "Rajesh P.N. Rao", "T.J. Sejnowski" ],
      "venue" : null,
      "citeRegEx" : "Rao and Sejnowski.,? \\Q2000\\E",
      "shortCiteRegEx" : "Rao and Sejnowski.",
      "year" : 2000
    }, {
      "title" : "Semisupervised learning with ladder",
      "author" : [ "Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko" ],
      "venue" : null,
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning a driving simulator",
      "author" : [ "Eder Santana", "George Hotz" ],
      "venue" : null,
      "citeRegEx" : "Santana and Hotz.,? \\Q2016\\E",
      "shortCiteRegEx" : "Santana and Hotz.",
      "year" : 2016
    }, {
      "title" : "On random weights and unsupervised feature learning",
      "author" : [ "Andrew Saxe", "Maneesh Bhand", "Zhenghao Chen", "Pang Wei Koh", "Bipin Suresh", "Andrew Y. Ng" ],
      "venue" : "In Workshop: Deep Learning and Unsupervised Feature Learning (NIPS)",
      "citeRegEx" : "Saxe et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2010
    }, {
      "title" : "Convolutional LSTM network: A machine learning approach for precipitation nowcasting",
      "author" : [ "Xingjian Shi", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai-Kin Wong", "Wang-chun Woo" ],
      "venue" : null,
      "citeRegEx" : "Shi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised learning of generative and discriminative weights encoding elementary image components in a predictive coding model of cortical function",
      "author" : [ "M.W. Spratling" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Spratling.,? \\Q2012\\E",
      "shortCiteRegEx" : "Spratling.",
      "year" : 2012
    }, {
      "title" : "Unsupervised learning of video representations using lstms",
      "author" : [ "Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Predictive codes for forthcoming perception in the frontal cortex",
      "author" : [ "Christopher Summerfield", "Tobias Egner", "Matthew Greene", "Etienne Koechlin", "Jennifer Mangels", "Joy Hirsch" ],
      "venue" : null,
      "citeRegEx" : "Summerfield et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Summerfield et al\\.",
      "year" : 2006
    }, {
      "title" : "Dl-sfa: Deeplylearned slow feature analysis for action recognition",
      "author" : [ "Lin Sun", "Kui Jia", "Tsung-Han Chan", "Yuqiang Fang", "Gang Wang", "Shuicheng Yan" ],
      "venue" : null,
      "citeRegEx" : "Sun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Lucas Theis", "Aaron van den Oord", "Matthias Bethge" ],
      "venue" : null,
      "citeRegEx" : "Theis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2016
    }, {
      "title" : "From neural pca to deep unsupervised learning",
      "author" : [ "Harri Valpola" ],
      "venue" : null,
      "citeRegEx" : "Valpola.,? \\Q2015\\E",
      "shortCiteRegEx" : "Valpola.",
      "year" : 2015
    }, {
      "title" : "Generating videos with scene dynamics",
      "author" : [ "Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba" ],
      "venue" : null,
      "citeRegEx" : "Vondrick et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vondrick et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised learning of visual representations using videos",
      "author" : [ "Xiaolong Wang", "Abhinav Gupta" ],
      "venue" : null,
      "citeRegEx" : "Wang and Gupta.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang and Gupta.",
      "year" : 2015
    }, {
      "title" : "Image quality assessment: From error visibility to structural similarity",
      "author" : [ "Zhou Wang", "Alan Bovik", "Hamid Sheikh", "Eero Simoncelli" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "Wang et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2004
    }, {
      "title" : "Understanding visual concepts with continuation learning",
      "author" : [ "William F. Whitney", "Michael Chang", "Tejas D. Kulkarni", "Joshua B. Tenenbaum" ],
      "venue" : null,
      "citeRegEx" : "Whitney et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Whitney et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning invariance from transformation sequences",
      "author" : [ "Laurenz Wiskott", "Terrence J. Sejnowski" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Wiskott and Sejnowski.,? \\Q2002\\E",
      "shortCiteRegEx" : "Wiskott and Sejnowski.",
      "year" : 2002
    }, {
      "title" : "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks",
      "author" : [ "Tianfan Xue", "Jiajun Wu", "Katherine L. Bouman", "William T. Freeman" ],
      "venue" : null,
      "citeRegEx" : "Xue et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 33,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 32,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 0,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 26,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 28,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 45,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 53,
      "context" : "Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Földiák, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O’Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 447
    }, {
      "referenceID" : 30,
      "context" : "While approaches that rely on temporal coherence have arguably not yet yielded representations as powerful as those learned by supervised methods, they nonetheless point to the potential of learning useful representations from video (Mohabi et al., 2009; Sun et al., 2014; Goroshin et al., 2015a; Maltoni & Lomonaco, 2015; Wang & Gupta, 2015).",
      "startOffset" : 233,
      "endOffset" : 342
    }, {
      "referenceID" : 47,
      "context" : "While approaches that rely on temporal coherence have arguably not yet yielded representations as powerful as those learned by supervised methods, they nonetheless point to the potential of learning useful representations from video (Mohabi et al., 2009; Sun et al., 2014; Goroshin et al., 2015a; Maltoni & Lomonaco, 2015; Wang & Gupta, 2015).",
      "startOffset" : 233,
      "endOffset" : 342
    }, {
      "referenceID" : 33,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 32,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 45,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 28,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 34,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 11,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 50,
      "context" : "Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O’Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 314
    }, {
      "referenceID" : 37,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 29,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 45,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 28,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 26,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 34,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 31,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 11,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 55,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 50,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 5,
      "context" : "Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al.",
      "startOffset" : 69,
      "endOffset" : 310
    }, {
      "referenceID" : 13,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 46,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 10,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 1,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 44,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 8,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 32,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 22,
      "context" : ", 2016), but we take particular inspiration from the concept of “predictive coding” from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O’Reilly et al., 2014; Kanai et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 365
    }, {
      "referenceID" : 13,
      "context" : "Predictive coding posits that the brain is continually making predictions of incoming sensory stimuli (Rao & Ballard, 1999; Friston, 2005).",
      "startOffset" : 102,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "Each layer consists of representation neurons (Rl), which output a layer-specific prediction at each time step (Âl), which is compared against a target (Al) (Bengio, 2014) to produce an error term (El), which is then propagated laterally and vertically in the network.",
      "startOffset" : 157,
      "endOffset" : 171
    }, {
      "referenceID" : 43,
      "context" : "For the representation neurons, we specifically use convolutional LSTM units (Hochreiter & Schmidhuber, 1997; Shi et al., 2015).",
      "startOffset" : 77,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "Although not explored here, other error unit implementations, potentially even probabilistic or adversarial (Goodfellow et al., 2014), could also be used.",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 48,
      "context" : "631 Quantitative evaluation of generative models is a difficult, unsolved problem (Theis et al., 2016), but here we report prediction error in terms of meansquared error (MSE) and the Structural Similarity Index Measure (SSIM) (Wang et al.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 52,
      "context" : ", 2016), but here we report prediction error in terms of meansquared error (MSE) and the Structural Similarity Index Measure (SSIM) (Wang et al., 2004).",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : "Synthetic sequences were chosen as the initial training set in order to better understand what is learned in different layers of the model, specifically with respect to the underlying generative model (Kulkarni et al., 2015).",
      "startOffset" : 201,
      "endOffset" : 224
    }, {
      "referenceID" : 36,
      "context" : "Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks (Pinto et al., 2009; Jarrett et al., 2009; Saxe et al., 2010).",
      "startOffset" : 159,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks (Pinto et al., 2009; Jarrett et al., 2009; Saxe et al., 2010).",
      "startOffset" : 159,
      "endOffset" : 220
    }, {
      "referenceID" : 42,
      "context" : "Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks (Pinto et al., 2009; Jarrett et al., 2009; Saxe et al., 2010).",
      "startOffset" : 159,
      "endOffset" : 220
    }, {
      "referenceID" : 49,
      "context" : "For the static face classification task, we compare the PredNets to a standard autoencoder and a variant of the Ladder Network (Valpola, 2015; Rasmus et al., 2015).",
      "startOffset" : 127,
      "endOffset" : 163
    }, {
      "referenceID" : 40,
      "context" : "For the static face classification task, we compare the PredNets to a standard autoencoder and a variant of the Ladder Network (Valpola, 2015; Rasmus et al., 2015).",
      "startOffset" : 127,
      "endOffset" : 163
    }, {
      "referenceID" : 35,
      "context" : "Inspired by (Pezeshki et al., 2015), where a learnable MLP improved results, and to be consistent in comparing to the PredNet, we used a purely convolutional combinator.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "As a testbed, we chose car-mounted camera videos, since these videos span across a wide range of settings and are characterized by rich temporal dynamics, including both self-motion of the vehicle and the motion of other objects in the scene (Agrawal et al., 2015).",
      "startOffset" : 242,
      "endOffset" : 264
    }, {
      "referenceID" : 14,
      "context" : "Models were trained using the raw videos from the KITTI dataset (Geiger et al., 2013), which were captured by a roof-mounted camera on a car driving around an urban environment in Germany.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "To assess that the network had indeed learned a robust representation, we tested on the CalTech Pedestrian dataset (Dollár et al., 2009), which consists of videos from a dashboard-mounted camera on a vehicle driving around Los Angeles.",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "6M (Ionescu et al., 2014) dataset, as reported by Finn et al.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "Finally, while these experiments measure the benefits of each component of our model, we also directly compare against recent work in a similar car-cam setting, by reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. (2016). Our PredNet model outperforms the model by Brabandere et al.",
      "startOffset" : 238,
      "endOffset" : 263
    }, {
      "referenceID" : 5,
      "context" : "Finally, while these experiments measure the benefits of each component of our model, we also directly compare against recent work in a similar car-cam setting, by reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. (2016). Our PredNet model outperforms the model by Brabandere et al. (2016) by 29%.",
      "startOffset" : 238,
      "endOffset" : 332
    }, {
      "referenceID" : 5,
      "context" : "Finally, while these experiments measure the benefits of each component of our model, we also directly compare against recent work in a similar car-cam setting, by reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. (2016). Our PredNet model outperforms the model by Brabandere et al. (2016) by 29%. Details can be found in Appendix 5.2. Also in Appendix 5.2, we present results for the Human3.6M (Ionescu et al., 2014) dataset, as reported by Finn et al. (2016). Without re-optimizing hyperparameters, our",
      "startOffset" : 238,
      "endOffset" : 503
    }, {
      "referenceID" : 4,
      "context" : "To test the implicit encoding of latent parameters in the car-cam setting, we used the internal representation in the PredNet to estimate the car’s steering angle (Bojarski et al., 2016; Biasini et al., 2016).",
      "startOffset" : 163,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "model underperforms the concurrently developed DNA model by Finn et al. (2016), but outperforms the model by Mathieu et al.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "model underperforms the concurrently developed DNA model by Finn et al. (2016), but outperforms the model by Mathieu et al. (2016). To test the implicit encoding of latent parameters in the car-cam setting, we used the internal representation in the PredNet to estimate the car’s steering angle (Bojarski et al.",
      "startOffset" : 60,
      "endOffset" : 131
    } ],
    "year" : 2017,
    "abstractText" : "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning — leveraging unlabeled examples to learn about the structure of a domain — remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (“PredNet”) architecture that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",
    "creator" : "LaTeX with hyperref package"
  }
}