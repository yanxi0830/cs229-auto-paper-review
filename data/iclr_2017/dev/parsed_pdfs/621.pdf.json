{
  "name" : "621.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Cheng-Zhi Anna Huang", "Tim Cooijmans", "Aaron Courville", "Douglas Eck" ],
    "emails" : [ "chengzhiannahuang@gmail.com", "tim.cooijmans@umontreal.ca", "adarob@google.com", "aaron.courville@umontreal.ca", "deck@google.com" ],
    "sections" : null,
    "references" : [ {
      "title" : "Harmonising chorales by probabilistic inference",
      "author" : [ "Moray Allan", "Christopher KI Williams" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Allan and Williams.,? \\Q2005\\E",
      "shortCiteRegEx" : "Allan and Williams.",
      "year" : 2005
    }, {
      "title" : "Dynamic capacity networks",
      "author" : [ "Amjad Almahairi", "Nicolas Ballas", "Tim Cooijmans", "Yin Zheng", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1511.07838,",
      "citeRegEx" : "Almahairi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Almahairi et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical inference for probabilistic functions of finite state markov chains",
      "author" : [ "Leonard E Baum", "Ted Petrie" ],
      "venue" : "The annals of mathematical statistics,",
      "citeRegEx" : "Baum and Petrie.,? \\Q1966\\E",
      "shortCiteRegEx" : "Baum and Petrie.",
      "year" : 1966
    }, {
      "title" : "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription",
      "author" : [ "Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Boulanger.Lewandowski et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boulanger.Lewandowski et al\\.",
      "year" : 2012
    }, {
      "title" : "music21: A toolkit for computer-aided musicology and symbolic music",
      "author" : [ "Michael Scott Cuthbert", "Christopher Ariza" ],
      "venue" : null,
      "citeRegEx" : "Cuthbert and Ariza.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cuthbert and Ariza.",
      "year" : 2010
    }, {
      "title" : "A learned representation for artistic style",
      "author" : [ "Vincent Dumoulin", "Johnathon Shlens", "Manjunath Kudlur" ],
      "venue" : "arXiv preprint arXiv:1610.07629,",
      "citeRegEx" : "Dumoulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural algorithm of artistic style",
      "author" : [ "Leon A Gatys", "Alexander S Ecker", "Matthias Bethge" ],
      "venue" : "arXiv preprint arXiv:1508.06576,",
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "Polyphonic music generation by modeling temporal dependencies using a rnn-dbn",
      "author" : [ "Kratarth Goel", "Raunaq Vohra", "JK Sahoo" ],
      "venue" : "In International Conference on Artificial Neural Networks,",
      "citeRegEx" : "Goel et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goel et al\\.",
      "year" : 2014
    }, {
      "title" : "Style imitation and chord invention in polyphonic music with exponential families",
      "author" : [ "Gaëtan Hadjeres", "Jason Sakellariou", "François Pachet" ],
      "venue" : "arXiv preprint arXiv:1609.05152,",
      "citeRegEx" : "Hadjeres et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hadjeres et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Discriminative regularization for generative models",
      "author" : [ "Alex Lamb", "Vincent Dumoulin", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1602.03220,",
      "citeRegEx" : "Lamb et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lamb et al\\.",
      "year" : 2016
    }, {
      "title" : "Bachbot: Automatic composition in style of bach chorales",
      "author" : [ "Feynman Liang" ],
      "venue" : null,
      "citeRegEx" : "Liang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liang.",
      "year" : 2016
    }, {
      "title" : "The collapsed gibbs sampler in bayesian computations with applications to a gene regulation problem",
      "author" : [ "Jun S Liu" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Liu.,? \\Q1994\\E",
      "shortCiteRegEx" : "Liu.",
      "year" : 1994
    }, {
      "title" : "URL https://research.googleblog.com/2015/06/ inceptionism-going-deeper-into-neural.html",
      "author" : [ "Alexander Mordvintsev", "Christopher Olah", "Mike Tyka" ],
      "venue" : "Inceptionism: Going deeper into neural networks,",
      "citeRegEx" : "Mordvintsev et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mordvintsev et al\\.",
      "year" : 2015
    }, {
      "title" : "Context encoders: Feature learning by inpainting",
      "author" : [ "Deepak Pathak", "Philipp Krahenbuhl", "Jeff Donahue", "Trevor Darrell", "Alexei A Efros" ],
      "venue" : "arXiv preprint arXiv:1604.07379,",
      "citeRegEx" : "Pathak et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning representations by backpropagating errors",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Cognitive modeling,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1988
    }, {
      "title" : "Information processing in dynamical systems: Foundations of harmony theory",
      "author" : [ "Paul Smolensky" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "Smolensky.,? \\Q1986\\E",
      "shortCiteRegEx" : "Smolensky.",
      "year" : 1986
    }, {
      "title" : "Nonuniversal critical dynamics in monte carlo simulations",
      "author" : [ "Robert H Swendsen", "Jian-Sheng Wang" ],
      "venue" : "Physical review letters,",
      "citeRegEx" : "Swendsen and Wang.,? \\Q1987\\E",
      "shortCiteRegEx" : "Swendsen and Wang.",
      "year" : 1987
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Lucas Theis", "Aäron van den Oord", "Matthias Bethge" ],
      "venue" : "arXiv preprint arXiv:1511.01844,",
      "citeRegEx" : "Theis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2015
    }, {
      "title" : "A deep and tractable density estimator",
      "author" : [ "Benigno Uria", "Iain Murray", "Hugo Larochelle" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Uria et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Uria et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural autoregressive distribution estimation",
      "author" : [ "Benigno Uria", "Marc-Alexandre Côté", "Karol Gregor", "Iain Murray", "Hugo Larochelle" ],
      "venue" : "arXiv preprint arXiv:1605.02226,",
      "citeRegEx" : "Uria et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Uria et al\\.",
      "year" : 2016
    }, {
      "title" : "On the equivalence between deep nade and generative stochastic networks",
      "author" : [ "Li Yao", "Sherjil Ozair", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Yao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016).",
      "startOffset" : 177,
      "endOffset" : 196
    }, {
      "referenceID" : 23,
      "context" : "We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016). Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling.",
      "startOffset" : 178,
      "endOffset" : 562
    }, {
      "referenceID" : 17,
      "context" : "This was shown recently by DeepDream (Mordvintsev et al., 2015), an optimization process that created psychedelic transformations of images.",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "A similar idea underlies a variety of style transfer algorithms (Gatys et al., 2015), which impose textures and colors from one image onto another.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "More recently, the multistyle pastiche generator (Dumoulin et al., 2016) exposes adjustable knobs that allow users of the system fine-grained control over style transfers.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "Moreover, convolutional neural networks have shown to be extremely versatile once trained, as shown by a variety of creative uses in the literature (Mordvintsev et al., 2015; Gatys et al., 2015; Almahairi et al., 2015; Lamb et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 237
    }, {
      "referenceID" : 6,
      "context" : "Moreover, convolutional neural networks have shown to be extremely versatile once trained, as shown by a variety of creative uses in the literature (Mordvintsev et al., 2015; Gatys et al., 2015; Almahairi et al., 2015; Lamb et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 237
    }, {
      "referenceID" : 1,
      "context" : "Moreover, convolutional neural networks have shown to be extremely versatile once trained, as shown by a variety of creative uses in the literature (Mordvintsev et al., 2015; Gatys et al., 2015; Almahairi et al., 2015; Lamb et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 237
    }, {
      "referenceID" : 14,
      "context" : "Moreover, convolutional neural networks have shown to be extremely versatile once trained, as shown by a variety of creative uses in the literature (Mordvintsev et al., 2015; Gatys et al., 2015; Almahairi et al., 2015; Lamb et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 237
    }, {
      "referenceID" : 4,
      "context" : "More recently, the multistyle pastiche generator (Dumoulin et al., 2016) exposes adjustable knobs that allow users of the system fine-grained control over style transfers. Neural doodle (Champandard, 2016) further closes the feedback loop between algorithm and artist. We wish to bring similar artistic tools to the domain of music. Whereas previous work in music has relied mainly on sequence models such as Hidden Markov Models (HMMs, Baum & Petrie (1966)) and Recurrent Neural Networks (RNNs, Rumelhart et al.",
      "startOffset" : 50,
      "endOffset" : 458
    }, {
      "referenceID" : 4,
      "context" : "More recently, the multistyle pastiche generator (Dumoulin et al., 2016) exposes adjustable knobs that allow users of the system fine-grained control over style transfers. Neural doodle (Champandard, 2016) further closes the feedback loop between algorithm and artist. We wish to bring similar artistic tools to the domain of music. Whereas previous work in music has relied mainly on sequence models such as Hidden Markov Models (HMMs, Baum & Petrie (1966)) and Recurrent Neural Networks (RNNs, Rumelhart et al. (1988)), we instead employ convolutional neural networks due to their emphasis on capturing local structure and their invariance properties.",
      "startOffset" : 50,
      "endOffset" : 520
    }, {
      "referenceID" : 1,
      "context" : ", 2015; Almahairi et al., 2015; Lamb et al., 2016). We introduce COCONET, a deep convolutional model trained to reconstruct partial scores. Once trained, COCONET provides direct access to all conditionals of the form p(xi | xC) where xC is a fragment of a musical score x and i / ∈ C is in its complement. Figure 1 shows an example of such conditionals used in completing a partial score. COCONET is an instance of deep orderless NADE Uria et al. (2014), and thus learns an ensemble of factorizations of the joint p(x).",
      "startOffset" : 8,
      "endOffset" : 454
    }, {
      "referenceID" : 1,
      "context" : ", 2015; Almahairi et al., 2015; Lamb et al., 2016). We introduce COCONET, a deep convolutional model trained to reconstruct partial scores. Once trained, COCONET provides direct access to all conditionals of the form p(xi | xC) where xC is a fragment of a musical score x and i / ∈ C is in its complement. Figure 1 shows an example of such conditionals used in completing a partial score. COCONET is an instance of deep orderless NADE Uria et al. (2014), and thus learns an ensemble of factorizations of the joint p(x). However, the sampling procedure for orderless NADE is not orderless. Sampling from an orderless NADE involves (randomly) choosing an ordering, and sampling ancestrally according to the chosen ordering. We have found that this produces poor results for the highly structured and complex domain of musical counterpoint. Instead, we propose to use blocked-Gibbs sampling, essentially improving sample quality through rewriting. An instance of this was previously explored by Yao et al. (2014) who employed a NADE in the transition operator for a Markov Chain, yielding a Generative Stochastic Network (GSN).",
      "startOffset" : 8,
      "endOffset" : 1010
    }, {
      "referenceID" : 1,
      "context" : ", 2015; Almahairi et al., 2015; Lamb et al., 2016). We introduce COCONET, a deep convolutional model trained to reconstruct partial scores. Once trained, COCONET provides direct access to all conditionals of the form p(xi | xC) where xC is a fragment of a musical score x and i / ∈ C is in its complement. Figure 1 shows an example of such conditionals used in completing a partial score. COCONET is an instance of deep orderless NADE Uria et al. (2014), and thus learns an ensemble of factorizations of the joint p(x). However, the sampling procedure for orderless NADE is not orderless. Sampling from an orderless NADE involves (randomly) choosing an ordering, and sampling ancestrally according to the chosen ordering. We have found that this produces poor results for the highly structured and complex domain of musical counterpoint. Instead, we propose to use blocked-Gibbs sampling, essentially improving sample quality through rewriting. An instance of this was previously explored by Yao et al. (2014) who employed a NADE in the transition operator for a Markov Chain, yielding a Generative Stochastic Network (GSN). The transition consists of a corruption process that masks out a subset x¬C of variables, followed by a process that independently resamples variables xi, i / ∈ C according to the distribution pθ(xi | xC) emitted by the NADE. Crucially, the effects of independent sampling are amortized by annealing the probability with which variables are masked out. Whereas Yao et al. (2014) treat their procedure as a cheap approximation to ancestral sampling, we find that it produces superior samples.",
      "startOffset" : 8,
      "endOffset" : 1504
    }, {
      "referenceID" : 1,
      "context" : ", 2015; Almahairi et al., 2015; Lamb et al., 2016). We introduce COCONET, a deep convolutional model trained to reconstruct partial scores. Once trained, COCONET provides direct access to all conditionals of the form p(xi | xC) where xC is a fragment of a musical score x and i / ∈ C is in its complement. Figure 1 shows an example of such conditionals used in completing a partial score. COCONET is an instance of deep orderless NADE Uria et al. (2014), and thus learns an ensemble of factorizations of the joint p(x). However, the sampling procedure for orderless NADE is not orderless. Sampling from an orderless NADE involves (randomly) choosing an ordering, and sampling ancestrally according to the chosen ordering. We have found that this produces poor results for the highly structured and complex domain of musical counterpoint. Instead, we propose to use blocked-Gibbs sampling, essentially improving sample quality through rewriting. An instance of this was previously explored by Yao et al. (2014) who employed a NADE in the transition operator for a Markov Chain, yielding a Generative Stochastic Network (GSN). The transition consists of a corruption process that masks out a subset x¬C of variables, followed by a process that independently resamples variables xi, i / ∈ C according to the distribution pθ(xi | xC) emitted by the NADE. Crucially, the effects of independent sampling are amortized by annealing the probability with which variables are masked out. Whereas Yao et al. (2014) treat their procedure as a cheap approximation to ancestral sampling, we find that it produces superior samples. We show the versatility of our method on unconditioned polyphonic music generation. Section 2 discusses previous work in the area of automatic musical composition. The details of our model and training procedure are laid out in Section 3. In Section 4 we show that our approach is equivalent to that of deep and orderless NADE Uria et al. (2014). We discuss sampling from our model in Section 5.",
      "startOffset" : 8,
      "endOffset" : 1963
    }, {
      "referenceID" : 14,
      "context" : "For instance, Liang (2016) serialize four-part Bach chorales by interleaving the parts, while Allan & Williams (2005) construct a chord vocabulary.",
      "startOffset" : 14,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "For instance, Liang (2016) serialize four-part Bach chorales by interleaving the parts, while Allan & Williams (2005) construct a chord vocabulary.",
      "startOffset" : 14,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "Boulanger-Lewandowski et al. (2012) adopt a piano roll representation, which is a binary matrix x such that xit is hot if some instrument is playing pitch i at time t.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "chine (RBM (Smolensky, 1986; Hinton et al., 2006)) or Neural Autoregressive Distribution Estimator (Uria et al.",
      "startOffset" : 11,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "chine (RBM (Smolensky, 1986; Hinton et al., 2006)) or Neural Autoregressive Distribution Estimator (Uria et al.",
      "startOffset" : 11,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : ", 2006)) or Neural Autoregressive Distribution Estimator (Uria et al., 2016) at each time step.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Hadjeres et al. (2016) sidestep the choice of causal factorization and instead employ an undirected Markov model to learn pairwise relationships between neighboring notes up to a specified number of steps away in a score.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "We approach the task of music composition with a deep convolutional neural network (Krizhevsky et al., 2012).",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "For the present work we will restrict ourselves to the study of four-part Bach chorales as used in prior work (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Goel et al., 2014; Liang, 2016; Hadjeres et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 225
    }, {
      "referenceID" : 7,
      "context" : "For the present work we will restrict ourselves to the study of four-part Bach chorales as used in prior work (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Goel et al., 2014; Liang, 2016; Hadjeres et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 225
    }, {
      "referenceID" : 15,
      "context" : "For the present work we will restrict ourselves to the study of four-part Bach chorales as used in prior work (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Goel et al., 2014; Liang, 2016; Hadjeres et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 225
    }, {
      "referenceID" : 8,
      "context" : "For the present work we will restrict ourselves to the study of four-part Bach chorales as used in prior work (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Goel et al., 2014; Liang, 2016; Hadjeres et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 225
    }, {
      "referenceID" : 9,
      "context" : "After every second convolution, we introduce a skip connection from the hidden state two levels below to reap the benefits of residual learning He et al. (2015). Finally, we obtain predictions for the pitch at each instrument/time pair:",
      "startOffset" : 144,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "Our approach is an instance of orderless and deep Neural Autoregressive Distribution Estimators (Uria et al., 2016).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "NADE can be trained for all orderings o simultaneously using the orderless NADE (Uria et al., 2014) training procedure.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "This correction, due to Uria et al. (2014), ensures consistent estimation of the negative log-likelihood of the joint pθ(x).",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "This is discussed in Pathak et al. (2016) for the case of images, where a model might learn only that pixels are similar to their neighbors.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "This is a form of blocked Gibbs sampling (Liu, 1994).",
      "startOffset" : 41,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "This is a form of blocked Gibbs sampling (Liu, 1994). Blocked sampling is crucial for mixing, as the high temporal resolution of our representation causes strong correlations between consecutive notes. For instance, without blocked sampling, it would take many steps to snap out of a long-held note. Similar observations hold for the Ising model from statistical mechanics, leading to the development of the Swendsen-Wang algorithm (Swendsen & Wang, 1987) in which large clusters of variables are resampled at once. We consider two strategies for resampling a given block of variables: ancestral sampling and independent sampling. Ancestral sampling invokes the orderless NADE sampling procedure described in Section 5.1 on the masked-out portion of the piano roll. Independent sampling simply treats the masked-out variables x¬C as independent given the context xC . Using independent blocked Gibbs to sample from a NADE model has been studied by Yao et al. (2014), who propose to use an annealed masking probability given by",
      "startOffset" : 42,
      "endOffset" : 966
    }, {
      "referenceID" : 3,
      "context" : "Results from Boulanger-Lewandowski et al. (2012) were based on an eighth-note temporal resolution (our resolution is sixteenth notes).",
      "startOffset" : 13,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "Bachbot (Liang, 2016) 0.",
      "startOffset" : 8,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "477 – NADE (Boulanger-Lewandowski et al., 2012) – 7.",
      "startOffset" : 11,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "19 RNN-RBM (Boulanger-Lewandowski et al., 2012) – 6.",
      "startOffset" : 11,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "27 RNN-NADE (Boulanger-Lewandowski et al., 2012) – 5.",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "Yao et al. (2014) treat independent blocked Gibbs as a cheap approximation to ancestral sampling.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "The literature features many variants of this dataset (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016), and we follow the unfortunate tradition of introducing our own adaptation.",
      "startOffset" : 54,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "The literature features many variants of this dataset (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016), and we follow the unfortunate tradition of introducing our own adaptation.",
      "startOffset" : 54,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "The literature features many variants of this dataset (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016), and we follow the unfortunate tradition of introducing our own adaptation.",
      "startOffset" : 54,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "We rebuilt our dataset from the Bach chorale musicXML scores readily available through (Cuthbert & Ariza, 2010), which was also the basis for the dataset used in (Liang, 2016).",
      "startOffset" : 162,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "However, evaluation of generative models is hard (Theis et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "The literature features many variants of this dataset (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016), and we follow the unfortunate tradition of introducing our own adaptation. Although this complicates comparisons against earlier work, we feel justified in doing so as our approach requires instruments to be separated, and other authors’ eighth-note temporal resolution is too coarse to accurately convey counterpoint. We rebuilt our dataset from the Bach chorale musicXML scores readily available through (Cuthbert & Ariza, 2010), which was also the basis for the dataset used in (Liang, 2016). The scores included 357 four-part Bach chorales. We excluded scores that included note durations less than sixteenth notes, resulting in 354 pieces. These pieces were split into train/valid/test in 60/20/20% ratios. We compare with Liang (2016) based on note-level likelihood and Boulanger-Lewandowski et al.",
      "startOffset" : 79,
      "endOffset" : 893
    }, {
      "referenceID" : 3,
      "context" : "The literature features many variants of this dataset (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016), and we follow the unfortunate tradition of introducing our own adaptation. Although this complicates comparisons against earlier work, we feel justified in doing so as our approach requires instruments to be separated, and other authors’ eighth-note temporal resolution is too coarse to accurately convey counterpoint. We rebuilt our dataset from the Bach chorale musicXML scores readily available through (Cuthbert & Ariza, 2010), which was also the basis for the dataset used in (Liang, 2016). The scores included 357 four-part Bach chorales. We excluded scores that included note durations less than sixteenth notes, resulting in 354 pieces. These pieces were split into train/valid/test in 60/20/20% ratios. We compare with Liang (2016) based on note-level likelihood and Boulanger-Lewandowski et al. (2012) based on frame-level likelihood.",
      "startOffset" : 79,
      "endOffset" : 964
    }, {
      "referenceID" : 3,
      "context" : "The literature features many variants of this dataset (Allan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016), and we follow the unfortunate tradition of introducing our own adaptation. Although this complicates comparisons against earlier work, we feel justified in doing so as our approach requires instruments to be separated, and other authors’ eighth-note temporal resolution is too coarse to accurately convey counterpoint. We rebuilt our dataset from the Bach chorale musicXML scores readily available through (Cuthbert & Ariza, 2010), which was also the basis for the dataset used in (Liang, 2016). The scores included 357 four-part Bach chorales. We excluded scores that included note durations less than sixteenth notes, resulting in 354 pieces. These pieces were split into train/valid/test in 60/20/20% ratios. We compare with Liang (2016) based on note-level likelihood and Boulanger-Lewandowski et al. (2012) based on frame-level likelihood. Note that train/valid/test differs among both prior work and also with our work, and that Liang (2016) uses a 80/10/10% split instead.",
      "startOffset" : 79,
      "endOffset" : 1100
    }, {
      "referenceID" : 25,
      "context" : "048 Independent Gibbs (Yao et al., 2014) 0.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "00) masking (NADE), independent Gibbs (Yao et al., 2014), and ancestral Gibbs with Contiguous(0.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "We see that although ancestral sampling on NADE performs poorly compared to Bach, both ancestral and independent Gibbs Yao et al. (2014) were considered at least as musical as fragments from Bach, with independent Gibbs Yao et al.",
      "startOffset" : 119,
      "endOffset" : 137
    }, {
      "referenceID" : 25,
      "context" : "We see that although ancestral sampling on NADE performs poorly compared to Bach, both ancestral and independent Gibbs Yao et al. (2014) were considered at least as musical as fragments from Bach, with independent Gibbs Yao et al. (2014) outperforming ancestral sampling (NADE) by a large margin.",
      "startOffset" : 119,
      "endOffset" : 238
    }, {
      "referenceID" : 24,
      "context" : "We introduced a convolutional approach to modeling musical scores based on the NADE (Uria et al., 2016) framework.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "We introduced a convolutional approach to modeling musical scores based on the NADE (Uria et al., 2016) framework. Our experiments show that the NADE ancestral sampling procedure yields poor samples for our domain, which we have argued is because some conditionals are not captured well by the model. We have shown that sample quality improves significantly when we use blocked Gibbs sampling to iteratively rewrite parts of the score. Moreover, annealed independent blocked Gibbs sampling as proposed by Yao et al. (2014) is not only faster but in fact produces better samples.",
      "startOffset" : 85,
      "endOffset" : 523
    } ],
    "year" : 2017,
    "abstractText" : "Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016). Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation. Step 0 Step 1 Step 4 Step 16 Step 64 Ground Truth Figure 1: Ancestral inpainting of a corrupted Bach chorale by COCONET. Colors are used to distinguish the four voices. Grayscale heatmaps show predictions p(xi | xC). The pitch sampled in the current step is indicated by a rectangular outline. The original Bach chorale is shown in the bottom right. Step 0 shows the corrupted Bach chorale. Step 64 shows the result. ∗Work done while at Google Brain. †Work done while at Google Brain.",
    "creator" : "LaTeX with hyperref package"
  }
}