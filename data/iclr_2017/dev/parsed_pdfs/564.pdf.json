{
  "name" : "564.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HIGHER ORDER RECURRENT NEURAL NETWORKS",
    "authors" : [ "Rohollah Soltani" ],
    "emails" : [ "rsoltani@cse.yorku.ca", "hj@cse.yorku.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the recent resurgence of neural networks in deep learning, deep neural networks have achieved successes in various real-world applications, such as speech recognition, computer vision and natural language processing. Deep neural networks (DNNs) with a deep architecture of multiple nonlinear layers are an expressive model that can learn complex features and patterns in data. Each layer of DNNs learns a representation and transfers them to the next layer and the next layer may continue to extract more complicated features, and finally the last layer generates the desirable output. From early theoretical work, it is well known that neural networks may be used as the universal approximators to map from any fixed-size input to another fixed-size output. Recently, more and more empirical results have demonstrated that the deep structure in DNNs is not just powerful in theory but also can be reliably learned in practice from a large amount of training data.\nSequential modeling is a challenging problem in machine learning, which has been extensively studied in the past. Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al. (2013). Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback. RNNs can learn to model sequential data over an extended period of time, then carry out rather complicated transformations on the sequential data. RNNs have been theoretically proved to be a turing complete machine Siegelmann & Sontag (1995). RNNs in principle can learn to map from one variable-length sequence to another. When unfolded in time, RNNs are equivalent to very deep neural networks that share model parameters and receive the input at each time step. The recursion in the hidden layer of RNNs can act as an excellent memory mechanism for the networks. In each time step, the learned recursion weights may decide what information to discard and what information to keep in order to relay onwards along time. While RNNs are theoretically powerful, the learning of RNNs needs to use the back-propagation through time (BPTT) method Werbos (1990) due to the internal recurrent cycles. Unfortunately, in practice, it turns out to be rather difficult to train RNNs to capture long-term dependency due to the fact that\nthe gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al. (2012), machine translation Cho et al. (2014), speech recognition Graves et al. (2013).\nIn this paper, we explore an alternative method to learn recurrent neural networks (RNNs) to model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding RNN states, which are all recurrently fed to the hidden layers as feedback through different weighted paths. Analogous to digital filters in signal processing, we call these new recurrent structures as higher order recurrent neural networks (HORNNs). At each time step, the proposed HORNNs directly combine multiple preceding hidden states from various history time steps, weighted by different matrices, to generate the feedback signal to each hidden layer. By aggregating more history information of the RNN states, HORNNs are provided with better short-term memory mechanism than the regular RNNs. Moreover, those direct connections to more previous RNN states allow the gradients to flow back smoothly in the BPTT learning stage. All of these ensure that HORNNs can be more effectively learned to capture long term dependency. Similar to RNNs and LSTMs, the proposed HORNNs are general enough for variety of sequential modeling tasks. In this work, we have evaluated HORNNs for the language modeling task on two popular data sets, namely the Penn Treebank (PTB) and English text8 sets. Experimental results have shown that HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Hierarchical recurrent neural network proposed in Hihi & Bengio (1996) is one of the earliest papers that attempt to improve RNNs to capture long term dependency in a better way. It proposes to add linear time delayed connections to RNNs to improve the gradient descent learning algorithm to find a better solution, eventually solving the gradient vanishing problem. However, in this early work, the idea of multi-resolution recurrent architectures has only been preliminarily examined for some simple small-scale tasks. This work is somehow relevant to our work in this paper but the higher order RNNs proposed here differs in several aspects. Firstly, we propose to use weighted connections in the structure, instead of simple multi-resolution short-cut paths. This makes our models fall into the category of higher order models. Secondly, we have proposed to use various pooling functions in generating the feedback signals, which is critical in normalizing the dynamic ranges of gradients flowing from various paths. Our experiments have shown that the success of our models is largely attributed to this technique.\nThe most successful approach to deal with vanishing gradients so far is to use long short term memory (LSTM) model Hochreiter & Schmidhuber (1997). LSTM relies on a fairly sophisticated structure made of gates to control flow of information to the hidden neurons. The drawback of the LSTM is that it is complicated and slow to learn. The complexity of this model makes the learning very time consuming, and hard to scale for larger tasks. Another approach to address this issue is to add a hidden layer to RNNs Mikolov et al. (2014). This layer is responsible for capturing longer term dependencies in input data by making its weight matrix close to identity. Recently, clockwork RNNs Koutnik et al. (2014) are proposed to address this problem as well, which splits each hidden layer into several modules running at different clocks. Each module receives signals from input and computes its output at a predefined clock rate. Gated feedback recurrent neural networks Chung et al. (2015) attempt to implement a generalized version using the gated feedback connection between layers of stacked RNNs, allowing the model to adaptively adjust the connection between consecutive hidden layers.\nBesides, short-cut skipping connections were considered earlier in Wermter (1992), and more recently have been found useful in learning very deep feed-forward neural networks as well, such as Lee et al. (2014); He et al. (2015). These skipping connections between various layers of neural networks can improve the flow of information in both forward and backward passes. Among them, highway networks Srivastava et al. (2015) introduce rather sophisticated skipping connections between layers, controlled by some gated functions."
    }, {
      "heading" : "3 HIGHER ORDER RECURRENT NEURAL NETWORKS",
      "text" : "A recurrent neural network (RNN) is a type of neural network suitable for modeling a sequence of arbitrary length. At each time step t, an RNN receives an input xt, the state of the RNN is updated recursively as follows (as shown in the left part of Figure 1):\nht = f(Winxt +Whht−1) (1)\nwhere f(·) is an nonlinear activation function, such as sigmoid or rectified linear (ReLU), and Win is the weight matrix in the input layer and Wh is the state to state recurrent weight matrix. Due to the recursion, this hidden layer may act as a short-term memory of all previous input data.\nGiven the state of the RNN, i.e., the current activation signals in the hidden layer ht, the RNN generates the output according to the following equation:\nyt = g(Woutht) (2)\nwhere g(·) denotes the softmax function and Wout is the weight matrix in the output layer. In principle, this model can be trained using the back-propagation through time (BPTT) algorithm Werbos (1990). This model has been used widely in sequence modeling tasks like language modeling Mikolov (2012)."
    }, {
      "heading" : "3.1 HIGHER ORDER RNNS (HORNNS)",
      "text" : "RNNs are very deep in time and the hidden layer at each time step represents the entire input history, which acts as a short-term memory mechanism. However, due to the gradient vanishing problem in back-propagation, it turns out to be very difficult to learn RNNs to model long-term dependency in sequential data.\nIn this paper, we extend the standard RNN structure to better model long-term dependency in sequential data. As shown in the right part of Figure 1, instead of using only the previous RNN state as the feedback signal, we propose to employ multiple memory units to generate the feedback signal at each time step by directly combining multiple preceding RNN states in the past, where these timedelayed RNN states go through separate feedback paths with different weight matrices. Analogous to the filter structures used in signal processing, we call this new recurrent structure as higher order RNNs, HORNNs in short. The order of HORNNs depends on the number of memory units used for feedback. For example, the model used in the right of Figure 1 is a 3rd-order HORNN. On the other hand, regular RNNs may be viewed as 1st-order HORNNs.\nIn HORNNs, the feedback signal is generated by combining multiple preceding RNN states. Therefore, the state of an N -th order HORNN is recursively updated as follows:\nht = f ( Winxt +\nN∑ n=1 Whnht−n\n) (3)\nwhere {Whn | n = 1, · · ·N} denotes the weight matrices used for various feedback paths. Similar to\nRNNs, HORNNs can also be unfolded in time to get rid of the recurrent cycles. As shown in Figure 2, we unfold a 3rd-order HORNN in time, which clearly shows that each HORNN state is explicitly decided by the current input xt and all previous 3 states in the past. This structure looks similar to the skipping short-cut paths in deep neural networks but each path in HORNNs maintains a learnable weight matrix. The new structure in HORNNs can significantly improve the model capacity to capture long-term dependency in sequential data. At each time step, by explicitly aggregating multiple preceding hidden activities, HORNNs may derive a good representation of the history information in sequences, leading to a significantly enhanced short-term memory mechanism.\nDuring the backprop learning procedure, these skipping paths directly connected to more previous hidden states of HORNNs may allow the gradients to flow more easily back in time, which eventually leads to a more effective learning of models to capture long term dependency in sequences. Therefore, this structure may help to largely alleviate the notorious problem of vanishing gradients in the RNN learning.\nObviously, HORNNs can be learned using the same BPTT algorithm as regular RNNs, except that the error signals at each time step need to be back-propagated to multiple feedback paths in the network. As shown in Figure 3, for a 3rd-order HORNN, at each time step t, the error signal from the hidden layer ht will have to be back-propagated into four different paths: i) the first one back to the input layer, xt; ii) three more feedback paths leading to three different histories in time scales, namely ht−1, ht−2 and ht−3.\nInterestingly enough, if we use a fully-unfolded implementation for HORNNs as in Figure 2, the overall computation complexity is comparable with regular RNNs. Given a whole sequence, we may first simultaneously compute all hidden activities (from xt to ht for all t). Secondly, we recursively update ht for all t using eq.(3). Finally, we use GPUs to compute all outputs together from the updated hidden states (from ht to yt for all t) based on eq.(2). The backward pass in learning can also be implemented in the same three-step procedure. Except the recursive updates in the second step (this issue remains the same in regular RNNs), all remaining computation steps can be formulated as large matrix multiplications. As a result, the computation of HORNNs can be implemented fairly efficiently using GPUs."
    }, {
      "heading" : "3.2 POOLING FUNCTIONS FOR HORNNS",
      "text" : "As discussed above, the shortcut paths in HORNNs may help the models to capture long-term dependency in sequential data. On the other hand, they may also complicate the learning in a different way. Due to different numbers of hidden layers along various paths, the signals flowing from different paths may vary dramatically in the dynamic range. For example, in the forward pass in Figure 2, three different feedback signals from different time scales, e.g. ht−1, ht−2 and ht−3, flow into\nthe hidden layer to compute the new hidden state ht. The dynamic range of these signals may vary dramatically from case to case. The situation may get even worse in the backward pass during the BPTT learning. For example, in a 3rd-order HORNN in Figure 2, the node ht−3 may directly receive an error signal from the node ht. In some cases, it may get so strong as to overshadow other error signals coming from closer neighbours of ht−1 and ht−2. This may impede the learning of HORNNs, yielding slow convergence or even poor performance.\nHere, we have proposed to use some pooling functions to calibrate the signals from different feedback paths before they are used to recursively generate a new hidden state, as shown in Figure 4. In the following, we will investigate three different choices for the pooling function in Figure 4, including max-based pooling, FOFE-based pooling and gated pooling."
    }, {
      "heading" : "3.2.1 MAX-BASED POOLING",
      "text" : "Max-based pooling is a simple strategy that chooses the most responsive unit (exhibiting the largest activation value) among various paths to transfer to the hidden layer to generate the new hidden state. Many biological experiments have shown that biological neuron networks tend to use a similar strategy in learning and firing.\nIn this case, instead of using eq.(3), we use the following formula to update the hidden state of HORNNs: ht = f ( Winxt +max N n=1 (Whnht−n) ) (4)\nwhere maximization is performed element-wisely to choose the maximum value in each dimension to feed to the hidden layer to generate the new hidden state. The aim here is to capture the most relevant feature and map it to a fixed predefined size.\nThe max pooling function is simple and biologically inspired. However, the max pooling strategy also has some serious disadvantages. For example, it has no forgetting mechanism and the signals may get stronger and stronger. Furthermore, it loses the order information of the preceding histories since it only choose the maximum values but it does not know where the maximum comes from."
    }, {
      "heading" : "3.2.2 FOFE-BASED POOLING",
      "text" : "The fixed-size ordinally-forgetting encoding (FOFE) method was proposed in Zhang et al. (2015) to encode any variable-length sequence of data into a fixed-size representation. In FOFE, a single forgetting factor α (0 < α < 1) is used to encode the position information in sequences based on the idea of exponential forgetting to derive invertible fixed-size representations. In this work, we borrow this simple idea of exponential forgetting to calibrate all preceding histories using a pre-selected forgetting factor as follows:\nht = f ( Winxt +\nN∑ n=1 αn ·Whnht−n\n) (5)\nwhere the forgetting factor α is manually pre-selected between 0 < α < 1. The above constant coefficients related to α play an important role in calibrating signals from different paths in both\nforward and backward passes of HORNNs since they slightly underweight the older history over the recent one in an explicit way."
    }, {
      "heading" : "3.2.3 GATED HORNNS",
      "text" : "In this section, we follow the ideas of the learnable gates in LSTMs Hochreiter & Schmidhuber (1997) and GRUs Cho et al. (2014) as well as the recent soft-attention in Bahdanau et al. (2014). Instead of using constant coefficients derived from a forgetting factor, we may let the network automatically determine the combination weights based on the current state and input. In this case, we may use sigmoid gates to compute combination weights to regulate the information flowing from various feedback paths. The sigmoid gates take the current data and previous hidden state as input to decide how to weight all of the precede hidden states. The gate function weights how the current hidden state is generated based on all the previous time-steps of the hidden layer. This allows the network to potentially remember information for a longer period of time. In a gated HORNN, the hidden state is recursively computed as follows:\nht = f ( Winxt +\nN∑ n=1 rn ( Whnht−n )) (6)\nwhere denotes element-wise multiplication of two equally-sized vectors, and the gate signal rn is calculated as\nrn = σ (W g 1nxt +W g 2nht−n) (7)\nwhere σ(·) is the sigmoid function, and W g1n and W g 2n denote two weight matrices introduced for each gate.\nNote that the computation complexity of gated HORNNs is comparable with LSTMs and GRUs, significantly exceeding the other HORNN structures because of the overhead from the gate functions in eq. (7)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section, we evaluate the proposed higher order RNNs (HORNNs) on several language modeling tasks. A statistical language model (LM) is a probability distribution over sequences of words in natural languages. Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al. (2015).\nIn our experiments, we use the mini-batch stochastic gradient decent (SGD) algorithm to train all neural networks. The number of back-propagation through time (BPTT) steps is set to 30 for all recurrent models. Each model update is conducted using a mini-batch of 20 subsequences, each of which is of 30 in length. All model parameters (weight matrices in all layers) are randomly initialized based on a Gaussian distribution with zero mean and standard deviation of 0.1. A hard clipping is set to 5.0 to avoid gradient explosion during the BPTT learning. The initial learning rate is set to 0.5 and we halve the learning rate at the end of each epoch if the cross entropy function on the validation set does not decrease. We have used the weight decay, momentum and column normalization Pachitariu & Sahani (2013) in our experiments to improve model generalization. In the FOFE-based pooling function for HORNNs, we set the forgetting factor, α, to 0.6. We have used 400 nodes in each hidden layer for the PTB data set and 500 nodes per hidden layer for the English text8 set. In our experiments, we do not use the dropout regularization Zaremba et al. (2014) in all experiments since it significantly slows down the training speed, not applicable to any larger corpora. 1\n1We will soon release the code for readers to reproduce all results reported in this paper."
    }, {
      "heading" : "4.1 LANGUAGE MODELING ON PTB",
      "text" : "The standard Penn Treebank (PTB) corpus consists of about 1M words. The vocabulary size is limited to 10k. The preprocessing method and the way to split data into training/validation/test sets are the same as Mikolov et al. (2011). PTB is a relatively small text corpus. We first investigate various model configurations for the HORNNs based on PTB and then compare the best performance with other results reported on this task."
    }, {
      "heading" : "4.1.1 EFFECT OF ORDERS IN HORNNS",
      "text" : "In the first experiment, we first investigate how the used orders in HORNNs may affect the performance of language models (as measured by perplexity). We have examined all different higher order model structures proposed in this paper, including HORNNs and various pooling functions in HORNNs. The orders of these examined models varies among 2, 3 and 4. We have listed the performance of different models on PTB in Table 1. As we may see, we are able to achieve a significant improvement in perplexity when using higher order RNNs for language models on PTB, roughly 10-20 reduction in PPL over regular RNNs. We can see that performance may improve slightly when the order is increased from 2 to 3 but no significant gain is observed when the order is further increased to 4. As a result, we choose the 3rd-order HORNN structure for the following experiments. Among all different HORNN structures, we can see that FOFE-based pooling and gated structures yield the best performance on PTB.\nIn language modeling, both input and output layers account for the major portion of model parameters. Therefore, we do not significantly increase model size when we go to higher order structures. For example, in Table 1, a regular RNN contains about 8.3 millions of weights while a 3rd-order HORNN (the same for max or FOFE pooling structures) has about 8.6 millions of weights. In comparison, an LSTM model has about 9.3 millions of weights and a 3rd-order gated HORNN has about 9.6 millions of weights.\nAs for the training speed, most HORNN models are only slightly slower than regular RNNs. For example, one epoch of training on PTB running in one NVIDIA’s TITAN X GPU takes about 80 seconds for an RNN, about 120 seconds for a 3rd-order HORNN (the same for max or FOFE pooling structures). Similarly, training of gated HORNNs is also slightly slower than LSTMs. For example, one epoch on PTB takes about 200 seconds for an LSTM, and about 225 seconds for a 3rd-order gates HORNN."
    }, {
      "heading" : "4.1.2 MODEL COMPARISON ON PENN TREEBANK",
      "text" : "At last, we report the best performance of various HORNNs on the PTB test set in Table 2. We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al. (2014), deep RNN Pascanu et al. (2014), FOFE-FNN Zhang et al. (2015) and LSTM Graves (2013). 2 From the results in Table 2, we can see that our proposed higher order RNN architectures significantly outperform all other baseline models reported on this task. Both FOFE-based pooling and gated HORNNs have achieved the state-of-the-art performance,\n2All models in Table 2 do not use the dropout regularization, which is somehow equivalent to data augmentation. In Zaremba et al. (2014); Kim et al. (2015), the proposed LSTM-LMs (word level or character level) achieve lower perplexity but they both use the dropout regularization and much bigger models and it takes days to train the models, which is not applicable to other larger tasks.\nTable 2: Perplexities on the PTB test set for various examined models.\nModels Test KN 5-gram Mikolov et al. (2011) 141 RNN Mikolov et al. (2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3rd order) 108 Max HORNN (3rd order) 109 FOFE HORNN (3rd order) 101 Gated HORNN (3rd order) 100\nTable 3: Perplexities on the text8 test set for various models.\nModels Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al. (2014) 156 SCRNN Mikolov et al. (2014) 161 E2E Mem Net Sukhbaatar et al. (2015) 147 HORNN (3rd order) 172 Max HORNN (3rd order) 163 FOFE HORNN (3rd order) 154 Gated HORNN (3rd order) 144\ni.e., 100 in perplexity on this task. To the best of our knowledge, this is the best reported performance on PTB under the same training condition."
    }, {
      "heading" : "4.2 LANGUAGE MODELING ON ENGLISH TEXT8",
      "text" : "In this experiment, we will evaluate our proposed HORNNs on a much larger text corpus, namely the English text8 data set. The text8 data set contains a preprocessed version of the first 100 million characters downloaded from the Wikipedia website. We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch.\nBecause the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al. (2014), SCRNN Mikolov et al. (2014) and end-to-end memory networks Sukhbaatar et al. (2015). Results have shown that all HORNN models work pretty well in this data set except the normal HORNN significantly underperforms the other three models. Among them, the gated HORNN model has achieved the best performance, i.e., 144 in perplexity on this task, which is slightly better than the recent result obtained by end-to-end memory networks (using a rather complicated structure). To the best of our knowledge, this is the best performance reported on this task."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "In this paper, we have proposed some new structures for recurrent neural networks, called as higher order RNNs (HORNNs). In these structures, we use more memory units to keep track of more preceding RNN states, which are all fed along various feedback paths to the hidden layer to generate the feedback signals. In this way, we may enhance the model to capture long term dependency in sequential data. Moreover, we have proposed to use several types of pooling functions to calibrate multiple feedback paths. Experiments have shown that the pooling technique plays a critical role in learning higher order RNNs effectively. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and text8 sets. Experimental results have shown that the proposed higher order RNNs yield the state-of-the-art per-\nformance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. As the future work, we are going to continue to explore HORNNs for other sequential modeling tasks, such as speech recognition, sequence-to-sequence modelling and so on."
    } ],
    "references" : [ {
      "title" : "Improving continuous space language models using auxiliary features",
      "author" : [ "Walid Aransa", "Holger Schwenk", "Loı̈c Barrault" ],
      "venue" : "In Proceedings of the 12th International Workshop on Spoken Language Translation,",
      "citeRegEx" : "Aransa et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Aransa et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "In arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Gated feedback recurrent neural networks",
      "author" : [ "J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "In Proceedings of International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "In arXiv:1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Speech recognition with deep recurrent neural",
      "author" : [ "A. Graves", "A. Mohamed", "G Hinton" ],
      "venue" : "In Proceedings of ICASSP,",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "In arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical recurrent neural networks for long-term dependencies",
      "author" : [ "Salah Hihi", "Yoshua Bengio" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Hihi and Bengio.,? \\Q1996\\E",
      "shortCiteRegEx" : "Hihi and Bengio.",
      "year" : 1996
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush" ],
      "venue" : "In arXiv:1508.06615,",
      "citeRegEx" : "Kim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "A clockwork rnn",
      "author" : [ "J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "In Proceedings of International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Koutnik et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Koutnik et al\\.",
      "year" : 2014
    }, {
      "title" : "Deeply supervised nets",
      "author" : [ "C.Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu" ],
      "venue" : "In arXiv:1409.5185,",
      "citeRegEx" : "Lee et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural networks for handwriting recognition, Book Chapter, Computational intelligence paradigms in advanced pattern classification",
      "author" : [ "M. Liwicki", "A. Graves", "H. Bunke" ],
      "venue" : null,
      "citeRegEx" : "Liwicki et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liwicki et al\\.",
      "year" : 2012
    }, {
      "title" : "Statistical Language Models based on Neural Networks",
      "author" : [ "T. Mikolov" ],
      "venue" : "PhD thesis, Brno University of Technology,",
      "citeRegEx" : "Mikolov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov.",
      "year" : 2012
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "T. Mikolov", "S. Kombrink", "L. Burget", "J.H. Černockỳ", "S. Khudanpur" ],
      "venue" : "In Proceedings ICASSP,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning longer memory in recurrent neural networks",
      "author" : [ "T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato" ],
      "venue" : "In arXiv 1412.7753,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2014
    }, {
      "title" : "Gated word-character recurrent language model",
      "author" : [ "Yasumasa Miyamoto", "Kyunghyun Cho" ],
      "venue" : "arXiv preprint arXiv:1606.01700,",
      "citeRegEx" : "Miyamoto and Cho.,? \\Q2016\\E",
      "shortCiteRegEx" : "Miyamoto and Cho.",
      "year" : 2016
    }, {
      "title" : "Regularization and nonlinearities for neural language models: when are they needed",
      "author" : [ "M. Pachitariu", "M. Sahani" ],
      "venue" : "In arXiv:1301.5650,",
      "citeRegEx" : "Pachitariu and Sahani.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pachitariu and Sahani.",
      "year" : 2013
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "In Proceedings of ICLR,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2014
    }, {
      "title" : "On the computational power of neural nets",
      "author" : [ "H.T. Siegelmann", "E.D. Sontag" ],
      "venue" : "Journal of computer and system sciences,",
      "citeRegEx" : "Siegelmann and Sontag.,? \\Q1995\\E",
      "shortCiteRegEx" : "Siegelmann and Sontag.",
      "year" : 1995
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "LSTM neural networks for language modeling",
      "author" : [ "M. Sundermeyer", "R. Schlter", "H. Ne" ],
      "venue" : "In Proceedings of Interspeech,",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2012
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "I. Sutskever", "J. Martens", "G Hinton" ],
      "venue" : "In Proceedings of International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q. Le" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "gen cnn: A convolutional architecture for word sequence prediction",
      "author" : [ "Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Wenbin Jiang", "Qun Liu" ],
      "venue" : "arXiv preprint arXiv:1503.05034,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "P.J. Werbos" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Werbos.,? \\Q1990\\E",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "A hybrid and connectionist architecture for a scanning understanding",
      "author" : [ "Stefan Wermter" ],
      "venue" : "In Proceedings of the 10th European conference on Artificial intelligence,",
      "citeRegEx" : "Wermter.,? \\Q1992\\E",
      "shortCiteRegEx" : "Wermter.",
      "year" : 1992
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "W. Zaremba", "I. Sutskever", "O.l Vinyals" ],
      "venue" : "In arXiv:1409.2329,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "The fixed-size ordinally-forgetting encoding method for neural network language models",
      "author" : [ "S. Zhang", "H. Jiang", "M. Xu", "J. Hou", "L. Dai" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al.",
      "startOffset" : 135,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al.",
      "startOffset" : 171,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al.",
      "startOffset" : 171,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al.",
      "startOffset" : 171,
      "endOffset" : 255
    }, {
      "referenceID" : 6,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al. (2013). Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback.",
      "startOffset" : 171,
      "endOffset" : 299
    }, {
      "referenceID" : 6,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al. (2013). Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback. RNNs can learn to model sequential data over an extended period of time, then carry out rather complicated transformations on the sequential data. RNNs have been theoretically proved to be a turing complete machine Siegelmann & Sontag (1995). RNNs in principle can learn to map from one variable-length sequence to another.",
      "startOffset" : 171,
      "endOffset" : 768
    }, {
      "referenceID" : 6,
      "context" : "Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al. (2013). Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback. RNNs can learn to model sequential data over an extended period of time, then carry out rather complicated transformations on the sequential data. RNNs have been theoretically proved to be a turing complete machine Siegelmann & Sontag (1995). RNNs in principle can learn to map from one variable-length sequence to another. When unfolded in time, RNNs are equivalent to very deep neural networks that share model parameters and receive the input at each time step. The recursion in the hidden layer of RNNs can act as an excellent memory mechanism for the networks. In each time step, the learned recursion weights may decide what information to discard and what information to keep in order to relay onwards along time. While RNNs are theoretically powerful, the learning of RNNs needs to use the back-propagation through time (BPTT) method Werbos (1990) due to the internal recurrent cycles.",
      "startOffset" : 171,
      "endOffset" : 1382
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time.",
      "startOffset" : 55,
      "endOffset" : 251
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al.",
      "startOffset" : 55,
      "endOffset" : 520
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively.",
      "startOffset" : 55,
      "endOffset" : 569
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al.",
      "startOffset" : 55,
      "endOffset" : 871
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al.",
      "startOffset" : 55,
      "endOffset" : 916
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al. (2012), machine translation Cho et al.",
      "startOffset" : 55,
      "endOffset" : 974
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al. (2012), machine translation Cho et al. (2014), speech recognition Graves et al.",
      "startOffset" : 55,
      "endOffset" : 1013
    }, {
      "referenceID" : 2,
      "context" : "the gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al. (2012), machine translation Cho et al. (2014), speech recognition Graves et al. (2013). In this paper, we explore an alternative method to learn recurrent neural networks (RNNs) to model long term dependency in sequential data.",
      "startOffset" : 55,
      "endOffset" : 1054
    }, {
      "referenceID" : 13,
      "context" : "Another approach to address this issue is to add a hidden layer to RNNs Mikolov et al. (2014). This layer is responsible for capturing longer term dependencies in input data by making its weight matrix close to identity.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "Recently, clockwork RNNs Koutnik et al. (2014) are proposed to address this problem as well, which splits each hidden layer into several modules running at different clocks.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "Gated feedback recurrent neural networks Chung et al. (2015) attempt to implement a generalized version using the gated feedback connection between layers of stacked RNNs, allowing the model to adaptively adjust the connection between consecutive hidden layers.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "Besides, short-cut skipping connections were considered earlier in Wermter (1992), and more recently have been found useful in learning very deep feed-forward neural networks as well, such as Lee et al.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "Besides, short-cut skipping connections were considered earlier in Wermter (1992), and more recently have been found useful in learning very deep feed-forward neural networks as well, such as Lee et al. (2014); He et al.",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "(2014); He et al. (2015). These skipping connections between various layers of neural networks can improve the flow of information in both forward and backward passes.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "(2014); He et al. (2015). These skipping connections between various layers of neural networks can improve the flow of information in both forward and backward passes. Among them, highway networks Srivastava et al. (2015) introduce rather sophisticated skipping connections between layers, controlled by some gated functions.",
      "startOffset" : 8,
      "endOffset" : 222
    }, {
      "referenceID" : 26,
      "context" : "In principle, this model can be trained using the back-propagation through time (BPTT) algorithm Werbos (1990). This model has been used widely in sequence modeling tasks like language modeling Mikolov (2012).",
      "startOffset" : 97,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "This model has been used widely in sequence modeling tasks like language modeling Mikolov (2012).",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 30,
      "context" : "The fixed-size ordinally-forgetting encoding (FOFE) method was proposed in Zhang et al. (2015) to encode any variable-length sequence of data into a fixed-size representation.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "In this section, we follow the ideas of the learnable gates in LSTMs Hochreiter & Schmidhuber (1997) and GRUs Cho et al. (2014) as well as the recent soft-attention in Bahdanau et al.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "(2014) as well as the recent soft-attention in Bahdanau et al. (2014). Instead of using constant coefficients derived from a forgetting factor, we may let the network automatically determine the combination weights based on the current state and input.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance.",
      "startOffset" : 78,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al.",
      "startOffset" : 78,
      "endOffset" : 646
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al.",
      "startOffset" : 78,
      "endOffset" : 669
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al.",
      "startOffset" : 78,
      "endOffset" : 717
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al. (2015). In our experiments, we use the mini-batch stochastic gradient decent (SGD) algorithm to train all neural networks.",
      "startOffset" : 78,
      "endOffset" : 762
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al. (2015). In our experiments, we use the mini-batch stochastic gradient decent (SGD) algorithm to train all neural networks. The number of back-propagation through time (BPTT) steps is set to 30 for all recurrent models. Each model update is conducted using a mini-batch of 20 subsequences, each of which is of 30 in length. All model parameters (weight matrices in all layers) are randomly initialized based on a Gaussian distribution with zero mean and standard deviation of 0.1. A hard clipping is set to 5.0 to avoid gradient explosion during the BPTT learning. The initial learning rate is set to 0.5 and we halve the learning rate at the end of each epoch if the cross entropy function on the validation set does not decrease. We have used the weight decay, momentum and column normalization Pachitariu & Sahani (2013) in our experiments to improve model generalization.",
      "startOffset" : 78,
      "endOffset" : 1578
    }, {
      "referenceID" : 2,
      "context" : "Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al. (2015). In our experiments, we use the mini-batch stochastic gradient decent (SGD) algorithm to train all neural networks. The number of back-propagation through time (BPTT) steps is set to 30 for all recurrent models. Each model update is conducted using a mini-batch of 20 subsequences, each of which is of 30 in length. All model parameters (weight matrices in all layers) are randomly initialized based on a Gaussian distribution with zero mean and standard deviation of 0.1. A hard clipping is set to 5.0 to avoid gradient explosion during the BPTT learning. The initial learning rate is set to 0.5 and we halve the learning rate at the end of each epoch if the cross entropy function on the validation set does not decrease. We have used the weight decay, momentum and column normalization Pachitariu & Sahani (2013) in our experiments to improve model generalization. In the FOFE-based pooling function for HORNNs, we set the forgetting factor, α, to 0.6. We have used 400 nodes in each hidden layer for the PTB data set and 500 nodes per hidden layer for the English text8 set. In our experiments, we do not use the dropout regularization Zaremba et al. (2014) in all experiments since it significantly slows down the training speed, not applicable to any larger corpora.",
      "startOffset" : 78,
      "endOffset" : 1924
    }, {
      "referenceID" : 15,
      "context" : "Note the perplexity of a regular RNN (1st order) is 123, as reported in Mikolov et al. (2011). Models 2 order 3 order 4 order HORNN 111 108 109 Max HORNN 110 109 108 FOFE HORNN 103 101 100 Gated HORNN 102 100 100",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "The preprocessing method and the way to split data into training/validation/test sets are the same as Mikolov et al. (2011). PTB is a relatively small text corpus.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al.",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al. (2014), deep RNN Pascanu et al.",
      "startOffset" : 91,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al. (2014), deep RNN Pascanu et al. (2014), FOFE-FNN Zhang et al.",
      "startOffset" : 91,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al. (2014), deep RNN Pascanu et al. (2014), FOFE-FNN Zhang et al. (2015) and LSTM Graves (2013).",
      "startOffset" : 91,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "(2015) and LSTM Graves (2013). 2 From the results in Table 2, we can see that our proposed higher order RNN architectures significantly outperform all other baseline models reported on this task.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 28,
      "context" : "In Zaremba et al. (2014); Kim et al.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "(2014); Kim et al. (2015), the proposed LSTM-LMs (word level or character level) achieve lower perplexity but they both use the dropout regularization and much bigger models and it takes days to train the models, which is not applicable to other larger tasks.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "Models Test KN 5-gram Mikolov et al. (2011) 141 RNN Mikolov et al.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Models Test KN 5-gram Mikolov et al. (2011) 141 RNN Mikolov et al. (2011) 123 CSLM5Aransa et al.",
      "startOffset" : 22,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al.",
      "startOffset" : 16,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.",
      "startOffset" : 16,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.",
      "startOffset" : 16,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al.",
      "startOffset" : 16,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al.",
      "startOffset" : 16,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al.",
      "startOffset" : 16,
      "endOffset" : 251
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3 order) 108 Max HORNN (3 order) 109 FOFE HORNN (3 order) 101 Gated HORNN (3 order) 100 Table 3: Perplexities on the text8 test set for various models.",
      "startOffset" : 16,
      "endOffset" : 284
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3 order) 108 Max HORNN (3 order) 109 FOFE HORNN (3 order) 101 Gated HORNN (3 order) 100 Table 3: Perplexities on the text8 test set for various models. Models Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al.",
      "startOffset" : 16,
      "endOffset" : 485
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3 order) 108 Max HORNN (3 order) 109 FOFE HORNN (3 order) 101 Gated HORNN (3 order) 100 Table 3: Perplexities on the text8 test set for various models. Models Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al. (2014) 156 SCRNN Mikolov et al.",
      "startOffset" : 16,
      "endOffset" : 516
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3 order) 108 Max HORNN (3 order) 109 FOFE HORNN (3 order) 101 Gated HORNN (3 order) 100 Table 3: Perplexities on the text8 test set for various models. Models Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al. (2014) 156 SCRNN Mikolov et al. (2014) 161 E2E Mem Net Sukhbaatar et al.",
      "startOffset" : 16,
      "endOffset" : 548
    }, {
      "referenceID" : 0,
      "context" : "(2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3 order) 108 Max HORNN (3 order) 109 FOFE HORNN (3 order) 101 Gated HORNN (3 order) 100 Table 3: Perplexities on the text8 test set for various models. Models Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al. (2014) 156 SCRNN Mikolov et al. (2014) 161 E2E Mem Net Sukhbaatar et al. (2015) 147 HORNN (3 order) 172 Max HORNN (3 order) 163 FOFE HORNN (3 order) 154 Gated HORNN (3 order) 144",
      "startOffset" : 16,
      "endOffset" : 589
    }, {
      "referenceID" : 15,
      "context" : "We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.",
      "startOffset" : 46,
      "endOffset" : 615
    }, {
      "referenceID" : 15,
      "context" : "We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch. Because the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al.",
      "startOffset" : 46,
      "endOffset" : 1054
    }, {
      "referenceID" : 15,
      "context" : "We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch. Because the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al. (2014), SCRNN Mikolov et al.",
      "startOffset" : 46,
      "endOffset" : 1082
    }, {
      "referenceID" : 15,
      "context" : "We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch. Because the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al. (2014), SCRNN Mikolov et al. (2014) and end-to-end memory networks Sukhbaatar et al.",
      "startOffset" : 46,
      "endOffset" : 1111
    }, {
      "referenceID" : 15,
      "context" : "We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch. Because the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al. (2014), SCRNN Mikolov et al. (2014) and end-to-end memory networks Sukhbaatar et al. (2015). Results have shown that all HORNN models work pretty well in this data set except the normal HORNN significantly underperforms the other three models.",
      "startOffset" : 46,
      "endOffset" : 1167
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.",
    "creator" : "LaTeX with hyperref package"
  }
}