{
  "name" : "375.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sanjeev Arora", "Yingyu Liang", "Tengyu Ma" ],
    "emails" : [ "arora@cs.princeton.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Word embeddings computed using diverse methods are basic building blocks for Natural Language Processing (NLP) and Information Retrieval (IR). They capture the similarities between words (e.g., (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)). Recent work has tried to compute embeddings that capture the semantics of word sequences (phrases, sentences, and paragraphs), with methods ranging from simple additional composition of the word vectors to sophisticated architectures such as convolutional neural networks and recurrent neural networks (e.g., (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)). Recently, (Wieting et al., 2016) learned general-purpose, paraphrastic sentence embeddings by starting with standard word embeddings and modifying them based on supervision from the Paraphrase pairs dataset (PPDB), and constructing sentence embeddings by training a simple word averaging model. This simple method leads to better performance on textual similarity tasks than a wide variety of methods and serves as a good initialization for textual classification tasks. However, supervision from the paraphrase dataset seems crucial, since they report that simple average of the initial word embeddings does not work very well.\nHere we give a new sentence embedding method that is embarrassingly simple: just compute the weighted average of the word vectors in the sentence and then remove the projections of the average vectors on their first principal component (“common component removal”). Here the weight of a word w is a/(a+ p(w)) with a being a parameter and p(w) the (estimated) word frequency; we call\nthis smooth inverse frequency (SIF). This method achieves significantly better performance than the unweighted average on a variety of textual similarity tasks, and on most of these tasks even beats some sophisticated supervised methods tested in (Wieting et al., 2016), including some RNN and LSTM models. The method is well-suited for domain adaptation settings, i.e., word vectors trained on various kinds of corpora are used for computing the sentence embeddings in different testbeds. It is also fairly robust to the weighting scheme: using the word frequencies estimated from different corpora does not harm the performance; a wide range of the parameters a can achieve close-to-best results, and an even wider range can achieve significant improvement over unweighted average.\nOf course, this SIF reweighting is highly reminiscent of TF-IDF reweighting from information retrieval (Sparck Jones, 1972; Robertson, 2004) if one treats a “sentence” as a “document” and make the reasonable assumption that the sentence doesn’t typically contain repeated words. Such reweightings (or related ideas like removing frequent words from the vocabulary) are a good rule of thumb but has not had theoretical justification in a word embedding setting.\nThe current paper provides a theoretical justification for the reweighting using a generative model for sentences, which is a simple modification for the Random Walk on Discourses model for generating text in (Arora et al., 2016). In that paper, it was noted that the model theoretically implies a sentence embedding, namely, simple average of embeddings of all the words in it.\nWe modify this theoretical model, motivated by the empirical observation that most word embedding methods, since they seek to capture word cooccurence probabilities using vector inner product, end up giving large vectors to frequent words, as well as giving unnecessarily large inner products to word pairs, simply to fit the empirical observation that words sometimes occur out of context in documents. These anomalies cause the average of word vectors to have huge components along semantically meaningless directions. Our modification to the generative model of (Arora et al., 2016) allows “smoothing” terms, and then a max likelihood calculation leads to our SIF reweighting.\nInterestingly, this theoretically derived SIF does better (by a few percent points) than traditional TFIDF in our setting. The method also improves the sentence embeddings of Wieting et al., as seen in Table 1. Finally, we discovered that —contrary to widespread belief—Word2Vec(CBOW) also does not use simple average of word vectors in the model, as misleadingly suggested by the usual expression Pr[w|w1, w2, . . . , w5] ∝ exp(vw · ( 15 ∑ i vwi)). A dig into the implementation shows it implicitly uses a weighted average of word vectors —again, different from TF-IDF— and this weighting turns out to be quite similar in effect to ours. (See Section 3.1.)"
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Word embeddings. Word embedding methods represent words as continuous vectors in a low dimensional space which capture lexical and semantic properties of words. They can be obtained from the internal representations from neural network models of text (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a) or by low rank approximation of co-occurrence statistics (Deerwester et al., 1990; Pennington et al., 2014). The two approaches are known to be closely related (Levy & Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016).\nOur work is most directly related work to (Arora et al., 2016), which proposed a random walk model for generating words in the documents. Our sentence vector can be seen as approximate inference of the latent variables in their generative model.\nPhrase/Sentence/Paragraph embeddings. Previous works have computed phrase or sentence embeddings by composing word embeddings using operations on vectors and matrices e.g., (Mitchell & Lapata, 2008; 2010; Blacoe & Lapata, 2012). They found that coordinate-wise multiplication of the vectors performed very well among the binary operations studied. Unweighted averaging is also found to do well in representing short phrases (Mikolov et al., 2013a). Another approach is recursive neural networks (RNNs) defined on the parse tree, trained with supervision (Socher et al., 2011) or without (Socher et al., 2014). Simple RNNs can be viewed as a special case where the parse tree is replaced by a simple linear chain. For example, the skip-gram model (Mikolov et al., 2013b) is extended to incorporate a latent vector for the sequence, or to treat the sequences rather than the word as basic units. In (Le & Mikolov, 2014) each paragraph was assumed to have a latent paragraph vector, which influences the distribution of the words in the paragraph. Skip-thought\nof (Kiros et al., 2015) tries to reconstruct the surrounding sentences from surrounded one and treats the hidden parameters as their vector representations. RNNs using long short-term memory (LSTM) capture long-distance dependency and have also been used for modeling sentences (Tai et al., 2015). Other neural network structures include convolution neural networks, such as (Blunsom et al., 2014) that uses a dynamic pooling to handle input sentences of varying length and do well in sentiment prediction and classification tasks.\nThe directed inspiration for our work is (Wieting et al., 2016) which learned paraphrastic sentence embeddings by using simple word averaging and also updating standard word embeddings based on supervision from paraphrase pairs; the supervision being used for both initialization and training."
    }, {
      "heading" : "3 A SIMPLE METHOD FOR SENTENCE EMBEDDING",
      "text" : "We briefly recall the latent variable generative model for text in (Arora et al., 2016). The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The process is driven by the random walk of a discourse vector ct ∈ <d. Each word w in the vocabulary has a vector in <d as well; these are latent variables of the model. The discourse vector represents “what is being talked about.” The inner product between the discourse vector ct and the (time-invariant) word vector vw for word w captures the correlations between the discourse and the word. The probability of observing a word w at time t is given by a log-linear word production model from Mnih and Hinton:\nPr[w emitted at time t | ct] ∝ exp (〈ct, vw〉) . (1)\nThe discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small random displacement vector), so that nearby words are generated under similar discourses. It was shown in (Arora et al., 2016) that under some reasonable assumptions this model generates behavior –in terms of word-word cooccurence probabilities—that fits empirical works like word2vec and Glove. The random walk model can be relaxed to allow occasional big jumps in ct, since a simple calculation shows that they have negligible effect on cooccurrence probabilities of words. The word vectors computed using this model are reported to be similar to those from Glove and word2vec(CBOW).\nOur improved Random Walk model. Clearly, it is tempting to define the sentence embedding as follows: given a sentence s, do a MAP estimate of the discourse vectors that govern this sentence. We note that we assume the discourse vector ct doesn’t change much while the words in the sentence were emitted, and thus we can replace for simplicity all the ct’s in the sentence s by a single discourse vector cs. In the paper (Arora et al., 2016), it was shown that the MAP estimate of cs is —up to multiplication by scalar—the average of the embeddings of the words in the sentence.\nIn this paper, towards more realistic modeling, we change the model (1) as follows. This model has two types of “smoothing term”, which are meant to account for the fact that some words occur out of context, and that some frequent words (presumably “the”, “and ” etc.) appear often regardless of the discourse. We first introduce an additive term αp(w) in the log-linear model, where p(w) is the unigram probability (in the entire corpus) of word and α is a scalar. This allows words to occur even if their vectors have very low inner products with cs. Secondly, we introduce a common discourse vector c0 ∈ <d which serves as a correction term for the most frequent discourse that is often related to syntax. (Other possible correction is left to future work.) It boosts the co-occurrence probability of words that have a high component along c0.\nConcretely, given the discourse vector cs, the probability of a word w is emitted in the sentence s is modeled by,\nPr[w emitted in sentence s | cs] = αp(w) + (1− α) exp (〈c̃s, vw〉)\nZc̃s , (2)\nwhere c̃s = βc0 + (1− β)cs, c0 ⊥ cs where α and β are scalar hyperparameters, and Zc̃s = ∑ w∈V exp (〈c̃s, vw〉) is the normalizing constant (the partition function). We see that the model allows a word w unrelated to the discourse cs to be omitted for two reasons: a) by chance from the term αp(w); b) if w is correlated with the common discourse vector c0.\nAlgorithm 1 Sentence Embedding Input: Word embeddings {vw : w ∈ V}, a set of sentences S, parameter a and estimated probabil-\nities {p(w) : w ∈ V} of the words. Output: Sentence embeddings {vs : s ∈ S}\n1: for all sentence s in S do 2: vs ← 1|s| ∑ w∈s a a+p(w)vw 3: end for 4: Compute the first principal component u of {vs : s ∈ S} 5: for all sentence s in S do 6: vs ← vs − uu>vs 7: end for\nComputing the sentence embedding. The word embeddings yielded by our model are actually the same 1 The sentence embedding will be defined as the max likelihood estimate for the vector cs that generated it. ( In this case MLE is the same as MAP since the prior is uniform.) We borrow the key modeling assumption of (Arora et al., 2016), namely that the word vw’s are roughly uniformly dispersed, which implies that the partition function Zc is roughly the same in all directions. So assume that Zc̃s is roughly the same, say Z for all c̃s. By the model (2) the likelihood for the sentence is\np[s | cs] = ∏ w∈s p(w | cs) = ∏ w∈s [ αp(w) + (1− α)exp (〈vw, c̃s〉) Z ] .\nLet\nfw(c̃s) = log\n[ αp(w) + (1− α)exp (〈vw, c̃s〉)\nZ ] denote the log likelihood of sentence s. Then, by simple calculus we have,\n∇fw(c̃s) = 1 αp(w) + (1− α) exp (〈vw, c̃s〉) /Z 1− α Z exp (〈vw, c̃s〉) vw.\nThen by Taylor expansion, we have,\nfw(c̃s) ≈ fw(0) +∇fw(0)>c̃s\n= constant + (1− α)/(αZ)\np(w) + (1− α)/(αZ) 〈vw, c̃s〉 .\nTherefore, the maximum likelihood estimator for c̃s on the unit sphere (ignoring normalization) is approximately,2\narg max ∑ w∈s fw(c̃s) ∝ ∑ w∈s\na\np(w) + a vw, where a = 1− α αZ . (3)\nThat is, the MLE is approximately a weighted average of the vectors of the words in the sentence. Note that for more frequent words w, the weight a/(p(w) + a) is smaller, so this naturally leads to a down weighting of the frequent words.\nTo estimate cs, we estimate the direction c0 by computing the first principal component of c̃s’s for a set of sentences. In other words, the final sentence embedding is obtained by subtracting the projection of c̃s’s to their first principal component. This is summarized in Algorithm 1."
    }, {
      "heading" : "3.1 CONNECTION TO SUBSAMPLING PROBABILITIES IN WORD2VEC",
      "text" : "Word2vec (Mikolov et al., 2013b) uses a sub-sampling technique which downsamples word w with probability proportional to 1/ √ p(w) where p(w) is the marginal probability of the word w. This\n1We empirically discovered the significant common component c0 in word vectors built by existing methods, which inspired us to propose our theoretical model of this paper.\n2Note that maxc:‖c‖=1 C + 〈c, g〉 = g/‖g‖ for any constant C.\nheuristic not only speeds up the training but also learns more regular word representations. Here we explain that this corresponds to an implicit reweighting of the word vectors in the model and therefore the statistical benefit should be of no surprise.\nRecall the vanilla CBOW model of word2vec:\nPr[wt | wt−1, . . . , wt−5] ∝ exp (〈v̄t, vw〉) , where v̄t = 1\n5 5∑ i=1 vwt−i . (4)\nIt can be shown that the loss (MLE) for the single word vector vw (from this occurrence) can be abstractly written in the form,\ng(vw) = γ(〈v̄t, vw〉) + negative sampling terms ,\nwhere γ(x) = log(1/(1 + e−x)) is the logistic function. Therefore, the gradient of g(vw) is\n∇g(vw) = γ′(〈v̄t, vw〉)v̄t = α(vwt−5 + vwt−4 + vwt−3 + vwt−2 + vwt−1) , (5)\nwhere α is a scalar. That is, without the sub-sampling trick, the update direction is the average of the word vectors in the window.\nThe sub-sampling trick in (Mikolov et al., 2013b) randomly selects the summands in equation (5) to “estimate” the gradient. Specifically, the sampled update direction is\n∇̃g(vw) = α(J5vwt−5 + J4vwt−4 + J3vwt−3 + J2vwt−2 + J1vwt−1) (6)\nwhere Jk’s are Bernoulli random variables with Pr [Jk = 1] = q(wt−k) , min { 1, √ 10−5\np(wt−k)\n} .\nHowever, we note that ∇̃g(vw) is (very) biased estimator! We have that the expectation of ∇̃g(vw) is a weighted sum of the word vectors,\nE [ ∇̃g(vw) ] = α(q(wt−5)vwt−5 + q(wt−4)vwt−4 + q(wt−3)vwt−3 + q(wt−2)vwt−2 + q(wt−1)vwt−1) .\nIn fact, the expectation E[∇̃g(vw)] corresponds to the gradient of a modified word2vec model with the average v̄t (in equation (4)) being replaced by the weighted average ∑5 k=1 q(wt−k)vwt−k . Such a weighted model can also share the same form of what we derive from our random walk model as in equation (3). Moreover, the weighting q(wi) closely tracks our weighting scheme a/(a + p(w)) when using parameter a = 10−4; see Figure 1 for an illustration. Therefore, the expected gradient here is approximately the estimated discourse vector in our model! Thus, word2vec with sub-sampling gradient heuristic corresponds to a stochastic gradient update method for using our weighting scheme."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 TEXTUAL SIMILARITY TASKS",
      "text" : "Datasets. We test our methods on the 22 textual similarity datasets including all the datasets from SemEval semantic textual similarity (STS) tasks (2012-2015) (Agirre et al., 2012; 2013; 2014; Agirrea et al., 2015), and the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014). The objective of these tasks is to predict the similarity between two given sentences. The evaluation criterion is the Pearson’s coefficient between the predicted scores and the ground-truth scores.\nExperimental settings. We will compare our method with the following:\n1. Unsupervised: ST, avg-GloVe, tfidf-GloVe. ST denotes the skip-thought vectors (Kiros et al., 2015), avg-GloVe denotes the unweighted average of the GloVe vectors (Pennington et al., 2014),3 and tfidf-GloVe denotes the weighted average of GloVe vectors using TF-IDF weights.\n2. Semi-supervised: avg-PSL. This method uses the unweighted average of the PARAGRAMSL999 (PSL) word vectors from (Wieting et al., 2015). The word vectors are trained using labeled data, but the sentence embedding are computed by unweighted average without training.\n3. Supervised: PP, PP-proj., DAN, RNN, iRNN, LSTM (o.g.), LSTM (no). All these methods are initialized with PSL word vectors and then trained on the PPDB dataset. PP and PPproj. are proposed in (Wieting et al., 2016). The first is an average of the word vectors, and the second additionally adds a linear projection. The word vectors are updated during the training. DAN denotes the deep averaging network of (Iyyer et al., 2015). RNN denotes the classical recurrent neural network, and iRNN denotes a variant with the activation being the identity, and the weight matrices initialized to identity. The LSTM is the version from (Gers et al., 2002), either with output gates (denoted as LSTM(o.g.)) or without (denoted as LSTM (no)).\nOur method can be applied to any types of word embeddings. So we denote the sentence embeddings obtained by applying our method to word embeddings method “XXX” as “XXX+WR”.4 To get a completely unsupervised method, we apply it to the GloVe vectors, denoted as GloVe+WR. The weighting parameter a is fixed to 10−3, and the word frequencies p(w) are estimated from the\n3We used the vectors that are publicly available at http://nlp.stanford.edu/projects/glove/. They are 300- dimensional vectors that were trained on the 840 billion token Common Crawl corpus.\n4“W” stands for the smooth inverse frequency weighting scheme, and “R” stands for removing the common components.\ncommoncrawl dataset.5 This is denoted by GloVe+WR in Table 1. We also apply our method on the PSL vectors, denoted as PSL+WR, which is a semi-supervised method.\nResults. The results are reported in Table 1. Each year there are 4 to 6 STS tasks. For clarity, we only report the average result for the STS tasks each year; the detailed results are in the appendix.\nThe unsupervised method GloVe+WR improves upon avg-GloVe significantly by 10% to 30%, and beats the baselines by large margins. It achieves better performance than LSTM and RNN and is comparable to DAN, even though the later three use supervision. This demonstrates the power of this simple method: it can be even stronger than highly-tuned supervisedly trained sophisticated models. Using TF-IDF weighting scheme also improves over the unweighted average, but not as much as our method.\nThe semi-supervised method PSL+WR achieves the best results for four out of the six tasks and is comparable to the best in the rest of two tasks. Overall, it outperforms the avg-PSL baseline and all the supervised models initialized with the same PSL vectors. This demonstrates the advantage of our method over the training for those models.\nWe also note that the top singular vectors c0 of the datasets seem to roughly correspond to the syntactic information or common words. For example, closest words (by cosine similarity) to c0 in the SICK dataset are “just”, “when”, “even”, “one”, “up”, “little”, “way”, “there”, “while”, and “but.”\nFinally, in the appendix, we showed that our two ideas all contribute to the improvement: for GloVe vectors, using smooth inverse frequency weighting alone improves over unweighted average by about 5%, using common component removal alone improves by 10%, and using both improves by 13%."
    }, {
      "heading" : "4.1.1 EFFECT OF WEIGHTING PARAMETER ON PERFORMANCE",
      "text" : "We study the sensitivity of our method to the weighting parameter a, the method for computing word vectors, and the estimated word probabilities p(w). First, we test the performance of three\n5It is possible to tune the parameter a to get better results. The effect of a and the corpus for estimating word frequencies are studied in Section 4.1.1.\ntypes of word vectors (PSL, GloVe, and SN) on the STS 2012 tasks. SN vectors are trained on the enwiki dataset (Wikimedia, 2012) using the method in (Arora et al., 2016), while PSL and GloVe vectors are those used in Table 1. We enumerate a ∈ {10−i, 3 × 10−i : 1 ≤ i ≤ 5} and use the p(w) estimated on the enwiki dataset. Figure 2a shows that for all three kinds of word vectors, a wide range of a leads to significantly improved performance over the unweighted average. Best performance occurs from a = 10−3 to a = 10−4.\nNext, we fix a = 10−3 and use four very different datasets to estimate p(w): enwiki (wikipedia, 3 billion tokens), poliblogs (Yano et al., 2009) (political blogs, 5 million), commoncrawl (Buck et al., 2014) (Internet crawl, 800 billion), text8 (Mahoney, 2008) (wiki subset, 1 million). Figure 2b shows performance is almost the same for all four settings.\nThe fact that our method can be applied on different types of word vectors trained on different corpora also suggests it should be useful across different domains. This is especially important for unsupervised methods, since the unlabeled data available may be collected in a different domain from the target application."
    }, {
      "heading" : "4.2 SUPERVISED TASKS",
      "text" : "The sentence embeddings obtained by our method can be used as features for downstream supervised tasks. We consider three tasks: the SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013). To highlight the representation power of the sentence embeddings learned unsupervisedly, we fix the embeddings and only learn the classifier. Setup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison, i.e., the classifier a linear projection followed by the classifier in (Kiros et al., 2015). The linear projection maps the sentence embeddings into 2400 dimension (the same as the skip-thought vectors), and is learned during the training. We compare our method to PP, DAN, RNN, and LSTM, which are the methods used in Section 4.1. We also compare to the skip-thought vectors (with improved training in (Lei Ba et al., 2016)).\nResults. Our method gets better or comparable performance compared to the competitors. It gets the best results for two of the tasks. This demonstrates the power of our simple method. We emphasize that our embeddings are unsupervisedly learned, while DAN, RNN, LSTM are trained with supervision. Furthermore, skip-thought vectors are much higher dimensional than ours (though projected into higher dimension, the original 300 dimensional embeddings contain all the information).\nThe advantage is not as significant as in the textual similarity tasks. This is possibly because similarity tasks rely directly upon cosine similarity, which favors our method’s approach of removing the common components (which can be viewed as a form of denoising), while in supervised tasks, with the cost of some label information, the classifier can pick out the useful components and ignore the common ones.\nFinally, we speculate that our method doesn’t outperform RNN’s and LSTM’s for sentiment tasks because (a) the word vectors —and more generally the distributional hypothesis of meaning —has known limitations for capturing sentiment due to the “antonym problem”, (b) also in our weighted average scheme, words like “not” that may be important for sentiment analysis are downweighted a lot. To address (a), there is existing work on learning better word embeddings for sentiment analysis (e.g., (Maas et al., 2011)). To address (b), it is possible to design weighting scheme (or learn weights) for this specific task."
    }, {
      "heading" : "4.3 THE EFFECT OF THE ORDER OF WORDS IN SENTENCES",
      "text" : "A interesting feature of our method is that it ignores the word order. This is in contrast to that RNN’s and LSTM’s can potentially take advantage of the word order. The fact that our method achieves better or comparable performance on these benchmarks raise the following question: is word order not important in these benchmarks? We conducted an experiment suggesting that word order does play some role.\nWe trained and tested RNN/LSTM on the supervised tasks where the words in each sentence are randomly shuffled, and the results are reported in Table 3.6 It can be observed that the performance drops noticeably. Thus our method —which ignores word order—must be much better at exploiting the semantics than RNN’s and LSTM’s. An interesting future direction is to explore if some ensemble idea can combine the advantages of both approaches."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "This work provided a simple approach to sentence embedding, based on the discourse vectors in the random walk model for generating text (Arora et al., 2016). It is simple and unsupervised, but achieves significantly better performance than baselines on various textual similarity tasks, and can even beat sophisticated supervised methods such as some RNN and LSTM models. The sentence embeddings obtained can be used as features in downstream supervised tasks, which also leads to better or comparable results compared to the sophisticated methods."
    }, {
      "heading" : "6 ACKNOWLEDGEMENTS",
      "text" : "We thank the reviewers for insightful comments. We also thank the authors of (Wieting et al., 2016; Bowman et al., 2015) for sharing their code or the preprocessed datasets.\nThis work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONRN00014- 16-1-2329. Tengyu Ma was supported in addition by Simons Award in Theoretical Computer Science and IBM PhD Fellowship."
    }, {
      "heading" : "A DETAILS OF EXPERIMENTAL SETTING",
      "text" : "A.1 UNSUPERVISED TASK: TEXTUAL SIMILARITY\nThe competitors. We give a brief overview of the competitors. RNN is the classical recurrent neural network:\nht = f(WxW xt w +Whht−1 + b)\nwhere f is the activation, Wx,Wh and b are parameters, and xt is the t-th token in the sentence. The sentence embedding of RNN is just the hidden vector of the last token. iRNN is a special RNN with the activation being the identity, the weight matrices initialized to identity, and b initialized to zero. LSTM (Hochreiter & Schmidhuber, 1997) is a recurrent neural network architecture designed to capture long-distance dependencies. Here, the version from (Gers et al., 2002) is used.\nThe supervised methods are initialized with PARAGRAM-SL999 (PSL) vectors, and trained using the approach of (Wieting et al., 2016) on the XL section of the PPDB data (Pavlick et al., 2015) which contains about 3 million unique phrase pairs. After training, the final models can be used to generate sentence embeddings on the test data. For hyperparameter tuning they used 100k examples sampled from PPDB XXL and trained for 5 epochs. Then after finding the hyperparameters that maximize Spearmans coefficients on the Pavlick et al. PPDB task, they are trained on the entire XL section of PPDB for 10 epochs. See (Wieting et al., 2016) and related papers for more details about these methods.\nThe tfidf-GloVe method is a weighted average of the GloVe vectors, where the weights are defined by the TF-IDF scheme. More precisely, the embedding of a sentence s is\nvs = 1 |s| ∑ w∈s IDFwvw (7)\nwhere IDFw is the inverse document frequency of w, and |s| denotes the number of words in the sentence. Here, the TF part of the TF-IDF scheme is taken into account by the sum over w ∈ s. Furthermore, when computing IDFw, each sentence is viewed as a “document”:\nIDFw := log 1 +N\n1 +Nw\nwhere N is the total number of sentences and Nw is the number of sentences containing w, and 1 is added to avoid division by 0. In the experiments, we use all the textual similarity datasets to compute IDFw.\nDetailed experimental results. In the main body we present the average results for STS tasks by year. Each year there are actually 4 to 6 STS tasks, as shown in Table 4. Note that tasks with the same name in different years are actually different tasks. Here we provide the results for each tasks in Table 5. PSL+WR achieves the best results on 12 out of 22 tasks, PP and PP-proj. achieves on 3, tfidf-GloVe achieves on 2, and DAN, iRNN, and GloVe+WR achieves on 1. In general, our method improves the performance significantly compared to the unweighted average, though on rare cases such as MSRpar it can decrease the performance.\nEffects of smooth inverse frequency and common component removal. There are two key ideas in our methods: smooth inverse frequency weighting (W) and common component removal (R). It is instructive to see their effects separately. Let GloVe+W denote the embeddings using only smooth inverse frequency, and GloVe+R denote that using only common component removal. Similarly define PSL+W and PSL+R. The results for these methods are shown in Table 6. When using GloVe vectors, W alone improves the performance of the unweighted average baseline by about 5%, R alone improves by 10%, W and R together improves by 13%. When using PSL vectors, W gets 10%, R gets 10%, W and R together gets 13%. In summary, both techniques are important for obtaining significant advantage over the unweighted average.\nA.2 SUPERVISED TASKS\nSetup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison: the sentence embeddings are fixed and fed into some classifier which are trained. For the SICK similarity task, given a pair of sentences with embeddings vL and vR, first do a linear projection:\nhL = WpvL, hR = WpvR\nwhere Wp is of size 300 × dp and is learned during training. dp = 2400 matches the dimension of the skip-thought vectors. Then hL and hR are used in the objective function from (Tai et al., 2015). Almost the same approach is used for the entailment task.For the sentiment task, the classifier has a fully-connected layer with a sigmoid activation followed by a softmax layer.\nRecall that our method has two steps: smooth inverse frequency weighting and common component removal. For the experiments here, we do not perform the common component removal, since it can be absorbed into the projection step. For the weighted average, the hyperparameter a is enumerated in {10−i, 3 × 10−i : 2 ≤ i ≤ 3}. The other hyperparameters are enumerated as in (Wieting et al., 2016), and the same validation approach is used to select the final values.\nA.3 ADDITIONAL SUPERVISED TASKS\nHere we report the experimental results on two more datasets, comparing to known results on them.\nSNLI. The first experiment is for the 3-class classification task on the SNLI dataset (Bowman et al., 2015). To compare to the results in (Bowman et al., 2015), we used their experimental setup. In particular, we applied our method to 300 dimensional GloVe vectors and used an additional tanh neural network layer to map these 300d embeddings into 300 dimensional space, then used the code provided by the authors of (Bowman et al., 2015), trained the classifier on our 100 dimensional sentence embedding for 120 passes over the data, using their default hyperparameters. The results are shown in Table 7. Our method indeed gets slightly better performance.\nOur test accuracy is worse than those using more sophisticated models (e.g., using attention mechanism), which are typically 83% - 88%; see the website of the SNLI project7 for a summary. An interesting direction is to study whether our idea can be combined with these sophisticated models to get improved performance.\nIMDB. The second experiment is the sentiment analysis task on the IMDB dataset, studied in (Wang & Manning, 2012). Since the intended application is semi-supervised or transfer learning, we also compared performance with fewer labeled examples.\n7http://nlp.stanford.edu/projects/snli/\nOur method gets worse performance on the full dataset, but its decrease in performance is better with less labeled examples, showing the benefit of using word embeddings. Note that our sentence embeddings are unsupervised, while that in the NB-SVM method takes advantage of the labels. Another comment is that sentiment analysis appears to be the best case for Bag-Of-Word methods, whereas it may be the worst case for word embedding methods (See Table 2) due to the well-known antonymy problem —distributional hypothesis fails for distinguishing “good” from “bad.”"
    } ],
    "references" : [ {
      "title" : "Semeval-2012 task 6: A pilot on semantic textual similarity",
      "author" : [ "Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre" ],
      "venue" : "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,",
      "citeRegEx" : "Agirre et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2012
    }, {
      "title" : "Sem 2013 shared task: Semantic textual similarity. in second joint conference on lexical and computational semanOn the random order datasets, the hyperparameters are enumerated",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo" ],
      "venue" : null,
      "citeRegEx" : "Agirre et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2013
    }, {
      "title" : "Semeval-2014 task 10: Multilingual semantic textual similarity",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe" ],
      "venue" : "In Proceedings of the 8th international workshop on semantic evaluation (SemEval",
      "citeRegEx" : "Agirre et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability",
      "author" : [ "Eneko Agirrea", "Carmen Baneab", "Claire Cardiec", "Daniel Cerd", "Mona Diabe", "Aitor Gonzalez-Agirrea", "Weiwei Guof", "Inigo Lopez-Gazpioa", "Montse Maritxalara", "Rada Mihalceab" ],
      "venue" : "In Proceedings of the 9th international workshop on semantic evaluation (SemEval",
      "citeRegEx" : "Agirrea et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agirrea et al\\.",
      "year" : 2015
    }, {
      "title" : "A latent variable model approach to PMI-based word embeddings",
      "author" : [ "Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski" ],
      "venue" : "Transaction of Association for Computational Linguistics,",
      "citeRegEx" : "Arora et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "A comparison of vector-based representations for semantic composition",
      "author" : [ "William Blacoe", "Mirella Lapata" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "Blacoe and Lapata.,? \\Q2012\\E",
      "shortCiteRegEx" : "Blacoe and Lapata.",
      "year" : 2012
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Blunsom et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Blunsom et al\\.",
      "year" : 2014
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,",
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "N-gram counts and language models from the common crawl",
      "author" : [ "Christian Buck", "Kenneth Heafield", "Bas van Ooyen" ],
      "venue" : "In Proceedings of the Language Resources and Evaluation Conference,",
      "citeRegEx" : "Buck et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Buck et al\\.",
      "year" : 2014
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Collobert and Weston.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott C. Deerwester", "Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman" ],
      "venue" : "Journal of the American Society for Information Science,",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Learning precise timing with lstm recurrent networks",
      "author" : [ "Felix A Gers", "Nicol N Schraudolph", "Jürgen Schmidhuber" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "Gers et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gers et al\\.",
      "year" : 2002
    }, {
      "title" : "Word embeddings as metric recovery in semantic spaces. Transactions of the Association for Computational Linguistics, 2016",
      "author" : [ "Tatsunori B. Hashimoto", "David Alvarez-Melis", "Tommi S. Jaakkola" ],
      "venue" : null,
      "citeRegEx" : "Hashimoto et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III" ],
      "venue" : "In Proceedings of the Association for Computational Linguistics,",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Kiros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc Le", "Tomas Mikolov" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Le and Mikolov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Levy and Goldberg.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
      "citeRegEx" : "Maas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Wikipedia text preprocess script",
      "author" : [ "Matt Mahoney" ],
      "venue" : "http://mattmahoney.net/dc/textdata.html,",
      "citeRegEx" : "Mahoney.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mahoney.",
      "year" : 2008
    }, {
      "title" : "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment",
      "author" : [ "Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig" ],
      "venue" : "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "Jeff Mitchell", "Mirella Lapata" ],
      "venue" : "In Association for Computational Linguistics,",
      "citeRegEx" : "Mitchell and Lapata.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2008
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "Mitchell and Lapata.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2010
    }, {
      "title" : "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification",
      "author" : [ "Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris CallisonBurch" ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Pavlick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "Proceedings of the Empiricial Methods in Natural Language Processing,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding inverse document frequency: on theoretical arguments for idf",
      "author" : [ "Stephen Robertson" ],
      "venue" : "Journal of documentation,",
      "citeRegEx" : "Robertson.,? \\Q2004\\E",
      "shortCiteRegEx" : "Robertson.",
      "year" : 2004
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Grounded compositional semantics for finding and describing images with sentences",
      "author" : [ "Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Socher et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "Karen Sparck Jones" ],
      "venue" : "Journal of documentation,",
      "citeRegEx" : "Jones.,? \\Q1972\\E",
      "shortCiteRegEx" : "Jones.",
      "year" : 1972
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1503.00075,",
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Baselines and bigrams: Simple, good sentiment and topic classification",
      "author" : [ "Sida Wang", "Christopher D Manning" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume",
      "citeRegEx" : "Wang and Manning.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang and Manning.",
      "year" : 2012
    }, {
      "title" : "Cse: Conceptual sentence embeddings based on attention model",
      "author" : [ "Yashen Wang", "Heyan Huang", "Chong Feng", "Qiang Zhou", "Jiahui Gu", "Xiong Gao" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "From paraphrase database to compositional paraphrase model and back",
      "author" : [ "John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Dan Roth" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Wieting et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards universal paraphrastic sentence embeddings",
      "author" : [ "John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Wieting et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)",
      "author" : [ "Wei Xu", "Chris Callison-Burch", "William B Dolan" ],
      "venue" : "Proceedings of SemEval,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Predicting response to political blog posts with topic models",
      "author" : [ "Tae Yano", "William W Cohen", "Noah A Smith" ],
      "venue" : "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Yano et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yano et al\\.",
      "year" : 2009
    }, {
      "title" : "summary, both techniques are important for obtaining significant advantage over the unweighted average. A.2 SUPERVISED TASKS Setup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison: the sentence embeddings are fixed and fed into some classifier",
      "author" : [ "W R" ],
      "venue" : null,
      "citeRegEx" : "R,? \\Q2016\\E",
      "shortCiteRegEx" : "R",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : ", (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)).",
      "startOffset" : 2,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : ", (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)).",
      "startOffset" : 2,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : ", (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : ", (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 142
    }, {
      "referenceID" : 29,
      "context" : ", (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : ", (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 142
    }, {
      "referenceID" : 33,
      "context" : ", (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 142
    }, {
      "referenceID" : 35,
      "context" : ", (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 142
    }, {
      "referenceID" : 37,
      "context" : "Recently, (Wieting et al., 2016) learned general-purpose, paraphrastic sentence embeddings by starting with standard word embeddings and modifying them based on supervision from the Paraphrase pairs dataset (PPDB), and constructing sentence embeddings by training a simple word averaging model.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 37,
      "context" : "This method achieves significantly better performance than the unweighted average on a variety of textual similarity tasks, and on most of these tasks even beats some sophisticated supervised methods tested in (Wieting et al., 2016), including some RNN and LSTM models.",
      "startOffset" : 210,
      "endOffset" : 232
    }, {
      "referenceID" : 28,
      "context" : "Of course, this SIF reweighting is highly reminiscent of TF-IDF reweighting from information retrieval (Sparck Jones, 1972; Robertson, 2004) if one treats a “sentence” as a “document” and make the reasonable assumption that the sentence doesn’t typically contain repeated words.",
      "startOffset" : 103,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "The current paper provides a theoretical justification for the reweighting using a generative model for sentences, which is a simple modification for the Random Walk on Discourses model for generating text in (Arora et al., 2016).",
      "startOffset" : 209,
      "endOffset" : 229
    }, {
      "referenceID" : 4,
      "context" : "Our modification to the generative model of (Arora et al., 2016) allows “smoothing” terms, and then a max likelihood calculation leads to our SIF reweighting.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "They can be obtained from the internal representations from neural network models of text (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a) or by low rank approximation of co-occurrence statistics (Deerwester et al.",
      "startOffset" : 90,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : ", 2013a) or by low rank approximation of co-occurrence statistics (Deerwester et al., 1990; Pennington et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : ", 2013a) or by low rank approximation of co-occurrence statistics (Deerwester et al., 1990; Pennington et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "The two approaches are known to be closely related (Levy & Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016).",
      "startOffset" : 51,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "The two approaches are known to be closely related (Levy & Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016).",
      "startOffset" : 51,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Our work is most directly related work to (Arora et al., 2016), which proposed a random walk model for generating words in the documents.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "Another approach is recursive neural networks (RNNs) defined on the parse tree, trained with supervision (Socher et al., 2011) or without (Socher et al.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : ", 2011) or without (Socher et al., 2014).",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "of (Kiros et al., 2015) tries to reconstruct the surrounding sentences from surrounded one and treats the hidden parameters as their vector representations.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 33,
      "context" : "RNNs using long short-term memory (LSTM) capture long-distance dependency and have also been used for modeling sentences (Tai et al., 2015).",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Other neural network structures include convolution neural networks, such as (Blunsom et al., 2014) that uses a dynamic pooling to handle input sentences of varying length and do well in sentiment prediction and classification tasks.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 37,
      "context" : "The directed inspiration for our work is (Wieting et al., 2016) which learned paraphrastic sentence embeddings by using simple word averaging and also updating standard word embeddings based on supervision from paraphrase pairs; the supervision being used for both initialization and training.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "We briefly recall the latent variable generative model for text in (Arora et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "It was shown in (Arora et al., 2016) that under some reasonable assumptions this model generates behavior –in terms of word-word cooccurence probabilities—that fits empirical works like word2vec and Glove.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "In the paper (Arora et al., 2016), it was shown that the MAP estimate of cs is —up to multiplication by scalar—the average of the embeddings of the words in the sentence.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : ") We borrow the key modeling assumption of (Arora et al., 2016), namely that the word vw’s are roughly uniformly dispersed, which implies that the partition function Zc is roughly the same in all directions.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 37,
      "context" : "Results collected from (Wieting et al., 2016) except tfidf-GloVe Our approach Supervised Su.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "We test our methods on the 22 textual similarity datasets including all the datasets from SemEval semantic textual similarity (STS) tasks (2012-2015) (Agirre et al., 2012; 2013; 2014; Agirrea et al., 2015), and the SemEval 2015 Twitter task (Xu et al.",
      "startOffset" : 150,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "We test our methods on the 22 textual similarity datasets including all the datasets from SemEval semantic textual similarity (STS) tasks (2012-2015) (Agirre et al., 2012; 2013; 2014; Agirrea et al., 2015), and the SemEval 2015 Twitter task (Xu et al.",
      "startOffset" : 150,
      "endOffset" : 205
    }, {
      "referenceID" : 38,
      "context" : ", 2015), and the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 Semantic Relatedness task (Marelli et al.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 21,
      "context" : ", 2015) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "ST denotes the skip-thought vectors (Kiros et al., 2015), avg-GloVe denotes the unweighted average of the GloVe vectors (Pennington et al.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : ", 2015), avg-GloVe denotes the unweighted average of the GloVe vectors (Pennington et al., 2014),3 and tfidf-GloVe denotes the weighted average of GloVe vectors using TF-IDF weights.",
      "startOffset" : 71,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : "This method uses the unweighted average of the PARAGRAMSL999 (PSL) word vectors from (Wieting et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 37,
      "context" : "are proposed in (Wieting et al., 2016).",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "DAN denotes the deep averaging network of (Iyyer et al., 2015).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "The LSTM is the version from (Gers et al., 2002), either with output gates (denoted as LSTM(o.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 37,
      "context" : "Results in Column 2 to 6 are collected from (Wieting et al., 2016), and those in Column 7 for skip-thought are from (Lei Ba et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "SN vectors are trained on the enwiki dataset (Wikimedia, 2012) using the method in (Arora et al., 2016), while PSL and GloVe vectors are those used in Table 1.",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 39,
      "context" : "Next, we fix a = 10−3 and use four very different datasets to estimate p(w): enwiki (wikipedia, 3 billion tokens), poliblogs (Yano et al., 2009) (political blogs, 5 million), commoncrawl (Buck et al.",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : ", 2009) (political blogs, 5 million), commoncrawl (Buck et al., 2014) (Internet crawl, 800 billion), text8 (Mahoney, 2008) (wiki subset, 1 million).",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : ", 2014) (Internet crawl, 800 billion), text8 (Mahoney, 2008) (wiki subset, 1 million).",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 30,
      "context" : "We consider three tasks: the SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013).",
      "startOffset" : 146,
      "endOffset" : 167
    }, {
      "referenceID" : 37,
      "context" : "Setup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison, i.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : ", the classifier a linear projection followed by the classifier in (Kiros et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : ", (Maas et al., 2011)).",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "This work provided a simple approach to sentence embedding, based on the discourse vectors in the random walk model for generating text (Arora et al., 2016).",
      "startOffset" : 136,
      "endOffset" : 156
    } ],
    "year" : 2017,
    "abstractText" : "The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR’16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN’s and LSTM’s. It even improves Wieting et al.’s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL’16) with new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.",
    "creator" : "LaTeX with hyperref package"
  }
}