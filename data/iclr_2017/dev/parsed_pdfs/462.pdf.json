{
  "name" : "462.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ON DETECTING ADVERSARIAL PERTURBATIONS",
    "authors" : [ "Jan Hendrik Metzen" ],
    "emails" : [ "JanHendrik.Metzen@de.bosch.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the last years, machine learning and in particular deep learning methods have led to impressive performance on various challenging perceptual tasks, such as image classification (Russakovsky et al., 2015; He et al., 2016) and speech recognition (Amodei et al., 2016). Despite these advances, perceptual systems of humans and machines still differ significantly. As Szegedy et al. (2014) have shown, small but carefully directed perturbations of images can lead to incorrect classification with high confidence on artificial systems. Yet, for humans these perturbations are often visually imperceptible and do not stir any doubt about the correct classification. In fact, so called adversarial examples are crucially characterized by requiring minimal perturbations that are quasi-imperceptible to a human observer. For computer vision tasks, multiple techniques to create such adversarial examples have been developed recently. Perhaps most strikingly, adversarial examples have been shown to transfer between different network architectures, and networks trained on disjoint subsets of data (Szegedy et al., 2014). Adversarial examples have also been shown to translate to the real world (Kurakin et al., 2016), e.g., adversarial images can remain adversarial even after being printed and recaptured with a cell phone camera. Moreover, Papernot et al. (2016a) have shown that a potential attacker can construct adversarial examples for a network of unknown architecture by training an auxiliary network on similar data and exploiting the transferability of adversarial inputs.\nThe vulnerability to adversarial inputs can be problematic and even prevent the application of deep learning methods in safety- and security-critical applications. The problem is particularly severe when human safety is involved, for example in the case of perceptual tasks for autonomous driving. Methods to increase robustness against adversarial attacks have been proposed and range from augmenting the training data (Goodfellow et al., 2015) over applying JPEG compression to the input (Dziugaite et al., 2016) to distilling a hardened network from the original classifier network (Papernot et al., 2016b). However, for some recently published attacks (Carlini & Wagner, 2016), no effective counter-measures are known yet.\nIn this paper, we propose to train a binary detector network, which obtains inputs from intermediate feature representations of a classifier, to discriminate between samples from the original data set and adversarial examples. Being able to detect adversarial perturbations might help in safety- and security-critical semi-autonomous systems as it would allow disabling autonomous operation and\nrequesting human intervention (along with a warning that someone might be manipulating the system). However, it might intuitively seem very difficult to train such a detector since adversarial inputs are generated by tiny, sometimes visually imperceptible, perturbations of genuine examples. Despite this intuition, our results on CIFAR10 and a 10-class subset of ImageNet show that a detector network that achieves high accuracy in detection of adversarial inputs can be trained successfully. Moreover, while we train a detector network to detect perturbations of a specific adversary, our experiments show that detectors generalize to similar and weaker adversaries. An obvious attack against our approach would be to develop adversaries that take into account both networks, the classification and the adversarial detection network. We present one such adversary and show that we can harden the detector against such an adversary using a novel training procedure."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "Since their discovery by Szegedy et al. (2014), several methods to generate adversarial examples have been proposed. Most of these methods generate adversarial examples by optimizing an image w.r.t. the linearized classification cost function of the classification network by maximizing the probability for all but the true class or minimizing the probability of the true class (e.g., (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1.\nSeveral approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation. In fact, the authors find that previous attacks were very fragile and could easily fail to find adversarial examples even when they existed. An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models.\nEven though the existence of adversarial examples has been demonstrated several times on many different classification tasks, the question of why adversarial examples exist in the first place and whether they are sufficiently regular to be detectable, which is studied in this paper, has remained open. Szegedy et al. (2014) speculated that the data-manifold is filled with “pockets” of adversarial inputs that occur with very low probability and thus are almost never observed in the test set. Yet, these pockets are dense and so an adversarial example is found virtually near every test case. The authors further speculated that the high non-linearity of deep networks might be the cause for the existence of these low-probability pockets. Later, Goodfellow et al. (2015) introduced the linear explanation: Given an input and some adversarial noise η (subject to: ||η||∞ < ), the dot product between a weight vector w and an adversarial input xadv = x+ η is given by wTxadv = wTx+ wTη. The adversarial noise η causes a neuron’s activation to grow by wTη. The max-norm constraint on η does not allow for large values in one dimension, but if x and thus η are high-dimensional, many small changes in each dimension of η can accumulate to a large change in a neuron’s activation. The conclusion was that “linear behavior in high-dimensional spaces is sufficient to cause adversarial examples”.\nTanay & Griffin (2016) challenged the linear-explanation hypothesis by constructing classes of images that do not suffer from adversarial examples under a linear classifier. They also point out that if the change in activation wTη grows linearly with the dimensionality of the problem, so does the activation\nwTx. Instead of the linear explanation, Tanay et al. provide a different explanation for the existence of adversarial examples, including a strict condition for the non-existence of adversarial inputs, a novel measure for the strength of adversarial examples and a taxonomy of different classes of adversarial inputs. Their main argument is that if a learned class boundary lies close to the data manifold, but the boundary is (slightly) tilted with respect to the manifold1, then adversarial examples can be found by perturbing points from the data manifold towards the classification boundary until the perturbed input crosses the boundary. If the boundary is only slightly tilted, the distance required by the perturbation to cross the decision-boundary is very small, leading to strong adversarial examples that are visually almost imperceptibly close to the data. Tanay et. al further argue that such situations are particularly likely to occur along directions of low variance in the data and thus speculate that adversarial examples can be considered an effect of an over-fitting phenomenon that could be alleviated by proper regularization, though it is completely unclear how to regularize neural networks accordingly.\nRecently, Moosavi-Dezfooli et al. (2016a) demonstrated that there even exist universal, imageagnostic perturbations which, when added to all data points, fool deep nets on a large fraction of ImageNet validation images. Moreover, they showed that these universal perturbations are to a certain extent also transferable between different network architectures. While this observation raises interesting questions about geometric properties and correlations of different parts of the decision boundary of deep nets, potential regularities in adversarial perturbations may also help detecting them. However, the existence of universal perturbations does not necessarily imply that the adversarial examples generated by data-dependent adversaries will be regular. Actually, Moosavi-Dezfooli et al. (2016a) show that universal perturbations are not unique and that there even exist many different universal perturbations which have little in common. This paper studies if data-dependent adversarial perturbations can nevertheless be detected reliably and answers this question affirmatively."
    }, {
      "heading" : "3 METHODS",
      "text" : "In this section, we introduce the adversarial attacks used in the experiments, propose an approach for detecting adversarial perturbations, introduce a novel adversary that aims at fooling both the classification network and the detector, and propose a training method for the detector that aims at counteracting this novel adversary."
    }, {
      "heading" : "3.1 GENERATING ADVERSARIAL EXAMPLES",
      "text" : "Let x be an input image x ∈ R3×width×height, ytrue(x) be a one-hot encoding of the true class of image x, and Jcls(x, y(x)) be the cost function of the classifier (typically cross-entropy). We briefly introduce different adversarial attacks used in the remainder of the paper.\nFast method: One simple approach to compute adversarial examples was described by Goodfellow et al. (2015). The applied perturbation is the direction in image space which yields the highest increase of the linearized cost function under `∞-norm. This can be achieved by performing one step in the direction of the gradient’s sign with step-width ε:\nxadv = x+ ε sgn(∇xJcls(x, ytrue(x)))\nHere, ε is a hyper-parameter governing the distance between adversarial and original image. As suggested in Kurakin et al. (2016) we also refer to this as the fast method due to its non-iterative and hence fast computation.\nBasic Iterative method (`∞ and `2): As an extension, Kurakin et al. (2016) introduced an iterative version of the fast method, by applying it several times with a smaller step size α and clipping all pixels after each iteration to ensure results stay in the ε-neighborhood of the original image:\nxadv0 = x, x adv n+1 = Clip ε x { xadvn + α sgn(∇xJcls(xadvn , ytrue(x))) } 1It is easier to imagine a linear decision-boundary - for neural networks this argument must be translated into\na non-linear equivalent of boundary tilting.\nFollowing Kurakin et al. (2016), we refer to this method as the basic iterative method and use α = 1, i.e., we change each pixel maximally by 1. The number of iterations is set to 10. In addition to this method, which is based on the `∞-norm, we propose an analogous method based on the `2-norm: in each step this method moves in the direction of the (normalized) gradient and projects the adversarial examples back on the ε-ball around x (points with `2 distance ε to x) if the `2 distance exceeds ε:\nxadv0 = x, x adv n+1 = Project ε x { xadvn + α\n∇xJcls(xadvn , ytrue(x)) ||∇xJcls(xadvn , ytrue(x))||2 } DeepFool method: Moosavi-Dezfooli et al. (2016b) introduced the DeepFool adversary which iteratively perturbs an image xadv0 . Therefore, in each step the classifier is linearized around x adv n and the closest class boundary is determined. The minimal step according to the `p distance from xadvn to traverse this class boundary is determined and the resulting point is used as xadvn+1. The algorithm stops once xadvn+1 changes the class of the actual (not linearized) classifier. Arbitrary `p-norms can be used within DeepFool, and here we focus on the `2- and `∞-norm. The technical details can be found in (Moosavi-Dezfooli et al., 2016b). We would like to note that we use the variant of DeepFool presented in the first version of the paper (https://arxiv.org/abs/1511.04599v1) since we found it to be more stable compared to the variant reported in the final version."
    }, {
      "heading" : "3.2 DETECTING ADVERSARIAL EXAMPLES",
      "text" : "We augment classification networks by (relatively small) subnetworks, which branch off the main network at some layer and produce an output padv ∈ [0, 1] which is interpreted as the probability of the input being adversarial. We call this subnetwork “adversary detection network” (or “detector” for short) and train it to classify network inputs into being regular examples or examples generated by a specific adversary. For this, we first train the classification networks on the regular (non-adversarial) dataset as usual and subsequently generate adversarial examples for each data point of the train set using one of the methods discussed in Section 3.1. We thus obtain a balanced, binary classification dataset of twice the size of the original dataset consisting of the original data (label zero) and the corresponding adversarial examples (label one). Thereupon, we freeze the weights of the classification network and train the detector such that it minimizes the cross-entropy of padv and the labels. The details of the adversary detection subnetwork and how it is attached to the classification network are specific for datasets and classification networks. Thus, evaluation and discussion of various design choices of the detector network are provided in the respective section of the experimental results."
    }, {
      "heading" : "3.3 DYNAMIC ADVERSARIES AND DETECTORS",
      "text" : "In the worst case, an adversary might not only have access to the classification network and its gradient but also to the adversary detector and its gradient2. In this case, the adversary might potentially generate inputs to the network that fool both the classifier (i.e., get classified wrongly) and fool the detector (i.e., look innocuous). In principle, this can be achieved by replacing the cost Jcls(x, ytrue(x)) by (1 − σ)Jcls(x, ytrue(x)) + σJdet(x, 1), where σ ∈ [0, 1] is a hyperparameter and Jdet(x, 1) is the cost (cross-entropy) of the detector for the generated x and the label one, i.e., being adversarial. An adversary maximizing this cost would thus aim at letting the classifier mis-label the input x and making the detectors output padv as small as possible. The parameter σ allows trading off these two objectives. For generating x, we propose the following extension of the basic iterative (`∞) method:\nxadv0 = x; x adv n+1 = Clip ε x { xadvn + α [ (1− σ) sgn(∇xJcls(xadvn , ytrue(x))) + σ sgn(∇xJdet(xadvn , 1)) ]} Note that we found a smaller α to be essential for this method to work; more specifically, we use α = 0.25. Since such an adversary can adapt to the detector, we call it a dynamic adversary. To\n2We would like to emphasize that is a stronger assumption than granting the adversary access to only the original classifier’s predictions and gradients since the classifier’s predictions need often be presented to a user (and thus also to an adversary). The same is typically not true for the predictions of the adversary detector as they will only be used internally.\ncounteract dynamic adversaries, we propose dynamic adversary training, a method for hardening detectors against dynamic adversaries. Based on the approach proposed by Goodfellow et al. (2015), instead of precomputing a dataset of adversarial examples, we compute the adversarial examples on-the-fly for each mini-batch and let the adversary modify each data point with probability 0.5. Note that a dynamic adversary will modify a data point differently every time it encounters the data point since it depends on the detector’s gradient and the detector changes over time. We extend this approach to dynamic adversaries by employing a dynamic adversary, whose parameter σ is selected uniform randomly from [0, 1], for generating the adversarial data points during training. By training the detector in this way, we implicitly train it to resist dynamic adversaries for various values of σ. In principle, this approach bears the risk of oscillation and unlearning for σ > 0 since both, the detector and adversary, adapt to each other (i.e., there is no fixed data distribution). In practice, however, we found this approach to converge stably without requiring careful tuning of hyperparameters."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS",
      "text" : "In this section, we present results on the detectability of adversarial perturbations on the CIFAR10 dataset (Krizhevsky, 2009), both for static and dynamic adversaries. Moreover, we investigate whether adversarial perturbations are also detectable in higher-resolution images based on a subset of the ImageNet dataset (Russakovsky et al., 2015)."
    }, {
      "heading" : "4.1 CIFAR10",
      "text" : "We use a 32-layer Residual Network (He et al., 2016, ResNet) as classifier. The structure of the network is shown in Figure 1. The network has been trained for 100 epochs with stochastic gradient descent and momentum on 45000 data points from the train set. The momentum term was set to 0.9 and the initial learning rate was set to 0.1, reduced to 0.01 after 41 epochs, and further reduced to 0.001 after 61 epochs. After each epoch, the network’s performance on the validation data (the remaining 5000 data points from the train set) was determined. The network with maximal performance on the validation data was used in the subsequent experiments (with all tunable weights being fixed). This network’s accuracy on non-adversarial test data is 91.3%. We attach an adversary detection subnetwork (called “detector” below) to the ResNet. The detector is a convolutional neural network using batch normalization (Ioffe & Szegedy, 2015) and rectified linear units. In the experiments, we investigate different positions where the detector can be attached (see also Figure 1)."
    }, {
      "heading" : "4.1.1 STATIC ADVERSARIES",
      "text" : "In this subsection, we investigate a static adversary, i.e., an adversary that only has access to the classification network but not to the detector. The detector was trained for 20 epochs on 45000 data points from the train set and their corresponding adversarial examples using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001 and β1 = 0.99, β2 = 0.999. The remaining 5000 data points from the CIFAR10 train set are used as validation data and used for model selection. The detector was attached to position AD(2) (see Figure 1) except for the DeepFool-based adversaries where the detector was attached to AD(4); see below for a discussion. For the “Fast” and “Iterative” adversaries, the parameter ε from Section 3.1 was chosen from [1, 2, 3, 4] for `∞-based methods and from [20, 40, 60, 80] for `2-based methods; larger values of ε generally result in reduced accuracy of the classifier but increased detectability. For the “Iterative” method with `2-norm, we used α = 20, i.e., in each iteration we make a step of `2 distance 20. Please note that these values of ε are based on assuming a range of [0, 255] per color channel of the input.\nFigure 2 (left) compares the detectability3 of different adversaries. In general, points in the lower left of the plot correspond to stronger adversaries because their adversarial examples are harder to detect and at the same time fool the classifier on most of the images. Detecting adversarial examples works surprisingly well given that no differences are perceivable to humans for all shown settings: the detectability is above 80% for all adversaries which decrease classification accuracy below 30% and above 90% for adversaries which decrease classification accuracy below 10%. Comparing the different adversaries, the “Fast” adversary can generally be considered as a weak adversary, the DeepFool based methods as relatively strong adversaries, and the “Iterative” method being somewhere in-between. Moreover, the methods based on the `2-norm are generally slightly stronger than their `∞-norm counter-parts.\nFigure 2 (right) compares the detectability of different adversaries for detectors attached at different points to the classification network. ε was chosen minimal under the constraint that the classification accuracy is below 30%. For the “Fast” and “Iterative” adversaries, the attachment position AD(2) works best, i.e., attaching to a middle layer where more abstract features are already extracted but still the full spatial resolution is maintained. For the DeepFool methods, the general pattern is similar except for AD(4), which works best for these adversaries.\nFigure 3 illustrates the generalizability of trained detectors for the same adversary with different choices of ε: while a detector trained for large ε does not generalize well to small ε, the other direction works reasonably well. Figure 4 shows the generalizability of detectors trained for one adversary when tested on data from other adversaries (ε was chosen again minimal under the constraint that the\n3Detectability refers to the accuracy of the detector. The detectability on the test data is calculated as follows: for every test sample, a corresponding adversarial example is generated. The original and the corresponding adversarial examples form a joint test set (twice the size of the original test set). This test set is shuffled and the detector is evaluated on this dataset. Original and corresponding adversarial example are thus processed independently.\nclassification accuracy is below 30%): we can see that detectors generalize well between `∞- and `2-norm based variants of the same approach. Moreover, detectors trained on the stronger “Iterative” adversary generalize well to the weaker “Fast” adversary but not vice versa. Detectors trained for the DeepFool-based methods do not generalize well to other adversaries; however, detectors trained for the “Iterative” adversaries generalize relatively well to the DeepFool adversaries."
    }, {
      "heading" : "4.1.2 DYNAMIC ADVERSARIES",
      "text" : "In this section, we evaluate the robustness of detector networks to dynamic adversaries (see Section 3.3). For this, we evaluate the detectability of dynamic adversaries for σ ∈ {0.0, 0.1, . . . , 1.0}. We use the same optimizer and detector network as in Section 4.1.1. When evaluating the detectability of dynamic adversaries with σ close to 1, we need to take into account that the adversary might choose to solely focus on fooling the detector, which is trivially achieved by leaving the input unmodified. Thus, we ignore adversarial examples that do not cause a misclassification in the evaluation of the detector and evaluate the detector’s accuracy on regular data versus the successful adversarial examples. Figure 5 shows the results of a dynamic adversary with ε = 1 against a static detector, which was trained to only detect static adversaries, and a dynamic detector, which was explicitly trained to resist dynamic adversaries. As can be seen, the static detector is not robust to dynamic adversaries since for certain values of σ, namely σ = 0.3 and σ = 0.4, the detectability is close to\nchance level while the predictive performance of the classifier is severely reduced to less than 30% accuracy. A dynamic detector is considerably more robust and achieves a detectability of more than 70% for any choice of σ."
    }, {
      "heading" : "4.2 10-CLASS IMAGENET",
      "text" : "In this section, we report results for static adversaries on a subset of ImageNet consisting of all data from ten randomly selected classes4. The motivation for this section is to investigate whether adversarial perturbations can be detected in higher-resolution images and for other network architectures than residual networks. We limit the experiment to ten classes in order to keep the computational resources required for computing the adversarial examples small and avoid having too similar classes which would oversimplify the task for the adversary. We use a pretrained VGG16 (Simonyan & Zisserman, 2015) as classification network and add a layer before the softmax which selects only the 10 relevant class entries from the logits vector. Based on preliminary experiments, we attach the detector network after the fourth max-pooling layer. The detector network consists of a sequence of five 3x3 convolutions with 196 feature maps each using batch-normalization and rectified linear units, followed by a 1x1 convolution which maps onto the 10 classes, global-average pooling, and a softmax layer. An additional 2x2 max-pooling layer is added after the first convolution. Note that we did not tune the specific details of the detector network; other topologies might perform better than the results reported below. When applicable, we vary ε ∈ [2, 4, 6] for `∞-based methods and ε ∈ [400, 800, 1200] for `2. Moreover, we limit changes of the DeepFool adversaries to an `∞ distance of 6 since the adversary would otherwise sometimes generate distortions which are clearly perceptible. We train the detector for 500 epochs using the Adam optimizer with a learning rate of 0.0001 and β1 = 0.99, β2 = 0.999.\nFigure 6 compares the detectability of different static adversaries. All adversaries fail to decrease predictive accuracy of the classifier below the chance level of 0.1 (note that predictive accuracy refers to the accuracy on the 10-class problem not on the full 1000-class problem) for the given values of ε. Nevertheless, detectability is 85% percent or more with the exception of the “Iterative” `2-based adversary with ε = 400. For this adversary, the detector only reaches chance level. Other choices of the detector’s attachment depth, internal structure, or hyperparameters of the optimizer might achieve\n4The synsets of the selected classes are: palace; joystick; bee; dugong, Dugong dugon; cardigan; modem; confectionery, confectionary, candy store; valley, vale; Persian cat; stone wall. Classes were selected by randomly drawing 10 ILSVRC2012 Synset-IDs (i.e. integers from [1, 1000]), using the randint function of the python-package numpy after initializing numpy’s random number generator seed with 0. This results in a train set of 10000 images, a validation set of 2848 images, and a test set (from ImageNet’s validation data) of 500 images.\nbetter results; however, this failure case emphasizes that the detector has to detect very subtle patterns and the optimizer might get stuck in bad local optima or plateaus.\nFigure 7 illustrates the transferability of the detector between different values of ε. The results are roughly analogous to the results on CIFAR10 in Section 4.1.1: detectors trained for an adversary for a small value of ε work well for the same adversary with larger ε but not vice versa. Note that a detector trained for the “Iterative” `2-based adversary with ε = 1200 can detect the changes of the same adversary with ε = 400 with 78% accuracy; this emphasizes that this adversary is not principally undetectable but that rather the optimization of a detector for this setting is difficult. Figure 8 shows the transferability between adversaries: transferring the detector works well between similar adversaries such as between the two DeepFool adversaries and between the Fast and Iterative adversary based on the `∞ distance. Moreover, detectors trained for DeepFool adversaries work well on all other adversaries. In summary, transferability is not symmetric and typically works best between similar adversaries and from stronger to weaker adversary."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "Why can tiny adversarial perturbations be detected that well? Adopting the boundary tilting perspective of Tanay & Griffin (2016), strong adversarial examples occur in situations in which classification boundaries are tilted against the data manifold such that they lie close and nearly parallel to the data manifold. A detector could (potentially) identify adversarial examples by detecting inputs which are slightly off the data manifold’s center in the direction of a nearby class boundary. Thus, the detector can focus on detecting inputs which move away from the data manifold in a certain direction, namely one of the directions to a nearby class boundary (the detector does not have explicit\nknowledge of class boundaries but it might learn about their direction implicitly from the adversarial training data). However, training a detector which captures these directions in a model with small capacity and generalizes to unseen data requires certain regularities in adversarial perturbations. The results of Moosavi-Dezfooli et al. (2016a) suggest that there may exist regularities in the adversarial perturbations since universal perturbations exist. However, these perturbations are not unique and data-dependent adversaries might potentially choose among many different possible perturbations in a non-regular way, which would be hard to detect. Our positive results on detectability suggest that this is not the case for the tested adversaries. Thus, our results are somewhat complementary to Moosavi-Dezfooli et al. (2016a): while they show that universal, image-agnostic perturbations exist, we show that image-dependent perturbations are sufficiently regular to be detectable. Whether a detector generalizes over different adversaries depends mainly on whether the adversaries choose among many different possible perturbations in a consistent way.\nWhy is the joint classifier/detector system harder to fool? For a static detector, there might be areas which are adversarial to both classifier and detector; however, this will be a (small) subset of the areas which are adversarial to the classifier alone. Nevertheless, results in Section 4.1.2 show that such a static detector can be fooled along with the classifier. However, a dynamic detector is considerably harder to fool: on the one hand, it might further reduce the number of areas which are both adversarial to classifier and detector. On the other hand, the areas which are adversarial to the detector might become increasingly non-regular and difficult to find by gradient descent-based adversaries."
    }, {
      "heading" : "6 CONCLUSION AND OUTLOOK",
      "text" : "In this paper, we have shown empirically that adversarial examples can be detected surprisingly well using a detector subnetwork attached to the main classification network. While this does not directly allow classifying adversarial examples correctly, it allows mitigating adversarial attacks against machine learning systems by resorting to fallback solutions, e.g., a face recognition might request human intervention when verifying a person’s identity and detecting a potential adversarial attack. Moreover, being able to detect adversarial perturbations may in the future enable a better understanding of adversarial examples by applying network introspection to the detector network. Furthermore, the gradient propagated back through the detector may be used as a source of regularization of the classifier against adversarial examples. We leave this to future work. Additional future work will be developing stronger adversaries that are harder to detect by adding effective randomization which would make selection of adversarial perturbations less regular. Finally, developing methods for training detectors explicitly such that they can detect many different kinds of attacks reliably at the same time would be essential for safety- and security-related applications."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Michael Herman and Michael Pfeiffer for helpful discussions and their feedback on drafts of this article. Moreover, we would like to thank the developers of Theano (The Theano Development Team, 2016), keras (https://keras.io), and seaborn (http:// seaborn.pydata.org/)."
    } ],
    "references" : [ {
      "title" : "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "author" : [ "Jun Zhan", "Zhenyao Zhu" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zhan and Zhu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhan and Zhu.",
      "year" : 2016
    }, {
      "title" : "Towards Evaluating the Robustness of Neural Networks",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "In arXiv:1608.04644,",
      "citeRegEx" : "Carlini and Wagner.,? \\Q2016\\E",
      "shortCiteRegEx" : "Carlini and Wagner.",
      "year" : 2016
    }, {
      "title" : "A study of the effect of JPG compression on adversarial images",
      "author" : [ "Gintare Karolina Dziugaite", "Zoubin Ghahramani", "Daniel M. Roy" ],
      "venue" : "In arXiv:1608.00853,",
      "citeRegEx" : "Dziugaite et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dziugaite et al\\.",
      "year" : 2016
    }, {
      "title" : "Explaining and Harnessing Adversarial Examples",
      "author" : [ "Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Computer Vision and Pattern Recognition",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In International Conference for Learning Representations (ICLR),",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Learning Multiple Layers of Features from Tiny Images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "Master’s thesis, University of Toronto,",
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Adversarial examples in the physical world",
      "author" : [ "Alexey Kurakin", "Ian Goodfellow", "Samy Bengio" ],
      "venue" : null,
      "citeRegEx" : "Kurakin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kurakin et al\\.",
      "year" : 2016
    }, {
      "title" : "Universal adversarial perturbations",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard" ],
      "venue" : null,
      "citeRegEx" : "Moosavi.Dezfooli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Moosavi.Dezfooli et al\\.",
      "year" : 2016
    }, {
      "title" : "DeepFool: A simple and accurate method to fool deep neural networks",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Moosavi.Dezfooli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Moosavi.Dezfooli et al\\.",
      "year" : 2016
    }, {
      "title" : "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z. Berkay Celik", "Ananthram Swami" ],
      "venue" : null,
      "citeRegEx" : "Papernot et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2016
    }, {
      "title" : "Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami" ],
      "venue" : "In Symposium on Security & Privacy,",
      "citeRegEx" : "Papernot et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2016
    }, {
      "title" : "Are accuracy and robustness correlated",
      "author" : [ "Andras Rozsa", "Terrance E. Boult", "Manuel Gunther" ],
      "venue" : "In International Conference on Machine Learning and Applications (ICMLA),",
      "citeRegEx" : "Rozsa et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rozsa et al\\.",
      "year" : 2016
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei" ],
      "venue" : "International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "A Boundary Tilting",
      "author" : [ "Thomas Tanay", "Lewis Griffin" ],
      "venue" : "Persepective on the Phenomenon of Adversarial Examples",
      "citeRegEx" : "Tanay and Griffin.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tanay and Griffin.",
      "year" : 2016
    }, {
      "title" : "Development Team. Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "The Theano" ],
      "venue" : null,
      "citeRegEx" : "Theano,? \\Q2016\\E",
      "shortCiteRegEx" : "Theano",
      "year" : 2016
    }, {
      "title" : "Improving the Robustness of Deep Neural Networks via Stability Training",
      "author" : [ "Stephan Zheng", "Yang Song", "Thomas Leung", "Ian Goodfellow" ],
      "venue" : "In Computer Vision and Pattern Recognition",
      "citeRegEx" : "Zheng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "In the last years, machine learning and in particular deep learning methods have led to impressive performance on various challenging perceptual tasks, such as image classification (Russakovsky et al., 2015; He et al., 2016) and speech recognition (Amodei et al.",
      "startOffset" : 181,
      "endOffset" : 224
    }, {
      "referenceID" : 4,
      "context" : "In the last years, machine learning and in particular deep learning methods have led to impressive performance on various challenging perceptual tasks, such as image classification (Russakovsky et al., 2015; He et al., 2016) and speech recognition (Amodei et al.",
      "startOffset" : 181,
      "endOffset" : 224
    }, {
      "referenceID" : 16,
      "context" : "Perhaps most strikingly, adversarial examples have been shown to transfer between different network architectures, and networks trained on disjoint subsets of data (Szegedy et al., 2014).",
      "startOffset" : 164,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "Adversarial examples have also been shown to translate to the real world (Kurakin et al., 2016), e.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "Methods to increase robustness against adversarial attacks have been proposed and range from augmenting the training data (Goodfellow et al., 2015) over applying JPEG compression to the input (Dziugaite et al.",
      "startOffset" : 122,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : ", 2015) over applying JPEG compression to the input (Dziugaite et al., 2016) to distilling a hardened network from the original classifier network (Papernot et al.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : ", 2015; He et al., 2016) and speech recognition (Amodei et al., 2016). Despite these advances, perceptual systems of humans and machines still differ significantly. As Szegedy et al. (2014) have shown, small but carefully directed perturbations of images can lead to incorrect classification with high confidence on artificial systems.",
      "startOffset" : 8,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : ", 2015; He et al., 2016) and speech recognition (Amodei et al., 2016). Despite these advances, perceptual systems of humans and machines still differ significantly. As Szegedy et al. (2014) have shown, small but carefully directed perturbations of images can lead to incorrect classification with high confidence on artificial systems. Yet, for humans these perturbations are often visually imperceptible and do not stir any doubt about the correct classification. In fact, so called adversarial examples are crucially characterized by requiring minimal perturbations that are quasi-imperceptible to a human observer. For computer vision tasks, multiple techniques to create such adversarial examples have been developed recently. Perhaps most strikingly, adversarial examples have been shown to transfer between different network architectures, and networks trained on disjoint subsets of data (Szegedy et al., 2014). Adversarial examples have also been shown to translate to the real world (Kurakin et al., 2016), e.g., adversarial images can remain adversarial even after being printed and recaptured with a cell phone camera. Moreover, Papernot et al. (2016a) have shown that a potential attacker can construct adversarial examples for a network of unknown architecture by training an auxiliary network on similar data and exploiting the transferability of adversarial inputs.",
      "startOffset" : 8,
      "endOffset" : 1164
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al.",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : ", 2015), (Kurakin et al., 2016)).",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : ", 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al.",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : ", 2014) and the fast gradient sign attack (Goodfellow et al., 2015).",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Since their discovery by Szegedy et al. (2014), several methods to generate adversarial examples have been proposed.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries.",
      "startOffset" : 3,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1. Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples.",
      "startOffset" : 3,
      "endOffset" : 454
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1. Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions.",
      "startOffset" : 3,
      "endOffset" : 706
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1. Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation.",
      "startOffset" : 3,
      "endOffset" : 1464
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1. Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation. In fact, the authors find that previous attacks were very fragile and could easily fail to find adversarial examples even when they existed. An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models. Even though the existence of adversarial examples has been demonstrated several times on many different classification tasks, the question of why adversarial examples exist in the first place and whether they are sufficiently regular to be detectable, which is studied in this paper, has remained open. Szegedy et al. (2014) speculated that the data-manifold is filled with “pockets” of adversarial inputs that occur with very low probability and thus are almost never observed in the test set.",
      "startOffset" : 3,
      "endOffset" : 2350
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1. Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation. In fact, the authors find that previous attacks were very fragile and could easily fail to find adversarial examples even when they existed. An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models. Even though the existence of adversarial examples has been demonstrated several times on many different classification tasks, the question of why adversarial examples exist in the first place and whether they are sufficiently regular to be detectable, which is studied in this paper, has remained open. Szegedy et al. (2014) speculated that the data-manifold is filled with “pockets” of adversarial inputs that occur with very low probability and thus are almost never observed in the test set. Yet, these pockets are dense and so an adversarial example is found virtually near every test case. The authors further speculated that the high non-linearity of deep networks might be the cause for the existence of these low-probability pockets. Later, Goodfellow et al. (2015) introduced the linear explanation: Given an input and some adversarial noise η (subject to: ||η||∞ < ), the dot product between a weight vector w and an adversarial input xadv = x+ η is given by wTxadv = wTx+ wTη.",
      "startOffset" : 3,
      "endOffset" : 2799
    }, {
      "referenceID" : 3,
      "context" : ", (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1. Several approaches exist to increase a model’s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation. In fact, the authors find that previous attacks were very fragile and could easily fail to find adversarial examples even when they existed. An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models. Even though the existence of adversarial examples has been demonstrated several times on many different classification tasks, the question of why adversarial examples exist in the first place and whether they are sufficiently regular to be detectable, which is studied in this paper, has remained open. Szegedy et al. (2014) speculated that the data-manifold is filled with “pockets” of adversarial inputs that occur with very low probability and thus are almost never observed in the test set. Yet, these pockets are dense and so an adversarial example is found virtually near every test case. The authors further speculated that the high non-linearity of deep networks might be the cause for the existence of these low-probability pockets. Later, Goodfellow et al. (2015) introduced the linear explanation: Given an input and some adversarial noise η (subject to: ||η||∞ < ), the dot product between a weight vector w and an adversarial input xadv = x+ η is given by wTxadv = wTx+ wTη. The adversarial noise η causes a neuron’s activation to grow by wTη. The max-norm constraint on η does not allow for large values in one dimension, but if x and thus η are high-dimensional, many small changes in each dimension of η can accumulate to a large change in a neuron’s activation. The conclusion was that “linear behavior in high-dimensional spaces is sufficient to cause adversarial examples”. Tanay & Griffin (2016) challenged the linear-explanation hypothesis by constructing classes of images that do not suffer from adversarial examples under a linear classifier.",
      "startOffset" : 3,
      "endOffset" : 3441
    }, {
      "referenceID" : 9,
      "context" : "Recently, Moosavi-Dezfooli et al. (2016a) demonstrated that there even exist universal, imageagnostic perturbations which, when added to all data points, fool deep nets on a large fraction of ImageNet validation images.",
      "startOffset" : 10,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "Recently, Moosavi-Dezfooli et al. (2016a) demonstrated that there even exist universal, imageagnostic perturbations which, when added to all data points, fool deep nets on a large fraction of ImageNet validation images. Moreover, they showed that these universal perturbations are to a certain extent also transferable between different network architectures. While this observation raises interesting questions about geometric properties and correlations of different parts of the decision boundary of deep nets, potential regularities in adversarial perturbations may also help detecting them. However, the existence of universal perturbations does not necessarily imply that the adversarial examples generated by data-dependent adversaries will be regular. Actually, Moosavi-Dezfooli et al. (2016a) show that universal perturbations are not unique and that there even exist many different universal perturbations which have little in common.",
      "startOffset" : 10,
      "endOffset" : 802
    }, {
      "referenceID" : 3,
      "context" : "Fast method: One simple approach to compute adversarial examples was described by Goodfellow et al. (2015). The applied perturbation is the direction in image space which yields the highest increase of the linearized cost function under `∞-norm.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "As suggested in Kurakin et al. (2016) we also refer to this as the fast method due to its non-iterative and hence fast computation.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Basic Iterative method (`∞ and `2): As an extension, Kurakin et al. (2016) introduced an iterative version of the fast method, by applying it several times with a smaller step size α and clipping all pixels after each iteration to ensure results stay in the ε-neighborhood of the original image:",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "Following Kurakin et al. (2016), we refer to this method as the basic iterative method and use α = 1, i.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "DeepFool method: Moosavi-Dezfooli et al. (2016b) introduced the DeepFool adversary which iteratively perturbs an image xadv 0 .",
      "startOffset" : 17,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Conv denotes a convolutional layer, Res∗5 denotes a sequence of 5 residual blocks as introduced by He et al. (2016), GAP denotes a global-average pooling layer and Dens a fully-connected layer.",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "Based on the approach proposed by Goodfellow et al. (2015), instead of precomputing a dataset of adversarial examples, we compute the adversarial examples on-the-fly for each mini-batch and let the adversary modify each data point with probability 0.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "In this section, we present results on the detectability of adversarial perturbations on the CIFAR10 dataset (Krizhevsky, 2009), both for static and dynamic adversaries.",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Moreover, we investigate whether adversarial perturbations are also detectable in higher-resolution images based on a subset of the ImageNet dataset (Russakovsky et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "The results of Moosavi-Dezfooli et al. (2016a) suggest that there may exist regularities in the adversarial perturbations since universal perturbations exist.",
      "startOffset" : 15,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "The results of Moosavi-Dezfooli et al. (2016a) suggest that there may exist regularities in the adversarial perturbations since universal perturbations exist. However, these perturbations are not unique and data-dependent adversaries might potentially choose among many different possible perturbations in a non-regular way, which would be hard to detect. Our positive results on detectability suggest that this is not the case for the tested adversaries. Thus, our results are somewhat complementary to Moosavi-Dezfooli et al. (2016a): while they show that universal, image-agnostic perturbations exist, we show that image-dependent perturbations are sufficiently regular to be detectable.",
      "startOffset" : 15,
      "endOffset" : 536
    } ],
    "year" : 2017,
    "abstractText" : "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small “detector” subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.",
    "creator" : "LaTeX with hyperref package"
  }
}