{
  "name" : "734.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEEP VARIATIONAL CANONICAL CORRELATION ANALYSIS",
    "authors" : [ "Weiran Wang", "Xinchen Yan", "Honglak Lee", "Karen Livescu" ],
    "emails" : [ "weiranwang@ttic.edu", "klivescu@ttic.edu", "xcyan@umich.edu", "honglak@umich.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the multi-view representation learning setting, we have multiple views/measurements of the same underlying signal, and the goal is to learn useful features of each view using complementary information contained in the views. The intuition underlying this setting is that the learned features can help uncover the common sources of variation in the views, which can be helpful for exploratory analysis or for downstream tasks.\nA classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al., 2013; Wang et al., 2015b). CCA projects two random vectors x ∈ Rdx and y ∈ Rdy into a lowerdimensional subspace so that the projections are maximally correlated. There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1 (left). Assume that x and y are linear functions of some lower-dimensional random variable z ∈ Rdz , where dz ≤ min(dx, dy). When the prior distribution of the latent variable p(z) and the conditional distributions p(x|z) and p(y|z) are Gaussian, Bach and Jordan (2005) showed that E[z|x] (resp. E[z|y]) lives in the same space as the linear CCA projection for x (resp. y). This generative interpretation of CCA is often lost in nonlinear extensions of CCA. For example, in deep CCA (DCCA, (Andrew et al., 2013)), to extend CCA to nonlinear mappings with greater representation power, one extracts nonlinear features from the original inputs of each view using two DNNs, f for x and g for y, so that the canonical correlation of the DNN outputs (measured by a linear CCA with projection matrices U and V) is maximized. Formally, given a dataset of N pairs of observations (x1,y1), . . . , (xN ,yN ) of the random vectors (x,y), DCCA optimizes\nmax Wf ,Wg\nU,V\ntr ( U>f(X)g(Y)>V ) s.t. U> ( f(X)f(X)> ) U = V> ( g(Y)g(Y)> ) V = NI, (1)\nwhere f(X) = [f(x1), . . . , f(xN )] and g(Y) = [g(y1), . . . ,g(yN )], and Wf denotes all weight parameters of the DNN f (and similarly for g).\nDCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015). However, a disadvantage of DCCA is that it directly looks for DNNs that can map inputs into the low-dimensional space, without a model for generating samples from the latent space. Although Wang et al. (2015b)’s deep canonically correlated autoencoders (DCCAE) model optimizes the combination of the autoencoder objective (reconstruction errors) and the canonical correlation objective, the authors found that in practice, the canonical correlation term tends to dominate the reconstruction error terms in the DCCAE objective when tuning performance for a downstream task (especially when the inputs are noisy), and as a result the inputs are not reconstructed well. At the same time, optimization of the DCCA and DCCAE objectives is challenging due to the constraints that couple all training samples.\nThe main contribution of this paper is the proposal of a new deep multi-view learning model named deep variational CCA (VCCA), which extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by DNNs. Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. Inspired by variational autoencoders (VAE, Kingma and Welling, 2014), we parameterize the posterior distribution of the latent variables with another DNN, and derive a variational lower bound of the data likelihood as the objective of VCCA, which is further approximated by Monte Carlo sampling. With the reparameterization trick, sampling for the Monte Carlo approximation is trivial and all DNN weights in VCCA can be optimized jointly via stochastic gradient descent, using unbiased gradient estimates from small minibatches. Interestingly, VCCA is related to multi-view autoencoders (Ngiam et al., 2011), with the key distinctions of additional regularization on the posterior distribution and the sampling procedure at the bottleneck layer.\nWe also propose a variant of VCCA called VCCA-private that can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision. Last but not least, as generative models, VCCA and VCCA-private enable us to obtain high-quality samples for the input of each view."
    }, {
      "heading" : "2 VARIATIONAL CCA",
      "text" : "The probabilistic latent variable model of CCA (Bach and Jordan, 2005) defines the following joint distribution over the random variables (x,y):\np(x,y, z) = p(z)p(x|z)p(y|z), p(x,y) = ∫ p(x,y, z)dz. (2)\nThe assumption underlying this model is that, conditioned on the latent variables z ∈ Rdz , the two views x and y are independent. However, linear observation models (p(x|z) and p(y|z) as shown in Figure 1 (left)) have limited representation power. In this paper, we consider nonlinear\nobservation models pθ(x|z;θx) and pθ(y|z;θy), parameterized by θx and θy respectively, which can be the collections of weights of DNNs. In this case, the marginal likelihood pθ(x,y) does not have a closed form. In addition, the inference problem pθ(z|x)—the problem of inferring the latent variables given one of the views—is also intractable.\nInspired by Kingma and Welling (2014)’s work on variational autoencoders (VAE), we approximate pθ(z|x) with the conditional density qφ(z|x;φz), where φz is the collection of parameters of another DNN.1 We can derive a lower bound on the marginal data likelihood using qφ(z|x):\nlog pθ(x,y) = log pθ(x,y) ∫ qφ(z|x)dz = ∫ log pθ(x,y)qφ(z|x)dz\n= ∫ qφ(z|x) ( log\nqφ(z|x) pθ(z|x,y) + log pθ(x,y, z) qφ(z|x)\n) dz\n= DKL(qφ(z|x)||pθ(z|x,y)) + Eqφ(z|x) [ log pθ(x,y, z)\nqφ(z|x) ] ≥ Eqφ(z|x) [ log pθ(x,y, z)\nqφ(z|x)\n] =: L(x,y;θ,φ) (3)\nwhere we used the fact that KL divergence is nonnegative in the last step. As a result, L(x,y;θ,φ) is a lower bound on the data log-likelihood logθ p(x,y). Substituting (2) into (3), we have\nL(x,y;θ,φ) = ∫ qφ(z|x) ( log p(z)\nqφ(z|x) + log pθ(x|z) + log pθ(y|z)\n) dz\n= −DKL(qφ(z|x)||p(z)) + Eqφ(z|x) [log pθ(x|z) + log pθ(y|z)] . (4) VCCA maximizes this variational lower bound on the data likelihood on the training set:\nmax θ,φ\n1\nN N∑ i=1 L(xi,yi;θ,φ). (5)\nThe first term in (4) measures the KL divergence between the approximate posterior distribution and the prior distribution of the latent variables z. When the parameterization qφ(z|x) is chosen properly, this term can be computed exactly in closed form. As a concrete example, let the variational approximate posterior be a multivariate Gaussian with diagonal covariance. That is, for a sample pair (xi,yi), we have\nlog qφ(zi|xi) = logN (zi;µi,Σi), Σi = diag ( σ2i1, . . . , σ 2 idz ) , (6)\nwhere the mean µi and covariance Σi are outputs of an encoding DNN f (and thus [µi,Σi] = f(xi;φz) are deterministic nonlinear functions of xi). In this case, we have\nDKL(qφ(zi|xi)||p(zi)) = − 1\n2 dz∑ j=1 ( 1 + log σ2ij − σ2ij − µ2ij ) .\nThe second term of (4) corresponds to the expected complete data likelihood under the approximate posterior distribution. Though still intractable, this term can be approximated by Monte Carlo sampling. In particular, we draw L samples z(l)i ∼ qφ(zi|xi):\nz (l) i = µi + Σi (l), where (l) ∼ N (0, I), for l = 1, . . . , L, (7) and have\nEqφ(zi|xi) [log pθ(xi|zi) + log pθ(yi|zi)] ≈ 1\nL L∑ l=1 log pθ ( xi|z(l)i ) + log pθ ( yi|z(l)i ) . (8)\nNotice that we parameterized qφ(zi|xi) above to obtain the VCCA objective; this is useful when the first view is available for downstream tasks, in which case we can directly apply qφ(zi|xi) to obtain its projection (as features). One could also derive likelihood lower bounds by parameterizing the approximate posteriors qφ(zi|yi) and qφ(zi|xi,yi), and optimize their convex combinations for training. We give a sketch of VCCA in Figure 1 (right).\n1For notational simplicity, we denote by θ the collection of parameters associated with the model probabilities pθ(·), and φ the collection of parameters associated with the variational approximate probabilities qφ(·), and often omit specific parameters inside the probabilities.\nConnection to multi-view autoencoder (MVAE) If we use the Gaussian observation models\nlog pθ(x|z) = logN (gx(z;θx), I), log pθ(y|z) = logN (gy(z;θy), I), we observe that log pθ ( xi|z(l)i ) and log pθ ( yi|z(l)i ) measure the reconstruction errors of each\nview’s inputs from samples z(l)i using the two DNNs gx and gy respectively. In this case, maximizing L(x,y;θ,φ) is equivalent to\nmin θ,φ\n1\nN N∑ i=1 DKL(qφ(zi|xi)||p(zi)) + 1 2NL N∑ i=1 L∑ l=1 ∥∥∥xi − gx (z(l)i ;θx)∥∥∥2 + ∥∥∥yi − gy (z(l)i ;θy)∥∥∥2 (9)\ns.t. z(l)i = µi + Σi (l), where (l) ∼ N (0, I), l = 1, . . . , L.\nNow, consider the case of Σi → 0, for i = 1, . . . , N , and we have z(l)i → µi which is a deterministic function of x (and there is no need for sampling). In the limit, the second term of (9) reduces to\n1\n2N N∑ i=1 ‖xi − gx(f(xi;φz);θx)‖ 2 + ‖yi − gy(f(xi;φz);θy)‖ 2 , (10)\nwhich is the objective of the multi-view autoencoder (MVAE, Ngiam et al., 2011). Note, however, that Σi → 0 is prevented by the VCCA objective as it results in a large penalty in DKL(qφ(zi|xi)||p(zi)). Compared with the MVAE objective, in the VCCA objective we are creating L different “noisy” versions of the latent representation and enforce that these versions reconstruct the original inputs well. The “noise” distribution (the variances Σi) are also learned and regularized by the KL divergence DKL(qφ(zi|xi)||p(zi)). Using the VCCA objective, we expect to learn different representations from those of MVAE, due to these regularization effects."
    }, {
      "heading" : "2.1 EXTRACTING PRIVATE VARIABLES",
      "text" : "So far, VCCA aims at extracting only the latent variables z that are common to both views. A potential disadvantage of this model is that it assumes the common variables are sufficient by themselves to generate the views, which can be too restrictive in practice. Consider the example of audio and articulatory measurements as two views for speech. Although the transcription is a common variable behind the views, it combines with the physical environment and the vocal tract anatomy to generate the individual views. In other words, there might be large variations in the input space that can not be explained by the common variables, making the objective (4) hard to optimize. It may then be beneficial to explicitly model the private variables within each view.\nWe therefore propose a new probabilistic graphical model, shown in Figure 2, that we refer to as VCCA-private. We introduce two sets of hidden variables hx ∈ Rdhx and hy ∈ Rdhy to explain the aspects of x and y not captured by the common variables z. Under this model, the data likelihood\nis defined by\npθ(x,y, z,hx,hy) = p(z)p(hx)p(hy)pθ(x|z,hx;θx)pθ(y|z,hy;θy), (11)\npθ(x,y) = ∫ ∫ ∫ pθ(x,y, z,hx,hy)dz dhx dhy.\nTo obtain tractable inference, we introduce the following factored variational posterior\nqφ(z,hx,hy|x,y) = qφ(z|x;φz)qφ(hx|x;φx)qφ(hy|y;φy), (12)\nwhere each factor is parameterized by a different DNN. Similarly to VCCA, we can derive a variational lower bound on the data likelihood for VCCA-private as\nlog pθ(x,y) ≥ ∫ ∫ ∫ qφ(z,hx,hy|x,y) log pθ(x,y, z,hx,hy)\nqφ(z,hx,hy|x,y) dz dhx dhy\n= ∫ ∫ ∫ qφ(z,hx,hy|x,y) [ log p(z)\nqφ(z|x) + log\np(hx)\nqφ(hx|x) + log\np(hy)\nqφ(hy|y) + log pθ(x|z,hx) + log pθ(y|z,hy) ] dz dhx dhy\n= −DKL(qφ(z|x)||p(z))−DKL(qφ(hx|x)||p(hx))−DKL(qφ(hy|y)||p(hy))\n+ ∫ ∫ qφ(z|x)qφ(hx|x) log pθ(x|z,hx)dz dhx + ∫ ∫ qφ(z|x)qφ(hy|y) log pθ(y|z,hy)dz dhy\n=: Lprivate(x,y;θ,φ). (13)\nAs in VCCA, the last two terms of (14) can be approximated by Monte Carlo sampling. For example, we draw samples of z and hx from their corresponding approximate posteriors, and concatenate their samples as inputs to the DNN parameterizing pθ(x|z,hx). In this paper, we use simple Gaussian prior distributions for the private variables, i.e., hx ∼ N (0, I) and hy ∼ N (0, I). We leave to future work to examine the effect of more sophisticated prior distributions for the latent variables.\nVCCA-private maximizes this lower bound on the training set, i.e.,\nmax θ,φ\n1\nN N∑ i=1 Lprivate(xi,yi;θ,φ). (14)\nOptimization The objectives (5) and (14) decouple over the training samples and can be trained efficiently using stochastic gradient descent. Enabled by the reparameterization trick, unbiased gradient estimates are obtained by Monte Carlo sampling and the standard backpropagation procedure on minibatches of training samples. We apply the ADAM algorithm (Kingma and Ba, 2015) for optimizing our objectives."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016). A common motivation behind these models is that, with the expressive power of DNNs, the generative models can capture distributions for complex inputs. Additionally, if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data, which may allow us to reduce the sample complexity for learning for downstream tasks. These previous models have mostly focused on single-view data. Here we focus on the multi-view setting where multiple views of the data are present for feature extraction but only one view is available at test time (in downstream tasks).\nSome recent work has explored deep generative models for (semi-)supervised learning. Kingma et al. (2014) built a generative model based on variational autoencoders (VAEs) for semi-supervised classification, where the authors model the input distribution with two set of latent variables: the class label (if it is missing) and another set that models the intra-class variabilities (styles). Sohn\net al. (2015) proposed a conditional generative model for structured output prediction, where the authors explicitly model the uncertainty in the input/output using Gaussian latent variables. While there are two set of observations (input and output labels) in these work, their graphical models are different from that of VCCA.\nOur work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014). We note that these are undirected graphical models for which both inference and learning are difficult, and one typically resorts to carefully designed variational approximation and Gibbs sampling procedures for training such models. In contrast, our models only require sampling from simple, standard distributions (such as Gaussians), and all parameters can be learned end-to-end by standard stochastic gradient methods. Therefore, our models are more scalable than the previous multi-view probabilistic models.\nOn the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013). Our methods differ from previous work in parameterizing the probability distributions using DNNs. This makes the model more powerful, while still having tractable objectives and efficient end-to-end training using the local reparameterization technique. We note that, unlike earlier work on probabilistic models of linear CCA (Bach and Jordan, 2005), VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA. However, we retain the terminology in order to clarify the connection with earlier work on probabilistic models for CCA, which we are extending with DNN models for the observations and for the variational posterior distribution approximation."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS",
      "text" : "In this section, we compare different multi-view representation learning algorithms on three tasks involving several domains: image-image, speech-articulation, and image-text. The algorithms we choose to compare below are closely related to the proposed model or have been shown to have strong empirical performance under similar settings.\n• Linear CCA: its probabilistic interpretation motivates this work.\n• Deep CCA (DCCA) (Andrew et al., 2013): see its objective in (1).\n• Deep canonically correlated autoencoders (DCCAE) (Wang et al., 2015b): combination of the DCCA objective and the reconstruction errors of each view.\n• Multi-view autoencoder (MVAE) (Ngiam et al., 2011): see its objective in (10).\n• Multi-view contrastive loss (Hermann and Blunsom, 2014): based on the intuition that the distance between embeddings of paired examples x+ and y+ should be smaller than the distance between embeddings of x+ and an unmatched negative example y− by a margin:\nmin f,g Lcontrast :=\n1\nN N∑ i max ( 0, m+ dis ( f(x+i ), g(y + i ) ) − dis ( f(x+i ), g(y − i ) )) ,\nwhere y−i is a randomly sampled view 2 example, and m is a margin hyperparameter. We use the cosine distance dis (a,b) = 1− 〈\na ‖a‖ , b ‖b‖\n〉 ."
    }, {
      "heading" : "4.1 NOISY MNIST DATASET",
      "text" : "We first demonstrate our algorithms on the noisy MNIST dataset used by Wang et al. (2015b). The dataset is generated using the MNIST dataset (LeCun et al., 1998), which consists of 28 × 28 grayscale digit images, with 60K/10K images for training/testing. We first linearly rescale the pixel values to the range [0, 1]. Then, we randomly rotate the images at angles uniformly sampled from [−π/4, π/4] and the resulting images are used as view 1 inputs. For each view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, add independent random noise uniformly sampled from [0, 1] to each pixel, and truncate the pixel final values to [0, 1] to obtain the corresponding view 2 sample. Selection of input images are given in Figure 3 (left). The original\ntraining set is further split into training/tuning sets of size 50K/10K. The data generation process ensures that the digit identity is the only common variable underlying both views.\nTo evaluate the amount of class information extracted by different methods, after unsupervised learning of latent representations, we reveal the labels and train a linear SVM on the projected view 1 training data (using the one-versus-all scheme), and use it to classify the projected test set. This experiment simulates the typical usage of multi-view learning methods, which is to extract useful representations for downstream discriminative tasks.\nNote that this synthetic dataset perfectly satisfies the multi-view assumption that the two views are independent given the class label, so the latent representation should contain precisely the class information. This is indeed achieved by CCA-based and contrastive loss-based multi-view approaches. In Figure 3 (right), we show 2D t-SNE (van der Maaten and Hinton, 2008) visualizations of the original view 1 inputs and view 1 projections by various deep multi-view methods.\nWe use DNNs with 3 hidden layers of 1024 rectified linear units (ReLUs, Nair and Hinton, 2010) each to parameterize the distributions: qφ(z|x), pθ(x|z), pθ(y|z) in VCCA, and additionally qφ(hx|x) and qφ(hy|y) in VCCA-private. The capacities of these networks are the same as those of their counterparts in DCCA and DCCAE from Wang et al. (2015b). The reconstruction networks pθ(x|z) or pθ(x|z,hx) model each pixel of x as an independent Bernoulli variable and parameterize its mean (using a sigmoid activation); pθ(y|z) and pθ(y|z,hy) model y with diagonal Gaussians and parameterize the mean (using a sigmoid activation) and standard deviation for each pixel dimension. We tune the dimensionality dz over {10, 20, 30, 40, 50}, and fix dhx = dhy = 30 for VCCA-private. We select the hyperparameter combination that yields the best SVM classification accuracy on the projected tuning set, and report the corresponding accuracy on the projected test set.\nThe effect of dropout We add dropout (Srivastava et al., 2014) to all intermediate layers and the input layers and find it to be very useful in our models, with most of the gain coming from dropout applied to the samples of z, hx and hy . This is because dropout encourages each latent dimension to reconstruct the inputs well in the absence of other dimensions, and therefore avoids learning coadapted features. Intuitively, in VCCA-private dropout also helps to prevent the degenerate situation where the pathways x → hx → x and y → hy → y achieve good reconstruction while ignoring z (e.g., by setting it to a constant). We use the same dropout rate for all layers and tune it over {0, 0.1, 0.2, 0.3, 0.4}. We show the 2D t-SNE embeddings of the common variables z learned by VCCA and VCCA-private on test set in Figure 4. We observe that in general, VCCA/VCCA-private tend to separate the classes\nin the projection well; dropout significantly improves the performance of both VCCA and VCCAprivate, with the latter slightly outperforming the former. While such class separation can also be achieved by DCCA/contrastive loss as well, these methods can not naturally generate samples in the input space. On the other hand, such separation is not achieved by multi-view autoencoders.\nThe effect of private variables on reconstructions We show sample reconstructions (mean and standard deviation) by VCCA for the view 2 images from the test set in Figure 5 (columns 2 and 3). We observe that for each input, the mean reconstruction of yi by VCCA is a prototypical image of the same digit, regardless of the individual style in yi. This is to be expected, as yi contains an arbitrary image of the same digit as xi, and the variation in background noise in yi does not appear in xi and can not be reflected in qφ(z|x); thus the best way for pθ(y|z) to model yi is to output a prototypical image of that class to achieve on average small reconstruction error. On the other hand, since yi contains little rotation of the digits, this variation is suppressed to a large extent in qφ(z|x) (it is no longer the major variation in z as in the original inputs).\nWe show sample reconstructions by VCCA-private for the same set of view 2 images in Figure 5 (columns 4 and 5). With the help of private variables hy (as part of the input to pθ(y|z,hy)), the model does a much better job in reconstructing the styles of y. And by disentangling the private variables from the shared variables, qφ(z|x) achieves even better class separation than VCCA does.\nWe also note that the standard deviation of the reconstruction is low within the digit and high outside the digit, implying that pθ(y|z,hy) is able to separate the background noise from the digit image.\nDisentanglement of private/shared variables In Figure 6 (in Appendix) we provide the 2D tSNE embeddings of the shared variables z (top row) and the private variables hx (bottom row) learned by VCCA-private. In the embedding of hx, digits with different identities but the same rotation are mapped close together, and the rotation varies smoothly from left to right, confirming that the private variables contain little class information but mainly style information.\nFinally, we give the test error rates of linear SVMs applied to the features learned with different models in Table 1. VCCA-private is comparable in performance to the best previous approach (DCCAE), while having the advantage that it can also generate."
    }, {
      "heading" : "4.2 XRMB SPEECH-ARTICULATION DATASET",
      "text" : "We now consider the task of learning acoustic features for speech recognition. We use data from the Wisconsin X-ray microbeam (XRMB) corpus (Westbury, 1994), which contains simultaneously recorded speech and articulatory measurements from 47 American English speakers. We follow the setup of Wang et al. (2015a,b) and use the learned features for speaker-independent phonetic recognition.2 The two input views are standard 39D acoustic features (13 mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and 16D articulatory features (horizontal/vertical displacement of 8 pellets attached to several parts of the vocal tract), each then concatenated over a 7-frame window around each frame to incorporate context. The speakers are split into disjoint sets of 35/8/2/2 speakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are fixed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tuning on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker, and remove the mean of the acoustic measurements for each utterance. All learned feature types are used in a “tandem” speech recognizer (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions.\nEach algorithm uses up to 3 ReLU hidden layers, each of 1500 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models as the inputs are real-valued. In contrast to the MNIST experiments, we do not learn the standard deviations of each output dimension on training data, as this leads to poor downstream task performance. Instead, we use isotropic covariances for each view, and tune the standard deviations by grid search. The best model uses a smaller standard deviation (0.1) for the view 2 than for view 1 (1.0), effectively putting more emphasis on the reconstruction of articulatory measurements. Our best performing VCCA model uses dz = 70, while the best performing VCCA-private model uses dz = 70 and dhx = dhy = 10.\n2As in Wang and Livescu (2016), we use the Kaldi toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models. Our results do not match Wang et al. (2015a,b) (who instead used the HTK toolkit (Young et al., 1999)) for the same types of features, but the relative results are consistent.\nThe mean phone error rates (PER) over 6 folds obtained by different algorithms are given in Table 1. Our methods achieve competitive performance in comparison to previous deep multi-view methods."
    }, {
      "heading" : "4.3 MIR-FLICKR DATASET",
      "text" : "Finally, we consider the task of learning cross-modality features for topic classification on the MIRFlickr database (Huiskes and Lew, 2008). The Flickr database contains 1 million images accompanied by user tags, among which 25000 images are labeled with 38 topic classes (each image may be categorized as multiple topics). We use the same image and text features as in previous work (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags.\nFollowing the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al. (2014). For each algorithm, we select the model which achieves the highest mean average precision (mAP) on the validation set, and report its performance on the test set.\nEach algorithm uses up to 4 ReLU hidden layers, each of 1024 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models with isotropic covariance for image features, with standard deviation tuned by grid search, and a Bernoulli model for text features. In this experiment, we also found it helpful to tune an additional trade-off parameter for the text-view likelihood (cross-entropy); the best VCCA/VCCA-private models prefer a large trade-off parameter of the level 104, emphasizing the reconstruction of the sparse text-view inputs. Our best performing VCCA model uses dz = 1024, while the best performing VCCA-private model uses dz = 1024 and dhx = dhy = 16.\nAs shown in Table 1, VCCA/VCCA-private achieve significantly higher mAPs than other methods considered here. Being much easier to train, the performance of our methods are competitive with the previous state-of-the-art mAP result of 0.607 achieved by the multi-view RBMs of Sohn et al. (2014) under the same setting."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We have proposed variational canonical correlation analysis (VCCA), a deep generative method for multi-view representation learning. Our method embodies a natural idea for multi-view learning: the multiple views can be generated from a small set of shared latent variables. VCCA is parameterized by DNNs and can be trained efficiently by backpropagation, and is therefore scalable. We have also shown that, by modeling the private variables that are specific to each view, the VCCA-private variant can disentangle shared/private variables and provide higher-quality reconstructions.\nIn the future, we will explore other prior distributions such as mixtures of Gaussians or discrete random variables, which may enforce clustering in the latent space and in turn work better for discriminative tasks. We will also explore other observation models, including replacing the autoencoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016). Another direction is to explicitly incorporate the structure of the inputs, such as the sequence structure of speech and text and the spatial structure of images.\nACKOWLEDGEMENTS\nThis research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. This research used GPUs donated by NVIDIA Corporation.\n3As in Sohn et al. (2014), we exclude about 250000 samples which contain fewer than two tags."
    }, {
      "heading" : "A ADDITIONAL T-SNE VISUALIZATION OF NOISY MNIST",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "A kernel method for canonical correlation analysis",
      "author" : [ "S. Akaho" ],
      "venue" : "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001),",
      "citeRegEx" : "Akaho.,? \\Q2001\\E",
      "shortCiteRegEx" : "Akaho.",
      "year" : 2001
    }, {
      "title" : "GSNs: Generative stochastic networks",
      "author" : [ "G. Alain", "Y. Bengio", "L. Yao", "J. Yosinski", "E. Thibodeau-Laufer", "S. Zhang", "P. Vincent" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Alain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alain et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep canonical correlation analysis",
      "author" : [ "G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Andrew et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Andrew et al\\.",
      "year" : 2013
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bach and Jordan.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan.",
      "year" : 2002
    }, {
      "title" : "A probabilistic interpretation of canonical correlation analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "Technical Report 688,",
      "citeRegEx" : "Bach and Jordan.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bach and Jordan.",
      "year" : 2005
    }, {
      "title" : "Image classification using random forests and ferns",
      "author" : [ "A. Bosch", "A. Zisserman", "X. Munoz" ],
      "venue" : "IEEE 11th International Conference on Computer Vision,",
      "citeRegEx" : "Bosch et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bosch et al\\.",
      "year" : 2007
    }, {
      "title" : "Importance weighted autoencoders",
      "author" : [ "Y. Burda", "R. Grosse", "R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Burda et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Multilingual distributed representations without word alignment",
      "author" : [ "K.M. Hermann", "P. Blunsom" ],
      "venue" : "In Int. Conf. Learning Representations,",
      "citeRegEx" : "Hermann and Blunsom.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hermann and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Tandem connectionist feature extraction for conventional HMM systems",
      "author" : [ "H. Hermansky", "D.P.W. Ellis", "S. Sharma" ],
      "venue" : "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,",
      "citeRegEx" : "Hermansky et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hermansky et al\\.",
      "year" : 2000
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "H. Hotelling" ],
      "venue" : "Biometrika, 28(3/4):321–377,",
      "citeRegEx" : "Hotelling.,? \\Q1936\\E",
      "shortCiteRegEx" : "Hotelling.",
      "year" : 1936
    }, {
      "title" : "The mir flickr retrieval evaluation",
      "author" : [ "M.J. Huiskes", "M.S. Lew" ],
      "venue" : "In Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval,",
      "citeRegEx" : "Huiskes and Lew.,? \\Q2008\\E",
      "shortCiteRegEx" : "Huiskes and Lew.",
      "year" : 2008
    }, {
      "title" : "Factorized latent spaces with structured sparsity",
      "author" : [ "Y. Jia", "M. Salzmann", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2010
    }, {
      "title" : "ADAM: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "In Int. Conf. Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "[stat.ML],",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Bayesian canonical correlation analysis",
      "author" : [ "A. Klami", "S. Virtanen", "S. Kaski" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Klami et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Klami et al\\.",
      "year" : 2013
    }, {
      "title" : "Kernel and nonlinear canonical correlation analysis",
      "author" : [ "P.L. Lai", "C. Fyfe" ],
      "venue" : "Int. J. Neural Syst.,",
      "citeRegEx" : "Lai and Fyfe.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lai and Fyfe.",
      "year" : 2000
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proc. IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Deep multilingual correlation for improved word embeddings. In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT",
      "author" : [ "A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial autoencoders",
      "author" : [ "A. Makhzani", "J. Shlens", "N. Jaitly", "I. Goodfellow" ],
      "venue" : "In Int. Conf. Learning Representations,",
      "citeRegEx" : "Makhzani et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Makhzani et al\\.",
      "year" : 2016
    }, {
      "title" : "Color and texture descriptors",
      "author" : [ "B.S. Manjunath", "J.-R. Ohm", "V.V. Vasudevan", "A. Yamada" ],
      "venue" : "IEEE Transactions on circuits and systems for video technology,",
      "citeRegEx" : "Manjunath et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Manjunath et al\\.",
      "year" : 2001
    }, {
      "title" : "Nonlinear feature extraction using generalized canonical correlation analysis",
      "author" : [ "T. Melzer", "M. Reiter", "H. Bischof" ],
      "venue" : "In Int. Conf. Artificial Neural Networks,",
      "citeRegEx" : "Melzer et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Melzer et al\\.",
      "year" : 2001
    }, {
      "title" : "Shared kernel information embedding for discriminative inference",
      "author" : [ "R. Memisevic", "L. Sigal", "D.J. Fleet" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Memisevic et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Memisevic et al\\.",
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted Boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Ng" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Ngiam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam et al\\.",
      "year" : 2011
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "Oliva and Torralba.,? \\Q2001\\E",
      "shortCiteRegEx" : "Oliva and Torralba.",
      "year" : 2001
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely" ],
      "venue" : "In IEEE Workshop on Automatic Speech Recognition and Understanding,",
      "citeRegEx" : "Povey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Factorized orthogonal latent spaces",
      "author" : [ "M. Salzmann", "C.H. Ek", "R. Urtasun", "T. Darrell" ],
      "venue" : "In Int. Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Salzmann et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Salzmann et al\\.",
      "year" : 2010
    }, {
      "title" : "Improved multimodal deep learning with variation of information",
      "author" : [ "K. Sohn", "W. Shang", "H. Lee" ],
      "venue" : null,
      "citeRegEx" : "Sohn et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "K. Sohn", "H. Lee", "X. Yan" ],
      "venue" : null,
      "citeRegEx" : "Sohn et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal learning with deep boltzmann machines",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava and Salakhutdinov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava and Salakhutdinov.",
      "year" : 2014
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "L.J.P. van der Maaten", "G.E. Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten and Hinton.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Bayesian CCA via group sparsity",
      "author" : [ "S. Virtanen", "A. Klami", "S. Kaski" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Virtanen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Virtanen et al\\.",
      "year" : 2011
    }, {
      "title" : "Variational Bayesian approach to canonical correlation analysis",
      "author" : [ "C. Wang" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "Wang.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2007
    }, {
      "title" : "Large-scale approximate kernel canonical correlation analysis",
      "author" : [ "W. Wang", "K. Livescu" ],
      "venue" : "In Int. Conf. Learning Representations,",
      "citeRegEx" : "Wang and Livescu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Livescu.",
      "year" : 2016
    }, {
      "title" : "Unsupervised learning of acoustic features via deep canonical correlation analysis",
      "author" : [ "W. Wang", "R. Arora", "K. Livescu", "J. Bilmes" ],
      "venue" : "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "On deep multi-view representation learning",
      "author" : [ "W. Wang", "R. Arora", "K. Livescu", "J. Bilmes" ],
      "venue" : "In Int. Conf. Machine Learning,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "X-Ray Microbeam Speech Production",
      "author" : [ "J.R. Westbury" ],
      "venue" : "Database User’s Handbook Version",
      "citeRegEx" : "Westbury.,? \\Q1994\\E",
      "shortCiteRegEx" : "Westbury.",
      "year" : 1994
    }, {
      "title" : "Deep correlation for matching images and text",
      "author" : [ "F. Yan", "K. Mikolajczyk" ],
      "venue" : "In IEEE Computer Society Conf. Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Yan and Mikolajczyk.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yan and Mikolajczyk.",
      "year" : 2015
    }, {
      "title" : "The HTK book version 2.2",
      "author" : [ "S.J. Young", "D. Kernshaw", "J. Odell", "D. Ollason", "V. Valtchev", "P. Woodland" ],
      "venue" : "Technical report, Entropic,",
      "citeRegEx" : "Young et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "We present deep variational canonical correlation analysis (VCCA), a deep multiview learning model that extends the latent variable model interpretation of linear CCA (Bach and Jordan, 2005) to nonlinear observation models parameterized by deep neural networks (DNNs).",
      "startOffset" : 167,
      "endOffset" : 190
    }, {
      "referenceID" : 26,
      "context" : "Interestingly, the resulting model resembles that of multiview autoencoders (Ngiam et al., 2011), with the key distinction of an additional sampling procedure at the bottleneck layer.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 232
    }, {
      "referenceID" : 23,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 232
    }, {
      "referenceID" : 3,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 232
    }, {
      "referenceID" : 2,
      "context" : ", 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al., 2013; Wang et al., 2015b).",
      "startOffset" : 75,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1 (left).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "For example, in deep CCA (DCCA, (Andrew et al., 2013)), to extend CCA to nonlinear mappings with greater representation power, one extracts nonlinear features from the original inputs of each view using two DNNs, f for x and g for y, so that the canonical correlation of the DNN outputs (measured by a linear CCA with projection matrices U and V) is maximized.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al., 2013; Wang et al., 2015b). CCA projects two random vectors x ∈ Rx and y ∈ Ry into a lowerdimensional subspace so that the projections are maximally correlated. There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1 (left). Assume that x and y are linear functions of some lower-dimensional random variable z ∈ Rz , where dz ≤ min(dx, dy). When the prior distribution of the latent variable p(z) and the conditional distributions p(x|z) and p(y|z) are Gaussian, Bach and Jordan (2005) showed that E[z|x] (resp.",
      "startOffset" : 176,
      "endOffset" : 843
    }, {
      "referenceID" : 4,
      "context" : "Figure 1: Left: Probabilistic interpretation of CCA (Bach and Jordan, 2005).",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "DCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015).",
      "startOffset" : 110,
      "endOffset" : 176
    }, {
      "referenceID" : 42,
      "context" : "DCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015).",
      "startOffset" : 110,
      "endOffset" : 176
    }, {
      "referenceID" : 26,
      "context" : "Interestingly, VCCA is related to multi-view autoencoders (Ngiam et al., 2011), with the key distinctions of additional regularization on the posterior distribution and the sampling procedure at the bottleneck layer.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : ", 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015). However, a disadvantage of DCCA is that it directly looks for DNNs that can map inputs into the low-dimensional space, without a model for generating samples from the latent space. Although Wang et al. (2015b)’s deep canonically correlated autoencoders (DCCAE) model optimizes the combination of the autoencoder objective (reconstruction errors) and the canonical correlation objective, the authors found that in practice, the canonical correlation term tends to dominate the reconstruction error terms in the DCCAE objective when tuning performance for a downstream task (especially when the inputs are noisy), and as a result the inputs are not reconstructed well.",
      "startOffset" : 11,
      "endOffset" : 266
    }, {
      "referenceID" : 4,
      "context" : "The probabilistic latent variable model of CCA (Bach and Jordan, 2005) defines the following joint distribution over the random variables (x,y):",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "Inspired by Kingma and Welling (2014)’s work on variational autoencoders (VAE), we approximate pθ(z|x) with the conditional density qφ(z|x;φz), where φz is the collection of parameters of another DNN.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "We apply the ADAM algorithm (Kingma and Ba, 2015) for optimizing our objectives.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 29,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 7,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 21,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 6,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 1,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 1,
      "context" : ", 2016; Alain et al., 2016). A common motivation behind these models is that, with the expressive power of DNNs, the generative models can capture distributions for complex inputs. Additionally, if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data, which may allow us to reduce the sample complexity for learning for downstream tasks. These previous models have mostly focused on single-view data. Here we focus on the multi-view setting where multiple views of the data are present for feature extraction but only one view is available at test time (in downstream tasks). Some recent work has explored deep generative models for (semi-)supervised learning. Kingma et al. (2014) built a generative model based on variational autoencoders (VAEs) for semi-supervised classification, where the authors model the input distribution with two set of latent variables: the class label (if it is missing) and another set that models the intra-class variabilities (styles).",
      "startOffset" : 8,
      "endOffset" : 776
    }, {
      "referenceID" : 33,
      "context" : "Our work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 163
    }, {
      "referenceID" : 31,
      "context" : "Our work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 163
    }, {
      "referenceID" : 37,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 30,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 36,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 24,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 17,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 4,
      "context" : "We note that, unlike earlier work on probabilistic models of linear CCA (Bach and Jordan, 2005), VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA.",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "• Deep CCA (DCCA) (Andrew et al., 2013): see its objective in (1).",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 26,
      "context" : "• Multi-view autoencoder (MVAE) (Ngiam et al., 2011): see its objective in (10).",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "• Multi-view contrastive loss (Hermann and Blunsom, 2014): based on the intuition that the distance between embeddings of paired examples x and y should be smaller than the distance between embeddings of x and an unmatched negative example y− by a margin:",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "The dataset is generated using the MNIST dataset (LeCun et al., 1998), which consists of 28 × 28 grayscale digit images, with 60K/10K images for training/testing.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 36,
      "context" : "We first demonstrate our algorithms on the noisy MNIST dataset used by Wang et al. (2015b). The dataset is generated using the MNIST dataset (LeCun et al.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 25,
      "context" : "We use DNNs with 3 hidden layers of 1024 rectified linear units (ReLUs, Nair and Hinton, 2010) each to parameterize the distributions: qφ(z|x), pθ(x|z), pθ(y|z) in VCCA, and additionally qφ(hx|x) and qφ(hy|y) in VCCA-private. The capacities of these networks are the same as those of their counterparts in DCCA and DCCAE from Wang et al. (2015b). The reconstruction networks pθ(x|z) or pθ(x|z,hx) model each pixel of x as an independent Bernoulli variable and parameterize its mean (using a sigmoid activation); pθ(y|z) and pθ(y|z,hy) model y with diagonal Gaussians and parameterize the mean (using a sigmoid activation) and standard deviation for each pixel dimension.",
      "startOffset" : 72,
      "endOffset" : 346
    }, {
      "referenceID" : 34,
      "context" : "The effect of dropout We add dropout (Srivastava et al., 2014) to all intermediate layers and the input layers and find it to be very useful in our models, with most of the gain coming from dropout applied to the samples of z, hx and hy .",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 41,
      "context" : "We use data from the Wisconsin X-ray microbeam (XRMB) corpus (Westbury, 1994), which contains simultaneously recorded speech and articulatory measurements from 47 American English speakers.",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "All learned feature types are used in a “tandem” speech recognizer (Hermansky et al., 2000), i.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "As in Wang and Livescu (2016), we use the Kaldi toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 43,
      "context" : "(2015a,b) (who instead used the HTK toolkit (Young et al., 1999)) for the same types of features, but the relative results are consistent.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "All learned feature types are used in a “tandem” speech recognizer (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. Each algorithm uses up to 3 ReLU hidden layers, each of 1500 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models as the inputs are real-valued. In contrast to the MNIST experiments, we do not learn the standard deviations of each output dimension on training data, as this leads to poor downstream task performance. Instead, we use isotropic covariances for each view, and tune the standard deviations by grid search. The best model uses a smaller standard deviation (0.1) for the view 2 than for view 1 (1.0), effectively putting more emphasis on the reconstruction of articulatory measurements. Our best performing VCCA model uses dz = 70, while the best performing VCCA-private model uses dz = 70 and dhx = dhy = 10. As in Wang and Livescu (2016), we use the Kaldi toolkit (Povey et al.",
      "startOffset" : 68,
      "endOffset" : 1072
    }, {
      "referenceID" : 12,
      "context" : "Finally, we consider the task of learning cross-modality features for topic classification on the MIRFlickr database (Huiskes and Lew, 2008).",
      "startOffset" : 117,
      "endOffset" : 140
    }, {
      "referenceID" : 33,
      "context" : "We use the same image and text features as in previous work (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al.",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 31,
      "context" : "We use the same image and text features as in previous work (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al.",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : ", 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al.",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 27,
      "context" : ", 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : ", 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : ", 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags. Following the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al.",
      "startOffset" : 114,
      "endOffset" : 344
    }, {
      "referenceID" : 5,
      "context" : ", 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags. Following the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al.",
      "startOffset" : 114,
      "endOffset" : 701
    }, {
      "referenceID" : 5,
      "context" : ", 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags. Following the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al. (2014). For each algorithm, we select the model which achieves the highest mean average precision (mAP) on the validation set, and report its performance on the test set.",
      "startOffset" : 114,
      "endOffset" : 721
    }, {
      "referenceID" : 5,
      "context" : ", 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags. Following the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al. (2014). For each algorithm, we select the model which achieves the highest mean average precision (mAP) on the validation set, and report its performance on the test set. Each algorithm uses up to 4 ReLU hidden layers, each of 1024 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models with isotropic covariance for image features, with standard deviation tuned by grid search, and a Bernoulli model for text features. In this experiment, we also found it helpful to tune an additional trade-off parameter for the text-view likelihood (cross-entropy); the best VCCA/VCCA-private models prefer a large trade-off parameter of the level 10, emphasizing the reconstruction of the sparse text-view inputs. Our best performing VCCA model uses dz = 1024, while the best performing VCCA-private model uses dz = 1024 and dhx = dhy = 16. As shown in Table 1, VCCA/VCCA-private achieve significantly higher mAPs than other methods considered here. Being much easier to train, the performance of our methods are competitive with the previous state-of-the-art mAP result of 0.607 achieved by the multi-view RBMs of Sohn et al. (2014) under the same setting.",
      "startOffset" : 114,
      "endOffset" : 1894
    }, {
      "referenceID" : 7,
      "context" : "We will also explore other observation models, including replacing the autoencoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "We will also explore other observation models, including replacing the autoencoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 194
    }, {
      "referenceID" : 31,
      "context" : "As in Sohn et al. (2014), we exclude about 250000 samples which contain fewer than two tags.",
      "startOffset" : 6,
      "endOffset" : 25
    } ],
    "year" : 2016,
    "abstractText" : "We present deep variational canonical correlation analysis (VCCA), a deep multiview learning model that extends the latent variable model interpretation of linear CCA (Bach and Jordan, 2005) to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multiview autoencoders (Ngiam et al., 2011), with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.",
    "creator" : "LaTeX with hyperref package"
  }
}