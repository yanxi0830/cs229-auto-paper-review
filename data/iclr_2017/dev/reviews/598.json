{"conference": "ICLR 2017 conference submission", "title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System", "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler. We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "A promising start, but a lot more context is needed and the current results are not so competitive", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\n\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.\n\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "citations missing", "IS_META_REVIEW": false, "comments": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "09 Dec 2016", "TITLE": "Publication at NIPS/end-to-end workshop", "IS_META_REVIEW": false, "comments": "A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Normalization in beam search", "IS_META_REVIEW": false, "comments": "When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Modified CTC/blank replacement", "IS_META_REVIEW": false, "comments": "Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Notation", "IS_META_REVIEW": false, "comments": "It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "logadd?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "How is this model different from conventional HMM based model?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "29 Nov 2016", "TITLE": "Is the ASG criterion really new?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "A promising start, but a lot more context is needed and the current results are not so competitive", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\n\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.\n\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "citations missing", "IS_META_REVIEW": false, "comments": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "09 Dec 2016", "TITLE": "Publication at NIPS/end-to-end workshop", "IS_META_REVIEW": false, "comments": "A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Normalization in beam search", "IS_META_REVIEW": false, "comments": "When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Modified CTC/blank replacement", "IS_META_REVIEW": false, "comments": "Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Notation", "IS_META_REVIEW": false, "comments": "It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "logadd?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "How is this model different from conventional HMM based model?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "29 Nov 2016", "TITLE": "Is the ASG criterion really new?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve", "accepted": false, "id": "598"}