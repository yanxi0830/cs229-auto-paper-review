{"conference": "ICLR 2017 conference submission", "title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "We uploaded a new version of the paper based on the peer review feedback.\n\n* Clarified that an unconstrained GAN is not a suitable baseline.\n* Added additional references pointed out by reviewer 2.\n* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.\n* Various modifications to improve clarity.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "05 Jan 2017", "TITLE": "General Rebuttal", "IS_META_REVIEW": false, "comments": "\nThank you very much for your reviews. Your feedback helped to improve the\nmanuscript significantly, and we are preparing a revised version of the\nmanuscript with changes outlined either below or in our responses to each\nreviewer. Multiple valid points of criticism were raised during the review\nprocess and have already been worked into the current version of the document.\nFor example, we included hand-designed augmentations for comparison with the\nlearned ones.\n\nHowever, in two of the three reviews there seems to be a major misunderstanding\nthat we would like to clarify here. Since this relates to the central finding of\nour paper, we would like to provide a detailed response to this point. We hope\nthat, in the light of this fact, the reviewer\u2019s rating of our contribution\u2019s\nimportance and novelty will be reconsidered.\n\n> Reviewer 2: \u201cA critical missing baseline is a comparison against a generic GAN.\n> Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also,\n> it would be worth seeing the result when one combines generated and real images\n> for the final task.\u201d\n\n> Reviewer 3: \u201c [...] the proposed method is more model driven that previous GAN\n> models. But does it pay off? how would a traditional GAN approach perform? [...]\n> The answers of the authors only partially addresses the point. The key proposal\n> of the submission seems parameterised modules that can be trained to match the\n> real data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs. The benefit of introducing a stronger model is unclear.\u201d\n\nThe main point of critique here is that a comparison with a generic GAN\n(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN\nand RenderGAN) share the same task domain, which is incorrect. The task we\naddress is generating _labeled_ data. We emphasize that we do not refer to the\nbinary class label but rather to higher dimensional labels. In our example\nscenario, this corresponds to images of bee markers and their respective bit\nconfiguration (its ID) and rotations in 3D space. A generic GAN cannot generate\nlabels, it learns to generate realistic images _without_ labels. Ultimately, we\nwant to train a convnet (\u2018decoder network\u2019) in a supervised setting to map an\nimage to its respective labels. Thus, we need labeled samples and hence,\na conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN\nsamples from the joint distribution p(l, x) of labels l and data x whereas\na conventional GAN can only sample from the data distribution p(x).\n\nThere are two alternative approaches to our RenderGAN, one being a conventional\n3-dimensional rendering pipeline that can be used to generate images of bee\nmarkers with known ID and spatial orientation. Secondly, one could train the\ndecoder network with manually labeled data. Both approaches have been\nimplemented and tested against the RenderGAN and do not perform satisfyingly. To\nimprove both alternatives\u2019 performance one would need to either tune the\nrendering pipeline to match the details of the real world imaging process, or\nlabel more data manually. Both measures are time-consuming and do not generalize\nwell when changing parts of the imaging process (lighting, cameras, compression,\netc.) or the marker design.\n\nOur approach is to extend a generic GAN by adding several network modules, the\nfirst being the network equivalent of a simple 3D model. Secondly, we learn\na number of parameterised augmentation functions. We would like to point out\nthat this approach was _not_ chosen to improve the generative capabilities of\nthe network but to constrain it in such a way that the image produced by the GAN\nis correct with respect to the labels fed into the network. In our use case,\neach image produced by the GAN has to preserve the given bit pattern and\nrotation in space provided by the 3D model for the labels to remain valid. This\npoint was already addressed in our paper and the pre-review questions:\n\n> Paper Introduction: \u201c[...] We constrain the augmentation of the images such that\n> the high-level information represented by the 3D model is preserved. The\n> RenderGAN framework allows us to generate images of which the labels are known\n> from the 3D model, and that also look strikingly real due to the GAN framework.\n> The training procedure of the RenderGAN framework does not require any labels.\n> We can generate high-quality, labeled data with a simple 3D model and a large\n> amount of unlabeled data.\u201d\n\n> Our reply to Reviewer 3: \u201c[...] The payoff is that we can generate labeled data\n> with only a simple 3D model and unlabeled data. You are right. A DCGAN\n> architecture can model all mentioned effects, even affine transformations. We\n> trained a DCGAN on the data, and the quality of the synthesized images is\n> similar. However, no labels can be collected in the conventional GAN framework.\n> [...]\u201d\n\nAll reviewers question the necessity of the constraints we introduced. One of\nour early approaches was to add an offset to the 3D model, i.e. x = t + g(t)\nwhere x is the synthesized image, t is an image from the 3D model, and g an\nunconstrained generator. However, in our experiments, the generator learned to\nsynthesize realistic images but ignored the given template t completely. Thus,\nno valid labels of the synthetic images could be collected. Since a decoder\nnetwork cannot be trained without labels, this approach cannot be used as\na baseline. We will revise our paper to clarify that an unconstrained GAN is not\na suitable baseline for our task.\n", "OTHER_KEYS": "Leon Sixt"}, {"TITLE": "The proposed model has potential merits, but the paper is missing a critical baseline in the evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "RenderGAN: Generating Realistic Labeled Data", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "Update paper", "IS_META_REVIEW": false, "comments": "We updated our paper based on the feedback from the pre-review questions.  We\nincluded handmade augmentation in the evaluation.  We also retrained the DCNN on\nthe real data. Thanks for the feedback.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "We uploaded a new version of the paper based on the peer review feedback.\n\n* Clarified that an unconstrained GAN is not a suitable baseline.\n* Added additional references pointed out by reviewer 2.\n* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.\n* Various modifications to improve clarity.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "05 Jan 2017", "TITLE": "General Rebuttal", "IS_META_REVIEW": false, "comments": "\nThank you very much for your reviews. Your feedback helped to improve the\nmanuscript significantly, and we are preparing a revised version of the\nmanuscript with changes outlined either below or in our responses to each\nreviewer. Multiple valid points of criticism were raised during the review\nprocess and have already been worked into the current version of the document.\nFor example, we included hand-designed augmentations for comparison with the\nlearned ones.\n\nHowever, in two of the three reviews there seems to be a major misunderstanding\nthat we would like to clarify here. Since this relates to the central finding of\nour paper, we would like to provide a detailed response to this point. We hope\nthat, in the light of this fact, the reviewer\u2019s rating of our contribution\u2019s\nimportance and novelty will be reconsidered.\n\n> Reviewer 2: \u201cA critical missing baseline is a comparison against a generic GAN.\n> Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also,\n> it would be worth seeing the result when one combines generated and real images\n> for the final task.\u201d\n\n> Reviewer 3: \u201c [...] the proposed method is more model driven that previous GAN\n> models. But does it pay off? how would a traditional GAN approach perform? [...]\n> The answers of the authors only partially addresses the point. The key proposal\n> of the submission seems parameterised modules that can be trained to match the\n> real data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs. The benefit of introducing a stronger model is unclear.\u201d\n\nThe main point of critique here is that a comparison with a generic GAN\n(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN\nand RenderGAN) share the same task domain, which is incorrect. The task we\naddress is generating _labeled_ data. We emphasize that we do not refer to the\nbinary class label but rather to higher dimensional labels. In our example\nscenario, this corresponds to images of bee markers and their respective bit\nconfiguration (its ID) and rotations in 3D space. A generic GAN cannot generate\nlabels, it learns to generate realistic images _without_ labels. Ultimately, we\nwant to train a convnet (\u2018decoder network\u2019) in a supervised setting to map an\nimage to its respective labels. Thus, we need labeled samples and hence,\na conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN\nsamples from the joint distribution p(l, x) of labels l and data x whereas\na conventional GAN can only sample from the data distribution p(x).\n\nThere are two alternative approaches to our RenderGAN, one being a conventional\n3-dimensional rendering pipeline that can be used to generate images of bee\nmarkers with known ID and spatial orientation. Secondly, one could train the\ndecoder network with manually labeled data. Both approaches have been\nimplemented and tested against the RenderGAN and do not perform satisfyingly. To\nimprove both alternatives\u2019 performance one would need to either tune the\nrendering pipeline to match the details of the real world imaging process, or\nlabel more data manually. Both measures are time-consuming and do not generalize\nwell when changing parts of the imaging process (lighting, cameras, compression,\netc.) or the marker design.\n\nOur approach is to extend a generic GAN by adding several network modules, the\nfirst being the network equivalent of a simple 3D model. Secondly, we learn\na number of parameterised augmentation functions. We would like to point out\nthat this approach was _not_ chosen to improve the generative capabilities of\nthe network but to constrain it in such a way that the image produced by the GAN\nis correct with respect to the labels fed into the network. In our use case,\neach image produced by the GAN has to preserve the given bit pattern and\nrotation in space provided by the 3D model for the labels to remain valid. This\npoint was already addressed in our paper and the pre-review questions:\n\n> Paper Introduction: \u201c[...] We constrain the augmentation of the images such that\n> the high-level information represented by the 3D model is preserved. The\n> RenderGAN framework allows us to generate images of which the labels are known\n> from the 3D model, and that also look strikingly real due to the GAN framework.\n> The training procedure of the RenderGAN framework does not require any labels.\n> We can generate high-quality, labeled data with a simple 3D model and a large\n> amount of unlabeled data.\u201d\n\n> Our reply to Reviewer 3: \u201c[...] The payoff is that we can generate labeled data\n> with only a simple 3D model and unlabeled data. You are right. A DCGAN\n> architecture can model all mentioned effects, even affine transformations. We\n> trained a DCGAN on the data, and the quality of the synthesized images is\n> similar. However, no labels can be collected in the conventional GAN framework.\n> [...]\u201d\n\nAll reviewers question the necessity of the constraints we introduced. One of\nour early approaches was to add an offset to the 3D model, i.e. x = t + g(t)\nwhere x is the synthesized image, t is an image from the 3D model, and g an\nunconstrained generator. However, in our experiments, the generator learned to\nsynthesize realistic images but ignored the given template t completely. Thus,\nno valid labels of the synthetic images could be collected. Since a decoder\nnetwork cannot be trained without labels, this approach cannot be used as\na baseline. We will revise our paper to clarify that an unconstrained GAN is not\na suitable baseline for our task.\n", "OTHER_KEYS": "Leon Sixt"}, {"TITLE": "The proposed model has potential merits, but the paper is missing a critical baseline in the evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "RenderGAN: Generating Realistic Labeled Data", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "Update paper", "IS_META_REVIEW": false, "comments": "We updated our paper based on the feedback from the pre-review questions.  We\nincluded handmade augmentation in the evaluation.  We also retrained the DCNN on\nthe real data. Thanks for the feedback.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Leon Sixt, Benjamin Wild, Tim Landgraf", "accepted": false, "id": "537"}