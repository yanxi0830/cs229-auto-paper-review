{"conference": "ICLR 2017 conference submission", "title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Promising direction, but the paper needs more work", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice paper to read", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.\n\nMinor comments:\nPlease add citation to [1] or [2] for neural variational inference, and [2] for VAE. \nA typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d\n\nIn table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?\n\nIn table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?\n\nHow does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?\n\nIt may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).\n\nOverall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. \n\n[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914\n[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "VAE model for LDA. Interesting idea, but a incremental.  ", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Comparison to NVDM looks unfair", "OTHER_KEYS": "(anonymous)", "comments": "The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "05 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"DATE": "08 Nov 2016", "TITLE": "Perplexity", "IS_META_REVIEW": false, "comments": "The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?", "OTHER_KEYS": "Erik Holmer"}, {"IS_META_REVIEW": true, "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Promising direction, but the paper needs more work", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice paper to read", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.\n\nMinor comments:\nPlease add citation to [1] or [2] for neural variational inference, and [2] for VAE. \nA typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d\n\nIn table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?\n\nIn table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?\n\nHow does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?\n\nIt may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).\n\nOverall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. \n\n[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914\n[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "VAE model for LDA. Interesting idea, but a incremental.  ", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Comparison to NVDM looks unfair", "OTHER_KEYS": "(anonymous)", "comments": "The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "05 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"DATE": "08 Nov 2016", "TITLE": "Perplexity", "IS_META_REVIEW": false, "comments": "The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?", "OTHER_KEYS": "Erik Holmer"}], "authors": "Akash Srivastava, Charles Sutton", "accepted": true, "id": "366"}