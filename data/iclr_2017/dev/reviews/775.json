{"conference": "ICLR 2017 conference submission", "title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?", "abstract": "The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.\n \n The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the \"default\" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.\n \n But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.\n \n I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Jan 2017", "TITLE": "New Revision", "IS_META_REVIEW": false, "comments": "We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.\n\nWe look forward to receiving additional feedback from the reviewers.\nThank you,", "OTHER_KEYS": "Xue Bin Peng"}, {"DATE": "21 Dec 2016", "TITLE": "Reply to reviews", "IS_META_REVIEW": false, "comments": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n", "OTHER_KEYS": "Xue Bin Peng"}, {"TITLE": "Nice paper comparing control parameterisations in simulated locomotion domains.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper is straightforward, easy to read, and has clear results. \n\nSince all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?\n\nWould we get the same result if there was no reference-pose cost, only a locomotion cost?\n\nWould we get the same result if the task was to spin a top? My guess is no. \n\nThis work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.\n\nThe video is nice.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting topic, sound experiments, hard to draw conclusions.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Sensitivity to the manually specified parameters", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Influence of objective and reward functions?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.\n \n The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the \"default\" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.\n \n But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.\n \n I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Jan 2017", "TITLE": "New Revision", "IS_META_REVIEW": false, "comments": "We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.\n\nWe look forward to receiving additional feedback from the reviewers.\nThank you,", "OTHER_KEYS": "Xue Bin Peng"}, {"DATE": "21 Dec 2016", "TITLE": "Reply to reviews", "IS_META_REVIEW": false, "comments": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n", "OTHER_KEYS": "Xue Bin Peng"}, {"TITLE": "Nice paper comparing control parameterisations in simulated locomotion domains.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper is straightforward, easy to read, and has clear results. \n\nSince all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?\n\nWould we get the same result if there was no reference-pose cost, only a locomotion cost?\n\nWould we get the same result if the task was to spin a top? My guess is no. \n\nThis work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.\n\nThe video is nice.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting topic, sound experiments, hard to draw conclusions.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Sensitivity to the manually specified parameters", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Influence of objective and reward functions?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Xue Bin Peng, Michiel van de Panne", "accepted": false, "id": "775"}