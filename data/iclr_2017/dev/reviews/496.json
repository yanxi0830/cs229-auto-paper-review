{"conference": "ICLR 2017 conference submission", "title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n\nQuestion) Can you extend it to bidirectional RNN? \n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "well evaluated and written paper, novel flush operation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "data size of Penn Treebank and toolkit", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "05 Dec 2016", "TITLE": "HM-RNN", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n\nQuestion) Can you extend it to bidirectional RNN? \n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "well evaluated and written paper, novel flush operation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "data size of Penn Treebank and toolkit", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "05 Dec 2016", "TITLE": "HM-RNN", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Junyoung Chung, Sungjin Ahn, Yoshua Bengio", "accepted": true, "id": "496"}