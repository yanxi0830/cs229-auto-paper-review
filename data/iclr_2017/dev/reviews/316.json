{"conference": "ICLR 2017 conference submission", "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data", "abstract": "Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ''teachers'' for a ''student'' model.  The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy.  These properties hold even if an adversary can not only query the student but also inspect its internal workings.  Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Simple question related with privacy loss", "IS_META_REVIEW": false, "comments": "I think this paper has great impact.\n\nMy question is what is the \"auxiliary input\" in Definition 2.\n\nCould you explain this term in theoretical view and what is that in your paper?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "11 Jan 2017 (modified: 12 Jan 2017)", "TITLE": "Question for student GAN training", "IS_META_REVIEW": false, "comments": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "10 Jan 2017", "TITLE": "Attacker's Model and Goal?", "IS_META_REVIEW": false, "comments": "Hi,\n\nI have few questions about the paper.\n\n1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? \n\n2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?\n\n3- Talking about neural networks:\n- Do you think there is any attack method to recover an exact training data from the learning model?\n- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?\n\n4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?\n\n5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. \n- How can the model \"memorize\" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?\n\n6- How do you compare the performance of your method with adversarial training?\n\nThanks.", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice paper, strong accept", "comments": "This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good theory", "comments": "This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. \n\nThe theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.\n\nThe experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "A nice contribution to differentially-private deep learning", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Input of student model", "IS_META_REVIEW": false, "comments": "Thanks for interesting and well-organized papers. I have a question about teacher-student model. \n\nTeachers are trained on sensitive data, and students are trained on non-sensitive data.\nI wonder how students work on the outputs of teachers.\nSensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. \n\nPlease give me some more details. Thanks.  \n", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Clarifications", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "What are the challenges involved in your proposed future work?", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "theory and experiment", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "18 Nov 2016 (modified: 19 Nov 2016)", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Simple question related with privacy loss", "IS_META_REVIEW": false, "comments": "I think this paper has great impact.\n\nMy question is what is the \"auxiliary input\" in Definition 2.\n\nCould you explain this term in theoretical view and what is that in your paper?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "11 Jan 2017 (modified: 12 Jan 2017)", "TITLE": "Question for student GAN training", "IS_META_REVIEW": false, "comments": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "10 Jan 2017", "TITLE": "Attacker's Model and Goal?", "IS_META_REVIEW": false, "comments": "Hi,\n\nI have few questions about the paper.\n\n1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? \n\n2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?\n\n3- Talking about neural networks:\n- Do you think there is any attack method to recover an exact training data from the learning model?\n- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?\n\n4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?\n\n5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. \n- How can the model \"memorize\" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?\n\n6- How do you compare the performance of your method with adversarial training?\n\nThanks.", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice paper, strong accept", "comments": "This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good theory", "comments": "This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. \n\nThe theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.\n\nThe experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "A nice contribution to differentially-private deep learning", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Input of student model", "IS_META_REVIEW": false, "comments": "Thanks for interesting and well-organized papers. I have a question about teacher-student model. \n\nTeachers are trained on sensitive data, and students are trained on non-sensitive data.\nI wonder how students work on the outputs of teachers.\nSensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. \n\nPlease give me some more details. Thanks.  \n", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Clarifications", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "What are the challenges involved in your proposed future work?", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "theory and experiment", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "18 Nov 2016 (modified: 19 Nov 2016)", "CLARITY": 5}], "authors": "Nicolas Papernot, Mart\u00edn Abadi, \u00dalfar Erlingsson, Ian Goodfellow, Kunal Talwar", "accepted": true, "id": "316"}