{"conference": "ICLR 2017 conference submission", "title": "Deep Variational Canonical Correlation Analysis", "abstract": "We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ``common variables'' underlying both views, extract the ``private variables'' within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "updated response", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. \n\nThanks!", "OTHER_KEYS": "Weiran Wang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review: Deep Variational Canonical Correlation Analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "UPDATE: I have read the replies on this thread. My opinion has not changed.\n\nThe authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.\n\nSince the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).\n\nThe connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)\n\nThat said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of \"private variables\" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. \n\nThere are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. \n\n+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 21 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Linear VCCA", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "revision notes", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe just uploaded a slightly modified version. This version \n\n-- contains the updated results on Flickr and the proposed methods significantly outperform others\n-- reorganized the figures and tables to make the paper more compact\n\nThanks!\n", "OTHER_KEYS": "Weiran Wang"}, {"DATE": "03 Dec 2016", "TITLE": "VCCA and DCCAE", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Lower bound and using q(z | x) instead of q(z | x, y)", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "updated response", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. \n\nThanks!", "OTHER_KEYS": "Weiran Wang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review: Deep Variational Canonical Correlation Analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "UPDATE: I have read the replies on this thread. My opinion has not changed.\n\nThe authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.\n\nSince the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).\n\nThe connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)\n\nThat said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of \"private variables\" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. \n\nThere are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. \n\n+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 21 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Linear VCCA", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "revision notes", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe just uploaded a slightly modified version. This version \n\n-- contains the updated results on Flickr and the proposed methods significantly outperform others\n-- reorganized the figures and tables to make the paper more compact\n\nThanks!\n", "OTHER_KEYS": "Weiran Wang"}, {"DATE": "03 Dec 2016", "TITLE": "VCCA and DCCAE", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Lower bound and using q(z | x) instead of q(z | x, y)", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}], "authors": "Weiran Wang, Xinchen Yan, Honglak Lee, Karen Livescu", "accepted": false, "id": "734"}