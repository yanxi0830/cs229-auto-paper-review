{"conference": "ICLR 2017 conference submission", "title": "Deep Information Propagation", "abstract": "We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted. ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Dec 2016", "TITLE": "dropout", "IS_META_REVIEW": false, "comments": "Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?", "OTHER_KEYS": "Greg Yang"}, {"TITLE": "interesting analysis - empirical results could be clarified", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.\n\nMinor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nIn interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nComments on central claims:\nPrevious work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "23 Dec 2016", "TITLE": "-", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "An important and thorough contribution to the theoretical analysis of deep neural networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Excellent analysis ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IS_META_REVIEW": true, "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted. ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Dec 2016", "TITLE": "dropout", "IS_META_REVIEW": false, "comments": "Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?", "OTHER_KEYS": "Greg Yang"}, {"TITLE": "interesting analysis - empirical results could be clarified", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.\n\nMinor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nIn interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nComments on central claims:\nPrevious work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "23 Dec 2016", "TITLE": "-", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "An important and thorough contribution to the theoretical analysis of deep neural networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Excellent analysis ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}], "authors": "Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, Jascha Sohl-Dickstein", "accepted": true, "id": "448"}