{
  "name" : "1602.05179.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards a Biologically Plausible Backprop",
    "authors" : [ "Benjamin Scellier", "Yoshua Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.e., moving towards configurations that better “explain” the observed sensory data. We can think of the configuration of internal neurons (hidden units or latent variables) as an “explanation” for the observed sensory data.\nThis work is only a stepping stone towards a full theory of learning in deep biological networks that performs a form of credit assignment that would be credible from a machine learning point of view and would scale to very large networks. We focus on a simple setup in which inputs are clamped, the network relaxes to a fixed point, at which predictions are read out. When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al. (2015a). Several points, elaborated at the end of this paper, still need to be elucidated before a complete theory of learning, inference and credit assignment is elaborated that is both biologically plausible and makes sense from a machine learning point of view for whole networks (with global optimization of the whole network and not being limited to learning of visible neurons that receive a target). In particular, the proposed energy-based model requires symmetry of connections, but note that hidden units in the model need not correspond exactly to actual neurons in the brain (it could be groups of neurons in a cortical microcircuit, for example). It remains to be shown how a form of symmetry could arise from the learning procedure itself or if a different formulation could eliminate the symmetry requirement.\nWe believe that the contributions of this article are the following.\n• We lay theoretical foundations that guarantee that our model (and in particular the STDP learning rule) makes sense from a mathematical and machine learning point of view, i.e., that the proposed STDP update rule corresponds to stochastic gradient descent on a prediction error.\n• This shows that leaky integrator neural computation in a recurrent neural network can be interpreted as performing both inference and back-propagation of errors. This avoids the need for a side network that is not biologically plausible for implementing back-propagation.\n∗Y.B. is also a Senior Fellow of CIFAR\nar X\niv :1\n60 2.\n05 17\n9v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n16\n• We show experimentally that with such a procedure, it is possible to train a model with 1, 2 and 3 hidden layers on MNIST and get the training error down to 0.00%.\n• The code for the model is available for replicating and extending the experiments."
    }, {
      "heading" : "2 Previous work (revisited)",
      "text" : "In this section, we present the model first introduced in Bengio and Fischer (2015); Bengio et al. (2015a,b). The model is a continuous-time process { (θ(t), s(t)) : t ∈ R } where, as usual in neural networks, s is the vector that represents the\nstates of the units and θ = (W, b) represents the set of free parameters, which includes the synaptic weights Wi,j and the neuron biases bi (which control the activation threshold for each unit i and also correspond to the weight from a virtual constant input). The units are continuous-valued and would correspond to averaged voltage potential across time, spikes, and possibly neurons in the same minicolumn. The units in the model need not correspond exactly to actual neurons in the brain. Finally, ρ is an activation function such that ρ(si) represents the firing rate of unit i. Next we will define neural computation (subsection 2.1) and see how it performs both inference (subsection 2.2) and error back-propagation (subsection 2.3)."
    }, {
      "heading" : "2.1 Neural computation as leaky integrator",
      "text" : "As usual in models of biological neurons, we assume that the neurons are performing leaky temporal integration of their past inputs. The time evolution of the neurons is assumed to follow the leaky integration equation\ndsi dt = Ri(s)− si τ , (1)\nwhere Ri(s) represents the pressure on neuron i from the rest of the network (and the value to which si would converge if Ri(s) would not change) while τ is a characteristic integration time. Moreover Ri(s) is assumed to be of the form\nRi(s) ∝ ∑ j 6=i Wj,iρ(sj) + bi. (2)"
    }, {
      "heading" : "2.2 Neural computation does inference: going down the energy",
      "text" : "One hypothesis in computational neuroscience is that biological neurons perform iterative inference. In our model, that means that the hidden units of the network gradually move towards configurations that are more probable, given the sensory input and according to the current \"model of the world\" associated with the parameters of the model.\nOne class of models based on iterative inference is the class of energy-based models, in which an energy function E(θ, s) drives the states of the units according to a dynamics of the form\nds dt ∝ −∂E ∂s .\nWith this dynamics, the network spontaneously moves toward low-energy configurations. Consider the following energy function, studied by Bengio et al. (2015a):\nE(θ, s) := 1\n2 ∑ i s2i − 1 2 ∑ i 6=j Wi,jρ(si)ρ(sj)− ∑ i biρ(si). (3)\nWe have\n∂E ∂si = si − ρ′(si) ∑ j 6=i 1 2 (Wi,j +Wj,i)ρ(sj) + bi  . (4) Thus, by defining\nds dt = − 1 τ ∂E ∂s , (5)\nwe get for the i-th unit dsi dt = − 1 τ ∂E ∂si = Ri(s)− si τ , (6)\nwhich is consistent with Eq. 1, with\nRi(s) = ρ ′(si) ∑ j 6=i 1 2 (Wi,j +Wj,i)ρ(sj) + bi  . (7) For Ri(s) to have the same form as in Eq. 2, we need to impose symmetric connections, i.e. Wi,j = Wj,i. Finally, we get\nRi(s) := ρ ′(si) ∑ j 6=i Wj,iρ(sj) + bi  . (8) The factor ρ′(si) would suggest that when a neuron is saturated (either being shut off or firing at the maximal rate), the external inputs have no impact on its state. In this case, the dynamics of si becomes dsidt = − si τ\n, driving si towards 0 and bringing it out of the saturation region and back into a regime where the neuron is sensitive to the outside feedback, so long as ρ(0) is not a saturated value."
    }, {
      "heading" : "2.3 Early inference recovers backpropagation",
      "text" : "In Bengio and Fischer (2015), it is shown how iterative inference can also backpropagate error signals in a multi-layer network. In this subsection we revisit this result and adapt it to neural networks with a general architecture where the units are split in visible and hidden units, i.e. s = (v, h). Like in previous work inspired by the Boltzmann machine, we will use the terminology of “positive phase” and “negative phase” to distinguish two phases of training, the positive phase being with v fully observed (or clamped) and the negative phase with v partially or fully unobserved. They actually correspond to the network following the gradient of the energy function, but with or without a term that drives some or all of the visible units towards the value of external signals (inputs and target outputs of the network).\nFor this purpose, we introduce a new term to the energy function that drives the neuron, a term that corresponds to prediction error and that can push visible units towards observed values for any subset of the visible units:\nCβ(v) := 1\n2 ∑ i βi(vdata,i − vi)2 (9)\nwhere βi ≥ 0 controls whether vi is pushed towards vdata,i or not, and by how much. The total actual energy is thus\nF (θ, s, β) := E(θ, s) + Cβ(v) = 1\n2 ∑ i s2i − 1 2 ∑ i6=j Wi,jρ(si)ρ(sj)− ∑ i biρ(si) + Cβ(v), (10)\nand the state s of the network evolves according to\nds dt ∝ −∂F ∂s . (11)\nWe may see F as an energy function for \"generalized phases\". The case β = 0 corresponds to the negative phase where all the units evolve freely according to the dynamics of the network, i.e. ds\ndt ∝ − ∂E ∂s . When βi > 0, an additional term\n∝ − ∂C ∂vi = vdata,i− vi is added that drives the visible unit vi towards the data vdata,i. In the limit βi → +∞, the visible unit vi move infinitely fast towards vdata,i, i.e. it is immediately clamped to vdata,i and is not sensitive to the pressure of its surroundings neurons anymore. The case when βi → +∞ for all visible units vi corresponds to the positive phase in the terminology of Boltzmann machines. Now we can define a notion of \"β-fixed point\": given β (with βi ≥ 0 for all i), we call \"β-fixed point\" and denote by sβ a state that satisfies\n∂F ∂s (θ, sβ , β) = 0. (12)\nThe state sβ is the fixed point to which the network eventually settles when it is driven by the dynamics of Eq. 11. For β = 0 on the visible units that we want the network to predict, we call s0 the negative fixed point and also denote it by s−. For β = +∞ (here we mean that βi → +∞ for all visible unit i), we call s+∞ the positive fixed point and also denote it by s+. β = +∞ corresponds to “clamping” the visible units.\nWhen addressing the supervised learning scenario where one wants to predict ydata from xdata, we distinguish two groups of visible units, the inputs x and the outputs y, with their respective β:\nCβ(v) := 1\n2 βx||xdata,i − xi||2 +\n1 2 βy||ydata,i − yi||2 (13)\nwhere (x, y) = v and βx and βy are non-negative scalars. In the supervised setting studied here, during the negative phase we have βx → +∞ and βy = 0, i.e., the inputs are observed but not the target outputs.\nSuppose that the network is settled to a (negative phase) equilibrium point s = s0 = s− = (xdata, h−, y−), that is\n∂F ∂h (xdata, h −, y−) = 0 and ∂F ∂y (xdata, h −, y−) = 0. (14)\nThen in the positive phase, ydata is observed and we change βy from 0 to a positive value, gradually driving the output units y from their fixed point value y−, towards ydata. This happens because the output units are also leaky integrator neurons, meaning that their state gradually changes based on the input they receive, in direction of the driving signal (now not only Rv(s−) but also ydata). Notice that because s− is already at equilibrium for all the units except those for which a new observation is made, just after that observation is made (we are starting the positive phase), we have\ndy dt = ydata − y− τ = − 1 τ ∂Cβ ∂y (y−). (15)\nThen, starting from Eq. 5 and differentiating wrt time, we have\nd2h\ndt2 = − 1 τ d dt ∂F ∂h = − 1 τ\n( ∂2F\n∂y∂h\ndy dt + ∂2F ∂h2 dh dt ) where we use that\ndf dt = ∂f ∂y dy dt + ∂f ∂h dh dt , (16)\nwith the function f(h, y) = ∂F ∂h (xdata, h, y). Injecting the initial values of dydt and dh dt at the point s− in Eq. 2.3 we get\nd2h dt2 = 1 τ2 ∂2F ∂h∂y ∂Cβ ∂y\n(17)\nwhere we use that the cross derivatives of F are symmetric. Indeed we are still at the fixed point, where the “position” has not changed, and only “velocity” on y is non-zero, due to a change in the energy function introduced by the novel observation. Since y has not changed, the pressure on h has not changed (yet). Therefore we have\nd2h\ndt2 = − 1 τ2 ∂Ry ∂h ∂C ∂y = − 1 τ2 ∂Ĉ ∂h , (18)\nwhere we define Ĉ(h) as the value of C(yh) taken at the point yh that is at equilibrium given h, i.e. yh is such that ∂F ∂y (h, yh) = 0. We used Eq. 6 for y rather than only for s, so that ∂ 2F ∂h∂y = − ∂Ry(s) ∂h .\nFrom the above, we see that the early change of h from its fixed point (where dh dt is initially 0) is in the direction opposite to the error gradient: early propagation of pertubations due to the output units moving towards their target propagates into perturbations of hidden units in the direction opposite to the gradient of the prediction error C. A related demonstration was made by Bengio and Fischer (2015) in the case of a regular feedforward network and the discrete-time setting, relying on small difference approximations."
    }, {
      "heading" : "2.4 STDP learning rule",
      "text" : "Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes. Although it is the result of experimental observations in biological neurons, its generalization (outside of the experimental setups in which it was measured) and interpretation as part of a learning procedure that could explain learning in deep networks remains a topic where more exploration is needed.\nExperimental results in Bengio et al. (2015a) show that if the weight changes satisfy\ndWij dt ∝ ρ(si) dsj dt , (19)\nthen we recover the biological observations made in Bi and Poo (2001) about Spike-Timing Dependent Plasticity. For this reason we refer to Eq. 19 as the STDP learning rule.\nIn this paper, we change the STDP learning rule from Bengio et al. (2015a) into\ndWij dt ∝ ρ(si) dρ(sj) dt . (20)\nThe two rules are the same up to a factor ρ′(sj). However, with the hard sigmoid nonlinearity ρ(s) = max(0,min(1, s)) chosen for our experiments, s is forced to stay in the “active” range where ρ′(s) = 1. Note that this form of the STDP update rule is the same as the one studied by Xie and Seung (2000).\nAn advantage of this form of the STDP update rule is that it leads to a more natural view of the update for the tied symmetric value Wij = Wji. Assuming this constraint to be somehow enforced, the update should take into account the pressures from both the i to j and j to i synapses, so that the total update under constraint is\ndWij dt ∝ ρ(si) dρ(sj) dt + ρ(sj) dρ(si) dt = d dt ρ(si)ρ(sj). (21)\nWe will call Eq. 21 the symmetric STDP learning rule.\nAs an aside, let us show that the symmetric STDP learning rule can be expressed in terms of the following quantity J (a kind of kinetic energy) :\nJ := ∥∥∥∥dsdt ∥∥∥∥2 . (22)\nUsing (5), J can be rewritten\nJ = − 〈 ∂E\n∂s , ds dt\n〉 = − dE\ndt ∣∣∣∣ W\n(23)\nwhere the notation |W indicates that the differentiation is performed with fixed W . Differentiating J with respect to W we get\n∂J ∂Wij = − ∂ ∂Wij dE dt = − d dt ∂E ∂Wij = d dt ρ(si)ρ(sj). (24)\nTherefore the symmetric STDP learning rule can be rewritten:\ndW dt ∝ ∂J ∂W . (25)"
    }, {
      "heading" : "2.5 Deriving Contrastive Hebbian Learning from the STDP learning rule",
      "text" : "A connection between Backpropagation and Contrastive Hebbian Learning was shown previously in Xie and Seung (2003). We have seen in subsection 2.3 a connection between iterative inference and backpropagation. In this subsection we show a connection between the STDP learning rule and Contrastive Hebbian Learning in energy-based models. In a differential form, the STDP learning rule (21) can be written\ndWij ∝ d (ρ(si)ρ(sj)) . (26)\nAs in subsection 2.3, we denote the state of the network by s = (x, h, y) where x, h and y are the input, hidden and output units respectively. Consider the following procedure:\n1. negative phase (βx = +∞ and βy = 0): let the network relax and settle to a fixed point s− = (xdata, h−, y−); don’t update the weights during this phase;\n2. positive phase (β = (βx, βy) with βx = +∞ and βy > 0): starting from s−, let the network relax and settle to a fixed point sβ = (xdata, hβ , yβ); update the weights according to the STDP rule (26) on the path from s− to sβ .\nBy integrating (26) on the path from s− to sβ , we get\n∆Wij ∝ ρ(sβi )ρ(s β j )− ρ(s − i )ρ(s − j ). (27)\nIf we assume that the weight updates on the path from s− to sβ are too small to influence the trajectory of s, then the two following update procedures must be equivalent:\n1. update the weights according to (26) continuously on the path from s− to sβ ;\n2. update the weights according to (27) at the end of the positive phase.\nNotice that, if we choose βy = +∞ in the positive phase (i.e. ydata is clamped), Eq. 27 is the Contrastive Hebbian Learning rule:\n∆Wij ∝ ρ(s+i )ρ(s + j )− ρ(s − i )ρ(s − j )."
    }, {
      "heading" : "3 Link to Recurrent Back-Propagation and Getting Rid of the Positive Phase Relaxation",
      "text" : "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients. We present an explanation for this discrepency here.\nWe have seen in Section 2.3 that the early steps of inference in the energy-based model recovers backpropagation of errors through the network. To obtain a full back-propagation algorithm we also show that such a short inference, combined with our STDP update rule, gives rise to stochastic gradient descent on the prediction error. To achieve this, we will consider a value of β that is only barely greater than zero, which corresponds to only nudging the output units towards a value that would reduce prediction error.\nBesides computational efficiency, another reason for avoiding the positive phase relaxation suggested by Eq. 32 is that it does not follow exactly the same kind of dynamics as the negative phase relaxation because it uses a linearization of the neural activation rather than the fully non-linear activation. From a biological plausibility point of view, having to use a different kind of hardware and computation for the “forward” and “backward” phases is not satisfying. This issue is “addressed” by Xie and Seung (2003) by assuming that the feedback weights are tiny compared to the feedforward weights (thus making the feedback weights only indice infinitesimal perturbations on the hidden units’ state). Again, this introduces a hypothesis which seems to hardly match biology.\nWe use a different trick that both gets rid of the need for a positive phase relaxation and avoids the assumption of infinitesimal feedback weights. The idea relies on the observation that we are only looking for the gradient of C, which only asks how a small change in h− or θ would yield a small change in C. Thus we do not need to relax y all the way to ydata: we only need to nudge it in the direction − ∂C∂∂y . This corresponds to picking βy = positive but infinitesimal."
    }, {
      "heading" : "3.1 Reformulation of the problem",
      "text" : "Recall that we write the state s = (x, h, y) where x, h and y are the inputs, the hiddens and the outputs respectively. The \"negative phase\" corresponds to the choice of β = (βx, βy) with βx = +∞ and βy = 0 for the \"generalized energy function\" F . We denote the negative phase fixed point by s−. We will argue that, after the network has settled to s−, the correct thing to do from a machine learning perspective is not a positive phase with βy = +∞, but a positive phase with a \"small\" βy > 0.\nWe can frame the training objective as the following constrained optimization problem:\nfind min θ,s C(s)\nsubject to ∂E\n∂s (θ, s) = 0.\nAs usual for constrained optimization problems, we introduce the Lagrangian:\nL(θ, s, λ) := C(s) + λ · ∂E ∂s (θ, s). (28)\nAs usual in this setting, starting from the current parameter θ, we first find s∗ and λ∗ such that\n∂L ∂λ (θ, s∗, λ∗) = 0 and\n∂L ∂s (θ, s∗, λ∗) = 0, (29)\nand then we do one step of gradient descent on L with respect to θ, i.e.\n∆θ ∝ −∂L ∂θ (θ, s∗, λ∗) = −λ∗ · ∂ 2E ∂θ∂s (θ, s∗). (30)\nThe first condition translates into ∂E ∂s (θ, s∗) = 0, i.e. s∗ = s− is the \"negative phase\" fixed point. The second condition translates into\n∂C ∂s (s−) + λ∗ · ∂\n2E\n∂s2 (θ, s−) = 0. (31)\nNote that solving the above equation in λ∗ can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987):\n∂C ∂s (s−) + (I −R′(s))λ∗ = 0⇒ λ∗ = R′(s)λ∗ − ∂C ∂s . (32)\nInstead, here, we will introduce and study a shortcut that does not require such a long relaxation, only the propagation of perturbations across the layers.\nFirst, let us consider another equation whose solution is equivalent in order to find a λ∗ that satisfies Eq. 31. We will show shortly that this solution is λ∗ = ds β\ndβ ∣∣∣ β=0\n, which in practice we will approximate by λ∗ ∝ sβ − s− for a small β > 0. Recall the definition of a β-fixed point (Eq. 12). Taking the differential of Eq. 12 for fixed θ, we get\n∂2F\n∂s2 · dsβ + ∂\n2F\n∂β∂s dβ = 0, (33)\nwhich can be rewritten ( ∂2E\n∂s2 + β\n∂2C\n∂s2\n) · dsβ + ∂C\n∂s dβ = 0 (34)\nsince ∂F ∂β = C. Finally, dividing by dβ and evaluating at β = 0 we get (recall that sβ = s− for β = 0)\n∂2E\n∂s2 (θ, s−) · ds\nβ\ndβ ∣∣∣∣ β=0 + ∂C ∂s (s−) = 0. (35)\nThus ds β\ndβ ∣∣∣ β=0 satisfies the same equation as λ∗ (Eq. 31) and we conclude that λ∗ = ds β dβ ∣∣∣ β=0 . Finally, injecting s∗ = s−\nand λ∗ = ds β\ndβ ∣∣∣ β=0 in Eq. 30 we get\n∆Wij ∝ −λ∗ · ∂2E\n∂s∂Wij =\ndsβ\ndβ ∣∣∣∣ β=0 · ∂ ∂s ρ(si)ρ(sj) ∣∣∣∣ s=s− = d dβ ρ(sβi )ρ(s β j ) ∣∣∣∣ β=0\n(36)\nusing ∂E ∂Wij = ρ(si)ρ(sj).\nIn practice, after running the negative phase and reaching the state s−, we run a β-phase (with small β) and reach sβ , and finally we update\n∆Wij ∝ ρ(sβi )ρ(s β j )− ρ(s − i )ρ(s − j ). (37)\nAs argued in subsection 2.5 this is equivalent to update the weights according to the symmetric STDP learning rule\ndWij dt ∝ d dt ρ(si)ρ(sj) (38)\nall the way from s− to sβ . In fact we don’t even need to let the network relax until it reaches the beta fixed point sβ during the β-phase. Indeed in Subsection 2.3 we showed that early moves of s at the point s− go in the direction that minimizes C. That means that they go in the direction that minimizes F since E does not contribute to these moves at s = s−.\nHence we have found that (a) stochastic gradient descent of the prediction error in a recurrent network with clamped inputs can be achieved with a very brief relaxation (just enough for signals to propagate from outputs into all the hidden layers) in which the output units are slightly driven towards their target and that (b) the update corresponds to the STDP update rule from Bengio et al. (2015a) as well as (when incorporating the symmetry constraint) to the contrastive Hebbian learning update."
    }, {
      "heading" : "4 Implementation of the model",
      "text" : "In this section, we provide experimental evidence that our model is trainable, and we analyze the influence of various factors on the training time, the generalization error as well as the biological plausibility of the model.\nWe show that the STDP learning rule (21) can be used to train a neural network with several layers of hidden units to classify the MNIST digits.\nFor each example, training proceeds in two phases. During the first phase (that we call negative phase by analogy with Boltzmann Machines), we clamp x = xdata and let the network relax until it settles to a (negative) fixed point s− = (xdata, h\n−, y−). During the negative phase, the weights W remain fixed. Then, during a second phase (called positive phase, or β-phase as discussed in section 3) we drive y towards ydata according to updates y ← (1− )y+ ydata and let the network relax for \"a little bit\" (see the next subsection for the a discussion about the duration of this relaxation)."
    }, {
      "heading" : "4.1 Finite difference method",
      "text" : "We choose τ = 1 for the characteristic time. The obvious way to implement the iterative inference procedure as defined in (5) is to discretize time into short time lapses of duration and update each unit si according to\nsi ← si − ∂E\n∂si = (1− )si + Ri(s),\nthat is\nsi ← (1− )si + ρ′(si) ∑ j 6=i Wj,iρ(sj) + bi  . (39) This is simply one step of gradient descent on the energy, with step size , as described in Bengio and Fischer (2015).\nFor our experiments (see subsection 4.2) we choose the hard sigmoid as an activation function, i.e. ρ(s) = 0 ∨ s ∧ 1, where ∨ denotes the max and ∧ the min. Also, rather than the standard gradient descent (39), we will use a slightly modified version:\nsi ← 0 ∨ (1− )si + ρ′(si) ∑ j 6=i Wj,iρ(sj) + bi  ∧ 1. (40) Indeed, for this choice of ρ, the form of Ri(s) (Eq. 8) shows that when si reaches si = 0, the fact that ρ′(0−) = 0 prevents si from going further in the range of negative values. Similarly, si cannot reach values above 1. Therefore si always remains in the domain 0 ≤ si ≤ 1. Hence Eq. 40. This little detail is more important than it seems at first glance: if at some point the ith unit was in the state si < 0, then Eq. 39 would simplify to si ← (1 − )si, which would give again si < 0. As a consequence si would be doomed to remain in the negative range forever, being totally insensitive to the pressure of the surrounding units. This issue was already mentioned in Bengio and Fischer (2015).\nTwo questions arise:\n1. What step size should we choose ?\n2. How long do the negative phase and positive phase relaxations need to last (as functions of τ )?\nStep size . Figure 1 shows that the choice of has little influence as long as 0 < < 1. In our experiments we choose = 0.5 to avoid extra unnecessary computations.\nDuration of the negative phase relaxation. We find experimentally that the number of iterations required for the negative phase relaxation is large and grows fast as the number of layers increases, which could slow down training. More\nexperimental and theoretical investigation would be needed to analyze the number of iterations required, but we leave that for future work.\nDuration of the positive phase. As discussed in section 3, during the positive phase we only need to initiate the move of the units. Notice that the characteristic time τ represents the time needed for a signal to propagate from a layer to the next one with \"significant amplitude\". So the time needed for the error signals to backpropagate in the network is nτ , where n is the number of layers."
    }, {
      "heading" : "4.2 Implementation details and results",
      "text" : "We train multi-layer neural networks with 1, 2 and 3 hidden layers. Each hidden layer has 500 units. There is no connection within a layer and no skip connections. For efficiency of the experiments, we use mini batches of 20 training examples and we use = 0.5 as the step size for each iteration. Only one global weight update is done at the end of the positive phase according to Eq. 27, rather than several weight updates after each step of the positive phase (Eq. 26).\nTo tackle the problem of the long negative phase relaxation and speed-up the simulations, we use \"persistent particles\" for the latent variables to re-use the previous fixed point configuration for a particular example as a starting point for the next negative phase relaxation on that example. This means that for each training example in the dataset, we store the state of the hidden layers at the end of the negative phase, and we use this to initialize the state of the network at the next epoch. This method is similar in spirit to the PCD algorithm for sampling from other energy-based models like the Boltzmann machine (Tieleman, 2008).\nFor the model with 1 hidden layer, we run 20 iterations for the negative phase and 4 iterations for the positive phase. For the model with 2 hidden layers, we run 100 iterations for the negative phase and 6 iterations for the positive phase. For the model with 3 hidden layers, we run 500 iterations for the negative phase and 8 iterations for the positive phase.\nWe find that it is important to choose different learning rates for the weight matrices of different layers. We denote by Wi the weight matrix between the layers Li−1 and Li. We choose the learning rate αi for Wi so that the quantities ‖∆Wi‖ ‖Wi‖\nfor i = 1, · · · , n are approximately the same in average, where ‖∆Wi‖ represents the weight change of Wi after seeing a minibatch. For the model with 1 hidden layer, we use α1 = 0.1 and α2 = 0.05. For the model with 2 hidden layers, we use α1 = 0.4, α2 = 0.1 and α3 = 0.01. For the model with 3 hidden layers, we use α1 = 0.128, α2 = 0.032, α3 = 0.008 and α4 = 0.002."
    }, {
      "heading" : "4.3 Speeding up the relaxation",
      "text" : "In this subsection we introduce an algorithm to accelerate the relaxation in both the negative phase and the positive phase. With the finite difference method (subsection 4.1), the number of iterations required to reach a negative fixed point with enough accuracy becomes very large as the number of hidden layers increases. The algorithm that we design here enables to train our networks 5 times as fast by speeding up convergence towards a fixed point. This algorithm is less biologically plausible but it establishes a link with forward propagation and backpropagation in feedforward networks, as well as Gibbs sampling in Boltzmann machines. The spirit of this algorithm is similar to going from ordinary Gibbs sampling (where only one unit is updated at a time) to block Gibbs sampling (where a whole layer is updated in parallel, given the layer below and the one above). Given the state of the layer above and the layer below some layer, we can analytically solve for the fixed point solution of the middle layer. By iterating these analytical fixed point solutions (conditional on clamping the other layers), we can greatly speed-up convergence of the relaxations.\nGiven the states s−i of all units but unit i, one denotes by s∗i the value of si that minimizes the energy:\ns∗i := arg min si E(θ, s−i, si) = arg min si s2i − ρ(si) ∑ j 6=i Wijρ(sj) + bi  . (41) The value s∗i represents the equilibrium potential of unit i subjected to the constant external pressure pi = ∑ j 6=iWijρ(sj)+ bi. The value s∗i is a function of pi, so we denote by π the mapping from pi to s ∗ i , i.e. s ∗ i = π(pi). If ρ is differentiable everywhere, then to achieve ds ∗ i\ndt = 0, we must have s∗i such that\ns∗i = ρ ′(s∗i ) pi. (42)\nFigure 3 shows the shape of the function π in two cases. When ρ is the hard sigmoid defined by ρ(s) = 0 ∨ s ∧ 1 (where ∨ denotes the max and ∧ denotes the min), then π is the hard sigmoid itself. When ρ is a soft sigmoid such\nas ρ(s) = 1/(1 + e2−4s), then π has no analytical expression and must be computed numerically. For this reason we choose the hard sigmoid for our experiments.\nThe function π can be used to update the units sequentially in a way similar to the Hopfield net or the Gibbs sampling algorithm in Boltzmann machines. Given the state s−i of all units except unit i, the update rule for si is\nsi ← π ∑ j 6=i Wjiρ(sj)  . (43) In the case of a multi-layer network with no connection within a layer, one can update in parallel all the units within a layer given the state of the units in the other layers. We denote by L0, L1, · · · , Ln the layers of the network, where L0 = x and Ln = y. For the negative relaxation phase, we use the algorithm 1 (Forwardprop). For the positive phase, we use the algorithm 2 (Backprop). These algorithms are similar in spirit to the block Gibbs sampling procedure in Deep Boltzmann Machines. Also notice that these algorithms still work with skip connections.\nAlgorithm 1 Forwardprop Clamp L0 ← xdata. repeat\nfor k = 1 to n− 1 do Lk ← π ( Wl−1Lk−1 +W T l Lk+1 ) end for Ln ← π (Wn−1Ln−1)\nuntil convergence.\nAlgorithm 2 Backprop Clamp L0 ← xdata and Ln ← ydata. repeat\nfor k = n− 1 down to 1 do Lk ← π ( Wl−1Lk−1 +W T l Lk+1 ) end for\nuntil desired."
    }, {
      "heading" : "5 Discussion and future work",
      "text" : "In the CD algorithm for RBMs, one starts from a positive equilibrium sample and then, during a negative phase, we do a few steps of iterative inference to get an approximate negative sample. However, for discriminative tasks, it makes more sense to do the contrary: start from a negative state and drive the prediction made towards the correct target during a positive phase. This is what our model does.\nThe same algorithms can be adapted to the unsupervised learning setting, and this is the subject of future work.\nWe could imagine introducing different types of units with different characteristic times τ to integrate the signals from the surrounding units.\nWe could introduce connections within each layer. The property of subsection 2.3 still applies in this setting. Therefore the finite difference method ought to work. Notice that the trick to speed up the relaxation (the forwardprop and backprop algorithms) does not work in this setting, but it is less biologically plausible anyway.\nA troubling issue from the point of view of biological plausibility is our model requires symmetric weights between the units. We need to find a way to either untie those weights or figure out how a symmetry in the connections could naturally arise, for example from autoencoder-like unsupervised learning. Encouraging cues from the observation that denoising autoencoders without tied weights often end up learning symmetric weights (Vincent et al., 2010). Another encouraging piece of evidence, also linked to autoencoders, is the theoretical result from Arora et al. (2015), showing that the symmetric solution minimizes the autoencoder reconstruction error between two successive layers of rectifying (ReLU) units.\nMore open problems remains. Both from a machine learning point of view and from a biological plausibility point of view, we would like to get rid of the requirement to have to run a full relaxation to a fixed point in the negative phase. From a machine learning point of view it makes computations much slower than in traditional feedforward neural networks. From a biological point of view it would not be practical for a brain to have to wait for the whole brain to settle near a fixed point before processing the next stimuli. This question is related to the issue of running a deterministic simulation. A more likely simulation would include some form of noise. As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function. Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks.\nAlso connected to the above question of having to wait for a negative phase fixed point is the question of timevarying input. Although this work makes back-propagation more plausible for the case of a static input, the brain is a recurrent network with time-varying inputs, and back-propagation through time seems even less plausible than static back-propagation. An encouraging direction is that proposed by Ollivier et al. (2015), which shown that computationally efficient estimators of the gradient can be obtained using a forward method (online estimation of the gradient), which avoids to need to store all past states in training sequences, at the price of a noisy estimator of the gradient."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Akram Erraqabi, Samira Shabanian and Asja Fischer for feedback and discussions, as well as NSERC, CIFAR, Samsung and Canada Research Chairs for funding, and Compute Canada for computing resources."
    } ],
    "references" : [ {
      "title" : "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment",
      "author" : [ "L.B. Almeida" ],
      "venue" : "IEEE International Conference on Neural Networks,",
      "citeRegEx" : "Almeida,? \\Q1987\\E",
      "shortCiteRegEx" : "Almeida",
      "year" : 1987
    }, {
      "title" : "Why are deep nets reversible: a simple theory, with implications for training",
      "author" : [ "S. Arora", "Y. Liang", "T. Ma" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Arora et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2015
    }, {
      "title" : "Early inference in energy-based models approximates back-propagation",
      "author" : [ "Y. Bengio", "A. Fischer" ],
      "venue" : "arXiv preprint arXiv:1510.02777",
      "citeRegEx" : "Bengio and Fischer,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio and Fischer",
      "year" : 2015
    }, {
      "title" : "STDP as presynaptic activity times rate of change of postsynaptic activity",
      "author" : [ "Y. Bengio", "T. Mesnard", "A. Fischer", "S. Zhang", "Y. Wu" ],
      "venue" : "arXiv preprint arXiv:1509.05936",
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards biologically plausible deep learning",
      "author" : [ "Y. Bengio", "Lee", "D.-H", "J. Bornschein", "Z. Lin" ],
      "venue" : "arXiv preprint arXiv:1502.04156",
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment",
      "author" : [ "P. Berkes", "G. Orban", "M. Lengyel", "J. Fiser" ],
      "venue" : null,
      "citeRegEx" : "Berkes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Berkes et al\\.",
      "year" : 2011
    }, {
      "title" : "Synaptic modification by correlated activity: Hebb’s postulate revisited",
      "author" : [ "Bi", "G.-q", "Poo", "M.-m" ],
      "venue" : "Annual review of neuroscience,",
      "citeRegEx" : "Bi et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bi et al\\.",
      "year" : 2001
    }, {
      "title" : "A neuronal learning rule for sub-millisecond temporal coding",
      "author" : [ "W. Gerstner", "R. Kempter", "J.L. van Hemmen", "H. Wagner" ],
      "venue" : null,
      "citeRegEx" : "Gerstner et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Gerstner et al\\.",
      "year" : 1996
    }, {
      "title" : "Learning and releaming in boltzmann machines. Parallel distributed processing: Explorations in the microstructure",
      "author" : [ "G.E. Hinton", "T.J. Sejnowski" ],
      "venue" : "of cognition,",
      "citeRegEx" : "Hinton and Sejnowski,? \\Q1986\\E",
      "shortCiteRegEx" : "Hinton and Sejnowski",
      "year" : 1986
    }, {
      "title" : "Action potentials propagating back into dendrites triggers changes in efficacy",
      "author" : [ "H. Markram", "B. Sakmann" ],
      "venue" : "Soc. Neurosci. Abs,",
      "citeRegEx" : "Markram and Sakmann,? \\Q1995\\E",
      "shortCiteRegEx" : "Markram and Sakmann",
      "year" : 1995
    }, {
      "title" : "Training recurrent networks online without backtracking",
      "author" : [ "Y. Ollivier", "C. Tallec", "G. Charpiat" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Ollivier et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ollivier et al\\.",
      "year" : 2015
    }, {
      "title" : "Generalization of back-propagation to recurrent neural networks",
      "author" : [ "F.J. Pineda" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Pineda,? \\Q1987\\E",
      "shortCiteRegEx" : "Pineda",
      "year" : 1987
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Training restricted boltzmann machines using approximations to the likelihood gradient",
      "author" : [ "T. Tieleman" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Tieleman,? \\Q2008\\E",
      "shortCiteRegEx" : "Tieleman",
      "year" : 2008
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A" ],
      "venue" : "J. Machine Learning Res.,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    }, {
      "title" : "Spike-based learning rules and stabilization of persistent neural activity",
      "author" : [ "X. Xie", "H.S. Seung" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Xie and Seung,? \\Q2000\\E",
      "shortCiteRegEx" : "Xie and Seung",
      "year" : 2000
    }, {
      "title" : "Equivalence of backpropagation and contrastive hebbian learning in a layered network",
      "author" : [ "X. Xie", "H.S. Seung" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Xie and Seung,? \\Q2003\\E",
      "shortCiteRegEx" : "Xie and Seung",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Abstract This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.",
      "startOffset" : 40,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.",
      "startOffset" : 40,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al.",
      "startOffset" : 188,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al. (2015a). Several points, elaborated at the end of this paper, still need to be elucidated before a complete theory of learning, inference and credit assignment is elaborated that is both biologically plausible and makes sense from a machine learning point of view for whole networks (with global optimization of the whole network and not being limited to learning of visible neurons that receive a target).",
      "startOffset" : 188,
      "endOffset" : 381
    }, {
      "referenceID" : 2,
      "context" : "In this section, we present the model first introduced in Bengio and Fischer (2015); Bengio et al.",
      "startOffset" : 58,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "Consider the following energy function, studied by Bengio et al. (2015a):",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "3 Early inference recovers backpropagation In Bengio and Fischer (2015), it is shown how iterative inference can also backpropagate error signals in a multi-layer network.",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "A related demonstration was made by Bengio and Fischer (2015) in the case of a regular feedforward network and the discrete-time setting, relying on small difference approximations.",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "4 STDP learning rule Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes.",
      "startOffset" : 124,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "4 STDP learning rule Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes.",
      "startOffset" : 124,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Experimental results in Bengio et al. (2015a) show that if the weight changes satisfy dWij dt ∝ ρ(si) dsj dt , (19)",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we change the STDP learning rule from Bengio et al. (2015a) into dWij dt ∝ ρ(si) dρ(sj) dt .",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Note that this form of the STDP update rule is the same as the one studied by Xie and Seung (2000). An advantage of this form of the STDP update rule is that it leads to a more natural view of the update for the tied symmetric value Wij = Wji.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "5 Deriving Contrastive Hebbian Learning from the STDP learning rule A connection between Backpropagation and Contrastive Hebbian Learning was shown previously in Xie and Seung (2003). We have seen in subsection 2.",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.",
      "startOffset" : 58,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.",
      "startOffset" : 58,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.",
      "startOffset" : 73,
      "endOffset" : 250
    }, {
      "referenceID" : 0,
      "context" : "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.",
      "startOffset" : 73,
      "endOffset" : 566
    }, {
      "referenceID" : 0,
      "context" : "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients. We present an explanation for this discrepency here. We have seen in Section 2.3 that the early steps of inference in the energy-based model recovers backpropagation of errors through the network. To obtain a full back-propagation algorithm we also show that such a short inference, combined with our STDP update rule, gives rise to stochastic gradient descent on the prediction error. To achieve this, we will consider a value of β that is only barely greater than zero, which corresponds to only nudging the output units towards a value that would reduce prediction error. Besides computational efficiency, another reason for avoiding the positive phase relaxation suggested by Eq. 32 is that it does not follow exactly the same kind of dynamics as the negative phase relaxation because it uses a linearization of the neural activation rather than the fully non-linear activation. From a biological plausibility point of view, having to use a different kind of hardware and computation for the “forward” and “backward” phases is not satisfying. This issue is “addressed” by Xie and Seung (2003) by assuming that the feedback weights are tiny compared to the feedforward weights (thus making the feedback weights only indice infinitesimal perturbations on the hidden units’ state).",
      "startOffset" : 73,
      "endOffset" : 1750
    }, {
      "referenceID" : 10,
      "context" : "(31) Note that solving the above equation in λ∗ can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987): ∂C ∂s (s) + (I −R(s))λ = 0⇒ λ = R(s)λ − ∂C ∂s .",
      "startOffset" : 186,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "(31) Note that solving the above equation in λ∗ can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987): ∂C ∂s (s) + (I −R(s))λ = 0⇒ λ = R(s)λ − ∂C ∂s .",
      "startOffset" : 201,
      "endOffset" : 216
    }, {
      "referenceID" : 3,
      "context" : "Hence we have found that (a) stochastic gradient descent of the prediction error in a recurrent network with clamped inputs can be achieved with a very brief relaxation (just enough for signals to propagate from outputs into all the hidden layers) in which the output units are slightly driven towards their target and that (b) the update corresponds to the STDP update rule from Bengio et al. (2015a) as well as (when incorporating the symmetry constraint) to the contrastive Hebbian learning update.",
      "startOffset" : 380,
      "endOffset" : 402
    }, {
      "referenceID" : 2,
      "context" : "This is simply one step of gradient descent on the energy, with step size , as described in Bengio and Fischer (2015).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "This issue was already mentioned in Bengio and Fischer (2015).",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "This method is similar in spirit to the PCD algorithm for sampling from other energy-based models like the Boltzmann machine (Tieleman, 2008).",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "Encouraging cues from the observation that denoising autoencoders without tied weights often end up learning symmetric weights (Vincent et al., 2010).",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "Another encouraging piece of evidence, also linked to autoencoders, is the theoretical result from Arora et al. (2015), showing that the symmetric solution minimizes the autoencoder reconstruction error between two successive layers of rectifying (ReLU) units.",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks.",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function. Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks. Also connected to the above question of having to wait for a negative phase fixed point is the question of timevarying input. Although this work makes back-propagation more plausible for the case of a static input, the brain is a recurrent network with time-varying inputs, and back-propagation through time seems even less plausible than static back-propagation. An encouraging direction is that proposed by Ollivier et al. (2015), which shown that computationally efficient estimators of the gradient can be obtained using a forward method (online estimation of the gradient), which avoids to need to store all past states in training sequences, at the price of a noisy estimator of the gradient.",
      "startOffset" : 18,
      "endOffset" : 903
    } ],
    "year" : 2016,
    "abstractText" : "This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00%. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.",
    "creator" : "LaTeX with hyperref package"
  }
}