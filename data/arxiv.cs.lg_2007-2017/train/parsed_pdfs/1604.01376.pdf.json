{
  "name" : "1604.01376.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "remi.emonet@univ-st-etienne.fr", "valentina.zantedeschi@univ-st-etienne.fr", "marc.sebban@univ-st-etienne.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 4.\n01 37"
    }, {
      "heading" : "1 Multi-variate Lipschitz continuity",
      "text" : "A function is said Lipschitz continuous if it takes similar values on points that are close. More precisely, the slope of the function is bounded by a constant that is independent of the choice of points. This means that the variation of a function that is Lipschitz continuous within a certain interval is small. The Lipschitz continuity is a strong form of uniform continuity: for instance, a function that is Lipschitz continuous is also continuous, but the reverse is not necessarily true. Let’s take the example of the square function: x2 is continuous on Rm but it is not Lipschitz continuous (the slope of x2 is not bounded).\nWe now consider the Lipschitz continuity for a function f : X 2 ⊂ Rd×Rd → R.\nDefinition 1.1. (Multi-variate Lipschitz continuity) A function f : X 2 ⊂ Rd× R\nd → R is said kn-lipschitz w.r.t. the norm ‖.‖n if ∀(x1, x2, x′1, x′2) ∈ X 4:\n|f(x1, x2)− f(x′1, x′2)| ≤ kn ∥ ∥ ∥\n∥\n(\nx1\nx2\n) − ( x′1 x′2\n)∥\n∥ ∥ ∥\nn\n. (1)\nIf f is differentiable on X 2 ⊂ Rd × Rd and X 2 is a convex space, the best constant kn, the characteristic Lipschitz coefficient, can be estimated considering the fact that\nkn = sup x1,x2,x ′\n1 ,x′ 2 ∈X\n(\n|f(x1, x2)− f(x′1, x′2)| ∥\n∥ ∥\n(\nx1 x2\n) − ( x′ 1\nx′ 2\n)\n∥ ∥ ∥\nn\n)\n=\n= sup x1,x2∈X\n‖∇f(x1, x2)‖n . (2)"
    }, {
      "heading" : "2 Derivation for Particular Functions",
      "text" : "In this section, we analyze the Lipschitz continuity of two classic metric functions: the Mahalanobis distance and the bilinear form. These two functions are largely used in the field of Machine Learning, especially in Metric Learning."
    }, {
      "heading" : "2.1 Derivation for Mahalanobis-like Distances",
      "text" : "We recall that the Mahalanobis distance of a pair (x1, x2) ∈ X 2 can be written as dM (x1, x2) = √\n(x1 − x2)TM(x1 − x2) where M is some Positive Semi-Definite matrix, whose coefficients can be optimized. By Def. 1.1, the function dM : X 2 → R is kn-lipschitz w.r.t. the norm ‖.‖n if ∀x1, x2 ∈ X , ‖∇dM (x1, x2)‖n can be bounded by a constant kn, where\n∇dM (x1, x2) = (∂dM (x1,x2) ∂x1\n∂dM (x1,x2) ∂x2\n)\nand, for this particular case:\n∂dM (x1, x2)\n∂x1 =\n1\n2 √ (x1 − x2)TM(x1 − x2) ∂ ∂x1\n( (x1 − x2)TM(x1 − x2) )\n= 1\n2 √ (x1 − x2)TM(x1 − x2) ∂ ∂x1 ( xT1 Mx1 − xT2 Mx1 − xT1 Mx2 + xT2 Mx2 )\n(3)\n= 2Mx1 −Mx2 −Mx2\n2 √ (x1 − x2)TM(x1 − x2) (4)\n= Mx1 −Mx2 √\n(x1 − x2)TM(x1 − x2) =\nM(x1 − x2) √\n(x1 − x2)TM(x1 − x2)\nand, in the same way:\n∂dM (x1, x2)\n∂x2 =\nM(x2 − x1) √\n(x1 − x2)TM(x1 − x2) .\nIn Eq. 3 and 4 we made use of the symmetry of the matrix M .\nLemma 2.1. The Mahalanobis distance dM (x1, x2) is k-lipschitz w.r.t. the norm ‖.‖2, with k = √ 2 ‖L‖2, where M = LTL, with L a lower triangular matrix.\nProof.\nmax x1,x2∈X\n‖∇dM (x1, x2)‖2\n= max x1,x2∈X\n√\n∥ ∥ ∥ ∥ ∂dM (x1, x2)\n∂x1\n∥ ∥ ∥ ∥ 2\n2\n+\n∥ ∥ ∥ ∥ ∂dM (x1, x2)\n∂x2\n∥ ∥ ∥ ∥ 2\n2\n= max x1,x2∈X\n√ √ √ √ ∥ ∥ ∥\n∥ ∥\nM(x1 − x2) √\n(x1 − x2)TM(x1 − x2)\n∥ ∥ ∥ ∥ ∥ 2\n2\n+\n∥ ∥ ∥ ∥ ∥ M(x2 − x1) √\n(x1 − x2)TM(x1 − x2)\n∥ ∥ ∥ ∥ ∥ 2\n2\n= max x1,x2∈X\n√ √ √ √2 ∥ ∥ ∥\n∥ ∥\nM(x1 − x2) √\n(x1 − x2)TM(x1 − x2)\n∥ ∥ ∥ ∥ ∥ 2\n2\n= max x1,x2∈X\n√ √ √ √2 ∥ ∥ ∥\n∥ ∥\nLTL(x1 − x2) √\n(x1 − x2)TLTL(x1 − x2)\n∥ ∥ ∥ ∥ ∥ 2\n2\n(5)\n= max x1,x2∈X\n√ √ √ √2 ∥ ∥ ∥\n∥ ∥\nLT (L(x1 − x2)) √\n(L(x1 − x2))TL(x1 − x2)\n∥ ∥ ∥ ∥ ∥ 2\n2\n= max x1,x2∈X\n√\n2\n∥ ∥ ∥ ∥ LT (L(x1 − x2)) ‖L(x1 − x2)‖2 ∥ ∥ ∥ ∥ 2 2\n≤ max x1,x2∈X\n√\n2 ‖LT‖2 ∥ ∥ ∥\n∥ (L(x1 − x2)) ‖L(x1 − x2)‖2\n∥ ∥ ∥ ∥ 2\n2\n(6)\n≤ √ 2 ∥ ∥LT ∥ ∥\n2 = k. (7)\nIn Eq. 5 we applied the Cholesky decomposition M = LTL, the bound in\n6 is due to the Cauchy-Schwarz inequality and in Eq. 7 ∥ ∥\n∥ (L(x1−x2)) ‖L(x1−x2)‖2\n∥ ∥ ∥\n2 = 1\nbecause it is a normalized vector."
    }, {
      "heading" : "2.2 Derivation for Bilinear Forms",
      "text" : "We recall that the bilinear form of a pair (x1, x2) is computed as dM (x1, x2) = xT1 Mx2, where M is a generic matrix that can be optimized. Then:\n∂dM (x1, x2)\n∂x1 =\n∂\n∂x1\n( xT1 Mx2) ) = Mx2\n∂dM (x1, x2)\n∂x2 =\n∂\n∂x2\n( xT1 Mx2) ) = MTx1.\nLemma 2.2. The bilinear similarity dM (x1, x2) = x T 1 Mx2 is k-lipschitz w.r.t.\nthe norm ‖.‖2, with k = √ 2 ‖M‖2 R, when ‖x‖2 ≤ R ∀x ∈ X .\nProof.\nmax ∀x1,x2∈U ‖∇dM (x1, x2)‖2 = max ∀x1,x2∈U\n√\n∥ ∥ ∥ ∥ ∂dM (x1, x2)\n∂x1\n∥ ∥ ∥ ∥ 2\n2\n+\n∥ ∥ ∥ ∥ ∂dM (x1, x2)\n∂x2\n∥ ∥ ∥ ∥ 2\n2\n= max ∀x1,x2∈U\n√\n‖Mx2‖22 + ‖MTx1‖ 2 2\n≤ √ 2 ‖M‖2 R = k. (8)"
    }, {
      "heading" : "3 Conclusion",
      "text" : "In this paper, we recalled a method for proving the Lipschitz continuity and for finding a tight Lipschitz constant of multivariate differentiable functions. Using this approach, we computed tight Lipschitz constants for two families of metrics that are heavily used, especially in metric learning. We have shown that the Mahalanobis distance is Lipschitz continuous and has a constant of √ 2 ‖L‖2 (where L is the square root of the correlation matrix). We have also shown that the bilinear form xMy is Lipschitz continuous with a constant √ 2 ‖M‖2 R (when the space is bounded by R). Many theoretical results in the machine learning domain rely on Lipschitz continuity and depend on the Lipschitz constants. For example, the generalization bounds obtained in the context of the uniform stability (see [1]) can be derived by constraining the studied functions to be Lipschitz continuous and the tightness of those bounds depends on the value of the Lipschitz constant. The derivations from this paper have been originally developed to derive theoretical bounds for [2]. We believe these results can also be used to derive tighter theoretical bounds in other domains of machine learning."
    } ],
    "references" : [ {
      "title" : "Metric learning",
      "author" : [ "A. Bellet", "A. Habrard", "M. Sebban" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Metric learning as convex combinations of local models with generalization guarantees",
      "author" : [ "V. Zantedeschi", "R. Emonet", "M. Sebban" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Many theoretical results in the machine learning domain stand only for functions that are Lipschitz continuous. Lipschitz continuity is a strong form of continuity that linearly bounds the variations of a function. In this paper, we derive tight Lipschitz constants for two families of metrics: Mahalanobis distances and bounded-space bilinear forms. To our knowledge, this is the first time the Mahalanobis distance is formally proved to be Lipschitz continuous and that such tight Lipschitz constants are derived.",
    "creator" : "LaTeX with hyperref package"
  }
}