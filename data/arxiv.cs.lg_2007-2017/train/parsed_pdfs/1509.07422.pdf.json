{
  "name" : "1509.07422.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Sequential Optimization with Applications to Machine Learning",
    "authors" : [ "Craig Wilson", "Venugopal V. Veeravalli" ],
    "emails" : [ "wilson60@illinois.edu", "vvv@illinois.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n07 42\n2v 1\n[ cs\n.L G\n] 2\n1 Introduction\nConsider solving a sequence of machine learning problems such as regression or classification by minimizing the expected value of a fixed loss function ℓ(x,z) at each time ns:\nmin x∈X\n{ fn(x), Ezn∼pn [ℓ(x,zn)] } ∀n ≥ 1 (1)\nFor regression, zn corresponds to the predictors and response pair at time n and x parameterizes the regression model. For classification zn corresponds to the feature and label pair at time n and x parameterizes the classifier. Although, motivated by regression and classification, our framework works for any loss function ℓ(x,z) that satisfies certain properties discussed later. In the learning context, a task consists of the loss function ℓ(x,z) and the distribution pn, and so our problem can be viewed as learning a sequence of tasks.\nThe problems change slowly at a constant but unknown rate in the sense that\n‖x∗n −x∗n−1‖= ρ ∀n ≥ 2 (2)\nwith x∗n the minimizer of fn(x). In an extended version of this paper [?], we also consider slow changes at a bounded but unknown rate\n‖x∗n −x∗n−1‖ ≤ ρ ∀n ≥ 2 (3) Under this model, we find approximate minimizers xn of each function fn(x) using Kn samples from distribution pn by applying an optimization algorithm. We evaluate the quality of our approximate minimizers xn through an excess risk criterion εn, i.e.,\nE [ fn(xn)]− fn(x∗n)≤ εn ∗This work was supported by the NSF under award CCF 11-11342 through the University of Illinois at Urbana-Champaign.\nwhich is a standard criterion for optimization and learning problems [1]. Our goal is to determine adaptively the number of samples Kn required to achieve a desired excess risk ε for each n with ρ unknown. As ρ is unknown, we will construct estimates of ρ . Given an estimate of ρ , we determine selection rules for the number of samples Kn to achieve a target excess risk ε .\n1.1 Related Work\nOur problem has connections with multi-task learning (MTL) and transfer learning. In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks. In transfer learning, knowledge from one source task is transferred to another target task either with or without additional training data for the target task [5]. Multi-task learning could be applied to our problem by running a MTL algorithm each time a new task arrives, while remembering all prior tasks. However, this approach incurs a memory and computational burden. Transfer learning lacks the sequential nature of our problem. For multi-task and transfer learning, there are theoretical guarantees on regret for some algorithms [6].\nWe can also consider the concept drift problem in which we observe a stream of incoming data that potentially changes over time, and the goal is to predict some property of each piece of data as it arrives. After prediction, we incur a loss that is revealed to us. For example, we could observe a feature wn and predict the label yn as in [7]. Some approaches for concept drift use iterative algorithms such as SGD, but without specific models on how the data changes. As a result, only simulation results showing good performance are available. There are also some bandit approaches in which one of a finite number of predictors must be applied to the data as in [8]. For this approach, there are regret guarantees using techniques for analyzing bandit problems.\nAnother relevant model is sequential supervised learning (see [9]) in which we observe a stream of data consisting of feature/label pairs (wn,yn) at time n, with wn being the feature vector and yn being the label. At time n, we want to predict yn given xn. One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn−i,yn−i)}Li=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data. Another approach is to assume that there is an underlying hidden Markov model (HMM) [12]. The label yn represents the hidden state and the pair (wn,yn) represents the observation with yn being a noisy version of yn. HMM inference techniques are used to estimate yn.\n2 Adaptive Sequential Optimization With ρ Known For analysis, we need the following assumptions on our functions fn(x) and the optimization algorithm:\nA.1 For the optimization algorithm under consideration, there is a function b(d0,Kn) such that\nE [ fn(xn)]− fn(x∗n)≤ b(d0,Kn)\nwith Kn the number of samples from pn and E‖xn(0)− x∗n‖2 ≤ d0, where xn(0) is the initial point of the optimization algorithm at time n. Finally, b(d0,Kn) is non-decreasing in d0.\nA.2 Each loss function ℓ(x,z) is differentiable in x. Each fn(x) is strongly convex with parameter m, i.e.,\nfn(y)≥ fn(x)+ 〈∇x fn(x),y−x〉+ 1 2 m‖y−x‖2\nA.3 diam(X )<+∞\nA.4 We can find initial points x1 and x2 that satisfy the excess risk criterion with ε1 and ε2 known, i.e.,\nE [ fi(xi)]− fi(x∗i )≤ εi i = 1,2\nRemarks: For assumption A.1 , we assume that the bound b(d0,Kn) depends on the number of samples Kn and not the number of iterations. For SGD, generally the number of iterations equals Kn as each sample is used to produce a noisy gradient. In addition, we often set xn(0) = xn−1. See Appendix A for a discussion of useful b(d0,Kn) bounds. For assumption A.4 , we can fix Ki and set εi = b(diam(X )2,Ki) for i = 1,2.\nNow, we examine the case when the change in minimizers, ρ in (2) or (3), is known. For the analysis of the section, whether (2) or (3) holds does not affect the analysis. Later we will estimate ρ and in this case whether (2) or (3) holds matters substantially.\nWe want to find a bound εn on the excess risk at time n in terms of Kn and ρ , i.e., εn such thatE[ fn(xn)]− fn(x∗n)≤ εn. The idea is to start with the bounds from assumption A.4 and proceed inductively using the previous εn−1 and ρ from (2). Suppose that εn−1 bounds the excess risk at time n− 1. Using the triangle inequality, strong convexity, and (2) we have\nE‖xn−1 −x∗n‖2 ≤ ( ‖xn−1 −x∗n−1‖+ ‖x∗n−x∗n−1‖ )2\n≤ ( √\n2 m E [ fn−1(xn−1)]− fn−1(x∗n−1)+ ‖x∗n −x∗n−1‖\n)2\n≤ ( √\n2εn−1 m +ρ\n)2\n(4)\nIn comparison, we could use the estimate diam2(X ) to bound E‖xn−1 −x∗n‖2 and select Kn. If the bound in (4) is much smaller than diam(X )2, then we need significantly fewer samples Kn to guarantee a desired excess risk. Now, by using the bound b(d0,Kn) from assumption A.1 , we can set\nεn = b\n\n\n(\n√\n2εn−1 m +ρ\n)2\n,Kn\n\n ∀n ≥ 3\nwhich yields a sequence of bounds on the excess risk. Note that this recursion only relies on the immediate past at time n− 1 through εn−1. To achieve εn ≤ ε for all n, we set\nK1 = min{K ≥ 1 | b ( diam(X )2,K ) ≤ ε}\nand Kn = K∗ for n ≥ 2 with\nK∗ = min\n\n\n\nK ≥ 1 ∣ ∣ ∣\n∣ ∣\nb\n\n\n(\n√\n2ε m +ρ\n)2\n,K\n\n≤ ε\n\n\n\n(5)\n3 Estimating ρ In practice, we do not know ρ , so we must construct an estimate ρ̂n using the samples from each distribution pn. We introduce two approaches to estimate ρ at one time step, ‖x∗i −x∗i−1‖, and methods to combine these estimates under assumptions (2) and (3). We show that for our estimate ρ̂n and appropriately chosen sequences {tn} for all n large enough ρ̂n + tn ≥ ρ almost surely. With this property, analysis similar to that in Section 2 holds.\n3.1 Allowed Ways to Choose Kn One of the sources of difficulty in estimating ρ is that we will allow Kn to be selected in a data dependent way, so Kn is itself a random variable. We make the assumption that Kn is selected using only information available at the end of time n− 1. To make this precise we define a filtration of sigma algebras to describe the available information. First, we define the sigma algebra K0 containing all the information on the initial conditions of our algorithm. For example, we may start at a random point x0 and then\nK0 = σ(x0)\nThe sigma algebra K0 may also contain information about K1 and K2. Next, we define the filtration\nKn = σ ( {zn(k)}Knk=1 ) ∨Kn−1 ∀n ≥ 1 (6)\nwhere F ∨G = σ (F ∪G )\nis the merge operator for sigma algebras. The sigma algebra Kn contains all the information available to us at the end of time n. We assume that Kn is Kn−1-measurable to capture the idea that Kn is chosen only using information available at the end of time n− 1.\n3.2 Estimating One Step Change\nFirst, we estimate the one step changes ‖x∗i −x∗i−1‖ denoted by ρ̃i. Implicitly, we assume that all one step estimates are capped by diam(X ), since trivially ‖x∗n −x∗n−1‖ ≤ diam(X ).\n3.2.1 Direct Estimate\nFirst, we construct an estimate ρ̃i of the one step changes ‖x∗i −x∗i−1‖. Using the triangle inequality and variational inequalities from [13] yields\n‖x∗i −x∗i−1‖ ≤ ‖xi −xi−1‖+ ‖xi−x∗i ‖+ ‖xi−1−x∗i−1‖\n≤ ‖xi −xi−1‖+ 1 m ‖∇x fi(xi)‖+ 1 m ‖∇x fi(xi−1)‖\nWe then approximate ‖∇x fi(xi)‖= ‖Ezi∼pi [∇xℓ(xi,zi)]‖ by ∥\n∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n∇xℓ(xi,zi(k)) ∥ ∥ ∥\n∥\nto yield the following estimate that we call the direct estimate:\nρ̃i , ‖xi −xi−1‖+ 1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ ∥ + 1 m ∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 ∇xℓ(xi−1,zi−1(k)) ∥ ∥ ∥ ∥ ∥\n3.2.2 Vector Integral Probability Metric Estimate\nGiven a class of functions F where each f ∈F maps Z →R, an integral probability metric (IPM) [14] between two distributions p and q is defined to be\nγF (p,q), sup f∈F\n∣ ∣Ez∼p[ f (z)]−Ez̃∼q[ f (z̃)] ∣ ∣\nWe consider an extension of this idea, which we call a vector IPM, in which the class of functions F maps Z → X :\nγVF (p,q), sup f∈F ‖Ez∼p[ f (z)]−Ez̃∼q[ f (z̃)]‖ (7)\nLemma 1 shows that a vector IPM can be used to bound the change in minimizer at time i and follows from variational inequalities in [13] and the assumption that {∇xℓ(x, ·) : x ∈ X } ⊂ F .\nLemma 1. Assume that {∇xℓ(x, ·) : x ∈ X } ⊂ F . Then ‖x∗i −x∗i−1‖ ≤ 1m γVF (pi, pi−1).\nProof. By exploiting variational inequalities from [13], we can show that\n‖x∗i −x∗i−1‖ ≤ 1 m ‖∇x fi(x∗i−1)−∇x fi−1(x∗i−1)‖\n= 1 m ‖Ezi∼pi [ ∇xℓ(x∗i−1,zi) ] −Ezi−1∼pi−1 [ ∇xℓ(x∗i−1,zi−1) ] ‖\nBy assumption {∇xℓ(x∗i−1, ·) : x ∈ X } ⊂ F , so ‖∇x fi(x∗i−1)−∇x fi−1(x∗i−1)‖ = ‖Ezi∼pi [ ℓ(x∗i−1,zi) ] −Ezi−1∼pi−1 [ ℓ(x∗i−1,zi−1) ]\n‖ ≤ sup\nf∈F ‖Ezi∼pi [ f (zi)]−Ezi−1∼pi−1 [ f (zi−1)]‖\n= γVF (pi, pi−1)\nWe cannot compute this vector IPM, since we do not know the distributions pi and pi−1. Instead, we plug in the empiricals p̂i and p̂i−1 to yield the estimate 1m γ V F (p̂i, p̂i−1). This estimate is biased upward, which ensures that ‖x∗i −x∗i−1‖ ≤ E [ 1 m γ V F (p̂i, p̂i−1) ]\n. Our estimate is still not in a closed form since there is a supremum over F in the computation of γV\nF (p̂i, p̂i−1).\nFor the class of functions F = { f ∣ ∣ ‖ f (z)− f (z̃)‖ ≤ r(z, z̃) } . (8)\nwe can compute an upper bound Γi on γVF (p̂i, p̂i−1) yielding a computable estimate ρ̃i = 1 m Γi. Set z̃i(k) = zi(k) if 1 ≤ k ≤ Ki and z̃i(k) = zi−1(k) if Ki + 1 ≤ k ≤ Ki +Ki−1. From (7), we have\nγVF (p̂i, p̂i−1) = sup f∈F\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 f (z̃i(k))− 1 Ki−1 Ki−1 ∑ k=1 f (z̃i(Ki + k)) ∥ ∥ ∥ ∥ ∥\nWe can relax this supremum by maximizing over the function value f (z̃i(k)) denoted by αk in the following nonconvex quadratically constrained quadratic program (QCQP):\nmaximize\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 αk − 1 Ki−1 Ki−1 ∑ k=1 αKi+k ∥ ∥ ∥ ∥ ∥\nsubject to ‖αk −α j‖ ≤ r(z̃i(k), z̃i( j)) ∀k < j\nThe constraints are imposed to ensure that the function values αk can correspond to a function in F from (8). The value of this QCQP exactly may not equal the vector IPM but at least provides an upper bound. Finally, we note that this QCQP can be converted to its dual form to yield an SDP, which is often easier to solve.\n3.2.3 Comparison of Estimates\nThe direct estimate is easier to compute but may be loose if ‖xn −x∗n‖ is large. If ‖xn −x∗n‖ is large, then the vector IPM approach is in general tighter. However, the vector IPM is more difficult to compute due to need to solve a QCQP or SDP and check the inclusion conditions in Lemma 1. Also, the number of constraints in the QCQP or SDP grows quadratically in the number of samples.\n3.3 Combining One Step Estimates For Constant Change\nAssuming that ‖x∗i −x∗i−1‖= ρ from (2), we average the one step estimates ρ̃i to yield a better estimate\nρ̂n = 1 n− 1 n ∑ i=2 ρ̃i\nof ρ at each time n under (2). To analyze the behavior of our combined estimates, we use sub-Gaussian concentration inequalities detailed in Appendix B. Lemma 22 is of particular importance to our analysis.\n3.3.1 Direct Estimate\nThe difficulty in analyzing the direct estimate comes because in approximating 1m‖∇ fi(xi)‖ by\n1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ ∥\nxi is dependent on all the samples {zi(k)}Kik=1. To illustrate the problem further, consider drawing two independent copies {zi(k)}Kik=1 iid∼ pi and {z̃i(k)}Kik=1 iid∼ pi of the samples. Suppose that we use the second copy {z̃i(k)}Kik=1 to compute xi using our optimization algorithm of choice starting from xi−1. Then we approximate 1m‖∇ fi(xi)‖ by\n1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ ∥\nNow, since xi is independent of {zi(k)}Kik=1 the quantity\n1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ ∥\nis the norm of an average of independent random variables conditioned on xi. This allows us to apply standard concentration inequalities for norms of random variables as in [15]. In this section, we argue that re-using the samples {zi(k)}Kik=1 to compute xi is not too far from using a second independent draw {z̃i(k)} Ki k=1.\nFor analysis, we need the following additional assumptions:\nB.1 The loss function ℓ(x,z) has uniform Lipschitz continuous gradients in x with modulus L, i.e.\n‖∇xℓ(x,z)−∇xℓ(x̃,z)‖ ≤ L‖x− x̃‖ ∀z ∈ Z\nB.2 Assuming X is d-dimensional, each component j of the gradient error ∇xℓ(x,zn)− fn(x) satisfies\nE\n[\nexp { s(∇xℓ(x,zn)−∇ fn(x)) j }\n∣ ∣ ∣ ∣ x ] ≤ exp { 1 2 Cg d2 s2 }\nAssumption B.1 is reasonable if the space Z containing z is compact. Although in practice, the distribution of gradient error could depend on x, we assume that the bound Cg does not depend on x. We can view this as a pessimistic assumption corresponding to choosing the worst case bound as a function of x and the resulting Cg. This is a common assumption for in high probability analysis of optimization algorithms as in [16] for example.\nTo proceed, we first define two other useful estimates for ρ . As discussed before, suppose that we make a second independent draw of samples {z̃i(k)}Kik=1 from pi. We use these samples to compute x̃i in the same manner as xi starting from xi−1 except with {z̃i(k)}Kik=1 used in place of {zi(k)} Ki k=1. Then define\nρ̃ (2)i , ‖x̃i − x̃i−1‖+ 1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(x̃i,zi(k)) ∥ ∥ ∥ ∥ ∥ + 1 m ∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 ∇xℓ(x̃i−1,zi−1(k)) ∥ ∥ ∥ ∥ ∥\nThis is the same form as the direct estimate with x̃i in place of xi. Next, define\nρ̃ (3)i , ‖x̃i − x̃i−1‖+ 1 m ‖∇ fi(xi)‖+ 1 m ‖∇ fi−1(xi−1)‖\nThis is in fact the bound that inspired the direct estimate. We also define the averaged estimates\nρ̂ (2)n , 1 n− 1 n ∑ i=2 ρ̃ (2)i\nand\nρ̂ (3)n , 1 n− 1 n ∑ i=2 ρ̃ (3)i\nWe know that ρ̂ (3)n ≥ ρ . Thus, if we can control the gap between the pair ρ̂n and ρ̂ (2)n and the pair ρ̂ (2)n and ρ̂ (3)n , then we can ensure that ρ̂n plus an appropriate constant upper bounds ρ for all n large enough as desired.\nFirst, we show that ρ̂ (2)n upper bounds ρ eventually.\nLemma 2. Suppose that the following conditions hold:\n1. B.1 -B.2 hold\n2. The sequence {tn} satisfies ∞\n∑ n=2 exp\n{\n− (n− 1)m 2t2n\n72Cg\n}\n< ∞\nThen for all n large enough it holds that ρ̂ (2)n + Ĉ (2) n + tn ≥ ρ almost surely with\nĈ(2)n , 1\ndm(n− 1)\n(\n√\nCg K1 + 2 n ∑ i=1\n√\nCg Ki +\n√\nCg Kn\n)\nProof. First, we have by the triangle equality and reverse triangle inequality\nm|ρ̃ (2)i − ρ̃ (3) i |\n=\n∣ ∣ ∣ ∣ ∣ (∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(x̃i,zi(k)) ∥ ∥ ∥ ∥ ∥ −‖∇x fi(x̃i)‖ ) + (∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 ∇xℓ(x̃i−1,zi−1(k)) ∥ ∥ ∥ ∥ ∥ −‖∇x fi−1(x̃i−1)‖ )∣ ∣ ∣ ∣ ∣\n≤ ∣ ∣ ∣\n∣ ∣\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(x̃i,zi(k)) ∥ ∥ ∥ ∥ ∥ −‖∇x fi(x̃i)‖ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 ∇xℓ(x̃i−1,zi−1(k)) ∥ ∥ ∥ ∥ ∥ −‖∇x fi−1(x̃i−1)‖ ∣ ∣ ∣ ∣ ∣\n≤ ∥ ∥ ∥\n∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n+\n∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 (∇xℓ(x̃i−1,zi−1(k))−∇x fi−1(x̃i−1)) ∥ ∥ ∥ ∥ ∥\nThen by the triangle inequality, we have\n|ρ̂ (2)n − ρ̂ (3)n | ≤ 1 m(n− 1) n ∑ i=2\n(∥\n∥ ∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n+\n∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 (∇xℓ(x̃i−1,zi−1(k))−∇x fi−1(x̃i−1)) ∥ ∥ ∥ ∥ ∥ )\n≤ 1 m(n− 1)\n(∥\n∥ ∥ ∥ ∥ 1 K1\nK1\n∑ k=1\n(∇xℓ(x̃1,z1(k))−∇x f1(x̃1)) ∥ ∥ ∥ ∥\n∥\n+2 n−1 ∑ i=2\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥\n+\n∥ ∥ ∥ ∥ ∥ 1 Kn Kn ∑ k=1 (∇xℓ(x̃n,zn(k))−∇x fn(x̃n)) ∥ ∥ ∥ ∥ ∥ )\n(9)\nWe will analyze the behavior of this bound on |ρ̂ (2)i − ρ̂ (3) i | using Lemma 22 in Appendix B. Define the filtration\nFi = σ\n(\ni ⋃\nj=1\n{z j(k)}K jk=1 ∪ i+1 ⋃\nj=1\n{z̃ j(k)}K jk=1\n)\n∨K0 i = 0, . . . ,n (10)\nwith K0 from (6). Note that Ki−1 ⊂ Fi−1, so Ki is Fi−1-measurable. In addition, x̃i but not xi is Fi−1-measurable. Define the random variables\nVi =\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥ −E [∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥ ∣ ∣ ∣ ∣ ∣ Fi−1 ] i = 1, . . . ,n\nClearly, Vi is Fi-measurable, since Vi is a function of x̃i, Ki, and {zi(k)}Kik=1 all of which are Fi-measurable. Conditioned on Fi−1, the sum\n1 Ki\nKi\n∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) (11)\nis a sum of iid random variables. We now work with the conditional measure P{· | Fi−1} to compute sub-Gaussian norms of (11) define in (24) and (25) of Appendix B. By assumption B.2 , we have\nτ2 ( (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) j ) ≤ Cg d2\nTherefore, applying Lemma 24 yields\nB\n(\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ) ≤ √\nCg Ki\ndue to the independence conditioned on Fi−1. By applying Lemma 25 from [17] to the conditional distribution P{·|Fi−1}, we have\nP\n{∥\n∥ ∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n> t\n∣ ∣ ∣ ∣ ∣ Fi−1 } ≤ 2exp { − t 2 2( √ Cg/Ki)2 }\n= 2exp\n{\n−Kit 2\n2Cg\n}\nSince\nE\n[∥\n∥ ∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n∣ ∣ ∣ ∣ ∣ Fi−1 ] ≥ 0,\nwe have\nP\n{\nVi > t\n∣ ∣ ∣ ∣ ∣ Fi−1 }\n= P\n{∥\n∥ ∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n−E [∥ ∥ ∥\n∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n∣ ∣ ∣ ∣ ∣ Fi−1 ] > t ∣ ∣ ∣ ∣ ∣ Fi−1 }\n≤ P {∥ ∥ ∥\n∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n> t\n∣ ∣ ∣ ∣ ∣ Fi−1 }\n≤ 2exp { −Kit 2\n2Cg\n}\n≤ 2exp { − t 2\n2Cg\n}\nSince E[Vi | Fi−1] = 0, we can apply Lemma 26 with c = 1/(2Cg) to yield\nE [ esVi ∣ ∣ Fi−1 ] ≤ exp { 1 2 (18Cg)s 2 }\nThis shows that the collection of random variables {Vi}ni=1 and the filtration {Fi}ni=0 satisfies the conditions of Lemma 22. Before applying Lemma 22, we bound the conditional expectations\nE\n\n\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥ 2 ∣ ∣ ∣ ∣ ∣ Fi−1\n\n\nBy a straightforward calculation conditioned on Fi−1, we have\nE\n\n\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥ 2 ∣ ∣ ∣ ∣ ∣ Fi−1\n\n\n= 1\nK2i\nKi\n∑ k=1\nKi ∑ j=1 E [〈∇xℓ(x̃i,zi(k))−∇x f (x̃i),∇xℓ(x̃i,zi( j))−∇x f (x̃i)〉 | Fi−1]\n= 1\nK2i\nKi\n∑ k=1 E [ ‖∇xℓ(x̃i,zi(k))−∇x f (x̃i)‖2 | Fi−1 ]\n(a) =\n1\nK2i\nKi\n∑ k=1\nd\n∑ q=1 E [ (∇xℓ(x̃i,zi(k))−∇x f (x̃i))2q | Fi−1 ]\n(b) ≤ 1\nK2i\nKi\n∑ k=1\nd Cg d2\n≤ Cg dKi\nwhere (a) is a decomposition into each component of the vector and (b) follows since a centered sub-Gaussian random variable with parameter Cg/d2 satisfies\nE [ (∇xℓ(x̃i,zi(k))−∇x f (x̃i))2q | Fi−1 ] ≤ Cg d2\nThen by Jensen’s inequality\nE\n[∥\n∥ ∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n∣ ∣ ∣ ∣ ∣ Fi−1 ] ≤ √ Cg dKi\nDefine the constants\na1 = an = 1\nm(n− 1)\na2 = · · ·= an−1 = 2\nm(n− 1)\nresulting in\n‖a‖22 = 2\nm2(n− 1)\nUsing the bound in (9) and Lemma 22 from Appendix B with this choice of a, it holds that\nP\n{\n|ρ̂ (2)n − ρ̂ (3)n |> n\n∑ i=1 ai\n√\nCg dKi + t\n}\n≤ P { n\n∑ i=1 ai\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥\n> n\n∑ i=1 aiE\n[∥\n∥ ∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n(∇xℓ(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥\n∥\n∣ ∣ ∣ ∣ ∣ Fi−1 ] + t }\n= P\n{\nn\n∑ i=1 aiVi > t\n}\n≤ exp { −m 2(n− 1)t2\n72Cg\n}\nCombining this bound with ρ̂ (3)n ≥ ρ yields\n∞\n∑ n=2 P\n{\nρ̂ (2)n < ρ − n\n∑ i=1 ai\n√\nCg dKi\n− tn } ≤ ∞\n∑ n=2 P\n{\nρ̂ (2)n < ρ̂ (3) n −\nn\n∑ i=1 ai\n√\nCg dKi\n− tn }\n≤ ∞\n∑ n=2 P\n{\n|ρ̂ (2)n − ρ̂ (3)n |> n\n∑ i=1 ai\n√\nCg dKi + tn\n}\n≤ ∞\n∑ n=2 exp\n{\n−m 2(n− 1)t2n\n72Cg\n}\n< ∞\nThe result follows from the Borel-Cantelli lemma. Note that as claimed\nĈ(2)n = 1\ndm(n− 1)\n(\n√\nCg K1 + 2 n−1 ∑ i=2 √ Cg Ki + √ Cg Kn\n)\nNext, we show that ρ̂n upper bounds ρ̂ (2) n eventually with a general assumption on the optimization algorithm.\nWhen the conditions of Lemmas 2 and 3 are satisfied, it holds that ρ̂n plus a constant upper bounds ρ .\nLemma 3. Suppose the following conditions hold:\n1. B.1-B.2 hold\n2. There exist bounds E [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ] ≤C(Ki) i = 1, . . . ,n\n3. The sequence {tn} satisfies ∞\n∑ n=2 exp\n{\n− (n− 1) 2t2n\n2n ( 1+ Lm )2 diam2(X )\n}\n<+∞\nThen for all n large enough it holds that ρ̂n + Ĉn + tn ≥ ρ̂ (2)n almost surely with\nĈn ,\n( 1+ Lm )\nn− 1\n(\nC(K1)+ 2 n−1 ∑ i=2 C(Ki)+C(Kn)\n)\nProof. We have by the triangle inequality, reverse triangle inequality, and the Lipschitz continuity of ∇xℓ(x,z) in x from assumption B.1\n|ρ̃i − ρ̃ (2)i | ≤ ∣ ∣‖xi −xi−1‖−‖x̃i− x̃i−1‖ ∣ ∣\n+\n∣ ∣ ∣ ∣ ∣ 1 m ∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ ∥ − 1 m ∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(x̃i,zi(k)) ∥ ∥ ∥ ∥ ∥ ∣ ∣ ∣ ∣ ∣\n+\n∣ ∣ ∣ ∣ ∣ 1 m ∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 ∇xℓ(xi−1,zi−1(k)) ∥ ∥ ∥ ∥ ∥ − 1 m ∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 ∇xℓ(x̃i−1,zi−1(k)) ∥ ∥ ∥ ∥ ∥ ∣ ∣ ∣ ∣ ∣\n≤ ‖(xi − x̃i)− (xi−1 − x̃i−1)‖\n+ 1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(xi,zi(k))−∇xℓ(x̃i,zi(k))) ∥ ∥ ∥ ∥ ∥\n+ 1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki−1 Ki−1 ∑ k=1 (∇xℓ(xi−1,zi−1(k))−∇xℓ(x̃i−1,zi−1(k))) ∥ ∥ ∥ ∥ ∥\n≤ ( 1+ L m ) (‖xi − x̃i‖+ ‖xi−1− x̃i−1‖)\nso\n|ρ̂n − ρ̂ (2)n | ≤ 1 n− 1 n ∑ i=2 |ρ̃i − ρ̃ (2)i |\n≤ ( 1+ Lm ) n− 1 n ∑ i=2 (‖xi − x̃i‖+ ‖xi−1− x̃i−1‖)\n=\n( 1+ Lm )\nn− 1\n(\n‖x1 − x̃1‖+ 2 n−1 ∑ i=2 ‖xi − x̃i‖+ ‖xn− x̃n‖ )\nWe will again apply Lemma 22 of Appendix B to analyze this upper bound using the sigma algebra\nFi = σ\n(\ni ⋃\nj=1\n{z j(k)}K jk=1 ∪ i ⋃\nj=1\n{z̃ j(k)}K jk=1\n)\n∨K0 i = 0, . . . ,n (12)\nDefine the random variable Vi = ‖xi − x̃i‖−E [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ]\nClearly, Vi is Fi-measurable. Since −diam(X )≤Vi ≤ diam(X ),\nand E [Vi | Fi−1] = 0, we can apply the conditional version Hoeffding’s Lemma from Lemma 23 to yield\nE [ esVi ∣ ∣ Fi−1 ] ≤ exp { 1 2 diam2(X )s2 }\nThe collection of random variables {Vi}ni=1 and the filtration {Fi}ni=0 satisfy the conditions of Lemma 22. Before applying Lemma 22, we bound the conditional expectations\nE [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ]\nBy assumption, we have E [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ] ≤C(Ki) i = 1, . . . ,n\nand so (\n1+ Lm ) n− 1\n(\nE [ ‖x1 − x̃1‖ ∣ ∣ F0 ] + 2 n−1 ∑ i=2 E [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ] ‖+E [ ‖xn − x̃n‖ ∣ ∣ Fn−1 ]\n)\n≤ ( 1+ Lm )\nn− 1\n(\nC(K1)+ 2 n−1 ∑ i=2 C(Ki)+C(Kn)\n)\n, Ĉn\nSet\na1 = an =\n( 1+ Lm )\nn− 1 and\na2 = · · ·= an−1 = ( 1+ Lm )\nn− 1 resulting in\n‖a‖22 = n ( 1+ Lm )2\n(n− 1)2\nApplying our bound in (12) and Lemma 22 with this choice of a yields\nP\n{ |ρ̂n − ρ̂ (2)n |> Ĉn + t }\n≤ P { ( 1+ Lm )\nn− 1\n(\n‖x1 − x̃1‖+ 2 n−1 ∑ i=2 ‖xi − x̃i‖+ ‖xn− x̃n‖ )\n>\n( 1+ Lm )\nn− 1\n(\nE [ ‖x1 − x̃1‖ ∣ ∣ F0 ] + 2 n−1 ∑ i=2 E [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ] ‖+E [ ‖xn − x̃n‖ ∣ ∣ Fn−1 ]\n)\n+ t\n}\n= P\n{\n( 1+ Lm )\nn− 1\n(\nV1 + 2 n−1 ∑ i=2 Vi +Vn\n)\n> t\n}\n= P\n{\nn\n∑ i=1 aiVi > t\n}\n≤ exp { − (n− 1) 2t2\n2n ( 1+ Lm )2 diam2(X )\n}\nFinally, we have\n∞\n∑ n=2 P\n{\nρ̂n < ρ̂ (2) n − Ĉn − tn\n} ≤ ∞\n∑ n=2 P\n{ |ρ̂n − ρ̂ (2)n |> Ĉn + tn }\n≤ ∞\n∑ n=2 exp\n{\n− (n− 1) 2t2n\n2n ( 1+ Lm )2 diam2(X )\n}\n<+∞\nThe claim follows from the Borel-Cantelli Lemma.\nIf Lemmas 2 and 3 hold for the sequence {tn/2}, then for all n large enough it holds that\nρ̂n + Ĉn + Ĉ (2) n + tn ≥ ρ\nalmost surely.\nLemma 4. It always holds that\nE [ ‖xi − x̃i‖ ∣ ∣ Fi−1 ] ≤ 2 √\n1 m b ( diam2(X ),Ki )\nTherefore, the choice\nC(Ki), 2\n√\n2 m b ( diam2(X ),Ki )\nsatisfies the conditions of Lemma 3.\nProof. Using the sigma algebras defined in (12) yields\nE [‖xi − x̃i‖ | Fi−1] ≤ E [‖xi−x∗i ‖ | Fi−1]+E [‖x̃i −x∗i ‖ | Fi−1]\n≤ E [ √\n2 m ( fi(xi)− fi(x∗i )) | Fi−1\n]\n+E\n[\n√\n2 m ( fi(x̃i)− fi(x∗i )) | Fi−1\n]\n≤ √\n2 m E [( fi(xi)− fi(x∗i )) | Fi−1]+\n√\n2 m E [( fi(x̃i)− fi(x∗i )) | Fi−1]\n≤ 2 √\n2 m b(diam2(X ),Ki)\nwhere the third inequality follows from Jensen’s inequality.\nThis choice of C(Kn)works for any algorithm with the associated b(d0,K). For any particular algorithm, we believe that we can produce tighter bounds independent of diam(X ) by copying the Lyapunov analysis used to analyze SGD as in Appendix A. The analysis becomes algorithm dependent in this case and is omitted.\nFinally, we state an overall theorem for the direct estimate that gives general combined conditions under which ρ̂n upper bounds ρ .\nTheorem 1. If B.1 -B.2 hold and the sequence {tn} satisfies ∑∞n=2 e−Cnt 2 n < ∞ for all C > 0, then for a sequence of constants {Cn} and for all n large enough it holds that ρ̂n +Cn + tn ≥ ρ almost surely.\nProof. Combine Lemmas 2 and 3 to yield the result with\nCn = Ĉn + Ĉ (2) n\n3.3.2 Vector IPM Estimate\nWe first derive a version of Hoeffding’s inequality that allows for some dependence among the random variables. We use this concentration inequality to analyze ρ̂n for the IPM estimate. Given an integer W , we construct a cover of {1,2, . . . ,n} by dividing the set into W groups of integers spaced by W , i.e.,\nA j =\n{\nj, j+W, j+ 2W . . . , j+\n⌊\nn− j W\n⌋\nW\n}\nj = 1, . . . ,W (13)\nNote that\n{1,2, . . . ,n}= W ⋃\nj=1\nA j\nand Ai ∩A j = /0 for i 6= j. The proof of Lemma 5 is nearly identical to the proof of the extension of Hoeffding’s inequality from [18] with Lemma 22 used instead. We assume that if we refer to a filtration Fi with i < 0, then we implicitly refer to F0.\nLemma 5 (Dependent Hoeffding’s Inequality). Suppose we are given a collection of random variable {Vi}ni=1 and a filtration {F}ni=0 such that\n1. ai ≤Vi ≤ bi for constants ai and bi i = 1, . . . ,n\n2. Vi is Fi-measurable i = 1, . . . ,n\n3. Given an integer W and a cover {A j}Wj=1 as in (13) for each j it holds that\nE\n[ V j+iW ∣ ∣ ∣ F j+(i−1)W ] = 0 i = 1, . . . ,\n⌊\nn− j W\n⌋\nand E [ V j ∣ ∣ ∣ F0 ] = 0\nThen it holds that\nP\n{\nn\n∑ i=1 Vi > t\n}\n≤ exp { − 2t 2 W ∑ni=1(bi − ai)2 }\nand\nP\n{\nn\n∑ i=1\nVi <−t } ≤ exp { − 2t 2 W ∑ni=1(bi − ai)2 }\nProof. Define\nU j ,\n⌊\nn− j W\n⌋\n∑ i=0 V j+iW\nfor j = 1, . . . ,W . Let {p j}Wj=1 be a probability distribution on {1, . . . ,W} to be specified later. By Jensen’s inequality, we have\nexp\n{\ns n\n∑ i=1 Vi\n}\n= exp\n{\nW ∑ j=1 p j s p j U j\n}\n≤ W\n∑ j=1 p j exp\n{\ns p j U j\n}\nThen it holds that\nE\n[\nexp\n{\ns n\n∑ i=1 Vi\n}]\n≤ W\n∑ j=1 p jE\n[\nexp\n{\ns p j U j\n}]\nNow consider one term\nE\n[\nexp\n{\ns p j U j\n}]\n= E\n\n  exp\n\n \n \ns p j\n⌊\nn− j W\n⌋\n∑ i=0 V j+iW\n\n \n \n\n \nSince a j+iW ≤V j+iW ≤ b j+iW and E [ V j+iW ∣ ∣ ∣ F j+(i−1)W ] = 0,\nwe can apply the conditional version Hoeffding’s Lemma from Lemma 23 to yield\nE [ esV j+iW ∣ ∣ F j+(i−1)W ] ≤ exp { 1 8 (b j+iW − a j+iW )2 s2 }\nThen we can apply Lemma 22 to {V j+iW} ⌊ n− j W ⌋ i=0 and {F j+iW} ⌊ n− j W ⌋ i=0 to yield\nE\n[\nexp\n{\ns p j U j\n}]\n≤ exp\n\n \n \ns2\n8p2j\n⌊\nn− j W\n⌋\n∑ i=0 (b j+iW − a j+iW )2\n\n \n \n=\n⌊\nn− j W\n⌋\n∏ i=0 exp\n{\ns2\n8p2j (bα − aα)2\n}\nThen we have\nE\n[\nexp\n{\ns n\n∑ i=1 Vi\n}]\n≤ W\n∑ j=1 p j\n⌊\nn− j W\n⌋\n∏ i=0 exp\n{\ns2\n8p2j (bα − aα)2\n}\n= W\n∑ j=1 p j exp\n{\ns2c j 8p2j\n}\nwith\nc j =\n⌊\nn− j W\n⌋\n∑ i=0 (b j+iW − a j+iW )2\nLet p j = √ c j/T and\nT = W\n∑ j=1\n√ c j.\nTherefore, we have\nE\n[\nexp\n{\ns n\n∑ i=1 Vi\n}]\n≤ exp { 1 8 T 2s2 }\nApplying the Chernoff bound [19] and optimizing yields\nP\n{\nn\n∑ i=1 Vi > t\n}\n≤ exp { −2t2/T 2 }\nBounding T with Cauchy-Schwarz yields\nT 2 ≤ ( W\n∑ j=1 1\n)(\nW ∑ j=1 c j\n)\n=W n\n∑ i=1 (bi − ai)2\nand the results follows. The proof for the other tail is nearly identical.\nIf we do not have the condition 3 of Lemma 5, then it holds that\nP\n\n \n \nn\n∑ i=1\nVi > W\n∑ j=1\n⌊\nn− j W\n⌋\n∑ i=0 E [ V j+iW ∣ ∣ F j+(i−1)W ] + t\n\n \n \n≤ exp { − 2t 2 W ∑ni=1(bi − ai)2 }\nIf we can bound the conditional expectation\nE [ V j+iW ∣ ∣ F j+(i−1)W ] ≤C j+iW ,\nby a F j+(i−1)W -measurable random variable, then we have\nP\n{\nn\n∑ i=1\nVi > n\n∑ i=1 Ci + t\n}\n= P\n\n \n \nn\n∑ i=1\nVi > W\n∑ j=1\n⌊\nn− j W\n⌋\n∑ i=0 C j+iW + t\n\n \n \n≤ P\n\n \n \nn\n∑ i=1\nVi > W\n∑ j=1\n⌊\nn− j W\n⌋\n∑ i=0 E [ V j+iW ∣ ∣ F j+(i−1)W ] + t\n\n \n \n≤ P\n\n \n \nW ∑ j=1\n⌊\nn− j W\n⌋\n∑ i=0\n( V j+iW −E [ V j+iW ∣ ∣ F j+(i−1)W ]) > t\n\n \n \n≤ exp { − 2t 2 W ∑ni=1(bi − ai)2 }\nWe have the following lemma characterizing the performance of the IPM estimate.\nLemma 6. For the IPM estimate and any sequence {tn} such that ∞\n∑ n=2 exp\n{ − nt 2 n\n4diam(X )2\n}\n< ∞\nfor all n large enough it holds that ρ̂n + tn ≥ ρ almost surely.\nProof. Define the random variables Vi = ρ̃i −E [ρ̃i | Ki−2]\nwith {Ki}ni=1 defined in (6). We have −diam(X )≤Vi ≤ diam(X )\nClearly, Vi is Ki-measurable and E[Vi | Ki−2] = 0. Now, we can apply Lemma 5 with W = 2 to yield\nP\n{\nn\n∑ i=1\nVi <−nt } ≤ exp { − 2(nt) 2\n(2) ( 4ndiam2(X ) )\n}\n= exp\n{\n− nt 2\n4diam2(X )\n}\nNone of the random variables {zi(k)}Kik=1 and {zi−1(k)} Ki−1 k=1 are Ki−2 measurable. Also, regardless of how many\nsamples Ki and Ki−1 are taken, the IPM estimate is biased upward. Thus, it holds that\nE [ρ̃i | Ki−2]≥ ρ\nTherefore, it follows that\nP{ρ̂n < ρ − t} ≤ P { n\n∑ i=1\nρ̃i < n\n∑ i=1\nE [ρ̃i | Ki−2]− nt }\n= P\n{\nn\n∑ i=1\nVi <−nt }\n≤ exp { − nt 2\n4diam2(X )\n}\nNote that we pay a price of two in the exponent due to ρ̃i and ρ̃i−1 both depending on the samples from pi−1. Since\n∞\n∑ n=2 exp\n{\n− nt 2 n\n4diam(X )2\n}\n< ∞\nit follows that ∞\n∑ n=2 P{ρ̂n + t < ρ}<+∞,\nThis in turn guarantees by way of the Borel-Cantelli Lemma that for n large enough\nρ̂n + tn ≥ ρ\nalmost surely.\n3.4 Combining One Step Estimates For Bounded Change\nWe now look at estimating ρ in the case that ‖x∗n −x∗n−1‖ ≤ ρ .\nWe set ρi , ‖x∗i −x∗i−1‖\nB.3 Assume that we have estimators ĥW : RW → R such that\n1. E[ĥW (ρ j, . . . ,ρ j−W+1)]≥ ρ for all j ≥ 1 and W ≥ 1 2. For any random variables {ρ̃i} such that E[ρ̃i]≥ E[ρi], we have\nE [ ĥW (ρ̃ j, . . . , ρ̃ j−W+1) ] ≥ E [ ĥW (ρ j, . . . ,ρ j−W+1) ]\nFor example, if ρi iid∼ Unif[0,ρ ], then\nĥW (ρi,ρi+1, . . . ,ρi+W−1) = W + 1\nW max{ρi,ρi+1, . . . ,ρi+W−1}\nis an estimator of ρ with the required properties. Also, note that the two conditions on the estimator in B.3 imply that\nE [ ĥW (ρ̃ j, . . . , ρ̃ j−W+1) ] ≥ E [ ĥW (ρ j, . . . ,ρ j−W+1) ] ≥ ρ\nGiven an estimator satisfying assumption B.3 , we compute\nρ̃ (i) = ĥW (ρ̃i, ρ̃i−1, . . . , ρ̃i−W+1)\nand set\nρ̂n = 1 n− 1 n ∑ i=2 ρ̃ (i) = 1 n− 1 n ∑ i=2 ĥmin{W,i−1}(ρ̃i, ρ̃i−1, . . . , ρ̃max{i−W+1,2}) (14)\nWe have\nE[ρ̂n] = 1 n− 1 n ∑ i=2 E[ρ̃ (i)]≥ ρ\nLemma 7 (IPM Single Step Estimates). For the estimator in (14) computed using the IPM estimate for ρ̃i and any sequence {tn} such that\n∞\n∑ n=2 exp\n{\n− 2(n− 1)t 2 n\n(W + 1)diam(X )2\n}\n< ∞\nit holds that for all n large enough ρ̂n + tn ≥ ρ almost surely.\nProof. We copy the proof of Lemma 6 with W +1 in place of 2 and note that ρ̃ (i) and ρ̃ ( j) with |i− j|>W +1 do not depend on the same samples. Lemma 5 and some simple algebra yields\nP{ρ̂n < ρ − t} ≤ exp { − 2(n− 1)t 2\n(W + 1)diam(X )2\n}\nWe pay a price of W + 1 in the denominator of the exponent due to the dependence of the ρ̃ (i). By the Borel-Cantelli Lemma, for all n large enough it holds that ρ̂n + tn ≥ ρ almost surely as long as\n∞\n∑ n=2 exp\n{\n− 2(n− 1)t 2 n\n(W + 1)diam(X )2\n}\n< ∞\nTo analyze the direct estimate, we need the following assumption\nB.4 Suppose that there exists absolute constants {bi}Wi=1 for any fixed W such that\n|ĥW (p1, . . . , pW )− ĥW (q1, . . . ,qW )| ≤ W\n∑ i=1 bi|pi − qi| ∀p,q ∈RW≥0\nFor the uniform case, we have ∣\n∣ ∣ W + 1 W max{p1, . . . , pW}− W + 1 W max{q1, . . . ,qW} ∣ ∣ ∣ ≤ W + 1 W max{|p1 − q1|, . . . , |pW − qW |}\n≤ W + 1 W\nW\n∑ i=1 |pi − qi|\nso\nb1 = · · ·= bW = W + 1\nW Under assumption B.4 , we can then show that\nρ̂n = 1 n−W n ∑ i=W+1 ρ̃ (i)\neventually upper bounds ρ by copying the proofs of the lemmas behind Theorem 1.\nLemma 8 (Direct Single Step Estimates). Suppose that the following conditions hold:\n1. B.1 -B.4 hold\n2. The sequence {tn} satisfies\n∞\n∑ n=W+1 exp\n\n \n \n− (n−W) 2t2n\n32n ( 1+ Lm )2 ( ∑Wj=1 b j )2 diam2(X )\n\n \n \n<+∞\nand\n∞\n∑ n=W+1 exp\n\n \n \n− (n−W) 2m2t2n\n144nCg ( ∑Wj=1 b j )2\n\n \n \n<+∞\n3. There are bounds C(K) such that E [‖xi − x̃i‖ | Fi−1]≤C(Ki)\nThen for all n large enough it holds that ρ̂n +Ûn + V̂n + tn ≥ ρ almost surely with\nÛn = 2 ( 1+ Lm ) ∑Wj=1 b j n−W n ∑ i=1 C(Ki)\nand\nV̂n = 2∑Wj=1 b j m(n−W) n ∑ i=1 √ Cg dKi\nProof. Define ρ̃ (2)i , ρ̃ (3) i , ρ̂ (2) i , and ρ̂ (3) i as in Lemmas 2 and 3. First, we have\n|ρ̂n − ρ̂ (3)n | ≤ 1 n−W n ∑ i=W+1 |ρ̃ (i)− ρ̃ (i)3 |\n≤ 1 n−W\nn\n∑ i=W+1\ni\n∑ j=i−W+1 b j|ρ̃ j − ρ̃ (3)j |\n≤ 1 n−W\nn\n∑ i=W+1\ni\n∑ j=i−W+1\nb j ( |ρ̃ j − ρ̃ (2)j |+ |ρ̃ (2) j − ρ̃ (3) j | )\n≤ ∑ W j=1 b j n−W n ∑ i=2 ( |ρ̃i − ρ̃ (2)i |+ |ρ̃ (2) i − ρ̃ (3) i | )\nSecond, define Ui , ‖xi − x̃i‖\nand\nVi ,\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇ fi(x̃i)) ∥ ∥ ∥ ∥ ∥\nThen we have\n|ρ̃i − ρ̃ (2)i | ≤ ‖xi − x̃i‖+ 1 m\n∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(xi,zi(k))−∇xℓ(x̃i,zi(k))) ∥ ∥ ∥ ∥ ∥\n≤ ( 1+ L m ) (Ui +Ui−1)\nand\n|ρ̃ (2)i − ρ̃ (3) i | ≤ 1 m (Vi +Vi−1)\nThen it follows that\n|ρ̂n − ρ̂ (3)n | ≤ ∑Wj=1 b j n−W n ∑ i=2 ( |ρ̃i − ρ̃ (2)i |+ |ρ̃ (2) i − ρ̃ (3) i | )\n≤ 2 ( 1+ Lm ) ∑Wj=1 b j n−W n ∑ i=1 Ui + 2∑Wj=1 b j m(n−W) n ∑ i=1 Vi\nSuppose that 2 (\n1+ Lm ) ∑Wj=1 b j n−W n ∑ i=1 E [Ui | Fi−1]≤ Ûn\nand 2∑Wj=1 b j m(n−W) n ∑ i=1 E [Vi | Fi−1]≤ V̂n\nThen it holds that\nP\n{ |ρ̂n − ρ̂ (3)n |> Ûn + V̂n + t }\n≤ P { 2 ( 1+ Lm ) ∑Wj=1 b j n−W n ∑ i=1 Ui + 2∑Wj=1 b j m(n−W) n ∑ i=1 Vi > Ûn + V̂n + t } ≤ P { 2 ( 1+ Lm )\n∑Wj=1 b j n−W n ∑ i=1 Ui > Ûn + t 2\n}\n+P\n{\n2∑Wj=1 b j m(n−W) n ∑ i=1 Vi > V̂n + t 2\n}\nWe can apply Lemma 22 to each term to yield\nP\n{\n2 ( 1+ Lm ) ∑Wj=1 b j n−W n ∑ i=1 Ui > Ûn + t 2\n}\n≤ exp\n\n \n \n− (n−W) 2t2\n32n ( 1+ Lm )2 ( ∑Wj=1 b j )2 diam2(X )\n\n \n \nand\nP\n{\n2∑Wj=1 b j m(n−W) n ∑ i=1 Vi > V̂n + t 2\n}\n≤ exp\n\n \n \n− (n−W) 2m2t2\n144nCg ( ∑Wj=1 b j )2\n\n \n \nThen it holds that\nP\n{ |ρ̂n − ρ̂ (3)n |> Ûn + V̂n + t }\n≤ exp\n\n \n \n− (n−W) 2t2\n32n ( 1+ Lm )2 ( ∑Wj=1 b j )2 diam2(X )\n\n \n \n+ exp\n\n \n \n− (n−W) 2m2t2\n144nCg ( ∑Wj=1 b j )2\n\n \n \nWe have by straightforward computation\nÛn = 2 ( 1+ Lm ) ∑Wj=1 b j n−W n ∑ i=1 C(Ki)\nand\nV̂n = 2∑Wj=1 b j m(n−W) n ∑ i=1 √ Cg dKi\nThen it holds that ∞\n∑ n=W+1 P { ρ̂n < ρ −Ûn − V̂n − tn }\n≤ ∞\n∑ n=W+1 P\n{\nρ̂n < ρ̂ (3) n −Ûn − V̂n − tn\n}\n≤ ∞\n∑ n=W+1 P\n{ |ρ̂n − ρ̂ (3)n |> Ûn + V̂n + tn }\n≤ ∞\n∑ n=W+1 exp\n\n \n \n− (n−W) 2t2n\n32n ( 1+ Lm )2 ( ∑Wj=1 b j )2 diam2(X )\n\n \n \n+ ∞\n∑ n=W+1 exp\n\n \n \n− (n−W) 2m2t2n\n144nCg ( ∑Wj=1 b j )2\n\n \n \n< ∞\nBy the Borel-Cantelli lemma, it follows that for all n large enough\nρ̂n +Ûn + V̂n + tn ≤ ρ almost surely.\n3.5 Parameter Estimation\nWe may need to estimate parameters of the functions { fn} such as the strong convexity parameter m to compute b(d0,K). We need the following assumption on our bound:\nD.1 Suppose that our bound b(d0,K,ψ) is parameterized by ψ , which depends on properties of the function ℓ(x,z) and the distributions {pn}∞n=1. Suppose that\nψ1 ≤ ψ2 ⇔ b(d0,K,ψ1)≤ b(d0,K,ψ2)\nD.2 There exists a true set of parameters ψ∗ such that\nψn = ψ∗ ∀n ≥ 1\nD.3 The spaces X and Z are compact\nD.4 There exists a constant L such that\n‖∇xℓ(x,z)−∇xℓ(x̃,z)‖ ≤ L‖x− x̃‖\nD.5 Suppose that we know that the parameters ψ ∈ P with P compact\nD.6 Suppose that ∇ fn(xn) has Lipschitz continuous gradients with modulus M\nAs a consequence of Assumption D.4 , it follows that there exists a constant G such that there exists a constant G such that ‖∇xℓ(x,z)‖ ≤ G ∀x ∈ X ,z ∈ Z Satisfying Assumption D.5 is usually easy due to the compactness assumptions in Assumption D.4 .\nIn most cases, we have\nψ =\n\n   −m M A B\n\n  \nwhere m is the parameter of strong convexity, M is the Lipschitz gradient modulus, and the pair (A,B) controls gradient growth, i.e.,\nE‖∇xℓ(x,z)‖2 ≤ A+B‖x−x∗‖2\nWe parameterize using −m, since smaller m increase the bound b(d0,K). We present several general methods for estimating these parameters, although in practice, problem specific estimators based on the form of the function may offer better performance. As an example, we present problem specific estimates for\nℓ(x,z) = 1 2 ( y−w⊤x )2 + 1 2 λ‖x‖2\nAs in estimating ρ , we produce one time instant estimates m̃i, M̃i, Ãi, and B̃i at time i and combine them. We only examine the case under Assumption D.4 , although we could examine an inequality constraints as with estimating ρ . We combine estimates by averaging to yield\n1. m̂n = 1n ∑ n i=1 m̃i\n2. M̂n = 1n ∑ n i=1 M̃i\n3. Ân = 1n ∑ n i=1 Ãi\n4. B̂n = 1n ∑ n i=1 B̃i\n3.5.1 Estimating Strong Convexity Parameter and Lipschitz Gradient Modulus\nWe seek one step estimators m̃n and M̃n such that\nE[m̃n | Kn−1]≤ m\nand E[M̃n | Kn−1]≥ M\nwith {Kn} defined in (6). Hessian Method: We exploit the fact that\n∇2 xx fn(x) mI ∀x ∈ X\nThis in turn implies that λmin ( ∇2 xx fn(x) ) ≥ m ∀x ∈ X\nThis suggests that given {zn(k)}Knk=1 we set\nm̃n , min x∈X λmin\n(\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x,zn(k))\n)\nSince λmin(A) = min\nv:‖v‖=1 〈Av,v〉 ,\nλmin(A) is a concave function of A. Then by Jensen’s inequality, we have\nE[m̃n] = E\n[\nmin x∈X λmin\n(\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x,zn(k))\n)\n∣ ∣ ∣ ∣ Kn−1\n]\n≤ min x∈X E\n[\nλmin\n(\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x,zn(k))\n)\n∣ ∣ ∣ ∣ Kn−1\n]\n≤ min x∈X λmin\n(\nE\n[\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x,zn(k))\n∣ ∣ ∣ ∣ Kn−1\n])\n= min x∈X\nλmin ( ∇2 xx fn(x) )\n= m\nSimilarly, we can set\nM̃n , max x∈X λmax\n(\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x,zn(k))\n)\nSince λmax(A) = max\nv:‖v‖=1 〈Av,v〉 ,\nλmax(A) is a convex function of A. By Jensen’s inequality, it holds that\nE[M̃n | Kn−1]≥ M\nGradient Method To Compute m̃n: To actually minimize over x, we can use gradient descent. To apply gradient descent, we use eigenvalue perturbation results [20]. Suppose that we have a base matrix T0 with eigenvectors v0i and eigenvalues λ0i. We want to find the eigenvectors vi and eigenvalues λi of a perturbed matrix T :\nT0v0i = λ0iv0i Tvi = λivi\nIn particular, we want to relate λ0i to λi. With δT , T −T0,\nwe have δλi = v⊤0i (δT )v0i\nand ∂λi ∂Ti j = v0i(i)v0 j(2− δi j)\nSuppose we are given a matrix-valued function T (x) with\nT (x)v(x) = λmin(x)v(x)\nThen it holds that\n∇xλmin (T (x)) = ∑ i, j ∂λmin ∂Ti j ∇xTi j(x)\n= ∑ i, j vi(x)v j(x)(2− δi j)∇xTi j(x)\nThen we can use gradient descent to solve\nmin x∈X λmin\n(\n1 Kn\nKn\n∑ k=1 ∇xℓ(x,zn(k))\n)\nStarting from any x(0), we can compute\nx(p) = ΠX\n[ x(p− 1)− µ∇xλmin (\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x,zn(k))\n)]\np = 1, . . . ,P\nand set\nm̂n , λmin\n(\n1 Kn\nKn\n∑ k=1 ∇2 xx ℓ(x(P),zn(k))\n)\n(15)\nHeuristic Method: For any two points x and y, we have by strong convexity\nfn(y)≥ fn(x)+ 〈∇ fn(x),y−x〉+ 1 2 m‖y−x‖2\nSuppose that we have N points x(1), . . . ,x(N). Then we know that for any two distinct points xi and x j\nm ≤ fn(x(i))− fn(x( j))−〈∇ fn(x( j)),x(i)−x( j)〉1 2‖x(i)−x( j)‖2\nThis suggests the estimator\nm̂n , min i6= j\n1 Kn ∑ Kn k=1 ℓ(x(i),zn(k))− 1Kn ∑ Kn k=1 ℓ(x( j),zn(k))−\n〈\n1 Kn ∑ Kn k=1 ∇xℓ(x( j),zn(k)),x(i)−x( j)\n〉\n1 2‖x(i)−x( j)‖2\n(16)\nfor the strong convexity parameter. Then we have\nE[m̂n]\n= E\n\nmin i6= j\n1 Kn ∑ Kn k=1 ℓ(x(i),zn(k))− 1Kn ∑ Kn k=1 ℓ(x( j),zn(k))−\n〈\n1 Kn ∑ Kn k=1 ∇xℓ(x( j),zn(k)),x(i)−x( j)\n〉\n1 2‖x(i)−x( j)‖2\n\n\n≤ min i6= j E\n\n\n1 Kn ∑ Kn k=1 ℓ(x(i),zn(k))− 1Kn ∑ Kn k=1 ℓ(x( j),zn(k))−\n〈\n1 Kn ∑ Kn k=1 ∇xℓ(x( j),zn(k)),x(i)−x( j)\n〉\n1 2‖x(i)−x( j)‖2\n\n\n≤ min i6= j fn(x(i))− fn(x( j))−〈∇ fn(x( j)),x(i)−x( j)〉 1 2‖x(i)−x( j)‖2\nIt is difficult to compare this estimator to m exactly. All we can say is that\nm ≤ min i6= j fn(x(i))− fn(x( j))−〈∇ fn(x( j)),x(i)−x( j)〉 1 2‖x(i)−x( j)‖2\nas well. In practice, this method produces estimates close to m. Similarly, we can set\nM̂n , max i6= j\n1 Kn ∑ Kn k=1 ℓ(x(i),zn(k))− 1Kn ∑ Kn k=1 ℓ(x( j),zn(k))−\n〈\n1 Kn ∑ Kn k=1 ∇xℓ(x( j),zn(k)),x(i)−x( j)\n〉\n1 2‖x(i)−x( j)‖2\n(17)\nProblem Specific: For the penalized quadratic, we have\n∇2 xx ℓ(x,z) = λI+ww⊤\nso ∇2\nxx fn(x) = λI+E[wnw⊤n ]\nThis suggests the simple closed-form estimates\nm̃n = λ +λmin\n(\n1 Kn\nKn\n∑ k=1\nwn(k)wn(k) ⊤ )\nand\nM̃n = λ +λmax\n(\n1 Kn\nKn\n∑ k=1\nwn(k)wn(k) ⊤ )\nAgain, by Jensen’s inequality, it holds that E[m̃n | Kn−1]≤ m\nand E[M̃n | Kn−1]≥ M\nCombining Estimates: We now look at combining the single time instant estimates of the strong convexity parameter and the Lipschitz gradient modulus.\nLemma 9. Choose tn such that for all C > 0 it holds that\n∞\n∑ n=1\ne−Cnt 2 n <+∞\nThen for all n large enough it holds that\n1. m̂n − tn ≤ m\n2. M̂n + tn ≥ M\nalmost surely.\nProof. By the compactness of the space P containing ψ , we can apply the dependent version of Hoeffding’s lemma (Lemma 23) to yield\nE [ esm̃i ∣ ∣ Ki−1 ] ≤ exp { 1 2 σ2ms 2 }\nand\nE\n[\nesM̃i ∣ ∣ Ki−1 ] ≤ exp { 1 2 σ2Ms 2 }\nfor some constants σ2m and σ2M derived from Hoeffding’s lemma. Then applying Lemma 22, it follows that\nP\n{\nm̂n > 1 n\nn\n∑ i=1\nE[m̃i | Ki−1]+ tn } ≤ exp { − nt 2 n\n2σ2m\n}\nWe know that 1 n n ∑ i=1 E[m̃i | Ki−1]> m\nso it follows that\nP{m̂n > m+ tn} ≤ exp { − nt 2 n\n2σ2m\n}\nSimilarly, for the Lipschitz gradient modulus, it holds that\nP { M̂n < M− tn } ≤ exp { − nt 2 n\n2σ2M\n}\nAs before, we have ∞\n∑ n=1\nP{m̂n > m+ tn} ≤ ∞\n∑ n=1 exp\n{\n− nt 2 n\n2σ2m\n}\n<+∞\nand ∞\n∑ n=1 P { M̂n < M− tn }\n≤ ∞\n∑ n=1 exp\n{\n− nt 2 n\n2σ2M\n}\n<+∞\nto ensure that almost surely for all n large enough it holds that\nm̂n − tn ≤ m\nand M̂n + tn ≥ m\nFor Lemma 9, we need tn to decay no faster that O(n−1/2).\n3.5.2 Estimating Gradient Parameters\nFrom Assumption D.6 , it holds that\nE‖∇xℓ(x,z)‖2 = E‖∇xℓ(x∗,z)+ (∇xℓ(x,z)−∇xℓ(x∗,z))‖2\n≤ 2E‖∇xℓ(x∗,z)‖2 + 2E‖∇xℓ(x,z)−∇xℓ(x∗,z)‖2 ≤ 2E‖∇xℓ(x∗,z)‖2 + 2M2‖x−x∗‖2\nThus, we can set B = 2M2\nand A = 2E‖∇xℓ(x∗,z)‖2\nThis suggests that given an estimate M̃n for M, we set\nB̃n = 2M̃2n\nThen by Jensen’s inequality, we have\nE[B̃n | Kn−1] = 2E[M̃2n | Kn−1] ≥ 2 ( E[B̃n | Kn−1] )2\n≥ 2M2\n= B\nLemma 10. Choose tn such that for all C > 0 it holds that ∞\n∑ n=1\ne−Cnt 2 n <+∞\nThen for all n large enough it holds that B̂n + tn ≥ B\nalmost surely.\nProof. By identical reasoning for the strong convexity and Lipschitz continuous gradients, it holds that\nP { B̂n < B− tn } ≤ exp { − nt 2 n\n2σ2B\n}\nSince we have ∞\n∑ n=1 exp\n{\n− nt 2 n\n2σ2B\n}\n<+∞\nfor all n large enough it holds that B̂n + tn ≥ B\nalmost surely.\nTo estimate A, consider using a point x to approximate x∗. It holds that\nE‖∇xℓ(x∗,z)‖2 = E‖∇xℓ(x,z)+ (∇xℓ(x∗,z)−∇xℓ(x,z))‖2\n≤ 2E‖∇xℓ(x,z)‖2 + 2E‖∇xℓ(x∗,z)−∇xℓ(x,z)‖2 ≤ 2E‖∇xℓ(x,z)‖2 + 2M2E‖x−x∗‖2 ≤ 2E‖∇xℓ(x,z)‖2 + 2 (\nM m\n)2\n‖∇ f (x)‖2\n≤ 2E‖∇xℓ(x,z)‖2 + 2 ( M m\n)2\n‖∇ f (x)‖2\nThis suggests the estimate\nÃn(x) = 2\nKn\nKn\n∑ k=1\n‖∇xℓ(x,zn(k))‖2 + 4 ( M̃n−1 + tn−1 m̃n−1 − tn−1\n)2∥ ∥\n∥ ∥ 1 Kn\nKn\n∑ k=1\n∇xℓ(x,zn(k)) ∥ ∥ ∥\n∥\n2\nLemma 11. For any x possibly random but not a function of {zn(k)}Knk=1 and all n large enough, it holds that\nE[Ãn | Kn−1]≥ A\nProof. For any x possibly random but not a function of {zn(k)}Knk=1, it holds that\nE[Ãn | Kn−1]\n= E\n[\n2 Kn\nKn\n∑ k=1\n‖∇xℓ(x,zn(k))‖2 + 4 ( M̃n−1 + tn−1 m̃n−1 − tn−1\n)2∥ ∥\n∥ ∥ 1 Kn\nKn\n∑ k=1\n∇xℓ(x,zn(k)) ∥ ∥ ∥\n∥\n2 ∣ ∣\n∣ ∣ Kn−1\n]\n= E\n[\n2 Kn\nKn\n∑ k=1\n‖∇xℓ(x,zn(k))‖2 ∣ ∣ ∣\n∣\nKn−1\n]\n+ 4\n(\nM̃n−1 + tn−1 m̃n−1 − tn−1\n)2\nE\n[\n∥ ∥ ∥ ∥ 1 Kn Kn ∑ k=1 ∇xℓ(x,zn(k)) ∥ ∥ ∥ ∥ 2 ∣ ∣ ∣ ∣ Kn−1\n]\n≥ 2E‖∇xℓ(x,zn)‖2 + 4 ( M̃n−1 + tn−1 m̃n−1 − tn−1\n)2\n‖∇ fn(x)‖2\nThe last inequality uses Jensen’s inequality. Then by our prior analysis, almost surely for all n sufficiently large it holds that\nM̃n−1 + tn−1 m̃n−1 − tn−1 ≥ M m\nand so for all n sufficiently large\nE[Ãn | Kn−1] ≥ 2E‖∇xℓ(x,zn)‖2 + 4 ( M m\n)2\n‖∇ fn(x)‖2\n= 2E‖∇xℓ(x∗n,zn)‖2 = A\nTherefore, for all n sufficiently large (dependent on estimation of m and M), it holds that\nE[Ãn | Kn−1]≥ A\nCombining Estimates for A: In practice, we use Ãn(xn), which complicates the analysis due to the fact that xn is computed using the same samples {zn(k)}Knk=1.\nLemma 12. Choose tn such that for all C > 0 it holds that\n∞\n∑ n=1\ne−Cnt 2 n <+∞\nThen for all n large enough it holds that Ân + tn ≥ A\nalmost surely.\nProof. Consider the following three estimates of A all computed with knowledge of m and M and x̃n as in Lemma 2:\nÃ(2)i = 2 Ki\nKi\n∑ k=1\n‖∇xℓ(xi,zi(k))‖2 + 4 ( M m )2∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ 2\nÃ(3)i = 2 Ki\nKi\n∑ k=1\n‖∇xℓ(x̃i,zi(k))‖2 + 4 ( M m )2∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(x̃i,zi(k)) ∥ ∥ ∥ ∥ 2\nÃ(4)i = 2E‖∇xℓ(x̃i,zi)‖2 + 4 ( M m\n)2\n‖∇ fi(x̃i)‖2\nDefine the averaged estimates\nÂ(2)n = 1 n\nn\n∑ i=1 Ã(2)i\nÂ(3)n = 1 n\nn\n∑ i=1 Ã(3)i\nÂ(4)n = 1 n\nn\n∑ i=1 Ã(4)i\nWe always have Ã(4)i ≥ A so Â(4)n ≥ A First, we show that Â(2)n is close to A (3) n . We have\n|Ã(2)i − Ã (3) i |\n≤ 2 ∣ ∣ ∣\n∣ 1 Ki\nKi\n∑ k=1\n( ‖∇xℓ(xi,zi(k))‖2 −‖∇xℓ(x̃i,zi(k))‖2 )\n∣ ∣ ∣ ∣\n+ 4\n(\nM m\n)2 ∣ ∣\n∣ ∣\n∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(xi,zi(k)) ∥ ∥ ∥ ∥ 2 − ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 ∇xℓ(x̃i,zi(k)) ∥ ∥ ∥ ∥ 2∣ ∣ ∣ ∣\n≤ 4G 1 Ki\nKi\n∑ k=1\n‖∇xℓ(xi,zi(k))−∇xℓ(x̃i,zi(k))‖+ 8G ( M m )2∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(xi,zi(k))−∇xℓ(x̃i,zi(k))) ∥ ∥ ∥ ∥ 2\n≤ ( 4+ 8 (\nM m\n)2 )\nGM‖xi − x̃i‖\nyielding\n|Â(2)n − Â(3)n | ≤ ( 4+ 8 (\nM m\n)2 )\nGM\n(\n1 n\nn\n∑ i=1\n‖xi − x̃i‖ )\nSecond, we have\n|Â(3)n − Â(4)n |\n≤ ∣ ∣ ∣\n∣ 1 n\nn\n∑ i=1\n(\n2 Ki\nKi\n∑ k=1\n( ‖∇xℓ(x̃i,zi(k))‖2 −E [ ‖∇xℓ(x̃i,zi)‖2 | Fn−1 ])\n)\n∣ ∣ ∣ ∣\n+ 8\n(\nM m\n)2\nG 1 n\nn\n∑ i=1\n∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇ fi(x̃i)) ∥ ∥ ∥ ∥\nCombining both inequalities, we know that\n|Â(2)n − Â(4)n |\n≤ ( 4+ 8 (\nM m\n)2 )\nGM\n(\n1 n\nn\n∑ i=1\n‖xi − x̃i‖ )\n+\n∣ ∣ ∣ ∣ 1 n n ∑ i=1\n(\n2 Ki\nKi\n∑ k=1\n( ‖∇xℓ(x̃i,zi(k))‖2 −E [ ‖∇xℓ(x̃i,zi)‖2 | Fn−1 ])\n)\n∣ ∣ ∣ ∣\n+ 8\n(\nM m\n)2\nG 1 n\nn\n∑ i=1\n∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xℓ(x̃i,zi(k))−∇ fi(x̃i)) ∥ ∥ ∥ ∥\nThe first and third terms in this bound can be controlled by the analysis of the direct estimate and the second term by Lemma (22). This shows that\nP\n{\nÂ(2)n < A− 1 n\nn\n∑ i=1 Ci√ Ki\n− tn }\n≤ P { Â(2)n < Â (4) n −\n1 n\nn\n∑ i=1 Ci√ Ki\n− tn }\n≤ P {\n|Â(2)n − Â(4)n |> 1 n\nn\n∑ i=1 Ci√ Ki tn\n}\n≤ 2exp { − nt 2 n\n2σ2A2\n}\nSince ∞\n∑ n=1 P\n{\nÂ(2)n < A− 1 n\nn\n∑ i=1 Ci√ Ki\n− tn } ≤ ∞\n∑ n=1 C exp\n{ − nt 2 n\n2σ2A2\n}\n<+∞\nalmost surely for all n large enough, it holds that\nÂ(2)n + 1 n\nn\n∑ i=1 Ci√ Ki + tn ≥ A\nIn addition, we have\nÂ(2)n + 1 n\nn\n∑ i=1 Ci√ Ki + 2tn ≥ A\nThere exists a random variable Ñ such that\nn ≥ Ñ ⇒ Mn + tn mn − tn ≥ M m\nThen for n ≥ Ñ, it holds that\nÂn − Â(2)n\n= 4 n\nn\n∑ i=1\n[\n(\nM̂i−1 + ti−1 m̂i−1 − ti−1\n)2\n− ( M m\n)2 ] ∥\n∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n∇xℓ(xi,zi(k)) ∥ ∥ ∥\n∥\n2\n≥ 4 n Ñ−1 ∑ i=1\n[\n(\nM̂i−1 + ti−1 m̂i−1 − ti−1\n)2\n− ( M m\n)2 ] ∥\n∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n∇xℓ(xi,zi(k)) ∥ ∥ ∥\n∥\n2\nSince our choice of tn can decay only as fast as C/ √ n, it follows that\n4 n Ñ−1 ∑ i=1\n[\n(\nM̂i−1 + ti−1 m̂i−1 − ti−1\n)2\n− ( M m\n)2 ] ∥\n∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n∇xℓ(xi,zi(k)) ∥ ∥ ∥\n∥\n2\n− tn < 0\nfor all n large enough. This implies that\nÂn + 1 n\nn\n∑ i=1 Ci√ Ki + tn\n≥ Ân − (\n4 n Ñ−1 ∑ i=1\n[\n(\nM m\n)2\n− ( M̂i−1 + ti−1 m̂i−1 + ti−1\n)2 ] ∥\n∥ ∥ ∥ 1 Ki\nKi\n∑ k=1\n∇xℓ(xi,zi(k)) ∥ ∥ ∥\n∥\n2 − tn )\n+ 1 n\nn\n∑ i=1 Ci√ Ki + tn\n≥ Â(2)n + 1 n\nn\n∑ i=1 Ci√ Ki + 2tn\n≥ A\nfor n large enough.\nUsing these estimates, we have constructed estimates ψ̂n such that for all n large enough it holds that\nψ̂n +Cn + tn1≥ ψ∗\nfor appropriate constants Cn almost surely. Therefore, by assumption for all n large enough it holds that\nb(d0,K,ψ∗)≤ b(d0,K, ψ̂n + tn)\n3.5.3 Effect on ρ Estimation\nOur analysis of estimating ρ assumes that we know the parameters of the function and in particular the strong convexity parameter m. We now argue that the effect of using estimated parameters instead is minimal. This happens because we know that for all n large enough it holds that\nψ̂n ≥ ψ∗\nalmost surely.\nLemma 13. We want to estimate a non-negative parameter φ∗ by producing a sequence of estimates φi for all i ≥ 1 and averaging to produce\nφ̂n = 1 n\nn\n∑ i=1 φi\nwhere the estimates φi are dependent on an auxiliary sequence ψi in the sense that φi(ψi). Suppose that the following conditions hold:\n1. Suppose that there exists a random variable Ñ such that n ≥ Ñ implies that ψ̂n ≥ ψ∗\n2. E[φi(ψ∗)]≥ φ∗\nThen it follows that\nliminf n→∞ E\n[\n1 n\nn\n∑ i=1 φi\n]\n≥ φ∗\nProof. It holds that\n1 n\nn\n∑ i=1 φi = 1 n Ñ−1 ∑ i=1 φi(ψi)+ 1 n n ∑ i=Ñ φi(ψi)\n≥ 1 n Ñ−1 ∑ i=1 φi(ψi)+ 1 n n ∑ i=Ñ φi(ψ∗i ) (18)\nTherefore, it follows that\nliminf n→∞ E\n[\n1 n\nn\n∑ i=1 φi\n]\n≥ liminf n→∞ E\n[\n1 n\nn\n∑ i=Ñ φi(ψ∗i )\n]\n≥ φ∗\nWe can extend all the concentration inequalities for estimating ρ as well by extending the inequality in (18) to yield\n1 n\nn\n∑ i=1 φi = 1 n Ñ−1 ∑ i=1 φi(ψi)+ 1 n n ∑ i=Ñ φi(ψi)\n≥ 1 n Ñ−1 ∑ i=1 φi(ψi)+ 1 n n ∑ i=Ñ φi(ψ∗i )\n≥ 1 n Ñ−1 ∑ i=1 (φi(ψi)−φi(ψ∗))+ 1 n n ∑ i=1 φi(ψ∗i )\n= 1 n\nn\n∑ i=1 φi(ψ∗i )+ o(1)\nBefore, we have analyzed 1 n n ∑ i=1 φi(ψ∗i )\nso for large enough n, we recover previous results, since the o(1) term goes to 0.\n4 Adaptive Sequential Optimization With ρ Unknown We now examine the case with ρ unknown. We extend the work of Section 2 using the estimates of ρ in Section 3. Our analysis depends on the following crucial assumption:\nC.1 For appropriate sequences {tn}, for all n sufficiently large it holds that ρ̂n + tn ≥ ρ almost surely.\nC.2 b(d0,Kn) factors as b(d0,Kn) = α(Kn)d0 +β (Kn)\nWe have demonstrated that assumption C.1 that holds for the direct and IPM estimates of ρ under (2) and (3). Note that whether we assume (2) or (3) does not matter for analysis.\n4.1 General Condition on Kn We start with a general result showing that for any choice of Kn such that Kn ≥ K∗ for all n large enough the excess risk is controlled in the sense that\nlimsup n→∞\n(E[ fn(xn)]− fn(x∗n))≤ ε\nWe then apply this result to two different selection rules for Kn. Consider the function\nφK(v) = α(K)\n(\n√\n2 m v+ρ\n)2\n+β (K)\nderived from assumption C.2. Note that as a function of v, φK(v) is clearly increasing and strictly concave. First, suppose that we select K∗ defined in (5). Then by definition it holds that\nφK∗(ε)≤ ε\nWe study fixed points of the function φK∗(v):\nLemma 14. The function φK∗(v) has a unique positive fixed point v̄ with\n1. v̄ = φK∗(v̄)≤ ε\n2. φ ′K∗(v̄)< 1\nProof. We have φK∗(0) = α(K∗)ρ2 +β (K∗)> 0\nSince lim v→0 φK∗(v) = φK∗(0)\nand φK∗(0)> 0, there exists a positive a sufficiently small that\nφK∗(a)> a\nNext, expanding φK(v) yields\nφK(v) = 2 m\nα(K)v+ 2α(K)ρ √\n2 m √ v+α(K)ρ2 +β (K)\nSince φK∗(ε)≤ ε , we obviously must have 2m α(K∗)≤ 1. Suppose that\n2 m α(K∗) = 1\nThen it holds that φK∗(ε) = ε + √ 2mρ √ ε +\nm 2 ρ2 +β (K)> ε\nThis is a contradiction, so it holds that 2 m α(K∗)< 1\nIt is thus readily apparent that v−φK∗(v)→ ∞\nas v → ∞. Therefore, there exists a point b > a such that\nφK∗(b)< b\nIt is easy to check that φK∗(v) is increasing and strictly concave. Therefore, we can apply Theorem 3.3 from [21] to conclude that there exists a unique, positive fixed point v̄ of φK∗(v).\nNext, suppose that φ ′K∗(v̄)> 1. Then by Taylor’s Theorem for v > v̄ sufficiently close to v̄, we have\nφK∗(v)> v\nHowever, we know that as v → ∞, it holds that v−φK∗(v)→ ∞. By the Intermediate Value Theorem, this implies that there is another fixed point on [v,∞). This is a contradiction, since v̄ is the unique, positive fixed point. Therefore, it holds that φ ′K∗(v̄)≤ 1. Now, suppose that φ ′K∗(v̄) = 1. Since φK∗(v) is strictly concave, its derivative is decreasing [22]. Therefore, on [0, v̄), it holds that\nφ ′K∗(v)> 1\nThis implies that\nφK∗(v̄) = φK∗(0)+ ∫ v̄\n0 φ ′K∗(v)dx\n≥ φK∗(0)+ v̄ > v̄\nThis is a contradiction, so it must be that φ ′K∗(v̄)< 1.\nAs a simple consequence of the concavity of φK∗(v), we can study a fixed point iteration involving φK(v). Define the n-fold composition mapping\nφ (n)K (v), (φK ◦ · · · ◦φK)(v)\nLemma 15. For any v > 0, it holds that lim n→∞ φ (n)K∗ (v) = v̄\nProof. Following [23], for any fixed point v̄, it holds that\n|φK∗(v)− v̄| ≤ φ ′K∗(v̄)|v− v̄|\nTherefore, applying the fixed point property repeatedly yields\n|φ (n)K∗ (v)− v̄| ≤ (φ ′K∗(v̄))n|v− v̄|\nBy Lemma 14, it holds that φ ′K∗(v̄)< 1\nand so the result follows.\nNow, we show that we appropriately control the excess risk when we estimate ρ . The extension of this argument to the case when we also estimate function parameters ψ is straightforward. If we have\np({zn(k)}Knk=1 | xn−1,Kn) = Kn\n∏ k=1 pn(zn(k))\nthen\nE [ fn(xn) | xn−1,Kn]− fn(x∗n)≤ b\n\n\n(\n√\n2 m ( fn−1(xn−1)− fn−1(x∗n−1) ) +ρ\n)2\n,Kn\n\n\nTherefore, it holds that\nE [ fn(xn)]− fn(x∗n)≤ E\n\nb\n\n\n(\n√\n2 m ( fn−1(xn−1)− fn−1(x∗n−1) ) +ρ\n)2\n,Kn\n\n\n\n\nSuppose that we set K∞ = σ ({Kn}∞n=1 ∪{ρ̂n}∞n=2)\nThis sigma algebra contains all the information about {ρ̂n} and thus {Kn}. Then, we do not have\np({zn(k)}Knk=1 | K∞) = Kn\n∏ k=1 pn(zn(k))\nsince Kn+1,Kn+2, . . . are a function of {Kn}Knk=1. We do not even have\nE [ fn(xn) | K∞]− fn(x∗n)≤ b\n\n\n(\n√\n2 m ( fn−1(xn−1)− fn−1(x∗n−1) ) +ρ\n)2\n,Kn\n\n\nHowever, we would expect that this is not too far from true. Conceptually, we consider running our approach twice on independent samples. The first run determines the required number of samples {Kn}∞n=1. We then run our process for a second run with these fixed choices of {Kn}∞n=1and independent samples as in Figure 1. For the second run, it is true that\np({z(2)n (k)}Knk=1 | K∞) = Kn\n∏ k=1\npn(z (2) n (k))\nand\nE\n[\nfn(x (2) n ) | K∞\n]\n− fn(x∗n)≤ b\n\n\n(\n√\n2 m ( fn−1(x (2) n−1)− fn−1(x∗n−1) ) +ρ\n)2\n,Kn\n\n\nIn practice, we do not need to run our process twice. This is only a proof technique. Now, for the second run the recursion\nε(2)n = b\n\n\n(\n√\n2 m ε(2)n−1 +ρ\n)2\n,Kn\n\n ∀n ≥ 3 (19)\nwith ε1 and ε2 from Assumption A.4 bounds the excess risk of the second run\nE[ fn(x (2) n ) | K∞]− fn(x∗n)≤ ε (2) n\nThen it follows that E[ fn(x (2) n )]− fn(x∗n)≤ E[ε (2) n ]\nWe now argue that E[ε(2)n ] also bounds the excess risk of the first run.\nLemma 16. For the first run, it holds that\nE[ fn(xn)]− fn(x∗n)≤ E[ε (2) n ]\nProof. We proceed by induction. For n = 1,2, we know that\nE[ fn(xn)]− fn(x∗n)≤ E[ε (2) n ]\nby definition. Next, suppose that\nE[ fn−1(xn−1)]− fn−1(x∗n−1)≤ E[ε (2) n−1]\nWe have\nE[ fn(xn)]− fn(x∗n)≤ E [ α(Kn) (√ fn−1(xn−1)− fn−1(x∗n−1)+ρ )2 +β (Kn) ]\nso it holds that\nE[ε(2)n ]− (E[ fn(xn)]− fn(x∗n))\n≥ E [ α(Kn) ( √ ε(2)n−1 +ρ )2 −α(Kn) (√ fn−1(xn−1)− fn−1(x∗n−1)+ρ )2 ]\n= E [ α(Kn) ( ε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) ) )]\n+E\n[ 2ρα(Kn) ( √ ε(2)n−1 − √ fn−1(xn−1)− fn−1(x∗n−1) )]\nBy the Monotone Convergence Theorem, it holds that\nE\n[ α(Kn) ( ε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) ) )]\n= lim q→∞ E\n[ max{α(Kn),1/q} ( ε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) ) )]\n≥ liminf q→∞ 1 q E [ ε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) ) ] ≥ 0\nwhere the last line follows, since by hypothesis\nE[ fn−1(xn−1)]− fn−1(x∗n−1)≤ E[ε (2) n−1]\nSimilarly, it holds that\nE\n[ 2ρα(Kn) ( √ ε(2)n−1 − √ fn−1(xn−1)− fn−1(x∗n−1) )]\n= E\n\n  2ρα(Kn)\nε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) )\n√\nε(2)n−1 + √ fn−1(xn−1)− fn−1(x∗n−1)\n\n \n= lim q→∞ E\n\n  2ρ max{α(Kn),1/q}\nε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) )\n√\nε(2)n−1 + √ fn−1(xn−1)− fn−1(x∗n−1)\n\n \n≥ limsup q→∞ 2ρ q E\n\n \nε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) )\n√\nε(2)n−1 + √ fn−1(xn−1)− fn−1(x∗n−1)\n\n \n≥ limsup q→∞ 2ρ q lim τ→∞ E\n\n \nε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) )\n√\nε(2)n−1 + √ fn−1(xn−1)− fn−1(x∗n−1) 1\n{ √ ε(2)n−1+ √ fn−1(xn−1)− fn−1(x∗n−1)≤τ}\n\n \n≥ limsup q→∞ 2ρ q limsup τ→∞ 1 τ E [ ε(2)n−1 − ( fn−1(xn−1)− fn−1(x∗n−1) ) ]\n≥ 0\nTherefore, we conclude that E[ fn(xn)]− fn(x∗n)≤ E[ε (2) n ]\nTheorem 2. Under assumptions C.1 - C.2 and with Kn ≥ K∗ for all n large enough almost surely with K∗ from (20), we have\nlimsupn→∞ (E[ fn(xn)]− fn(x∗n))≤ ε\nProof. Let v̄ be the fixed point associated with φK∗(v) from Lemma 14. We know that\nv̄ = φK∗(v̄)≤ ε\nand φ (n)K∗ (v)→ v̄ ≤ ε with v̄ ≤ ε . Since we have Kn ≥ K∗ for all n large enough almost surely, there exists a random variable Ñ such that\nn ≥ Ñ ⇒ Kn ≥ K∗\nThen we have almost surely\nlimsup n→∞ ε(2)n ≤ limsup n→∞ (φKn ◦ · · · ◦φKÑ )(εÑ−1)\n≤ limsup n→∞\nφ (n−Ñ+1)K∗ (εÑ−1)\n= v̄\n≤ ε\nFinally, applying Lemma 19 and Fatou’s lemma yields\nlimsup n→∞ (E[ fn(xn)]− fn(x∗n)) ≤ limsup n→∞ E\n[ ε(2)n ]\n≤ E [\nlimsup n→∞\nε(2)n ]\n≤ ε\n4.2 Update Past Excess Risk Bounds\nWe first consider updating all past excess risk bounds as we go. At time n, we plug-in ρ̂n−1 + tn−1 in place of ρ and follow the analysis of Section 2. Define for i = 1, . . . ,n\nε̂(n)i = b\n\n\n(\n√\n2 m ε̂(n)i−1 +(ρ̂n−1+ tn−1)\n)2\n,Ki\n\n\nIf it holds that ρ̂n−1 + tn−1 ≥ ρ , then E [ fn(xn)]− fn(x∗n)≤ ε̂ (i) n for i = 1, . . . ,n. Assumption C.1 guarantees that this holds for all n large enough almost surely. We can thus set Kn equal to the smallest K such that\nb\n\n\n(\n√\n2 m max{ε̂(n−1)n−1 ,ε}+(ρ̂n−1 + tn−1) )2 ,K\n\n≤ ε\nfor all n ≥ 3 to achieve excess risk ε . The maximum in this definition ensures that when ρ̂n−1 + tn−1 ≥ ρ , Kn ≥ K∗ with K∗ from (5). We can therefore apply Theorem 2.\n4.3 Do Not Update Past Excess Risk Bounds\nUpdating all past estimates of the excess risk bounds from time 1 up to n imposes a computational and memory burden. Suppose that for all n ≥ 3 we set\nKn = min\n\n\n\nK ≥ 1 ∣ ∣ ∣\n∣ ∣\nb\n\n\n(\n√\n2ε m +(ρ̂n−1+ tn−1)\n)2\n,K\n\n≤ ε\n\n\n\n(20)\nThis is the same form as the choice in (5) with ρ̂n−1+ tn−1 in place of ρ . Due to assumption C.1 , for all n large enough it holds that ρ̂n + tn ≥ ρ almost surely. Then by the monotonicity assumption in A.1 , for all n large enough we pick Kn ≥ K∗ almost surely. We can therefore apply Theorem 2.\n5 Experiments\nWe focus on two regression applications for synthetic and real data as well as two classification applications for synthetic and real data. For the synthetic regression problem, we can explicitly compute ρ and x∗n and exactly evaluate the performance of our method. It is straightforward to check that all requirements in A.1 -A.4 are satisfied for the problems considered in this section. We apply the do not update past excess risk choice of Kn here.\n5.1 Synthetic Regression\nConsider a regression problem with synthetic data using the penalized quadratic loss\nℓ(x,z) = 1 2 ( y−w⊤x )2 + 1 2 λ‖x‖2\nwith z = (w,y) ∈Rd+1. The distribution of zn is zero mean Gaussian with covariance matrix [\nσ2 w I rwn,yn\nr⊤ wn,yn σ 2 yn\n]\nUnder these assumptions, we can analytically compute minimizers x∗n of fn(x) = Ezn∼pn [ℓ(x,zn)]. We change only rwn,yn and σ2yn appropriately to ensure that ‖x∗n −x∗n−1‖ = ρ holds for all n. We find approximate minimizers using SGD with λ = 0.1. We estimate ρ using the direct estimate.\nWe let n range from 1 to 20 with ρ = 1, a target excess risk ε = 0.1, and Kn from (20). We average over twenty runs of our algorithm. Figure 2 shows ρ̂n, our estimate of ρ , which is above ρ in general. Figure 3 shows the number of samples Kn, which settles down. We can exactly compute fn(xn)− fn(x∗n), and so by averaging over the twenty runs of our algorithm, we can estimate the excess risk (denoted “sample average estimate”). Figure 4 shows this estimate of the excess risk, the target excess risk, and our bound on the excess risk from Section 4.3. We achieve at least our targeted excess risk\nFigure 2: ρ Estimate n 2 4 6 8 10 12 14 16 18 20\nn 2 4 6 8 10 12 14 16 18 20\nE xc\nes s\nR is\nk\n0\n0.05\n0.1\n0.15\n0.2\n0.25 Direct Estimate Sample Average Estimate\nFigure 4: Excess Risk\n5.2 Panel Study on Income Dynamics Income - Regression\nThe Panel Study of Income Dynamics (PSID) surveyed individuals every year to gather demographic and income data annually from 1981-1997 [24]. We want to predict an individual’s annual income (y) from several demographic features (w) including age, education, work experience, etc. chosen based on previous economic studies in [25]. The\nidea of this problem conceptually is to rerun the survey process and determine how many samples we would need if we wanted to solve this regression problem to within a desired excess risk criterion ε .\nWe use the same loss function, direct estimate for ρ , and minimization algorithm as the synthetic regression problem. The income is adjusted for inflation to 1997 dollars with mean $20,294. We average over twenty runs of our algorithm by resampling without replacement [26]. We compare to taking an equivalent number of samples up front. Figure 5 shows the test losses over time evaluated over twenty percent of the available samples. The test loss for our approach is substantially less than taking the same number of samples up front. The square root of the average test loss over this time period for our approach and all samples up front are $1153± 352 and $2805± 424 respectively in 1997 dollars.\n5.3 Synthetic Classification\nConsider a binary classification problem using ℓ(x,z) = 12 (1− y(w⊤x))2++ 12 λ‖x‖2 with z = (w,y) ∈ Rd ×R and (y)+ = max{y,0}. This is a smoothed version of the hinge loss used in support vector machines (SVM) [26]. We suppose that at time n, the two classes have features drawn from a Gaussian distribution with covariance matrix σ2I but different means µ (1)n and µ (2) n , i.e., wn | {yn = i} ∼ N (µ (i)n ,σ2I). The class means move slowly over uniformly spaced points on a unit sphere in Rd as in Figure 6 to ensure that (2) holds. We find approximate minimizers using SGD with λ = 0.1. We estimate ρ using the direct estimate with tn ∝ 1/n3/8.\nWe let n range from 1 to 20 and target a excess risk ε = 0.1. We average over twenty runs of our algorithm. As a comparison, if our algorithm takes {Kn}20n=1 samples, then we consider taking ∑20n=1 Kn samples up front at n = 1. This is what we would do if we assumed that our problem is not time varying. Figure 7 shows ρ̂n, our estimate of ρ . Figure 8 shows the average test loss for both sampling strategies. To compute test loss we draw Tn additional samples\n{ztestn (k)}Tnk=1 from pn and compute 1Tn ∑ Tn k=1 ℓ(xn,z test n (k)). We see that our approach achieves substantially smaller test loss than taking all samples up front.\nn 2 4 6 8 10 12 14 16 18 20\nρ\n1.8\n1.9\n2\n2.1\n2.2 Direct Estimate\nFigure 7: ρ Estimate n 2 4 6 8 10 12 14 16 18 20\nT es\nt L os\ns\n0.2\n0.4\n0.6\n0.8\n1 All Samples Up Front Direct Estimate\nFigure 8: Test Loss\n5.4 General Social Survey - Classification\nThe General Social Survey (GSS) surveyed individuals every year to gather socio-economic data annually from 1981- 2013 [27]. We want to predict an individual’s marital status (y) from several demographic features (w) including age, education, etc. We model this as a binary classification problem using loss\nℓ(x,z) = 1 2 (1− y(w⊤x))2++ 1 2 λ‖x‖2\nwith z = (w,y) ∈ Rd ×R and (y)+ = max{y,0}. This is a smoothed version of the hinge loss used in support vector machines [26]. We find approximate minimizers using SGD with λ = 0.1. Figure 9 shows the test loss. We see that our approach achieves smaller test loss than taking all samples up front. We also plot receiver operating characteristics (ROC) [26] to characterize the performance of our classifiers. In particular we plot the ROC for 1974 in Figure 10 and the ROC for 2012 in Figure 11. By examining the ROC, we see that taking all samples up front is much better in 1974 but much worse in 2012.\n6 Conclusion\nWe introduced a framework for adaptively solving a sequence of optimization problems with applications to machine learning. We developed estimates of the change in the minimizers used to determine the number of samples Kn needed to achieve a target excess risk ε . Experiments with synthetic and real data demonstrate that this approach is effective.\nReferences\n[1] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of Machine Learning, The MIT Press, 2012.\n[2] A. Agarwal, H. Daumé, and S. Gerber, “Learning multiple tasks using manifold regularization.,” in NIPS, 2011, pp. 46–54.\n[3] T. Evgeniou and M. Pontil, “Regularized multi–task learning,” in Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA, 2004, KDD ’04, pp. 109–117, ACM.\n[4] Y. Zhang and D. Yeung, “A convex formulation for learning task relationships in multi-task learning,” CoRR, vol. abs/1203.3536, 2012.\n[5] S. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359, Oct 2010.\nFalse Positive 0 0.2 0.4 0.6 0.8 1\nT ru\ne P\nos iti\nve\n0\n0.2\n0.4\n0.6\n0.8\n1\nDirect Estimate All Samples Up Front\nFigure 10: ROC for 1974 False Positive 0 0.2 0.4 0.6 0.8 1\nT ru\ne P\nos iti\nve\n0\n0.2\n0.4\n0.6\n0.8\n1\nAll Samples Up Front Direct Estimate\nFigure 11: ROC for 2012\n[6] A. Agarwal, A. Rakhlin, and P. Bartlett, “Matrix regularization techniques for online multitask learning,” Tech. Rep. UCB/EECS-2008-138, EECS Department, University of California, Berkeley, Oct 2008.\n[7] Z. Towfic, J. Chu, and A. Sayed, “Online distirubted online classifcation in the midst of concept drifts,” Neurocomputing, vol. 112, pp. 138–152, 2013.\n[8] C. Tekin, L. Canzian, and M. van der Schaar, “Context adaptive big data stream mining,” in Allerton Conference, 2014, pp. 46–54.\n[9] T. Dietterich, “Machine learning for sequential data: A review,” in Structural, Syntactic, and Statistical Pattern Recognition, 2002, pp. 15–30.\n[10] T. Fawcett and F. Provost, “Adaptive fraud detection.,” Data Min. Knowl. Discov., vol. 1, no. 3, pp. 291–316, 1997.\n[11] N. Qian and T. Sejnowski, “Predicting the secondary structure of globular proteins using neural network models,” Journal of Molecular Biology, vol. 202, pp. 865–884, Aug 1988.\n[12] Y. Bengio and P. Frasconi, “Input-output HMM’s for sequence processing,” IEEE Transactions on Neural Networks, vol. 7(5), pp. 1231–1249, 1996.\n[13] A. Dontchev and R. Rockafellar, Implicit Functions and Solution Mappings: A View from Variational Analysis, Springer, New York, New York, 2009.\n[14] B. Sriperumbudur, “On the empirical estimation of integral probability metrics,” Electronic Journal of Statistics, pp. 1550–1599, 2012.\n[15] R. Veryshin, “Introduction to non-asymptotic analysis of random matrices,” Tech. Rep., University of Michigan, 2012.\n[16] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, “Stochastic approximation approach to stochastic programming,” SIAM Journal on Optimization, vol. 19, pp. 1574–1609, 2009.\n[17] V.V Buldygin and E.D. Pechuk, “Inequalities for the distributions of functionals of sub-gaussian vectors,” Theor. Probability and Math. Statist., pp. 25–36, 2010.\n[18] S. Janson, “Large deviations for sums of partly dependent random variables,” Random Structures Algorithms, vol. 24, pp. 234–248, 2004.\n[19] S. Boucheron, G. Lugosi, and P. Massart, Concentration Inequalities: A Nonasymptotic Theory of Independence, Oxford University Press, 2013.\n[20] L. Trefethen, Numerical Linear Algebra, SIAM, 1997.\n[21] J. Kennan, “Uniqueness of positive fixed points for increasing concave functions on rn: An elementary result,” Review of Economic Dynamics, vol. 4, pp. 893âĂŞ899, 2001.\n[22] Stephen Boyd and Lieven Vandenberghe, Convex Optimization, Cambridge University Press, New York, NY, USA, 2004.\n[23] A. Granas and J. Dugundji, Fixed Point Theory, Springer-Verlag, 2003.\n[24] “Panel study of income dynamics: public use dataset,” Survey Research Center, 2015.\n[25] S. Jenkins and P. Van Kerm, “Trends in income inequality, pro-poor income growth, and income mobility,” Oxford Economic Papers, vol. 58, no. 3, pp. 531–548, 2006.\n[26] T. Hastie, R. Tibshirani, and J.H. Friedman, The elements of statistical learning: data mining, inference, and prediction: with 200 full-color illustrations, New York: Springer-Verlag, 2001.\n[27] “General social survey,” National Opinion Research Center, 2015.\n[28] F. Bach and E. Moulines, “Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning,” in Advances in Neural Information Processing Systems (NIPS), Spain, 2011.\n[29] D. Bertsekas, Nonlinear Programming, Athena Scientific, 1999.\n[30] Léon Bottou, “Online learning and stochastic approximations,” 1998.\n[31] A. Nedic and S. Lee, “Analysis of mirror descent for strongly convex functions,” ArXiV, 2013.\n[32] Yu. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course, Kluwer Academic Publishers, Norwell, Massachusetts, USA, 2004.\n[33] R. Antonini and Y. Kozachenko, “A note on the asymptotic behavior of sequences of generalized subgaussian random vectors,” Random Op. and Stoch. Equ., vol. 13, pp. 39–52, 2005.\nA Examples of b(d0,K):\nFor this section, we drop the n index for convenience. The bounds of this form depend on the strong convexity parameter m and an assumption on how the gradients grow. In general, we assume that\nEz∼p‖∇xℓ(x,z)‖2 ≤ A+B‖x−x∗‖2\nThe base algorithm we look at is SGD. First, we generate iterates x(0), . . . ,x(K) through SGD as follows:\nx(ℓ+ 1) = ΠX [x(ℓ)− µ(ℓ+ 1)∇xℓ(x(ℓ),z(ℓ))] ℓ= 0, . . . ,K − 1\nwith x(0) fixed. We then combine the iterates to yield a final approximate minimizer\nx̄(K) = φ(x(0), . . . ,x(K))\nFor our choice of φ , we look at two cases:\n1. No iterate averaging, i.e., φ(x(0), . . . ,x(K)) = x(K)\n2. Iterate averaging, i.e, for a convex combination {λ (ℓ)}Kℓ=0\nφ(x(0), . . . ,x(K)) = K\n∑ ℓ=0 λ (ℓ)x(ℓ)\nDefine d(ℓ), ‖x(ℓ)−x∗‖2 (21)\nFirst we bound E[d(ℓ)] in Lemma 17.\nLemma 17. Suppose that the function f (x) has Lipschitz continuous gradients. Then it holds that\nE[d(ℓ)]≤ ℓ\n∏ k=1\n(1− 2mµ(ℓ)+Bµ2(ℓ))+ ℓ\n∑ k=1\nℓ\n∏ i=k+1 (1− 2mµ(i)+Bµ2(i))µ2(k)\nProof. Following the standard SGD analysis (see [16]), it holds that\nd(ℓ) ≤ ‖x(ℓ− 1)−x∗− µ(ℓ)∇xℓ(x(ℓ− 1),z(ℓ))‖2\n≤ d(ℓ− 1)− 2µ(ℓ)〈x(ℓ− 1)−x∗,∇xℓ(x(ℓ− 1),z(ℓ))〉+ µ2(ℓ)‖∇xℓ(x(ℓ− 1),z(ℓ))‖2\nThen it follows that\nE[d(ℓ) | x(ℓ− 1)] ≤ d(ℓ− 1)− 2µ(ℓ)〈x(ℓ− 1)−x∗,∇ f (x(ℓ− 1))〉+ µ2(ℓ)E[‖∇xℓ(x(ℓ− 1),z(ℓ))‖2 | x(ℓ− 1)] ≤ (1− 2mµ(ℓ)+Bµ2(ℓ))d(ℓ− 1)+ µ2(ℓ− 1)A\nand E[d(ℓ)]≤ (1− 2mµ(ℓ)+Bµ2(ℓ))E[d(ℓ− 1)]+ µ2(ℓ− 1)A\nSince B > m, we have\n2mµ −Bµ2 ≤ 2 √\nB 2 µ\n(\n1− √\nB 2 µ\n)\n≤ 2 1 4 = 1 2\nand so\n1− 2mµ(ℓ)+Bµ2(ℓ)≥ 1− 1 2 = 1 2\nSince this quantity is non-negative, we can unwind this recursion to yield\nE[d(ℓ)]≤ ℓ\n∏ k=1\n(1− 2mµ(ℓ)+Bµ2(ℓ))+ ℓ\n∑ k=1\nℓ\n∏ i=k+1 (1− 2mµ(i)+Bµ2(i))µ2(k)\nThe bound in Lemma 17 can be further bounded into a closed form as follows from [28]: Define\nϕβ (t) =\n{\ntβ−1 β , if β 6= 0 log(t), if β = 0\nThen with µ(ℓ) =Cℓ−α , it holds that\nE[d(ℓ)]≤ { 2exp { 2BC2ϕ1−2α(ℓ) } exp { −mC4 ℓ1−α }( E[d(0)]+ AB ) + 2ACmℓα , if 0 ≤ α < 1 exp{BC2}\nℓmC\n( E[d(0)]+ AB )\n+AC2 ϕmC/2−1(ℓ)\nℓmC/2 , if α = 1\nNote that this bound is a closed form but is substantially looser than Lemma 17. In the case that the functions in question have Lipschitz continuous gradients, we introduce a bound on the excess risk using Lemma 17. This case corresponds to choosing\nφ(x(0), . . . ,x(K)) = x(K)\nLemma 18. With arbitrary step sizes and assuming that f (x) has Lipschitz continuous gradients with modulus M, it holds that\nE[ f (x)]− f (x∗)≤ 1 2 ME[d(K)]\nand therefore, we set\nb(d0,K) = 1 2 M\n(\nK\n∏ ℓ=1\n(1− 2mµ(ℓ)+Bµ2(ℓ))+ K\n∑ ℓ=1\nK\n∏ i=ℓ+1\n(1− 2mµ(i)+Bµ2(i))µ2(ℓ) )\nProof. Using the descent lemma from [29], it holds that\nE[ f (x)]− f (x∗)≤ 1 2 ME[d(K)]\nPlugging in the bound from Lemma 17 yields the bound b(d0,K).\nNext, we introduce a bound inspired by [30] for the case where φ(x(0), . . . ,x(K)) corresponds to forming a convex combination of the iterates.\nLemma 19. With a constant step size and averaging with\nλ (ℓ) = { γ(ℓ) ∑Kτ=1 γ(τ) , if ℓ > 0\n0, if ℓ= 0\nwhere γ(ℓ) = (1−mµ +Bµ2)−ℓ\nit holds that\nb(d0,K) = d0\n2µ ∑Kℓ=0 γ(ℓ) +\n1 2 Aµ\nProof. By strong convexity, it holds that\n−〈x(ℓ− 1)−x∗,∇ f (x(ℓ− 1))〉 ≤ −m‖x(ℓ− 1)−x∗‖2 − ( f (x(ℓ− 1))− f (x∗))\nFollowing the Lyapunov-style analysis of Lemma 17, it holds that\nE[d(ℓ)]≤ (1−mµ +Bµ2)E[d(ℓ− 1)]− 2µ (E[ f (x(ℓ− 1))]− f (x∗))+Aµ2\nRearranging, using the telescoping sum, and using convexity, it holds that\nE[ f (x)]− f (x∗)≤ d0 2µ ∑Kτ=0 γ(τ) + 1 2 Aµ\nIf we set µ = 1√ K , then it holds that\nb(d0,K) = O\n(\n1√ K\n)\nfor Lemma 19. We consider an extension of the averaging scheme in [31]. The bound in this paper only works with B = 0, so we extend it slightly to handle B > 0.\nLemma 20. Consider the choice of step sizes given by\nµ(ℓ) = 1\nmℓ ∀ℓ≥ 1\nThen\nb(d0,K) = 1 2 d(0)+ 1 2 (K + 1)A+ 1 2 B∑ K ℓ=0 γ(ℓ)\n1+ 12 m(K + 1)(K+ 2)\nwhere E[d(ℓ)]≤ γ(ℓ)\nNote that we can use the bound in Lemma 17 here.\nProof. We have using Lyapunov style analysis\nE[d(ℓ)]≤ (1− 2mµ(ℓ)+Bµ2(ℓ))E[d(ℓ− 1)]− 2µ(ℓ)(E[ f (x(ℓ))]− f (x∗))+Aµ2(ℓ)\nThen we have\n1 µ2(ℓ) E[d(ℓ)]≤ ( 1− 2mµ(ℓ) µ2(ℓ) +B ) E[d(ℓ− 1)]− 2 µ(ℓ) (E[ f (x(ℓ))]− f (x∗)+A\nIt holds that\n1− 2mµ(ℓ) µ2(ℓ) − 1 µ2(ℓ− 1) = 1 µ2(ℓ) − 2m 1 µ(ℓ) − 1 µ2(ℓ− 1)\n= ℓ2 C2 − 2mℓ C − (ℓ− 1) 2 C2 = 2(mC− 1)L− 1\nC2\nAs long as we have\nmC− 1 ≤ 1 ⇔ C ≤ 2 m\nthen we get\n1 µ2(ℓ) E[d(ℓ)]− 1 µ2(ℓ− 1)E[d(ℓ− 1)]≤ BE[d(ℓ− 1)]− 2 µ(ℓ) (E[ f (x(ℓ))]− f (x∗)+A\nSumming an rearranging yields\nK\n∑ ℓ=0\n1 µ(ℓ) (E[ f (x(ℓ))]− f (x∗))≤ 1 2 d(0)+ 1 2 (K + 1)A+ 1 2 B K\n∑ ℓ=0 E[d(ℓ)]\nwith µ(0) = 1 by convention. With the weights\nγ(ℓ) = 1 µ(ℓ)\n∑ℓj=0 1µ( j)\nwe have\nE[ f (x̄(K))]− f (x∗)≤ 1 2 d(0)+ 1 2(K + 1)A+ 1 2 B∑ K ℓ=0E[d(ℓ)]\n∑Kτ=0 1µ(τ) Then it holds that\nK\n∑ τ=0\n= 1+ K\n∑ τ=1 mτ = 1+ 1 2 m(K + 1)(K + 2)\nso\nE[ f (x̄(K))]− f (x∗)≤ 1 2 d(0)+ 1 2(K + 1)A+ 1 2 B∑ K ℓ=0E[d(ℓ)]\n1+ 12 m(K + 1)(K+ 2)\nFor the choice of step sizes in Lemma 20 from Lemma 17, it holds that\nE[d(ℓ)] = O\n(\n1 ℓ\n)\nSince K\n∑ ℓ=1 1 ℓ = O (logK)\nit holds that\nE[ f (x̄(K))]− f (x∗) = O ( d(0) K2 + log(K) K2 + 1 K )\nNote that a rate of O( 1K ) is minimax optimal for stochastic minimization of a strongly convex function [32]. Next, we look at a special case of averaging for functions such that\nE‖∇xℓ(x,z)−∇xℓ(x̃,z)−∇2xxℓ(x̃,z)(x− x̃)‖2 = 0\nfrom [28]. For example, quadratics satisfy this condition.\nLemma 21. Assuming that\nE‖∇xℓ(x,z)−∇xℓ(x̃,z)−∇2xxℓ(x̃,z)(x− x̃)‖2 = 0,\nwe select step sizes µ(ℓ) =Cℓ−α\nwith α > 1/2, and\nλ (ℓ) =\n{\n1 K , if ℓ > 0 0, if ℓ= 0\nit holds that\n( E[d̄(K)] )1/2\n≤ 1 m1/2 K−1 ∑ k=1 ∣ ∣ ∣ ∣ 1 µ(k+ 1) − 1 µ(k) ∣ ∣ ∣ ∣ (E[d(k)])1/2 + 1 m1/2µ(1) (E[d(0)])1/2 +\n1\nm1/2µ(K) (E[d(K)])1/2\n+\n√\nA mK +\n√\n2B mK2\nK\n∑ k=1 E[d(k− 1)]\nwith d̄(K) = ‖x̄(K)−x∗‖2. If in addition f has Lipschitz continuous gradients with modulus M, then it holds that\nE[ f (x̄(K))]− f (x∗)≤ 1 2 ME[d̄(K)]\nProof. Suppose that we set\nx̄(K) = 1 n\nK\n∑ k=1 x(k)\nThen it holds that\n∇2 xx f (x∗)(x(k)−x∗) = ∇xℓ(x(k− 1),z(k− 1))−∇xℓ(x∗,z(k− 1)) + [\n∇2 xx f (x∗)−∇2 xx ℓ(x∗,z(k− 1)) ] (x(k− 1)−x∗)\nyielding\n∇2 xx f (x∗)(x̄(k)−x∗) = 1 K\nK\n∑ k=1\n∇xℓ(x(k− 1),z(k− 1))− 1 K\nK\n∑ k=1\n∇xℓ(x∗,z(k− 1))\n+ 1 K\nK\n∑ k=1\n[\n∇2 xx f (x∗)−∇2 xx ℓ(x∗,z(k− 1)) ] (x(k− 1)−x∗)\nFirst, we have\n1 K\nK\n∑ k=1\n∇xℓ(x(k− 1),z(k− 1)) = 1 K\nK\n∑ k=1 ∇xℓ(x(ℓ− 1),z(ℓ− 1))\n= 1 K\nK\n∑ k=1\n1 µ(k) (x(ℓ− 1)−x(ℓ))\n= 1 K\nK\n∑ k=1\n1 µ(k) (x(ℓ− 1)−x∗)− 1 K\nK\n∑ k=1\n1 µ(k) (x(ℓ)−x∗)\n= 1 K K−1 ∑ k=1 ( 1 µ(k+ 1) − 1 µ(k) ) (x(ℓ)−x∗)+ 1 µ(1) (x(0)−x∗)\n− 1 µ(K) (x(K)−x∗)\nSecond, we have\nE\n∥ ∥ ∥ ∥ 1 K K\n∑ k=1\n∇xℓ(x∗,z(k− 1)) ∥ ∥ ∥\n∥\n2\n= 1\nK2\nK\n∑ k=1\nE‖∇xℓ(x∗,z(k− 1))‖2\n≤ A n2\nThird, we have\nE\n∥ ∥ ∥ ∥ 1 K K ∑ k=1 [ ∇2 xx f (x∗)−∇2 xx ℓ(x∗,z(k− 1)) ] (x(k− 1)−x∗) ∥ ∥ ∥ ∥ 2 ≤ 2B K2 K ∑ k=1 E[d(k− 1)]\nCombining these bounds with Minkowski’s inequality yields\n( mE[d̄(K)] )1/2\n≤ ( E‖∇2 xx f (x∗)(x̄(K)−x∗)‖2 )1/2 ≤ K−1 ∑ k=1 ∣ ∣ ∣ ∣ 1 µ(k+ 1) − 1 µ(k) ∣ ∣ ∣ ∣ (E[d(k)])1/2 + 1 µ(1) (E[d(0)])1/2 + 1 µ(K) (E[d(K)])1/2\n+\n√\nA K +\n√\n2B K2\nK\n∑ k=1 E[d(k− 1)]\nThen we have (\nE[d̄(K)] )1/2\n≤ 1 m1/2 K−1 ∑ k=1 ∣ ∣ ∣ ∣ 1 µ(k+ 1) − 1 µ(k) ∣ ∣ ∣ ∣ (E[d(k)])1/2 + 1 m1/2µ(1) (E[d(0)])1/2 +\n1\nm1/2µ(K) (E[d(K)])1/2\n+\n√\nA mK +\n√\n2B mK2\nK\n∑ k=1 E[d(k− 1)]\nThis decays at rate O ( 1 K ) as long as µ(ℓ) =Cℓ−α with 12 ≤ α ≤ 1.\nB Useful Concentration Inequalities\nFor our analysis of both the direct and IPM estimates, we need the following key technical lemma from [33]. This lemma controls the concentration of sums of random variables that are sub-Gaussian conditioned on a particular filtration {Fi}ni=0. Such a collection of random variables is referred to as a sub-Gaussian martingale sequence. We include the proof for completeness.\nLemma 22 (Theorem 7.5 of [33]). Suppose we have a collection of random variables {Vi}ni=1 and a filtration {Fi}ni=0 such that for each random variable Vi it holds that\n1. E [ esVi ∣ ∣ Fi−1 ] ≤ e 12 σ 2i s2 with σ2i a constant\n2. Vi is Fi-measurable\nThen for every a ∈Rn it holds that\nP\n{\nn\n∑ i=1 aiVi > t\n}\n≤ exp { − t 2\n2ν\n}\n∀t > 0\nand\nP\n{\nn\n∑ i=1\naiVi <−t } ≤ exp { − t 2\n2ν\n}\n∀t > 0\nwith\nν = n\n∑ i=1 σ2i a 2 i\nProof. We bound the moment generating function of ∑ni=1 aiVi by induction. As a base case, we have\nE [ esa1V1 ] = E [ E [ esa1V1 ∣ ∣\n∣ F0\n]]\n≤ e 12 σ 21 a21s2\nAssume for induction that we have\nE\n[\nexp\n{\ns j\n∑ i=1 aiVi\n}] ≤ exp {\n1 2\n(\nj\n∑ i=1 σ2i a 2 i\n) s2 }\nThen we have\nE\n[\nexp\n{\nj+1 ∑ i=1 aiVi\n}]\n= E\n[\nexp\n{\ns j\n∑ i=1 aiVi\n}\nesa j+1X j+1\n]\n= E\n[\nE\n[\nexp\n{\ns j\n∑ i=1 aiVi\n}\nesa j+1X j+1 ∣ ∣\n∣ F j+1\n]]\n(a) = E\n[\nexp\n{\ns j\n∑ i=1 aiVi\n}\nE\n[ esa j+1X j+1 ∣ ∣\n∣ F j+1\n]\n]\n(b) ≤ E\n[\nexp\n{\ns j\n∑ i=1 aiVi\n}]\ne 1 2 σ 2 j+1a 2 j+1s 2\n(c) ≤ exp\n{\n1 2\n(\nj+1 ∑ i=1 σ2i a 2 i\n) s2 }\nwhere (a) follows since ∑ ji=1 aiVi is F j measurable, (b) follows since\nE\n[ esa j+1X j+1 ∣ ∣\n∣ F j+1\n]\n≤ e 1 2 σ 2 j+1a 2 j+1s 2 ,\nand (c) is the inductive assumption. This proves that\nE\n[\nexp\n{\ns n\n∑ i=1 aiVi\n}] ≤ exp {\n1 2\n(\nn\n∑ i=1 σ2i a 2 i\n) s2 } ≤ exp {\n1 2 νs2 }\nUsing the Chernoff bound [19], we have\nP\n{\nn\n∑ i=1 aiVi > t\n} ≤ e−stE [ exp { s n\n∑ i=1 aiVi\n}]\n≤ exp { −st + 1 2 νs2 }\nOptimizing the bound over s yields\nP\n{\nn\n∑ i=1 aiVi > t\n}\n≤ exp { − t 2\n2ν\n}\nThe proof for the other tail is similar.\nIf the random variables instead satisfy\n1. E [ exp { s ( Vi −E [ Vi ∣ ∣ Fi−1 ])} ∣ ∣ Fi−1 ] ≤ e 12 σ 2i s2 with σ2i a constant 2. Vi is Fi-measurable\nthen Lemma 22 can be applied to {Vi−E [ Vi ∣ ∣ Fi−1 ] }ni=1 to yield\nP\n{\nn\n∑ i=1\naiVi > n\n∑ i=1 aiE [ Vi ∣ ∣ Fi−1 ] + t\n}\n≤ exp { − t 2\n2ν\n}\nIf we can upper bound the conditional expectations\nE [ Vi ∣ ∣ Fi−1 ] ≤Ci, by Fi−1-measurable random variables Ci, then we have\nP\n{\nn\n∑ i=1\naiVi > n\n∑ i=1 aiCi + t\n} ≤ P { n\n∑ i=1\naiVi > n\n∑ i=1 aiE [ Vi ∣ ∣ Fi−1 ] + t\n}\n≤ exp { − t 2\n2ν\n}\nFor our analysis, we generally cannot compute E [ Vi ∣ ∣ Fi−1 ]\n, but we can find “nice” Ci. To find σ2i for use in Lemma 22, we frequently use the following conditional version of Hoeffding’s Lemma.\nLemma 23 (Conditional Hoeffding’s Lemma). If a random variable V and a sigma algebra F satisfy a ≤V ≤ b and E[V |F ] = 0, then\nE [ esV | F ] ≤ exp { 1 8 (b− a)2s2 }\nProof. We follow standard proof of Hoeffding’s Lemma from [19]. Since esx is convex, it follows that\nesx ≤ b− x b− ae sa + x− a b− ae sb a ≤ x ≤ b\nTherefore, taking the conditional expectation with respect to F yields\nE [ esV ∣ ∣ F ] ≤ b−E [V | F ] b− a e sa + E [V | F ]− a b− a e sb (22)\nLet h = s(b− a), p =− ab−a , and L(h) =−hp+ log(1− p+ peh). Then we have\neL(h) = b\nb− ae sa + −a b− ae sb\n= b−E [V | F ]\nb− a e sa + E [V | F ]− a b− a e sb (23)\nsince E [V | F ] = 0. Since L(h) = L′(h) = 0 and L′′(h)≤ 14 ,, it holds that L(h)≤ 18 (b− a)2s2. Combining this bound on L(h) with (22) and (23) yields the result.\nBefore proceeding with our analysis, we need to introduce a few useful concentration inequalities for sub-Gaussian vector-valued random variables. First, for a scalar random variable ξ , define the sub-Gaussian norm\nτ(ξ ) = inf { a > 0\n∣ ∣ ∣ ∣ E[esξ ]≤ e 12 a2s2 ∀s ≥ 0 }\n(24)\nClearly, if τ(ξ )<+∞, then ξ is sub-Gaussian. Second, for a random vector v in Rd , define\nB(v) = d\n∑ i=1 τ((v)i) (25)\nwhere (v)i is the ith component of v. We define v to be sub-Gaussian if B(v)<+∞. Of crucial importance in our analysis is analyzing the norm of an average of vector-valued sub-Gaussian random variables. The following lemma describes how to control the sub-Gaussian norm in such a situation.\nLemma 24. Suppose that {vi}Ki=1 is a collection of independent sub-Gaussian random variables in Rd . Then it holds that\nB\n(\n1 K\nK\n∑ i=1 vi\n)\n≤ 1 K\nd\n∑ j=1\n√\nK\n∑ i=1 τ2((vi) j)\nIf in addition the random variables {vi}Ki=1 satisfy\nmax i=1,...,K max j=1,...,d\nτ2((vi) j)≤ τ2\nthen it holds that\nB\n(\n1 K\nK\n∑ i=1 vi\n)\n≤ τd√ K\nProof. We analyze one component of the sum 1K ∑ K i=1vi. It holds that\nE\n\nexp\n\n\n\ns\n(\n1 K\nK\n∑ i=1 vi\n)\nj\n\n\n\n\n = E\n[\nexp\n{\ns K\nK\n∑ i=1 (vi) j\n}]\n= K\n∏ i=1 E\n[ exp { s\nK (vi) j\n}]\n≤ K\n∏ i=1 exp\n{\n1 2 1 K2 τ2((vi) j)s2 }\n= exp\n{\n1 2\n(\n1 K2\nK\n∑ i=1 τ2((vi) j)\n) s2 }\nThis implies that\nτ\n\n\n(\n1 K\nK\n∑ i=1 vi\n)\nj\n\n≤ 1 K\n√\nK\n∑ i=1 τ2((vi) j)\nand so\nB\n(\n1 K\nK\n∑ i=1 vi\n)\n≤ 1 K\nd\n∑ j=1\n√\nK\n∑ i=1 τ2((vi) j)\nFinally, if τ2((vi) j)≤ τ2, then we have\nB\n(\n1 K\nK\n∑ i=1 vi\n)\n≤ 1 K\nd\n∑ j=1\n√\nK\n∑ i=1 τ2((vi) j)\n≤ d K\n√\nK\n∑ i=1 τ2\n= τd√\nK\nExample 3.2 from [17], a consequence of Theorem 3.1 in [17], is useful for the concentration of the norm of sub-Gaussian vector random variables.\nLemma 25 (Example 3.2 of [17]). If v is a random vector in Rd with B(v)<+∞, then\nP{‖v‖> t} ≤ 2exp { − t 2\n2B2(v)\n}\nFinally, we will also need to deal with dependent random variables that are sub-Gaussian with respect to a particular filtration.\nLemma 26. Suppose that a random variable V and a sigma algebra F satisfies\n1. E [V | F ] = 0\n2. P { |V |> t ∣ ∣ F } ≤ 2e−ct2 with c a constant. Then it holds that\nE[esV ∣ ∣ F ] ≤ exp { 1 2 ( 9 c ) s2 }\nfor all s ≥ 0.\nProof. Adapted from the characterization of sub-Gaussian random variables in [15]. First, we have for any a < c that\nE\n[ eaV 2 ∣ ∣ ∣ F ] ≤ 1+ ∫ ∞\n0 2ateat 2 P{|V |> t | F}dt\n≤ 1+ ∫ ∞\n0 2ate−(c−a)t 2 dt\n= 1+ 2a\nc− a\nSetting a = c3 yields the bound\nE\n[ eaV 2 ∣ ∣ ∣ F ] ≤ 2\nSince E [V | F ] = 0, by a Taylor expansion we have\nE [ esV ∣ ∣ F ] = 1+ ∫ ∞\n0 (1− y)E\n[ (sV )2eysV ∣ ∣ ∣ F ] dy\n≤ ( 1+ s2\na\n)\ne s2 2a\n≤ exp { 5s2\n2a\n}\n= exp\n{\n1 2\n(\n9 c\n) s2 }"
    } ],
    "references" : [ {
      "title" : "Foundations of Machine Learning",
      "author" : [ "M. Mohri", "A. Rostamizadeh", "A. Talwalkar" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and S",
      "author" : [ "A. Agarwal", "H. Daumé" ],
      "venue" : "Gerber, “Learning multiple tasks using manifold regularization.,” in NIPS",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Regularized multi–task learning,",
      "author" : [ "T. Evgeniou", "M. Pontil" ],
      "venue" : "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "A convex formulation for learning task relationships in multi-task learning,",
      "author" : [ "Y. Zhang", "D. Yeung" ],
      "venue" : "CoRR, vol. abs/1203.3536,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A survey on transfer learning,",
      "author" : [ "S. Pan", "Q. Yang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Matrix regularization techniques for online multitask learning,",
      "author" : [ "A. Agarwal", "A. Rakhlin", "P. Bartlett" ],
      "venue" : "Tech. Rep. UCB/EECS-2008-138, EECS Department,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "and A",
      "author" : [ "Z. Towfic", "J. Chu" ],
      "venue" : "Sayed, “Online distirubted online classifcation in the midst of concept drifts,” Neurocomputing, vol. 112, pp. 138–152",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and M",
      "author" : [ "C. Tekin", "L. Canzian" ],
      "venue" : "van der Schaar, “Context adaptive big data stream mining,” in Allerton Conference",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Machine learning for sequential data: A review,",
      "author" : [ "T. Dietterich" ],
      "venue" : "Structural, Syntactic, and Statistical Pattern Recognition,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Adaptive fraud detection.,",
      "author" : [ "T. Fawcett", "F. Provost" ],
      "venue" : "Data Min. Knowl. Discov.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Predicting the secondary structure of globular proteins using neural network models,",
      "author" : [ "N. Qian", "T. Sejnowski" ],
      "venue" : "Journal of Molecular Biology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1988
    }, {
      "title" : "Input-output HMM’s for sequence processing,",
      "author" : [ "Y. Bengio", "P. Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Implicit Functions and Solution Mappings: A View from Variational Analysis",
      "author" : [ "A. Dontchev", "R. Rockafellar" ],
      "venue" : "Springer, New York, New York",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the empirical estimation of integral probability metrics,",
      "author" : [ "B. Sriperumbudur" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Introduction to non-asymptotic analysis of random matrices,",
      "author" : [ "R. Veryshin" ],
      "venue" : "Tech. Rep., University of Michigan,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "and A",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan" ],
      "venue" : "Shapiro, “Stochastic approximation approach to stochastic programming,” SIAM Journal on Optimization, vol. 19, pp. 1574–1609",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Pechuk, “Inequalities for the distributions of functionals of sub-gaussian vectors,",
      "author" : [ "E.D.V.V Buldygin" ],
      "venue" : "Theor. Probability and Math. Statist., pp",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Large deviations for sums of partly dependent random variables,",
      "author" : [ "S. Janson" ],
      "venue" : "Random Structures Algorithms,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Concentration Inequalities: A Nonasymptotic Theory of Independence",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : "Oxford University Press",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Numerical Linear Algebra",
      "author" : [ "L. Trefethen" ],
      "venue" : "SIAM",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Uniqueness of positive fixed points for increasing concave functions on rn: An elementary result,",
      "author" : [ "J. Kennan" ],
      "venue" : "Review of Economic Dynamics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2001
    }, {
      "title" : "Fixed Point Theory",
      "author" : [ "A. Granas", "J. Dugundji" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Trends in income inequality",
      "author" : [ "S. Jenkins", "P. Van Kerm" ],
      "venue" : "pro-poor income growth, and income mobility,” Oxford Economic Papers, vol. 58, no. 3, pp. 531–548",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The elements of statistical learning: data mining",
      "author" : [ "T. Hastie", "R. Tibshirani", "J.H. Friedman" ],
      "venue" : "inference, and prediction: with 200 full-color illustrations, New York: Springer-Verlag",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning,",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Nonlinear Programming",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Athena Scientific",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Online learning and stochastic approximations,",
      "author" : [ "Léon Bottou" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1998
    }, {
      "title" : "Analysis of mirror descent for strongly convex functions,",
      "author" : [ "A. Nedic", "S. Lee" ],
      "venue" : "ArXiV,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Yu. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "which is a standard criterion for optimization and learning problems [1].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "In transfer learning, knowledge from one source task is transferred to another target task either with or without additional training data for the target task [5].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "For multi-task and transfer learning, there are theoretical guarantees on regret for some algorithms [6].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "For example, we could observe a feature wn and predict the label yn as in [7].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "There are also some bandit approaches in which one of a finite number of predictors must be applied to the data as in [8].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "Another relevant model is sequential supervised learning (see [9]) in which we observe a stream of data consisting of feature/label pairs (wn,yn) at time n, with wn being the feature vector and yn being the label.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn−i,yn−i)}i=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn−i,yn−i)}i=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "Another approach is to assume that there is an underlying hidden Markov model (HMM) [12].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "Using the triangle inequality and variational inequalities from [13] yields ‖xi −xi−1‖ ≤ ‖xi −xi−1‖+ ‖xi−xi ‖+ ‖xi−1−xi−1‖ ≤ ‖xi −xi−1‖+ 1 m ‖∇x fi(xi)‖+ 1 m ‖∇x fi(xi−1)‖ We then approximate ‖∇x fi(xi)‖= ‖Ezi∼pi [∇xl(xi,zi)]‖ by",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "2 Vector Integral Probability Metric Estimate Given a class of functions F where each f ∈F maps Z →R, an integral probability metric (IPM) [14] between two distributions p and q is defined to be γF (p,q), sup f∈F ∣ Ez∼p[ f (z)]−Ez̃∼q[ f (z̃)] ∣",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "We consider an extension of this idea, which we call a vector IPM, in which the class of functions F maps Z → X : γV F (p,q), sup f∈F ‖Ez∼p[ f (z)]−Ez̃∼q[ f (z̃)]‖ (7) Lemma 1 shows that a vector IPM can be used to bound the change in minimizer at time i and follows from variational inequalities in [13] and the assumption that {∇xl(x, ·) : x ∈ X } ⊂ F .",
      "startOffset" : 300,
      "endOffset" : 304
    }, {
      "referenceID" : 12,
      "context" : "By exploiting variational inequalities from [13], we can show that ‖xi −xi−1‖ ≤ 1 m ‖∇x fi(xi−1)−∇x fi−1(xi−1)‖ = 1 m ‖Ezi∼pi [ ∇xl(xi−1,zi) ] −Ezi−1∼pi−1 [ ∇xl(xi−1,zi−1) ] ‖ By assumption {∇xl(xi−1, ·) : x ∈ X } ⊂ F , so ‖∇x fi(xi−1)−∇x fi−1(xi−1)‖ = ‖Ezi∼pi [ l(xi−1,zi) ] −Ezi−1∼pi−1 [ l(xi−1,zi−1) ] ‖ ≤ sup f∈F ‖Ezi∼pi [ f (zi)]−Ezi−1∼pi−1 [ f (zi−1)]‖ = γ F (pi, pi−1) We cannot compute this vector IPM, since we do not know the distributions pi and pi−1.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "This allows us to apply standard concentration inequalities for norms of random variables as in [15].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "This is a common assumption for in high probability analysis of optimization algorithms as in [16] for example.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "By applying Lemma 25 from [17] to the conditional distribution P{·|Fi−1}, we have P {∥ ∥ ∥ ∥ ∥ 1 Ki Ki ∑ k=1 (∇xl(x̃i,zi(k))−∇x fi(x̃i)) ∥ ∥ ∥ ∥ ∥ > t ∣ ∣ ∣ ∣ ∣ Fi−1 }",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "The proof of Lemma 5 is nearly identical to the proof of the extension of Hoeffding’s inequality from [18] with Lemma 22 used instead.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "Applying the Chernoff bound [19] and optimizing yields P { n ∑ i=1 Vi > t }",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "To apply gradient descent, we use eigenvalue perturbation results [20].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "3 from [21] to conclude that there exists a unique, positive fixed point v̄ of φK∗(v).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 21,
      "context" : "Following [23], for any fixed point v̄, it holds that |φK∗(v)− v̄| ≤ φ ′ K∗(v̄)|v− v̄| Therefore, applying the fixed point property repeatedly yields |φ (n) K∗ (v)− v̄| ≤ (φ ′ K∗(v̄))|v− v̄| By Lemma 14, it holds that φ ′ K∗(v̄)< 1 and so the result follows.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 22,
      "context" : "chosen based on previous economic studies in [25].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "We average over twenty runs of our algorithm by resampling without replacement [26].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "This is a smoothed version of the hinge loss used in support vector machines (SVM) [26].",
      "startOffset" : 83,
      "endOffset" : 87
    } ],
    "year" : 2015,
    "abstractText" : "A framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such as stochastic gradient descent (SGD). The optimization problems change slowly in the sense that the minimizers change at either a fixed or bounded rate. A method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples needed from the distributions underlying each problem in order to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. Experiments with synthetic and real data are used to confirm that this approach performs well.",
    "creator" : "LaTeX with hyperref package"
  }
}