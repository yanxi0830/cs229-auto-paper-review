{
  "name" : "1506.02222.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "No penalty no tears: Least squares in high-dimensional linear models",
    "authors" : [ "Xiangyu Wang", "David Dusnon", "Chenlei Leng" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms."
    }, {
      "heading" : "1 Introduction",
      "text" : "Long known for its consistency, simplicity and optimality under mild conditions, ordinary least squares (OLS) is the most widely used technique for fitting linear models. Developed originally for fitting fixed dimensional linear models, unfortunately, classical OLS fails in high dimensional linear models where the number of predictors p far exceeds the number of observations n. To deal with this problem, Tibshirani[1] proposed `1-penalized regression, a.k.a, lasso, which triggered the recent overwhelming exploration in both theory and methodology of penalization-based methods. These methods usually assume that only a small number of coefficients are nonzero (known as the sparsity assumption), and minimize the same least squares loss function as OLS by including an additional penalty on the coefficients, with the typical choice being the `1 norm. Such “penalization” constrains the solution space to certain directions favoring sparsity of the solution, and thus overcomes the non-unique issue with OLS. It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].\nar X\niv :1\n50 6.\n02 22\n2v 1\n[ st\nat .M\nE ]\n7 J\nun 2\n01 5\nDespite the success of the methods based on regularization, there are important issues that can not be easily neglected. On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6]. On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense. These concerns have limited the practical use of regularized methods, motivating alternative strategies such as direct hard thresholding [7].\nIn this article, we aim to solve the problem of fitting high-dimensional sparse linear models by reconsidering OLS and answering the following simple question: Can ordinary least squares consistently fit these models with some suitable algorithms? Our result provides an affirmative answer to this question under fairly general settings. In particular, we give a generalized form of OLS in high dimensional linear regression, and develop two algorithms that can consistently estimate the coefficients and recover the support. These algorithms involve least squares type of fitting and hard thresholding, and are non-iterative in nature. Extensive empirical experiments are provided in Section 4 to compare the proposed estimators to many existing penalization methods. The performance of the new estimators is very competitive under various setups in terms of model selection, parameter estimation and computational time.\nRelated works The work that is most closely related to ours is [8], in which the authors proposed an algorithm based on OLS and the ridge regression. However, both their methodology and theory are still within the `1 regularization framework, and their conditions (especially their C-Ridge and C-OLS conditions) are overly strong and can be easily violated in practice. [7] proposed an iterative hard thresholding algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm. Nevertheless, their motivation is completely different, their algorithm lacks theoretical guarantees for consistent support recovery, and they require an iterative estimation procedure.\nOur contributions We provide a generalized form of OLS for fitting high dimensional data motivated by ridge regression, and develop two algorithms that can consistently fit a sparse linear model and recover its support. We summarize the advantages of our new algorithms in three points. First, our algorithms work for highly correlated features under random designs. The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso. Second, our algorithms can achieve consistent support recovery for general noise (with finite second-order moment) in the ultra-high dimension setting where log p = o(n). This is remarkable as most methods\n(c.f. [4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise. [6] proved that lasso also works for a second-order condition similar to ours, but requires two additional strong assumptions. Third, the algorithms are simple, efficient and scale well for large p. In particular, the matrix operations are fully parallelizable with very few communications for very large p, while regularization methods are either hard to be computed in parallel in the feature space, or the parallelization requires a large amount of machine communications.\nThe remainder of this article is organized as follows. In Section 2 we generalize the ordinary least squares estimator for high dimensional problems where p > n, and propose two three-step algorithms consisting only of least squares fitting and hard thresholding in a loose sense. Section 3 provides consistency theory for the algorithms. Section 4 evaluates the empirical performance. We conclude and discuss further implications of our algorithms in the last section. All the proofs are provided in the supplementary materials."
    }, {
      "heading" : "2 High dimensional ordinary least squares",
      "text" : "Consider the usual linear model\nY = Xβ + ε,\nwhere X is the n× p design matrix, Y is the n× 1 response vector and β is the coefficient. As is common in the high dimensional literature, we assume that most βi’s are zero except for a small subset S = supp(β) with cardinality s; i.e., S = {i|βi 6= 0} is the support of β and s = card(S).\nTo carefully tailor the low-dimensional OLS estimator for a high dimensional scenario, one needs to answer the following two questions. i) What is the correct form of OLS in the high dimensional setting? ii) How to correctly use this estimator? To answer these, we reconsider OLS from a different perspective. In fact, OLS can be viewed as the limit of the ridge estimator when the ridge parameter goes to zero, i.e.,\n(XTX)−1XTY = lim r→0 (XTX + rIp) −1XTY.\nOne nice property of the ridge estimator is that it exists regardless of the relationship between p and n. A keen observation[12] reveals the following relationship immediately.\nLemma 1. For any p, n, r > 0, we have\n(XTX + rIp) −1XTY = XT (XXT + rIn) −1Y. (1)\nNotice that the right hand side of (1) exists when p > n and r = 0. Consequently, we can naturally extend the classical OLS to the high dimensional scenario by letting r tend to zero in (1). Denote this high dimensional version of the OLS as\nβ̂(HD) = lim r→0 XT (XXT + rIn) −1Y = XT (XXT )−1Y.\nThe above equation indicates that β̂(HD) is essentially an orthogonal projection of β onto the row space of X. Unfortunately, this (low dimensional) projection does not have good general performance in estimating sparse vectors in high-dimensional cases. Instead of directly estimating β as β̂HD, however, this new estimator of β may be used for dimension reduction by observing β̂(HD) = XT (XXT )−1Xβ + XT (XXT )−1ε = Φβ + η [12]. Since η is stochastically small, if Φ is close to a diagonally dominant matrix and β is sparse, then the zero and non-zero coefficients can be separated by simply thresholding the small entries of β̂(HD). The exact meaning of this statement will be discussed in next section. Some simple examples demonstrating the diagonal dominance of XT (XXT )−1X are illustrated immediately in Figure 1, where the rows of X in the left two plots are drawn from N(0,Σ) with σij = 0.6 or σij = 0.99 |i−j|. The sample size and data dimension are chosen as (n, p) = (50, 1000). The right plot takes the standardized design matrix directly from the real data in Section 4 with (n, p) = (120, 5000). A clear diagonal dominance pattern is visible in each plot.\nThis ability to separate zero and non-zero coefficients allows us to first obtain a smaller model with size d such that s < d < p which includes all the nonzero variables in S. Once d is below n, one can directly apply the usual OLS to obtain an estimator, which will be thresholded further to obtain a more refined model. The final estimator will then be obtained\nby an OLS fit on the refined model. This three-stage non-iterative algorithm is termed Leastsquares adaptive thresholding (LAT) and the concrete procedure is described in Algorithm 1.\nAlgorithm 1 The Least-squares Adaptive Thresholding Algorithm (LAT) Initialization: 1: Input (Y,X), d, δ 2: # where X, Y are standardized data, n is the sample size, p is the number of features, d is the number of variables selected at stage 1 and δ ∈ (0, 1) is a tuning parameter determining the selection confidence Stage 1 : Pre-selection 3: Compute β̂(HD) = XT (XXT )−1Y . Rank the importance of the variables by |β̂(HD)i |; 4: Denote the model corresponding to the d largest |β̂(HD)i | as M̃d. Alternatively use eBIC\nin [13] in conjunction with the obtained variable importance to select the best submodel. Stage 2 : Hard thresholding\n5: β̂(OLS) = (XTM̃d XM̃d) −1XTM̃d Y ; 6: σ̂2 = ∑n\ni=1(y − ŷ)2/(n− d); 7: C̄ = (XTM̃d XM̃d) −1;\n8: Hard threshold β̂(OLS) by mean( √ 2σ̂2C̄ii log(4d/δ)) or use BIC to select the best sub-\nmodel. Denote the chosen model as M̂. Stage 3 : Refinement\n9: β̂M̂ = (X T M̂XM̂) −1XTM̂Y ;\n10: β̂i = 0,∀i 6∈ M̂; 11: return β̂\nThe C̄ in Stage 2 can be replaced by its ridge version (XTM̃d XM̃d + rId) −1 to stabilize numerical computation. This variant of the algorithm is referred to as the Ridge Adaptive Thresholding (RAT) algorithm."
    }, {
      "heading" : "3 Theory",
      "text" : "In this section, we prove the consistency of Algorithm 1 in selecting the true model and provide concrete forms for all the values needed for the algorithm to work. Recall the linear model Y = Xβ + ε. We consider the random design where the rows of X are drawn from a multivariate Gaussian distribution N(0,Σ). This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15]. The noise ε, as mentioned earlier, is only assumed to have the second-order moment, i.e., var(ε) = σ2 < ∞, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11]. This relaxation is similar to [6]; however we do not require any further assumptions needed by [6]. In Algorithm 1, we also propose to use extended BIC and BIC for parameter\ntuning. However, the corresponding details will not be pursued here, as their consistency is straightforwardly implied by the results from this section and the existing literature on extended BIC and BIC [13].\nDefine κ = cond(Σ) and τ = mini∈S |βi|. We state our result in three theorems.\nTheorem 1. Assume p > c0n for some c0 > 1 and var(Y ) ≤ M0. If s log p = O(nν) for some ν < 1, n > 4c0/(c0− 1)2, and γ is chosen to be γ = c1κ −1τ 2 n p , where c1 is some absolute constant specified in Lemma 2 in the supplementary materials, then for any δ ∈ (0, 1) we have\nP ( max i 6∈S |β̂(HD)i | ≤ γ ≤ min i∈S |β̂(HD)i | ) = 1−O ( σ2κ4 log p τ 2n1−δ ) .\nTheorem 1 guarantees the model selection consistency of the first stage of Algorithm 1. The proof of Theorem 1 relies on the diagonal dominance of matrix Φ = XT (XXT )−1X. In particular, it is shown that the diagonal terms of Φ are O(n p ) while the off-diagonal terms are O( √ n p ) [16]. Thus, with an appropriate signal-to-noise ratio and true model size, Φβ is likely to preserve a correct magnitude order of zero and nonzero coefficients, which can then be separated by a threshold γ. As γ is not easily computable based on data, we propose to rank the |β̂i|′s and select d largest coefficients. Alternatively, we can construct a series of nested models formed by ranking the largest n coefficients and adopt the extended BIC [13] to select the best submodel. Once the submodel M̃d is obtained, we proceed to the second stage by obtaining an estimate via ordinary least squares β̂(OLS) corresponding to M̃d. From Theorem 1, if d > s, we have that with probability tending to one,M∗ ⊆ M̃d, whereM∗ is the true model. Then for β̂(OLS) we have the following result.\nTheorem 2. Assume n ≥ 64κd log p, log p = O(nν) and d − s ≤ c̃ for some ν < 1 and c̃ > 0. If there exists some δ ∈ (0, 1) such that τ ≥ 2σ\nnδ/2 , then by choosing γ′ = σ nδ/2 we have\nP ( max i 6∈S |β̂(OLS)i | ≤ γ′ ≤ min i∈S |β̂(OLS)i | ) = 1−O ( κ log p log d n1−δ ) .\nTheorem 2 states that if τ = mini∈S |βi| ≥ γ′, where γ′ = σ/nδ/2, then by thresholding β̂(OLS) at γ′, we can identify the exact model with probability tending to 1. In fact, we have a similar result for ridge regression.\nTheorem 3 (Ridge regression). Assume the conditions in Theorem 2. If there exists some\nδ ∈ (0, 1) such that τ ≥ 4σ nδ/2 , then if the ridge parameter r satisfies that\nr ≤ O { min (√ n\nκ , σ\n1 2n1−δ/4\n82M 1 2\n0 κ 3 2\n)} ,\nwhere M0 is defined in Theorem 1, then by choosing γ ′ = 2σ\nnδ/2 we have\nP ( max i 6∈S |β̂(Ridge)i (r)| ≤ γ′ ≤ min i∈S |β̂(Ridge)i (r)| ) = 1−O ( κ log p log d n1−δ ) .\nNote that the ridge parameter r can be chosen as a constant, bypassing the need to specify r at least in theory. When the noise follows a Gaussian distribution, we can obtain a more explicit form of the threshold γ′, as the following Corollary shows.\nCorollary 1 (Gaussian noise). Assume ε ∼ N(0, σ2). For any δ ∈ (0, 1), define γ′ = 8 √ 2σ̂ √\n2κ log(4d/δ) n\n, where σ̂ is the estimated standard error as σ̂2 = ∑n\ni=1(yi − ŷi)2/(n − d). For sufficiently large n, if d ≤ n − 4K2 log(2/δ)/c for some absolute constants c, K and τ ≥ 24σ √ 2κ log(4d/δ)\nn , then with probability at least 1− 2δ, we have\n|β̂(OLS)i | ≥ γ′ ∀i ∈ S and |β̂ (OLS) i | ≤ γ′ ∀i 6∈ S.\nWrite C̄ = (XTM̃d XM̃d) −1 as in Algorithm 1. In practice, we propose to use γ′ = mean( √ 2σ̂2C̄ii log(4d/δ)) as the threshold (see Algorithm 1), because the estimation er-\nror takes a form of √ σ2C̄ii log(4d/δ). Alternatively, instead of identifying an explicit form of the threshold value (as is hard for general noise), one may also use BIC on nested models formed by ranking |β̂(OLS)| to search for the true model. Once the final model is obtained, as in Stage 3 of Algorithm 1, we refit it again using ordinary least squares. The final output will have the same output as if we knew the true model a priori with probability tending to 1, i.e., we have the following result.\nTheorem 4. Let M̂ and β̂ be the final output from LAT or RAT. Assume all conditions in Theorem 1, 2 and 3. Then with probability at least 1−O ( σ2κ4 log p τ2n1−δ + κ log p log d n1−δ ) we have\nM̂ =M∗, ‖β̂ − β‖22 ≤ 2sσ2\nnδ , and ‖β̂ − β‖∞ ≤\n2σ\nnδ/2 .\nAs implied by Theorem 1 – 4, LAT and RAT achieve consistent support recovery in the ultra-high dimensional (log p = o(n)) setting only with two assumptions: τ = O( √ (log p)/n) and var(ε) <∞, in contrast to most existing methods that require ε ∼ N(0, σ2) or ‖ε‖∞ < ∞."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we provide extensive numerical experiments for assessing the performance of LAT and RAT. In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4]. As it is well-known that the lasso estimator is biased, we also consider two variations of it by combining lasso with Stage 2 and 3 of our LAT and RAT algorithms, denoted as lasLAT (las1 in Figures) and lasRAT (las2 in Figures) respectively. We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+."
    }, {
      "heading" : "4.1 Synthetic datasets",
      "text" : "The model used in this section for comparison is the linear model Y = Xβ + ε, where ε ∼ N(0, σ2) and X ∼ N(0,Σ). To control the signal-to-noise ratio, we define r = ‖β‖2/σ, which is chosen to be 2.3 for all experiments. The sample size and the data dimension are chosen to be (n, p) = (200, 1000) or (n, p) = (500, 10000) for all experiments. For evaluation purposes, we consider four different structures of Σ below.\n(i) Independent predictors. The support is set as S = {1, 2, 3, 4, 5}. We generate Xi from a standard multivariate normal distribution with independent components. The coefficients are specified as\nβi = (−1)ui(|N(0, 1)|+ 1), where ui ∼ Ber(0.5) for i ∈ S and βi = 0 for i 6∈ S.\n(ii) Compound symmetry . All predictors are equally correlated with correlation ρ = 0.6. The coefficients are set to be βi = 3 for i = 1, ..., 5 and βi = 0 otherwise.\n(iii) Group structure . This example is Example 4 in [5], for which we allocate the 15 true variables into three groups. Specifically, the predictors are generated as\nx1+3m = z1 +N(0, 0.01), x2+3m = z2 +N(0, 0.01), x3+3m = z3 +N(0, 0.01),\nwhere m = 0, 1, 2, 3, 4 and zi ∼ N(0, 1) are independent. The coefficients are set as\nβi = 3, i = 1, 2, · · · , 15; βi = 0, i = 16, · · · , p.\n(iv) Factor models. This model is also considered in [20] and [21]. Let φj, j = 1, 2, · · · , k be independent standard normal variables. We set predictors as xi = ∑k j=1 φjfij + ηi, where fij and ηi are generated from independent standard normal distributions. The number of\nfactors is chosen as k = 5 in the simulation while the coefficients are specified the same as in Example (ii).\nTo compare the performance of all methods, we simulate 200 synthetic datasets for (n, p) = (200, 1000) and 100 for (n, p) = (500, 10000) for each example, and record i) the root mean squared error (RMSE): ‖β̂ − β‖2, ii) the false negatives (# FN), iii) the false positives (# FP) and iv) the actual runtime (in milliseconds). We use the extended BIC [13] to choose the parameters for any regularized algorithm. Due to the huge computation expense for scad and mc+, we only find the first d√pe predictors on the solution path (because we know s << √ p). For RAT and LAT, d is set to 0.3 × n. For RAT and larsRidge, we adopt a 10-fold cross-validation procedure to tune the ridge parameter r for a better finite-sample performance, although the theory allows r to be fixed as a constant. For all hard-thresholding steps, we fix δ = 0.5. The results for (n, p) = (200, 1000) are plotted in Figure 2, 3, 4 and 5 and more comprehensive results (average values for RMSE, # FPs, # FNs, runtime) are summarized in Table 1 and 2.\nAs can be seen from both the plots and the tables, the performance of LAT and RAT are on par with lasLAT for Example (i), (ii) and (iv), and are often among the best of\nall methods. For Example (iii), RAT and enet achieve the best performance while all the other methods fail to work. In addition, the runtime of LAT and RAT are also competitive compared to that of lasso and enet. We thus conclude that LAT and RAT achieve similar or even better performance compared to the usual regularized methods."
    }, {
      "heading" : "4.2 Real data",
      "text" : "This dataset, taken from [22], was collected to study mammalian eye diseases, with gene expression for the eye tissues of 120 twelve-week-old male F2 rats recorded. One gene coded as TRIM32 responsible for causing Bardet-Biedl syndrome is of particular interest, and is the response of interest.\nFollowing the method in [22], 18976 probes were selected as they exhibited sufficient signal for reliable analysis and at least 2-fold variation in expressions. Because TRIM32 is believed to be only linked to a small number of genes, we confine our attention to the top 5000 genes with the highest sample variance. The eight methods used in the simulation study are compared, where the performance is assessed via 10-fold cross validation. Because extended BIC does not offer a competitive prediction accuracy (It focuses on ensuring a good\nvariable selection performance) for regularized methods, for a fair comparison, we apply the conventional BIC instead of the extended BIC to all regularization methods, and record the means and the standard errors of the cross-validation. As a reference, we also report these values for the null model.\nIt can be seen that enet and lasso achieve the smallest cross-validation errors overall, followed by RAT and LAT. One caveat for the good performance of enet or lasso is the large number of variables it selected. If a more parsimonious model for interpretability is preferred, one might want to trade-off some accuracy by obtaining a model with a fewer number of variables given by LAT or RAT."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have proposed two novel algorithms Lat and Rat that only rely on least-squares type of fitting and hard thresholding, based on a high-dimensional generalization of OLS. The two methods are simple, easily implementable, and can consistently fit a high dimensional linear model and recover its support. The performance of the two methods are competitive compared to existing regularization methods. It is of great interest to further extend this framework to other models such as generalized linear models and models for survival analysis."
    }, {
      "heading" : "Appendix A: Proof of Theorem 1",
      "text" : "Recall the estimator β̂(HD) = XT (XXT )−1Y = XT (XXT )−1Xβ + XT (XXT )−1ε = ξ + η. The following two lemmas will be used to bound ξ and η respectively.\nLemma 2. Let Φ = XT (XXT )−1X. Assume p > c0n for some c0 > 1, then for any C > 0 there exists some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i ∈ Q, j 6= i,\nP ( |Φii| < c1κ−1 n\np ) ≤ 2e−Cn, P (|Φii| > c2κ\nn p\n) ≤ 2e−Cn (2)\nand\nP ( |Φij| > c4κt √ n\np\n) ≤ 5e−Cn + 2e−t2/2, (3)\nwhere c4 = √ c2(c0−c1)√ c3(c0−1) .\nThis is exactly the Lemma 3 in [16].\nLemma 3. Assume X follows N(0,Σ). If var( ) = σ2 and log p = o(n), then for any 0 < δ < 1 we have\nP ( ‖η‖∞ ≤ c1κ −1τ\n4\nn p\n) ≥ 1−O ( σ2κ4 log p\nτ 2n1−δ\n) ,\nwhere τ = mini∈S |βi| and κ = cond(Σ).\nTo prove Lemma 3 we need the following two propositions.\nProposition 1. (Lounici, 2008 [6]; Nemirovski, 2000 [23]) Let Yi ∈ Rp be random vectors with zero means and finite variances. Then we have for any k norm with k ∈ [2,∞] and p ≥ 3, we have\nE ∥∥ n∑ i=1 Yi ∥∥2 k ≤ C̃ min{k, log p} n∑ i=1 E‖Yi‖2k, (4)\nwhere C̃ is some absolute constant.\nAs each row of X is an iid draw from N(0,Σ), we define Z = XΣ−1/2, then Z ∼ N(0, Ip). For Z, we have the following result.\nProposition 2. Let Z ∼ N(0, Ip), then we have the minimum eigenvalue of ZZT/p satisfies that\nP ( λmin(ZZ\nT/p) > (1− n p − t p\n)2 ) ≥ 1− 2 exp(−t2/2)\nfor any t > 0. Assume p > c0n for c0 > 1 and take t = √ n. When n > 4c20/(c0 − 1)2, we have\nP ( λmin(ZZ T/p) > c ) ≥ 1− 2 exp(−n/2), (5)\nwhere c = (c0−1) 2\n4c20 .\nThe proof follows Corollary 5.35 in [24].\nProof of Lemma 3. Let A = pXT (XXT )−1 and define Z = XΣ−1/2. Consider the standard SVD on Z as Z = V DUT , where V and D are n× n matrices and U is a p× n matrix. Because Z is a matrix of iid Gaussian variables, its distribution is invariant under both left and right orthogonal transformation. In particular, for any T ∈ O(n), we have\nTV DUT (d) = V DUT ,\ni.e., V is uniformly distributed onO(n) conditional on U and D (they are in fact independent, but we don’t need such a strong condition). Therefore, we have\nA = pXT (XXT )−1 = pΣ 1 2ZT (ZΣZT )−1 = pΣ 1 2UDV T (V DUTΣUDV T )−1\n= pΣ 1 2U(UTΣU)−1D−1V T = √ pΣ 1 2U(UTΣU)−1 ( D √ p )−1 V T .\nBecause V is uniformly distributed conditional on U and D, the distribution of A is also invariant under right orthogonal transformation conditional on U and D, i.e., for any T ∈ O(n), we have\nA (d) = AT. (6)\nOur first goal is to bound the magnitude of individual entries Aij. Let vi = e T i AA T ei, which is a function of U and D (see below). From (6), we know that eTi A is uniformly distributed\non the sphere Sn−1( √ vi) if conditional on vi (i.e., conditional on U,D), which implies that\neTi A (d) = √ vi ( x1√∑n j=1 x 2 j , x2√∑n j=1 x 2 j , · · · , xn√∑n j=1 x 2 j ) , (7)\nwhere x′js are iid standard Gaussian variables. Thus, Aij can be bounded easily if we can bound vi. Notice that for vi we have\nvi = e T i AA T ei = pe T i Σ\n1 2U(UTΣU)−1 (D2 p )−1 (UTΣU)−1UTΣ 1 2 ei.\n= peTi H(U TΣU)− 1 2 (D2 p )−1 (UTΣU)− 1 2HT ei\n≤ peTi HHT ei · λ−1min(UTΣU) · λ−1min (D2 p ) Here H = Σ\n1 2U(UTΣU)−1/2 is defined the same as in [12] and can be bounded as eTi HH T ei ≤ c2nκ/p with probability 1 − 2 exp(−Cn) (see the proof of Lemma 3 in [16]). Therefore, we have\nP ( vi ≤ c2κ2λ−1min (D2 p ) n ) ≥ 1− 2 exp(−Cn)\nNow applying the tail bound and the concentration inequality to (7) we have for any t > 0 and any C > 0\nP (|xj| > t) ≤ 2 exp(−t2/2) P (∑n j=1 x 2 j\nn ≤ c3\n) ≤ exp(−Cn). (8)\nPutting the pieces all together, we have for any t > 0 and any C > 0 that\nP ( max ij |Aij| ≤ κt √ c2 c3 λ − 1 2 min (D2 p )) ≥ 1− 2np exp(−t2/2)− 3p exp(−Cn).\nNow according to (5), we can further bound λmin(D 2/p) and obtain that\nP ( max ij |Aij| ≤ √ c2 cc3 κt ) ≥ 1− 2np exp(−t2/2)− 3p exp(−Cn)− 2 exp(−n/2). (9)\nThe second step is to use (9) and Proposition 1 to bound η. The procedure follows almost the same as in Lounici’s paper. Define Zj = (A1j j, A2j j, · · · , Apj j). It’s clear that\nη = ∑n\nj=1 Zj/p. Applying Proposition 1 to Z ′ js and choosing the l∞ norm, we have\nE ∥∥ n∑ j=1 Zj ∥∥2 ∞ ≤ log p n∑ j=1 E‖Zj‖2∞ ≤ c2 cc3 σ2κ2t2n log p.\nUsing the Markov inequality on η, we have for any r > 0\nP ( ‖η‖∞ ≥ √ nr\np\n) = P ( p√ n ‖η‖∞ ≥ r ) ≤ p 2E‖η‖2∞ nr2 = E‖ ∑n j=1 Zj‖2∞ nr2\n≤ c2σ 2κ2t2 log p\ncc3r2 .\nTo match our previous result, we take r = c1 √ nτκ−1/4 and t = nδ/2 for some small δ,\nP ( ‖η‖∞ ≤ c1κ −1τ\n4\nn p\n) ≥ 1− c2σ 2κ4\nc21cc3τ 2\nlog p n1−δ − 2np exp(−nδ/2)− 3p exp(−Cn)− 2 exp(−n/2)\n≥ 1−O ( σ2κ4 log p\nτ 2n1−δ\n) .\nNow we are ready to prove Theorem 1\nProof of Theorem 1. Recall the definition of ξ as ξ = XT (XXT )−1Xβ. For any i ∈ S we have\nξi = e T i X T (XXT )−1Xβ = ∑ j∈S Φiiβi + ∑ j 6=i,j∈S Φijβj,\nand for i 6∈ S,\nξi = e T i X T (XXT )−1Xβ = ∑ j∈S Φijβj.\nAccording to our assumption we have mini∈S |βi| ≥ τ and var(Y ) = var(Xβ) = βTΣβ ≤ M0 for some M0. The latter one imples that\nM0 ≥ βTΣβ ≥ λmin(Σ)‖β‖22.\nTherefore, we have for any i ∈ S\n|ξi| ≥ c1κ−1τ n\np − ‖β‖2 √ ∑ j 6=i,j∈S Φ2ij ≥ c1κ−1τ n p − c4κ √ sM0t λ 1 2 min(Σ) √ n p = 3c1κ −1τ 4 n p ,\nif t is taken to be t = c1λ\n1 2 min(Σ)κ −2τ √ n\n4c4 √ M0s\n≥ c1κ − 52 τ √ n\n4c4 √ M0s . Hence, one can compute the probability to be greater than 1− 7 exp(−Cn)− 2 exp ( − c 2 1κ −5τ2 32c24M0s n ) . Similarly, with the same t we can show that for i 6∈ S\n|ξi| ≤ ‖β‖2 √ ∑\nj 6=i,j∈S\nΦ2ij ≤ c1κ −1τ\n4\nn p ,\nwith probability greater than 1 − 7 exp(−Cn) − 2 exp ( − c 2 1κ −5τ2 32c24M0s n ) . Next, using the result from Lemma 3, we can obtain\nP ( min i∈S |β̂i| ≥ c1κ −1τ 2 n p ) ≥ 1−O ( σ2κ4 log p τ 2n1−δ ) ,\nand\nP ( max i∈6S |β̂i| ≤ c1κ −1τ 2 n p ) ≥ 1−O ( σ2κ4 log p τ 2n1−δ ) .\nTaking γ = c1κ −1τ 2 np, we have\nP ( min i∈S |β̂i| ≥ γ ≥ max i 6∈S |β̂i| ) ≥ 1−O ( σ2κ4 log p τ 2n1−δ ) .\nProof of Theorem 2 and 3\nLemma 4. Let M̃d be a submodel that contains the true model M∗ and has a size of d. Define A = n(XTM̃d XM̃d) −1XTM̃d\nwhere XM̃d is the principal submatrix indexed by M̃d. Then for any t > 0 and C > 0, there exists some c3 > 0 such that\nP ( max\n|M̃d|=d,M∗⊆M̃d max ij |Aij| ≤ t√ c3λ0\n) ≥ 1− 2dn(p− s)d−s exp ( − t 2\n2\n) − d(p− s)d−s exp(−Cn),\nwhere λ0 = min|M̃d|=d,M∗⊆M̃d λmin(X T M̃d XM̃d/n).\nProof of Lemma 4. The proof is similar to the argument in Lemma 3. For a given M̃d, XM̃d follows N(0,ΣM̃d). Similarly, defining Z = XΣ −1/2 M̃d\n, then Z ∼ N(0, I). Assuming the singular value decomposition of Z is Z = V DUT where V is a n × d matrix and D,U are d × d matrices, and conditional on U,D, V is uniformly distributed on Vn,d. Therefore, we\nhave\nA = n(XTM̃dXM̃d) −1XTM̃d = nΣ 1/2 M̃d (ZTZ)−1ZT = nΣ 1/2 M̃d UD−1V T .\nWe observe that\n‖eTi A‖22 = n2Σ 1/2 M̃d UD−2UTΣ 1/2 M̃d = n2(XTM̃dXM̃d) −1 ≤ n λmin(XTM̃d XM̃d/n) .\nNext, following exactly the same argument in Lemma 3, we know that the distribution of A is invariant under the right orthogonal transformation and conditional on vi = ‖eTi A‖2, eTi A is uniformly distributed on Sn−1(vi). Using the same inequality in (8), we have\nP ( max ij |Aij| ≤\nt√ c3λmin(XTM̃d XM̃d/n)\n) ≥ 1− 2dn exp(−t2/2)− d exp(−Cn).\nNow the total number of possible M̃d is bounded by (p−s)× (p−s−1)×· · ·× (p−d+1) ≤ (p− s)(d−s). Therefore, we have\nP ( max\n|M̃d|=d,M∗⊆M̃d max ij |Aij| ≤ t√ c3λ0\n) ≥ 1− 2dn(p− s)d−s exp ( − t 2\n2\n) − d(p− s)d−s exp(−Cn),\nwhere λ0 = min|M̃d|=d,M∗⊆M̃d λmin(X T M̃d XM̃d/n).\nLemma 5 (Garvesh, Wainwright and Yu. (2010) [15]). There exists some absolute constant c′, c′′ > 0 such that\n‖Xv‖2√ n ≥ 1 4 ‖Σ 1 2v‖2 − 9ρ(Σ)\n√ log p\nn ‖v‖1, ∀v ∈ Rp,\nwith probability at least 1− c′′ exp(−c′n), where ρ(Σ) = maxi=1,2,··· ,p Σii.\nIn our case, for any v with d nonzero coordinates, we have ‖v‖1 ≤ √ d‖v‖2, ρ(Σ) = 1\nand ‖Σ1/2v‖2 ≥ κ− 1 2‖v‖2. Therefore,\n‖Xv‖2√ n ≥ ( κ−1/2 4 − 9 √ d log p n ) ‖v‖2, ‖v‖0 ≤ d.\nProof of Theorem 2. Lemma 5 essentially states that for any d × d principal submatrix of X, we can bound its smallest eigenvalue. Therefore, for any selected submodel M̃d from\nthe first stage, we have with probability at least 1−O(exp(−c′n))\nmin |M̃d|=d\nλ 1 2 min(X T M̃d XM̃d/n) ≥\nκ−1/2 4 − 9 √ d log p n ≥ κ −1/2 8 ,\nas long as n ≥ 64κd log p, i.e., λ0 ≥ κ −1\n64 , where λ0 is defined in Lemma 4.\nA direct calculation shows that β̂(OLS) = β+ (XTM̃d XM̃d) −1XTM̃d ε. Therefore, we want to\nbound the error\nη̃ = (XTM̃dXM̃d) −1XTM̃dε = Aε/n.\nFollowing the same argument as Lemma 3, we define Zj = (A1jεj, · · · , Adjεj) and η̃ =∑n j=1 Zj/n. Using Proposition 1 and Lemma 4 we have with probability at least 1− 2d(p− s)d−s exp(−t2/2)− d(p− s)d−s exp(−Cn)\nE ∥∥ n∑ j=1 Zj ∥∥2 ∞ ≤ log d n∑ j=1 E‖Zj‖2∞ ≤ σ2nt2 log d c3λ0 ≤ 64c−13 κσ2t2n log d. (10)\nThus, for any r > 0\nP ( ‖η̃‖∞ ≥ r\nn\n) = P (∥∥ n∑ j=1 Zj ∥∥ ∞ ≥ r ) ≤ E ∥∥∑n j=1 Zj ∥∥2 ∞ r2 ≤ 64κnσ 2t2 log d c3r2 .\nIf we take t = √ 2(c̃+ 3) log p for any δ ∈ (0, 1), then it is ensured that\n1− 2dn(p− s)d−s exp ( − t 2\n2\n) − d(p− s)d−s exp(−Cn)\n≥ 1− 2 exp ( (c̃+ 2) log p− (c̃+ 3) log p ) − exp ( (c̃+ 1) log p− Cn ) = 1−O ( 1\np\n) ≥ 1−O ( 1\nn\n) .\nNow taking r = σn1−δ/2 for any δ ∈ (0, 1) we have\nP ( ‖η̃‖∞ ≤ σ\nnδ/2\n) ≥ 1−O ( κ log p log d\nn1−δ\n) . (11)\nConsequently, for any δ > 0 we have\n‖β̂(OLS) − βM̃d‖∞ ≤ σ\nnδ/2 , (12)\nwith probability at least 1 − O ( κ log p log d\nn1−δ\n) . So if τ ≥ 2σ\nnδ/2 , then by choosing γ′ = σ nδ/2 we\nhave\nmin i∈S |β̂(OLS)i | ≥ γ′ ≥ max i 6∈S |β̂(OLS)i |.\nProof of Theorem 3. Denoting XM̃d by X, the definition of β̂(r) (Ridge) becomes\nβ̂(r)(Ridge) = (XTX + rId) −1XTXβ + (XTX + rId) −1XT ε\n= β − r(XTX + rId)−1β + (XTX + rId)−1XT ε = β − ξ̃(r) + η̃(r).\nFor ξ̃(r) we have\nmax |ξ̃(r)| ≤ r2βT (XTX + rId)−2β ≤ r2‖β‖22\nn2λ2min(X TX/n+ r/n)\n≤ 8 4r2κ3M0 n2\nwith probability 1 − c′′ exp(−c′n) if n ≥ 64κd log p. This result is because of Lemma 5 and M0 ≥ var(Y ) ≥ ‖β‖22λmax(Σ).\nFor η̃(r), we follow the same technique in the proof of Theorem 2. Basically, one just needs to show a similar result as Lemma 4 exists. Let A = n(XTX)−1XT , which is the key quantity in Lemma 4, and Ã = n(XTX + rId) −1XT . If we can show that Ã does not differ too much from A, then the proof is completed. Consider the singular value decomposition directly on X as X = V DUT (not on Z), where V is a n× d matrix and D and U are d× d matrices. We then have\nA = n(UD2UT )−1UDV T = nUD−1V T ,\nand\nÃ = n(UD2UT + rId) −1UDV T = nUD−1 { Id + r\nn ( D√ n )−2}−1 V T .\nWhen r ≤ nλmin(XTX/n)/2, we can apply Taylor expansion on the inverse. Thus\nÃ = nUD−1 { Id + ∞∑ k=1 ( r n )k( D√ n )−2k} V T\n= A+ rUD−1 ( D√ n )−2 V T + nUD−1 { ∞∑ k=2 ( r n )k( D√ n )−2k} V T = A+ rU(D/ √ n)−3V T\nn1/2 + nUD−1 { ∞∑ k=2 ( r n )k( D√ n )−2k} V T .\nClearly, we have\nλmax\n( rU(D/ √ n)−3V T\nn1/2\n) ≤ 8\n3rκ3/2√ n ,\nand\nλmax\n[ nUD−1 { ∞∑ k=2 ( r n )k( D√ n )−2k} V T ] ≤ √ nλ−1min ( D√ n ) ∞∑ k=2 rk nk λ−kmin ( D2 n )\n≤ √ n(8κ 1 2 ) ∞∑ k=2 ( 82rκ n )k ≤ √ n(8κ 1 2 )(8 2rκ n )2 1− 82rκ n ≤ 2 · 8 5κ 5 2 r2\nn3/2 .\nThe last inequality is because we assume r ≤ nλmin(XTX/n)/2. Together, we have\n‖Ã‖∞ ≤ ‖A‖∞ + 83rκ3/2√\nn +\n2 · 85κ 52 r2\nn3/2 ,\nwith probability at least 1 − c′′ exp(−c′n) if n ≥ 64κd log p and r ≤ n 128κ . In the proof of Theorem 2, the value of t in Lemma 4 is chosen to be O(log p). Thus, as long as r ≤ O(κ−1 √ n), (10) and (11) hold for η̃(r) as well, i.e., for any δ ∈ (0, 1) we have\nP ( ‖η̃(r)‖∞ ≤ σ\nnδ/2\n) ≥ 1−O ( κ log p log d\nn1−δ\n) .\nOn the other hand, if we require r ≤ 8−2M−1/20 κ−3/2σ1/2n1−δ/4, then we have\nmax |ξ̃(r)| ≤ 8 4r2κ3M0 n2 ≤ σ nδ/2 .\nConsequently, if the tuning parameter satisfies that\nr ≤ O { min (√ n\nκ , σ\n1 2n1−δ/4\n82M 1 2\n0 κ 3 2\n)} ,\nand n ≥ 64κd log p, then we have\nP ( ‖β̂(Ridge)(r)− βM̃d‖∞ ≤ σ\nnδ/2\n) ≥ 1−O ( κ log p log d\nn1−δ\n) . (13)\nTherefore, if τ ≥ 4σ nδ/2 , then by choosing γ′(r) = 2σ nδ/2 we have\nmin i∈S |β̂(Ridge)i (r)| ≥ γ′ ≥ max i 6∈S |β̂(Ridge)i (r)|.\nProof of Corollary 1. As mentioned before, we have β̂(OLS) = βM̃d+(X T M̃d XM̃d) −1XM̃dε. Because εi ∼ N(0, σ2) for i = 1, 2, · · · , n, we have for any i ∈ M̃d,\nη̃i = e T i (X T M̃d XM̃d) −1XTM̃dε ∼ N(0, σ 2eTi (X T M̃d XM̃d) −1ei) (d) = σ √ eTi (X T M̃d XM̃d) −1eiN(0, 1).\n(14)\nLikewise in the proof of Lemma 4, we know that as long as n ≥ 64κd log p\nλmin(X T M̃d XM̃d/n) ≥\n1\n64κ .\nThus, we have\nmax i∈M̃d\neTi (X T M̃d XM̃d) −1ei ≤ 64κ/n.\nTherefore, for any t > 0 and i ∈ M̃d, with probability at least 1 − c′′ exp(−c′n) − 2 exp(−t2/2) we have\n|η̃i| ≤ σt √ eTi (X\nT M̃d XM̃d)\n−1ei ≤ 8κ 1 2σt√ n .\nThen for any δ > 0, if n > log(2c′′/δ)/c′, then with probability at least 1− δ we have\nmax i∈M̃d\n|η̃i| ≤ 8σ √ 2κ log(4d/δ)\nn . (15)\nBecause σ needs to estimated from the data, we need to obtain a bound as well. Notice that\nσ̂2 is an unbiased estimator for σ, and\nσ̂2 = σ2 T (In −XM̃d(X T M̃d XM̃d) −1XM̃d) ∼ σ2X 2(n− d) n− d ,\nwhere X 2(k) denotes a chi-square random variable with degree of freedom k. Using Proposition 5.16 in [24], we can bound σ̂2 as follows. Let K = ‖X 2(1)− 1‖ψ1 . There exists some c5 > 0 such that for any t ≥ 0 we have,\nP (∣∣∣∣X 2(n− d)n− d − 1 ∣∣∣∣ ≥ t) ≤ 2 exp{− c5 min(t2(n− d)K2 , t(n− d)K )} .\nHence for any δ > 0, if n > d+4K2 log(2/δ)/c5, then with probability at least 1−δ we have,\n|σ̂2 − σ2| ≤ σ2/2,\nwhich implies that\n1 2 σ2 ≤ σ̂2 ≤ 3 2 σ2.\nThen we know that\nmax i∈M̃d\n|η̃i| ≤ 8σ √ 2κ log(4d/δ)\nn ≤ 8 √ 2σ̂\n√ 2κ log(4d/δ)\nn ≤ 8 √ 3σ\n√ 2κ log(4d/δ)\nn .\nNow define γ′ = 8 √ 2σ̂ √\n2κ log(4d/δ) n . If the signal τ = mini∈S |βi| satisfies that\nτ ≥ 24σ √ 2κ log(4d/δ)\nn ,\nthen with probability at least 1− 2δ, for any i 6∈ S\n|β̂i| = |η̃i| ≤ 8σ √ 2κ log(4d/δ)\nn ≤ γ′,\nand for i ∈ S we have\n|β̂i| ≥ τ − max i∈M̃d\n|η̃i| ≥ 16σ √ 2κ log(4d/δ)\nn ≥ γ′.\nProof of Theorem 4\nThe result of Theorem 4 can be immediately implied from Theorem 1, 2, 3, (12) and (13)."
    } ],
    "references" : [ {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Statistical Methodology),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "Peng Zhao", "Bin Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "author" : [ "Jianqing Fan", "Runze Li" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Nearly unbiased variable selection under minimax concave penalty",
      "author" : [ "Cun-Hui Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators",
      "author" : [ "Karim Lounici" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "On iterative hard thresholding methods for high-dimensional m-estimation",
      "author" : [ "Prateek Jain", "Ambuj Tewari", "Purushottam Kar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Elementary estimators for highdimensional linear regression",
      "author" : [ "Eunho Yang", "Aurelie Lozano", "Pradeep Ravikumar" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso)",
      "author" : [ "Martin J Wainwright" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Orthogonal matching pursuit for sparse signal recovery with noise",
      "author" : [ "T Tony Cai", "Lie Wang" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "The sparsity and bias of the lasso selection in highdimensional linear regression",
      "author" : [ "Cun-Hui Zhang", "Jian Huang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "High-dimensional ordinary least-squares projection for screening",
      "author" : [ "Xiangyu Wang", "Chenlei Leng" ],
      "venue" : "variables. https://stat.duke.edu/~xw56/holp-paper.pdf,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Extended bayesian information criteria for model selection with large model spaces",
      "author" : [ "Jiahua Chen", "Zehua Chen" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Peter J Bickel", "Ya’acov Ritov", "Alexandre B Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Restricted eigenvalue properties for correlated gaussian designs",
      "author" : [ "Garvesh Raskutti", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "On the consistency theory of high dimensional variable screening",
      "author" : [ "Xiangyu Wang", "Chenlei Leng", "David B Dunson" ],
      "venue" : "arXiv preprint arXiv:1502.06895,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Regularization paths for generalized linear models via coordinate descent",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Rob Tibshirani" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Path following and empirical bayes model selection for sparse regression",
      "author" : [ "Hua Zhou", "Artin Armagan", "David B Dunson" ],
      "venue" : "arXiv preprint arXiv:1201.3528,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "A path algorithm for constrained estimation",
      "author" : [ "Hua Zhou", "Kenneth Lange" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Stability selection",
      "author" : [ "Nicolai Meinshausen", "Peter Bühlmann" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "High dimensional variable selection via tilting",
      "author" : [ "Haeran Cho", "Piotr Fryzlewicz" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Regulation of gene expression in the mammalian eye and its relevance to eye disease",
      "author" : [ "Todd E Scheetz", "Kwang-Youn A Kim", "Ruth E Swiderski", "Alisdair R Philp", "Terry A Braun", "Kevin L Knudtson", "Anne M Dorrance", "Gerald F DiBona", "Jian Huang", "Thomas L Casavant" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To deal with this problem, Tibshirani[1] proposed `1-penalized regression, a.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense.",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense.",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "These concerns have limited the practical use of regularized methods, motivating alternative strategies such as direct hard thresholding [7].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "Related works The work that is most closely related to ours is [8], in which the authors proposed an algorithm based on OLS and the ridge regression.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "[7] proposed an iterative hard thresholding algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso.",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso.",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "[6] proved that lasso also works for a second-order condition similar to ours, but requires two additional strong assumptions.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "A keen observation[12] reveals the following relationship immediately.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "Instead of directly estimating β as β̂, however, this new estimator of β may be used for dimension reduction by observing β̂ = X (XX )−1Xβ + X (XX )−1ε = Φβ + η [12].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "Alternatively use eBIC in [13] in conjunction with the obtained variable importance to select the best submodel.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 14,
      "context" : "This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : ", var(ε) = σ < ∞, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : ", var(ε) = σ < ∞, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : ", var(ε) = σ < ∞, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : ", var(ε) = σ < ∞, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : ", var(ε) = σ < ∞, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "This relaxation is similar to [6]; however we do not require any further assumptions needed by [6].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "This relaxation is similar to [6]; however we do not require any further assumptions needed by [6].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "However, the corresponding details will not be pursued here, as their consistency is straightforwardly implied by the results from this section and the existing literature on extended BIC and BIC [13].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 15,
      "context" : "In particular, it is shown that the diagonal terms of Φ are O( p ) while the off-diagonal terms are O( √ n p ) [16].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Alternatively, we can construct a series of nested models formed by ranking the largest n coefficients and adopt the extended BIC [13] to select the best submodel.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "This example is Example 4 in [5], for which we allocate the 15 true variables into three groups.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "This model is also considered in [20] and [21].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "This model is also considered in [20] and [21].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "We use the extended BIC [13] to choose the parameters for any regularized algorithm.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "This dataset, taken from [22], was collected to study mammalian eye diseases, with gene expression for the eye tissues of 120 twelve-week-old male F2 rats recorded.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "Following the method in [22], 18976 probes were selected as they exhibited sufficient signal for reliable analysis and at least 2-fold variation in expressions.",
      "startOffset" : 24,
      "endOffset" : 28
    } ],
    "year" : 2015,
    "abstractText" : "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}