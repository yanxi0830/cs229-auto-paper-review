{
  "name" : "1605.02536.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Random Fourier Features for Operator-Valued Kernels",
    "authors" : [ "Romain Brault", "Florence d’Alché-Buc", "Markus Heinonen" ],
    "emails" : [ "ro.brault@telecom-paristech.fr", "florence.dalche@telecom-paristech.fr", "markus.o.heinonen@aalto.fi" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operatorvalued kernels. We propose a general principle for Operator-valued Random Fourier\n∗ro.brault@telecom-paristech.fr †florence.dalche@telecom-paristech.fr ‡markus.o.heinonen@aalto.fi\nar X\niv :1\nFeature construction relying on a generalization of Bochner’s theorem for translationinvariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al., 2011), vector field learning (Baldassarre et al., 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure. A pdimensional vector-valued model can account for couplings between the outputs for improved performance in comparison to p independent scalar-valued models. In this paper we are interested in a general and flexible approach to efficiently implement and learn vector-valued functions, while allowing couplings between the outputs. To achieve this goal, we turn to shallow architectures, namely the product of a (nonlinear) feature matrix Φ̃(x) and a parameter vector θ such that f̃(x) = Φ̃(x)∗θ, and combine two appealing methodologies: Operator-Valued Kernel Regression and Random Fourier Features.\nOperator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; Álvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions. As in the scalar case, operator-valued kernels (OVKs) are used to build Reproducing Kernel Hilbert Spaces (RKHS) in which representer theorems apply as for ridge regression or other appropriate loss functional. In these cases, learning a model in the RKHS boils down to learning a function of the form f(x) = ∑n i=1 K(x, xi)αi where x1, . . . , xn are the training input data and each αi, i = 1, . . . , n is a vector of the output space Y and each K(x, xi), an operator on vectors of Y . However, OVKs suffer from the same drawback as classic kernel machines: they scale poorly to very large datasets because they are very demanding in terms of memory and computation. Therefore, focusing on the case Y = Rp, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines. The RFF approach linearizes a shift-invariant kernel model by generating explicitly an approximated feature map φ̃. RFFs has been shown to be efficient on large datasets and further improved by efficient matrix computations of FastFood (Le et al., 2013), and is considered as one of the best large scale implementations of kernel methods, along with Nÿstrom approaches (Yang et al., 2012).\nIn this paper, we propose general Random Fourier Features for functions in vector-\nvalued RKHS. Here are our contributions: (1) we define a general form of Operator Random Fourier Feature (ORFF) map for shift-invariant operator-valued kernels, (2) we construct explicit operator feature maps for a simple bounded kernel, the decomposable kernel, and more complex unbounded kernels curl-free and divergence-free kernels, (3) the corresponding kernel approximation is shown to uniformly converge towards the target kernel using appropriate Bernstein matrix concentration inequality, for both bounded and unbounded operator-valued kernels and (4) we illustrate the theoretical approach by a few numerical results.\nThe paper is organized as follows. In section 1.2, we recall Random Fourier Feature and Operator-valued kernels. In section 2, we use extension of Bochner’s theorem to propose a general principle of Operator Random Fourier Features and provide examples for decomposable, curl-free and divergence-free kernels. In section 3, we present a theorem of uniform convergence for bounded and unbounded ORFFs (proof is given in appendix B) and the conditions of its application. Section 4 shows an numerical illustration on learning linear ORFF-models. Section 5 concludes the paper. The main proofs of the paper are presented in Appendix."
    }, {
      "heading" : "1.1 Notations",
      "text" : "The euclidean inner product in Rd is denoted 〈·, ·〉 and the euclidean norm is denoted ‖·‖. The unit pure imaginary number √ −1 is denoted i. B(Rd) is the Borel σ-algebra on Rd. For a function f : Rd → R, if dx is the Lebesgue measure on Rd, we denote F [f ] its Fourier transform defined by:\n∀x ∈ Rd,F [f ] (x) = ∫ Rd e−i〈ω,x〉f(ω)dω.\nThe inverse Fourier transform of a function g is defined as F−1 [g] (ω) = ∫ Rd ei〈x,ω〉f(ω)dx.\nIt is common to define the Fourier transform of a (positive) measure µ by F [µ] (x) = ∫ Rd e−i〈ω,x〉dµ(ω).\nIf X and Y are two vector spaces, we denote by F(X ;Y) the vector space of functions f : X → Y and C(X ;Y) ⊂ F(X ;Y) the subspace of continuous functions. If H is an Hilbert space we denote its scalar product by 〈., .〉H and its norm by ‖.‖H. We set L(H) = L(H;H) to be the space of linear operators from H to itself. If W ∈ L(H), Ker W denotes the nullspace, Im W the image and W ∗ ∈ L(H) the adjoint operator (transpose in the real case)."
    }, {
      "heading" : "1.2 Background",
      "text" : "Random Fourier Features: we first consider scalar-valued functions. Denote k : Rd × Rd → R a positive definite kernel on Rd. A kernel k is said to be shift-invariant for the addition if for any a ∈ Rd, ∀(x, x′) ∈ Rd×Rd, k(x− a, z− a) = k(x, z). Then, we define k0 : Rd → R the function such that k(x, z) = k0(x − z). k0 is called the signature of kernel k. Bochner theorem is the theoretical result that leads to the Random Fourier Features.\nTheorem 1.1 (Bochner’s theorem1). Every positive definite complex valued function is the Fourier transform of a non-negative measure. This implies that any positive definite, continuous and shift-invariant kernel k is the Fourier transform of a non-negative measure µ:\nk(x, z) = k0(x− z) = ∫ Rd e−i〈ω,x−z〉dµ(ω). (1)\nWithout loss of generality for the Random Fourier methodology, we assume that µ is a probability measure, i.e. ∫ Rd dµ(ω) = 1. Then we can write eq. (1) as an expectation\nover µ: k0(x − z) = Eµ [ e−i〈ω,x−z〉 ] . Both k and µ are real-valued, and the imaginary part is null if and only if µ(ω) = µ(−ω). We thus only write the real part:\nk(x, z) = Eµ[cos〈ω, x− z〉] = Eµ [cos〈ω, z〉 cos〈ω, x〉+ sin〈ω, z〉 sin〈ω, x〉] .\nLet ⊕D\nj=1 xj denote the Dm-length column vector obtained by stacking vectors xj ∈ Rm. The feature map φ̃ : Rd → R2D defined as\nφ̃(x) = 1√ D D⊕ j=1 ( cos 〈x, ωj〉 sin 〈x, ωj〉 ) , ωj ∼ µ (2)\nis called a Random Fourier Feature map. Each ωj, j = 1, . . . , D is independently sampled from the inverse Fourier transform µ of k0. This Random Fourier Feature map provides the following Monte-Carlo estimator of the kernel:\nK̃(x, z) = φ̃(x)∗φ̃(z), (3)\nthat is proven to uniformly converge towards the true kernel described in eq. (1). The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach\n1See Rudin (1994).\nonly requires two steps before learning: (1) define the inverse Fourier transform of the given shift-invariant kernel, (2) compute the randomized feature map using the spectral distribution µ. For the Gaussian kernel k(x − z) = exp(−γ‖x− z‖2), the spectral distribution µ(ω) is Gaussian Rahimi and Recht (2007).\nOperator-valued kernels: we now turn to vector-valued functions and consider vectorvalued Reproducing Kernel Hilbert spaces (vv-RKHS) theory. The definitions are given for input space X ⊂ Cd and output space Y ⊂ Cp. We will define operator-valued kernel as reproducing kernels following the presentation of Carmeli et al. (2010). Given X and Y , a map K : X × X → L(Y) is called a Y-reproducing kernel if\nN∑ i,j=1 〈K(xi, xj)yj, yi〉 ≥ 0,\nfor all x1, . . . , xN in X , all y1, . . . , yN in Y and N ≥ 1. Given x ∈ X , Kx : Y → F(X ;Y) denotes the linear operator whose action on a vector y is the function Kxy ∈ F(X ;Y) defined by (Kxy)(z) = K(z, x)y, ∀z ∈ X .\nAdditionally, given a Y-reproducing kernelK, there is a unique Hilbert spaceHK ⊂ F(X ;Y) satisfying Kx ∈ L(Y ;HK), ∀x ∈ X and f(x) = K∗xf, ∀x ∈ X ,∀f ∈ HK , where K∗x : HK → Y is the adjoint of Kx. The space HK is called the (vector-valued) Reproducing Kernel Hilbert Space associated with K. The corresponding product and norm are denoted by 〈., .〉K and ‖.‖K , respectively. As a consequence (Carmeli et al., 2010) we have:\nK(x, z) = K∗xKz ∀x, z ∈ X HK = span {Kxy | ∀x ∈ X , ∀y ∈ Y}\nAnother way to describe functions ofHK consists in using a suitable feature map.\nProposition 1.1 (Carmeli et al. (2010)). Let H be a Hilbert space and Φ : X → L(Y ;H), with Φx , Φ(x). Then the operatorW : H → F(X ;Y) defined by (Wg)(x) = Φ∗xg, ∀g ∈ H,∀x ∈ X is a partial isometry fromH onto the reproducing kernel Hilbert spaceHK with reproducing kernel\nK(x, z) = Φ∗xΦz, ∀x, z ∈ X ."
    }, {
      "heading" : "W ∗W is the orthogonal projection onto",
      "text" : "Ker W⊥ = span {Φxy | ∀x ∈ X , ∀y ∈ Y} .\nThen ‖f‖K = inf {‖g‖H | ∀g ∈ H, Wg = f}.\nWe call Φ a feature map, W a feature operator andH a feature space.\nIn this paper, we are interested on finding feature maps of this form for shift-invariant Rp-Mercer kernels using the following definitions. A reproducing kernel K on Rd is a Rp-Mercer provided that HK is a subspace of C(Rd;Rp). It is said to be a shiftinvariant kernel or a translation-invariant kernel for the addition if K(x + a, z + a) = K(x, z), ∀(x, z, a) ∈ X 3. It is characterized by a function K0 : X → L(Y) of completely positive type such that K(x, z) = K0(δ), with δ = x− z."
    }, {
      "heading" : "2 Operator-valued Random Fourier Features",
      "text" : ""
    }, {
      "heading" : "2.1 Spectral representation of shift-invariant vector-valued Mercer kernels",
      "text" : "The goal of this work is to build approximated matrix-valued feature map for shiftinvariant Rp-Mercer kernels, denoted with K, such that any function f ∈ HK can be approximated by a function f̃ defined by:\nf̃(x) = Φ̃(x)∗θ\nwhere Φ̃(x) is a matrix of size (m × p) and θ is an m-dimensional vector. We propose a randomized approximation of such a feature map using a generalization of the Bochner theorem for operator-valued functions. For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al. (2012) for our case of interest of X = Rd and Y = Rp. The following proposition extends Bochner’s theorem to any shift-invariant Rp-Mercer kernel.\nProposition 2.1 (Operator-valued Bochner’s theorem2). A continuous function K from Rd × Rd to L(Rp) is a shift-invariant reproducing kernel if and only if ∀x, z ∈ Rd, it is the Fourier transform of a positive operator-valued measureM : B(Rd)→ L+(Rp):\nK(x, z) = ∫ Rd e−i〈x−z,ω〉dM(ω),\nwhereM belongs to the set of all the L+(Rp)-valued measures of bounded variation on the σ-algebra of Borel subsets of Rd.\n2Equation (36) in Zhang et al. (2012).\nHowever it is much more convenient to use a more explicit result that involves realvalued (positive) measures. The following proposition instantiates Prop. 13 in Carmeli et al. (2010) to matrix-valued operators.\nProposition 2.2 (Carmeli et al. (2010)). Let µ be a positive measure on Rd and A : Rd → L(Rp) such that 〈A(.)y, y′〉 ∈ L1(Rd, dµ) for all y, y′ ∈ Rp and A(ω) ≥ 0 for µ-almost all ω. Then, for all δ ∈ Rd, ∀`,m ∈ {1, . . . , p},\nK0(δ)`m = ∫ Rd e−i〈δ,ω〉A(ω)`mdµ(ω) (4)\nis the kernel signature of a shift-invariant Rp-Mercer kernel K such that K(x, z) = K0(x − z). In other terms, each real-valued function K0(·)`m is the Fourier transform of A(·)`mpµ(·) where pµ(ω) = dµdω is the Radon-Nikodym derivative of the measure µ, which is also called the density of the measure µ. Any shift-invariant kernel is of the above form for some pair (A(ω), µ(ω)).\nThis theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, µ is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2.\nProposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(ω) such that A(ω) = B(ω)B(ω)∗. Then the function Φ : Rp → L2(Rd, µ;Rp) defined for all x ∈ Rp by\n∀y ∈ Rp, (Φxy) (ω) = ei〈x,ω〉B(ω)∗y (5)\nis a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in Rd, Φ∗xΦz = K(x, z).\nThus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(ω), µ(ω) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to Rp-Mercer kernel gives the solution.\nProposition 2.4 (Carmeli et al. (2010)). Let K be a shift-invariant Rp-Mercer kernel. Suppose that ∀z ∈ Rd,∀y, y′ ∈ Rp, 〈K0(.)y, y′〉 ∈ L1(Rd, dx) where dx denotes the Lebesgue measure. Define C such that ∀ω ∈ Rd,∀`,m ∈ {1, . . . , p},\nC(ω)`m = ∫ Rd ei〈δ,ω〉K0(δ)`mdδ. (6)\nThen\ni) C(ω) is an non-negative matrix for all ω ∈ Rd,\nii) 〈C(.)y, y′〉 ∈ L1(Rd, dω) for all y, y′ ∈ Rp,\niii) for all δ ∈ Rd,∀`,m ∈ {1, . . . , p},\nK0(δ)`m = ∫ Rd e−i〈δ,ω〉C(ω)`mdω.\nFrom eq. (4) and eq. (6), we can write the following equality concerning the matrixvalued kernel signature K0, coefficient by coefficient: ∀δ ∈ Rd,∀i, j ∈ {1, . . . , p},∫\nRd e−i〈δ,ω〉C(ω)ijdω = ∫ Rd e−i〈δ,ω〉A(ω)ijdµ(ω).\nWe then conclude that the following equality holds almost everywhere for ω ∈ Rd: C(ω)ij = A(ω)ijpµ(ω) where pµ(ω) = dµdω . Without loss of generality we assume that∫ Rd dµ(ω) = 1 and thus, µ is a probability distribution. Note that this is always possible through an appropriate normalization of the kernel. Then pµ is the density of µ. The proposition 2.2 thus results in an expectation:\nK0(x− z) = Eµ[e−i〈x−z,ω〉A(ω)] (7)"
    }, {
      "heading" : "2.2 Construction of Operator Random Fourier Feature",
      "text" : "Given a Rp-Mercer shift-invariant kernel K on Rd, we build an Operator-Valued Random Fourier Feature (ORFF) map in three steps:\n1) compute C : Rd → L(Rp) from eq. (6) by using the inverse Fourier transform (in the sense of proposition 2.4) of K0, the signature of K;\n2) find A(ω), pµ(ω) and compute B(ω) such that A(ω) = B(ω)B(ω)∗;\n3) build an randomized feature map via Monte-Carlo sampling from the probability measure µ and the application B."
    }, {
      "heading" : "2.3 Monte-Carlo approximation",
      "text" : "Let ⊕D\nj=1 Xj denote the block matrix of size rD × s obtained by stacking D matrices X1, . . . , XD of size r × s. Assuming steps 1 and 2 have been performed, for all j = 1, . . . , n, we find a decomposition A(ωj) = B(ωj)B(ωj)∗ either by exhibiting a general analytical closed-form or using a numerical decomposition. Denote p×p′ the dimension\nof the matrix B(ω). We then propose a randomized matrix-valued feature map: ∀x ∈ Rd,\nΦ̃(x) = 1√ D D⊕ j=1 Φx(ωj), ωj ∼ µ\n= 1√ D D⊕ j=1 e−i〈x,ωj〉B(ωj) ∗, ωj ∼ µ.\n(8)\nThe corresponding approximation for the kernel is then: ∀x, z ∈ Rd\nK̃(x, z) = Φ̃(x)∗Φ̃(z)\n= 1\nD ∑D j=1 e−i〈x,ωj〉B(ωj)e i〈z,ωj〉B(ωj) ∗\n= 1\nD ∑D j=1 e−i〈x−z,ωj〉A(ωj).\nThe Monte-Carlo estimator Φ̃(x)∗Φ̃(z) converges in probability to K(x, z) when D tends to infinity. Namely,\nK̃(x, z) = Φ̃(x)∗Φ̃(z) p.−−−→ D→∞ Eµ [ e−i〈x−z,ω〉A(ω) ] = K(x, z)\nAs for the scalar-valued kernel, a real-valued matrix-valued function has a real matrixvalued Fourier transform if A(ω) is even with respect to ω. Taking this point into account, we define the feature map of a real matrix-valued kernel as\nΦ̃(x) = 1√ D D⊕ j=1 ( cos 〈x, ωj〉B(ωj)∗ sin 〈x, ωj〉B(ωj)∗ ) , ωj ∼ µ.\nThe kernel approximation becomes\nΦ̃(x)∗Φ̃(z) = 1\nD D∑ j=1 cos 〈x,ωj〉 cos 〈z,ωj〉A(ωj) sin 〈x,ωj〉 sin 〈z,ωj〉A(ωj)+\n= 1\nD D∑ j=1 cos 〈x− z, ωj〉A(ωj).\nIn the following, we give an explicit construction of ORFFs for three well-known RpMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al. (2012) for vector field learning. All these kernels are defined using a scalar-valued shift-invariant Mercer kernel k :\nRd×Rd → R whose signature is denoted k0. A usual choice is to choose k as a Gaussian kernel with k0(δ) = exp ( −‖δ‖ 2\n2σ2\n) , which gives µ = N (0, σ−2I) (Huang et al., 2013) as\nits inverse Fourier transform.\nDefinition 2.1 (Decomposable kernel). Let A be a (p× p) positive semi-definite matrix. K defined as ∀(x, z) ∈ Rd × Rd, K(x, z) = k(x, z)A is a Rp-Mercer shift-invariant reproducing kernel.\nMatrix A encodes the relationships between the outputs coordinates. If a graph coding for the proximity between tasks is known, then it is shown in Evgeniou et al. (2005); Baldassarre et al. (2010) that A can be chosen equal to the pseudo inverse L† of the graph Laplacian, and then the `2 norm in HK is a graph-regularizing penalty for the outputs (tasks). When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015). Another interesting property of the decomposable kernel is its universality. A reproducing kernel K is said universal if the associated RKHSHK is dense in the space C(X ,Y).\nExample 2.1 (ORFF for decomposable kernel).\nCdec(ω)`m = ∫ X ei〈δ,ω〉k0(δ)A`mdδ = A`mF−1 [k0] (ω)\nHence, Adec(ω) = A and pdecµ (ω) = F−1 [k0] (ω).\nORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p). Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlström et al., 2013). These kernels discussed in Fuselier (2006) allow encoding input-dependent similarities between vector-fields.\nDefinition 2.2 (Curl-free and Div-free kernel). We have d = p. The divergence-free kernel is defined as\nKdiv(x, z) = Kdiv0 (δ) = (∇∇∗ −∆I)k0(δ)\nand the curl-free kernel as\nKcurl(x, z) = Kcurl0 (δ) = −∇∇∗k0(δ),\nwhere ∇∇∗ is the Hessian operator and ∆ is the Laplacian operator.\nAlthough taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012). For the divergence-free and curl-free kernel we use the differentiation properties of the Fourier transform.\nExample 2.2 (ORFF for curl-free kernel:). ∀`,m ∈ {1, . . . , p},\nCcurl(ω)`m = −F−1 [ ∂\n∂δ`\n∂\n∂δm k0\n] (ω)\n= ω`ωmF−1 [k0] (ω)\nHence,Acurl(ω) = ωω∗ and pcurlµ (ω) = F−1 [k0] (ω). We can obtain directly: Bcurl(ω) = ω.\nFor the divergence-free kernel we first compute the Fourier transform of the Laplacian of a scalar kernel using differentiation and linearity properties of the Fourier transform. We denote δ{`=m} as the Kronecker delta which is 1 if ` = m and zero otherwise.\nExample 2.3 (ORFF for divergence-free kernel:).\nCdiv(ω)`m = F−1 [ ∂\n∂δ`\n∂\n∂δm k0 − δ{`=m}∆k0 ] = F−1 [ ∂\n∂δ`\n∂\n∂δm k0\n] − δ{`=m}F−1 [∆k0]\n= (δ{`=m} − ω`ωm)‖ω‖22F −1 [k0] ,\nsince\nF−1 [∆k0(δ)] = p∑\nk=1\nF−1 [ ∂\n∂δk k0\n] = −‖ω‖22F −1 [k0] .\nHence Adiv(ω) = I‖ω‖22 − ωω∗ and pdivµ (ω) = F−1 [k0] (ω). Here, Bdiv(ω) has to be obtained by a numerical decomposition such as Cholesky or SVD."
    }, {
      "heading" : "3 Uniform error bound on ORFF approximation",
      "text" : "We are now interested on measuring how close the approximation K̃(x, z) = Φ̃(x)∗Φ̃(z) is close to the target kernel K(x, z) for any x, z in a compact set C. If A is a real matrix, we denote ‖A‖2 its spectral norm, defined as the square root of the largest eigenvalue of\nA. For x and z in some compact C ⊂ Rd, we consider: F (x− z) = K̃(x, z)−K(x, z) and study how the uniform norm\n‖F‖∞ = sup x,z∈C ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2\n(9)\nbehaves according to D. Figure 1 empirically shows convergence of three different OVK approximations for x, z from the compact [−1, 1]4 using an increasing number of sample points D. The log-log plot shows that all three kernels have the same convergence rate, up to a multiplicative factor.\nIn order to bound the error with high probability, we turn to concentration inequalities devoted to random matrices (Boucheron et al.). In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, ∥∥∥K̃(x, z)−K(x, z)∥∥∥\n2 = ‖A‖2 ∥∥∥k̃(x, z)− k(x, z)∥∥∥ This theorem and its proof are presented in corollary A.1.1.\nMore interestingly, we propose a new bound for Operator Random Fourier Feature approximation in the general case. It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al., 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels). For the latter, we notice that their norms behave as subexponential random variables (Koltchinskii et al., 2013). Before introducing the new theorem, we give the definition of the Orlicz norm and subexponential random variables.\nDefinition 3.1 (Orlicz norm). We follow the definition given by Koltchinskii et al. (2013). Let ψ : R+ → R+ be a non-decreasing convex function with ψ(0) = 0. For a random variable X on a measured space (Ω, T (Ω), µ),\n‖X‖ψ , inf {C > 0 | E[ψ (|X|/C)] ≤ 1} .\nHere, the function ψ is chosen as ψ(u) = ψα(u) where ψα(u) , eu α − 1. When α = 1, a random variable with finite Orlicz norm is called a subexponential variable because its tails decrease at least exponentially fast.\nTheorem 3.1. Let C be a compact subset of Rd of diameter l. Let K be a shift-invariant Rp-Mercer kernel on Rd, K0 its signature and pµ(·)A(·) the inverse Fourier transform of the kernel’s signature (in the sense of proposition 2.4) where pµ is the density of a probability measure µ considering appropriate normalization. Let D be a positive integer and ω1, . . . , ωD, i.i.d. random vectors drawn according to the probability law µ. For x, z ∈ C, we recall\nK̃(x, z) = D∑ j=1 cos〈x− z, ωj〉A(ωj).\nWe note for all j ∈ {1, . . . , D},\nFj(x− z) = 1\nD ( D∑ j=1 cos〈x− z, ωj〉A(ωj)−K(x, z) )\nand F (x− z) = K̃(x, z)−K(x, z). ‖F‖∞ denotes the infinite norm of F (x− z) on the compact C as introduced in eq. (9). If one can define the following terms (bD,m, σ2p) ∈ R3+:\nbD = sup x,z∈C D ∥∥∥∥∥Eµ [ D∑ j=1 (Fj(x− z))2 ]∥∥∥∥∥\n2\n,\nm = 4 ( ‖‖A(ω)‖2‖ψ1 + sup\nx,z∈C ‖K(x, z)‖\n) , ω ∼ µ,\nσ2p = Eµ [ ‖ω‖22‖A(ω)‖ 2 2 ] .\nThen for all in R+,\nP {‖F‖∞ ≥ } ≤ Cd ( σpl ) 2 1+2/d exp ( − 2D 8(d+2)(bD+ ūD6 ) ) if ūD ≤ 2(e−1)bD exp ( − D\n(d+2)(e−1)ūD\n) otherwise,\nwhere ūD = 2m log ( 2 3 2 ( m bD )2) and Cd = p (( d 2 ) −d d+2 + ( d 2 ) 2 d+2 ) 2 6d+2 d+2 .\nWe detail the proof of the theorem in appendix B. It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.1)."
    }, {
      "heading" : "3.1 Application to some operator-valued kernel",
      "text" : "To apply theorem 3.1 to operator-valued kernels, we need to ensure that all the constants exist. In the following, we first show how to bound the constant term bD. Then we exhibit the upper bounds for the three operator-valued kernels we took as examples. Eventually, we ensure that the random variable ‖A(ω)‖ has a finite Orlicz norm with ψ = ψ1 in these three cases.\nBounding the term bD(δ): Proposition 3.1. Define the matrix Vµ[A(ω)] as follows: for all `,m ∈ {1, . . . , p},\nVµ[A(ω)]`m = p∑ r=1 Covµ[A(ω)`r, A(ω)rm]\nFor a given δ = x− z, define:\nbD(δ) = D ∥∥∥∥∥Eµ [ D∑ j=1 (Fj(δ)) 2 ]∥∥∥∥∥ 2 .\nThen we have:\nbD(δ) ≤ 1\n2 ∥∥(K0(2δ) +K0(0))Eµ[A(ω)]− 2K0(δ)2∥∥2 + ‖Vµ[A(ω)]‖2. The proof uses trigonometry properties and various properties of the moments and is given in appendix C. Now, we compute the upper bound given by proposition 3.1 for the three kernels we have taken as examples.\ni) Decomposable kernel: notice that in the case of the Gaussian decomposable kernel, i.e. A(ω) = A, K0(δ) = Ak0(δ), k0(δ) ≥ 0 and k0(δ) = 1, then we have:\nbD(δ) ≤ 1\n2 (1 + k0(2δ))‖A‖2 + k0(δ) 2\nii) curl-free and div-free kernels: recall that in this case p = d. For the (Gaussian) curl-free kernel, A(ω) = ωω∗ where ω ∈ Rd ∼ N (0, σ−2Id) thus Eµ[A(ω)] = Id/σ 2 and Vµ[A(ω)] = (d+ 1)Id/σ4 (see Petersen et al. (2008)). Hence,\nbD(δ) ≤ 1\n2 ∥∥∥∥ 1σ2K0(2δ)− 2K0(δ)2 ∥∥∥∥\n2\n+ (d+ 1)\nσ4\nEventually for the Gaussian divergence-free kernel, A(ω) = I‖ω‖22 − ωω∗, thus Eµ[A(ω)] = Id(d − 1)/σ2 and Vµ[A(ω)] = d(4d − 3)Id/σ4 (see Petersen et al. (2008)). Hence,\nbD(δ) ≤ 1\n2 ∥∥∥∥(d− 1)σ2 K0(2δ)− 2K0(δ)2 ∥∥∥∥\n2\n+ d(4d− 3)\nσ4\nAn empirical illustration of these bounds is shown in fig. 6.\nComputing the Orlicz norm: For a random variable with strictly monotonic moment generating function (MGF), one can characterize its ψ1 Orlicz norm by taking the functional inverse of the MGF evaluated at 2. In other words\n‖X‖−1ψ1 = MGF(x) −1 X (2).\nFor the Gaussian curl-free and divergence-free kernel∥∥Adiv(ω)∥∥ 2 = ∥∥Acurl(ω)∥∥ 2 = ‖ω‖22\nwhere ω ∼ N (0, Id/σ2), hence ‖A(ω)‖2 ∼ Γ(p/2, 2/σ2). The MGF of this gamma distribution is MGF(x)−1(t) = (1− 2t/σ2)−(p/2). Eventually\n∥∥∥∥Adiv(ω)∥∥ 2 ∥∥−1 ψ1 = ∥∥∥∥Acurl(ω)∥∥ 2 ∥∥−1 ψ1 = σ2\n2\n( 1− 4− 1 p ) ."
    }, {
      "heading" : "4 Learning with ORFF",
      "text" : "In practise, the previous bounds are however too large to find a safe value for D. In the following, numerical examples of ORFF-approximations are presented."
    }, {
      "heading" : "4.1 Penalized regression with ORFF",
      "text" : "Once we have an approximated feature map, we can use it to provide a feature matrix of size p′D× p with matrix B(ω) of size p× p′ such that A(ω) = B(ω)B(ω)∗. A function f ∈ HK is then approximated by a linear model\nf̃(x) = Φ̃(x)∗θ, where θ ∈ Rp′D.\nLet S = {(xi, yi) ∈ Rd × Rp, i = 1, . . . , N} be a collection of i.i.d training samples. Given a local loss function L : S → R+ and a `2 penalty, we minimize\nL(θ) = 1 N N∑ i=1 L ( Φ̃(xi) ∗θ, yi ) + λ‖θ‖22, (10)\ninstead of minimizing L(f) = 1 N ∑N i=1 L(f(xi), yi) + λ‖f‖ 2 HK . To find a minimizer of the optimization problem eq. (10) many optimization algorithms are available. For instance, in large-scale context, a stochastic gradient descent algorithm would be be suitable: we can adapt the algorithm to the kind of kernel/problematic. We investigate two optimization algorithms: a Stein equation solver appropriate for the decomposable kernel and a (stochastic) gradient descent for non-decomposable kernels (e.g. the curlfree and divergence-free kernels).\nClosed form for the decomposable kernel: for the real decomposable kernelK0(δ) = k(δ)A when L(y, y′) = ‖y − y′‖22 (Kernel Ridge regression in HK), the learning problem described in eq. (10) can be re-written in terms of matrices to find the unique minimizer Θ∗, where vec(Θ) = θ such that θ is a p′D vector and Θ a p′ ×D matrix. If φ̃ is a feature map (φ̃(X) is a matrix of size D ×N ) for the scalar kernel k0, then\nΦ̃(x)∗θ = (φ̃(x)∗ ⊗B)θ = BΘφ̃(x)\nand θ∗ = arg min\nΘ∈Rp′×D\n∥∥∥BΘφ̃(X)− Y ∥∥∥2 F + λ‖Θ‖2F . (11)\nThis is a convex optimization problem and a sufficient condition is:\nφ̃(X)φ̃(X)∗Θ∗B ∗B − φ̃(X)Y ∗B + λΘ∗ = 0,\nwhich is a Stein equation.\nGradient computation for the general case. When it is not possible or desirable to use Stein’s equations solver one can apply a (stochastic) gradient descent algorithm. The gradient computation for and `2-loss applied to ORFF model is briefly recalled in appendix D.1."
    }, {
      "heading" : "4.2 Numerical illustration",
      "text" : "We present a few experiments to complete the theoretical contribution and illustrate the behavior of ORFF-regression. Other experimentalresults with noisy output data are shown in appendix D.2.\nDatasets: the first dataset is the handwritten digits recognition dataset MNIST3 We select a training set of 12000 images and a test set of 10000 images. The inputs are images represented as a vector xi ∈ [0, 255]784 and the targets are integers between 0 and 9. First we scaled the inputs such that they take values in [−1, 1]784. Then we binarize the targets such that each number is represented by a unique binary vector of length 10. To predict classes, we use simplex coding method presented in Mroueh et al. (2012). The intuition behind simplex coding is to project the binarized labels of dimension p onto the most separated vectors on the hypersphere of dimension p − 1. For ORFF we can encode directly this projection in the B matrix of the decomposable kernel K0(δ) = BB∗k0(δ) where k0 is a Gaussian kernel. For OVK we project the binarized targets on the simplex as a preprocessing step, before learning with the kernel K0(δ) = Ipk0(δ), where k0 is a also Gaussian kernel.\n3available at http://yann.lecun.com/exdb/mnist.\nThe second dataset corresponds to a 2D-vector field with structure. We generated a scalar field as a mixture of five Gaussians located at [0, 0], [0, 1], [0,−1], with positive values and at [−1, 0], [1, 0] with negative values. The curl-free field has been generated by taking the gradient of the scalar-field, and the divergence-free field by taking the orthogonal of the curl-free field. These 2D-datasets are depicted in fig. 7.\nApproximation: We trained both an ORFF and an OVK model on the handwritten digits recognition dataset (MNIST) with a decomposable Gaussian kernel with signature K0(δ) = exp(−‖δ‖/σ2)A. To find a solution of the optimization problem described in eq. (11), we use off-the-shelf solver4 able to handle Stein’s equation. For both methods we choose σ = 20 and use a 2-fold cross validation on the training set to select the optimal λ. First, fig. 2 shows the running time comparison between OVK and ORFF models using D = 1000 Fourier features against the number of datapoints N . The loglog plot shows ORFF scaling better than the OVK w.r.t the number of points. Second, fig. 3 shows the test prediction error versus the number of ORFFs D, when using N = 1000 training points. As expected, the ORFF model converges toward the OVK model when the number of features increases.\nIndependent (RFF) prediction vs Structured prediction on vector fields: we perform a similar experiment over a simulated dataset designed for learning a 2D-vector field with structure. Figure 4 reports the Mean Squared Error versus the number of ORFF D. For this experiment we use a Gaussian curl-free kernel and tune its σ hyperparameter as well as the λ on a grid. The curl-free ORFF outperforms classic RFFs by tending more quickly towards the noise level. Figure 5 shows the computation time between curl-ORFF and curl-OVK indicating that the OVK solution does not scale to large datasets, while ORFF scales well with when the number of data increases. When N > 104 exact OVK is not able to be trained in reasonable time (> 1000 seconds)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduced a general and versatile framework for operator-valued kernel approximation with Operator Random Fourier Features. We showed the uniform convergence of these approximations by proving a matrix concentration inequality for bounded and unbounded ORFFs. The complexity in time of these approximations together with the linear learning algorithm make this implementation scalable with the number of data and therefore interesting compared to OVK regression. The numerical illustration shows the behavior expected from theory. ORFFs are especially a very promising approach in vector field learning or on noisy datasets. Another appealing direction is to use this archi-\n4 http://ta.twi.tudelft.nl/nw/users/gijzen/IDR.html\ntecture to automatically learn operator-valued kernels by learning a mixture of ORFFs in order to choose appropriate kernels, a working direction closely related to the recent method called “Alacarte” (Yang et al., 2015) based on the very efficient “FastFood” method (Le et al., 2013) for scalar kernels. Finally this work opens the door to building deeper architectures by stacking vector-valued functions while keeping a kernel view for large datasets."
    }, {
      "heading" : "A Reminder on Random Fourier Feature in the scalar case",
      "text" : "Rahimi and Recht (2007) proved the uniform convergence of Random Fourier Feature (RFF) approximation for a scalar shift invariant kernel.\nTheorem A.1 (Uniform error bound for RFF, Rahimi and Recht (2007)). Let C be a compact of subset of Rd of diameter l. Let k a shift invariant kernel, differentiable with a bounded first derivative and µ its normalized inverse Fourier transform. Let D the dimension of the Fourier feature vectors. Then, for the mapping φ̃ described in section 2, we have :\nP {\nsup x,z∈C ∥∥∥k̃(x, z)− k(x, z)∥∥∥ 2 ≥ } ≤ 28 ( dσl )2 exp ( − 2D 4(d+ 2) ) (12)\nFrom theorem A.1, we can deduce the following corollary about the uniform convergence of the ORFF approximation of the decomposable kernel.\nCorollary A.1.1 (Uniform error bound for decomposable ORFF). Let C be a compact of subset of Rd of diameter l. Kdec is a decomposable kernel built from a p×p semi-definite matrixA and k, a shift invariant and differentiable kernel whose first derivative is bounded. Let k̃ the Random Fourier approximation for the scalar-valued kernel k. We recall that: for a given pair (x, z) ∈ C, K̃(x, z) = Φ̃(x)∗Φ̃(z) = Ak̃(x, z) and K0(x− z) = AEµ[k̃(x, z)].\nP {\nsup x,z∈C ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2 ≥ } ≤ 28 ( dσ‖A‖2l )2 exp ( −\n2D\n4‖A‖22(d+ 2) ) Proof. The proof directly extends A.1 given by Rahimi and Recht (2007). Since\nsup x,z∈C ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2 = sup x,z∈C ‖A‖2. ∣∣∣K̃(x, z)− k(x, z)∣∣∣\nand then, taking ′ = ‖A‖2 gives the following result for all positive ′:\nP {\nsup x,z∈C ∥∥∥A(K̃(x, z)− k(x, z))∥∥∥ 2 ≥ ′ } ≤ 28 ( dσ‖A‖2l ′ )2 exp ( −\n′2D\n4‖A‖22(d+ 2)\n)\nPlease note that a similar corollary could have been obtained for the recent result of Sutherland and Schneider (2015) who refined the bound proposed by Rahimi and Recht by using a Bernstein concentration inequality instead of the Hoeffding inequality."
    }, {
      "heading" : "B Proof of the uniform error bound for ORFF approxima-",
      "text" : "tion\nThis section present a proof of theorem 3.1.\nWe note δ = x − z, K̃(x, z) = Φ̃(x)∗Φ̃(z), K̃j(x, z) = Φ̃j(x)∗Φ̃j(z) and K0(δ) = K(x, z). For sake of simplicity, we use the following notation:\nF (δ) = F (x− z) = K̃(x, z)−K(x, z) Fj(δ) = Fj(x− z) = (K̃j(x, z)−K(x, z))/D\nCompared to the scalar case, the proof follows the same scheme as the one described in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015) but requires to consider matrix norms and appropriate matrix concentration inequality. The main feature of theorem 3.1 is that it covers the case of bounded ORFF as well as unbounded ORFF: in the case of bounded ORFF, a Bernstein inequality for matrix concentration such that the one proved in Mackey et al. (2014) (Corollary 5.2) or the formulation of Tropp (2012) recalled in Koltchinskii et al. (2013) is suitable. However some kernels like the curl and the div-free kernels do not have bounded ‖Fj‖ but exhibit Fj with subexponential tails. Therefore, we will use a Bernstein matrix concentration inequality adapted for random matrices with subexponential norms (Koltchinskii et al. (2013)).\nB.1 Epsilon-net Let DC = {x− z | x, z ∈ C} with diameter at most 2l where l is the diameter of C. Since C is supposed compact, so is DC . It is then possible to find an -net covering DC with at most T = (4l/r)d balls of radius r.\nLet us call δi, i = 1, . . . , T the center of the i-th ball, also called anchor of the -net. Denote LF the Lipschitz constant of F . Let ‖.‖ be the `2 norm on L(Rp), that is the spectral norm. Now let use introduce the following lemma:\nLemma B.0.1. ∀δ ∈ DC , if (1): LF ≤ 2r and (2): ‖F (δi)‖ ≤ 2 ,for all 0 < i < T , then ‖F (δ)‖ ≤ .\nProof. ‖F (δ)‖ = ‖F (δ)− F (δi) + F (δi)‖ ≤ ‖F (δ)− F (δi)‖+‖F (δi)‖, for all 0 < i < T . Using the Lipschitz continuity of F we have ‖F (δ)− F (δi)‖ ≤ LF ‖δ − δi‖ ≤ rLF hence ‖F (δ)‖ ≤ rLF + ‖F (δi)‖.\nTo apply the lemma, we must bound the Lipschitz constant of the matrix-valued function F (condition (1)) and ‖F (δi)‖, for all i = 1, . . . , T as well (condition (2)).\nB.2 Regularity condition We first establish that ∂∂δEK̃(δ) = E ∂ ∂δ K̃(δ). Since K̃ is a finite dimensional matrix-valued function, we verify the integrability coefficient-wise, following Sutherland and Schneider (2015)’s demonstration. Namely, without loss of generality we show [\n∂\n∂δ EK̃(δ) ] lm = E ∂ ∂δ [ K̃(δ) ] lm\nwhere [A]lm denotes the l-th row and m-th column element of the matrix A.\nProposition B.1 (Differentiation under the integral sign). Let X be an open subset of Rd and Ω be a measured space. Suppose that the function f : X × Ω→ R verifies the following conditions:\n• f(x, ω) is a measurable function of ω for each x in X .\n• For almost all ω in Ω, the derivative ∂f(x, ω)/∂xi exists for all x in X .\n• There is an integrable function Θ : Ω→ R such that |∂f(x, ω)/∂xi| ≤ Θ(ω) for all x in X .\nThen ∂\n∂xi ∫ Ω f(x, ω)dω = ∫ Ω ∂ ∂xi f(x, ω)dω.\nDefine the function G̃i,l,mx,y (t, ω) : R × Ω → R by G̃i,l,mx,y (t, ω) = [ K̃(x+ tei − y) ] lm = [ G̃ix,y(t, ω) ] lm ,\nwhere ei is the i-th standard basis vector. Then G̃i,l,mx,y is integrable w.r.t. ω since∫ Ω G̃i,l,mx,y (t, ω)dω = E [ K̃(x+ tei − y) ] lm = [K(x+ tei − y)]lm <∞.\nAdditionally for any ω in Ω, ∂/∂tG̃i,l,mx,y (t, ω) exists and satisfies\nE ∣∣∣∣ ∂∂t G̃i,l,mx,y (t, ω) ∣∣∣∣ = E ∣∣∣∣∣∣ 1D D∑ j=1 A(ω)lm ( sin〈y, ωj〉 ∂ ∂t sin(〈x, ωj〉+ tωij) + cos〈y, ωj〉 ∂ ∂t cos(〈x, ωj〉+ tωij) )∣∣∣∣∣∣ = E\n∣∣∣∣∣∣ 1D D∑ j=1 A(ω)lm (ωji sin〈y, ωj〉 sin(〈x, ωj〉+ tωji)− ωji cos〈y, ωj〉 cos(〈x, ωj〉+ tωji)) ∣∣∣∣∣∣ ≤ E  1 D D∑ j=1 |A(ω)lmωji sin〈y, ωj〉 sin(〈x, ωj〉+ tωji)|+ |A(ω)lmωji cos〈y, ωj〉 cos(〈x, ωj〉+ tωji)|\n ≤ E  1 D D∑ j=1 2|A(ω)lmωji|\n . Hence\nE ∣∣∣∣ ∂∂tG̃ix,y(t, ω) ∣∣∣∣ ≤ 2E [|ω ⊗A(ω)|] .\nwhich is assumed to exist since in finite dimensions all norms are equivalent and Eµ [ ‖ω‖2‖A(ω)‖2 ] is assume\nto exists. Thus applying proposition B.1 we have [ ∂ ∂δi EK̃(δ) ] lm = E ∂∂δi [ K̃(δ) ] lm The same holds for y by symmetry. Combining the results for each component xi and for each element lm, we get that ∂∂δEK̃(δ) = E ∂∂δ K̃(δ).\nB.3 Bounding the Lipschitz constant Since F is differentiable, LF = ∥∥∂F ∂δ (δ ∗) ∥∥ where δ∗ = arg maxδ∈DC∥∥∂F∂δ (δ)∥∥.\nEµ,δ∗ [ L2f ] = Eµ,δ∗ ∥∥∥∥∥∂K̃∂δ (δ∗)− ∂K0∂δ (δ∗) ∥∥∥∥∥ 2\n≤ Eδ∗ Eµ ∥∥∥∥∥∂K̃∂δ (δ∗) ∥∥∥∥∥ 2 − 2 ∥∥∥∥∂K0∂δ (δ∗) ∥∥∥∥Eµ ∥∥∥∥∥∂K̃∂δ (δ∗) ∥∥∥∥∥+ ∥∥∥∥∂K0∂δ (δ∗) ∥∥∥∥2 \nUsing Jensen’s inequality ∥∥∥Eµ ∂K̃∂δ (δ∗)∥∥∥ ≤ Eµ∥∥∥∂K̃∂δ (δ∗)∥∥∥ and ∂∂δEK̃(δ) = E ∂∂δ K̃(δ). Since K̃ (see ap-\npendix B.2), Eµ ∂K̃∂δ (δ ∗) = ∂∂δEµK̃(δ ∗) = ∂K0∂δ (δ ∗) thus\nEµ,δ∗ [ L2f ] ≤ Eδ∗ Eµ ∥∥∥∥∥∂K̃∂δ (δ∗) ∥∥∥∥∥ 2 − 2 ∥∥∥∥∂K0∂δ (δ∗) ∥∥∥∥2 + ∥∥∥∥∂K0∂δ (δ∗) ∥∥∥∥2 \n= Eµ,δ∗ ∥∥∥∥∥∂K̃∂δ (δ∗) ∥∥∥∥∥ 2 − Eδ∗ ∥∥∥∥∂K0∂δ (δ∗) ∥∥∥∥2\n≤ Eµ,δ∗ ∥∥∥∥∥∂K̃∂δ (δ∗) ∥∥∥∥∥ 2\n= Eµ,δ∗ ∥∥∥∥ ∂∂δ∗ cos〈δ∗, ω〉A(ω) ∥∥∥∥2 = Eµ,δ∗‖−ω sin(〈δ∗, ω〉)⊗A(ω)‖2\n≤ Eµ [ ‖ω‖2‖A(ω)‖2 ] , σ2p\nEventually applying Markov’s inequality yields\nP { LF ≥\n2r\n} = P { L2F ≥ ( 2r )2} ≤ σ2p ( 2r )2 . (13)\nB.4 Bounding F on a given anchor point δi To bound ‖F (δi)‖2, Hoeffding inequality devoted to matrix concentration Mackey et al. (2014) can be applied. We prefer here to turn to tighter and refined inequalities such as Matrix Bernstein inequalities (Sutherland and Schneider (2015) also pointed that for the scalar case).\nIf we had bounded ORFF, we could use the following Bernstein matrix concentration inequality proposed in Ahlswede and Winter; Tropp (2012); Koltchinskii et al. (2013).\nTheorem B.1 (Bounded non-commutative Bernstein concentration inequality). Verbatim from Theorem 3 of Koltchinskii et al. (2013), consider a sequence (Xj)Dj=1 of D independent Hermitian (here symmetric) p × p random matrices that satisfy EXj = 0 and suppose that for some constant U > 0, ‖Xj‖ ≤ U for each index j. Denote BD =\n∥∥E[X21 + . . . X2D]∥∥. Then, for all ≥ 0, P\n ∥∥∥∥∥∥ D∑ j=1 Xj ∥∥∥∥∥∥ ≥  ≤ p exp ( −\n2\n2BD + 2U /3\n)\nHowever, to cover the general case including unbounded ORFFs like curl and div-free ORFFs, we choose a version of Bernstein matrix concentration inequality proposed in Koltchinskii et al. (2013) that allow to consider matrices are not uniformly bounded but have subexponential tails.\nTheorem B.2 (Unbounded non-commutative Bernstein concentration inequality). Verbatim from Theorem 4 of Koltchinskii et al. (2013). Let X1, . . . , XD be independent Hermitian p× p random matrices, such that EXj = 0 for all j = 1, . . . , D. Let ψ = ψ1. Define\nF(D) , D∑ j=1 Xj and BD , ∥∥∥∥∥∥E  D∑ j=1 X2j ∥∥∥∥∥∥. Suppose that,\nM = 2 max 1≤j≤D ‖‖Xj‖‖ψ\nLet δ ∈ ] 0; 2e−1 [ and\nŪ ,M log\n( 2\nδ\nM2 B2D + 1 ) Then, for Ū ≤ (e− 1)(1 + δ)BD,\nP {∥∥F(D)∥∥ ≥ } ≤ 2p exp(− 2\n2(1 + δ)BD + 2 Ū/3\n) (14)\nand for Ū > (e− 1)(1 + δ)BD, P {∥∥F(D)∥∥ ≥ } ≤ 2p exp(−\n(e− 1)Ū\n) . (15)\nTo use this theorem, we set: Xj = Fj(δi). We have indeed: Eµ[Fj(δi)] = 0 since K̃(δi) is the Monte-Carlo approximation of K0(δi) and the matrices Fj(δi) are symmetric. We assume we can bound all the Orlicz norms of the Fj(δi) = 1D (K̃j(δi)−K0(δi)). Please note that in the following we use a constant m such that m = DM ,\nm = 2D max 1≤j≤D ‖‖Fj(δi)‖‖ψ\n≤ 2 max 1≤j≤D ∥∥∥∥∥∥K̃j(δi)∥∥∥∥∥∥ ψ + 2‖‖K0(δi)‖‖ψ\n< 4 max 1≤j≤D ‖‖A(ωj)‖‖ψ + 4‖K0(δi)‖\nThen Ū can be re-written using m and D:\nŪ = m\nD log\n( 2\nδ\nm2 b2D + 1 ) We define: ū = DŪ and bD = DBD. Then, we get: for Ū ≤ (e− 1)(1 + δ)BD,\nP {‖F (δi)‖ ≥ } ≤ 2p exp ( − D 2\n2(1 + δ)bD + 2 ū/3\n) (16)\nand for Ū > (e− 1)(1 + δ)BD, P {‖F (δi)‖ ≥ } ≤ 2p exp ( − D\n(e− 1)ū\n) . (17)\nB.5 Union bound Now take the union bound over the centers of the -net:\nP { ∗⋃ i=1 ‖F (δi)‖ ≥ 2 } ≤ 4Tp exp ( −\n2D\n8((1+δ)bD+ 2 6 ū)\n) if ŪD ≤ (e− 1)(1 + δ)BD\nexp ( − D2(e−1)ū ) otherwise.\n(18)\nB.5.1 Optimizing over r Combining eq. (18) and eq. (13) and taking δ = 1 < 2/(e− 1) yields\nP {\nsup δ∈DC\n‖F (δ)‖ ≤ } ≥ 1− κ1r−d − κ2r2,\nwith\nκ2 = 4σ 2 p −2 and κ1 = 2p(4l)d exp ( −\n2D\n16(bD+ 6 ūD)\n) if ŪD ≤ 2(e− 1)BD\nexp ( − D2(e−1)ūD ) otherwise.\nwe choose r such that dκ1r−d−1 − 2κ2r = 0, i.e. r = ( dκ1 2κ2 ) 1 d+2 . Eventually let\nC ′d =\n(( d\n2\n) −d d+2\n+\n( d\n2\n) 2 d+2 ) the bound becomes\nP {\nsup δ∈DC ∥∥∥F̃i(δ)∥∥∥ ≥ } ≤ C ′dκ 2d+21 κ dd+22 = C ′d ( 4σ2p −2) dd+2 2p(4l)d exp ( −\n2D\n16(BD+ 6 ŪD)\n) if ŪD ≤ 2(e− 1)BD\nexp ( − D\n2(e−1)ŪD\n) otherwise\n 2 d+2\n= pC ′d2 2+4d+2d d+2\n( σpl ) 2d d+2 exp ( −\n2\n8(d+2)(BD+ 6 ŪD)\n) if ŪD ≤ 2(e− 1)BD\nexp ( −\n(d+2)(e−1)ŪD\n) otherwise\n= pC ′d2 6d+2 d+2\n( σpl ) 2 1+2/d exp ( −\n2\n8(d+2)(BD+ 6 ŪD)\n) if ŪD ≤ 2(e− 1)BD\nexp ( −\n(d+2)(e−1)ŪD\n) otherwise.\nConclude the proof by taking Cd = C ′d2 6d+2 d+2 ."
    }, {
      "heading" : "C Application of the bounds to decomposable, curl-free, divergence-",
      "text" : "free kernels\nProposition C.1 (Bounding the term bD). Define the random matrix Vµ[A(ω)] as follows: `,m ∈ {1, . . . , p}, Vµ[A(ω)]`m = ∑p r=1Covµ[(A(ω)`r, A(ω)rm]. For a given δ = x− z, with the previous notations\nbD = D ∥∥∥∥∥∥Eµ  D∑ j=1 ( F̃j(δ) )2∥∥∥∥∥∥ 2 ,\nwe have: bD ≤ 1 2 ( ∥∥(K0(2δ) +K0(0))Eµ[A(ω)]− 2K0(δ)2∥∥2 + 2‖Vµ[A(ω)]‖2).\nProof. We fix δ = x − z. For sake of simplicity, we note: BD = ∥∥Eµ [F1(δ)2 + . . .+ FD(δ)2]∥∥2 and we have bD = DBD, with the notations of the theorem. Then\nBD = ∥∥∥∥∥∥Eµ  D∑ j=1 1 D2 (K̃j(δ)−K0(δ))2 ∥∥∥∥∥∥ 2\n= 1\nD2 ∥∥∥∥∥∥ D∑ j=1 Eµ [( K̃j(δ) 2 − K̃j(δ)K0(δ)−K0(δ)K̃j(δ) +K0(δ)2 )]∥∥∥∥∥∥\n2\n.\nAs K0(δ)∗ = K0(δ) and A(ωj)∗ = A(ωj), then K̃j(δ)∗ = K̃j(δ), we have\nBD = 1\nD2 ∥∥∥∥∥∥ ∑ j Eµ [ K̃j(δ) 2 − 2K̃j(δ)K0(δ) +K0(δ)2 ]∥∥∥∥∥∥\n2\n.\nFrom the definition of K̃j , Eµ[K̃j(δ)] = K0(δ) which leads to\nBD = 1\nD2 ∥∥∥∥∥∥ D∑ j=1 Eµ ( K̃j(δ) 2 −K0(δ)2 )∥∥∥∥∥∥\n2\nNow we omit the j index since all vectors ωj are identically distributed and consider a random vector ω ∼ µ:\nBD = 1 D2 ∥∥DEµ [(cos〈ω, δ〉)2A(ω)2]−K0(δ)2∥∥2\nA trigonometry property gives us: (cos〈ω, δ〉)2 = 12 (cos〈ω, 2δ〉+ cos〈ω, 0〉)\nBD = 1\nD2 ∥∥∥∥D2 Eµ [(cos〈ω, 2δ〉+ cos〈ω, 0〉)A(ω)2]−K0(δ)2 ∥∥∥∥\n2\n= 1\n2D ∥∥∥∥Eµ [(cos〈ω, 2δ〉+ cos〈ω, 0〉)A(ω)2]− 2DK0(δ)2 ∥∥∥∥\n2\n(19)\nMoreover, we write the expectation of a matrix product, coefficient by coefficient, as: ∀`,m ∈ {1, . . . , p},\nEµ [( cos〈ω, 2δ〉A(ω)2 )] `m = ∑ r Eµ [cos〈ω, 2δ〉A(ω)]`r Eµ [A(ω)]rm + Covµ[cos〈ω, 2δ〉A(ω)`r, A(ω)rm]\nEµ [( cos〈ω, 2δ〉A(ω)2 )] = Eµ[cos〈ω, 2δ〉A(ω)]Eµ[A(ω)] + Σcos\n= K0(2δ)Eµ[A(ω)] + Σcos\nwhere the random matrix Σcos is defined by: Σcos`m = ∑ rCovµ[cos〈ω, 2δ〉A(ω)`r, A(ω)rm]. Similarly, we get:\nEµ [ cos〈ω, 0〉A(ω)2 ] = K0(0)Eµ [A(ω)] + Σcos.\nHence, we have:\nBD = 1\n2D ∥∥(K0(2δ) +K0(0))Eµ[A(ω)]− 2K0(δ)2 + 2Σcos∥∥2 ≤ 1\n2D [∥∥(K0(2δ) +K0(0))Eµ[A(ω)]− 2K0(δ)2∥∥2 + 2‖Vµ[A(ω)]‖2] , using ‖Σcos‖2 ≤ ‖Vµ[A(ω)]‖2, where Vµ[A(ω)] = Eµ[(A(ω) − Eµ[A(ω)])\n2] and for all `,m ∈ {1, . . . , p}, Vµ[A(ω)]`m = ∑p r=1Covµ[A(ω)`r, A(ω)rm].\nFor the three kernels of interest, we illustrate this bound in fig. 6."
    }, {
      "heading" : "D Additional information and results",
      "text" : "D.1 Implementation detail For each ωj ∼ µ, let B(ωj) be a p by p′ matrix such that B(ωj)B(ωj)∗ = A(ωj). In practice, making a prediction y = h(x) using directly the formula h(x) = Φ̃(x)∗θ is prohibitive. Indeed, if Φ(x) = ⊕D j=1 exp(−i〈x, ωj〉)B(ωj)∗B(ωj), it would cost O(Dp′p) operation to make a prediction, since ˜Φ(x) is a Dp′ by p matrix.\nD 100 101 102 103 104 105\nEmpirical Upper Bound\nVariance 10-4 10-3 10-2 10-1 100 101 102 E rr or\nDecomposable kernel\nD\n100 101 102 103 104\nEmpirical Upper Bound\nVariance 10-4 10-3 10-2 10-1 100 101 102 E rr or\nCurl-free kernel\nD.2 Simulated dataset"
    } ],
    "references" : [ {
      "title" : "Kernels for vector-valued functions: a review",
      "author" : [ "M.A. Álvarez", "L. Rosasco", "N.D. Lawrence" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Álvarez et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Álvarez et al\\.",
      "year" : 2012
    }, {
      "title" : "On the equivalence between quadrature rules and random features",
      "author" : [ "F. Bach" ],
      "venue" : null,
      "citeRegEx" : "Bach.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bach.",
      "year" : 2015
    }, {
      "title" : "Vector field learning via spectral filtering",
      "author" : [ "L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri" ],
      "venue" : "editors, ECML/PKDD,",
      "citeRegEx" : "Baldassarre et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baldassarre et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-output learning via spectral filtering",
      "author" : [ "L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Baldassarre et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Baldassarre et al\\.",
      "year" : 2012
    }, {
      "title" : "Vector valued reproducing kernel hilbert spaces and universality",
      "author" : [ "C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanità" ],
      "venue" : "Analysis and Applications,",
      "citeRegEx" : "Carmeli et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Carmeli et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning output kernels with block coordinate descent",
      "author" : [ "F. Dinuzzo", "C. Ong", "P. Gehler", "G. Pillonetto" ],
      "venue" : "In Proc. of the 28th Int. Conf. on Machine Learning,",
      "citeRegEx" : "Dinuzzo et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dinuzzo et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning multiple tasks with kernel methods",
      "author" : [ "T. Evgeniou", "C.A. Micchelli", "M. Pontil" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Evgeniou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Evgeniou et al\\.",
      "year" : 2005
    }, {
      "title" : "A course in abstract harmonic analysis",
      "author" : [ "G.B. Folland" ],
      "venue" : "CRC press,",
      "citeRegEx" : "Folland.,? \\Q1994\\E",
      "shortCiteRegEx" : "Folland.",
      "year" : 1994
    }, {
      "title" : "Lsmr: An iterative algorithm for sparse least-squares problems",
      "author" : [ "D.C.-L. Fong", "M. Saunders" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Fong and Saunders.,? \\Q2011\\E",
      "shortCiteRegEx" : "Fong and Saunders.",
      "year" : 2011
    }, {
      "title" : "Refined Error Estimates for Matrix-Valued Radial Basis Functions",
      "author" : [ "E. Fuselier" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Fuselier.,? \\Q2006\\E",
      "shortCiteRegEx" : "Fuselier.",
      "year" : 2006
    }, {
      "title" : "Random features for kernel deep convex network",
      "author" : [ "P.-S. Huang", "L. Deng", "M. Hasegawa-Johnson", "X. He" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Huang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2013
    }, {
      "title" : "Using the fisher kernel method to detect remote protein homologies",
      "author" : [ "T. Jaakkola", "M. Diekhans", "D. Haussler" ],
      "venue" : "In ISMB,",
      "citeRegEx" : "Jaakkola et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 1999
    }, {
      "title" : "A remark on low rank matrix recovery and noncommutative bernstein type inequalities. In From Probability to Statistics and Back: High-Dimensional Models and Processes–A Festschrift in Honor of Jon A",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "Institute of Mathematical Statistics,",
      "citeRegEx" : "Koltchinskii,? \\Q2013\\E",
      "shortCiteRegEx" : "Koltchinskii",
      "year" : 2013
    }, {
      "title" : "Fastfood - computing hilbert space expansions in loglinear time",
      "author" : [ "Q.V. Le", "T. Sarlós", "A.J. Smola" ],
      "venue" : "In Proceedings of ICML 2013, Atlanta,",
      "citeRegEx" : "Le et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2013
    }, {
      "title" : "Operator-valued kernel-based vector autoregressive models for network inference",
      "author" : [ "N. Lim", "F. d’Alché-Buc", "C. Auliac", "G. Michailidis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Lim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lim et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning div-free and curl-free vector fields by matrix-valued kernels",
      "author" : [ "Y. Macedo", "R. Castro" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Macedo and Castro.,? \\Q2008\\E",
      "shortCiteRegEx" : "Macedo and Castro.",
      "year" : 2008
    }, {
      "title" : "Matrix concentration inequalities via the method of exchangeable pairs",
      "author" : [ "L. Mackey", "M.I. Jordan", "R. Chen", "B. Farrel", "J. Tropp" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "Mackey et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mackey et al\\.",
      "year" : 2014
    }, {
      "title" : "On learning vector-valued functions",
      "author" : [ "C.A. Micchelli", "M.A. Pontil" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Micchelli and Pontil.,? \\Q2005\\E",
      "shortCiteRegEx" : "Micchelli and Pontil.",
      "year" : 2005
    }, {
      "title" : "Matrix-valued kernels for shape deformation analysis",
      "author" : [ "M. Micheli", "J. Glaunes" ],
      "venue" : "Technical report, Arxiv report,",
      "citeRegEx" : "Micheli and Glaunes.,? \\Q2013\\E",
      "shortCiteRegEx" : "Micheli and Glaunes.",
      "year" : 2013
    }, {
      "title" : "Multiclass learning with simplex coding",
      "author" : [ "Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-j. Slotine" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mroueh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mroueh et al\\.",
      "year" : 2012
    }, {
      "title" : "The matrix cookbook",
      "author" : [ "K.B. Petersen", "M.S. Pedersen" ],
      "venue" : "Technical University of Denmark,",
      "citeRegEx" : "Petersen and Pedersen,? \\Q2008\\E",
      "shortCiteRegEx" : "Petersen and Pedersen",
      "year" : 2008
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In NIPS 20,",
      "citeRegEx" : "Rahimi and Recht.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi and Recht.",
      "year" : 2007
    }, {
      "title" : "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality",
      "author" : [ "V. Sindhwani", "H.Q. Minh", "A. Lozano" ],
      "venue" : "In Proc. of UAI’13,",
      "citeRegEx" : "Sindhwani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sindhwani et al\\.",
      "year" : 2013
    }, {
      "title" : "Idr (s): A family of simple and fast algorithms for solving large nonsymmetric systems of linear equations",
      "author" : [ "P. Sonneveld", "M.B. van Gijzen" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Sonneveld and Gijzen.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sonneveld and Gijzen.",
      "year" : 2008
    }, {
      "title" : "Optimal rates for random fourier features",
      "author" : [ "B. Sriperumbudur", "Z. Szabo" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sriperumbudur and Szabo.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sriperumbudur and Szabo.",
      "year" : 2015
    }, {
      "title" : "On the error of random fourier features",
      "author" : [ "D.J. Sutherland", "J.G. Schneider" ],
      "venue" : "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Sutherland and Schneider.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sutherland and Schneider.",
      "year" : 2015
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "J.-A. Tropp" ],
      "venue" : "Comput. Math.,",
      "citeRegEx" : "Tropp.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2012
    }, {
      "title" : "Modeling magnetic fields using gaussian processes",
      "author" : [ "N. Wahlström", "M. Kok", "T. Schön", "F. Gustafsson" ],
      "venue" : "In in Proc. of the 38th ICASSP,",
      "citeRegEx" : "Wahlström et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wahlström et al\\.",
      "year" : 2013
    }, {
      "title" : "Nyström method vs random fourier features: A theoretical and empirical comparison",
      "author" : [ "T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z. Zhou" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2012
    }, {
      "title" : "A la carte - learning fast kernels",
      "author" : [ "Z. Yang", "A.J. Smola", "L. Song", "A.G. Wilson" ],
      "venue" : "CoRR, abs/1412.6493,",
      "citeRegEx" : "Yang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "A la carte - learning fast kernels",
      "author" : [ "Z. Yang", "A.G. Wilson", "A.J. Smola", "L. Song" ],
      "venue" : "In Proc. of AISTATS",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Refinement of operator-valued reproducing kernels",
      "author" : [ "H. Zhang", "Y. Xu", "Q. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "Compared to the scalar case, the proof follows the same scheme as the one described in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015) but requires to consider matrix norms and appropriate matrix concentration inequality. The main feature of theorem 3.1 is that it covers the case of bounded ORFF as well as unbounded ORFF: in the case of bounded ORFF, a Bernstein inequality for matrix concentration such that the one proved in Mackey et al",
      "author" : [ "Koltchinskii" ],
      "venue" : null,
      "citeRegEx" : "Koltchinskii,? \\Q2012\\E",
      "shortCiteRegEx" : "Koltchinskii",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "1 Introduction Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al.",
      "startOffset" : 37,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "1 Introduction Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al., 2011), vector field learning (Baldassarre et al.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : ", 2011), vector field learning (Baldassarre et al., 2012) and vector autoregression (Sindhwani et al.",
      "startOffset" : 31,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : ", 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure.",
      "startOffset" : 34,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : ", 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure.",
      "startOffset" : 34,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; Álvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.",
      "startOffset" : 24,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; Álvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.",
      "startOffset" : 24,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; Álvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.",
      "startOffset" : 24,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.",
      "startOffset" : 135,
      "endOffset" : 270
    }, {
      "referenceID" : 13,
      "context" : "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.",
      "startOffset" : 135,
      "endOffset" : 270
    }, {
      "referenceID" : 29,
      "context" : "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.",
      "startOffset" : 135,
      "endOffset" : 270
    }, {
      "referenceID" : 24,
      "context" : "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.",
      "startOffset" : 135,
      "endOffset" : 270
    }, {
      "referenceID" : 1,
      "context" : "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.",
      "startOffset" : 135,
      "endOffset" : 270
    }, {
      "referenceID" : 25,
      "context" : "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.",
      "startOffset" : 135,
      "endOffset" : 270
    }, {
      "referenceID" : 13,
      "context" : "RFFs has been shown to be efficient on large datasets and further improved by efficient matrix computations of FastFood (Le et al., 2013), and is considered as one of the best large scale implementations of kernel methods, along with Nÿstrom approaches (Yang et al.",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : ", 2013), and is considered as one of the best large scale implementations of kernel methods, along with Nÿstrom approaches (Yang et al., 2012).",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015).",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015).",
      "startOffset" : 128,
      "endOffset" : 232
    }, {
      "referenceID" : 21,
      "context" : "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach 1See Rudin (1994).",
      "startOffset" : 128,
      "endOffset" : 264
    }, {
      "referenceID" : 21,
      "context" : "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach 1See Rudin (1994).",
      "startOffset" : 128,
      "endOffset" : 355
    }, {
      "referenceID" : 21,
      "context" : "For the Gaussian kernel k(x − z) = exp(−γ‖x− z‖), the spectral distribution μ(ω) is Gaussian Rahimi and Recht (2007).",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "We will define operator-valued kernel as reproducing kernels following the presentation of Carmeli et al. (2010). Given X and Y , a map K : X × X → L(Y) is called a Y-reproducing kernel if",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "As a consequence (Carmeli et al., 2010) we have: K(x, z) = K∗ xKz ∀x, z ∈ X HK = span {Kxy | ∀x ∈ X , ∀y ∈ Y} Another way to describe functions ofHK consists in using a suitable feature map.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "As a consequence (Carmeli et al., 2010) we have: K(x, z) = K∗ xKz ∀x, z ∈ X HK = span {Kxy | ∀x ∈ X , ∀y ∈ Y} Another way to describe functions ofHK consists in using a suitable feature map. Proposition 1.1 (Carmeli et al. (2010)).",
      "startOffset" : 18,
      "endOffset" : 230
    }, {
      "referenceID" : 4,
      "context" : "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)).",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)).",
      "startOffset" : 44,
      "endOffset" : 278
    }, {
      "referenceID" : 4,
      "context" : "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al.",
      "startOffset" : 44,
      "endOffset" : 523
    }, {
      "referenceID" : 4,
      "context" : "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al. (2012) for our case of interest of X = R and Y = R.",
      "startOffset" : 44,
      "endOffset" : 544
    }, {
      "referenceID" : 31,
      "context" : "2Equation (36) in Zhang et al. (2012).",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "13 in Carmeli et al. (2010) to matrix-valued operators.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "13 in Carmeli et al. (2010) to matrix-valued operators. Proposition 2.2 (Carmeli et al. (2010)).",
      "startOffset" : 6,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, μ is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1).",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, μ is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2. Proposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(ω) such that A(ω) = B(ω)B(ω)∗. Then the function Φ : R → L(R, μ;R) defined for all x ∈ R by ∀y ∈ R, (Φxy) (ω) = ei〈x,ω〉B(ω)∗y (5) is a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in R, ΦxΦz = K(x, z). Thus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(ω), μ(ω) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to R-Mercer kernel gives the solution.",
      "startOffset" : 26,
      "endOffset" : 868
    }, {
      "referenceID" : 4,
      "context" : "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, μ is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2. Proposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(ω) such that A(ω) = B(ω)B(ω)∗. Then the function Φ : R → L(R, μ;R) defined for all x ∈ R by ∀y ∈ R, (Φxy) (ω) = ei〈x,ω〉B(ω)∗y (5) is a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in R, ΦxΦz = K(x, z). Thus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(ω), μ(ω) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to R-Mercer kernel gives the solution. Proposition 2.4 (Carmeli et al. (2010)).",
      "startOffset" : 26,
      "endOffset" : 960
    }, {
      "referenceID" : 14,
      "context" : "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al.",
      "startOffset" : 156,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al.",
      "startOffset" : 271,
      "endOffset" : 296
    }, {
      "referenceID" : 2,
      "context" : "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al. (2012) for vector field learning.",
      "startOffset" : 297,
      "endOffset" : 323
    }, {
      "referenceID" : 10,
      "context" : "A usual choice is to choose k as a Gaussian kernel with k0(δ) = exp ( − 2 2σ2 ) , which gives μ = N (0, σ−2I) (Huang et al., 2013) as its inverse Fourier transform.",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 22,
      "context" : "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 14,
      "context" : "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "If a graph coding for the proximity between tasks is known, then it is shown in Evgeniou et al. (2005); Baldassarre et al.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "(2005); Baldassarre et al. (2010) that A can be chosen equal to the pseudo inverse L† of the graph Laplacian, and then the `2 norm in HK is a graph-regularizing penalty for the outputs (tasks).",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).",
      "startOffset" : 141,
      "endOffset" : 219
    }, {
      "referenceID" : 3,
      "context" : "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).",
      "startOffset" : 141,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).",
      "startOffset" : 141,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : "Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlström et al.",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlström et al., 2013).",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p). Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlström et al., 2013). These kernels discussed in Fuselier (2006) allow encoding input-dependent similarities between vector-fields.",
      "startOffset" : 167,
      "endOffset" : 492
    }, {
      "referenceID" : 15,
      "context" : "Although taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012).",
      "startOffset" : 209,
      "endOffset" : 260
    }, {
      "referenceID" : 3,
      "context" : "Although taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012).",
      "startOffset" : 209,
      "endOffset" : 260
    }, {
      "referenceID" : 25,
      "context" : "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2 = ‖A‖2 ∥∥∥k̃(x, z)− k(x, z)∥∥∥ This theorem and its proof are presented in corollary A.",
      "startOffset" : 218,
      "endOffset" : 281
    }, {
      "referenceID" : 24,
      "context" : "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2 = ‖A‖2 ∥∥∥k̃(x, z)− k(x, z)∥∥∥ This theorem and its proof are presented in corollary A.",
      "startOffset" : 218,
      "endOffset" : 281
    }, {
      "referenceID" : 16,
      "context" : "It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al., 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.",
      "startOffset" : 298,
      "endOffset" : 319
    }, {
      "referenceID" : 26,
      "context" : ", 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels).",
      "startOffset" : 76,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels).",
      "startOffset" : 49,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2 = ‖A‖2 ∥∥∥k̃(x, z)− k(x, z)∥∥∥ This theorem and its proof are presented in corollary A.",
      "startOffset" : 176,
      "endOffset" : 200
    }, {
      "referenceID" : 19,
      "context" : "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, ∥∥∥K̃(x, z)−K(x, z)∥∥∥ 2 = ‖A‖2 ∥∥∥k̃(x, z)− k(x, z)∥∥∥ This theorem and its proof are presented in corollary A.1.1. More interestingly, we propose a new bound for Operator Random Fourier Feature approximation in the general case. It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al.",
      "startOffset" : 176,
      "endOffset" : 801
    }, {
      "referenceID" : 12,
      "context" : "For the latter, we notice that their norms behave as subexponential random variables (Koltchinskii et al., 2013). Before introducing the new theorem, we give the definition of the Orlicz norm and subexponential random variables. Definition 3.1 (Orlicz norm). We follow the definition given by Koltchinskii et al. (2013). Let ψ : R+ → R+ be a non-decreasing convex function with ψ(0) = 0.",
      "startOffset" : 86,
      "endOffset" : 320
    }, {
      "referenceID" : 21,
      "context" : "It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.",
      "startOffset" : 39,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "To predict classes, we use simplex coding method presented in Mroueh et al. (2012). The intuition behind simplex coding is to project the binarized labels of dimension p onto the most separated vectors on the hypersphere of dimension p − 1.",
      "startOffset" : 62,
      "endOffset" : 83
    } ],
    "year" : 2016,
    "abstractText" : "Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operatorvalued kernels. We propose a general principle for Operator-valued Random Fourier ∗ro.brault@telecom-paristech.fr †florence.dalche@telecom-paristech.fr ‡markus.o.heinonen@aalto.fi 1 ar X iv :1 60 5. 02 53 6v 3 [ cs .L G ] 2 4 M ay 2 01 6 Feature construction relying on a generalization of Bochner’s theorem for translationinvariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}