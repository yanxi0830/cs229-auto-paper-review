{
  "name" : "1605.09227.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Combinatorial Functions from Pairwise Comparisons",
    "authors" : [ "Maria-Florina Balcan", "Ellen Vitercik", "Colin White" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support."
    }, {
      "heading" : "1 Introduction",
      "text" : "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7]. For example, a business might wish to learn its consumers’ combinatorial valuation functions, since this will allow them to better set prices, choose which goods to sell as bundles, and determine inventory levels. Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16]. However, for real-valued functions, this cardinal data may not be accessible. Indeed, it may be difficult for a consumer to provide the real-valued number corresponding to her valuation for a bundle of goods. Instead, it might be more natural for her to express whether she likes one bundle of goods more than another. After all, it is well-known that humans are significantly better at comparing than scoring [4, 22]. Therefore, we may hope to learn a consumer’s valuation function up to pairwise comparisons, from pairwise comparisons.\nOf course, economics is not the only field where it would be valuable to learn an underlying function up to pairwise comparisons. Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4]. For example, in a social network, one might wish to learn the influence of subgroups and individuals, but it could be difficult to consistently assign real-valued numbers as measurements of this influence. Rather, it might be easier to simply answer which of two subgroups is more influential. Although the number of subgroups in a social network may be exponential in the number of nodes, through a polynomial number of such queries, we may hope to learn a pairwise comparison function that allows us to accurately predict which of any two subgroups is more influential.\n∗Authors’ addresses: {ninamf,vitercik,crwhite}@cs.cmu.edu.\nar X\niv :1\n60 5.\n09 22\n7v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 6"
    }, {
      "heading" : "1.1 Our Results",
      "text" : "In this paper, we prove that many classes of combinatorial functions can be learned up to comparisons. Our formal definition of what it means to learn a function up to comparisons is similar to the PAC setting: we say that a class of functions is comparator-learnable if there exists is an efficient algorithm which outputs a comparison function that, with high probability over the choice of examples, has small error over the distribution. For some function classes, we require that the function value of two sets be sufficiently far apart in order to guarantee that the learned comparator predicts accurately on those sets.\nMore formally, in Section 3, we show that for a fixed class F of combinatorial functions which map 2[n] to R, if any function in F can be multiplicatively approximated up to a factor of α(n) by some power of a linear function, then we can learn any function in F up to comparisons on pairs of sets whose values differ by at least an α(n) multiplicative factor. In this case, we say that F is comparator-learnable with separation α(n). Our results are summarized in Tables 1 and 2. Using existing approximation results [12, 2], we immediately conclude that several broad classes of combinatorial functions are comparator-learnable, including many that are ubiquitous in microeconomic theory. These include the nested classes of monotone submodular, XOS, and subadditive functions, all of which are used to model consumer preferences that exhibit diminishing marginal utility. In particular, we show that submodular functions are comparator-learnable with separation α(n) = √ n and provide a nearly-matching lower bound of α(n) = Ω̃(n1/3). Further, we show that the classes of XOS and subadditive functions are comparator-learnable with separation α(n) = Θ̃( √ n).\nWe also rely on results from [16] and [2] to achieve stronger bounds for submodular functions if the curvature is small. Curvature is a well-studied measure of submodular complexity which quantifies how close a function is to being fully additive. We prove that the separation factor approaches 1 (which is optimal) as the function class approaches full additivity, i.e. as the maximum curvature approaches 0. Further, for XOS functions with polynomially-many SUM trees, we show that the separation factor decreases as a function of the number of trees. In this way, the more structured a class in question is, the stronger our results in Section 3 are.\nIn Section 3 we only guarantee the accuracy of the learned comparator on pairs of sets whose values differ by a sufficiently large multiplicative factor. We show in Section 4 that if the underlying distribution over subsets of [n] is uniform, then we can take advantage of key insights regarding the Fourier spectrum of monotone submodular functions with range in [0, 1], presented in [11], to learn such a function up to comparisons on pairs of sets whose values differ by a sufficiently large additive factor. We extend this result to XOS functions with range in [0, 1] as well.\nIn Section 5, we show that our algorithm from Section 3 applies to a wide range of other classes of combinatorial functions. In particular, we present results for functions with sparse Fourier support [23] and functions with bounded nonlinear interactions [24]. For these more structured function classes we demonstrate a much better α(n) = 1, meaning we do not need to assume f(S) and f(S′) are sufficiently far apart to predict correctly. Finally, for coverage functions [9, 1], we achieve α(n) = 1 + . In Appendix A, we study comparator-learning k-submodular functions (submodular functions with range in {1, . . . , k}) in the membership query model, in which the algorithm may ask for labels on examples of its own construction. We show how to learn a k-submodular function up to a multiplicative separation of α with sample complexity and running time O ( nk/α ) ."
    }, {
      "heading" : "1.2 Our Techniques",
      "text" : "Our techniques depart significantly from learning with real-valued labeled examples. When attempting to learn a combinatorial function from cardinal data, rather than ordinal data, the existence of\nan approximating linear function implies a natural learning algorithm, via a reduction to learning linear separators. In our model, where we are only allowed to make pairwise comparison queries, we require a substantially different algorithm and analysis. At a high level, the existence of an approximating linear function still implies useful structure: once we know that such a function ~w exists and approximates the underlying function f up to an α(n) factor, then given two sets S and S′ such that f(S) and f(S′) are α(n) apart, we can learn a linear separator that classifies all sets with value less than f(S) as negative and all sets with value greater than f(S′) as positive. However, we would like to predict accurately over all random pairs of subsets, not only those whose values are separated by f(S) and f(S′). Even more problematic, using only pairwise comparisons, we cannot know if f(S) and f(S′) are α(n) apart in the first place.\nTo surmount this obstacle, we show that we can discretize the range of f using a set of “landmarks,” i.e. a sorted set of random examples. For every pair of landmarks Si and Sj , we attempt to solve for a linear separator that classifies a set S as negative if its value is less than f(Si) and positive if its value is greater than f(Sj). If f(Si) and f(Sj) are at least α(n) apart, then we are guaranteed that a such a linear separator must exist. Naturally, on a random pair of subsets T and T ′, the learned comparator simply searches for a linear separator that classifies T as positive and T ′ as negative, which implies that f(T ) < f(T ′), or vice versa. The key idea which guarantees the correctness of the algorithm is the fact that on one such random query, it is highly unlikely that f(T ) and f(T ′) are α(n) apart, and yet there does not exist a pair of landmarks Si and Sj such that (1) f(Si) and f(Sj) sit between f(T ) and f(T\n′) and (2) f(Si) and f(Sj) are also α(n) apart. This is exactly what we need, because if such a pair of landmarks does exist, then we will have solved for a linear separator that correctly classifies T and T ′."
    }, {
      "heading" : "1.3 Related Work",
      "text" : "Past work has explored the learnability of submodular and related functions when given access to a set of random examples labeled by the underlying function. Goemans et al. showed how to\nlearn an approximation of a submodular function within a multiplicative Õ( √ n) factor [12] in the\nmembership query model, i.e. the queries are selected adaptively by the algorithm. Balcan and Harvey showed how to efficiently learn a function that approximates the given submodular factor up to a √ n factor on a 1− fraction of the test inputs, with probability 1− δ,\nin the supervised learning setting [3]. They call this model the PMAC model of learning, where PMAC stands for “Probably Mostly Approximately Correct,” due to similarity to the PAC model of learning. They also show an Ω(n 1 3 ) lower bound in this model. A later paper by Balcan et al.\nshow near tight bounds on the PMAC learnability of subadditive functions and XOS (fractionally subadditive) functions [2].\nThere is also a large body of work on learning submodular functions with additive, rather than multiplicative, guarantees, when the underlying distribution over subsets of [n] is uniform. Gupta et al. gave an algorithm with runtime nO(log(1/δ)/\n2) which learns an approximation h to a submodular function f such that with high probability, |f(x)−h(x)| ≤ [13]. Feldman et al. show an algorithm with runtime 2Õ(1/\n4/5) · n2 for approximating a submodular function with L2 error [11]. Both of these results are accomplished by proving there exist low degree polynomials which approximate submodular functions.\nBadanidiyuru et al. showed that submodular functions always have an approximate function with a small sketch [1], and Iyer et al. showed parameterized bounds based on the curvature of the submodular function (how close the function is to being fully additive) [16].\nWe conclude this section by reviewing related works on ranking via pairwise comparisons. Jamieson and Nowak study this problem under the assumption that the n objects they wish to rank are embedded into a d-dimensional Euclidean space and that the ranking reflects the objects’ relative distance to some fixed point in Rd. They show an algorithm to learn the rank using O(d log n) queries on average [17]. Shah et al. study the ranking problem by assuming that the ranking reflects the inherent “qualities” of the objects, as defined by a vector ~w∗ ∈ Rn [21]. They work under the standard Bradley-Terry-Luce and Thurstone models, and prove upper and lower bounds on the optimal error when estimating ~w∗ in these models."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Combinatorial Functions",
      "text" : "Throughout this paper, we study different classes of combinatorial functions. All functions we study are defined over subsets of a ground set [n] = {1, . . . , n} and map 2[n] to R. We use χ(S) to represent the indicator function of the set S, so (χ(S))i = 1 if and only if i ∈ S, otherwise (χ(S))i = 0.\nWe define three important function classes here and defer the rest of the definitions to their respective sections. Subadditive functions. A function f is subadditive if and only if f(S ∪ S′) ≤ f(S) + f(S′), for all S, S′ ⊆ [n]. Intuitively, the value of a set is at most the sum of its parts. Submodular functions. A function f is submodular if and only if f(T ∪ {i}) − f(T ) ≤ f(S ∪ {i}) − f(S) for all S ⊆ T ⊆ [n]. Submodular functions model valuations that satisfy diminishing returns. Submodularity can also be thought of as the discrete analog of convex functions. XOS functions. A function f is XOS if and only if f(S) = maxj=1...k w T j χ(S) where wj ∈ Rn≥0. Alternatively, an XOS function is a MAX of SUM trees. For example, several vacation sites might offer overlapping amenities, and a vacationer might value the amenities differently depending on the site. She will likely then choose the site that has the maximum sum value for the amenities offered there, which means that her valuation function is XOS."
    }, {
      "heading" : "2.2 Learning Model",
      "text" : "We now define our notion of learnability up to comparisons. Let f be an unknown function from some class F (for instance, the class of submodular functions) and suppose that sets are drawn from some distribution D over 2[n]. Moreover, suppose that we have access to a pairwise comparison oracle which, on input S, S′ ∈ 2[n], returns 1 if f(S) ≤ f(S′) and 0 otherwise. Clearly, we cannot hope to learn f well in absolute terms in this model. Rather, our goal is to produce a hypothesis g such that for most pairs S, S′ ∼ D, either g predicts correctly which of f(S) or f(S′) is larger, or f(S) and f(S′) are separated by less than a multiplicative α(n) factor. We formally define this learning model as follows.\nDefinition 1 (comparator-learnable with separation α(n)). A class F of functions is comparatorlearnable with multiplicative separation α(n) if for all , δ ∈ (0, 1) and all f ∈ F , given a sample of sets over a distribution D with size polynomial in n, 1 , and\n1 δ , and given access to a pairwise\ncomparison oracle, then there exists an algorithm which outputs a pairwise function g with the following guarantee: with probability at least 1 − δ, for S, S′ ∼ D, the probability that α(n)f(S) ≤ f(S′) yet g incorrectly predicts that f(S) > f(S′) is at most .\nIn Section 3, we present a general algorithm which can be used to efficiently comparator-learn over a variety of combinatorial function classes with separation α(n), where the value of α(n) depends on the complexity of the function class at hand. For example, for fully additive functions, α(n) = 1, and for submodular functions that are close to being fully additive, i.e. those with small curvature κ, we have that α(n) = 11−κ . Meanwhile, for complex function classes, such as monotone submodular, α(n) = √ n. We note that even when given access to real-valued labeled examples, we\ndo not know how to learn a function that approximates a monotone submodular function up to any multiplicative factor better than √ n. In fact, we prove a nearly-matching lower bound for monotone\nsubmodular functions, namely that it is not possible to comparator-learn over this function class with separation o(n1/3/ log n).\nNext, in Section 4, we introduce a related definition: comparator-learnable with additive separation β. In this case, for most pairs S, S′ ∼ D, we guarantee that the learned comparator either\npredicts correctly which of f(S) or f(S′) is larger, or f(S) and f(S′) are separated by less than an additive β factor. Finally, in Section 5 we show that for certain highly structured function classes, we are able to learn with no separation. In other words, the learned comparator will predict accurately on a large fraction of all pairs, not just pairs which are sufficiently far apart."
    }, {
      "heading" : "3 General Framework for Learning with Comparisons",
      "text" : "In this section, we present a general algorithm for learning combinatorial functions up to pairwise comparisons. We guarantee that for an underlying combinatorial function f , with high probability our algorithm outputs a hypothesis g, where given a random S, S′, the probability that f(S) and f(S′) differ by a large multiplicative factor yet g predicts incorrectly is low.\nWe describe our algorithm for a large family of general function classes, each of which has the property that any function in the class can be approximated by the p-th root of linear function up to an α(n) factor. We then instantiate this algorithm for many classes of combinatorial functions. To make this notion more concrete, we define the following characterization of a class of functions.\nDefinition 2 (α(n)-approximable). A class F of set functions is α(n)-approximable if there exists a p ∈ R such that for all f ∈ F , there exists a vector ~wf ∈ Rn such that for all S ⊆ [n], f(S) ≤ (~wf · χ(S))p ≤ α(n)f(S).\nHigh Level Description of the Algorithm and Analysis. As a crucial first step in our algorithm design, we show that if the underlying function f is α(n)-approximable, then there exists a vector ~wf ∈ Rn such that for any sets Si and Sj in our sample, if α(n)f(Si) < f(Sj), then ~wf · χ(Si) < ~wf · χ(Sj). As one would expect, this is simply the vector ~wf referred to in Definition 2. Taking one step further, this means that there exists a linear separator such that if f(S) < f(Si) < f(Sj) < f(S\n′) for two sets S and S′, then S is labeled as negative and S′ is labeled as positive by the linear separator. This situation is illustrated by Figure 1a. This fact alone will not be enough when designing our comparator learning algorithm. After all, we want to predict accurately on all random pairs, not just those that span f(Si) and f(Sj). Moreover, without real-valued labels, it is impossible to know if f(Si) and f(Sj) are α(n) apart to begin with.\nThis suggests the following algorithm. We begin by discretizing the range of f using a series of “landmarks.” This is simply an initial random sample, which we sort according to f . Since f is α(n)-approximable, we know that if two landmarks Si and Sj are at least α(n) apart, then there exists a weight vector ~wij ∈ Rn and threshold θij ∈ R that classifies a set S as negative if f(S) < f(Si) and positive if f(S) > f(Sj). Namely, ~wij = ~wf and θij = 1 2 [~wf · χ(Si) + ~wf · χ(Sj)].\nTherefore, we attempt to learn ~wij , θij for every landmark pair Si and Sj such that f(Si) ≤ f(Sj). From pairwise comparison queries alone, we cannot know if f(Si) and f(Sj) are α(n) apart, which would mean that (wij , θij) has no training error, so we must attempt to solve for such a linear separator for all landmark pairs. Using a set R, we save the indices of the landmark pairs whose corresponding linear separator has no training error. Crucially, we guarantee that it is highly unlikely, on a random query (S, S′), that f(S) and f(S′) are α(n) apart, and yet there does not exist a pair of landmarks Si and Sj such that (1) f(Si) and f(Sj) fall in between f(S) and f(S\n′) and (2) f(Si) and f(Sj) are also separated by an α(n) factor. This guarantee is illustrated by Figure 1c. If such a pair of landmarks Si and Sj do exist, we can be confident that we solve for a linear separator that correctly classifies S and S′.\nUltimately, on input (S, S′), the learned comparator will search for a pair (i, j) in R such that the corresponding linear separator (~wij , θij) classifies S as positive and S\n′ as negative, or vice versa, in which case f(S) < f(S′) or the opposite, respectively. However, we cannot guarantee that this will work as planned just yet. After all, suppose that on a random query (S, S′), f(S) and f(S′) are α(n) apart but there is some pair of landmarks Si and Sj such that f(S) and f(S\n′) fall in between f(Si) and f(Sj), instead of the other way around. This event is illustrated in Figure 1b. In this situation, we can guarantee nothing about how S and S′ will be classified. To avoid this case, we remove all but the minimal pairs in R. In other words, if there exists (i, j) and (i′, j′) in R such that f(Si) ≤ f(Si′) < f(Sj′) ≤ f(Sj), then we remove (i, j) from R. Therefore, so long as there do exist two landmarks falling between f(S) and f(S′) that are α(n) apart, we can be guaranteed that any bad landmark pair (Si, Sj), as illustrated by Figure 1b, is removed, and the learned comparator never considers (~wij , θij) when making a prediction.\nNow we state the algorithm formally, and in Corollary 1, show that it can be instantiated for many different combinatorial function classes, achieving polynomial sample complexity as well as efficient runtime.\nAlgorithm 1: Algorithm for comparator-learning combinatorial functions.\nInput: Sample S ∼ D of size Õ( n 3 ), pairwise comparison oracle Output: Function g : 2[n] × 2[n] → {0, 1} 1. Remove m = 2 log(\n1 δ ) samples uniformly at random from S. Label this set S1,\nand label S \\ S1 as S2. 2. Sort S1 into f(S1) ≤ · · · ≤ f(Sm). 3. Sort S2 into the sets Sij = {S | S ∈ S2 and f(S) ≤ f(Si) or f(S) ≥ f(Sj)} for all Si, Sj ∈ S1 such that i < j. 4. For each Si, Sj ∈ S1 (wlog i < j), attempt to find θij ∈ R and ~wij ∈ Rn such that for all S ∈ Sij , f(S) < f(Si) =⇒ ~wij · χ(S) < θij and f(Sj) < f(S) =⇒ ~wij · χ(S) > θij . 5. If the previous step is successful, put (i, j) into R. 6. Remove all but the “minimal” pairs in R: if there exists (i, j), (i′, j′) ∈ R such that i ≤ i′ and j ≥ j′, remove (i, j) from R. 7. Define the function g(S, S′) as follows. Return 1 if ∃(i, j) ∈ R such that ~wij · χ(S) < θij < ~wij · χ(S′). Otherwise, return 0.\nTheorem 1. Let F be an α(n)-approximable class. Then F is comparator-learnable with separation α(n), using Algorithm 1.\nProof. First, we show there exists a ~w ∈ Rn such that if α(n)f(S) < f(S′), then ~w ·χ(S) < ~w ·χ(S′). Since f is from an α(n)-approximable class, we know there exists f̂(S) = (~wf · χ(S))p such that\nf(S) ≤ f̂(S) ≤ α(n)f(S) for all S. This implies that if α(n)f(S) < f(S′), then f̂(S) < f̂(S′), which in turn implies that (~wf · χ(S))p < (~wf · χ(S′))p. Finally, this means that ~wf · χ(S) < ~wf · χ(S′).\nNow we prove that the learned comparator has low error by splitting the analysis into two parts. First, we show that on a random pair S, S′, it is unlikely that f(S) and f(S′) are an α(n) factor apart and yet there is no landmark pair Si, Sj ∈ S1 such that (1) f(Si) and f(Sj) fall in between f(S) and f(S′) (i.e. f(S) < f(Si) < f(Sj) < f(S\n′)) and (2) f(Si) and f(Sj) are separated by at least an α(n) factor. This is exactly what we need, because if such a pair Si, Sj does exist, then during Step 4 of Algorithm 1, we will have solved for a linear separator (~wij , θij) that will label S as negative and S′ as positive, with high probability. We prove this formally in Claim 1.\nThe only case where one of the linear separators (~wij , θij) would fail to label S or S ′ correctly is if S or S′ contribute to the learning error inherent to learning linear separators. To handle this case, we show that on a random pair S, S′ ∼ D, the probability that f(S) and f(S′) are at least an α(n) factor apart and yet some linear separator learned during Step 4 mislabels S or S′ is low. We prove this in Claim 2.\nWe combine Claim 1 and Claim 2 to prove the correctness of Algorithm 1 in the following way. We claim that with probability at least 1 − δ, on a random pair S, S′ ∼ D, the probability that α(n)f(S) < f(S′) yet the learned comparator g predicts that f(S′) < f(S) is low. This will happen whenever there exists a pair (i, j) ∈ R such that ~wij ·χ(S) > θij and ~wij ·χ(S′) < θij . In particular, we want to bound\nPr S,S′∼D\n[α(n)f(S) < f(S′) and ∃(i, j) ∈ R such that ~wij · χ(S) > θij and ~wij · χ(S′) < θij ].\nTo analyze this probability, we partition the pairs (i, j) ∈ R into two sets:\nR1 = {(i, j) | f(Si) < f(S) and f(S′) < f(Sj)} and R2 = {(i, j) | f(S) ≤ f(Si) or f(Sj) ≤ f(S′)}.\nClearly, the probability that g predicts incorrectly on S, S′ ∼ D due to a pair (i, j) ∈ R is simply the probability that g predicts incorrectly due to a pair (i, j) ∈ R1 or a pair (i, j) ∈ R2. With this in mind, we first analyze\nPr S,S′∼D\n[α(n)f(S) < f(S′) and ∃(i, j) ∈ R1 such that ~wij · χ(S) > θij and ~wij · χ(S′) < θij ].\nRecall that in Step 6 of Algorithm 1, all non-minimal pairs were from R. This means that if (i, j) ∈ R1, then it must be minimal, so there must not exist Si′ , Sj′ ∈ S1 such that α(n)f(S) < α(n)f(Si′) < f(Sj′) < f(S). After all, if such a pair Si′ , Sj′ did exist, then we would have obtained the linear separator (~wi′j′ , θi′j′) in Step 4, and (i, j) would have no longer been minimal. Therefore, the probability that g predicts incorrectly due to a pair (i, j) ∈ R1 is simply\nPr S,S′∼D\n[ α(n)f(S) < f(S′) and 6 ∃Si, Sj ∈ S1 : α(n)f(S) ≤ α(n)f(Si) < f(Sj) ≤ f(S′) ] .\nThis is exactly the probability we bound in Claim 1, which means that if we set ′ = 2 and δ ′ = δ2 , then with probability at most δ2 ,\nPr S,S′∼D\n[α(n)f(S) < f(S′) and ∃(i, j) ∈ R1 s.t. ~wij · χ(S) > θij and ~wij · χ(S′) < θij ] >\n2 .\nMeanwhile, whenever g predicts incorrectly due to a pair (i, j) ∈ R2, it means that f(S) ≤ f(Si) and ~wij · χ(S) > θij or f(Sj) ≤ f(S′) and ~wij · χ(S′) < θij . In other words, S or S′ contributes to\nthe learning error of (~wij , θij). This is the probability we bound in Claim 2, which means that if we set ′ = 2 and δ ′ = δ2 , we have that with probability at most δ 2 ,\nPr S,S′∼D\n[α(n)f(S) < f(S′) and ∃(i, j) ∈ R2 s.t. ~wij · χ(S) > θij and ~wij · χ(S′) < θij ] >\n2 .\nPutting these bounds together, we have that with probability at most δ,\nPr S,S′∼D\n[α(n)f(S) < f(S′) and ∃(i, j) ∈ R such that ~wij · χ(S) > θij and ~wij · χ(S′) < θij ] > .\nTherefore, with probability at least 1 − δ, the probability that f(S)α(n) < f(S′) and g predicts incorrectly is at most .\nNow we prove the first claim, which guarantees that with probability at least 1− δ, at most an density of pairs S, S′ ∼ D have the property that α(n)f(S) < f(S′) and yet there does not exist a landmark pair Si, Sj ∈ S1 such that f(Si) and f(Sj) are separated by a α(n) multiplicative factor and f(Si) and f(Sj) sit between f(S) and f(S ′).\nClaim 1. A sample size m = O ( 1 log\n1 δ\n) is sufficient so that with probability at least 1 − δ,\nS1 = {S1, . . . , Sm} has the property that\nPr S,S′∼D\n[ α(n)f(S) < f(S′) and 6 ∃Si, Sj ∈ S1 : α(n)f(S) ≤ α(n)f(Si) < f(Sj) ≤ f(S′) ] ≤ .\nProof. First, note that the distribution D induces a distribution over values f(S) ∈ R≥0. Partition R≥0 into 1/ buckets of probability mass . If m = O ( 1 log 1 δ ) , then with probability at least 1− δ, our sample S1 = {S1, S2, . . . , Sm} contains at least one set from every bucket. Let us assume below this is indeed the case.\nNow, define two buckets bi and bj to be close if all points in bi are within an α(n) factor of all points in bj . Define the buckets to be far if all points in bi have a gap of size greater than α(n) with all points in bj . Otherwise (some points in bi are close to some points in bj and some are far), say that the two buckets are in conflict. Say that S is in bucket bi′ and S\n′ is in bucket bj′ (WLOG, i′ ≤ j′). If bi′+1 and bj′−1 are far, then we have the desired Si and Sj (from buckets bi′+1 and bj′−1). Also, if bi′ and bj′ are close buckets, then we are fine (the bad event in the statement does not occur) since S and S′ must have values that do not differ by a α(n) factor. The final case is if bi′+1 is not far from bj′−1 and bi′ is not close to bj′ . This is captured by the case where: bi′+1 is in conflict with bj′−1, bi′+1 is in conflict with bj′ , bi′ is in conflict with bj′−1, or bi′ is in conflict with bj′ .\nNow we show the fraction of pairs of buckets in conflict is low. If, say, bi′ and bj′ are in conflict, then bi′−1 and bj′+1 must be far and, assuming i < j−1, bi′+1 and bj′−1 must be close. This implies there are at most 1/ pairs of buckets in conflict because for every bucket bi, there is at most one bucket bj it can be in conflict with. Since there are at most 1/ pairs of buckets in conflict but 1/ 2 pairs of buckets overall, this implies the probability that two buckets are in conflict is O( ).\nWe now provide a proof sketch of Claim 2. The full proof can be found in Appendix B.\nClaim 2. Let P1 = PrS∼D[∃(i, j) ∈ R such that f(S) ≤ f(Si) yet ~wij · χ(S) > θij ] and P2 = PrS∼D[∃(i, j) ∈ R such that f(S) ≥ f(Sj) yet ~wij · χ(S) < θij ]. A sample of size\n|S2| = O ( m2 [ n log m2 + log 1\nδ ]) is sufficient so that with probability at least 1− δ, P1 + P2 < .\nProof sketch. For each (i, j), we define a class of loss functions Lij = {L(~wij ,θij) | ~wij ∈ Rn, θij ∈ R}, where for any (~wij , θij) and any S ⊆ [n], L(~wij ,θij)(S) is equal to 1 if f(S) ≤ f(Si) and ~wij ·χ(S) > θij or if f(S) ≥ f(Sj) and ~wij · χ(S) < θ, and 0 otherwise. It is straightforward to show the VC dimension of Lij is the same as the VC dimension of the class of linear separators over Rn, which is n + 1. We know that for any pair Si and Sj such that α(n)f(Si) ≤ f(Sj), the empirical risk minimizer of Lij will have zero loss over S2. Therefore, by standard VC dimension bounds, if |S2| = O(m 2 [n log m2 + log 1 δ ]), then with probability at least 1 − δ, the error of h\n∗ over D is at most\nm2 . Now we union bound over all m2 pairs (i, j) on which Algorithm 1 attempts to solve for\na linear threshold (~wij , θij) and thereby achieve an overall error of .\nThese claims complete the proof of Theorem 1. By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).\nCorollary 1. The following statements are true: 1. The class of monotone submodular functions is efficiently comparator-learnable with separa-\ntion √ n.\n2. The class of XOS functions is efficiently comparator-learnable with separation O( √ n).\n3. The class of monotone subadditive functions is efficiently comparator-learnable with separation√ n log n.\n4. The class of submodular functions with curvature κ is efficiently comparator-learnable with separation min {√\nn, 11−κ\n} .\n5. For any ξ > 0, the class of XOS functions with R SUM trees is comparator-learnable with separation Rξ. In this case, the sample complexity and running time of Algorithm 1 are both\npolynomial in n 1 ξ ."
    }, {
      "heading" : "3.1 Lower Bounds",
      "text" : "Now, we show that the class of non-negative, monotone, submodular functions is not comparatorlearnable with separation o(n1/3/ log n). This lower bound nearly matches our upper bound from Corollary 1.1. To prove this result, we use a special family of matroid rank functions, which form a subset of the class of monotone submodular functions, presented in [3] and described as follows.\nTheorem 2 (Theorem 7 in [3]). For any k ≥ 8 with k = 2o(n1/3), there exists a family of sets A ⊆ 2[n] and a family of matroids M = {MB : B ⊆ A} with the following properties. • |A| = k and for each A ∈ A, |A| = n1/3. • For each B ⊆ A and A ∈ A,\nrankMB(A) = { 8 log k if A ∈ B |A| otherwise.\nWe now formally present our lower bound.\nTheorem 3. Let ALG be an arbitrary learning algorithm that uses only a polynomial number of training samples drawn i.i.d. from the underlying distribution and produces a predictor g. There exists a distribution D and a submodular target function f∗ such that, with probability at least 1/25 (over the draw of the training samples),\nPr S,S′∼D\n[ α(n)f(S) ≤ f(S′) and g predicts that f(S) > f(S′) ] ≥ 1\n25 ,\nwhere α(n) = Ω(n1/3/ log n).\nProof. As in [3], we use the family of matroids presented in Theorem 2 to show that for a superpolynomial sized set of k points in {0, 1}n, and for any partition of those points into High and Low, we can construct a matroid where the points labeled High have rank rhigh and the points labeled Low have rank rlow, and that rhigh/rlow = Ω̃(n\n1/3). We prove that this implies hardness for comparator-learning over the uniform distribution on these k points from any polynomial-sized sample.\nFormally, we use the probabilistic method to prove the existence of the f∗ referred to in the theorem statement. To this end, suppose that ALG uses ` ≤ nc training examples for some constant c. To construct a hard family of submodular functions, we apply Theorem 2 with k = 2t where t = c log n + 3. Let A and M be the families that are guaranteed to exist, and let rankMB be the rank function of a matroid MB ∈ M. Let the underlying distribution D on 2[n] be the uniform distribution on A.\nAssume that ALG uses a set S of ` training examples. For any S, S′ ∈ A \\ S such that S 6= S′, we claim that the algorithm ALG has no information about how f∗(S) compares to f∗(S′), for any target function f∗ = rankMB where MB ∈M. After all, by Theorem 2, for a fixed labeling of S by the values rlow and rhigh, there exist exactly 2\n|A\\S| matroids inM whose respective rank functions label S according to this fixed labeling. Moreover, for any partition of the points in A \\ S into High and Low, there exists exactly one matroid among those 2|A\\S| matroids such that the points labeled High have rank rhigh and the points labeled Low have rank rlow. In other words, if MB is drawn uniformly at random from M and f∗ = rankMB , then the conditional distribution of f∗(S) given S is uniform in {rlow, rhigh}.\nThe set of non-training examples has measure 1−2−t+log `, and as we have seen, in expectation, half of the non-training sets will have rank rhigh and half will have rank rlow. Therefore,\nEf∗,S\n[ Pr\nS,S′∼D\n[ S, S′ 6∈ S, f∗(S) = rlow, and f∗(S′) = rhigh ]] = ( 1− 2−t+log `\n2\n)2 .\nMoreover, for S, S′ 6∈ S, due to the uniform conditional distribution of f∗(S) and f∗(S′) given S, ALG cannot determine whether f∗(S) ≥ f∗(S′) or vice versa better than randomly guessing between the two alternatives. Therefore\nEf∗,S\n[ Pr\nS,S′∼D\n[ α(n)f∗(S) ≤ f∗(S′) and g predicts that f∗(S) > f∗(S′) ]] ≥ Ef∗,S [ Pr\nS,S′∼D\n[ S, S′ 6∈ S, f∗(S) = rlow, f∗(S′) = rhigh, and g predicts that f∗(S) > f∗(S′) ]] ≥ 1\n2\n( 1− 2−t+log `\n2 )2 ≥ 49\n512 .\nTherefore, there exists a rank function f∗ such that\nES\n[ Pr\nS,S′∼D\n[ α(n)f∗(S) ≤ f∗(S′) and g predicts that f∗(S) > f∗(S′) ]] ≥ 49\n512 .\nWe claim that this means that for this fixed f∗,\nPr S\n[ Pr\nS,S′∼D\n[ α(n)f(S) ≤ f(S′) and g predicts that f(S) > f(S′) ] ≥ 1\n25\n] ≥ 1\n25 .\nAfter all, suppose not, so\nPr S\n[ Pr\nS,S′∼D\n[ α(n)f(S) ≤ f(S′) and g predicts that f(S) > f(S′) ] ≥ 1\n25\n] < 1\n25 .\nThen setting X := PrS,S′∼D [α(n)f(S) ≤ f(S′) and g predicts that f(S) > f(S′)], we have that\nES [X] ≤ 1 · Pr [ X ≥ 1\n25\n] + 1\n25 · Pr\n[ X < 1\n25\n] < 1 · 1\n25 +\n1 25 · 1 = 2 25 < 49 512 .\nOf course, this is a contradiction, so the theorem statement holds.\nBy a similar argument, we show that the class of XOS functions is not comparator-learnable with separation o( √ n/ log n). The proof of Theorem 4 makes use of a special family of XOS functions\npresented in [2] and follows the same logic as in the proof of Theorem 3. It can be found in Appendix B.\nTheorem 4. Let ALG be an arbitrary learning algorithm that uses only a polynomial number of training samples drawn i.i.d. from the underlying distribution and produces a predictor g. There exists a distribution D and an XOS target function f∗ such that, with probability at least 1/25 (over the draw of the training samples),\nPr S,S′∼D\n[ α(n)f(S) ≤ f(S′) and g predicts that f(S) > f(S′) ] ≥ 1\n25 ,\nwhere α(n) = Ω( √ n/ log n).\nWe also show a lower bound parameterized by the curvature of a submodular function.\nTheorem 5. Let ALG be an arbitrary learning algorithm that uses only a polynomial number of training samples drawn i.i.d. from the underlying distribution and produces a predictor g. There exists a distribution D and a submodular target function f∗ with curvature κ (possibly known to the algorithm) such that, with probability at least 1/25 (over the draw of the training samples),\nPr S,S′∼D\n[ α(n)f(S) ≤ f(S′) and g predicts that f(S) > f(S′) ] ≥ 1\n25 ,\nwhere α(n) = n 1/3\nO(κ logn)+(1−κ)n1/3 .\nProof. Given a submodular function f with curvature 1, we may convert it to a submodular function with curvature κ by setting fκ(X) = κf(X) + (1− κ)|X|. This idea allows us to easily modify the proof of Theorem 3.\nSuppose that ALG uses ` ≤ nc training examples for some constant c. We apply Theorem 2 with k = 2t where t = c log n+3. LetA andM be the families that are guaranteed to exist. Again, rankMB denotes the rank function of a matroid MB ∈ M. Now we define rankκMB(A) = κ · rankMB(A) + (1−κ)|A|. Let the underlying distribution D on 2[n] be the uniform distribution on A. Then for all B ⊆ A and A ∈ A, if A ∈ B, then rankκMB(A) = 8κ log k + (1− κ)|A| = 8κ log k + (1− κ)n 1/3, and if A /∈ B, then rankκMB(A) = κn 1/3 + (1− κ)|A| = n1/3. Therefore, rhigh/rlow = n 1/3 8κ log k+(1−κ)n1/3 .\nAs in the previous proof, ALG has no information about pairs of sets which are not in the training set. The rest of the proof is similar to the proof of Theorem 3."
    }, {
      "heading" : "4 Additive Separation Analysis",
      "text" : "Let f be a monotone submodular function with range in [0, 1] and fix D to be the uniform distribution over the n-dimensional boolean cube. A slight variant on Algorithm 1 allows us to learn a comparator g which, on input S, S′ ⊆ [n], returns whether f(S) is greater than f(S′) or vice versa whenever f(S) and f(S′) differ by a sufficiently large additive factor β, rather than a multiplicative factor as in Section 3. In this case, we say that f is comparator-learnable with additive separation β.\nThis result relies on key insights into the Fourier spectrum of submodular functions. In particular, we use the fact that any monotone submodular function f with range in [0, 1] is γ-close to a polynomial p of degree O (\n1 γ4/5 log 1γ\n) in the `2 norm, i.e. √ E[(f(x)− p(x))2] < γ [11]. Specifi-\ncally, p is a truncation of the Fourier expansion of f , consisting only of terms with degree at most O (\n1 γ4/5 log 1γ\n) . The polynomial p can easily be extended to a linear function hf in n O ( 1 γ4/5 log 1 γ ) -\ndimensional space that closely approximates f in the `2 norm. With this result in hand, we are in a similar position as we were in Section 3, although we\nnow have a means of additively approximating a submodular function, rather than multiplicatively. However, the analysis from Section 3 does not carry over directly. Recall that in that section, we knew that there existed a p-th power of a linear function that approximated the underlying function everywhere. Now, due to the probabilistic nature of the `2 norm, we can only say that in expectation, for a set S drawn at random from D, hf (S) will be close to f(S).\nWe address this subtlety in the design of our additive separation algorithm, Algorithm 2, in the following way. First, we sample the sets S1 and S2 as before, and sort them according to the underlying submodular function f . Again, S1 will serve as our landmarks; the size of S1 is large enough so that we can be ensured that if S and S′ are drawn uniformly at random, it is unlikely that f(S) and f(S′) are at least β apart additively, and yet there is no pair of landmarks Si and Sj such that (1) f(Si) and f(Sj) fall between f(S) and f(S\n′) and (2) f(Si) and f(Sj) are also separated by a β additive factor.\nNext, we solve for the suite of linear separators which allow the output comparator to make pre-\ndictions. Recall that the linear function hf which approximates f is in n O\n( 1\nγ4/5 log 1 γ ) -dimensional\nspace, so rather than mapping each set S to the characteristic vector χ(S) in order to solve for a lin-\near separator, we must map it to n O\n( 1\nγ4/5 log 1 γ ) -dimensional space. This mapping is straightforward\nand is described later on. The way in which we learn the linear separators in Algorithm 2 is the main difference between\nthis algorithm and Algorithm 1. This is because when designing Algorithm 1, we knew that the approximating function approximated f everywhere, whereas hf only approximates f in the probabilistic `2 norm. Therefore, it may be the case that two landmarks Si and Sj are separated by a β additive factor, yet for some S such that f(S) < f(Si), it happens that hf (S) > hf (Si), for example. In other words, there may be noise in the training set S2.\nTherefore, when learning the suite of linear separators, we only save the indices of any landmark pair whose corresponding linear separator has low error rate over S2. We ensure that S2 is large enough so that it is unlikely that for any two landmarks Si and Sj , f(Si) and f(Sj) are separated by a β additive factor and yet the empirical risk minimizing linear separator that classifies S as negative if f(S) < f(Si) and positive if f(S) > f(Sj) has high error over S2. Moreover, we ensure that S2 is large enough so that it is unlikely that we learn a linear separator that has a much lower error rate over S2 than over the entire distribution. We can conclude that the linear separators we\nkeep track of will have low total error over the distribution. Finally, as in Algorithm 1, we keep track of only the “minimal” linear separators: we never keep\ntrack of a linear separator corresponding to a pair of landmarks Si and Sj if we are also keeping track of a linear separator corresponding to a pair Si′ and Sj′ such that f(Si′) and f(Sj′) fall in between f(Si) and f(Sj).\nThe output comparator predicts on input S and S′ as in Algorithm 1. It searches for a linear separator (~pij , θij) among the remaining minimal linear separators such that S is classified as negative and S′ is classified as positive. If it finds one, it outputs 1 (f(S) < f(S′)). Otherwise, it outputs 0 (f(S′) < f(S)).\nWe now present our guarantees on the performance of Algorithm 2.\nAlgorithm 2: Algorithm for learning submodular functions up to pairwise comparisons with an additive factor difference.\nInput: Parameters , δ, β ∈ (0, 1) and `, a sample S from the uniform distribution over 2[n] of size `, and a pairwise comparison oracle Output: Function g : 2[n] × 2[n] → {0, 1}\n1. Set γ = β (\n1 + 2 log 1 δ\n√ 2 )−1 , k = 25\nγ4/5 log 3√2 γ .\n2. Remove m = 2 log( 1 δ ) samples uniformly at random from S. Label this set S1, and\nlabel S \\ S1 as S2. 3. Sort S1 such that f(S1) ≤ · · · ≤ f(Sm) and sort S2 into the sets Sij = {S | S ∈ S2 and f(S) ≤ f(Si) or f(S) ≥ f(Sj)} for all Si, Sj ∈ S1 such that i < j. 4. Let S1, . . . , Snk be an arbitrary ordering of the subsets of [n] of size at most k, and for\nS ⊆ [n], define v(S) ∈ Rnk such that the ith component of v(S) equals χSi(S). 5. For each Si, Sj ∈ S1, find θij ∈ R and ~pij ∈ Rn that minimizes the number of sets S ∈ Sij , such that f(S) < f(Si) and ~pij · v(S) > θij or f(Sj) < f(S) and ~pij · v(S) < θij . If the fraction of such sets over S2 is at most 4m2 , put (i, j) into R. 6. Remove all but the “minimal” pairs in R: if there exists (i, j), (i′, j′) ∈ R such that i ≤ i′ and j ≥ j′, remove (i, j) from R. 7. Define the function g(S, S′) as follows. Return 1 if ∃(i, j) ∈ R such that ~pij · v(S) < θij < ~pij · v(S′). Otherwise, return 0.\nTheorem 6. Let F be the class of monotone submodular functions with range in [0, 1]. For any β ∈ (0, 1), accuracy parameter , and confidence parameter δ, F is comparator learnable with\nseparation β given a sample of size Õ ( n O ( 1 γ4/5 log 1 γ ) / 3 ) , where γ = Õ ( β/ 3/2 ) .\nWe note that Algorithm 2 is efficient given access to an ERM oracle for agnostically learning linear separators. Moreover, even in the model where the learning algorithm has access to realvalued function labels, for a monotone submodular function f with range in [0, 1], the best known results for learning a function h such that ||f − h||2 ≤ require running time 2Õ(1/ 4/5) · n2 and 2Õ(1/ 4/5) log n random examples [11].\nFor ease of notation, we set k = 25 γ4/5 log 3√2 γ for the remainder of this section, where the constants come from the analysis in [11]. Let f be a monotone submodular function with range in [0, 1]. As we alluded to in the intro-\nduction of this section, we exploit existence of a polynomial p of degree O (\n1 γ4/5 log 1γ\n) that closely\napproximates f in the `2 norm to show that there exists a vector ~p in n k-dimensional space such that for S, S′ ∼ D, with probability at least 1− δ, f(S) +β < f(S′) and ~p · v(S) < ~p · v(S′). Here, v is a mapping from n-dimensional space to nk-dimensional space, which we describe in the following analysis. Once we know that this vector ~p exists, we can attempt to solve for a set of the linear threshold functions ~pij , θij as in Algorithm 1, which will allow us to define the output predictor g. To this end, we now show that such a vector ~p in nk-dimensional space does exist.\nTheorem 7. Let f : 2[n] → [0, 1] be a monotone submodular function, D be the uniform distribution over 2[n], and γ ∈ (0, 1). There exists a vector ~p in nk-dimensional space and a mapping v from 2[n] to 2[n k] such that √ ES∼D[(f(S)− ~p · v(S))2] ≤ γ.\nProof. [11] proved that for γ ∈ (0, 1), if we set κ = 1 γ4/5 log 1γ , then there exists L ⊆ [n], |L| ≤ 24 γ4/5 log 3√2 γ , such that if p(T ) = ∑ S:|S\\L|≤κ f̂(S)χS(T ), then ||f − p||2 = √ ES∼D[(f(S)− p(S))2] < γ. Here, χS(T ) = (−1)|T∩S| and f̂(S) is the Fourier coefficient of f on S. Unfortunately, we cannot find L without knowing the value f on our samples. However, we can\nsimply extend the polynomial p to include all summands from the Fourier expansion of f up to sets of size k ≥ |L|+ κ and thus obtain an even better approximation to f , a polynomial which we call p0. Next, we can easily write p0 as a linear mapping from n\nk-dimensional space to R as follows. Let S1, . . . , S` be an ordering of the sets S ⊆ [n] such that |S| ≤ k. Next, let\n~p = ( f̂ (S1) , . . . , f̂ (S`) ) and v(S) = (χS1(S) . . . , χS`(S)) (1)\nfor all S ⊆ [n]. Then p0(S) = ~p · v(S).\nWe are now in a similar situation as we were in Section 3 when we were analyzing the case with a multiplicative approximation factor. In particular, we knew that so long as α(n)f(S) < f(S′), then there had to exist a weight vector ~w such that ~w · χ(S) < ~w · χ(S′). Now, we know that there is some probability that f(S) + β < f(S′) and ~p · v(S) < ~p · v(S′). In the following lemmas, we derive a lower bound on that probability, which in turn allows us to provide strong guarantees on the performance of Algorithm 2.\nLemma 1 is an immediate consequence of Parseval’s identity and Chebychev’s inequality and the proof of Lemma 2 can be found in Appendix C.\nLemma 1. Given γ, ξ ∈ (0, 1), let ~p and v be defined by Equation 1. Then PrS∼D [ |f(S)− ~p · v(S)| > γ ( 1 + √ 1/ξ )] < ξ.\nLemma 2. Given γ, ξ ∈ (0, 1), let ~p and v be defined by Equation 1. Then PrS1,S2∼D [ f(S1) + 2γ ( 1 + √ 2/ξ ) < f(S2) and ~p · v(S1) ≤ ~p · v(S2) ] > 1− ξ.\nWe are now ready to prove the correctness of Algorithm 2.\nTheorem 6 proof sketch. In keeping with the outline of the proof of Theorem 1, we prove Claim 3, a parallel to Claim 1, and Claim 4, a parallel to Claim 2. We then combine Claim 3 and Claim 4 as we combined Claim 1 and Claim 2 to prove Theorem 6.\nFor the most part, Claim 3 follows from Claim 1. The proof of the latter is not specific to a multiplicative factor difference; it can easily be extended to an additive factor difference. In particular, it follows that if S1, our set of “landmarks” which discretize the range of f , is sufficiently large, then on a random draw of S and S′, it is unlikely that f(S) and f(S′) are separated by an additive β factor and yet there does not exist two landmarks Si and Sj such that (1) f(Si) and\nf(Sj) fall in between f(S) and f(S ′) and (2) f(Si) and f(Sj) are separated by an additive factor of β. However, this does not mean that a pair (i, j) such that f(Si) + β < f(Sj) will necessarily be added to R. This again highlights the difference between this analysis and the analysis in Section 3. In that section, we were guaranteed that if α(n)f(Si) < f(Sj), then there had to exist a linear threshold function that will label S as negative if f(S) < f(Si) and positive if f(S) > f(Sj). Now, we can only make a probabilistic argument about the likelihood that f(Si) + β < f(Sj) and (i, j) is added to R. In particular, in Claim 3, we show that if |S1| and |S2| are sufficiently large, then on a random draw of S and S′, it is unlikely that f(S) and f(S′) are separated by an additive β factor and yet there does not exist two landmarks Si and Sj such that (1) f(Si) and f(Sj) fall in between f(S) and f(S′) and (2) the corresponding linear separator has small training error.\nNext, we show in Claim 4 that the probability that there exists a linear separator with much lower empirical error than true error is small. Since we only save linear separators that have small empirical error, this means that the true error will be small as well. In other words, for any linear separator corresponding to two landmarks Si and Sj that we save, the probability that f(S) > f(Sj) yet the linear separator classifies S as negative is small, and the probability that f(S) < f(Si) yet the linear separator classifies S as positive is small.\nFinally, we rely on both Claim 3 and Claim 4 to show that on a random draw of S, S′ ∼ D, it is unlikely that f(S) and f(S′) are separated by an additive β factor and yet the algorithm predicts incorrectly. The formal way in which we combine Claim 3 and Claim 4 to prove Theorem 6 is similar to the proof of Theorem 1, and can be found in Appendix C.\nNow, we provide the formal statement and proof of Claim 3. The proof relies on the Chernoff bounding technique to show that for two landmarks Si and Sj , it is unlikely that that f(Si) and f(Sj) are at least β apart and yet the empirical risk minimizing linear separator has high empirical error over S2.\nClaim 3. Sample sizes |S1| = O ( 1 log 1 δ ) and |S2| = O ( m2 log m2 ) are sufficient so that with probability ≥ 1− δ, PrS,S′∼D [f(S ′) > f(S) + β and 6 ∃(i, j) ∈ R : f(S′) ≥ f(Sj) > f(Si) + β ≥ f(S) + β] ≤ .\nProof. First, we bound the probability that for a fixed Si, Sj ∈ S1, f(Si)+β < f(Sj), yet (i, j) 6∈ R. Recall that the pair (i, j) is added to R in Step 5 of Algorithm 2 if, for the empirical risk minimizing threshold function ERMij(S2) = (~p∗ij , θ∗ij), the fraction of sets S ∈ S2 such that f(S) < f(Si) and ~p∗ij · v(S) > θ∗ij or f(Sj) < f(S) and ~p∗ij · v(S) < θ∗ij is at most 8m2 .\nIn other words, (~p∗ij , θ ∗ ij) is the linear threshold function which minimizes the loss function\nL(~pij ,θij)(S) =  1 if f(S) ≤ f(Si) and ~pij · v(S) > θij\nor f(S) ≥ f(Sj) and ~pij · v(S) < θij 0 otherwise .\nLet |S2| = m′. For a given (~pij , θij), let\nL(~pij ,θij)(S2) = 1\nm′ ∑ S∈S2 L(~pij ,θij)(S)\nbe the empirical loss of (~pij , θij) over S2. We claim that a sample size of m′ = O ( m2 log m2 ) is sufficient to ensure that PrS2∼Dm′ [L(~p∗ij ,θ∗ij)(S2) > 8m2 ] < 2m2 . We prove this using Chernoff’s bounding technique.\nFirst, notice that\nES∼D[L(~pij ,θij)(S)] = Pr[f(S) ≤ f(Si) and ~pij · v(S) > θij ] + Pr[f(Sj) ≤ f(S) and ~pij · v(S) < θij ].\nWe will see that it is enough to find an upper bound on ES∼D[L(~p,θ̃ij)(S)], where θ̃ij = f(Si)+f(Sj) 2 and ~p is defined by Equation 1. We begin by finding an upper bound on Pr[f(Sj) ≤ f(S) and ~p · v(S) < θ̃ij ]. Notice that θ̃ij = f(Si)+f(Sj) 2 < f(Sj)− β 2 since f(Sj) > f(Si) + β. Therefore,\nPr[f(Sj) ≤ f(S) and ~p · v(S) < θ̃ij ] < Pr [ f(Sj) ≤ f(S) and ~p · v(S) ≤ f(Sj)− β\n2\n] .\nHowever, so long as |f(S)− ~p ·v(S)| < β2 , we know that f(Sj)− β 2 ≤ f(S)− β 2 < ~p ·v(S). Therefore, the only way that ~p · v(S) ≤ f(Sj) − β2 is if |f(S) − ~p · v(S)| ≥ β 2 , which we know from Lemma 1 happens with probability at most 8m2 since\nβ 2 = γ 2\n( 1 + 2 log 1\nδ\n√ 2 ) = γ\n2\n( 1 + √ 8m2 ) .\nTo apply Lemma 1, we simply use γ2 instead of γ and 8m2 instead of δ. Therefore,\nPr[f(Sj) ≤ f(S) and ~p · v(S) < θ̃ij ] <\n8m2 .\nBy a symmetric argument,\nPr[f(S) ≤ f(Si) and ~p · v(S) > θ̃ij ] <\n8m2\nas well. This means that ES∼D[L(~p,θ̃ij)(S)] < 4m2 .\nUsing Chernoff’s bounding technique, we have that\nPr S2∼Dm′\n[ L(~p,θ̃ij)(S2) >\n4m2\n] <\n2m2\nfor m′ = O ( m2 log m2 ) . Since it will always be the case that LERMij(S2)(S2) < L(~p,θ̃ij)(S2), we have that Pr\nS2∼Dm′\n[ LERMij(S2)(S2) >\n4m2\n] <\n2m2\nas well. By a union bound over all m2 (i, j) pairs in S1, we have that for S, S′ ∼ D, the probability that f(S′) > f(S) + β and there exists Si, Sj ∈ S1 such that f(S′) ≥ f(Sj) > f(Si) + β ≥ f(S) + β yet (i, j) 6∈ R is at most 2 .\nFinally, by the same argument as in the proof of Claim 3, a sample S1 of size |S1| = O ( 1 log 1 δ ) is sufficient the guarantee that with probability at least 1−δ, the probability that f(S′) > f(S)+β and there does not exist Si, Sj ∈ S1 such that f(S′) ≥ f(Sj) > f(Si) + β ≥ f(S) + β is at most . Putting these two arguments together, we have that with probability at least 1− δ,\nPr S,S′∼D\n[ f(S′) > f(S) + β and 6 ∃(i, j) ∈ R : f(S′) ≥ f(Sj) > f(Si) + β ≥ f(S) + β ] ≤ .\nFinally, we state Claim 4. The proof takes advantage of agnostic learning VC dimension bounds to prove that the size of S2 is sufficiently large to ensure that the true error of the learned linear separators is close to the empirical error. The full proof can be found in Appendix C.\nClaim 4. A sample size |S2| = O ( 1 3 log 1 δ [ n O ( 1 γ4/5 log 1 γ ) log ( 1 2 log 1 δ ) + log 1δ ]) is sufficient so\nthat with probability at least 1− δ,\nPr S∼D  ∃(i, j) ∈ R such that f(S) ≤ f(Si) yet ~pij · χ(S) > θij or f(S) ≥ f(Sj) yet ~pij · χ(S) < θij  < 2 .\nWe can obtain similar results for the class of XOS functions, as follows.\nCorollary 2. Let F be the class of XOS functions with range in [0, 1]. For any β ∈ (0, 1), accuracy parameter , and confidence parameter δ, F is comparator learnable with additive separation β given a sample of size Õ ( nO(1/γ)/ 3 ) , where γ = Õ ( β/ 3/2 ) .\nProof. This follows from the same line of reason as in the proof of Theorem 6 and the fact that for any XOS function f : 2[n] → [0, 1], there is a polynomial p of degree O(1/γ) such that ||f −p||2 ≤ γ [11]. In particular, p(T ) = ∑ S:|S|≤ √ 5\n(2γ)\nf̂(S)χS(T )."
    }, {
      "heading" : "5 Application to Other Combinatorial Functions",
      "text" : "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9]. We summarize the function classes we investigate in the following sections, providing a brief motivation and a description of our guarantees."
    }, {
      "heading" : "5.1 Valuation Functions with Limited Nonlinear Interactions",
      "text" : "A valuation function f is simply a set function such that f(∅) = 0. We show that if, intuitively speaking, the underlying valuation function expresses nonlinear interactions between sets of size at most k (i.e. it is a function with k-limited nonlinear interactions), then Algorithm 1 learns f up to comparisons using a sample of size Õ(nk/ 3). Notably, we do not require, on input S and S′, that f(S) and f(S′) be sufficiently far apart in order to guarantee that the learned comparator will predict correctly with high probability. This is in contrast to the previous results, where in order the guarantee that the learned comparator will predict correctly, we required that cf(S) < f(S′) for c sufficiently large.\nTo define what we mean by limited nonlinear interactions, we use the notion of an interaction function [24]. Let f : 2[n] → R be a valuation function and let g : 2[n] \\ ∅ :→ R be defined such that for all S ⊆ [n],\nf(S) = ∑\nT⊆[n]:S∩T 6=∅\ng(T ).\nThe function g is called the interaction function of f . Vainsencher et al. proved that every valuation function f has a unique interaction function g such that g(∅) = 0 [24]. We then say that f has degree k if for all T ∈ 2[n] such that |T | > k, g(T ) = 0 and we define Fk to be the set of valuation functions f of degree at most k. Intuitively, Fk contains all valuation functions that express nonlinear interactions between subsets of size at most k.\nWe note that Fk contains many natural valuation functions. A valuation function f likely falls in Fk when the n objects in the ground set either form instances of j-wise complements or j-wise substitutes, where j is at most k. For example, stamps are typically produced as members of a small collection under a unified theme, such as recently released movies or commemorations of a country’s leaders or monuments. A stamp collector will likely value a set S of stamps from the same collection more than she will value any stamp in S on its own, and thus her valuation function is supermodular. Moreover, if k is an upper bound on the size of any collection she has her eye on, then it possible to show that her valuation function will fall in Fk.\nVainsencher et al. suggested sensor placement as another natural application domain. Each problem instance consists of a set of n possible points where sensors may be placed, and the valuation function is determined by the amount of area covered by a particular selection of sensor placements. If at most k sensors cover any point in the domain, then it is easy to see that the valuation function is submodular and falls in Fk.\nWith these motivations in mind, we now summarize our guarantees in Theorem 8 regarding the performance of Algorithm 1 when we know that the underlying valuation function f is a member of Fk. The proof can be found in Section D.\nTheorem 8. For all 1 ≤ k ≤ n, the class of functions in Fk is comparator-learnable with sample complexity and runtime polynomial in nk, 1/ , and 1/δ, and with no separation factor, using Algorithm 1."
    }, {
      "heading" : "5.2 Fourier Sparse Set Functions",
      "text" : "We can extend Algorithm 1 to general combinatorial functions with Fourier support contained in a set P ⊆ 2[n]. Moreover, we achieve even better sample complexity if we are guaranteed that the size of the Fourier support is bounded by a constant k ≤ |P|. Again, we do not need to require that f(S) and f(S′) be separated by a sufficiently large multiplicative factor in order to guarantee that the learned comparator predicts correctly with high probability.\nAn important example of a function with sparse Fourier support is the cut function of a graph G = (V,E), equipped with a weight function w : E → R. This function is defined as fG(A) =∑\ns∈A,t∈V \\Aw(s, t). Stobbe and Krause show that the Fourier support of f is contained in P = {S | |S| = 2} ∪ ∅ [23]. We show that we can learn f up to comparisons using Õ ( |V |2 3 ) examples in time polynomial in |V |, 1/ , and 1/δ. Building on this, the binary function fG(A,B) = ∑ s∈A,t∈B w(s, t) can be seen as a function over {0, 1}2n with Fourier support contained in P = {S | |S| ≤ 2}. Therefore, we can learn f up to comparisons using Õ ( |V |2 3 ) examples in time polynomial in |V |, 1/ , and 1/δ. In analysis of social\nnetworks, one might wish to learn the function fG(A,B) up to comparisons as a means to order the influence of individuals and groups in that network.\nWe now present our main result for learning Fourier sparse set functions, the proof of which can be found in Appendix D.\nTheorem 9. For all k ≥ 1, the class of functions with at most k nonzero Fourier coefficients and support contained in P ⊆ 2[n] is comparator-learnable with sample complexity polynomial in k, 1/ , and 1/δ, running time polynomial in |P|Θ(k), 1/ , and 1/δ, and with no separation. Alternatively, this class is also comparator-learnable using running time and sample complexity polynomial in |P|, 1/ , and 1/δ and with no separation."
    }, {
      "heading" : "5.3 Coverage Functions",
      "text" : "Coverage functions form a subset of the class of submodular functions, and have applications in combinatorial optimization, machine learning, and algorithmic game theory. A coverage function f is defined on [n], and each element in [n] corresponds to a subset of a universe U , whose elements have nonnegative weights. The value of f on S ⊆ [n] is the weight of the union of the corresponding subsets in U .\nWe combine structural results specific to coverage functions from [1] and [9] to prove that coverage functions are comparator-learnable with multiplicative separation (1 + ), using Õ(n3/ 5) queries, given access to an ERM oracle for learning linear separators. In particular, we prove the following theorem, the proof of which can be found in Appendix D.\nTheorem 10. The class of coverage functions is comparator-learnable with multiplicative separation (1 + ) and sample complexity polynomial in n, 1/ , and 1/δ."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we explore the learning model where the goal is to learn an underlying combinatorial function up to pairwise comparisons, from pairwise comparisons. We present several general algorithms that can be used to learn over a variety of combinatorial function classes, including those consisting of submodular, XOS, subadditive, coverage, and Fourier sparse functions. We also prove nearly matching lower bounds for submodular and XOS functions, and for submodular functions with bounded curvature.\nIn particular, we show that if the functions in a class F can be approximated by the p-th root of a linear function to within a multiplicative factor of α(n), then we can learn a comparison function that for most pairs S, S′ ∼ D, either predicts correctly which of f(S) or f(S′) is larger, or f(S) and f(S′) are separated by less than a multiplicative α(n) factor. We extend this algorithm to account for an additive separation factor, rather than a multiplicative separation factor, by taking advantage of key structural properties of the Fourier spectrum of the functions we consider. In this case, we require that the underlying distribution be uniform and that the underlying function be XOS or monotone submodular with range in [0, 1]. Finally, we show that it is possible to learn over some combinatorial function classes, such as the class of Fourier sparse functions, with no separation factor. In this way, the power and adaptability of our general algorithmic framework is exemplified by our results over a hierarchy of function classes, with significantly stronger separation factor guarantees the more structure a class exhibits.\nDetermining the exact approximation factor for comparator-learning submodular functions is an open question, as there is a gap between the Õ(n1/2) upper bound and the Ω̃(n1/3) lower bound. Another open question is determining whether the sample complexity for the additive error results in Section 4 can be improved. We note, both of these questions are unresolved even in the setting where the sample consists of function values and the goal is to learn an approximate function. Another interesting question is to find nontrivial generalizations of the pairwise comparison model and show corresponding results. For instance, the distribution is over k-tuples and the top k′ sets in the tuple are ranked."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Peter Bartlett for insightful initial discussions which led to the development of this research project.\nThis work was supported in part by NSF grants CCF-1451177, CCF-1422910, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) fellowship."
    }, {
      "heading" : "B Proofs from Section 3",
      "text" : "Proof of Claim 2. First we will show the probability of the bad event happening for each (i, j) ∈ R is low, and then we union bound over all possible pairs (i, j). Formally, let\nP1 = Pr S∼D\n[f(S) ≤ f(Si) and (i, j) ∈ R yet ~wij · χ(S) > θij ]\nand P2 = Pr\nS′∼D [f(S′) ≥ f(Sj) and (i, j) ∈ R yet ~wij · χ(S′) < θij ].\nWe show that with probability at least 1− δ, P1 + P2 ≤ m2 . First we bound the VC dimension of each class of loss functions Lij = {L(~wij ,θij) | ~wij ∈ Rn, θij ∈ R}, where\nL(~wij ,θij)(S) =  1 if f(S) ≤ f(Si) and ~wij · χ(S) > θij\nor f(S) ≥ f(Sj) and ~wij · χ(S) < θij 0 otherwise .\nClearly, if S = {S1, . . . , St} can be shattered, then it cannot contain any set S such that f(Si) < f(S) < f(Sj) because for such a set S, L(~wij ,θij)(S) = 0 for all choices of (~wij , θij). Therefore, the problem reduces to finding the VC dimension of the class of linear separators over Rn in the following way.\nFirst, suppose that S can be labeled in every possible way by the class of linear separators over Rn. We claim that for any A ⊆ [t], there exists a linear separator (~wij , θij) such that L(~wij ,θij)(S`) = 1 if ` ∈ A and L(~wij ,θij)(S`) = 0 if ` 6∈ A. To this end, let\nA≤ = {` | ` ∈ A, f(S`) ≤ f(Si)}, A≥ = {` | ` ∈ A, f(S`) ≥ f(Sj)}, B≤ = {` | ` 6∈ A, f(S`) ≤ f(Si)}, and B≥ = {` | ` 6∈ A, f(S`) ≥ f(Sj)}.\nThen (~wij , θij) is the linear separator that labels A≤ ∪ B≥ as positive and A≥ ∪ B≤ as negative. Such a linear separator must exist by assumption.\nNow suppose that S can be labeled in every possible way by functions in Lij . We claim that for any A ⊆ [t], there exists a linear separator (~w, θ) such that ~w ·χ(S`) ≥ θ if ` ∈ A and ~w ·χ(S`) < θ if ` 6∈ A. Indeed, (~w, θ) is the linear separator such that L(~w,θ)(S`) = 1 if ` ∈ A≤ ∪ B≥ and L(~w,θ)(S`) = 0 if ` ∈ A≥ ∪B≤. We know that (~w, θ) by assumption.\nTherefore, the VC dimension of Lij is the same as the VC dimension of the class of linear separators over Rn, which is n+ 1.\nNow, let h∗ be the empirical risk minimizer of Lij over S2. We know that so long as √ nf(Si) ≤ f(Sj), h ∗ will have zero loss over the S2. Therefore, by standard VC dimension bounds, if |S2| = O(m 2 [n log 1 + log\n1 δm2 ]), then with probability at least 1− δ, the error of h∗ over D is at most m2\n. Now we union bound over all m2 pairs (i, j) on which Algorithm 1 attempts to solve for a linear threshold (~wij , θij), to achieve an overall error of .\nProof of Corollary 1. The statements follow by applying Theorem 1 with the following structural results.\n1. The class of submodular functions is √ n-approximable because for all submodular functions f , there exists w ∈ Rn such that f(S) ≤ √ w · χ(S) ≤ √ nf(S) [12]. 2. The class of XOS functions is O( √ n)-approximable because for all XOS functions f , there\nexists w ∈ Rn such that f(S) ≤ √ w · χ(S) ≤ α(n)f(S), where α(n) = O( √ n) [2].\n3. The class of subadditive functions is √ n log n-approximable because for all subadditive func-\ntions f , there exists a submodular function g such that for all S ⊆ [n], f(S) ≤ g(S) ≤ f(S) log n [3]. From this, we can use the approximation guarantee for submodular functions from item 1 of this corollary to obtain the result.\n4. Recall the curvature of a submodular function f is defined as\nκf = 1− min j∈[n] f([n])− f([n] \\ {j}) f(j) .\nIntuitively, the curvature f is the extent to which the function deviates from a modular function. From [16], we have the following bound,\n∀S ⊆ [n], (1− κf ) ∑ j∈[n] f(j) ≤ f(S) ≤ ∑ j∈[n] f(j).\nTherefore, the class of functions with curvature at most κ is min {√\nn, 11−κ\n} -approximable.\n5. [2] proved that for any XOS function f with R SUM trees, there exists a function g(S) = w · χM (S) such that f(S) ≤ g(S) ≤ Rξ · g(S), where χM denotes the indicator function for all subsets of size at most 1ξ over [n]. Specifically, χM (S)i1,i2,...,iL = 1 if {i1, i2, . . . iL} ⊆ S and\nχM (S)i1,i2,...,iL = 0 otherwise. We feed the sample with the new features into Algorithm 1 in order to learn a comparator with separation Rξ for this class of functions. Now that the feature space consists of n1/ξ features, the sample complexity and running time are polynomial in n1/ξ, rather than n.\nProof of Theorem 4. First, we use Theorem 1 from [2], which guarantees that there exists a family of subsets A = {A1, . . . , Ak} ⊂ 2[n] such that for any B ⊆ A, there exists an XOS function fB such that fB(Ai) = Ω( √ n) if Ai ∈ B whereas fB(Ai) = O(log n) if Ai 6∈ B. Moreover, k = n 1 3\nlog logn. Just as in the proof of Theorem 3, this implies hardness for comparator-learning over the uniform distribution on A from any polynomial-sized samples. After all, for an arbitrary algorithm ALG, suppose that ALG has access to pairwise comparisons over a sample of sets S, where |S| = ` ≤ nc for some constant c. Then in expectation, half of the non-training set samples have values in O(log n) and half have values in Ω( √ n). This follows by the same argument as in the proof of Theorem 3. Moreover, the measure of the set of non-training examples is 1 − nc− 1 3\nlog logn. Therefore, for n sufficiently large,\nEf∗,S\n[ Pr\nS,S′∼D\n[ S, S′ 6∈ S, f∗(S) = O(log n), and f∗(S′) = Ω( √ n) ]] = ( 1− nc− 1 3 log logn\n2 )2 ≥ 49\n512 .\nThe remainder of the proof follows as in the proof of Theorem 3."
    }, {
      "heading" : "C Proofs from Section 4",
      "text" : "Proof of Lemma 2. Suppose S1, S2 ⊆ [n] are such that f(S1) + 2γ ( 1 + √ 2 ξ ) < f(S2). We know from Lemma 1 and the union bound that with probability at least 1− ξ,\nf(Si)− γ ( 1 + √ 2\nξ\n) ≤ p0(Si) ≤ f(Si) + γ ( 1 + √ 2\nξ ) for both i ∈ {1, 2}. Therefore, with probability at least 1− ξ,\n~p · v(S1) = p0(S1) ≤ f(S1) + γ ( 1 + √ 2\nξ\n) ≤ f(S2)− γ ( 1 + √ 2\nξ\n) ≤ p0(S2) = ~p · v(S2).\nProof of Claim 4. First, in order to derive the sample complexity result, we need to bound the VC dimension of each class of loss functions Lij = {L(~pij ,θij) | ~pij ∈ Rn k , θij ∈ R}, where\nL(~pij ,θij)(S) =  1 if f(S) ≤ f(Si) and ~pij · v(S) > θij\nor f(S) ≥ f(Sj) and ~pij · v(S) < θij 0 otherwise .\nBy the same reasoning as in the proof of Claim 2 , the VC dimension of each Lij is simply nk. Therefore, by standard VC-dimension bounds, we need\nm′ = O\n( m2\n2\n[ n O ( 1 γ4/5 log 1 γ ) ln m2 + ln 1\nδ\n])\n= O\n( 1\n4 log\n1\nδ\n[ n O ( 1 γ4/5 log 1 γ ) log ( 1\n3 log\n1\nδ\n) + log 1\nδ\n])\nexamples to ensure that P [ |LERMij(S2)(S2)− LERMij(S2)(D)| >\n4m2\n] < δ.\nSince we only add (i, j) to R if LERMij(S2)(S2) < 4m2 , this means that with probability at least 1− δ,\nLERMij(S2)(D) = PrS∼D  f(S) ≤ f(Si) yet ~pij · χ(S) > θij or f(S) ≥ f(Sj) yet ~pij · χ(S) < θij  < 2m2 .\nBy a union bound over all m2 pairs in R, we have that with probability at least 1− δ,\nPr S∼D  ∃(i, j) ∈ R such that f(S) ≤ f(Si) yet ~pij · χ(S) > θij or f(S) ≥ f(Sj) yet ~pij · χ(S) < θij  < 2 .\nProof of Theorem 6. We combine Claim 3 and Claim 4 to prove Theorem 6. To this end, let g be the comparison function returned by Algorithm 2. We want to bound the probability that for S, S′ ∼ D, f(S) + β < f(S′) but g predicts that f(S′) ≤ f(S). Equivalently, we want to bound the probability that for S, S′ ∼ D, f(S) +β < f(S′) but there exists (i, j) ∈ R such that ~pij ·v(S) > θij and ~pij · v(S′) < θij . To analyze this probability, we partition the pairs (i, j) ∈ R into two sets:\nR1 = {(i, j) | f(S) ≤ f(Si) or f(S′) ≥ f(Sj)} and R2 = {(i, j) | f(S) > f(Si) and f(S′) < f(Sj)}.\nClearly,\nPr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R such that ~pij · v(S) > θij and ~pij · v(S′) < θij ]\n≤ Pr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R1 such that ~pij · v(S) > θij and ~pij · v(S′) < θij ]\n+ Pr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R2 such that ~pij · v(S) > θij and ~pij · v(S′) < θij ].\nFirst, notice that\nPr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R1 such that ~pij · v(S) > θij and ~pij · v(S′) < θij ]\n≤ Pr S,S′∼D  f(S) + β < f(S′) and ∃(i, j) ∈ R such that f(S) ≤ f(Si) yet ~pij · χ(S) > θij ] or f(S′) ≥ f(Sj) yet ~pij · χ(S′) < θij  .\nFrom Claim 4, with probability at least 1− δ2 , this probability is at most 2 . Next, we analyze\nPr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R2 such that ~pij · v(S) > θij and ~pij · v(S′) < θij ].\nRecall that the algorithm removed all non-minimal pairs from R. Therefore, the probability that there exists (i, j) ∈ R2 is simply the probability that there does not exist Si, Sj ∈ S1 such that f(S) + β ≤ f(Si) + β < f(Sj) ≤ f(S′) and (i, j) ∈ R. Therefore,\nPr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R2 such that ~pij · v(S) > θij and ~pij · v(S′) < θij ]\n≤ Pr S,S′∼D\n[ f(S) + β < f(S′) and 6 ∃(i, j) ∈ R such that f(S) + β ≤ f(Si) + β < f(Sj) ≤ f(S′) ] .\nBy using confidence and accuracy parameters δ/2 and /2, respectively, in Claim 3, we have that with probability at least 1− δ2 , this probability is at most 2 .\nPutting these bounds together, we have that with probability least 1− δ,\nPr S,S′∼D\n[f(S) + β < f(S′) and ∃(i, j) ∈ R such that ~pij · v(S) > θij and ~pij · v(S′) < θij ] < .\nTherefore, if g is the classifier that Algorithm 2 outputs, with probability at least 1 − δ, the probability that f(S) + β < f(S′) and g predicts incorrectly is at most . Therefore, we have the desired result."
    }, {
      "heading" : "D Proofs from Section 5",
      "text" : "Proof of Theorem 8. The interaction function allows us to express f ∈ Fk as a linear function in nk-dimensional space. In particular, let S1, . . . , Snk be an ordering of all subsets of [n] of size at most k and define χk(S) to be a vector in Rn k\nwhose ith component is 1 if Si ∩ S 6= ∅ and 0 otherwise. Next, let ~gk be a vector in Rn k whose ith component is g(Si). Then f(S) = ~gk · χk(S).\nThis suggests a straightforward adjustment to Algorithm 1: If we know that the underlying valuation function f is in Fk, then we can map each sample S ⊆ [n] to χk(S) and attempt to learn linear threshold functions wij , θij over Rn k rather than Rn for all i, j ∈ S1.\nNote the sample complexity dependence on drops from 1 3 (in Theorem 1) to 1 2\nsince we only need to attempt to learn wij , θij for adjacent Si, Sj ∈ S1 in the ordered list, because it is not required f(Si) and f(Sj) need to be sufficiently far apart to guarantee successfully learning wij and θij . This implies the union bound is over m events instead of m 2 events.\nProof of Theorem 9. We know that for any S ⊆ [n], f(S) = ∑\nT⊆[n] f̂(T )χT (S), where χT (S) =\n(−1)|T∩S|. Since the Fourier support of f is contained in P, this equation simplifies to f(S) =∑ T⊆P f̂(T )χT (S). Let T1, . . . , T|P| be an ordering of P, and let ~w ∈ RP be defined such that w[i] = f̂(Ti). By assumption, ~w has k non-zero entries. If we map S to RP by defining v(S) to be a vector such that the ith component is χTi(S), then f(S) = ~w · v(S).\nTherefore, in Algorithm 1, we may attempt to learn linear threshold functions wij , θij over R|P| rather than Rn for all i, j ∈ S1. Since the VC dimension of all k-sparse halfspaces in R|P| is k log |P|, we can learn these linear separators to the precision required in the proof of Algorithm 1 by using Õ ( k 3 ) examples in time polynomial in |P|Θ(k), 1/ , and 1/δ [8].\nFor the same reason as in Theorem 8, the dependence on in the sample complexity can be lowered to 1\n2 .\nRemark 1. By ignoring the sparsity of the linear separators, we can learn them using Õ ( |P| 3 ) examples in time polynomial in |P|, 1/ , and 1/δ.\nProof of Theorem 10. To begin with, we rely on the following result from [1].\nTheorem 14. [1] For any coverage function c : 2[n] → R≥0, there exists a coverage function ĉ on a universe U ′ with |U ′| ≤ 27n2 2 such that for all S ∈ 2[n], c(S)/(1 + ) ≤ ĉ(S) ≤ c(S) with probability at least 1− 2n+1e−n.\nWe also use the following lemma from [9].\nLemma 3. [9] A function c : 2[n] → R≥0 is a coverage function on some universe U if and only if there exist non-negative coefficients αS for every S ⊆ [n], S 6= ∅ such that c(T ) = ∑ S⊆[n],S 6=∅ αS · ORS(T ), and at most |U | of the coefficients αS are non-zero.\nHere, ORS : 2 [n] → {0, 1} is defined such that for any T ⊆ [n], ORS(T ) = 0 if and only if T ⊆ S. Now, let S1, . . . , S2n−1 be an ordering of 2 [n] \\ ∅ and for S ⊆ [n], let v(S) ∈ {0, 1}2n−1 be the vector defined as v(S)[i] = ORSi(S). We know from Theorem 14 and Lemma 3 that with probability at least 1 − 2n+1e−n, there exists a vector ~α ∈ R2n−1 such that for all S ⊆ [n] and ∈ (0, 1), c(S)/(1 + ) ≤ ~α · v(S) ≤ c(S). Moreover, from Lemma 3, we know that ~α has at most 27n 2 2 non-zero entries. Therefore, with probability at least 1− 2n+1e−n, for any S, S′ ⊆ [n], if (1 + )c(S) ≤ c(S′), then v(S) · ~α ≤ c(S) ≤ c(S′)/(1 + ) ≤ v(S′) · ~α. This means that in Algorithm 1, for each Si, Sj ∈ S1 such that c(Si) ≤ c(Sj), we can solve for a linear separator αij , θij such that v(S) · αij < θij if c(S) ≤ c(Si) and v(S) · αij > θij if c(S) ≥ c(Sj). With probability at least 1−2n+1e−n, such a linear threshold function will exists for all Si, Sj such that (1 + )c(Si) ≤ c(Sj).\nIt is well known that the VC dimension of the class of linear threshold functions (~w,w0) over Rd such that ||~w||0 ≤ r has VC dimension O(r log d) (ex. [18]). Therefore, the class of linear threshold functions Algorithm 1 learns over has VC dimension O(|U ′| log(2n− 1)) = O(n3/ 2). Therefore, we change the size of S2 to be\n|S2| = O ( m2 [ n3\n2 log\n1\n+ log\n1\nδm2\n]) ,\nwhere m = 1 log 1 δ . Moreover, we need to map each S ∈ S2 to R 2n−1 in order to learn the linear separators αij , θij . To do this, let S1, . . . , S2n−1 be an ordering of 2 [n] \\ ∅. Then we define the mapping v : 2[n] → R2n−1 such that for all S ⊆ [n], the ith component of v(S) is ORSi(S). With these changes, the analysis of Algorithm 1 in Section 3 holds, with α(n) = 1 + ."
    } ],
    "references" : [ {
      "title" : "Sketching valuation functions",
      "author" : [ "Ashwinkumar Badanidiyuru", "Shahar Dobzinski", "Hu Fu", "Robert Kleinberg", "Noam Nisan", "Tim Roughgarden" ],
      "venue" : "In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning valuation functions",
      "author" : [ "Maria-Florina Balcan", "Florin Constantin", "Satoru Iwata", "Lei Wang" ],
      "venue" : "In The 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Learning submodular functions",
      "author" : [ "Maria-Florina Balcan", "Nicholas JA Harvey" ],
      "venue" : "In Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "The modern theory of consumer behavior: Ordinal or cardinal",
      "author" : [ "William Barnett" ],
      "venue" : "The Quarterly Journal of Austrian Economics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Rank analysis of incomplete block designs: I. the method of paired comparisons",
      "author" : [ "Ralph Allan Bradley", "Milton E Terry" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1952
    }, {
      "title" : "Pairwise ranking aggregation in a crowdsourced setting",
      "author" : [ "Xi Chen", "Paul N Bennett", "Kevyn Collins-Thompson", "Eric Horvitz" ],
      "venue" : "In Proceedings of the sixth ACM international conference on Web search and data mining,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Networks, Crowds, and Markets",
      "author" : [ "David Easley", "Jon Kleinberg" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Open problem: The statistical query complexity of learning sparse halfspaces",
      "author" : [ "Vitaly Feldman" ],
      "venue" : "In Conference on Learning Theory, pages 1283–1289,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Learning coverage functions and private release of marginals",
      "author" : [ "Vitaly Feldman", "Pravesh Kothari" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Optimal bounds on approximation of submodular and xos functions by juntas",
      "author" : [ "Vitaly Feldman", "Jan Vondrak" ],
      "venue" : "In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Tight bounds on low-degree spectral concentration of submodular and XOS functions",
      "author" : [ "Vitaly Feldman", "Jan Vondrák" ],
      "venue" : "In IEEE 56th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Approximating submodular functions everywhere",
      "author" : [ "Michel X Goemans", "Nicholas JA Harvey", "Satoru Iwata", "Vahab Mirrokni" ],
      "venue" : "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Privately releasing conjunctions and the statistical query barrier",
      "author" : [ "Anupam Gupta", "Moritz Hardt", "Aaron Roth", "Jonathan Ullman" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Using the method of pairwise comparison to obtain reliable teacher assessments",
      "author" : [ "Sandra Heldsinger", "Stephen Humphry" ],
      "venue" : "The Australian Educational Researcher,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Options, Futures, and Other Derivatives",
      "author" : [ "John C. Hull" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Curvature and optimal algorithms for learning and minimizing submodular functions",
      "author" : [ "Rishabh K Iyer", "Stefanie Jegelka", "Jeff A Bilmes" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Active ranking using pairwise comparisons",
      "author" : [ "Kevin G Jamieson", "Robert Nowak" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Sparse solutions for linear prediction problems",
      "author" : [ "Tyler Neylon" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Algorithmic Game Theory",
      "author" : [ "Noam Nisan", "Tim Roughgarden", "Eva Tardos", "Vijay V. Vazirani" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Learning pseudo-boolean k-dnf and submodular functions",
      "author" : [ "Sofya Raskhodnikova", "Grigory Yaroslavtsev" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence",
      "author" : [ "Nihar B. Shah", "Sivaraman Balakrishnan", "Joseph K. Bradley", "Abhay Parekh", "Kannan Ramchandran", "Martin J. Wainwright" ],
      "venue" : "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Absolute identification by relative judgment",
      "author" : [ "Neil Stewart", "Gordon DA Brown", "Nick Chater" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Learning fourier sparse set functions",
      "author" : [ "Peter Stobbe", "Andreas Krause" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].",
      "startOffset" : 219,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].",
      "startOffset" : 219,
      "endOffset" : 236
    }, {
      "referenceID" : 9,
      "context" : "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].",
      "startOffset" : 219,
      "endOffset" : 236
    }, {
      "referenceID" : 1,
      "context" : "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].",
      "startOffset" : 219,
      "endOffset" : 236
    }, {
      "referenceID" : 15,
      "context" : "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].",
      "startOffset" : 219,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "After all, it is well-known that humans are significantly better at comparing than scoring [4, 22].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "After all, it is well-known that humans are significantly better at comparing than scoring [4, 22].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].",
      "startOffset" : 201,
      "endOffset" : 214
    }, {
      "referenceID" : 5,
      "context" : "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].",
      "startOffset" : 201,
      "endOffset" : 214
    }, {
      "referenceID" : 4,
      "context" : "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].",
      "startOffset" : 201,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].",
      "startOffset" : 201,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "Using existing approximation results [12, 2], we immediately conclude that several broad classes of combinatorial functions are comparator-learnable, including many that are ubiquitous in microeconomic theory.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Using existing approximation results [12, 2], we immediately conclude that several broad classes of combinatorial functions are comparator-learnable, including many that are ubiquitous in microeconomic theory.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "We also rely on results from [16] and [2] to achieve stronger bounds for submodular functions if the curvature is small.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "We also rely on results from [16] and [2] to achieve stronger bounds for submodular functions if the curvature is small.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "We show in Section 4 that if the underlying distribution over subsets of [n] is uniform, then we can take advantage of key insights regarding the Fourier spectrum of monotone submodular functions with range in [0, 1], presented in [11], to learn such a function up to comparisons on pairs of sets whose values differ by a sufficiently large additive factor.",
      "startOffset" : 210,
      "endOffset" : 216
    }, {
      "referenceID" : 10,
      "context" : "We show in Section 4 that if the underlying distribution over subsets of [n] is uniform, then we can take advantage of key insights regarding the Fourier spectrum of monotone submodular functions with range in [0, 1], presented in [11], to learn such a function up to comparisons on pairs of sets whose values differ by a sufficiently large additive factor.",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 0,
      "context" : "We extend this result to XOS functions with range in [0, 1] as well.",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "In particular, we present results for functions with sparse Fourier support [23] and functions with bounded nonlinear interactions [24].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Finally, for coverage functions [9, 1], we achieve α(n) = 1 + .",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "Finally, for coverage functions [9, 1], we achieve α(n) = 1 + .",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "XOS functions with distributional assumptions and range in [0,1] β ∈ (0, 1) Õ ( n O ( 1 γ ) / 3 ) ,",
      "startOffset" : 59,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Submodular functions with distributional assumptions and range in [0,1] β ∈ (0, 1) Õ (",
      "startOffset" : 66,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "learn an approximation of a submodular function within a multiplicative Õ( √ n) factor [12] in the membership query model, i.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Balcan and Harvey showed how to efficiently learn a function that approximates the given submodular factor up to a √ n factor on a 1− fraction of the test inputs, with probability 1− δ, in the supervised learning setting [3].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 1,
      "context" : "show near tight bounds on the PMAC learnability of subadditive functions and XOS (fractionally subadditive) functions [2].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "gave an algorithm with runtime nO(log(1/δ)/ 2) which learns an approximation h to a submodular function f such that with high probability, |f(x)−h(x)| ≤ [13].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "show an algorithm with runtime 2Õ(1/ 4/5) · n2 for approximating a submodular function with L2 error [11].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "showed that submodular functions always have an approximate function with a small sketch [1], and Iyer et al.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "showed parameterized bounds based on the curvature of the submodular function (how close the function is to being fully additive) [16].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "They show an algorithm to learn the rank using O(d log n) queries on average [17].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "study the ranking problem by assuming that the ranking reflects the inherent “qualities” of the objects, as defined by a vector ~ w∗ ∈ Rn [21].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 1,
      "context" : "By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 2,
      "context" : "To prove this result, we use a special family of matroid rank functions, which form a subset of the class of monotone submodular functions, presented in [3] and described as follows.",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "Theorem 2 (Theorem 7 in [3]).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "As in [3], we use the family of matroids presented in Theorem 2 to show that for a superpolynomial sized set of k points in {0, 1}n, and for any partition of those points into High and Low, we can construct a matroid where the points labeled High have rank rhigh and the points labeled Low have rank rlow, and that rhigh/rlow = Ω̃(n 1/3).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "The proof of Theorem 4 makes use of a special family of XOS functions presented in [2] and follows the same logic as in the proof of Theorem 3.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Let f be a monotone submodular function with range in [0, 1] and fix D to be the uniform distribution over the n-dimensional boolean cube.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "In particular, we use the fact that any monotone submodular function f with range in [0, 1] is γ-close to",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "√ E[(f(x)− p(x))2] < γ [11].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Let F be the class of monotone submodular functions with range in [0, 1].",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "Moreover, even in the model where the learning algorithm has access to realvalued function labels, for a monotone submodular function f with range in [0, 1], the best known results for learning a function h such that ||f − h||2 ≤ require running time 2Õ(1/ 4/5) · n2 and 2Õ(1/ 4/5) log n random examples [11].",
      "startOffset" : 150,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "Moreover, even in the model where the learning algorithm has access to realvalued function labels, for a monotone submodular function f with range in [0, 1], the best known results for learning a function h such that ||f − h||2 ≤ require running time 2Õ(1/ 4/5) · n2 and 2Õ(1/ 4/5) log n random examples [11].",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 10,
      "context" : "For ease of notation, we set k = 25 γ4/5 log 3 2 γ for the remainder of this section, where the constants come from the analysis in [11].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Let f be a monotone submodular function with range in [0, 1].",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Let f : 2[n] → [0, 1] be a monotone submodular function, D be the uniform distribution over 2[n], and γ ∈ (0, 1).",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "[11] proved that for γ ∈ (0, 1), if we set κ = 1 γ4/5 log 1 γ , then there exists L ⊆ [n], |L| ≤ 24 γ4/5 log 3 2 γ , such that if p(T ) = ∑ S:|S\\L|≤κ f̂(S)χS(T ), then ||f − p||2 = √ ES∼D[(f(S)− p(S))2] < γ.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Let F be the class of XOS functions with range in [0, 1].",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "This follows from the same line of reason as in the proof of Theorem 6 and the fact that for any XOS function f : 2[n] → [0, 1], there is a polynomial p of degree O(1/γ) such that ||f −p||2 ≤ γ [11].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "This follows from the same line of reason as in the proof of Theorem 6 and the fact that for any XOS function f : 2[n] → [0, 1], there is a polynomial p of degree O(1/γ) such that ||f −p||2 ≤ γ [11].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 22,
      "context" : "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9].",
      "startOffset" : 244,
      "endOffset" : 250
    }, {
      "referenceID" : 8,
      "context" : "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9].",
      "startOffset" : 244,
      "endOffset" : 250
    }, {
      "referenceID" : 22,
      "context" : "Stobbe and Krause show that the Fourier support of f is contained in P = {S | |S| = 2} ∪ ∅ [23].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "We combine structural results specific to coverage functions from [1] and [9] to prove that coverage functions are comparator-learnable with multiplicative separation (1 + ), using Õ(n3/ 5) queries, given access to an ERM oracle for learning linear separators.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "We combine structural results specific to coverage functions from [1] and [9] to prove that coverage functions are comparator-learnable with multiplicative separation (1 + ), using Õ(n3/ 5) queries, given access to an ERM oracle for learning linear separators.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "In this case, we require that the underlying distribution be uniform and that the underlying function be XOS or monotone submodular with range in [0, 1].",
      "startOffset" : 146,
      "endOffset" : 152
    } ],
    "year" : 2016,
    "abstractText" : "A large body of work in machine learning has focused on the problem of learning a close approximation to an underlying combinatorial function, given a small set of labeled examples. However, for real-valued functions, cardinal labels might not be accessible, or it may be difficult for an expert to consistently assign real-valued labels over the entire set of examples. For instance, it is notoriously hard for consumers to reliably assign values to bundles of merchandise. Instead, it might be much easier for a consumer to report which of two bundles she likes better. With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support.",
    "creator" : "LaTeX with hyperref package"
  }
}