{
  "name" : "1605.07784.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Algorithms for Robust PCA via Gradient Descent",
    "authors" : [ "Xinyang Yi", "Dohyung Park", "Yudong Chen", "Constantine Caramanis" ],
    "emails" : [ "yixy@utexas.edu", "dhpark@utexas.edu", "constantine@utexas.edu", "yudong.chen@cornell.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Principal component analysis (PCA) aims to find a low rank subspace that best-approximates a data matrix Y ∈ Rd1×d2 . The simple and standard method of PCA by singular value decomposition (SVD) fails in many modern data problems due to missing and corrupted entries, as well as sheer scale of the problem. Indeed, SVD is highly sensitive to outliers by virtue of the squared-error criterion it minimizes. Moreover, its running time scales as O(rd2) to recover a rank r approximation of a d-by-d matrix.\nWhile there have been recent results developing provably robust algorithms for PCA (e.g., [8, 27]), the running times range from O(r2d2) to O(d3)1 and hence are significantly worse than SVD. Meanwhile, the literature developing sub-quadratic algorithms for PCA (e.g., [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.\nOur contribution lies precisely in this area: provably robust algorithms for PCA with improved run-time. Specifically, we provide an efficient algorithm with running time that matches SVD while nearly matching the best-known robustness guarantees. In the case where rank is small compared to dimension, we develop an algorithm with running time that is nearly linear in the dimension. This last algorithm works by subsampling the data, and therefore we also show that our algorithm solves the Robust PCA problem with partial observations (a generalization of matrix completion and Robust PCA).\n1For precise dependence on error and other factors, please see details below.\nar X\niv :1\n60 5.\n07 78\n4v 1\n[ cs\n.I T\n] 2\n5 M\nay 2\n01 6"
    }, {
      "heading" : "1.1 The Model and Prior Work",
      "text" : "We consider the following setting for robust PCA. Suppose we are given a matrix Y ∈ Rd1×d2 that has decomposition Y = M∗+S∗, where M∗ is a rank r matrix and S∗ is a sparse corruption matrix containing entries with arbitrary magnitude. The goal is to recover M∗ and S∗ from Y . To ease notation, we let d1 = d2 = d in the remainder of this section.\nProvable solutions for this model are first provided in the works of [9] and [8]. They propose to solve this problem by convex relaxation:\nmin M,S |||M |||nuc + λ‖S‖1, s.t. Y = M + S, (1)\nwhere |||M |||nuc denotes the nuclear norm of M . Despite analyzing the same method, the corruption models in [8] and [9] differ. In [8], the authors consider the setting where the entries of M∗ are corrupted at random with probability α. They show their method succeeds in exact recovery with α as large as 0.1, which indicates they can tolerate a constant fraction of corruptions. Work in [9] considers a deterministic corruption model, where nonzero entries of S∗ can have arbitrary position, but the sparsity of each row and column does not exceed αd. They prove that for exact recovery, it can allow α = O(1/(µr √ d)). This was subsequently further improved to α = O(1/(µr)), which is in fact optimal [12, 18]. Here, µ represents the incoherence of M∗ (see Section 2 for details). In this paper, we follow this latter line and focus on the deterministic corruption model.\nThe state-of-the-art solver [20] for (1) has time complexity O(d3/ε) to achieve error ε, and is thus much slower than SVD, and prohibitive for even modest values of d. Work in [22] considers the deterministic corruption model, and improves this running time without sacrificing the robustness guarantee on α. They propose an alternating projection (AltProj) method to estimate the low rank and sparse structures iteratively and simultaneously, and show their algorithm has complexity O(r2d2 log(1/ε)), which is faster than the convex approach but still slower than SVD.\nNon-convex approaches have recently seen numerous developments for applications in low-rank estimation, including alternating minimization (see e.g. [19, 17, 16]) and gradient descent (see e.g. [4, 11, 24, 25, 30, 31]). These works have fast running times, yet do not provide robustness guarantees. One exception is [11], where the authors analyze a row-wise `1 projection method for recovering S∗. Their analysis hinges on positive semidefinite M∗, and the algorithm requires prior knowledge of the `1 norm of every row of S∗ and is thus prohibitive in practice. Another exception is work [16], which analyzes alternating minimization plus an overall sparse projection. Their algorithm is shown to tolerate at most a fraction of α = O(1/(µ2/3r2/3d)) corruptions. As we discuss below, we can allow S∗ to have much higher sparsity α = O(1/(µr1.5)), which is close to optimal.\nIt is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23]. It might be interesting to bring robust considerations to these works."
    }, {
      "heading" : "1.2 Our Contributions",
      "text" : "In this paper, we develop efficient non-convex algorithms for robust PCA. We propose a novel algorithm based on the projected gradient method on the factorized space. We also extend it to solve robust PCA in the setting with partial observations, i.e., in addition to gross corruptions, the data matrix has a large number of missing values. Our main contributions are summarized as\nfollows.2\n1. We propose a novel sparse estimator for the setting of deterministic corruptions. For the lowrank structure to be identifiable, it is natural to assume that deterministic corruptions are “spread out” (no more than some number in each row/column). We leverage this information in a simple but critical algorithmic idea, that is tied to the ultimate complexity advantages our algorithm delivers.\n2. Based on the proposed sparse estimator, we propose a projected gradient method on the matrix factorized space. While non-convex, the algorithm is shown to enjoy linear convergence under proper initialization. Along with a new initialization method, we show that robust PCA can be solved within complexity O(rd2 log(1/ε)) while ensuring robustness α = O(1/(µr1.5)). Our algorithm is thus faster than the best previous known algorithm by a factor of r, and enjoys superior empirical performance as well.\n3. Algorithms for Robust PCA with partial observations still rely on a computationally expensive convex approach, as apparently this problem has evaded treatment by non-convex methods. We consider precisely this problem. In a nutshell, we show that our gradient method succeeds (it is guaranteed to produce the subspace of M∗) even when run on no more than O(µ2r2d log d) random entries of Y . The computational cost is O(µ3r4d log d log(1/ε)). When rank r is small compared to the dimension d, in fact this dramatically improves on our bound above, as our cost becomes nearly linear in d. We show, moreover, that this savings and robustness to erasures comes at no cost in the robustness guarantee for the deterministic (gross) corruptions. While this demonstrates our algorithm is robust to both outliers and erasures, it also provides a way to reduce computational costs even in the fully observed setting, when r is small.\n4. An immediate corollary of the above result provides a guarantee for exact matrix completion, with general rectangular matrices, usingO(µ2r2d log d) observed entries andO(µ3r4d log d log(1/ε)) time, thereby improving on existing results in [11, 24]."
    }, {
      "heading" : "1.3 Organization and Notation",
      "text" : "The remainder of this paper is organized as follows. In Section 2, we formally describe our problem and assumptions. In Section 3, we present and describe our algorithms for fully (Algorithm 1) and partially (Algorithm 2) observed settings. In Section 4.1, we establish theoretical guarantees of Algorithm 1. The theory for partially observed setting are presented in Section 4.2. Numerical results are collected in Section 5. Sections 6, 7 and Appendix A contain all the proofs and technical lemmas.\nFor any index set Ω ⊆ [d1]×[d2], we let Ω(i,·) := { (i, j) ∈ Ω ∣∣ j ∈ [d2]}, Ω(·,j) := {(i, j) ∈ Ω ∣∣ i ∈ [d1]}.\nFor any matrix A ∈ Rd1×d2 , we denote its projector onto support Ω by ΠΩ (A), i.e., the (i, j)-th entry of ΠΩ (A) is equal to A if (i, j) ∈ Ω and zero otherwise. The i-th row and j-th column of A are denoted by A(i,·) and A(·,j). The (i, j)-th entry is denoted as A(i,j). Operator norm of A is |||A|||op. Frobenius norm of A is |||A|||F. The `a/`b norm of A is denoted by |||A|||b,a, i.e., the `a norm of the vector formed by the `b norm of every row. For instance, |||A|||2,∞ stands for maxi∈[d1] ‖A(i,·)‖2.\n2To ease presentation, the discussion here assumes M∗ has constant condition number, whereas our results below show the dependence on condition number explicitly."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We consider the problem where we observe a matrix Y ∈ Rd1×d2 that satisfies Y = M∗+S∗, where M∗ has rank r, and S∗ is corruption matrix with sparse support. Our goal is to recover M∗ and S∗. In the partially observed setting, in addition to sparse corruptions, we have erasures. We assume that each entry of M∗ + S∗ is revealed independently with probability p ∈ (0, 1). In particular, for any (i, j) ∈ [d1]× [d2], we consider the Bernoulli model where\nY(i,j) = { (M∗ + S∗)(i,j), with probability p; ∗, otherwise.\n(2)\nWe denote the support of Y by Φ = {(i, j) | Y(i,j) 6= ∗}. Note that we assume S∗ is not adaptive to Φ. As is well-understood thanks to work in matrix completion, this task is impossible in general – we need to guarantee that M∗ is not both low-rank and sparse. To avoid such identifiability issues, we make the following standard assumptions on M∗ and S∗: (i) M∗ is not near-sparse or “spiky.” We impose this by requiring M∗ to be µ-incoherent – given a singular value decomposition (SVD)3 M∗ = L∗Σ∗R∗>, we assume that\n|||L∗|||2,∞ ≤ √ µr\nd1 , |||R∗|||2,∞ ≤\n√ µr\nd2 .\n(ii) The entries of S∗ are “spread out” – for α ∈ [0, 1), we assume S∗ ∈ Sα, where\nSα := { A ∈ Rd1×d2 ∣∣ ‖A(i,·)‖0 ≤ αd2 for all i ∈ [d1] ; ‖A(·,j)‖0 ≤ αd1 for all j ∈ [d2]} . (3) In other words, S∗ contains at most α-fraction nonzero entries per row and column."
    }, {
      "heading" : "3 Algorithms",
      "text" : "For both the full and partial observation settings, our method proceeds in two phases. In the first phase, we use a new sorting-based sparse estimator to produce a rough estimate Sinit for S∗ based on the observed matrix Y , and then find a rank r matrix factorized as U0V >0 that is a rough estimate of M∗ by performing SVD on (Y − Sinit). In the second phase, given (U0, V0), we perform an iterative method to produce series {(Ut, Vt)}∞t=0. In each step t, we first apply our sparse estimator to produce a sparse matrix St based on (Ut, Vt), and then perform a projected gradient descent step on the low-rank factorized space to produce (Ut+1, Vt+1). This flow is the same for full and partial observations, though a few details differ. Algorithm 1 gives the full observation algorithm, and Algorithm 2 gives the partial observation algorithm. We now describe the key details of each algorithm.\nSparse Estimation. A natural idea is to keep those entries of residual matrix Y −M that have large magnitude. At the same time, we need to make use of the dispersed property of Sα that every column and row contain at most α-fraction of nonzero entries. Motivated by these two principles, we\n3Throughout this paper, we refer to SVD of rank r matrix by form LΣR> where Σ ∈ Rr×r.\nintroduce the following sparsification operator: For any matrix A ∈ Rd1×d2 : for all (i, j) ∈ [d1]×[d2], we let\nTα [A] :=\n{ A(i,j), if |A(i,j)| ≥ |A (αd2) (i,·) | and |A(i,j)| ≥ |A (αd1) (·,j) |\n0, otherwise , (4)\nwhere A(k)(i,·) and A (k) (·,j) denote the elements of A(i,·) and A(·,j) that have the k-th largest magnitude respectively. In other words, we choose to keep those elements that are simultaneously among the largest α-fraction entries in the corresponding row and column. In the case of entries having identical magnitude, we break ties arbitrarily. It is thus guaranteed that Tα [A] ∈ Sα.\nAlgorithm 1 Fast RPCA INPUT: Observed matrix Y with rank r and corruption fraction α; parameters γ, η; number of\niterations T . // Phase I: Initialization. 1: Sinit ← Tα [Y ] // see (4) for the definition of Tα [·]. 2: [L,Σ, R]← SVDr[Y − Sinit] 4 3: U0 ← LΣ1/2, V0 ← RΣ1/2. Let U ,V be defined according to (7).\n// Phase II: Gradient based iterations. 4: U0 ← ΠU (U0), V0 ← ΠV (V0) 5: for t = 0, 1, . . . , T − 1 do 6: St ← Tγα [ Y − UtV >t\n] 7: Ut+1 ← ΠU ( Ut − η∇UL(Ut, Vt;St)− 12ηUt(U > t Ut − V >t Vt)\n) 8: Vt+1 ← ΠV ( Vt − η∇V L(Ut, Vt;St)− 12ηVt(V > t Vt − U>t Ut)\n) 9: end for\nOUTPUT: (UT , VT )\nInitialization. In the fully observed setting, we compute Sinit based on Y as Sinit = Tα [Y ]. In the partially observed setting with sampling rate p, we let Sinit = T2pα [Y ]. In both cases, we then set U0 = LΣ 1/2 and V0 = RΣ1/2, where LΣR> is an SVD of the best rank r approximation of Y −Sinit.\nGradient Method on Factorized Space. After initialization, we proceed by projected gradient descent. To do this, we define loss functions explicitly in the factored space, i.e., in terms of U, V and S:\nL(U, V ;S) := 1 2 |||UV > + S − Y |||2F, (fully observed) (5) L̃(U, V ;S) := 1 2p |||ΠΦ ( UV > + S − Y ) |||2F. (partially observed) (6)\nRecall that our goal is to recover M∗ that satisfies the µ-incoherent condition. Given an SVD M∗ = L∗ΣR∗>, we expect that the solution (U, V ) is close to (L∗Σ1/2, R∗Σ1/2) up to some rotation. In order to serve such µ-incoherent structure, it is natural to put constraints on the row norms of\n4SVDr[A] stands for computing a SVD of the matrix that is the best rank r approximation of A.\nU, V based on |||M∗|||op. As |||M∗|||op is unavailable, given U0, V0 computed in the first phase, we rely on the sets U , V defined as\nU := { A ∈ Rd1×r ∣∣ |||A|||2,∞ ≤√2µr d1 |||U0|||op } , V := { A ∈ Rd2×r ∣∣ |||A|||2,∞ ≤√2µr d2 |||V0|||op } . (7)\nNow we consider the following optimization problems with constraints:\nmin U∈U ,V ∈V,S∈Sα\nL(U, V ;S) + 1 8 |||U>U − V >V |||2F, (fully observed) (8)\nmin U∈U ,V ∈V,S∈Spα\nL̃(U, V ;S) + 1 64 |||U>U − V >V |||2F. (partially observed) (9)\nThe regularization term in the objectives above is used to encourage that U and V have the same scale. Given (U0, V0), we propose the following iterative method to produce series {(Ut, Vt)}∞t=0 and {St}∞t=0. We give the details for the fully observed case – the partially observed case is similar. For t = 0, 1, . . ., we update St using the sparse estimator St = Tγα [ Y − UtV >t ] , followed by a projected gradient update on Ut and Vt\nUt+1 = ΠU ( Ut − η∇UL(Ut, Vt;St)− 1\n2 ηUt(U\n> t Ut − V >t Vt)\n) ,\nVt+1 = ΠV ( Vt − η∇V L(Ut, Vt;St)− 1\n2 ηVt(V\n> t Vt − U>t Ut)\n) .\nHere α is the model parameter that characterizes the corruption fraction, γ and η are algorithmic tunning parameters, which we specify in our analysis. Essentially, the above algorithm corresponds to applying projected gradient method to optimize (8), where S is replaced by the aforementioned sparse estimator in each step.\nAlgorithm 2 Fast RPCA with partial observations INPUT: Observed matrix Y with support Φ; parameters τ, γ, η; number of iterations T . // Phase I: Initialization. 1: Sinit ← T2pα [ΠΦ(Y )] 2: [L,Σ, R]← SVDr[1p(Y − Sinit)] 3: U0 ← LΣ1/2, V0 ← RΣ1/2. Let U ,V be defined according to (7).\n// Phase II: Gradient based iterations. 4: U0 ← ΠU (U0), V0 ← ΠV (V0) 5: for t = 0, 1, . . . , T − 1 do 6: St ← Tγpα [ ΠΦ ( Y − UtV >t\n)] 7: Ut+1 ← ΠU ( Ut − η∇U L̃(Ut, Vt;St)− 116ηUt(U > t Ut − V >t Vt)\n) 8: Vt+1 ← ΠV ( Vt − η∇V L̃(Ut, Vt;St)− 116ηVt(V > t Vt − U>t Ut)\n) 9: end for\nOUTPUT: (UT , VT )"
    }, {
      "heading" : "4 Main Results",
      "text" : "In this section, we establish theoretical guarantees for Algorithm 1 in the fully observed setting and for Algorithm 2 in the partially observed setting."
    }, {
      "heading" : "4.1 Analysis of Algorithm 1",
      "text" : "We begin with some definitions and notation. It is important to define a proper error metric because the optimal solution corresponds to a manifold and there are many distinguished pairs (U, V ) that minimize (8). Given the SVD of the true low-rank matrixM∗ = L∗Σ∗R∗>, we let U∗ := L∗Σ∗1/2 and V ∗ := R∗Σ∗1/2. We also let σ∗1 ≥ σ∗2 ≥ . . . ≥ σ∗r be sorted nonzero singular values ofM∗, and denote the condition number of M∗ by κ, i.e., κ := σ∗1/σ∗r . We define estimation error d(U, V ;U∗, V ∗) as the minimal Frobenius norm between (U, V ) and (U∗, V ∗) with respect to the optimal rotation, namely\nd(U, V ;U∗, V ∗) := min Q∈Qr\n√ |||U − U∗Q|||2F + |||V − V ∗Q|||2F, (10)\nfor Qr the set of r-by-r orthonormal matrices. This metric controls reconstruction error, as\n|||UV > −M∗|||F . √ σ∗1d(U, V ;U ∗, V ∗), (11)\nwhen d(U, V ;U∗, V ∗) ≤ √ σ∗1. We denote the local region around the optimum (U\n∗, V ∗) with radius ω as B2 (ω) := { (U, V ) ∈ Rd1×r × Rd2×r ∣∣ d(U, V ;U∗, V ∗) ≤ ω} .\nThe next two theorems provide guarantees for the initialization phase and gradient iterations, respectively, of Algorithm 1. The proofs are given in Sections 6.1 and 6.2.\nTheorem 1 (Initialization). Consider the paired (U0, V0) produced in the first phase of Algorithm 1. If α ≤ 1/(16κµr), we have\nd(U0, V0;U ∗, V ∗) ≤ 28 √ καµr √ r √ σ∗1.\nTheorem 2 (Convergence). Consider the second phase of Algorithm 1. Suppose we choose γ = 2 and η = c/σ∗1 for any c ≤ 1/36. There exist constants c1, c2 such that when α ≤ c1/(κ2µr), given any (U0, V0) ∈ B2 ( c2 √ σ∗r/κ ) , the iterates {(Ut, Vt)}∞t=0 satisfy\nd2(Ut, Vt;U ∗, V ∗) ≤ ( 1− c\n8κ\n)t d2(U0, V0;U ∗, V ∗).\nTherefore, using proper initialization and step size, the gradient iteration converges at a linear rate with a constant contraction factor 1 − O(1/κ). To obtain relative precision ε compared to the initial error, it suffices to perform O(κ log(1/ε)) iterations. Note that the step size is chosen according to 1/σ∗1. When α . 1/(µ √ κr3), Theorem 1 and the inequality (11) together imply that |||U0V >0 −M∗|||op ≤ 12σ ∗ 1. Hence we can set the step size as η = O(1/σ1(U0V >0 )) using being the top singular value σ1(U0V >0 ) of the matrix U0V >0 Combining Theorems 1 and 2 implies the following result, proved in Section 6.3, that provides an overall guarantee for Algorithm 1.\nCorollary 1. Suppose that\nα ≤ cmin\n{ 1\nµ √ κr\n3 , 1\nµκ2r } for some constant c. Then for any ε ∈ (0, 1), Algorithm 1 T = O(κ log(1/ε)) outputs a pair (UT , VT ) that satisfies\n|||UTV >T −M∗|||F ≤ ε · σ∗r . (12)\nRemark 1 (Time Complexity). For simplicity we assume d1 = d2 = d. Our sparse estimator (4) can be implemented by finding the top αd elements of each row and column via partial quick sort, which has running time O(d2 log(αd)). Performing rank-r SVD in the first phase and computing the gradient in each iteration both have complexity O(rd2).5 Algorithm 1 thus has total running time O(κrd2 log(1/ε)) for achieving an accuracy as in (12). We note that when κ = O(1), our algorithm is orderwise faster than the AltProj algorithm in [22], which has running time O(r2d2 log(1/ε)). Moreover, our algorithm only requires computing one singular value decomposition.\nRemark 2 (Robustness). Assuming κ = O(1), our algorithm can tolerate corruption at a sparsity level up to α = O(1/(µr √ r)). This is worse by a factor √ r compared to the optimal statistical guarantee 1/(µr) obtained in [12, 18, 22]. This looseness is a consequence of the condition for (U0, V0) in Theorem 2. Nevertheless, when µr = O(1), our algorithm can tolerate a constant α fraction of corruptions."
    }, {
      "heading" : "4.2 Analysis of Algorithm 2",
      "text" : "We now move to the guarantees of Algorithm 2. We show here that not only can we handle partial observations, but in fact subsampling the data in the fully observed case can significantly reduce the time complexity from the guarantees given in the previous section without sacrificing robustness. In particular, for smaller values of r, the complexity of Algorithm 2 has near linear dependence on the dimension d, instead of quadratic.\nIn the following discussion, we let d := max{d1, d2}. The next two results, proved in Sections 6.4 and 6.5, control the quality of the initialization step, and then the gradient iterations.\nTheorem 3 (Initialization, partial observations). Suppose the observed indices Φ follow the Bernoulli model given in (2). Consider the pair (U0, V0) produced in the first phase of Algorithm 2. There exist constants {ci}3i=1 such that for any ∈ (0, √ r/(8c1κ)), if\nα ≤ 1 64κµr\n, p ≥ c2 ( µr2\n2 +\n1\nα\n) log d\nd1 ∧ d2 , (13)\nthen we have d(U0, V0;U ∗, V ∗) ≤ 51 √ καµr √ r √ σ∗1 + 7c1 √ κσ∗1,\nwith probability at least 1− c3d−1. 5In fact, it suffices to compute the best rank-r approximation with running time independent of the eigen gap.\nTheorem 4 (Convergence, partial observations). Suppose the observed indices Φ follow the Bernoulli model given in (2). Consider the second phase of Algorithm 2. Suppose we choose γ = 3, and η = c/(µrσ∗1) for a sufficiently small constant c. There exist constants {ci}4i=1 such that if\nα ≤ c1 κ2µr and p ≥ c2 κ4µ2r2 log d d1 ∧ d2 , (14)\nthen with probability at least 1− c3d−1, the iterates {(Ut, Vt)}∞t=0 satisfy\nd2(Ut, Vt;U ∗, V ∗) ≤ ( 1− c\n64µrκ\n)t d2(U0, V0;U ∗, V ∗)\nfor all (U0, V0) ∈ B2 ( c4 √ σ∗r/κ ) .\nThe above result ensures linear convergence to (U∗, V ∗) (up to rotation) even when the gradient iterations are computed using partial observations. Note that setting p = 1 recovers Theorem 2 up to an additional factor µr in the contraction factor. For achieving ε relative accuracy, now we need O(µrκ log(1/ε)) iterations.\nPutting Theorems 3 and 4 together, we have the following overall guarantee, proved in Section 6.6, for Algorithm 2.\nCorollary 2. Suppose that\nα ≤ cmin\n{ 1\nµ √ κr\n3 , 1\nµκ2r\n} , p ≥ c′κ 4µ2r2 log d\nd1 ∧ d2 ,\nfor some constants c, c′. With probability at least 1 − O(d−1), for any ε ∈ (0, 1), Algorithm 2 with T = O(µrκ log(1/ε)) outputs a pair (UT , VT ) that satisfies\n|||UTV >T −M∗|||F ≤ ε · σ∗r . (15)\nThis result shows that partial observations do not compromise robustness to sparse corruptions: as long as the observation probability p satisfies the condition in Corollary 2, Algorithm 2 enjoys the same robustness guarantees as the method using all entries. Below we provide two remarks on the sample and time complexity. For simplicity, we assume d1 = d2 = d, κ = O(1). Remark 3 (Sample complexity and matrix completion). Using the lower bound on p, it is sufficient to have O(µ2r2d log d) observed entries. In the special case S∗ = 0, our partial observation model is equivalent to the model of exact matrix completion (see, e.g., [7]). We note that our sample complexity (i.e., observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24]. Accordingly, our result reveals the important fact that we can obtain robustness in matrix completion without deterioration of our statistical guarantees. It is known that that any algorithm for solving exact matrix completion must have sample size Ω(µrd log d) [7], and a nearly tight upper bound O(µrd log2 d) is obtained in [10] by convex relaxation. While sub-optimal by a factor µr, our algorithm is much faster than convex relaxation as shown below.\nRemark 4 (Time complexity). Our sparse estimator on the sparse matrix with support Φ can be implemented via partial quick sort with running time O(pd2 log(αpd)). Computing the gradient in each step involves the two terms in the objective function (9). Computing the gradient of the first term L̃ takes time O(r|Φ|), whereas the second term takes time O(r2d). In the initialization phase, performing rank-r SVD on a sparse matrix with support Φ can be done in time O(r|Φ|). We conclude that when |Φ| = O(µ2r2d log d), Algorithm 2 achieves the error bound (15) with running time O(µ3r4d log d log(1/ε)). Therefore, in the small rank setting with r d1/3, even when full observations are given, it is better to use Algorithm 2 by subsampling the entries of Y ."
    }, {
      "heading" : "5 Numerical Results",
      "text" : "In this section, we provide numerical results and compare the proposed algorithms with existing methods, including the inexact augmented lagrange multiplier (IALM) approach [20] for solving the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21]. All algorithms are implemented in MATLAB 6, and the codes for existing algorithms are obtained from their authors. SVD computation in all algorithms uses the PROPACK library.7 We ran all simulations on a machine with Intel 32-core Xeon (E5-2699) 2.3GHz with 240GB RAM."
    }, {
      "heading" : "5.1 Synthetic Datasets.",
      "text" : "We generate a squared data matrix Y = M∗ + S∗ ∈ Rd×d as follows. The low-rank part M∗ is given by M∗ = AB>, where A,B ∈ Rd×r have entries drawn independently from a zero mean Gaussian distribution with variance 1/d. For a given sparsity parameter α, each entry of S∗ is set to be nonzero with probability α, and the values of the nonzero entries are sampled uniformly from [−5r/d, 5r/d].\nThe results are summarized in Figure 1. Figure 1a shows the convergence of our algorithms for different random instances with different sub-sampling rate p (note that p = 1 corresponds to\n6Our code is available at https://people.orie.cornell.edu/yudong.chen/rpca_gd/RPCA_GD.zip. 7http://sun.stanford.edu/~rmunk/PROPACK/\nthe fully observed setting). As predicted by Theorems 2 and 4, our gradient method converges geometrically with a contraction factor nearly independent of p, Figure 1b shows the running time of our algorithm with partially observed data. We see that the running time scales linearly with d, again consistent with the theory. We note that our algorithm is memory-efficient: in the large scale setting with d = 2×105, using approximately 0.1% entries is sufficient for the successful recovery In contrast, AltProj and IALM are designed to manipulate the entire matrix with d2 = 4×1010 entries, which is prohibitive on a single machine. Figure 1c compares our algorithms with AltProj and IALM by showing reconstruction error versus real running time. Our algorithm requires significantly less computation to achieve the same accuracy level, and using only a subset of the entries provides additional speed-up."
    }, {
      "heading" : "5.2 Foreground-background Separation",
      "text" : "We apply our method to the task of foreground-background (FB) separation in a video. We use two public benchmarks, the Restaurant and ShoppingMall datasets.8, Each dataset contains a video with static background. By vectorizing and stacking the frames as columns of a matrix Y , the FB separation problem can be cast as RPCA, where the static background corresponds to a low rank matrix M∗ with identical columns, and the moving objects in the video can be modeled as sparse corruptions S∗. Figure 2 shows the output of different algorithms on two frames from the dataset. Our algorithms require significantly less running time than both AltProj and IALM. Moreover, even with 20% sub-sampling, our methods still appear to achieve better separation quality (note that in each of the frames our algorithms remove a person that is not identified by the other algorithms).\nFigure 3 shows recovery results for several more frames. Again, our algorithms enjoy better running time and outperform AltProj and IALM in separating persons from the background images. In Appendix B, we describe the detailed parameter settings for our algorithm.\n8http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html"
    }, {
      "heading" : "6 Proofs",
      "text" : "In this section we provide the proofs for our main theoretical results in Theorems 1–4 and Corollaries 1–2."
    }, {
      "heading" : "6.1 Proof of Theorem 1",
      "text" : "Let Y := Y − Sinit. As Y = M∗ + S∗, we have Y −M∗ = S∗ − Sinit. We obtain Y −M∗ ∈ S2α because S∗, Sinit ∈ Sα.\nWe claim that ‖Y −M∗‖∞ ≤ 2‖M∗‖∞. Denote the support of S∗, Sinit by Ω∗ and Ω respectively. Since Y −M∗ is supported on Ω ∪ Ω∗, to prove the claim it suffices to consider the following three cases.\n• For (i, j) ∈ Ω∗ ∩ Ω, due to rule of sparse estimation, we have (S∗ − Sinit)(i,j) = 0.\n• For (i, j) ∈ Ω∗ \\ Ω, we must have |S∗(i,j)| ≤ 2‖M ∗‖∞. Otherwise, we have |Y(i,j)| = |(S∗ +\nM∗)(i,j)| > ‖M∗‖∞. So |Y(i,j)| is larger than any uncorrupted entries in its row and column. Since there are at most α fraction corruptions per row and column, we have Y(i,j) ∈ Ω, which violates the prior condition (i, j) ∈ Ω∗ \\ Ω.\n• For the last case (i, j) ∈ Ω \\ Ω∗, since (Sinit)(i,j) = M∗(i,j), trivially we have |(Sinit)(i,j)| ≤ ‖M∗‖∞.\nThe following result, proved in Section 7.1, relates the operator norm of Y −M∗ to its infinite norm.\nLemma 1. For any matrix A ∈ Rd1×d2 that belongs to Sα given in (3), we have |||A|||op ≤ α √ d1d2‖A‖∞.\nWe thus obtain |||Y −M∗|||op ≤ 2α √ d1d2‖Y −M∗‖∞ ≤ 4α √ d1d2‖M∗‖∞ = 4αµrσ∗1. (16)\nIn the last step, we use the fact that M∗ satisfies the µ-incoherent condition, which leads to\n‖M∗‖∞ ≤ |||M∗|||op|||L∗|||2,∞|||R∗|||2,∞ ≤ µr√ d1d2 |||M∗|||op. (17)\nWe denote the i-th largest singular value of Y by σi. By Weyl’s theorem, we have |σ∗i − σi| ≤ |||Y −M∗|||op for all i ∈ [d1 ∧ d2]. Since σ∗r+1 = 0, we have σr+1 ≤ |||Y −M∗|||op. Recall that U0V >0 is the best rank r approximation of Y . Accordingly, we have\n|||U0V >0 −M∗|||op ≤ |||U0V >0 − Y |||op + |||Y −M∗|||op = σr+1 + |||Y −M∗|||op ≤ 2|||Y −M∗|||op ≤ 8αµrσ∗1.\nUnder condition αµr ≤ 116κ , we obtain |||U0V > 0 −M∗|||op ≤ 12σ ∗ r . Applying Lemma 5.14 in [25]\n(we provide it as Lemma 15 for the sake of completeness), we obtain\nd2(U0, V0;U ∗, V ∗) ≤ 2√ 2− 1 |||U0V >0 −M∗|||2F σ∗r ≤ 10r|||U0V >0 −M∗|||2op σ∗r .\nPlugging the upper bound of |||U0V >0 −M∗|||op into the above inequality completes the proof."
    }, {
      "heading" : "6.2 Proof of Theorem 2",
      "text" : "We essentially follow the general framework developed in [11] for analyzing the behaviors of gradient descent in factorized low-rank optimization. But it is worth to note that [11] only studies the symmetric and positive semidefinite setting, while we avoid such constraint on M∗. The techniques for analyzing general asymmetric matrix in factorized space is inspired by the recent work [25] on solving low-rank matrix equations. In our setting, the technical challenge is to verify the local descent condition of the loss function (8), which not only has a bilinear dependence on U and V , but also involves our sparse estimator (4).\nWe begin with some notations. Define the equivalent set of optimal solution as E(M∗) := { (A,B) ∈ Rd1×r × Rd2×r ∣∣ A = L∗Σ∗1/2Q,B = R∗Σ∗1/2Q, where Q ∈ Qr} . (18) Given (U0, V0) ∈ B2 ( c2 √ σ∗r/κ ) , by (11), we have |||U0V >0 −M∗|||op ≤ 12σ ∗ r when c2 is sufficiently\nsmall. By Weyl’s theorem We thus have√ σ∗1/2 ≤ |||U0|||op ≤ √ 3σ∗1/2, and √ σ∗1/2 ≤ |||V0|||op ≤ √ 3σ∗1/2.\nAs a result, for U ,V constructed according to (7), we have\nE(M∗) ⊆ U × V, and U ⊆ U ,V ⊆ V, (19)\nwhere\nU := { A ∈ Rd1×r ∣∣ |||A|||2,∞ ≤√3µrσ∗1 d1 } , V := { A ∈ Rd2×r ∣∣ |||A|||2,∞ ≤√3µrσ∗1 d2 } .\nWe let G(U, V ) := 1\n8 |||U>U − V >V |||2F. (20)\nFor L(U, V ;S), we denote the gradient with respect to M by ∇ML(U, V ;S), i.e. ∇ML(U, V ;S) = UV > + S − Y .\nThe local descent property is implied by combining the following two results, which are proved in Section 6.7 and 6.8 respectively. Lemma 2 (Local descent property of L). Suppose U ,V satisfy (19). For any (U, V ) ∈ (U × V) ∩ B2( √ σ∗1), we let S = Tγα [ Y − UV > ] , where we choose γ = 2. Then we have that for (Uπ∗ , Vπ∗) ∈ argmin(A,B)∈E(M∗) |||U −A|||2F + |||V −B|||2F and β > 0,\n〈〈∇ML(U, V ;S), UV > − Uπ∗V >π∗ + ∆U∆>V 〉〉 ≥ |||UV > − Uπ∗V >π∗ |||2F − νσ∗1δ − 3 √ σ∗1δ 3. (21)\nHere ∆U := U − Uπ∗, ∆V := V − Vπ∗, δ := |||∆U |||2F + |||∆V |||2F, and ν := 9(β + 6)αµr + 5β−1.\nLemma 3 (Local descent property of G). For any (U, V ) ∈ B2( √ σ∗r ) and\n(Uπ∗ , Vπ∗) ∈ arg min (A,B)∈E(M∗) |||U −A|||2F + |||V −B|||2F,\nwe have\n〈〈∇UG(U, V ), U − Uπ∗〉〉+ 〈〈∇V G(U, V ), V − Vπ∗〉〉\n≥ 1 8 |||U>U − V >V |||2F + 1 8 σ∗rδ −\n√ σ∗1δ 3\n2 − 1 2 |||UV > − Uπ∗V >π∗ |||2F,\nwhere δ is defined according to Lemma 2.\nAs another key ingredient, we establish the following smoothness condition, proved in Section 6.9, which indicates that the Frobenius norm of gradient decreases as (U, V ) approaches the optimal manifold.\nLemma 4 (Smoothness). For any (U, V ) ∈ B2( √ σ∗1), we let S = Tγα [ Y − UV > ] , where we choose γ = 2. We have that |||∇ML(U, V ;S)|||2F ≤ 6|||UV > −M∗|||2F, (22)\nand |||∇UG(U, V )|||2F + |||∇V G(U, V )|||2F ≤ 2σ∗1|||U>U − V >V |||2F. (23)\nWith the above results in hand, we are ready to prove Theorem 2.\nProof of Theorem 2. We use shorthands\nδt := d 2(Ut, Vt;U ∗, V ∗), Lt := L(Ut, Vt;St), and Gt := G(Ut, Vt).\nFor (Ut, Vt), let (U tπ∗ , V tπ∗) := argmin(A,B)∈E(M∗) |||Ut − A|||2F + |||Vt − B|||2F. Define ∆tU := Ut − U tπ∗ , ∆tV := Vt − V tπ∗ .\nWe prove Theorem 2 by induction. It’s sufficient to consider one step of iteration. For any t ≥ 0, under the induction hypothesis (Ut, Vt) ∈ B2 ( c2 √ σ∗r/κ ) . We find that\nδt+1 ≤ |||Ut+1 − U tπ∗ |||2F + |||Vt+1 − V tπ∗ |||2F ≤ |||Ut − η∇ULt − η∇UGt − U tπ∗ |||2F + |||Vt − η∇V Lt − η∇V Gt − V tπ∗ |||2F ≤ δt − 2η 〈〈∇ULt +∇UGt, Ut − U tπ∗〉〉︸ ︷︷ ︸\nW1\n−2η 〈〈∇V Lt +∇V Gt, Vt − V tπ∗〉〉︸ ︷︷ ︸ W2\n+ η2 |||∇ULt +∇UGt|||2F︸ ︷︷ ︸ W3 +η2 |||∇V Lt +∇V Gt|||2F︸ ︷︷ ︸ W4 , (24)\nwhere the second step follows from the non-expansion property of projection onto U ,V, which is implied by E(M∗) ⊆ U × V shown in (19). Since ∇ULt = [∇MLt]V and ∇V Lt = [∇MLt]> U , we have\n〈〈∇ULt, Ut − U tπ∗〉〉+ 〈〈∇V Lt, Vt − V tπ∗〉〉 = 〈〈∇MLt, UtV >t − U tπ∗V t>π∗ + ∆tU∆t>V 〉〉.\nCombining Lemma 2 and 3, under condition δt < σ∗r , we have that\nW1 +W2 ≥ 1 2 |||UtV >t −M∗|||2F + 1 8 |||U>t Ut − V >t Vt|||2F + 1 8 σ∗rδt − νσ∗1δt − 4\n√ σ∗1δ 3 t .\nOn the other hand, we have\nW3 +W4 ≤ 2|||∇ULt|||2F + 2|||∇UGt|||2F + 2|||∇V Lt|||2F + 2|||∇V Gt|||2F ≤ 2(|||Ut|||2op + |||Vt|||2op)|||∇MLt|||2F + 2|||∇UGt|||2F + 2|||∇V Gt|||2F ≤ 36σ∗1|||UtV >t −M∗|||2F + 4σ∗1|||U>t Ut − V >t Vt|||2F,\nwhere the last step is implied by Lemma 4 and the assumption (Ut, Vt) ∈ B2 ( c2 √ σ∗r/κ ) that leads\nto |||Ut|||op ≤ √ 3σ∗1/2, |||Vt|||op ≤ √\n3σ∗1/2. By the assumption η = c/σ∗1 for any constant c ≤ 1/36, we thus have\n−2η(W1 +W2) + η2(W3 +W4) ≤ − 1 4 ησ∗rδt + 2ηνσ ∗ 1δt + 8η\n√ σ∗1δ 3 t .\nIn Lemma 2, choosing β = 320κ and assuming α . 1/(κ2µr), we can have ν ≤ 1/(32κ). By assuming δt . σ∗r/κ leads to 14 √ σ∗1δ 3 t ≤ 116σ ∗ rδt. We thus obtain\nδt+1 ≤ ( 1− ησ ∗ r\n8\n) δt. (25)\nUnder initial condition δ0 . σ∗r/κ, since estimation error decays geometrically after each iteration, then such condition holds for all t. Then applying (25) for all iterations, we conclude that for all t = 0, 1, . . .,\nδt ≤ ( 1− ησ ∗ r\n8\n)t δ0."
    }, {
      "heading" : "6.3 Proof of Corollary 1",
      "text" : "We need α . 1 κ2µr due to the condition of Theorem 2. In order to ensure the linear convergence happens, it suffices to let the initial error shown in Theorem 1 be less than the corresponding condition in Theorem 2. Accordingly, we need\n28 √ καµr √ r √ σ∗1 . √ σ∗r/κ,\nwhich leads to α . 1 µ √ rκ 3 . Using the conclusion that gradient descent has linear convergence, choosing T = O(κ log(1/ε)), we have d2(UT , VT ;U\n∗, V ∗) ≤ ε2d2(U0, V0;U∗, V ∗) . ε2 σ∗r κ .\nFinally, applying the relationship between d(UT , VT ;U∗, V ∗) and |||UTV >T −M∗|||F shown in (11), we complete the proof."
    }, {
      "heading" : "6.4 Proof of Theorem 3",
      "text" : "Let Y := 1p(Y − Sinit). Similar to the proof of Theorem 1, we first establish an upper bound on |||Y −M∗|||op. We have that\n|||Y −M∗|||op ≤ |||Y − 1\np ΠΦM\n∗|||op + ||| 1\np ΠΦ(M\n∗)−M∗|||op. (26)\nFor the first term, we have Y − 1pΠΦM ∗ = 1p(ΠΦ(S ∗)−Sinit) because Y = ΠΦ(M∗+S∗). Lemma 10 shows that under condition p & log dα(d1∧d2) , there are at most 3 2pα-fraction nonzero entries in each row and column of ΠΦ(S∗) with high probability. Since Sinit ∈ S2pα, we have\nΠΦ(S ∗)− Sinit ∈ S4pα. (27)\nIn addition, we prove below that\n‖ΠΦ(S∗)− Sinit‖∞ ≤ 2‖M∗‖∞. (28)\nDenote the support of ΠΦ(S∗) and Sinit by Ω∗o and Ω. For (i, j) ∈ Ω∗o ∩ Ω and (i, j) ∈ Ω \\ Ω∗o, we have (ΠΦ(S∗)− Sinit)(i,j) = 0 and (ΠΦ(S∗)− Sinit)(i,j) = −M∗(i,j), respectively. To prove the claim, it remains to show that for (i, j) ∈ Ω∗o \\Ω, |S∗(i,j)| < 2‖M\n∗‖∞. If this is not true, then we must have |Y(i,j)| > ‖M∗‖∞. Accordingly, |Y(i,j)| is larger than the magnitude of any uncorrupted entries in its row and column. Note that on the support Φ, there are at most 32pα corruptions per row and column, we have (i, j) ∈ Ω, which violates our prior condition (i, j) ∈ Ω∗o \\ Ω.\nUsing these two properties (27), (28) and applying Lemma 1, we have\n|||Y − 1 p ΠΦM ∗|||op ≤ 4α\n√ d1d2‖ΠΦ(S∗)− Sinit‖∞ ≤ 8α √ d1d2‖M∗‖∞ ≤ 8αµrσ∗1, (29)\nwhere the last step follow from (17). For the second term in (26), we use the following lemma proved in [10].\nLemma 5 (Lemma 2 in [10]). Suppose A ∈ Rd1×d2 is a fixed matrix. We let d := max{d1, d2}. There exists a constant c such that with probability at least 1−O(d−1),\n|||1 p ΠΦ(A)−A|||op ≤ c\n( log d\np ‖A‖∞ +\n√ log d\np max\n{ |||A|||2,∞, |||A>|||2,∞ }) .\nGiven the SVD M∗ = L∗ΣR∗>, for any i ∈ [d1], we have\n‖M∗(i,·)‖2 = ‖L ∗ (i,·)ΣR ∗>‖2 ≤ σ∗1‖L∗(i,·)‖2 ≤ σ ∗ 1\n√ µr\nd1 .\nWe can bound |||M∗>|||2,∞ similarly. Lemma 5 leads to\n|||1 p ΠΦ(M ∗)−M∗|||op ≤ c′ log d p µr√ d1d2 σ∗1 + c ′\n√ log d\np\n√ µr\nd1 ∧ d2 σ∗1 ≤ c′ σ∗1/\n√ r (30)\nunder condition p ≥ 4µr 2 log d\n2(d1∧d2) . Putting (29) and (30) together, we obtain\n|||Y −M∗|||op ≤ 8αµrσ∗1 + c′ σ∗1/ √ r.\nThen using the fact that U0V >0 is the best rank r approximation of Y and applying Wely’s theorem (see the proof of Theorem 1 for a detailed argument), we have\n|||U0V >0 −M∗|||op ≤ |||U0V >0 − Y |||op + |||Y −M∗|||op ≤ 2|||Y −M∗|||op ≤ 16αµrσ∗1 + 2c′ σ∗1/ √ r\nUnder our assumptions, we have 16αµrσ∗1 + 2c′ σ∗1/ √ r ≤ 12σ ∗ r . Accordingly, Lemma 15 gives\nd2(U0, V0;U ∗, V ∗) ≤ 2√ 2− 1 |||U0V >0 −M∗|||2F σ∗r ≤ 10r|||U0V >0 −M∗|||2op σ∗r .\nWe complete the proof by combining the above two inequalities."
    }, {
      "heading" : "6.5 Proof of Theorem 4",
      "text" : "In this section, we turn to prove Theorem 4. Similar to the proof of Theorem 2, we rely on establishing the local descent and smoothness conditions. Compared to the full observation setting, we replace L by L̃ given in (6), while the regularization term G̃(U, V ) := 164 |||U\n>U − V >V |||2F merely differs from G(U, V ) given in (20) by a constant factor. It is thus sufficient to analyze the properties of L̃.\nDefine E(M∗) according to (18). Under the initial condition, we still have\nE(M∗) ⊆ U × V, and U ⊆ U ,V ⊆ V. (31)\nWe prove the next two lemmas in Section 6.10 and 6.11 respectively. In both lemmas, for any (U, V ) ∈ U × V, we use shorthands\n(Uπ∗ , Vπ∗) = arg min (A,B)∈E(M∗)\n|||U −A|||2F + |||V −B|||2F,\n∆U := U − Uπ∗ , ∆V := V − Vπ∗ , and δ := |||∆U |||2F + |||∆V |||2F. Recall that d := max{d1, d2}.\nLemma 6 (Local descent property of L̃). Suppose U ,V satisfy (31). Suppose we let\nS = Tγpα [ ΠΦ ( Y − UV > )] ,\nwhere we choose γ = 3. For any β > 0 and ∈ (0, 14), we define ν := (14β+81)αµr+26 √ +18β−1. There exist constants {ci}2i=1 such that if\np ≥ c1 ( µ2r2\n2 +\n1 α + 1\n) log d\nd1 ∧ d2 , (32)\nthen with probability at least 1− c2d−1,\n〈〈∇M L̃(U, V ;S), UV > − Uπ∗V >π∗ + ∆U∆>V 〉〉 ≥ 3 16 |||UV >−Uπ∗V >π∗ |||2F− νσ∗1δ− 10\n√ σ∗1δ 3− 2δ2 (33)\nfor all (U, V ) ∈ (U × V) ∩ B2 (√ σ∗1 ) .\nLemma 7 (Smoothness of L̃). Suppose U ,V satisfy (31). Suppose we let S = Tγαp [ ΠΦ ( Y − UV > )] for γ = 3. There exist constants {ci}3i=1 such that for any ∈ (0, 14), when p satisfies condition (32), with probability at least 1− c2d−1, we have that for all (U, V ) ∈ (U × V) ∩ B2( √ σ∗1),\n|||∇U L̃(U, V ;S)|||2F + |||∇V L̃(U, V ;S)|||2F ≤ c3 [ µrσ∗1|||UV > − Uπ∗V >π∗ |||2F + µrσ∗1δ(δ + σ∗1) ] . (34)\nIn the remainder of this section, we condition on the events in Lemma 6 and 7. Now we are ready to prove Theorem 4.\nProof of Theorem 4. We essentially follow the process for proving Theorem 2. Let the following shorthands be defined in the same fashion: δt, (U tπ∗ , V tπ∗), (∆tU ,∆ t V ), L̃t, G̃t.\nHere we show error decays in one step of iteration. The induction process is the same as the proof of Theorem 2, and is thus omitted. For any t ≥ 0, similar to (24) we have that\nδt+1 ≤ δt − 2η 〈〈∇U L̃t +∇U G̃t, Ut − U tπ∗〉〉︸ ︷︷ ︸ W1 −2η 〈〈∇V L̃t +∇V G̃t, Vt − V tπ∗〉〉︸ ︷︷ ︸ W2\n+ η2 |||∇U L̃t +∇U G̃t|||2F︸ ︷︷ ︸ W3 +η2 |||∇V L̃t +∇V G̃t|||2F︸ ︷︷ ︸ W4 .\nWe also have\n〈〈∇U L̃t, Ut − U tπ∗〉〉+ 〈〈∇V L̃t, Vt − V tπ∗〉〉 = 〈〈∇M L̃t, UtV >t − U tπ∗V t>π∗ + ∆tU∆t>V 〉〉,\nwhich can be lower bounded by Lemma 6. Note that G̃ differs from G by a constant, we can still leverage Lemma 3. Hence, we obtain that\nW1 +W2 ≥ 1 8 |||UtV >t −M∗|||2F + 1 64 |||U>t Ut − V >t Vt|||2F + 1 64 σ∗rδt − νσ∗1δt − 11\n√ σ∗1δ 3 t − 2δ2t .\nOn the other hand, we have\nW3 +W4 ≤ 2|||∇U L̃t|||2F + 2|||∇U G̃t|||2F + 2|||∇V L̃t|||2F + 2|||∇V G̃t|||2F ≤ c [ µrσ∗1|||UtV >t −M∗|||2F + µrσ∗1δt(δt + σ∗1) + σ∗1|||U>t Ut − V >t Vt|||2F ] ,\nwhere c is a constant, and the last step is implied by Lemma 4 and Lemma 7. By the assumption η = c′/[µrσ∗1] for sufficiently small constant c′, we thus have\n−2η(W1 +W2) + η2(W3 +W4) ≤ − 1 32 ησ∗rδt + 2ηνσ ∗ 1δt + 22η\n√ σ∗1δ 3 t + 4ηδ 2 t .\nRecall that ν := (14β + 81)αµr + 26 √ + 18β−1. By letting β = c1κ, = c2/κ2 and assuming α ≤ c3/(µrκ2) and δt ≤ c4σ∗r/κ for some sufficiently small constants {ci}4i=1, we can have −2η(W1 + W2) + η 2(W3 +W4) ≤ − 164ησ ∗ rδt, which implies that\nδt+1 ≤ ( 1− ησ ∗ r\n64\n) δt,\nand thus completes the proof."
    }, {
      "heading" : "6.6 Proof of Corollary 2",
      "text" : "We need α . 1 µκ2r due to the condition of Theorem 4. Letting the initial error provided in Theorem 3 be less than the corresponding condition in Theorem 4, we have\n51 √ καµr √ r √ σ∗1 + 7c1 √ κσ∗1 . √ σ∗r/κ,\nwhich leads to α .\n1\nµ √ r3κ3 , . 1√ κ3 .\nPlugging the above two upper bounds into the second term in (13), it suffices to have\np & κ3µr2 log d\nd1 ∧ d2 .\nComparing the above bound with the second term in (14) completes the proof."
    }, {
      "heading" : "6.7 Proof of Lemma 2",
      "text" : "Let M := UV >. We observe that\n∇ML(U, V ;S) = M + S −M∗ − S∗.\nPlugging it back into the left hand side of (21), we obtain\n〈〈∇ML(U, V ;S), UV > − Uπ∗V >π∗ + ∆U∆>V 〉〉 = 〈〈M + S −M∗ − S∗, M −M∗ + ∆U∆>V 〉〉 ≥ |||M −M∗|||2F − |〈〈S − S∗, M −M∗〉〉|︸ ︷︷ ︸\nT1\n− |〈〈M + S −M∗ − S∗, ∆U∆>V 〉〉|︸ ︷︷ ︸ T2 . (35)\nNext we derive upper bounds of T1 and T2 respectively.\nUpper bound of T1. We denote the support of S, S∗ by Ω and Ω∗ respectively. Since S − S∗ is supported on Ω∗ ∪ Ω, we have\nT1 ≤ |〈〈ΠΩ(S − S∗), M −M∗〉〉|︸ ︷︷ ︸ W1 + |〈〈ΠΩ∗\\Ω(S − S∗), M −M∗〉〉|︸ ︷︷ ︸ W2 .\nRecall that for any (i, j) ∈ Ω, we have S(i,j) = (M∗ + S∗ −M)(i,j). Accordingly, we have\nW1 = |||ΠΩ(M −M∗)|||2F. (36)\nNow we turn to bound W2. Since S(i,j) = 0 for any (i, j) ∈ Ω∗ \\ Ω, we have\nW2 = |〈〈ΠΩ∗\\ΩS∗, M −M∗〉〉|.\nLet ui be the i-th row of M −M∗, and vj be the j-th column of M −M∗. For any k ∈ [d2], we let u\n(k) i denote the element of ui that has the k-th largest magnitude. Similarly, for any k ∈ [d1], we let v(k)j denote the element of vj that has the k-th largest magnitude. From the design of sparse estimator (4), we have that for any (i, j) ∈ Ω∗ \\Ω, |(M∗+S∗−M)(i,j)| is either smaller than the γαd2-th largest entry of the i-th row of M∗ + S∗ −M or smaller than the γαd1-th largest entry of the j-th column of M∗ + S∗ −M . Note that S∗ only contains at most α-fraction nonzero entries per row and column. As a result, |(M∗+S∗−M)(i,j)| has to be less than the magnitude of u(γαd2−αd2)i or v (γαd1−αd1) j . Formally, we have for (i, j) ∈ Ω∗ \\ Ω,\n|(M∗ + S∗ −M)(i,j)| ≤ max { |u(γαd2−αd2)i |, |v (γαd1−αd1) j | } ︸ ︷︷ ︸\nbij\n. (37)\nFurthermore, we obtain\nb2ij ≤ |u (γαd2−αd2) i | 2 + |v(γαd1−αd1)j | 2 ≤ ‖ui‖ 2 2\n(γ − 1)αd2 + ‖vj‖22 (γ − 1)αd1 . (38)\nMeanwhile, for any (i, j) ∈ Ω∗ \\ Ω, we have\n|S∗(i,j) · (M −M ∗)(i,j)| = |(M∗ + S∗ −M −M∗ +M)(i,j) · (M −M∗)(i,j)|\n≤ |(M −M∗)(i,j)|2 + |(M∗ + S∗ −M)(i,j)| · |(M −M∗)(i,j)| ≤ |(M −M∗)(i,j)|2 + bij · |(M −M∗)(i,j)|\n≤ ( 1 + β\n2\n) |(M −M∗)(i,j)|2 +\nb2ij 2β , (39)\nwhere β in the last step can be any positive number. Combining (38) and (39) leads to W2 ≤ ∑\n(i,j)∈Ω∗\\Ω\n|S∗(i,j) · (M −M ∗)(i,j)|\n≤ ( 1 + β\n2\n) |||ΠΩ∗\\Ω(M −M∗)|||2F + ∑ (i,j)∈Ω∗\\Ω b2ij 2β\n≤ ( 1 + β\n2\n) |||ΠΩ∗\\Ω(M −M∗)|||2F + 1\n2β ∑ (i,j)∈Ω∗\\Ω ( ‖ui‖22 (γ − 1)αd2 + ‖vj‖22 (γ − 1)αd1 )\n≤ ( 1 + β\n2\n) |||ΠΩ∗\\Ω(M −M∗)|||2F +\n1\nβ(γ − 1) |||M −M∗|||2F. (40)\nIn the last step, we use∑ (i,j)∈Ω∗\\Ω ( 1 d2 ‖ui‖22 + 1 d1 ‖vj‖22 ) ≤ ∑ (i,j)∈Ω∗ ( 1 d2 ‖ui‖22 + 1 d1 ‖vj‖22 ) ≤ ∑ i∈[d] ∑ j∈Ω∗\n(i,·)\n1\nd2 ‖ui‖22 + ∑ j∈[d] ∑ i∈Ω∗\n(·,j)\n1\nd1 ‖vj‖22\n≤ α ∑ i∈[d] ‖ui‖22 + α ∑ j∈[d] ‖vj‖22 = 2α|||M −M∗|||2F. (41)\nWe introduce shorthand δ := |||∆U |||2F + |||∆V |||2F. We prove the following inequality in the end of this section.\n|||M −M∗|||F ≤ √ 5σ∗1δ. (42)\nCombining (36), (40) and (42) leads to T1 ≤ |||ΠΩ(M −M∗)|||2F + ( 1 + β\n2\n) |||ΠΩ∗\\Ω(M −M∗)|||2F + 5σ∗1δ\nβ(γ − 1)\n≤ 9(2γ + β + 2)αµrσ∗1δ + 5σ∗1δ\nβ(γ − 1) , (43)\nwhere the last step follows from Lemma 14 by noticing that ΠΩ(M −M∗) has at most γα-fraction nonzero entries per row and column.\nUpper bound of T2. To ease notation, we let C := M + S −M∗ − S∗. We observe that C is supported on Ωc, we have\nT2 ≤ |〈〈ΠΩ∗c∩Ωc(M −M∗), ∆U∆>V 〉〉|︸ ︷︷ ︸ W3 + |〈〈ΠΩ∗∩ΩcC, ∆U∆>V 〉〉|︸ ︷︷ ︸ W4 .\nBy Cauchy-Swartz inequality, we have\nW3 ≤ |||ΠΩ∗c∩Ωc(M −M∗)|||F|||∆U∆>V |||F ≤ |||M −M∗|||F|||∆U |||F|||∆V |||F ≤ √ 5σ∗1δ 3/2,\nwhere the last step follows from (42) and |||∆U |||F|||∆V |||F ≤ δ/2. It remains to bound W4. By Cauchy-Swartz inequality, we have\nW4 ≤ |||ΠΩ∗∩ΩcC|||F|||∆U∆>V |||F ≤ |||ΠΩ∗∩Ωc(M∗ + S∗ −M)|||F|||∆U∆>V |||F\n(a) ≤ √ ∑\n(i,j)∈Ω∗\\Ω\nb2ij |||∆U |||F|||∆V |||F (b) ≤  ∑ (i,j)∈Ω∗\\Ω ‖ui‖22 (γ − 1)αd2 + ‖vj‖22 (γ − 1)αd1 1/2 |||∆U |||F|||∆V |||F. (c)\n≤ √ 2\nγ − 1 |||M −M∗|||F|||∆U |||F|||∆V |||F ≤\n√ 5σ∗1δ 3\n2(γ − 1) ,\nwhere step (a) is from (37), step (b) follows from (38), and step (c) follows from (41). Combining the upper bounds of W3 and W4, we obtain\nT2 ≤ √ 5σ∗1δ 3/2 + √ 5σ∗1δ 3\n2(γ − 1) . (44)\nCombining pieces. Now we choose γ = 2. Then inequality (43) implies that\nT1 ≤ [9(β + 6)αµr + 5β−1]σ∗1δ.\nInequality (44) then implies that\nT2 ≤ 3 √ σ∗1δ 3.\nPlugging the above two inequalities into (35) completes the proof.\nProof of inequality (42). We find that\n|||M −M∗|||2F ≤ [√ σ∗1(|||∆V |||F + |||∆U |||F) + |||∆U |||F|||∆V |||F ]2\n≤ [√ σ∗1(|||∆V |||F + |||∆U |||F) + 1\n2\n√ σ∗1|||∆U |||F + 1\n2\n√ σ∗1|||∆V |||F ]2 ≤ 5σ∗1(|||∆U |||2F + |||∆V |||2F),\nwhere the first step follows from the upper bound of |||M −M∗|||F shown in Lemma 12, and the second step follows from the assumption |||∆U |||F, |||∆V |||F ≤ √ σ∗1."
    }, {
      "heading" : "6.8 Proof of Lemma 3",
      "text" : "We first observe that\n∇UG(U, V ) = 1 2 U(U>U − V >V ), ∇V G(U, V ) = 1 2 V (V >V − U>U),\nTherefore, we obtain\n〈〈∇UG(U, V ), U − Uπ∗〉〉+ 〈〈∇V G(U, V ), V − Vπ∗〉〉\n= 1\n2 〈〈U>U − V >V , U>U − V >V − U>Uπ∗ + V >Vπ∗〉〉\n= 1\n4 |||U>U − V >V |||2F +\n1 4 〈〈U>U − V >V , U>U − V >V − 2U>Uπ∗ + 2V >Vπ∗〉〉. (45)\nNote that\nU>U − V >V = (Uπ∗ + ∆U )>(Uπ∗ + ∆U )− (Vπ∗ + ∆V )>(Vπ∗ + ∆V ) = U>π∗∆U + ∆ > UUπ∗ + ∆ > U∆U − V >π∗∆V −∆>V Vπ∗ −∆>V ∆V ,\nwhere we use U>π∗Uπ∗ = V >π∗Vπ∗ in the last step. Furthermore, since U>U − V >V is symmetric, we have\n〈〈U>U − V >V , U>π∗∆U + ∆>UUπ∗ − V >π∗∆V −∆>V Vπ∗〉〉 = 〈〈U>U − V >V , 2∆>UUπ∗ − 2∆>V Vπ∗〉〉.\nUsing these arguments, for the second term in (45), denoted by T2, we have\nT2 = 1 4 〈〈U>U − V >V , ∆>U∆U + ∆>V ∆V 〉〉.\nMoreover, we have 4T2 ≤ |〈〈U>U − V >V , ∆>U∆U + ∆>V ∆V 〉〉| ≤ |||U>U − V >V |||F ( |||∆U |||2F + |||∆V |||2F ) ≤ ( |||U>U − U>π∗Uπ∗ |||F + |||V >V − V >π∗Vπ∗ |||F ) δ\n≤ 2 (|||Uπ∗ |||op|||∆U |||F + |||Vπ∗ |||op|||∆V |||F) δ ≤ 2 √ 2σ∗1δ 3. (46)\nWe still need to find a lower bound of |||U>U −V >V |||F. The following inequality,which we prove later, is true:\n|||U>U − V >V |||2F ≥ |||UU> − Uπ∗U>π∗ |||2F + |||V V > − Vπ∗V >π∗ |||2F − 2|||UV > − Uπ∗V >π∗ |||2F. (47)\nProceeding with the first term in (45) by using (47), we get\n1 4 |||U>U − V >V |||2F = 1 8 |||U>U − V >V |||2F + 1 8 |||U>U − V >V |||2F ≥ 1 8 |||U>U − V >V |||2F + 1 8 |||UU> − Uπ∗U>π∗ |||2F + 1 8 |||V V > − Vπ∗V >π∗ |||2F − 1 4 |||UV > − Uπ∗V >π∗ |||2F = 1\n8 |||U>U − V >V |||2F +\n1 8 |||FF> − Fπ∗F>π∗ |||2F − 1 2 |||UV > − Uπ∗V >π∗ |||2F, (48)\nwhere we let\nF := [ U V ] , Fπ∗ := [ Uπ∗\nVπ∗\n] .\nIntroduce ∆F := F −Fπ∗ . Recall that δ := |||∆U |||2F + |||∆V |||2F. Equivalently δ = |||∆F |||2F. We have\n|||FF> − Fπ∗F>π∗ |||F = |||∆FF>π∗ + Fπ∗∆>F + ∆F∆>F |||F ≥ |||∆FF>π∗ + Fπ∗∆>F |||F − |||∆F |||2F = |||∆FF>π∗ + Fπ∗∆>F |||F − δ.\nFor the first term, we have\n|||∆FF>π∗ + Fπ∗∆>F |||2F = 2|||∆FF>π∗|||2F + 〈〈∆FF>π∗, Fπ∗∆>F 〉〉 ≥ 2σr(Fπ∗)2|||∆F |||2F + 〈〈∆FF>π∗, Fπ∗∆>F 〉〉 = 4σ∗r |||∆F |||2F + 〈〈∆FF>π∗, Fπ∗∆>F 〉〉.\nFor the cross term, by the following result, proved in [11] (we also provide a proof in Section 7.5 for the sake of completeness), we have 〈〈∆FF>π∗, Fπ∗∆>F 〉〉 ≥ 0. Lemma 8. When |||F − Fπ∗ |||op < √ 2σ∗r , we have that ∆>FFπ∗ is symmetric.\nAccordingly, we have |||FF>−Fπ∗F>π∗ |||F ≥ 2 √ σ∗rδ−δ ≥ √ σ∗rδ under condition δ ≤ σ∗r . Plugging\nthis lower bound into (48), we obtain\n1 4 |||U>U − V >V |||2F ≥ 1 8 |||U>U − V >V |||2F + 1 8 σ∗rδ − 1 2 |||UV > − Uπ∗V >π∗ |||2F.\nPutting (45), (46) and the above inequality together completes the proof.\nProof of inequality (47). For the term on the left hand side of (47), it is easy to check that\n|||U>U − V >V |||2F = |||UU>|||2F + |||V V >|||2F − 2|||UV >|||2F. (49)\nThe property U>π∗Uπ∗ = V >π∗Vπ∗ implies that |||Uπ∗U>π∗ |||F = |||Vπ∗V >π∗ |||F = |||Uπ∗V >π∗ |||F. Therefore, expanding those quadratic terms on the right hand side of (47), one can show that it is equal to\n|||UU>|||2F + |||V V >|||2F − 2|||U>π∗U |||2F − 2|||V >π∗V |||2F + 4〈〈U>π∗U, V >π∗V 〉〉 − 2|||UV >|||2F. (50)\nComparing inequalities (49) and (50), it thus remains to show that\n−2|||U>π∗U |||2F − 2|||V >π∗V |||2F + 4〈〈U>π∗U, V >π∗V 〉〉 ≤ 0.\nEquivalently, we always have |||U>π∗U − V >π∗V |||2F ≥ 0, and thus prove (47)."
    }, {
      "heading" : "6.9 Proof of Lemma 4",
      "text" : "First, we turn to prove (23). As\n∇UG(U, V ) = 1 2 U(U>U − V >V ), ∇V G(U, V ) = 1 2 V (V >V − U>U),\nwe have |||∇UG(U, V )|||2F + |||∇V G(U, V )|||2F ≤ 1\n4\n( |||U |||2op + |||V |||2op ) |||U>U − V >V |||2F.\nAs (U, V ) ∈ B2( √ σ∗1), we thus have |||U |||op ≤ |||Uπ∗ |||op + |||Uπ∗ − U |||op ≤ 2 √ σ∗1, and similarly\n|||V |||op ≤ 2 √ σ∗1. We obtain\n|||∇UG(U, V )|||2F + |||∇V G(U, V )|||2F ≤ 2σ∗1|||U>U − V >V |||2F.\nNow we turn to prove (22). We observe that\n∇ML(U, V ;S) = M + S −M∗ − S∗,\nwhere we let M := UV >. We denote the support of S, S∗ by Ω and Ω∗ respectively. Based on the sparse estimator (4) for computing S, ∇ML(U, V ;S) is only supported on Ωc. We thus have\n|||∇ML(U, V ;S)|||F ≤ |||ΠΩc\\Ω∗(M −M∗)|||F + |||ΠΩc∩Ω∗(M −M∗ − S∗)|||F ≤ |||M −M∗|||F + |||ΠΩc∩Ω∗(M −M∗ − S∗)|||F.\nIt remains to upper bound the second term on the right hand side. Following (37) and (38), we have\n|||ΠΩc∩Ω∗(M −M∗ − S∗)|||2F ≤ ∑\n(i,j)∈Ωc∩Ω∗\n‖ui‖22 (γ − 1)αd2 + ‖vj‖22 (γ − 1)αd1 ≤ 2 γ − 1 |||M −M∗|||2F,\nwhere the last step is proved in (41). By choosing γ = 2, we thus conclude that\n|||∇ML(U, V ;S)|||F ≤ (1 + √ 2)|||M −M∗|||F."
    }, {
      "heading" : "6.10 Proof of Lemma 6",
      "text" : "We denote the support of ΠΦ(S∗), S by Ω∗o and Ω. We always have Ω∗o ⊆ Φ and Ω ⊆ Φ. In the sequel, we establish several results that characterize the properties of Φ. The first result, proved in Section 7.2, shows that the Frobenius norm of any incoherent matrix whose row (or column) space are equal to L∗ (or R∗) is well preserved under partial observations supported on Φ.\nLemma 9. Suppose M∗ ∈ Rd1×d2 is a rank r and µ-incoherent matrix that has SVD M∗ = L∗Σ∗R∗>. Then there exists an absolute constant c such that for any ∈ (0, 1), if p ≥ c µr log d\n2(d1∧d2) , then with probability at least 1− 2d−3, we have that for all A ∈ Rd2×r, B ∈ Rd1×r,\n(1− )|||L∗A> +BR∗>|||2F ≤ p−1|||ΠΦ ( L∗A> +BR∗> ) |||2F ≤ (1 + )|||L∗A> +BR∗>|||2F.\nWe need the next result, proved in Section 7.3, to control the number of nonzero entries per row and column in Ω∗o and Φ.\nLemma 10. If p ≥ 563 log d α(d1∧d2) , then with probability at least 1− 6d −1, we have ∣∣|Φ(i,·)| − pd2∣∣ ≤ 12pd2, ∣∣|Φ(·,j)| − pd1∣∣ ≤ 12pd1, |Ω∗o(i,·)| ≤ 32αpd2, |Ω∗o(·,j)| ≤ 32αpd1, for all i ∈ [d1] and j ∈ [d2].\nThe next lemma, proved in Section 7.4, can be used to control the projection of small matrices to Φ.\nLemma 11. There exists constant c such that for any ∈ (0, 1), if p ≥ c µ 2r2 log d\n2(d1∧d2) , then with probability at least 1−O(d−1), for all matrices Z ∈ Rd1×d2, U ∈ Rd1×r and V ∈ Rd2×r that satisfy |||U |||2,∞ ≤ √ µr/d1,|||V |||2,∞ ≤ √ µr/d2, we have\np−1|||ΠΦ(UV >)|||2F ≤ |||U |||2F|||V |||2F + |||U |||F|||V |||F; (51)\np−1|||ΠΦ(Z)V |||2F ≤ 2µr|||ΠΦ(Z)|||2F; (52) p−1|||U>ΠΦ(Z)|||2F ≤ 2µr|||ΠΦ(Z)|||2F. (53)\nIn the remainder of this section, we condition on the events in Lemmas 9, 10 and 11. Now we are ready to prove Lemma 6.\nProof of Lemma 6. Using shorthand M := UV >, we have\n∇M L̃(U, V ;S) = p−1ΠΦ (M + S −M∗ − S∗) .\nPlugging it back into the left hand side of (21), we obtain\n〈〈∇M L̃(U, V ;S), UV > − Uπ∗V >π∗ + ∆U∆>V 〉〉\n= 1\np 〈〈ΠΦ (M + S −M∗ − S∗), M −M∗ + ∆U∆>V 〉〉\n≥ 1 p |||ΠΦ (M −M∗) |||2F︸ ︷︷ ︸\nT1\n− 1 p |〈〈ΠΦ (S − S∗), M −M∗〉〉|︸ ︷︷ ︸\nT2\n− 1 p |〈〈ΠΦ (M + S −M∗ − S∗), ∆U∆>V 〉〉|︸ ︷︷ ︸\nT3\n.\n(54)\nNext we derive lower bounds of T1, upper bounds of T2 and T3 respectively.\nLower bound of T1. We observe thatM−M∗ = U∗π∗∆>V +∆UV >π∗+∆U∆>V . By triangle inequality, we have |||ΠΦ(M −M∗)|||F ≥ |||ΠΦ(Uπ∗∆>V + ∆UV >π∗)|||F − |||ΠΦ(∆U∆>V )|||F. Note that when c ≥ a− b for a, b ≥ 0, we always have c2 ≥ 12a 2 − b2. We thus have\nT1 ≥ 1 2p |||ΠΦ(Uπ∗∆>V + ∆UV >π∗)|||2F − 1 p |||ΠΦ(∆U∆>V )|||2F\n≥ 1 2 (1− )|||Uπ∗∆>V + ∆UV >π∗ |||2F − 1 p |||ΠΦ(∆U∆>V )|||2F\n≥ 1 2 (1− )|||M −M∗ −∆U∆>V |||2F − |||∆U |||2F|||∆V |||2F − 9 σ∗1|||∆U |||F|||∆V |||F ≥ 1 4 (1− )|||M −M∗|||2F − 1 2 (1− )|||∆U∆V |||2F − |||∆U |||2F|||∆V |||2F − 9 σ∗1|||∆U |||F|||∆V |||F ≥ 1 4 (1− )|||M −M∗|||2F − 2δ2 − 5 σ∗1δ.\nwhere the second step is implied by Lemma 9, the third step follows from (51) in Lemma 11 by noticing that |||∆U |||2,∞ ≤ 3 √ µrσ∗1/d1 and |||∆V |||2,∞ ≤ 3 √ µrσ∗1/d1, which is further implied by (31).\nUpper bound of T2. Since S − S∗ is supported on Ω∗0 ∪ Ω, we have\npT2 ≤ |〈〈ΠΩ∗o\\Ω(S ∗), ΠΩ∗o\\Ω(M −M ∗)〉〉|+ |〈〈ΠΩ(S − S∗), ΠΩ(M −M∗)〉〉|. (55)\nFor any (i, j) ∈ Ω, we have (S − S∗)(i,j) = (M∗ −M)(i,j). Therefore, for the second term on the right hand side, we have\n|〈〈ΠΩ(S − S∗), ΠΩ(M −M∗)〉〉| ≤ |||ΠΩ(M −M∗)|||2F ≤ 18γpαµrσ∗1δ, (56)\nwhere the last inequality follows from Lemma 14 and the fact that |Ω(i,·)| ≤ γpαd2, |Ω(·,j)| ≤ γpαd1 for all i ∈ [d1], j ∈ [d2].\nWe denote the i-th row of ΠΦ(M −M∗) by ui, and we denote the j-th column of ΠΦ(M −M∗) by vj . We let u (k) i denote the element of ui that has the k-th largest magnitude. We let v (k) j denote the element of vj that has the k-th largest magnitude. For the first term on the right hand side of (55), we first observe that for (i, j) ∈ Ω∗o \\Ω, |(M∗+ S∗ −M)(i,j)| is either less than the γpαd2-th largest element in the i-th row of ΠΦ(M∗ + S∗ −M), or less than γpαd1-th largest element in the j-th row of ΠΦ(M∗ + S∗ −M). Based on Lemma 10, ΠΦ(S∗) has at most 3pαd2/2 nonzero entries per row and at most 3pαd1/2 nonzero entries per column. Therefore, we have\n|(M∗ + S∗ −M)(i,j)| ≤ max { |u((γ−1.5)pαd2)i |, |v ((γ−1.5)pαd1) j | } . (57)\nIn addition, we observe that\n|〈〈ΠΩ∗o\\Ω(S ∗), ΠΩ∗o\\Ω(M −M ∗)〉〉| ≤ ∑\n(i,j)∈Ω∗o\\Ω\n|(M∗ + S∗ −M)(i,j)||(M∗ −M)(i,j)|+ |(M∗ −M)(i,j)|2\n≤ ( 1 + β\n2\n) |||ΠΩ∗o(M ∗ −M)|||2F + 1\n2β ∑ (i,j)∈Ω∗o\\Ω |(M∗ + S∗ −M)(i,j)|2,\n≤ (27 + 14β)pαµrσ∗1δ + 1\n2β ∑ (i,j)∈Ω∗o\\Ω |(M∗ + S∗ −M)(i,j)|2, (58)\nwhere the second step holds for any β > 0 and the last step follows from Lemma 14 under the size constraints of Ω∗o shown in Lemma 10. For the second term in (58), using (57), we have∑\n(i,j)∈Ω∗o\\Ω\n|(M∗ + S∗ −M)(i,j)|2 ≤ ∑\n(i,j)∈Ω∗o\n|u((γ−1.5)pαd2)i | 2 + |v((γ−1.5)pαd1)j | 2\n= ∑ i∈[d1] ∑ j∈Ω∗\no(i,·)\n|u((γ−1.5)pαd2)i | 2 + ∑ j∈[d2] ∑ i∈Ω∗\no(·,j)\n|v((γ−1.5)pαd1)j | 2\n≤ ∑ i∈[d1] 1.5 γ − 1.5 ‖ui‖22 + ∑ j∈[d2] 1.5 γ − 1.5 ‖vj‖22 ≤\n3\nγ − 1.5 |||ΠΦ(M −M∗)|||2F. (59)\nMoreover, we have\n|||ΠΦ(M −M∗)|||2F ≤ 2|||ΠΦ(Uπ∗∆>V + ∆UV >π∗)|||2F + 2|||ΠΦ(∆U∆>V )|||2F ≤ 2(1 + )p|||Uπ∗∆>V + ∆UV >π∗ |||2F + 2p|||∆U |||2F|||∆V |||2F + 18p σ∗1|||∆U |||F|||∆V |||F ≤ 4(1 + )p ( |||Uπ∗ |||2op|||∆V |||2F + |||Vπ∗ |||2op|||∆U |||2F ) + 2p|||∆U |||2F|||∆V |||2F + 18p σ∗1|||∆U |||F|||∆V |||F\n≤ (4 + 13 )pσ∗1δ + 2pδ2, (60)\nwhere the second step follows from Lemma 9 and inequality (51) in Lemma 11. Putting (55)-(60) together, we obtain\nT2 ≤ (18γ + 14β + 27)αµrσ∗1δ + 3[(2 + 7 )σ∗1δ + δ 2]\nβ(γ − 1.5) .\nUpper bound of T3. By Cauchy-Schwarz inequality, we have\npT3 ≤ |||ΠΦ(M −M∗ + S − S∗)|||F|||ΠΦ(∆U∆>V )|||F ≤ |||ΠΦ(M −M∗ + S − S∗)|||F √ p|||∆U |||2F|||∆V |||2F + 9p σ∗1|||∆U |||F|||∆V |||F\n≤ |||ΠΦ(M −M∗ + S − S∗)|||F √ pδ2 + 5p σ∗1δ.\nwhere we use (51) in Lemma 11 in the second step. We observe that ΠΦ(M −M∗ + S − S∗) is supported on Φ \\ Ω. Therefore, we have\n|||ΠΦ(M −M∗ + S − S∗)|||F ≤ |||ΠΦ∩Ωc∩Φ∗c(M −M∗)|||F + |||ΠΦ∩Ωc∩Φ∗(M −M∗ − S∗)|||F ≤ |||ΠΦ(M −M∗)|||F + |||ΠΩc∩Φ∗(M −M∗ − S∗)|||F\n≤ |||ΠΦ(M −M∗)|||F + √\n3\nγ − 1.5 |||ΠΦ(M −M∗)|||F\n≤ ( 1 + √ 3\nγ − 1.5\n)√ (4 + 13 )pσ∗1δ + 2pδ 2,\nwhere the third step follows from (59), and the last step is from (60). Under assumptions γ = 3, ≤ 1/4 and δ ≤ σ∗1, we have\nT3 ≤ 3 √ 9σ∗1δ + 2δ 2 √ δ2 + 5 σ∗1δ ≤ 10 √ σ∗1δ 3 + 23 √ σ∗1δ.\nCombining pieces. Under the aforementioned assumptions, putting all pieces together leads to\n〈〈∇M L̃(U, V ;S), UV > − Uπ∗V >π∗ + ∆U∆>V 〉〉\n≥ 3 16 |||M −M∗|||2F − (14β + 81)αµrσ∗1δ −\n( 26 √ + 18\nβ\n) σ∗1δ − 10 √ σ∗1δ 3 − 2δ2."
    }, {
      "heading" : "6.11 Proof of Lemma 7",
      "text" : "Let M := UV >. We find that\n∇U L̃(U, V ;S) = p−1ΠΦ (M + S −M∗ − S∗)V, ∇V L̃(U, V ;S) = p−1ΠΦ (M + S −M∗ − S∗)> U.\nConditioning on the event in Lemma 11, since (U, V ) ∈ U ×V, inequalities (52) and (53) imply that\n|||∇U L̃(U, V ;S)|||2F + |||∇V L̃(U, V ;S)|||2F ≤ 12 p µrσ∗1|||ΠΦ (M + S −M∗ − S∗) |||2F.\nIt remains to bound the term |||ΠΦ (M + S −M∗ − S∗) |||2F. Let Ω∗o and Ω be the support of ΠΦ(S∗) and S respectively. We observe that\n|||ΠΦ (M + S −M∗ − S∗) |||2F = |||ΠΩ∗o\\Ω (M −M ∗ − S∗) |||2F + |||ΠΦ∗c∩Ωc∩Φ (M −M∗) |||2F\n≤ |||ΠΩ∗o\\Ω (M −M ∗ − S∗) |||2F + |||ΠΦ (M −M∗) |||2F.\nIn the proof of Lemma 6, it is shown in (59) that\n|||ΠΩ∗o\\Ω (M −M ∗ − S∗) |||2F ≤\n3\nγ − 1.5 |||ΠΦ(M −M∗)|||2F.\nMoreover, following (60), we have that\n|||ΠΦ(M −M∗)|||2F ≤ 2(1 + )p|||Uπ∗∆>V + ∆UV >π∗ |||2F + 2p|||∆U |||2F|||∆V |||2F + 18p σ∗1|||∆U |||F|||∆V |||F ≤ 4(1 + )p|||M −M∗|||2F + (6 + 4 )p|||∆U |||2F|||∆V |||2F + 18p σ∗1|||∆U |||F|||∆V |||F ≤ 4(1 + )p|||M −M∗|||2F + (6 + 4 )pδ2 + 9p σ∗1δ.\nWe thus finish proving our conclusion by combining all pieces and noticing that γ = 3 and ≤ 1/4."
    }, {
      "heading" : "7 Proofs for Technical Lemmas",
      "text" : "In this section, we prove several technical lemmas that are used in the proofs of our main theorems."
    }, {
      "heading" : "7.1 Proof of Lemma 1",
      "text" : "We observe that |||A|||op = sup\nx∈Sd1−1 sup y∈Sd2−1 x>Ay.\nWe denote the support of A by Ω. For any x ∈ Rd1 , y ∈ Rd2 and β > 0, we have\nx>Ay = ∑\n(i,j)∈Ω\nxiA(i,j)yj ≤ ∑\n(i,j)∈Ω\n1 2 ‖A‖∞(β−1x2i + βy2j )\n= 1\n2 ‖A‖∞ ∑ i ∑ j∈Ω(i,·) β−1x2i + ∑ j ∑ i∈Ω(·,j) βy2j  ≤ 1\n2 ‖A‖∞\n( αd2β −1‖x‖22 + αd1β‖y‖22 ) .\nIt is thus implied that |||A|||op ≤ 12α(β −1d2 +βd1)‖A‖∞. Choosing β = √ d2/d1 completes the proof."
    }, {
      "heading" : "7.2 Proof of Lemma 9",
      "text" : "We define a subspace K ⊆ Rd1×d2 as\nK := { X ∣∣ X = L∗A> +BR∗> for some A ∈ Rd2×r, B ∈ Rd1×r } .\nLet ΠK be Euclidean projection onto K. Then according to Theorem 4.1 in [6], under our assumptions, for all matrices X ∈ Rd1×d2 , inequality\np−1||| (ΠKΠΦΠK − pΠK)X|||F ≤ |||X|||F (61)\nholds with probability at least 1− 2d−3. In our setting, by restricting X = L∗A> + BR∗>, we have ΠKX = X. Therefore, (61) implies that |||ΠKΠΦX − pX|||F ≤ p |||X|||F.\nFor |||ΠΦX|||2F, we have\n|||ΠΦX|||2F = 〈〈ΠΦX, ΠΦX〉〉 = 〈〈ΠΦX, X〉〉 = 〈〈ΠKΠΦX, X〉〉 ≤ |||ΠKΠΦX|||F|||X|||F ≤ p(1 + )|||X|||2F.\nOn the other hand, we have\n|||ΠΦX|||2F = 〈〈ΠKΠΦX, X〉〉 = 〈〈ΠKΠΦX − pX + pX, X〉〉 = p|||X|||2F − 〈〈X, −ΠKΠΦX + pX〉〉 ≥ p|||X|||2F − |||X|||F|||ΠKΠΦX − pX|||F ≥ p(1− )|||X|||2F.\nCombining the above two inequalities, we complete the proof."
    }, {
      "heading" : "7.3 Proof of Lemma 10",
      "text" : "We observe that |Φ(i,·)| is a summation of d2 i.i.d. binary random variables with mean p and variance p(1− p). By Bernstein’s inequality, for any i ∈ [d1],\nPr [∣∣|Φ(i,·)| − pd2∣∣ ≥ 12pd2 ] ≤ 2 exp ( −\n−12(pd2/2) 2\nd2p(1− p) + 13(pd2/2)\n) ≤ 2 exp ( − 3\n28 pd2\n) .\nBy probabilistic union bound, we have\nPr [ sup i∈[d1] ∣∣|Φ(i,·)| − pd2∣∣ ≥ 12pd2 ] ≤ 2d1 exp ( − 3 28 pd2 ) ≤ 2d−1,\nwhere the last inequality holds by assuming p ≥ 563 log d d2 . The term |Ω∗o(i,·)| is a summation of at most αd2 i.i.d. binary random variables with mean p and variance p(1− p). Again, applying Bernstein’s inequality leads to\nPr [ |Ω∗o(i,·)| − E [ |Ω∗o(i,·)| ] ≥ 1\n2 pαd2\n] ≤ exp ( − 3\n28 pαd2\n) .\nAccordingly, by the assumption p ≥ 563 log d αd2 , we obtain\nPr [ sup i∈[d1] |Ω∗o(i,·)| − pk ≥ 1 2 pk ] ≤ d1 exp ( − 3 28 pαd2 ) ≤ d−1.\nThe proofs for |Φ(·,j)| and |Ω∗o(·,j)| follow the same idea."
    }, {
      "heading" : "7.4 Proof of Lemma 11",
      "text" : "According to Lemma 3.2 in [8], under condition p ≥ c1 µ log dd1∧d2 , for any fixed matrix A ∈ R d1×d2 , we have\n|||A− p−1ΠΦA|||op ≤ c2\n√ d log d\np ‖A‖∞,\nholds with probability at least 1−O(d−3). Letting A be all-ones matrix, then we have that for all u ∈ Rd1 , v ∈ Rd2 , ∑\n(i,j)∈Φ\nuivj ≤ p‖u‖1‖v‖1 + c2 √ pd log d‖u‖2‖v‖2.\nWe find that |||ΠΦ(UV >)|||2F ≤ ∑\n(i,j)∈Φ\n‖U(i,·)‖22‖V(j,·)‖22\n≤ p|||U |||2F|||V |||2F + c2 √ pd log d √∑ i∈[d1] ‖U(i,·)‖42 √∑ j∈[d2] ‖V(j,·)‖42\n≤ p|||U |||2F|||V |||2F + c2 √ pd log d|||U |||F|||V |||F|||U |||2,∞|||V |||2,∞\n≤ p|||U |||2F|||V |||2F + c2\n√ pµ2r2d log d\nd1d2 |||U |||F|||V |||F.\nBy the assumption p & µ 2r2 log d\n2(d1∧d2) , we finish proving (51).\nAccording to the proof of Lemma 10, if p ≥ c log dd1∧d2 , with probability at least 1 − O(d −1), we have |Φ(i,·)| ≤ 32pd2 and |Φ(·,j)| ≤ 3 2pd1 for all i ∈ [d1] and j ∈ [d2]. Conditioning on this event, we have\n|||ΠΦ(Z)V |||2F = ∑ i∈[d1] ∑ k∈[r] 〈(ΠΦ(Z))(i,·), H(·,k)〉2\n≤ ∑ i∈[d1] ∑ k∈[r] ‖(ΠΦ(Z))(i,·)‖22 ∑ j∈Ω(i,·) V 2(j,k)\n= |||ΠΦZ|||2F ∑\nj∈Ω(i,·)\n‖V(i,·)‖22\n≤ |||ΠΦZ|||2F 3\n2 pd2 · |||V |||22,∞ ≤ 2µrp|||ΠΦZ|||2F.\nWe thus finish proving (52). Inequality (53) can be proved in the same way."
    }, {
      "heading" : "7.5 Proof of Lemma 8",
      "text" : "Recall that we let F := [U ;V ] and Fπ∗ := [U∗;V ∗]Q for some matrix Q ∈ Qr, which minimizes the following function\n|||F − [U∗;V ∗]Q|||2F. (62)\nLet F ∗ := [U∗;V ∗]. Expanding the above term, we find that Q is the maximizer of 〈〈F, F ∗Q〉〉 = Tr(F>F ∗Q). Suppose F>F ∗ has SVD with form Q1ΛQ>2 for Q1, Q2 ∈ Qr. When the minimum\ndiagonal term of Λ is positive, we conclude that the minimizer of (62) is unique and Q = Q2Q>1 . To prove this argument, we note that\nTr(F>F ∗Q) = ∑ i∈[r] Λ(i,i)〈pi, qi〉,\nwhere pi is the i-th column of Q1 and qi is the i-th column of Q>Q2. Hence, Tr(F>F ∗Q) ≤∑ i∈[r] Λ(i,i) and the equality holds if and only if pi = qi for all i ∈ [r] since every Λ(i,i) > 0. We have Q1 = Q>Q2 and thus finish proving the argument. Under our assumption |||F − Fπ∗ |||op < √ 2σ∗r , for any nonzero vector u ∈ Rr, we have\n‖F>Fπ∗u‖2 ≥ ‖F>π∗Fπ∗u‖2 − ‖(Fπ∗ − F )>Fπ∗u‖2 ≥ ( √ 2σ∗r − |||Fπ∗ − F |||op)|||Fπ∗u|||F > 0.\nIn the second step, we use the fact that the singular values of Fπ∗ are equal to the diagonal terms of √\n2Σ∗1/2. Hence, F>Fπ∗ has full rank. Furthermore, it implies that F>F ∗ has full rank and only contains positive singular values.\nProceeding with the proved argument, we have\nF>Fπ∗ = Q1ΛQ > 2 Q2Q > 1 = Q1ΛQ > 1 ,\nwhich implies that F>Fπ∗ is symmetric. Accordingly, we have (F − Fπ∗)>Fπ∗ is also symmetric."
    }, {
      "heading" : "Acknowledgment",
      "text" : "Y. Chen acknowledges support from the School of Operations Research and Information Engineering, Cornell University."
    }, {
      "heading" : "A Supporting Lemmas",
      "text" : "In this section, we provide several technical lemmas used for proving our main results.\nLemma 12. For any (U∗, V ∗) ∈ E(M∗), U ∈ Rd1×r and V ∈ Rd2×r, we have\n|||UV > − U∗V ∗>|||F ≤ √ σ∗1(|||∆V |||F + |||∆U |||F) + |||∆U |||F|||∆V |||F,\nwhere ∆U := U − U∗, ∆V := V − V ∗.\nProof. We observe that UV > − U∗V ∗> = U∗∆>V + ∆UV ∗> + ∆U∆>V . Hence,\n|||UV > − U∗V ∗>|||F ≤ |||U∗∆>V |||F + |||∆UV ∗>|||F + |||∆U∆>V |||F ≤ |||U∗|||op|||∆V |||F + |||V ∗|||op|||∆U |||F + |||∆U |||F|||∆V |||F.\nFurthermore, assuming (U, V ) ∈ U × V, where U and V satisfy the conditions in (19), we have the next result.\nLemma 13. For any (i, j) ∈ [d1]× [d2], we have\n|(UV > − U∗V ∗>)(i,j)| ≤ 3 √ µrσ∗1 d1 ‖∆V (j,·)‖2 + 3 √ µrσ∗1 d2 ‖∆U(i,·)‖2 (63)\nProof. We observe that\n|(UV > −M∗)(i,j)| ≤ |〈U∗(i,·), ∆V (j,·)〉|+ |〈V ∗ (j,·), ∆U(i,·)〉|+ |〈∆U(i,·), ∆V (j,·)〉| ≤ √ µrσ∗1 d1 ‖∆V (j,·)‖2 + √ µrσ∗1 d2 ‖∆U(i,·)‖2 + 1 2 |||∆U |||2,∞‖∆V (j,·)‖2 + 1 2 |||∆V |||2,∞‖∆U(i,·)‖2.\nBy noticing that\n|||∆U |||2,∞ ≤ |||U∗|||2,∞ + |||U |||2,∞ ≤ 3 √ µrσ∗1 d1 , |||∆V |||2,∞ ≤ |||V ∗|||2,∞ + |||V |||2,∞ ≤ 3 √ µrσ∗1 d2 ,\nwe complete the proof.\nLemma 13 can be used to prove the following result.\nLemma 14. For any α ∈ [0, 1], suppose Ω ⊆ [d1] × [d2] satisfies |Ω(i,·)| ≤ αd2 for all i ∈ [d1] and |Ω(·,j)| ≤ αd1 for all j ∈ [d2]. Then we have\n|||ΠΩ(UV > − U∗V ∗>)|||2F ≤ 18αµrσ∗1(|||∆V |||2F + |||∆U |||2F).\nProof. Using Lemma 13 for bounding each entry of UV > − U∗V ∗>, we have that\n|||ΠΩ(UV > − U∗V ∗>)|||2F ≤ ∑\n(i,j)∈Ω\n|(UV > − U∗V ∗>)(i,j)|2\n≤ ∑\n(i,j)∈Ω\n18µrσ∗1 d1 ‖∆V (j,·)‖22 + 18µrσ∗1 d2 ‖∆U(i,·)‖22\n≤ ∑ j ∑ i∈Ω(·,j) 18µrσ∗1 d1 ‖∆V (j,·)‖22 + ∑ i ∑ j∈Ω(i,·) 18µrσ∗1 d2 ‖∆U(i,·)‖22 ≤ 18αµrσ∗1(|||∆V |||2F + |||∆U |||2F).\nDenote the i-th largest singular value of matrix M by σi(M).\nLemma 15 (Lemma 5.14 in [25]). Let M1,M2 ∈ Rd1×d2 be two rank r matrices. Suppose they have SVDs M1 = L1Σ1R>1 and M2 = L2Σ2R > 2 . Suppose |||M1 −M2|||op ≤ 12σr(M1). Then we have\nd2(L2Σ 1/2 2 , R2Σ 1/2 2 ;L1Σ 1/2 1 , R1Σ 1/2 1 ) ≤ 2√ 2− 1 |||M2 −M1|||2F σr(M1) ."
    }, {
      "heading" : "B Parameter Settings for FB Separation Experiments",
      "text" : "We approximate the FB separation problem by the RPCA framework with r = 10, α = 0.2, µ = 10. Our algorithmic parameters are set as γ = 1, η = 1/(2σ̂∗1), where σ̂∗1 is an estimate of σ∗1 obtained from the initial SVD. The parameters of AltProj are kept as provided in the default setting. For IALM, we use the tradeoff paramter λ = 1/ √ d1, where d1 is the number of pixels in each frame (the number of rows in Y ). Note that both IALM and AltProj use the stopping criterion\n‖Y −Mt − St‖F /‖Y ‖F ≤ 10−3.\nOur algorithm never explicitly forms the d1-by-d2 matrixMt = UtV >t , which is favored in large scale problems, but also renders the above criterion inapplicable. Instead, we use the following stopping criterion\n|||Ut+1 − Ut|||2F + |||Vt+1 − Vt|||2F |||Ut|||2F + |||Vt|||2F ≤ 4× 10−4.\nThis rule checks whether the iterates corresponding to low-rank factors becomes stable. In fact, our stopping criterion seems more natural and practical because in most real applications, matrix Y cannot be strictly decomposed into low-rank M and sparse S that satisfy Y = M + S. Instead of forcing M + S to be close to Y , our rule relies on seeking a robust subspace that captures the most variance of Y ."
    } ],
    "references" : [ {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky" ],
      "venue" : "In The Journal of Machine Learning Research",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "Sivaraman Balakrishnan", "Martin J. Wainwright", "Bin Yu" ],
      "venue" : "In arXiv preprint arXiv:1408.2156",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Tighter low-rank approximation via sampling the leveraged element",
      "author" : [ "Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Dropping convexity for faster semi-definite optimization",
      "author" : [ "Srinadh Bhojanapalli", "Anastasios Kyrillidis", "Sujay Sanghavi" ],
      "venue" : "In arXiv preprint arXiv:1509.03917",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Phase retrieval via Wirtinger flow: Theory and algorithms",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Mahdi Soltanolkotabi" ],
      "venue" : "In IEEE Transactions on Information Theory",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J. Candès", "Benjamin Recht" ],
      "venue" : "In Foundations of Computational mathematics",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "Emmanuel J. Candès", "Terence Tao" ],
      "venue" : "In IEEE Transactions on Information Theory",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Robust principal component analysis?",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "In Journal of the ACM (JACM)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Ranksparsity incoherence for matrix decomposition",
      "author" : [ "Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo A. Parrilo", "Alan S. Willsky" ],
      "venue" : "In SIAM Journal on Optimization",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Incoherence-Optimal Matrix Completion",
      "author" : [ "Yudong Chen" ],
      "venue" : "In IEEE Transactions on Information Theory",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees",
      "author" : [ "Yudong Chen", "Martin J. Wainwright" ],
      "venue" : "In arXiv preprint arXiv:1509.03025",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Low-rank Matrix Recovery from Errors and Erasures",
      "author" : [ "Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Constantine Caramanis" ],
      "venue" : "In IEEE Transactions on Information Theory",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Solving random quadratic systems of equations is nearly as easy as solving linear systems",
      "author" : [ "Yuxin Chen", "Emmanuel J. Candès" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Fast Monte-Carlo algorithms for finding low-rank approximations",
      "author" : [ "Alan Frieze", "Ravi Kannan", "Santosh Vempala" ],
      "venue" : "In Journal of the ACM (JACM)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Low-Rank and Sparse Structure Pursuit via Alternating Minimization",
      "author" : [ "Quanquan Gu", "Zhaoran Wang", "Han Liu" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Understanding alternating minimization for matrix completion",
      "author" : [ "Moritz Hardt" ],
      "venue" : "IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS). IEEE",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Robust matrix decomposition with sparse corruptions",
      "author" : [ "Daniel Hsu", "Sham M. Kakade", "Tong Zhang" ],
      "venue" : "In IEEE Transactions on Information Theory",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices",
      "author" : [ "Zhouchen Lin", "Minming Chen", "Yi Ma" ],
      "venue" : "In Arxiv preprint arxiv:1009.5055v3",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Phase Retrieval using Alternating Minimization",
      "author" : [ "Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi" ],
      "venue" : "In Arxiv preprint arxiv:arXiv:1306.0160",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Non-convex robust PCA",
      "author" : [ "Praneeth Netrapalli", "UN Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "When Are Nonconvex Problems Not Scary?",
      "author" : [ "Ju Sun", "Qing Qu", "John Wright" ],
      "venue" : "In arXiv preprint arXiv:1510.06096",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Guaranteed matrix completion via nonconvex factorization",
      "author" : [ "Ruoyu Sun", "Zhi-Quan Luo" ],
      "venue" : "IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS). IEEE",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Low-rank solutions of linear matrix equations via procrustes flow",
      "author" : [ "Stephen Tu", "Ross Boczar", "Mahdi Soltanolkotabi", "Benjamin Recht" ],
      "venue" : "In arXiv preprint arXiv:1507.03566",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality",
      "author" : [ "Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Robust PCA via Outlier Pursuit",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "In IEEE Transactions on Information Theory",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees",
      "author" : [ "Xinyang Yi", "Constantine Caramanis" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow",
      "author" : [ "Huishuai Zhang", "Yuejie Chi", "Yingbin Liang" ],
      "venue" : "In arXiv preprint arXiv:1603.03805",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "A Nonconvex Optimization Framework for Low Rank Matrix Estimation",
      "author" : [ "Tuo Zhao", "Zhaoran Wang", "Han Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : ", [8, 27]), the running times range from O(r2d2) to O(d3)1 and hence are significantly worse than SVD.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 26,
      "context" : ", [8, 27]), the running times range from O(r2d2) to O(d3)1 and hence are significantly worse than SVD.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : ", [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : ", [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : ", [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "Provable solutions for this model are first provided in the works of [9] and [8].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Provable solutions for this model are first provided in the works of [9] and [8].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "Despite analyzing the same method, the corruption models in [8] and [9] differ.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "Despite analyzing the same method, the corruption models in [8] and [9] differ.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "In [8], the authors consider the setting where the entries of M∗ are corrupted at random with probability α.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "Work in [9] considers a deterministic corruption model, where nonzero entries of S∗ can have arbitrary position, but the sparsity of each row and column does not exceed αd.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "This was subsequently further improved to α = O(1/(μr)), which is in fact optimal [12, 18].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "This was subsequently further improved to α = O(1/(μr)), which is in fact optimal [12, 18].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "The state-of-the-art solver [20] for (1) has time complexity O(d3/ε) to achieve error ε, and is thus much slower than SVD, and prohibitive for even modest values of d.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "Work in [22] considers the deterministic corruption model, and improves this running time without sacrificing the robustness guarantee on α.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "[19, 17, 16]) and gradient descent (see e.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : "[19, 17, 16]) and gradient descent (see e.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "[19, 17, 16]) and gradient descent (see e.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "[4, 11, 24, 25, 30, 31]).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "[4, 11, 24, 25, 30, 31]).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 23,
      "context" : "[4, 11, 24, 25, 30, 31]).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 24,
      "context" : "[4, 11, 24, 25, 30, 31]).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 29,
      "context" : "[4, 11, 24, 25, 30, 31]).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "One exception is [11], where the authors analyze a row-wise `1 projection method for recovering S∗.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "Another exception is work [16], which analyzes alternating minimization plus an overall sparse projection.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 27,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 22,
      "context" : "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 10,
      "context" : "An immediate corollary of the above result provides a guarantee for exact matrix completion, with general rectangular matrices, usingO(μ2r2d log d) observed entries andO(μ3r4d log d log(1/ε)) time, thereby improving on existing results in [11, 24].",
      "startOffset" : 239,
      "endOffset" : 247
    }, {
      "referenceID" : 23,
      "context" : "An immediate corollary of the above result provides a guarantee for exact matrix completion, with general rectangular matrices, usingO(μ2r2d log d) observed entries andO(μ3r4d log d log(1/ε)) time, thereby improving on existing results in [11, 24].",
      "startOffset" : 239,
      "endOffset" : 247
    }, {
      "referenceID" : 21,
      "context" : "We note that when κ = O(1), our algorithm is orderwise faster than the AltProj algorithm in [22], which has running time O(r2d2 log(1/ε)).",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "This is worse by a factor √ r compared to the optimal statistical guarantee 1/(μr) obtained in [12, 18, 22].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "This is worse by a factor √ r compared to the optimal statistical guarantee 1/(μr) obtained in [12, 18, 22].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "This is worse by a factor √ r compared to the optimal statistical guarantee 1/(μr) obtained in [12, 18, 22].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : ", [7]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 10,
      "context" : ", observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : ", observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 23,
      "context" : ", observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "It is known that that any algorithm for solving exact matrix completion must have sample size Ω(μrd log d) [7], and a nearly tight upper bound O(μrd log d) is obtained in [10] by convex relaxation.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "It is known that that any algorithm for solving exact matrix completion must have sample size Ω(μrd log d) [7], and a nearly tight upper bound O(μrd log d) is obtained in [10] by convex relaxation.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 19,
      "context" : "In this section, we provide numerical results and compare the proposed algorithms with existing methods, including the inexact augmented lagrange multiplier (IALM) approach [20] for solving the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "In this section, we provide numerical results and compare the proposed algorithms with existing methods, including the inexact augmented lagrange multiplier (IALM) approach [20] for solving the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21].",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 21,
      "context" : "2, AltProj [22], and IALM [20].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "2, AltProj [22], and IALM [20].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "14 in [25] (we provide it as Lemma 15 for the sake of completeness), we obtain",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "2 Proof of Theorem 2 We essentially follow the general framework developed in [11] for analyzing the behaviors of gradient descent in factorized low-rank optimization.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "But it is worth to note that [11] only studies the symmetric and positive semidefinite setting, while we avoid such constraint on M∗.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "The techniques for analyzing general asymmetric matrix in factorized space is inspired by the recent work [25] on solving low-rank matrix equations.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "For the second term in (26), we use the following lemma proved in [10].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "Lemma 5 (Lemma 2 in [10]).",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "For the cross term, by the following result, proved in [11] (we also provide a proof in Section 7.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "1 in [6], under our assumptions, for all matrices X ∈ Rd1×d2 , inequality p−1||| (ΠKΠΦΠK − pΠK)X|||F ≤ |||X|||F (61) holds with probability at least 1− 2d−3.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "2 in [8], under condition p ≥ c1 μ log d d1∧d2 , for any fixed matrix A ∈ R d1×d2 , we have |||A− pΠΦA|||op ≤ c2 √ d log d p ‖A‖∞, holds with probability at least 1−O(d−3).",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2016,
    "abstractText" : "We consider the problem of Robust PCA in the the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomialtime algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with r denoting rank and d dimension, we reduce the complexity from O(rd log(1/ε)) to O(rd log(1/ε)) – a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than O(rd log d log(1/ε)). Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where r is small compared to d, it also allows for near-linear-in-d run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.",
    "creator" : "LaTeX with hyperref package"
  }
}