{
  "name" : "1611.10283.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weighted bandits or: How bandits learn distorted values that are not expected",
    "authors" : [ "Aditya Gopalan", "Prashanth L A", "Michael Fu", "Steve Marcus" ],
    "emails" : [ "aditya@ece.iisc.ernet.in", "prashla@umd.edu", "mfu@isr.umd.edu", "marcus@umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "2 α −1 ) , where n is the number of plays, ∆ is the gap in distorted expected value between the\nbest and next best arm, L and α are the Hölder constants for the distortion function, and M is an upper bound on costs, and a problem-independent regret bound of O((KL2M2)α/2n(2−α)/2). We also present a matching lower bound on the regret, showing that the regret of W-UCB is essentially unimprovable over the class of Hölder -continuous weight distortions. For the linearly parameterized setting, we develop a new algorithm, a variant of the Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm Abbasi-Yadkori et al. [2011] called WOFUL (Weight-distorted OFUL), and show that it has regret O(d √ n polylog(n)) with high probability, for sub-Gaussian cost distributions. Finally, numerical examples demonstrate the advantages resulting from using distortion-aware learning algorithms."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider the following two-armed bandit problem: The rewards of Arm 1 are $1 million w.p. 1/106 and 0 otherwise, while Arm 2 rewards are $1000 w.p. 1/103 and 0 otherwise. In this case, humans would usually prefer Arm 1 over Arm 2. The human preferences get flipped if we change to costs, i.e., Arm 1 loses a million with a very low probability of 1/106, while Arm 2 loses $1000 w.p. 1/103. In this case, Arm 2 is preferred over Arm 1. The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems.\nViolations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4]. Probabilistic distortions have a long history in behavioral science and economics, and we bring this idea to a multi-armed bandit setup. In particular, we base our approach on rank-dependent expected utility (RDEU) Quiggin [2012], which includes the popular cumulative prospect theory (CPT) of Tversky and Kahneman Tversky and Kahneman [1992]. ∗aditya@ece.iisc.ernet.in †prashla@umd.edu ‡mfu@isr.umd.edu §marcus@umd.edu\nar X\niv :1\n61 1.\n10 28\n3v 1\n[ cs\n.L G\n] 3\n0 N\nThe distortions happen via a weight function w : [0, 1] → [0, 1] that transforms probabilities in a nonlinear fashion. As illustrated in Figure 1, a typical weight function, say w : [0, 1] → [0, 1], has the inverted-S shape. In other words, w inflates low probabilities and deflates large probabilities and can explain human preferences well. For instance, in the example above, if we choose w(1/106) > 1/106 and take expectations w.r.t. the w-distorted distribution, then Arm 1 would be preferable when the problem is setup with rewards and Arm 2 for the problem with costs. The suitability of this approach, esp. with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].\nWe use a traveler’s route choice as a running example to illustrate the main ideas in this paper. The setting here is that a human travels from a source (e.g., home) to a destination (e.g., office), both fixed, every day. He/she has multiple routes to choose from, and each route incurs a stochastic delay with unknown distributions. The problem then is to choose a route that minimizes some function of delay. Using expected delay may not lead to a routing choice that is appealing to the human traveler. In addition to the examples mentioned earlier, intuitively humans would prefer a route with a slight excess of delay over another that has a small probability of getting into a traffic jam that takes hours to be resolved. The requirement here is for an automated routing algorithm, say one that sits as an application on the traveler’s mobile device, that learns the best route for the traveler. The algorithm is online and uses the delay information for a recommended route as feedback to find the best route. We treat this problem in two regimes: first, a setting where the number of routes is small, so the traveler can afford to try each of the routes a small number of times before fixing on the “best” route; second, a big road network setting that involves a large number of routes, which prohibits an approach that requires trying the bulk of the routes before deciding which is the “best”.\nWe formalize two probabilistically distorted bandit settings that correspond to the two routing setups mentioned above. The first is the classic K-armed setting, while the second is the linear bandit setting. In both settings, we define the weight-distorted value µx for any arm x in the space of arms X as follows:\nµx= ∫ ∞ 0 w(P [Yx > z])dz− ∫ ∞ 0 w(P [−Yx > z])dz, (1)\nwhere w is the weight function that satisfies w(0) = 0 and w(1) = 1 and Yx is the random variable (r.v.) corresponding to the stochastic rewards from arm x ∈ X . By choosing the identity weight function w(p) = p, we obtain µx = E(Y+x ) − E(Y−x ) = E(Yx), where y+ = max(y, 0) and y− = max(−y, 0) denote the positive and negative parts of y ∈ R, respectively. Thus, µx as in (1) generalizes standard expected value. As discussed earlier, w has to be chosen in a non-linear fashion, to capture human preferences, which has strong empirical support.\nIn our setting, the goal is find an arm x∗ that maximizes (1). The problem is challenging because the current bandit solutions, for instance, the popular UCB algorithm, cannot handle distortions. This is because the environment provides samples from the distribution Fx when arm x is pulled, while the integral in (1) involves a distorted distribution. The implication is that a simple sample mean and a confidence term suggested by the Hoeffding\ninequality is enough to derive the UCB values for any arm in the regular setting involving expected values. On the other hand, one requires a good enough estimate of Fx to estimate µx. Just for the sake of example, a (α-Hölder continuous) weight function such as w(t) := tα, when applied to a Bernoulli(p) distribution, distorts the mean to pα from p, and can introduce an arbitrarily large scaling for arms with real expectations close to 0. It follows that nonlinear weight distortion can, in fact, change the order of the optimal arm, resulting in a distortion-unaware algorithm like UCB converging to the wrong arm and incurring linear regret. The W-UCB algorithm that we propose incorporates a empirical distribution-based approach, similar to that of Prashanth et al. [2016], to estimate µx. However, unlike the latter, our algorithm incorporates a confidence term relying on the Dvoretzky-KieferWolfowitz (DKW) inequality [Wasserman, 2015] that ensures the W-UCB values are a high-probability bound on the true value µx. We provide upper bounds on the regret of W-UCB, assuming w is Hölder continuous and provide empirical demonstrations on a setting from [Tversky and Kahneman, 1992].\nNext, we consider a linear bandit setting with weight distortions that can be motivated as follows: Consider a network graph G = (V,E), |E| = d, with a source s ∈ V and destination t ∈ V . The interaction proceeds over multiple rounds, where in each round m, the user picks a route xm from s to t (a route is a collection of edges encoded by a vector of 0−1 values in d dimensions) and experiences a stochastic delay xTm (θ +Nm). Here θ ∈ Rd is an underlying model parameter and Nm ∈ Rd is a random noise vector, both unknown to the learner. The physical interpretation is that the nodes represent geographical locations (say junctions) and the edges are roads that connect nodes. An edge from i to j will have an edge weight θij , which quantifies the delay for this edge.\nNotice that the observations include a noise component that scales with the route chosen – this is unlike the model followed in earlier linear bandit works [Abbasi-Yadkori et al., 2011, Dani et al., 2008], where the noise was independent of the arm chosen. Our noise model makes practical sense because the observed delay in a road traffic network depends on the length of the route, for e.g., one would expect more noise in a detour involving ten roads than in a direct one-road route. The aim is to find a low-delay route that satisfies the user. The setting is such that the number of routes is large (so the regular K-armed bandits don’t scale) and one needs to utilize the linearity in the costs (delays) to find the optimal route, where optimality is qualified in terms of a weight-distorted expectation.\nFor the linear bandit setting, we propose a variant of the OFUL algorithm [Abbasi-Yadkori et al., 2011], that incorporates weight-distorted values in the arm selection step. The regret analysis of the resulting WOFUL algorithm poses novel challenges compared to that in the linear bandit problem, primarily because the instantaneous cost (and hence regret) at each round is in fact a nonlinear function of the features of the played arm. This occurs due to the distortion in expectation caused by the weight function, although the actual observation (e.g., network delay in the example above) is linear in expectation over the played arm’s features. The weight function can not only change the optimal arm but can also potentially amplify small differences in real expected values of arms to much larger values, leading to a blowing up of overall regret. However, our analysis shows that regret in weightdistorted cost as the performance metric can be controlled at the same rate as that in standard linear bandit models, irrespective of the structure of the distortion function. More specifically, we show, using a careful analysis of the effect of weight distortion, that the regret of the WOFUL algorithm is no more than O (d √ n polylog(n)) in n rounds with high probability, similar to the guarantee enjoyed by the OFUL algorithm in linear bandits (note however that the identity of the optimal arm may be different due to weight distortion in costs).\nRelated work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting. In contrast, we formulate two multi-armed bandit models that incorporate weight-distortions. From a theoretical standpoint, we handle the exploration-exploitation tradeoff via UCB-inspired algorithms, while the focus of Prashanth et al. [2016] was to devise a policy-gradient scheme given biased estimates of a certain CPT-value defined for each policy. Moreover, we provide finite-time regret bounds for both bandit settings, while the guarantees for the policy gradient algorithm of Prashanth et al. [2016] are asymptotic in nature.\nPrevious works involving RDEU and CPT are huge in number; at a conceptual level, the work in this paper integrates machine learning (esp. bandit learning) with an RDEU approach that involves a probabilistic distortions\nvia a weight function. To the best of our knowledge, RDEU/CPT papers in the literature assume model information, i.e., a setting where the distributions of the arms are known, while we have a model-free setting where one can only obtain sample values from the arms’ distributions. Our setting makes practical sense; for instance, in the traveler’s route choice problem one can only obtain sample delays for a particular route, while the distribution governing the delays for any route is not known explicitly.\nThe rest of the paper is organized as follows: In Section 2, we formalize the K-armed bandit problem with weight distortions and present the W-UCB algorithm. Next, in Section 3, we formulate the linear bandit problem with arm-dependent noise model and present the WOFUL algorithm. In Section 4, we provide the proofs of the regret bounds for W-UCB as well as WOFUL algorithms. In Section 5, we present simulation experiments on a synthetic setup and a traffic routing application, respectively. Finally, in Section 6 we provide the concluding remarks.\n2 K-armed bandit with weight distortion Suppose there are K arms with unknown distributions Fk, k = 1, . . . ,K. In each round m = 1, . . . , n, the algorithm pulls an arm Im ∈ {1, . . . ,K} and obtains a sample cost from the distribution FIm of arm Im.\nThe classic objective is to play (or pull) the arm whose expected cost is the least. In this paper, we take a different approach inspired by non-expected utility (EU) approaches and use the weight distorted-cost µk as the performance criterion for any arm k. The latter quantity, defined in (1), can be seen to be equivalent to the following:\nµk := ∫ ∞ 0 w(1− Fk(z))dz − ∫ ∞ 0 w(Fk(−z))dz, (2)\nwhere w : [0, 1]→ [0, 1] is a weight function that distorts probabilities. The optimal arm is one that minimizes the weight-distorted cost, i.e., µ∗ = mink µk. The optimal arm is not necessarily unique, i.e., there may exist multiple arms with the optimal weight-distorted cost µ∗. With the above notion of weight-distorted cost, we define the cumulative regretRn asRn = ∑K k=1 Tk(n)µk −\nnµ∗, where Tk(n) = ∑n m=1 I(Im = k) is the number of times arm k is pulled up to time n. The expected regret\ncan be written as ERn = ∑K k=1 E[Tk(n)]∆k, where ∆k = µk − µ∗ denotes the gap between the weight-distorted costs of the optimal arm and of arm k. Note that this definition of regret arises from the interpretation that each arm k is associated with a deterministic value µk. The least possible cumulative cost that can be suffered in n rounds is thus nµ∗, while that suffered by a given strategy is ∑K k=1 Tk(n)µk. Thus, the regret as defined above is a measure of the rate at which a strategy converges to playing the optimal arm in the sense of weighted or distorted cost. We remark that the regret performance measure, as defined above, is explicitly defined within a stochastic model for rewards. Thus, low-regret algorithms designed for the nonstochastic setting, e.g., EXP3 [Auer et al., 2003], are not inherently suitable for this problem, as they do not factor in the distortion caused in (expected) reward. A similar observation holds for conventional stochastic bandit algorithms such as UCB, and algorithms sensitive to arm reward variances such as UCB-V [Audibert et al., 2009] – once weight distortion is incorporated, the algorithm will converge to an arm that is not weight-distorted value optimal. Thus, applying a variance-sensitive algorithm (like UCB-V) will still yield linear regret in the distorted setting."
    }, {
      "heading" : "2.1 W-UCB Algorithm",
      "text" : "Estimating the weight-distorted cost for any arm k is challenging, and one cannot use a Monte Carlo approach with sample means because weight-distorted cost involves a distorted distribution, whereas the samples come from the undistorted distribution Fk. Thus, one needs to estimate the entire distribution, and for this purpose, we adapt the quantile-based approach of [Prashanth et al., 2016].\nEstimating µk: At time instant m, let Yk,1, . . . , Yk,l denote the samples from the cost distribution Fk for arm k, where we have used l to denote the number of samples Tk(m− 1) for notational convenience. Order the samples\nin ascending fashion as Y[k,1] ≤ Y[k,2] ≤ · · · ≤ Y[k,lb] ≤ 0 ≤ Y[k,lb+1] ≤ · · · ≤ Y[k,l], where lb ∈ {0, 1, 2, . . . , l} denotes the index of a ‘boundary’ sample after which a sign change occurs. The first integral in (2) is estimated by the quantity\nµ̂+k,l := l∑ i=lb+1 Y[k,i] ( w ( l + 1− i l ) −w ( l − i l )) , (3)\nwhile the second integral in (2) is estimated by the quantity\nµ̂−k,l := lb∑ i=1 Y[k,i] ( w ( i− 1 l ) − w ( i l )) . (4)\nWe finally estimate µk as follows:\nµ̂k,l = µ̂ + k,l − µ̂ − k,l. (5)\nFrom (2) and (5) above, it can be seen that µ̂k,l is the weight-distorted cost of the empirical distribution of samples from arm k seen thus far, i.e.,\nµ̂k,l := ∫ ∞ 0 w(1− F̂k,l(z))dz − ∫ ∞ 0 w(F̂k,l(−z))dz,\nwhere F̂k,l(x) := 1l ∑l i=1 I[Yk,i≤x] denotes the empirical distribution of r.v. Yx. In particular, the first and second integral above correspond to (3) and (4), respectively. We next provide a sample complexity result for the accuracy of the estimator µ̂k,l under the following assumptions: (A1) The weight function w is Hölder continuous with constant L and exponent α ∈ (0, 1]: supx 6=y |w(x)−w(y)| |x−y|α ≤ L. (A2) The arms’ costs are bounded by M > 0 almost surely.\n(A1) is necessary to ensure that the weight-distorted value µk, k = 1, . . . ,K is finite. Moreover, the popular choice for the weight function, proposed by Tversky and Kahneman [1992] and illustrated in Figure 1, is Hölder continuous.\nTheorem 1 (Sample complexity of estimating distorted cost). Assume (A1)-(A2). Then, for any > 0 and any k ∈ {1, . . . ,K}, we have P (|µ̂k,m − µk| > ) ≤ 2 exp ( −2m( /LM)2/α ) .\nFor the special case of Lipschitz weight functions w, setting α = 1 in the above theorem, we obtain a sample complexity of order O ( 1/ 2 ) for accuracy .\nProof. See Section 4.1.1.\nB-values (weighted UCB values): At instantm, define the B-value for any arm k, as a function of the number of samples l and constants α ∈ [0, 1], L > 0,M > 0, as: Bm,l(k) = µ̂k,l− γm,l, where γm,l := LM ( 3 logm\n2l\n)α 2\n.\nThe r.v. µ̂k,l, defined by (5), is an estimate of µk that uses the l = Tk(m−1) sample costs of arm k seen so far and γm,l is the confidence width, which together with µ̂m,l ensures that the true weight-distorted value µk lies within [µ̂k,l − γm,l, µ̂k,l + γm,l] with high probability, i.e., for k = 1, . . . ,K, both P (µ̂k,l + γm,l ≤ µk) ≤ 2m−3 and P (µ̂k,l − γm,l ≥ µk) ≤ 2m−3.\nUsing the B-values defined above, the W-UCB algorithm chooses the arm Im at instant m as follows:\nIf m ≤ K, then play Im = m (initial round-robin phase), Else, play Im = arg min\nk={1,...,K} Bm,Tk(m−1)(k). (6)"
    }, {
      "heading" : "2.2 Main results",
      "text" : "Theorem 2 (Regret bound). Under (A1)-(A2), the expected cumulative regretRn of W-UCB is bounded as follows:\nERn ≤ ∑\n{k:∆k>0}\n3(2LM)2/α log n\n2∆ 2/α−1 k\n+MK ( 1 + 2π2\n3\n) .\nProof. See Section 4.1.2.\nThe theorem above involves the gaps ∆k. We next present a gap-independent regret bound in the following result:\nCorollary 1 (Gap-independent regret). Under (A1)-(A2), the expected cumulative regret Rn of W-UCB satisfies the following gap-independent bound. There exists a universal constant c > 0 such that for all n, ERn ≤ MKα/2 ( 3 2 (2L) 2/α log n+ c )α 2 n 2−α 2 .\nProof. See Section 4.1.3. Remark 1. (Lipschitz weights) We can recover the O( √ n) regret bound (or same dependence on the gaps) as in regular UCB for the case when α = 1, i.e., Lipschitz weights. On the other hand, when α < 1, the regret bounds are weaker than O( √ n).\nThe following result shows that one cannot hope to obtain better regret than that of W-UCB (Theorem 2) over the class of Hölder-continuous weight functions, i.e., weight functions satisfying (A1)-(A2), by exhibiting a matching lower bound.\nTheorem 3 (Regret lower bound). For any learning algorithm with sub-polynomial regret in the time horizon, there exists (1) a weight function which is monotone increasing and α-Hölder continuous with constant L, and (2) a set of cost distributions for the arms with support bounded by M , for which the algorithm’s regret satisfies\nE [Rn] = Ω (∑ {k:∆k>0} (LM)2/α log n\n4∆ 2/α−1 k\n) .\nWe defer a more precise statement of the above result to Section 4.1.4.\nRemark 2. (CPT adaptation) As remarked in the introduction, a popular approach using a weight function to distort probabilities is the so-called cumulative prospect theory (CPT) [Tversky and Kahneman, 1992]. In addition to the weight function, a salient feature of CPT is to employ different utility functions, say u+ and u−, to handle gains and losses, respectively. The B-values and the theoretical guarantees can be easily extended to incorporate CPT-style utility functions, and the reader is referred to Section 4.1.5 for details."
    }, {
      "heading" : "3 Linearly parameterized bandit with weight distortion",
      "text" : ""
    }, {
      "heading" : "3.1 Arm-dependent noise setting",
      "text" : "The setting here involves arms that are given as the compact set X ⊂ Rd (each element of X is interpreted as a vector of features associated with an arm). The learning game proceeds as follows. At each round m = 1, 2, . . ., the learner (a) plays an arm xm ∈ X , possibly depending on the history of observations thus far, and (b) observes a stochastic, nonnegative cost given by\ncm := x T m (θ +Nm) , (7)\nwhereNm := (N1m, . . . , N d m) is a vector of i.i.d. standard Gaussian random variables, independent of the previous vectors N1, . . . , Nm−1, and θ ∈ Rd is an underlying model parameter. Both θ and Nm, m ≥ 1, are unknown to the learner.\nAlgorithm 1 WOFUL Input: regularization constant λ ≥ 0, confidence δ ∈ (0, 1), norm bound β, weight function w. Initialization: A1 = λId×d (d× d identity matrix), b1 = 0, θ̂1 = 0. for m = 1, 2, . . . do\nConfidence set computation Set Cm := { θ ∈ Rd : ∥∥∥θ − θ̂m∥∥∥ Am ≤ Dm } and\nDm :=\n√ 2 log ( det(Am)1/2 λd/2\nδ\n) + β √ λ.\nArm selection + feedback Let (xm, θ̃m) := arg min\n(x,θ′)∈X×Cm µx(θ\n′).\nChoose arm xm and observe cost cm. Update statistics\nUpdate Am+1 = Am + xmx\nT m\n‖xm‖2 ,\nbm+1 = bm + cmxm ‖xm‖ , and θ̂m+1 = A −1 m+1bm+1\nend for\nGiven a weight function w : [0, 1] → [0, 1], we define the weight-distorted cost µ(x, θ) for arm x ∈ X , with underlying model parameter θ, to be the quantity\nµx(θ) := ∫ ∞ 0 w(1−F θx (z))dz+ ∫ ∞ 0 w(F θx (−z))dz, (8)\nwhere F θx (z) := P [xT(θ +N) ≤ z], z ∈ R, is the cumulative distribution function of the stochastic cost from playing arm x ∈ X . An arm x is said to be optimal if its weight-distorted cost equals the least possible weightdistorted cost achieved across all arms, i.e., if µx = µ∗ := minx′∈X µx′(θ). As in the K-armed setting, the performance measure is the cumulative regret Rn over n rounds, defined as Rn = ∑n m=1 µxm(θ) − nµ∗, where xm is the arm chosen by the bandit algorithm in round m."
    }, {
      "heading" : "3.2 The WOFUL algorithm",
      "text" : "Algorithm 1 presents the pseudocode for the proposed algorithm, which follows the general template for linear bandit algorithms (cf. ConfidenceBall in [Dani et al., 2008] or OFUL in [Abbasi-Yadkori et al., 2011]), but deviates in the step when an arm is chosen. In particular, in any round m of the algorithm, WOFUL uses µx(θ) as the decision criterion for any arm x ∈ X and θ ∈ Cm, where µx(θ) is the weight-distorted value that is defined in (8) and Cm is the confidence ellipsoid that is specified in Algorithm 1. This is unlike regular linear bandit algorithms, which use xTθ as the cost for any arm x ∈ X and θ ∈ Cm. Note that the “in-parameter” or arm-dependent noise model (7) also necessitates modifying the standard confidence ellipsoid construction of Abbasi-Yadkori et al. [2011] by rescaling with the arm size (the Am and bm variables in Algorithm 1). For a positive semidefinite matrix M and a vector x, we use the notation ‖x‖M = √ xTMx to denote the Euclidean norm of x weighted by M .\nRemark 3. (Computation cost) The computationally intensive step in WOFUL is the optimization of the weightdistorted value over an ellipsoid in the parameter space (the third line in the for loop). This can be explicitly solved as follows. For a fixed x ∈ X , we can let θ̄m,x := arg min\nθ′∈Cm µx(θ ′) = arg min θ′∈Cm xTθ′ = θ̂m−DmA−1m x/ ‖x‖A−1 This\nis because the weight-distorted value is monotone under translation (see Lemma 5 below). The cost-minimizing arm is thus computed as xm = arg min{µx1(θ̄m,1), . . . , µx|X|(θ̄m,|X |)}."
    }, {
      "heading" : "3.3 Main results",
      "text" : "Theorem 4 (Regret bound for WOFUL). Suppose that the weight function w satisfies 0 ≤ w(p) ≤ 1, ∀p ∈ (0, 1), ∀x ∈ X : xTθ ∈ [−1, 1], and ‖θ‖2 ≤ β. Then, for any δ > 0, the regret Rn of WOFUL, run with parameters λ > 0, B, δ and w, satisfies P ( Rn ≤ √ 32dnDn log n ∀n ≥ 1 ) ≥ 1− δ.\nRemark 4. If for all x ∈ X , ‖x‖2 ≤ `, then the quantity Dn appearing in the regret bound above is\nO (√ d log ( n`2\nλδ\n)) [Abbasi-Yadkori et al., 2011, Lemma 10]; thus, the overall regret is1 Õ (d √ n).\nRemark 5. For the identity weight function w(t) = t, 0 ≤ t ≤ 1 with L = α = 1, we recover the stochastic linear bandit setting, and the associated Õ (d √ n) regret bound for linear bandit algorithms such as ConfidenceBall1 and ConfidenceBall2 [Dani et al., 2008], OFUL [Abbasi-Yadkori et al., 2011]. Hence, the result above is a generalization of regret bounds for standard linear bandit optimization to the case where a non-linear weight function of the cost distribution is to be optimized from linearly parameterized observations. The distortion of the cost distribution via a weight function, rather interestingly, does not impact the order of the bound on problemindependent regret, and we obtain Õ (d √ n) here as well.\nRemark 6. Note that the weight function w can be any non-linear function bounded in [0, 1]; unlike the K-armed setting, we do not impose a Hölder continuity assumption on w. Remark 7. A lower bound of essentially the same order as Theorem 4 (O(d √ n)) holds for regret in (undistorted) linear bandits [Dani et al., 2007]. One can show a similar lower bound argument with distortions, implying that the result of the theorem is not improvable (order-wise) across weight functions.\nRemark 8. (Linear bandits with arm-independent additive noise) An alternative to modelling “in-parameter” or arm-dependent noise (7) is to have independent additive noise, i.e., cm := xTmθ + ηm. This is a standard model of stochastic observations adopted in the linear bandit literature [Abbasi-Yadkori et al., 2011, Dani et al., 2008]. The key difference here is that, unlike the setting in (7), the noise component ηm does not depend on the arm played xm. In this case, Lemma 5 below shows that µX+a ≥ µX , i.e., the distorted CPT value µ preserves order under translations of random variables. As a consequence of this fact, the WOFUL algorithm reduces to the OFUL algorithm in the standard linear bandit setting with arm-independent noise.\nProof sketch for Theorem 4. We upper-bound the instantaneous regret rm as follows: Letting x̂m = xm‖xm‖ and N to be a standard Gaussian r.v. in d dimensions, we have\nrm = µxm(θ)− µx∗(θ) ≤ µxm(θ)− µxm(θ̃m) = ‖xm‖ ( µW+x̂Tmθ − µW+x̂Tmθ̃m ) (9)\n≤ 2 ‖xm‖ ∣∣∣x̂Tm(θ − θ̃m)∣∣∣ , (10)\nand the rest of the proof uses the standard confidence ellipsoid result that ensures θ resides in Cm with high probability. A crucial observation necessary to ensure (9) is that, for any r.v. X and any a ∈ R, the difference in weight-distorted cost µX+a−µX is a non-linear function of a (see Lemma 5 below). Thus, it is not straightforward to compute the weight-distorted cost after translation and this poses a significant challenge in the analysis of WOFUL for the arm-dependent noise model that we consider here.\nLemma 5. Let µX := ∫∞ 0 w(P [X > z])dz − ∫∞ 0 w(P [−X > z])dz. Then, for any a ∈ R, we have µX+a =\nµX + ∫ 0 −a [w(P [X > u]) + w(P [X < u])] du. Consequently, since w is bounded by 1, we have |µX+a − µX | ≤ 2|a|, for any a ∈ R.\nThe reader is referred to Section 4.2 for the detailed proof.\n1Õ(·) is a variant of the O(·) that ignores log-factors."
    }, {
      "heading" : "4 Convergence proofs",
      "text" : ""
    }, {
      "heading" : "4.1 Proofs for K-armed bandit",
      "text" : ""
    }, {
      "heading" : "4.1.1 Proof of Theorem 1",
      "text" : "The proof is very similar to that of Proposition 2 in [Prashanth et al., 2016]. The proof relies on the DvoretzkyKiefer-Wolfowitz (DKW) inequality, which gives finite-sample exponential concentration of the empirical distribution F̂k,m around the true distribution Fk, as measured by the || · ||∞ function norm [Wasserman, 2015, Chapter 2]."
    }, {
      "heading" : "4.1.2 Proof of Theorem 2",
      "text" : "Proof. As in the case of regular UCB, we bound the number of times a sub-optimal arm is pulled using the technique from Munos [2014].\nLet k∗ denote the optimal arm. Suppose that a sub-optimal arm k is pulled at time m, which implies that\nµ̂k,Tk(m−1) − γm,Tk(m−1) ≤ µ̂k∗,Tk∗ (m−1) − γm,Tk∗ (m−1). The B-value of arm k can be smaller than that of k∗ only if one of the following three conditions holds:\nµ∗ < µ̂k∗,Tk∗ (m−1) − γm,Tk∗ (m−1), (11) or\nµ̂k,Tk(m−1) + γm,Tk(m−1) < µk, (12) or\nµk − 2 γm,Tk(m−1) ≤ µ∗. (13)\nNote that condition (13) above is equivalent to requiring Tk(m− 1) ≤ 22/α−13(LM)2/α logm\n∆ 2/α k\n.\nLet κ = 22/α−13(LM)2/α log n\n∆ 2/α k\n+ 1. When Tk(m − 1) ≥ κ, i.e., the condition in (13) does not hold, then\neither (i) arm k is not pulled at time m, or (ii) (11) or (12) occurs. Thus, we have\nTk(n) ≤κ+ n∑\nm=κ+1\nI(Im = k;Tk(m) > κ) (14)\n≤κ+ n∑\nm=κ+1\nI((11) or (12) occur). (15)\nFrom Theorem 1, we can upper bound the probability of occurence of (11) as follows:\nP (∃1 < l < m such that Bl,Tk∗ (l−1)(k∗) < µ∗) ≤ m∑ l=1 2 m3 ≤ 2 m2 .\nA similar argument works for (12). Plugging the bounds on the events governed by (11) and (12) into (15) and taking expectations, we obtain\nE[Tk(n)] ≤κ+ 2 n∑\nm=κ+1\n2\nm2 (16)\n≤2 2/α−13(LM)2/α log n\n∆ 2/α k\n+ ( 1 + 2π2\n3\n) . (17)\nThe claim follows by plugging the inequality above into the definition of expected regret, and using the fact that ∆k ≤M for all arms k, as it follows that the weight-distorted cost of any distribution bounded by M again admits an upper bound of M ."
    }, {
      "heading" : "4.1.3 Proof of Corollary 1",
      "text" : "Proof. We have, by the analysis in the proof of Theorem 2, that for all k with ∆k > 0,\nE[Tk(n)] ≤ 3(2LM)2/α log n\n2∆ 2/α k\n+ ( 1 + 2π2\n3\n) .\nThus, we can write ERn = ∑ k ∆k E[Tk(n)] = ∑ k ( ∆k E[Tk(n)] α 2 ) ( E[Tk(n)]1− α 2 ) ≤\n(∑ k ∆ 2/α k E[Tk(n)] )α 2 (∑ k E[Tk(n)] )1−α2 (Hölder’s inequality with exponents 2/α and 2/(2− α))\n≤ ( 3K(2LM)2/α log n\n2 +KM2/α\n( 1 + 2π2\n3\n))α 2\nn 2−α 2 , (using ∆2/αk ≤M 2/α).\nThis proves the result."
    }, {
      "heading" : "4.1.4 Formal Regret Lower Bound",
      "text" : "We will need the following definition. For two probability distributions p and q on R, define the relative entropy or Kullback-Leibler (KL) divergence D(p||q) between p and q as\nD(p||q) = ∫ log ( dp\ndq (x)\n) dp(x)\nif p is absolutely continuous w.r.t. q (i.e., q(A) = 0⇒ p(A) = 0 for all Borel setsA), andD(p||q) =∞ otherwise. For instance, if p and q are Bernoulli distributions with parameters a ∈ (0, 1) and b ∈ (0, 1), respectively, then a simple calculation gives that D(p||q) = a log ab + (1 − a) log 1−a 1−b . We will often use D(F ||G) to mean the KL divergence between distributions represented by their cumulative distribution functions F and G, respectively.\nTheorem 6. [Regret lower bound] Consider a learning algorithm for the K-armed weight-distorted bandit problem with the following property. For any weight distortion function w : [0, 1]→ [0, 1], any set of cost distributions with costs bounded by M , any a > 0 and any arm k with ∆k > 0, the expected number of plays of arm k satisfies E [Tk(n)] = o(na). Then, for any constants L > 0 and α ∈ (0, 1], there exists a weight function w which is monotone increasing and α-Hölder continuous with constant L, and a set of cost distributions bounded by M , for which the algorithm’s regret satisfies\nlim inf n→∞ E [Rn] log n\n≥ ∑\n{k:∆k>0}\n(LM)2/α 4∆ 2/α−1 k .\nProof. We first show the result for costs bounded by M = 1, the extension to general M follows. The key ingredient in the proof is the seminal lower-bound on the number of suboptimal arm plays derived by Lai and Robbins [1985]. Their result shows that for any algorithm that plays suboptimal arms only a subpolynomial number of times in the time horizon,\nlim inf n→∞ E [Tk(n)] log n ≥ 1 D(Fk||Fk∗)\nfor any suboptimal arm k, where k∗ denotes the optimal arm. (Note that the argument at its core uses only a changeof-measure idea and the sub-polynomial regret hypothesis, and is thus unaffected by the fact that we measure regret by distorted, i.e., non-expected, costs.)\nUsing this along with the definition of regret, we get\nlim inf n→∞ E [Rn] log n = lim inf n→∞\n∑ {k:∆k>0} E[Tk(n)]∆k\nlog n ≥ ∑\n{k:∆k>0}\n∆k · lim inf n→∞ E[Tk(n)] log n\n≥ ∑\n{k:∆k>0}\n∆k D(Fk||Fk∗)\n= ∑\n{k:∆k>0}\n(2LM)2/α\n∆ 2/α−1 k\n· (∆k/2LM) 2/α\nD(Fk||Fk∗) . (18)\n(Recall that ∆k = µk − µk∗ represents the difference between the weight-distorted values of arms k and k∗.)\nWe now design a set of cost distributions for the arms, for which the limiting property claimed in the theorem holds. In fact, we will show that it enough to design Bernoulli distributions for the arms’ costs, i.e., arm k’s cost is Ber(pk), or equivalently, Fk(x) = (1 − pk)1[0,1)(x) + 1[1,∞)(x). This gives a simple expression for the weightdistorted cost of any arm k as µk = ∫ 1 0 w(pk)dz = w(pk), and, consequently, ∆k = w(pk) − w(pk∗), for any weight function w : [0, 1]→ [0, 1] with w(0) = w(1)− 1 = 0. Consider now a weight function w : [0, 1] → [0, 1] which is monotone increasing from 0 to 1, Hölder continuous with any desired constant L > 0 and exponent α ∈ (0, 1], and, moreover, satisfies the following “Hölder continuity property from below”: for some p̃ ∈ (0, 1) (say, p̃ = 1/2), |w(p)− w(p̃)| ≥ L|p− p̃|α. Such a weight function can always be constructed, e.g., for α = 1/2, L = 1, take the function w(x) = 12 − 1√ 2 √ 1 2 − x for\nx ∈ [0, 1/2], and w(x) = 12 + 1√ 2 √ x− 12 for x ∈ (1/2, 1]. (This is essentially formed by gluing together two inverted and scaled copies of the function √ x, to make an S-shaped function infinitely steep at x = 1/2.)\nFor such a weight function, putting pk∗ = p̃ = 1/2 and pk > pk∗ , k 6= k∗, we can use the standard bound2\nD(Fk||Fk∗) = pk log pk pk∗ + (1− pk) log 1− pk 1− pk∗\n≤ (pk − pk ∗)2\npk∗(1− pk∗) = 4(pk − pk∗)2\n≤ 4 ( w(pk)− w(pk∗)\nL\n)2/α (by the “lower-Hölder ” property of w)\n= 4 ( ∆k L )2/α .\nSince M = 1 for the Bernoulli family of cost distributions, this implies, together with (18), that\nlim inf n→∞ E [Rn] log n\n≥ ∑\n{k:∆k>0}\nL2/α\n4∆ 2/α−1 k\n.\nFor general M , one can modify this construction and consider arms’ cost distributions to be Bernoulli scaled by the constant M (i.e., equal to M with probability p and 0 otherwise). This completes the proof.\n2This results from using log x ≤ x− 1."
    }, {
      "heading" : "4.1.5 Cumulative prospect theory (CPT) for K-armed bandit",
      "text" : "As remarked in the introduction, a popular approach using a weight function to distort probabilities is the so-called cumulative prospect theory (CPT) of Tversky and Kahneman [1992]. In the following, we extend the W-UCB algorithm to incorporate CPT-style utility functions.\nDefine the CPT-value µk for any arm k as follows:\nµk := ∫ ∞ 0 w+(Pk(u +(X) > z))dz − ∫ ∞ 0 w−(Pk(u −(X) > z))dz, (19)\nwhere Pk(·) is the probability of event A for arm k, u+, u− : R → R+ are the utility functions and w+, w− : [0, 1]→ [0, 1] are weight functions.\nThe optimal arm is one that maximizes the CPT-value, i.e., µ∗ = maxk µk. At time instant n, let Xk,1, . . . , Xk,m be m samples from the cost distribution Fk for arm k. Order the samples\nin an ascending fashion using u+ as (u+(X[1,n]), . . . , u +(X[m,n]). The first integral in (19) is estimated by µ̂+m,n, defined by\nµ̂+k,m := m−1∑ i=1 u+(X[k,i]) ( w+( m− i m )− w+(m− i− 1 m ) ) .\nAlong similar lines, using u− obtain the following decreasing sample set: (u−(X[k,1]), . . . , u −(X[k,m]). The quantity µ̂ − k,m, defined below, serves as a proxy for the second integral in (19).\nµ̂−k,m := m−1∑ i=1 u−(X[k,i]) ( w−( i m )− w−( i− 1 m ) ) .\nNow, we define an m-sample approximation to µk as follows:\nµ̂k,m = µ̂ + k,m − µ̂ − k,m. (20)\nThe sample complexity result in Theorem 1 and the reget bounds in Theorems 2–3 can be easily extended to incorporate CPT style utilities described above."
    }, {
      "heading" : "4.2 Proofs for linear bandit",
      "text" : "We establish a few technical results in the following lemmas that are necessary for the proof of Theorem 4. The first step in the proof of Theorem 4 is to bound the instantaneous regret rm at instant m, which is the difference in weight-distorted value of arm x∗ and the arm xm chosen by the algorithm. For bounding the instantaneous regret, it is necessary to relate the difference in weight-distorted values of x∗ and xm to the difference in their means, i.e., (xT∗θ − xTmθ), and Lemma 5 provides this connection."
    }, {
      "heading" : "4.2.1 Proof of Lemma 5",
      "text" : "Proof.\nµX+a = ∫ ∞ 0 w(P [X + a > z])dz − ∫ ∞ 0 w(P [−X − a > z])dz\n= ∫ ∞ 0 w(P [X > z − a])dz − ∫ ∞ 0 w(P [−X > z + a])dz\n= ∫ ∞ −a w(P [X > u])du− ∫ ∞ a w(P [−X > u])du\n= ∫ 0 −a w(P [X > u])du− ∫ 0 a w(P [−X > u])du+ µX\n= ∫ 0 −a w(P [X > u])du+ ∫ 0 −a w(P [X < v])dv + µX .\nThis proves the main claim. Since w is bounded by 1, it is easy to infer that |µX+a − µX | ≤ 2|a|.\nLemma 7. Under the hypotheses of Theorem 4, for any δ > 0, the following event occurs with probability at least 1− δ: ∀m ≥ 1, θ lies in the set Cm defined in Algorithm 1, i.e.,\nCm=\n{ θ ∈ Rd : ∥∥∥θ − θ̂m∥∥∥ Am ≤ Dm } , where\nDm=\n√ 2 log ( det(Am)1/2 det(λId×d)1/2\nδ\n) + β √ λ,\nAm = λId×d + m−1∑ l=1 xlx T l ‖xl‖2 ,\nθ̂m = A −1 m bm, and bm = m−1∑ l=1 clxk ‖xl‖ .\nProof. The proof is a consequence of Theorem 2 in [Abbasi-Yadkori et al., 2011]. Indeed, the hypotheses of the stated theorem are satisfied with the sub-Gaussianity parameter R = 1 since by (7), the stochastic cost cm normalised by ‖xm‖ at each instant satisfies\ncm ‖xm‖ = xm ‖xm‖ T θ + xm ‖xm‖ T Nm,\nwhose distribution is Gaussian with mean xm‖xm‖ Tθ and variance 1, and which is thus sub-Gaussian with parameter R = 1.\nProof of Theorem 4\nWe first prove the following lemma which we will need in the proof of Theorem 4. It relates the weight-distorted value µx(θ) of the stochastic cost from an arm x ∈ X to the weight-distorted value of a translated standard normal distribution.\nLemma 8 (Weight-distorted value under scaling). If Z denotes a standard normal random variable, then for any arm x and θ,\nµx(θ) = ‖x‖ · µZ+ xTθ‖x‖ ,\nwhere µY denotes the weight-distorted value of a random variable Y .\nProof. Let X denote the stochastic cost from arm x with θ as the parameter; we have that X is normal with mean xTθ and variance ‖x‖2. With N being a standard normal d-dimensional vector and x̂ = x‖x‖ , we can write\nµX := ∫ ∞ 0 w(P [X > z])dz − ∫ ∞ 0 w(P [−X > z])dz\n= ∫ ∞ 0 w(P [xTmθ + xTmN > z])dz − ∫ ∞ 0 w(P [−xTmθ − xTmN > z])dz\n= ∫ ∞ 0 w(P [ x̂Tmθ + x̂ T mN > z ‖xm‖ ] )dz − ∫ ∞ 0 w(P [ −x̂Tmθ − x̂TmN > z ‖xm‖ ] )dz\n= ‖xm‖ (∫ ∞\n0\nw(P [x̂Tmθ + x̂TmN > y])dy − ∫ ∞\n0\nw(P [−x̂Tmθ − x̂TmN > y])dy )\n= ‖xm‖µZ+x̂Tmθ;\nProof of Theorem 4. Let rm = µxm(θ) − µx∗(θ) denote the instantaneous regret incurred by the algorithm that chooses arm xm ∈ X in round m. Let Yx (resp. Px) denote the r.v. (resp. probability function) governing the rewards of the arm x ∈ X .\nLetting x̂m = xm‖xm‖ and W to be a standard Gaussian r.v., we upper-bound the instantaneous regret rm at round m as follows:\nrm = µxm(θ)− µx∗(θ) ≤ µxm(θ)− µxm(θ̃m) (21)\n= ‖xm‖ ( µW+x̂Tmθ − µW+x̂Tmθ̃m ) (22)\n≤ 2 ‖xm‖ ∣∣∣x̂Tm(θ − θ̃m)∣∣∣ (23)\n= 2 ∣∣∣xTm(θ − θ̃m)∣∣∣ (24)\n≤ 2 (∣∣∣xTm(θ̂m − θ)∣∣∣+ ∣∣∣xTm(θ̃m − θ̂m)∣∣∣) (25)\n= 2 (∥∥∥θ̂m − θ∥∥∥ Am ‖xm‖A−1m + ∥∥∥θ̃m − θ̂m∥∥∥ Am ‖xm‖A−1m ) (26)\n≤ 4wm √ Dm whenever θ ∈ Cm, (27)\nwhere wm = √ xTmA −1 m xm and Dm is as defined in Algorithm 1. The inequalities above are derived as follows:\n• (21) follows from the choice of xm and θ̃m in Algorithm 1;\n• (22) follows from the scaling lemma (Lemma 8);\n• (23) follows by applying the translation lemma (Lemma 5);\n• (24) follows from the definition of x̂m;\n• (26) is by the Cauchy-Schwarz inequality applied to the pair of dual norms ‖·‖Am and ‖·‖A−1m on R d;\n• (27) follows from the definition of the confidence ellipsoid Cm in Algorithm 1, as θ̂m, θ̃m and θ belong to Cm. Note that Lemma 7 guarantees that this is true with probability at least 1− δ.\nThe cumulative regret Rn of WOFUL can now be bounded as follows. With probability at least 1− δ,\nRn = n∑\nm=1\nrm\n≤ n∑\nm=1\n4 √ Dmwm\n≤ 4 √ Dn n∑ m=1 wm ≤ 4 √ Dn ( n\nn∑ m=1 w2m\n)1/2 (28)\n≤ √ 32dnDn log n. (29)\nInequality (28) follows by an application of Cauchy-Schwarz inequality, while the inequality in (29) follows from (the standard by now) Lemma 9 in [Dani et al., 2008], which shows that ∑n m=1 w 2 m ≤ 2d log n. The claim follows."
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "We describe two sets of experiments pertaining to K-armed and linear bandit settings, respectively. For both problems, we take the S-shaped weight function w(p) = p η\n(pη+(1−p)η)1/η , with η = 0.61, to model perceived distortions in cost. Tversky and Kahneman observe that this distortion weight function is a good fit to explain distorted value preferences among human beings.\n5.1 Experiments for K-armed Bandit We study two stylized 2-armed bandit problems which, in part, draw upon experiments carried out by Tversky and Kahneman [1992] on human subjects in their studies of non-EU cumulative prospect theory. Figures 2 (a) and (b) describe each problem setting in detail (following the convention of this paper of modeling costs, ”$x w.p. p” is taken to mean a loss of $x suffered with a probability of p.)\nFor the first problem, the weight function w gives the distorted cost of arm 1 as $10.55, much higher than its expected cost of $5 and thus more expensive due to the deterministic Arm 2 with a (distorted and expected) cost of $7. The distortion in costs thus shifts the optimal arm from Arm 1 to Arm 2, and an online learning algorithm must be aware of this effect in order to attain low regret with respect to choosing Arm 2. A similar pattern is true for the other problem involving truly stochastic arms – weight distortion favors the arm with the higher cost in true expectation.\nWe benchmark the cumulative regret of two algorithms – (a) the well-known UCB algorithm [Auer et al., 2002], and (b) W-UCB; the results are as in Figure 2. In the experiments, UCB is not aware of the distorted weighting and hence attains linear regret with respect to playing the optimal distorted arm, due to converging to essentially a ‘wrong’ arm. On the other hand, the W-UCB algorithm, being designed to explicitly account for distorted cost perception, estimates the distortion using sample-based quantiles and exhibits significantly lower regret."
    }, {
      "heading" : "5.2 Experiments for Linear Bandit",
      "text" : "We study the problem of optimizing the route choice of a human traveler using Green light district (GLD) traffic simulation software Wiering et al. [2004]. In this setup, a source-destination pair is fixed in a given road network. Learning proceeds in an online fashion, where the algorithm chooses a route in each round and the system provides the (stochastic) delay for the chosen route. The objective is to find the “best” path that optimizes some function of delay, while not exploring too much. While traditional algorithms minimized the expected delay, in this work, we consider the distorted value (as defined in (8)) as the performance metric.\nWe implement both OFUL and WOFUL algorithms for this problem. Since the weight functionw is non-linear, a closed form expression for µx(θ̂m) is not available and we employ the empirical distribution scheme, described for the K-armed bandit setting (see (5)), for estimating the weight-distorted value. For this purpose, we simulate 25000 samples of the Gaussian distribution, as defined in (7).\nFor computing a performance benchmark to evaluate OFUL/WOFUL, we perform an experiment to learn a linear (additive) relationship between the delay of a route and the delays along each of its component lanes. This is reasonable considering the fact the individual delays along each lane stablize when the traffic network reaches steady state and the delay incurred along any route x would be centered around xTθ. For learning the linear relationship, we simulate 100000 steps of the traffic simulator to collect the delays along any route of the source-destination pair. Using these samples, we perform ridge regression to obtain θ̂off, i.e., we solve\nθ̂off = arg min θ\n1\n2 τ∑ i=1 (ci − θTxi)2 + λ ‖θ‖2 , (30)\nwhere ci is the ith delay sample corresponding to a route choice xi and τ is total number of delay samples for the source-destination pair considered. Further, λ is a regularization parameter. It is well known that θ̂off = A−1b,\nwhere A = τ∑ i=1 xix T i + λId and b = τ∑ i=1 cixi. Here Id denotes the d-dimensional identity matrix, with d set to the number of lanes in the road network considered. We set the various parameters of the problem as well as bandit routing algorithms - OFUL and WOFUL, as follows:\nAlgorithm Parameters\nOFUL Regularization parameter λ = 1, confidence δ = 0.99, norm bound β = 400\nWOFUL Regularization parameter λ = 1, confidence δ = 0.99, norm bound β = 400, weight function w(p) = p 0.61\n(p0.61+(1−p)0.61) 1 0.61\nWe observed that both OFUL and WOFUL algorithms learn the parameter underlying the linear relationship (Eq. (7) in the main paper) to a good accuracy. For instance, the normalized difference ∥∥∥θalg − θ̂off∥∥∥2 / ∥∥∥θ̂off∥∥∥2 ≈\n0.08 for OFUL (resp. 0.07 for WOFUL) on the 3x3-grid network. Figure 4 shows two road networks - a 3x3-grid and a 11-junction network - used for our experiments. Figure 3 presents the expected and weight-distorted values for OFUL and WOFUL for the 3x3-grid network, while Table 1 presents similar results for the 11-junction network. These values are calculated using a ridge regression-based estimate θ̂off of the true parameter θ. As expected, OFUL (resp. WOFUL) algorithm recommends a route xOFUL (resp. xWOFUL) with minimum mean delay (resp. weight-distorted value). As shown in Figure 3, xOFUL is the shortest path, while xWOFUL involves a detour. The lower value of distorted value (delay) for the longer path preferred by WOFUL is presumably due to the fact that the longer route has larger variance in end-to-end delays. Thus, the chance that the route’s delay is small (a rare event) is overestimated by the weight function, making it more appealing to the distortion-conscious WOFUL strategy."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We have designed online learning algorithms to minimize weight-distorted cost – a generalization of expected value – in both the standard (unstructured) k-armed bandit and the linearly parameterized bandit settings. Moving forward, it is of interest to study the general online reinforcement learning problem with weight-distorted cost metrics. Existing algorithms for expected value maximization such as UCRL [Jaksch et al., 2010] and PSRL [Osband et al., 2013] could be adapted for this purpose. Other interesting directions include considering contextual versions of the cost-distorted bandit problem, and perceived distortions in the observations."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Yasin Abbasi-Yadkori", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Le comportement de l’homme rationel devant le risque",
      "author" : [ "M. Allais" ],
      "venue" : "Critique des postulats et axioms de l’ecole americaine. Econometrica,",
      "citeRegEx" : "Allais.,? \\Q1953\\E",
      "shortCiteRegEx" : "Allais.",
      "year" : 1953
    }, {
      "title" : "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits",
      "author" : [ "Jean-Yves Audibert", "Rémi Munos", "Csaba Szepesvári" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2009
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2003
    }, {
      "title" : "An experimental test of several generalized utility theories",
      "author" : [ "Colin F Camerer" ],
      "venue" : "Journal of Risk and Uncertainty,",
      "citeRegEx" : "Camerer.,? \\Q1989\\E",
      "shortCiteRegEx" : "Camerer.",
      "year" : 1989
    }, {
      "title" : "Recent tests of generalizations of expected utility theory. In Utility Theories: Measurements and Applications, pages 207–251",
      "author" : [ "Colin F Camerer" ],
      "venue" : null,
      "citeRegEx" : "Camerer.,? \\Q1992\\E",
      "shortCiteRegEx" : "Camerer.",
      "year" : 1992
    }, {
      "title" : "New measures for performance evaluation",
      "author" : [ "Alexander Cherny", "Dilip Madan" ],
      "venue" : "Review of Financial Studies,",
      "citeRegEx" : "Cherny and Madan.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cherny and Madan.",
      "year" : 2009
    }, {
      "title" : "Three variants on the Allais example",
      "author" : [ "John Conlisk" ],
      "venue" : "The American Economic Review, pages 392–407,",
      "citeRegEx" : "Conlisk.,? \\Q1989\\E",
      "shortCiteRegEx" : "Conlisk.",
      "year" : 1989
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "Varsha Dani", "Sham M. Kakade", "Thomas P. Hayes" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dani et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2007
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Varsha Dani", "Thomas P Hayes", "Sham M Kakade" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "On the shape of the probability weighting function",
      "author" : [ "Richard Gonzalez", "George Wu" ],
      "venue" : "Cognitive Psychology,",
      "citeRegEx" : "Gonzalez and Wu.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gonzalez and Wu.",
      "year" : 1999
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "T. Jaksch", "R. Ortner", "P. Auer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "Herbert Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning",
      "author" : [ "Rémi Munos" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Munos.,? \\Q2014\\E",
      "shortCiteRegEx" : "Munos.",
      "year" : 2014
    }, {
      "title" : "More) Efficient reinforcement learning via posterior sampling",
      "author" : [ "Ian Osband", "Dan Russo", "Benjamin Van Roy" ],
      "venue" : "In Proc. Neural Information Processing Systems,",
      "citeRegEx" : "Osband et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2013
    }, {
      "title" : "Cumulative prospect theory meets reinforcement learning: prediction and control",
      "author" : [ "L.A. Prashanth", "Jie Cheng", "Michael Fu", "Steve Marcus", "Csaba Szepesvári" ],
      "venue" : "Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Prashanth et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Prashanth et al\\.",
      "year" : 2016
    }, {
      "title" : "The probability weighting function",
      "author" : [ "Drazen Prelec" ],
      "venue" : "Econometrica, pages 497–527,",
      "citeRegEx" : "Prelec.,? \\Q1998\\E",
      "shortCiteRegEx" : "Prelec.",
      "year" : 1998
    }, {
      "title" : "Generalized Expected Utility Theory: The Rank-dependent",
      "author" : [ "John Quiggin" ],
      "venue" : "Model. Springer Science & Business Media,",
      "citeRegEx" : "Quiggin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Quiggin.",
      "year" : 2012
    }, {
      "title" : "Developments in non-expected utility theory: The hunt for a descriptive theory of choice under risk",
      "author" : [ "Chris Starmer" ],
      "venue" : "Journal of Economic Literature,",
      "citeRegEx" : "Starmer.,? \\Q2000\\E",
      "shortCiteRegEx" : "Starmer.",
      "year" : 2000
    }, {
      "title" : "Advances in prospect theory: Cumulative representation of uncertainty",
      "author" : [ "A. Tversky", "D. Kahneman" ],
      "venue" : "Journal of Risk and Uncertainty,",
      "citeRegEx" : "Tversky and Kahneman.,? \\Q1992\\E",
      "shortCiteRegEx" : "Tversky and Kahneman.",
      "year" : 1992
    }, {
      "title" : "Simulation and optimization of traffic in a city",
      "author" : [ "M. Wiering", "J. Vreeken", "J. van Veenen", "A. Koopman" ],
      "venue" : "In IEEE Intelligent Vehicles Symposium,",
      "citeRegEx" : "Wiering et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Wiering et al\\.",
      "year" : 2004
    }, {
      "title" : "Curvature of the probability weighting function",
      "author" : [ "George Wu", "Richard Gonzalez" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Wu and Gonzalez.,? \\Q1996\\E",
      "shortCiteRegEx" : "Wu and Gonzalez.",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For the linearly parameterized setting, we develop a new algorithm, a variant of the Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm Abbasi-Yadkori et al. [2011] called WOFUL (Weight-distorted OFUL), and show that it has regret O(d √ n polylog(n)) with high probability, for sub-Gaussian cost distributions.",
      "startOffset" : 152,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems.",
      "startOffset" : 149,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems. Violations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4].",
      "startOffset" : 149,
      "endOffset" : 483
    }, {
      "referenceID" : 1,
      "context" : "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems. Violations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4]. Probabilistic distortions have a long history in behavioral science and economics, and we bring this idea to a multi-armed bandit setup. In particular, we base our approach on rank-dependent expected utility (RDEU) Quiggin [2012], which includes the popular cumulative prospect theory (CPT) of Tversky and Kahneman Tversky and Kahneman [1992].",
      "startOffset" : 149,
      "endOffset" : 741
    }, {
      "referenceID" : 1,
      "context" : "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems. Violations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4]. Probabilistic distortions have a long history in behavioral science and economics, and we bring this idea to a multi-armed bandit setup. In particular, we base our approach on rank-dependent expected utility (RDEU) Quiggin [2012], which includes the popular cumulative prospect theory (CPT) of Tversky and Kahneman Tversky and Kahneman [1992]. ∗aditya@ece.",
      "startOffset" : 149,
      "endOffset" : 854
    }, {
      "referenceID" : 20,
      "context" : "The weight function used in the figure is the one recommended by Tversky and Kahneman Tversky and Kahneman [1992] based on empirical tests involving human subjects.",
      "startOffset" : 65,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].",
      "startOffset" : 123,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].",
      "startOffset" : 123,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].",
      "startOffset" : 162,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].",
      "startOffset" : 178,
      "endOffset" : 224
    }, {
      "referenceID" : 5,
      "context" : "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999]. We use a traveler’s route choice as a running example to illustrate the main ideas in this paper.",
      "startOffset" : 178,
      "endOffset" : 248
    }, {
      "referenceID" : 20,
      "context" : "We provide upper bounds on the regret of W-UCB, assuming w is Hölder continuous and provide empirical demonstrations on a setting from [Tversky and Kahneman, 1992].",
      "startOffset" : 135,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "For the linear bandit setting, we propose a variant of the OFUL algorithm [Abbasi-Yadkori et al., 2011], that incorporates weight-distorted values in the arm selection step.",
      "startOffset" : 74,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "The W-UCB algorithm that we propose incorporates a empirical distribution-based approach, similar to that of Prashanth et al. [2016], to estimate μx.",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Related work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : "Related work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting. In contrast, we formulate two multi-armed bandit models that incorporate weight-distortions. From a theoretical standpoint, we handle the exploration-exploitation tradeoff via UCB-inspired algorithms, while the focus of Prashanth et al. [2016] was to devise a policy-gradient scheme given biased estimates of a certain CPT-value defined for each policy.",
      "startOffset" : 66,
      "endOffset" : 419
    }, {
      "referenceID" : 16,
      "context" : "Related work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting. In contrast, we formulate two multi-armed bandit models that incorporate weight-distortions. From a theoretical standpoint, we handle the exploration-exploitation tradeoff via UCB-inspired algorithms, while the focus of Prashanth et al. [2016] was to devise a policy-gradient scheme given biased estimates of a certain CPT-value defined for each policy. Moreover, we provide finite-time regret bounds for both bandit settings, while the guarantees for the policy gradient algorithm of Prashanth et al. [2016] are asymptotic in nature.",
      "startOffset" : 66,
      "endOffset" : 684
    }, {
      "referenceID" : 4,
      "context" : ", EXP3 [Auer et al., 2003], are not inherently suitable for this problem, as they do not factor in the distortion caused in (expected) reward.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "A similar observation holds for conventional stochastic bandit algorithms such as UCB, and algorithms sensitive to arm reward variances such as UCB-V [Audibert et al., 2009] – once weight distortion is incorporated, the algorithm will converge to an arm that is not weight-distorted value optimal.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 16,
      "context" : "Thus, one needs to estimate the entire distribution, and for this purpose, we adapt the quantile-based approach of [Prashanth et al., 2016].",
      "startOffset" : 115,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Moreover, the popular choice for the weight function, proposed by Tversky and Kahneman [1992] and illustrated in Figure 1, is Hölder continuous.",
      "startOffset" : 66,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "(CPT adaptation) As remarked in the introduction, a popular approach using a weight function to distort probabilities is the so-called cumulative prospect theory (CPT) [Tversky and Kahneman, 1992].",
      "startOffset" : 168,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "ConfidenceBall in [Dani et al., 2008] or OFUL in [Abbasi-Yadkori et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : ", 2008] or OFUL in [Abbasi-Yadkori et al., 2011]), but deviates in the step when an arm is chosen.",
      "startOffset" : 19,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : ", 2008] or OFUL in [Abbasi-Yadkori et al., 2011]), but deviates in the step when an arm is chosen. In particular, in any round m of the algorithm, WOFUL uses μx(θ) as the decision criterion for any arm x ∈ X and θ ∈ Cm, where μx(θ) is the weight-distorted value that is defined in (8) and Cm is the confidence ellipsoid that is specified in Algorithm 1. This is unlike regular linear bandit algorithms, which use xθ as the cost for any arm x ∈ X and θ ∈ Cm. Note that the “in-parameter” or arm-dependent noise model (7) also necessitates modifying the standard confidence ellipsoid construction of Abbasi-Yadkori et al. [2011] by rescaling with the arm size (the Am and bm variables in Algorithm 1).",
      "startOffset" : 20,
      "endOffset" : 627
    }, {
      "referenceID" : 10,
      "context" : "For the identity weight function w(t) = t, 0 ≤ t ≤ 1 with L = α = 1, we recover the stochastic linear bandit setting, and the associated Õ (d √ n) regret bound for linear bandit algorithms such as ConfidenceBall1 and ConfidenceBall2 [Dani et al., 2008], OFUL [Abbasi-Yadkori et al.",
      "startOffset" : 233,
      "endOffset" : 252
    }, {
      "referenceID" : 0,
      "context" : ", 2008], OFUL [Abbasi-Yadkori et al., 2011].",
      "startOffset" : 14,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "A lower bound of essentially the same order as Theorem 4 (O(d √ n)) holds for regret in (undistorted) linear bandits [Dani et al., 2007].",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "1 Proof of Theorem 1 The proof is very similar to that of Proposition 2 in [Prashanth et al., 2016].",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "As in the case of regular UCB, we bound the number of times a sub-optimal arm is pulled using the technique from Munos [2014]. Let k∗ denote the optimal arm.",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "The key ingredient in the proof is the seminal lower-bound on the number of suboptimal arm plays derived by Lai and Robbins [1985]. Their result shows that for any algorithm that plays suboptimal arms only a subpolynomial number of times in the time horizon,",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "5 Cumulative prospect theory (CPT) for K-armed bandit As remarked in the introduction, a popular approach using a weight function to distort probabilities is the so-called cumulative prospect theory (CPT) of Tversky and Kahneman [1992]. In the following, we extend the W-UCB algorithm to incorporate CPT-style utility functions.",
      "startOffset" : 208,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "The proof is a consequence of Theorem 2 in [Abbasi-Yadkori et al., 2011].",
      "startOffset" : 43,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "(29) Inequality (28) follows by an application of Cauchy-Schwarz inequality, while the inequality in (29) follows from (the standard by now) Lemma 9 in [Dani et al., 2008], which shows that ∑n m=1 w 2 m ≤ 2d log n.",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "We benchmark the cumulative regret of two algorithms – (a) the well-known UCB algorithm [Auer et al., 2002], and (b) W-UCB; the results are as in Figure 2.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "1 Experiments for K-armed Bandit We study two stylized 2-armed bandit problems which, in part, draw upon experiments carried out by Tversky and Kahneman [1992] on human subjects in their studies of non-EU cumulative prospect theory.",
      "startOffset" : 132,
      "endOffset" : 160
    }, {
      "referenceID" : 21,
      "context" : "2 Experiments for Linear Bandit We study the problem of optimizing the route choice of a human traveler using Green light district (GLD) traffic simulation software Wiering et al. [2004]. In this setup, a source-destination pair is fixed in a given road network.",
      "startOffset" : 165,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Existing algorithms for expected value maximization such as UCRL [Jaksch et al., 2010] and PSRL [Osband et al.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : ", 2010] and PSRL [Osband et al., 2013] could be adapted for this purpose.",
      "startOffset" : 17,
      "endOffset" : 38
    } ],
    "year" : 2016,
    "abstractText" : "Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the cost distributions: the classic K-armed bandit and the linearly parameterized bandit. In both settings, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate cost distortions, and exhibit sublinear regret assuming Hölder continuous weight distortion functions. For the K-armed setting, we show that the algorithm, called W-UCB, achieves problem-dependent regret O ( LM logn/∆ 2 α −1 ) , where n is the number of plays, ∆ is the gap in distorted expected value between the best and next best arm, L and α are the Hölder constants for the distortion function, and M is an upper bound on costs, and a problem-independent regret bound of O((KL2M2)α/2n(2−α)/2). We also present a matching lower bound on the regret, showing that the regret of W-UCB is essentially unimprovable over the class of Hölder -continuous weight distortions. For the linearly parameterized setting, we develop a new algorithm, a variant of the Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm Abbasi-Yadkori et al. [2011] called WOFUL (Weight-distorted OFUL), and show that it has regret O(d √ n polylog(n)) with high probability, for sub-Gaussian cost distributions. Finally, numerical examples demonstrate the advantages resulting from using distortion-aware learning algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}