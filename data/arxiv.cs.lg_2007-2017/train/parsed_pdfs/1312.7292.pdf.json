{
  "name" : "1312.7292.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning for Sleep–Wake Scheduling in Sensor Networks",
    "authors" : [ "Prashanth L A", "Abhranil Chatterjee", "Shalabh Bhatnagar" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 2.\n72 92\nv1 [\ncs .S\nY ]\nKeywords: Sensor Networks, Sleep-Wake Scheduling, Reinforcement Learning, Q-learning, Simultaneous Perturbation, Function Approximation, SPSA."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of optimizing the number of sensors active in an intrusion detection application. We consider a centralized control setting for a sensor network involving N sensors and assume for simplicity that the sensors fully cover the area of interest. Each sensor can be either awake (i.e., active) or asleep. The control center collects sensing information periodically and then decides on the sleeping policy for the sensors. The movement of the intruder is described by a Markov chain whose state is the current location of the object, to within the accuracy of a sensing region. The challenge is to balance the conflicting objectives of minimizing the number of sensors awake to reduce the energy cost, while at the same time having enough number of sensors awake to ensure a good tracking accuracy.\nWe formulate this problem as a partially-observable Markov decision process (POMDP) in a manner similar to the one considered in Fuemmeler and Veeravalli [2008]. However, unlike their total cost objective, in this paper we consider infinite horizon average as well as discounted cost objectives. The rationale behind the average cost objective is to understand the steady-state system behavior, whereas the discounted cost objective is more\n∗prashanth.la@inria.fr †abhranilc@ee.iisc.ernet.in ‡shalabh@csa.iisc.ernet.in\nsuitable for studying the transient behavior of the system. We develop two novel reinforcement learning based algorithms, each for the average and the discounted settings, respectively. Function approximation and featurebased representations are incorporated in both our algorithms to handle the curse of dimensionality associated with high-dimensional state spaces that we encounter for the sleep-wake scheduling problem considered in this paper. Further, these techniques also considerably simplify the implementation of our algorithms, owing to the computational efficiency (both space and time) advantages of function approximation. To the best of our knowledge, reinforcement learning with function approximation for sleep-wake scheduling has not been considered previously in the literature.\nIn the average cost POMDP setting, we propose two algorithms. The first algorithm that we propose uses the Q-learning analogue for the average cost setting with function approximation. On the other hand, the second algorithm proposed is a novel two-timescale algorithm that performs on-policy Q-learning while employing function approximation. This algorithm incorporates a policy gradient update using a one-simulation simultaneous perturbation stochastic approximation (SPSA) estimate on the faster timescale, while the Q-value parameter (arising from a linear function approximation architecture for the Q-values) is updated in an on-policy temporal difference (TD) algorithm-like fashion on the slower timescale. The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material.\nNext, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation.\nFinally, we also consider a setting where the mobility model of the intruder is not available and develop an online scheme that estimates the same. We combine this estimation procedure with the sleep-wake scheduling algorithms mentioned above using a multi-timescale scheme.\nWe study our algorithms on a simple two-dimensional network setting (see Fig. 1) and compare their performance with the QMDP and FCR algorithms from Fuemmeler and Veeravalli [2008]. Our algorithms are seen to be easily implementable, converge rapidly with a short transient period and provide more consistent results than the QMDP and FCR algorithms. Further, we observe that the procedure for estimating the mobility model of the intruder converges empirically to the true model. A short version of this paper with only the average cost criterion and without the convergence proofs is available in Prashanth et al. [2014].\nThe rest of the paper is organized as follows: In Section 2, we review relevant literature in the area of sleepwake scheduling as well as reinforcement learning. In Section 3, we formulate the sleep-wake scheduling problem as a POMDP and describe the long-run performance objectives (both average and discounted) for our algorithms. In Section 4, we present two novel RL-based sleep-wake scheduling algorithms for the average cost setting, while in Section 5, we extend these algorithms to the discounted cost setting. In Section 6, we present the mobility model estimation scheme. In Section 7, we describe the experimental setup and present the results in both average and discounted cost settings. Finally, in Section 8, we provide the concluding remarks and outline a few future research\ndirections."
    }, {
      "heading" : "2 Literature Review",
      "text" : "Sleep-wake scheduling: A Markov decision process (MDP) model for intrusion detection has also been formulated in Premkumar and Kumar [2008], where the authors present three sleep/wake scheduling algorithms to control the number of sensors in the wake state. However, the algorithms proposed there assume a system model, whereas we propose model-free RL based algorithms that attempt to find a ‘good’ sleep/wake scheduling policy. In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking. However, unlike us, their algorithms are under the waking channel assumption, i.e., a setting where the central controller can communicate with a sensor that is in the sleep state. In Jiang et al. [2008], a sleep/wake scheduling algorithm based on the target’s moving direction has been proposed. In Jin et al. [2006], the authors present a heuristic algorithm that uses dynamic clustering of sensors to balance energy cost and tracking error. In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality.\nIn comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a ‘good enough’ policy that minimizes the long-run average sum of this cost. The term ‘good enough’ here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL. Q-learning Watkins and Dayan [1992] is a well-known RL algorithm for finding the optimal policy. However, for problems involving high-dimensional state spaces, the Q-learning algorithm with function approximation may diverge or may show large oscillations, Baird [1995]. This is primarily due to the inherent nonlinearity in the Q-learning update rule resulting from the explicit maximization/minimization in the update procedure. A two-timescale variant of the Q-learning algorithm, proposed in Bhatnagar and Lakshmanan [2012], avoids this problem by using two timescales, where on the faster timescale the policy parameter is tuned in the negative gradient direction using SPSA estimates of the gradient and on the slower timescale, a TD-like update for the parameters is performed."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "We consider a centralized control setting for a sensor network with N sensors and assume for simplicity that the sensors cover the area of interest without overlaps. Each sensor can be either awake (i.e., active) or asleep. The control center collects sensing information in each period and decides on the sleeping policy for each sensor for the next period. The location of the intruder at any instant can be any one of the N cells corresponding to the N sensors. The intruder movement is given by a probability transition matrix P of size N ×N , where each entry Pij is the probability of the intruder moving from location i to j."
    }, {
      "heading" : "3.1 States, Actions and Observations",
      "text" : "The state sk at instant k for our problem is sk = (lk, rk), where rk = (rk(1), . . . , rk(N)), is the vector of residual (or remaining) sleep times, with rk(i) denoting the residual sleep time of sensor i at time instant k. Further, lk refers to the location of the object at instant k and can take values 1, . . . , N . The residual sleep time vector rk evolves as follows: ∀i = 1, . . . , N ,\nrk+1(i) = (rk(i)− 1)I{rk(i)>0} + ak(i)I{rk(i)=0}. (1)\nThe first term above indicates that the residual sleep time is decremented by 1 if sensor i is in sleep state, while the second term expresses that if sensor i is in wake state, it is assigned a sleep time of ak(i). Here ak = (ak(1), . . . , ak(N)) denotes the chosen sleep configuration of the N sensors at instant k.\nThe single-stage cost function has two components - an energy cost for sensors in the wake state and a tracking cost. We use an energy cost c ∈ (0, 1) for each sensor that is awake and a tracking cost of 1 if the intruder location is unknown. Let Sk denote the set of indices of sensors that are in sleep state. Then the single-stage cost g(sk, ak) at instant k has the form,\ng(sk, ak) = ∑\n{i:rk(i)=0}\nc+ I{rk(lk)>0}. (2)\nIn the above, lk denotes the location of the object at time k. The algorithms that we design subsequently find the optimal strategy for minimizing the single-stage cost (2) in the long-run average cost sense. Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1.\nThe states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value ǫ otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak−1), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is ŝk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above. Note that pk = (pk(1), ..., pk(N)) is the distribution at time step k of the object being in one of the locations 1, 2, ..., N and evolves according to\npk+1 = elk+1I{rk+1(lk+1)=0} + pkPI{rk+1(lk+1)>0}, (3)\nwhere ei denotes an N -dimensional unit vector with 1 in the ith position and 0 elsewhere. The idea behind the evolution of pk is as follows: (i) the first term refers to the case when the location of the intruder is known, i.e., the sensor at lk+1 is in the wake state; (ii) the second term refers to the case when intruder’s location is not known and hence, the intruder transitions to the next distribution pk+1 from the current pk via the transition probability matrix P . Note that the evolution of pk in our setting differs from Fuemmeler and Veeravalli [2008], as we do not have the termination state. With an abuse of terminology, henceforth we shall refer to the sufficient statistic ŝk as the state vector in the algorithms we propose next. Further, we would like to emphasize here that our algorithms do not require full observation of the state vector. Instead, by an intelligent choice of features that rely only on pk, the algorithms obtain a sleeping policy that works well.\n1Since we study long-run average sum of (2) (see (4) below), we can consider the problem of tracking an intruder in an infinite horizon, whereas a termination state in Fuemmeler and Veeravalli [2008] was made necessary as they considered a total cost objective."
    }, {
      "heading" : "3.2 Average Cost Objective",
      "text" : "The long-run average cost J(π) for a given policy π is defined as follows:\nJ(π) = lim N→∞\n1\nN\nN−1 ∑\nn=0\ng(sn, an), (4)\nstarting from any given state i (i.e., with s0 = i). In the above, the policy π = {π1, π2, . . .} with πn governing the choice of action an at each instant n.\nThe aim here is to find a policy π∗ = argminπ∈Π J(π), where Π is the set of all admissible policies. A policy π is admissible if it suggests a feasible action at each time instant n.\nLet h(i) be the differential cost function corresponding to state i, under policy π. Then,\nh(i) =\n∞ ∑\nn=1\nE [g(sn, an)− J(π)| s0 = i, π] , (5)\nis the expected sum of the differences between the single-stage cost and the average cost under policy π when i ∈ S is the initial state. Let J∗ = minπ∈Π J(π) △ = J(π∗) denote the optimal average cost. The following Bellman equation is satisfied (Chapter 4 of Bertsekas [2007]):\nJ∗ + h∗(i) = min a\n(g(i, a) + ∑\nj\np(i, j, a)h∗(j)), ∀i ∈ S, (6)\nwhere h∗ is the optimal differential cost function corresponding to the policy π∗. Now, define the optimal Q-factors Q∗(i, a), i ∈ S, a ∈ A(i) as\nQ∗(i, a) = g(i, a) + ∑\nj\np(i, j, a)h∗(j). (7)\nFrom (6) and (7), we have J∗ + h∗(i) = min\na Q∗(i, a), ∀i ∈ S. (8)\nNow from (7) and (8), we have\nQ∗(i, a) = g(i, a) + ∑\nj\np(i, j, a)(min b\nQ∗(j, b)− J∗) or\nJ∗ +Q∗(i, a) = g(i, a) + ∑\nj\np(i, j, a) min b∈A(j)\nQ∗(j, b), (9)\n∀i ∈ S, a ∈ A(i). An advantage with (9) is that it is amenable to stochastic approximation because the minimization is now (unlike (6)) inside the conditional expectation. However, in order to solve (9), one requires knowledge of the transition probabilities p(i, j, a) that constitute the system model. Moreover, one requires the state and action spaces to be manageable in size. The algorithms presented subsequently work under lack of knowledge about the system model and further, are able to effectively handle large state and action spaces by incorporating feature based representations and function approximation."
    }, {
      "heading" : "3.3 The Discounted Cost Objective",
      "text" : "We now describe the discounted cost objective. For a policy π, define the value function V π : S → R as follows:\nV π(i) = E\n[\n∞ ∑\nm=0\nγmg(sm, am) | X0 = i\n]\n, (10)\nfor all i ∈ S. In the above, γ ∈ (0, 1) is a given discount factor. The aim then is to find an optimal value function V ∗ : S → R, i.e.,\nV ∗(i) = min π∈Π\nV π(i) △ = V (π∗), (11)\nwhere π∗ is the optimal policy, i.e., the one for which V ∗ is the value function. It is well known, see Puterman [1994], that the optimal value function V ∗(·) satisfies the following Bellman equation of optimality in the discounted cost case:\nV ∗(i) = min a∈A(i)\n\ng(i, a) + γ ∑\nj∈S\np(i, j, a)V ∗(j)\n\n , (12)\nfor all i ∈ S. As for the average cost, our algorithms in the discounted cost setting do not require knowledge of the system model and incorporate function approximation."
    }, {
      "heading" : "4 Algorithms for Average Cost Setting",
      "text" : "Before we describe our algorithms, we first discuss a well-known RL algorithm called Q-learning that uses fullstate representations and then discuss the difficulty in using this algorithm on a high-dimensional state space (as is the case with the sleep-wake control MDP).\nQ-learning with full state representation\nThis algorithm is based on the relative Q-value iteration procedure. Let sn+1 denote the state of the system at instant (n+ 1) when the state at instant n is i and action chosen is a. Let Qn(i, a) denote the Q-value estimate at instant n associated with the tuple (i, a). The relative Q-value iteration (RQVI) scheme is\nQn+1(i, a) =g(i, a) + ∑\nj\np(i, j, a) min b∈A(j) Qn(j, b)− min r∈A(s) Qn(s, r), (13)\nwhere s ∈ S is a prescribed (arbitrarily chosen) state. The Q-learning algorithm for the average cost setting estimates the ‘Q-factors’ Q(i, a) of all feasible state-action tuples (i, a), i.e., those with i ∈ S and a ∈ A(i) using the stochastic approximation version of (13) (see Abounadi et al. [2002]). The update rule for this algorithm is given by\nQn+1(i, a) =Qn(i, a) + a(n)(g(i, a) + min b∈A(j) Qn(j, b)− min r∈A(s) Qn(s, r)), (14)\nfor all i ∈ S and a ∈ A(s). In the above, j is the simulated next state after i when action a is chosen in state i and a(n), n ≥ 0 are the step-sizes that satisfy the standard stochastic approximation conditions, i.e., ∑\nn a(n) = ∞ and ∑\nn a(n) 2 < ∞. The last term minr∈A(s)Qn(s, r) in (14) asymptotically converges to the optimal average\ncost per stage. Further, the iterates in (14) converge to the optimal Q-values Q∗(i, a) that satisfy the corresponding Bellman equation (9) and mina∈A(i) Qn(i, a) gives the optimal differential cost h∗(i). The optimal action in state i corresponds to argmina∈A(i) Q ∗(i, a).\nNeed for function approximation\nWhile Q-learning does not require knowledge of the system model, it does suffer from the computational problems associated with large state and action spaces as it stores the Q(s, a) values in a look-up table and requires updates of all Q(s, a) values at each step for convergence. In our setting, this algorithm becomes intractable as the stateaction space becomes very large. Even when we quantize probabilities as multiples of 0.01, and with 7 sensors, the cardinality of the state-action space |S × A(S)| is approximately 1008 × 47 × 47 if we use an upper bound of 3 for the sleep time alloted to any sensor. The situation gets aggravated when we consider larger sensing regions (with corresponding higher number of sensors). To deal with this problem of the curse of dimensionality,\nwe develop a feature based Q-learning algorithm as in Prashanth and Bhatnagar [2011a]. While the full state Qlearning algorithm in (14) cannot be used on even moderately sized sensing regions, its function approximation based variant can be used over larger network settings."
    }, {
      "heading" : "4.1 Algorithm Structure",
      "text" : "Both our algorithms parameterize the Q-function using a linear approximation architecture as follows:\nQ(s, a) ≈ θTσs,a, ∀s ∈ S, a ∈ A(s). (15)\nIn the above, σs,a is a given d-dimensional feature vector associated with the state-action tuple (s, a), where d << |S ×A(S)| and θ is a tunable d-dimensional parameter.\nOur algorithms are online, incremental and obtain the sleeping policy by sampling from a trajectory of the system. After observing a simulated sample of the single-stage cost, the parameter θ is updated in the negative descent direction in both our algorithms as follows:\nθn+1 = Γ(θn − a(n)σ(sn, an)mn), (16)\nwhere mn is an algorithm-specific magnitude term and Γ is a projection operator that keeps the parameter θ bounded (a crucial requirement towards ensuring convergence of the scheme). Further, a(n) are the step-sizes that satisfy standard stochastic approximation conditions. Note that ∇θQ(s, a) = σs,a and hence (16) updates the parameter θ in the negative descent direction. The overall structure of our algorithms is given in Algorithm 1.\nAlgorithm 1 Structure of our algorithms\n1: Initialization: policy parameter θ = θ0; initial state s0 2: for n = 0, 1, 2, . . . do 3: Take action an based on a (algorithm-specific) policy depending on θn. 4: Observe the single-stage cost g(sn, an) and the next state sn+1. 5: Update θn+1 in a algorithm-specific manner. 6: end for 7: return Q-value parameter θ, policy parameter w.\nWe first present a sleep–wake scheduling algorithm which is the function approximation analogue of the Qlearning with average cost algorithm proposed in Abounadi et al. [2002]. While this algorithm that we refer to as QSA-A is shown to work well in the numerical experiments, it does not possess theoretical convergence guarantees. The second algorithm (TQSA-A) that we propose is a two-timescale algorithm with proven convergence to the optimal policy. A detailed proof of convergence of this algorithm is provided in the supplementary material."
    }, {
      "heading" : "4.2 Q-learning based Sleep–wake Algorithm (QSA-A)",
      "text" : "Let sn, sn+1 denote the state at instants n, n + 1, respectively, measured online. Let θn be the estimate of the parameter θ at instant n. Let s be any fixed state in S. The algorithm QSA-A uses the following update rule:\nθn+1 =θn + a(n)σsn,an\n(\ng(sn, an) + min v∈A(sn+1) θTnσsn+1,v − min r∈A(s) θTnσs,r − θ T nσsn,an\n)\n, (17)\nwhere θ0 is set arbitrarily. In (17), the action an is chosen in state sn according to an ǫ-greedy policy, i.e., with a probability of (1− ǫ), a greedy action given by an = argminv∈A(sn) θ T nσsn,v is chosen and with probability ǫ, an action in A(sn) is randomly chosen."
    }, {
      "heading" : "4.3 Feature selection",
      "text" : "The idea behind the feature selection scheme is to select an energy-efficient sleep configuration, i.e., a configuration that keeps as many sensors in the wake state as possible to track the intruder while at the same time has minimal energy cost. This is done by first pruning the actions so as to select only those actions that ensure that the energy cost is ǫ-close to the tracking error and then, among the ǫ-optimal actions, selecting an action that minimizes the approximate Q-value.\nThe pruning of actions is performed as follows: Consider an action an(i) for the sensor i at time instant n. The sum of probabilities that the intruder will be at location i, over time instants 1, . . . , an(i) is a measure of the tracking error (denoted T in Fig. 2). On the other hand, the energy saved by having sensor i sleep for an(i) time units is proportional to c\nan(i)+1 (denoted E in Fig. 2). As illustrated with the two-dashed lines in Fig. 2, we now\nconsider all those actions an(i) such that the above two components are within ǫ distance of each other and set the feature value σsn,an to the above difference. On the other hand, for those actions that are outside the ǫ-boundary, we set σsn,an to a large constant. Formally, the choice of features is given by\nσsn,an = (σsn,an(1), ..., σsn,an(N)) T , (18)\nwhere σsn,an(i), i ≤ N is the feature value corresponding to sensor i. These values are defined as follows:\nδan(i)n = 1\n(an(i) + 1) −\n∑an(i) j=1 [pP j ]i ∑∞\nj=1[pP j]i\n, (19)\nσsn,an(i) =\n{\nδ an(i) n if 0 ≤ |δ an(i) n | ≤ ǫ, ⊤ otherwise. (20)\nIn the above, ⊤ is a fixed large constant used to prune out the actions that are not ǫ-close."
    }, {
      "heading" : "4.4 Two-timescale Q-learning based sleep–wake algorithm (TQSA-A)",
      "text" : "Although Q-learning with function approximation has been successful in many cases, it is theoretically difficult to prove that it converges to the optimal solution. In fact, there have been instances in which it has been shown to be unstable (cf. Prashanth and Bhatnagar [2011b]). A possible reason behind this problem is the off-policy characteristic of QSA-A accompanied by the resolution problem introduced by the feature based Q-learning with function approximation. The off-policy problem here arises because of the presence of the min operation in the Qlearning algorithm that introduces nonlinearity in the update rule. Note that if instead of the min operation, actions are selected according to a given policy, then the Q-learning update would resemble a temporal difference (TD) learning update for the joint (state-action) Markov chain. It has been shown in Tsitsiklis and Van Roy [1997] that\nTD with linear function approximation converges. The resolution problem arises due to the fact that the feature dimension is much less than the cardinality of the state-action space leading to inaccuracies in the estimates.\nTQSA-A performs a stochastic gradient descent w.r.t. the approximate Q-value function. A popular scheme for estimating the gradient of a function from simulation is SPSA and we employ a one-simulation SPSA scheme with deterministic perturbations for estimating ∇θQ(s, a).\nA standard assumption in policy gradient RL algorithms is that the policy π(s, a) is continuously differentiable in the parameter θ, for any state–action pair (s, a). A commonly used class of distributions that satisfy this assumption for the policy π is the parameterized Boltzmann family, where the distributions have the form\nπw(s, a) = ew\n⊤σs,a\n∑\na′∈A(s) e w⊤σs,a′\n, ∀s ∈ S , ∀a ∈ A. (21)\nIn the above, the parameter w = (w1, . . . , wN )T is assumed to take values in a compact and convex set C ⊂ RN . Further, the parameter θ = (θ1, . . . , θd)T is assumed to take values in a compact and convex set D ⊂ Rd. As illustrated in Fig. 3, the idea in the gradient estimate is to simulate the system with the perturbed policy parameter w+δ∆, where δ > 0 is a fixed small constant and∆ = (∆1, . . . ,∆N )T are perturbations constructed using certain Hadamard matrices (see Lemma 3.3 of Bhatnagar et al. [2003] for details of the construction). Given the output from the perturbed simulation, the gradient of the approximate Q-value function Q(s, a) ≈ θTσs,a is estimated as:\n∇wQ(s, a) ≈ θTσs,a\nδ ∆−1. (22)\nIt has been shown in Bhatnagar et al. [2003] that an incremental stochastic recursive algorithm that incorporates the RHS of (22) as its update direction essentially performs a search in the gradient direction when δ is small.\nThe overall update of the TQSA-A proceeds on two different timescales as follows: (i) On the faster timescale, the policy parameter is updated along a gradient descent direction using an SPSA estimate (22); (ii) On the slower timescale, the average cost (4) is estimated and the Q-value parameter is updated. The update rule for the TQSA-A algorithm is given as follows: ∀n ≥ 0,\nθn+1 = Γ1\n(\nθn + b(n)σsn,an(g(sn, an)− Ĵn+1 + θ T nσsn+1,an+1 − θ T nσsn,an)\n)\n, (23)\nĴn+1 =Ĵn + c(n) ( g(sn, an)− Ĵn ) , (24)\nwn+1 = Γ2\n(\nwn − a(n) θTnσsn,an\nδ ∆−1n\n)\n. (25)\nIn the above, (i) the choice of features σsn,an is the same as in the algorithm, QSA-A and is described in Section 4.3. (ii) the step-sizes a(n) and b(n) are chosen such that the policy parameter w is on the faster timescale and average cost Ĵ and Q-value parameter θ are on the slower timescale. To ensure this timescale separation, the step-sizes b(n), c(n), a(n) satisfy the following requirements:\n∑\nn\na(n) = ∑\nn\nb(n) = ∞, ∑\nn\n(a2(n) + b2(n)) < ∞ and lim n→∞\nb(n) a(n) = 0.\nFurther, c(n) = ka(n) for some k > 0. (iii) Γ1 : Rd → D, Γ2 : RN → C are certain projection operators that project the iterates θn and wn, n ≥ 1 to certain prescribed compact and convex sets D and C, respectively. The recursions (23) and (25) remain stable because of these projection operators, a crucial requirement for convergence of TQSA-A. It turns out that because of the timescale difference, the recursion (25) converges almost surely to a set w(θ) that is a function of parameter θ and is seen to be a compact subset of RN . Further, the slower recursion (23) can be seen to track a differential inclusion and converges almost surely to a closed connected internally chain transitive invariant set of this differential inclusion. This claim is made precise by the convergence result in Appendix A."
    }, {
      "heading" : "5 Algorithms for Discounted Cost Setting",
      "text" : "In this section, we present two algorithms for sleep-wake scheduling with the goal of minimizing a discounted cost objective described in Section 3.3. The overall structure of both the algorithms follow the scheme provided in Algorithm 1. However, in comparison to the average cost algorithms described earlier, the parameter θ is updated in a different fashion here to cater to the discounted cost objective."
    }, {
      "heading" : "5.1 Q-learning based Sleep–wake Scheduling Algorithm (QSA-D)",
      "text" : "As in the case of the average cost setting, the Q-learning algorithm cannot be used without employing function approximation because of the size of the state-action space. The function approximation variant of Q-learning in the discounted cost setting parameterizes the Q-values in a similar manner as the average cost setting, i.e., according to (15). The algorithm works with a single online simulation trajectory of states and actions, and updates θ according to\nθn+1 = θn + a(n)σsn,an\n(\ng(sn, an) + γ min b∈A(sn+1)\nθTnσsn+1,b − θ T nσsn,an\n)\n, (26)\nwhere θ0 is set arbitrarily. In the above, sn and sn+1 denote the state at instants n and n + 1, respectively, and θn denotes the nth update of the parameter. In (26), the action an is chosen in state sn according to an ǫ−greedy policy, as in the case of the QSA-A algorithm."
    }, {
      "heading" : "5.2 Two-timescale Q-learning based sleep–wake scheduling algorithm (TQSA-D)",
      "text" : "As with the average cost setting, the Q-learning algorithm with function approximation in the discounted setting is not guaranteed to converge because of the off-policy problem. A variant of Q-learning Bhatnagar and Lakshmanan [2012] has been recently proposed and has been shown to be convergent. This algorithm uses two-timescale simultaneous perturbation stochastic approximation (SPSA) with Hadamard matrix based deterministic perturbation sequences Bhatnagar et al. [2003].\nThe TQSA-D algorithm is a two timescale stochastic approximation algorithm that employs a linear approximation architecture and parameterizes the policy. As in the case of TQSA-A, we assume here that the policy π(s, a) is continuously differentiable in the parameter θ, for any state–action pair (s, a). The function approximation parameter θ is tuned on the slower timescale in a TD-like fashion, while the policy parameter w is tuned on the faster timescale in the negative gradient descent direction using SPSA. Let π′n △ = π(wn+δ∆n) = (π(wn+δ∆n)(i, a), i ∈ S, a ∈ A(i))T , where δ > 0 is a given small constant, be the randomized policy parameterized by (wn + δ∆n) during the nth instant. Here ∆n, n ≥ 0 are perturbations obtained from the Hadamard matrix based construction described before. The update rule of the TQSA-D algorithm is given as follows: ∀n ≥ 0,\nθn+1 = Γ1 ( θn + b(n)σsn,an ( r(sn, an) + γθ T nσsn+1,an+1 − θ T nσsn,an )) ,\nwn+1 = Γ2\n(\nwn − a(n) θTnσsn,an\nδ ∆−1n\n)\n. (27)\nThe projection operators Γ1,Γ2 and the step-sizes a(n), b(n) for all n ≥ 0 are the same as in TQSA-A and the features σsn,an are as in the previous algorithms."
    }, {
      "heading" : "6 Mobility Model Estimation",
      "text" : "The algorithms described in the previous sections assume knowledge of the transition dynamics (the matrix P ) of the Markov chain governing the intruder movement. However, in practice, this information is not available. In this section, we present a procedure to estimate P and combine the same with the sleep-wake scheduling algorithms described in the previous section. We assume that P is stationary, i.e., it does not change with time.\nThe estimation procedure for P is online and convergent. The combination with the sleep-wake scheduling algorithms happens via multi-timescale stochastic approximation. In essence, we run the estimation procedure for P on the faster timescale while the updates for the parameters of the sleep-wake scheduling algorithms are conducted on the slower timescale. Thus, the update recursions for the individual sleep-wake algorithms see the estimate for P as equilibrated, i.e., converged.\nLet P̂0 be the initial estimate of the transition probability matrix P . Then, the estimate P̂n at time instant n is tuned as follows:\nP̂n+1 = Π ( P̂n + d(n)p̂np̂ T n+1 ) . (28)\nIn the above, p̂n = [pn(i) : i = 1, 2, . . . , N + 1] T is a column vector signifying current location of the intruder. Further,Π(·) is a projection operator that ensures that the iterates P̂n satisfy the properties of a transition probability matrix. Also,{d(n)} is a step-size sequence chosen such that it is on the faster timescale, while the θ-recursion of the algorithm described earlier is on the slower timescale.\nThe idea behind the above update rule can be explained as follows: Suppose the locations of the intruder at instants n and n + 1 are known. Then, p̂n and p̂n+1 would be vectors with the value 1 in lkth position and 0 elsewhere. The quantity p̂np̂Tn+1 would thus result in a matrix with 1 at row index lk and column index lk+1 and 0 elsewhere. The recursion (28) then results in a sample averaging behavior (due to stochastic approximation) for estimating the transition dynamics P . The same logic can be extended to the remaining cases, for instance, known lk and unknown lk+1 and so on.\nEmpirically we observe that the update (28) converges to the true transition probability matrix P for each of the proposed algorithms, in all the network settings considered."
    }, {
      "heading" : "7 Simulation Experiments",
      "text" : ""
    }, {
      "heading" : "7.1 Implementation",
      "text" : "We perform our experiments on a 2-D network setting (see Fig. 1) of 121 sensors, i.e., a 11 × 11 grid. The sensor regions overlap here, with each sensor’s sensing region overlapping with that of its neighboring nodes. In particular, the sensing regions of sensors in the interior of the grid overlap with eight neighboring nodes. We implemented our sleep–wake scheduling algorithms - QSA-A and TQSA-A for the average cost setting and QSAD and TQSA-D for the discounted cost setting, respectively. For the sake of comparison, we also implemented the FCR and QMDP algorithms proposed in Fuemmeler and Veeravalli [2008]. Note that for each of these algorithms, the knowledge of the mobility model of the intruder is assumed. We briefly recall these algorithms below: FCR: This algorithm approximates the state evolution (3) by pt+1 = ptP , and then attempts to find the sleep time for each sensor by solving the following balance equation:\nV (l)(p) = min\nu\n(\nu ∑\nj=1\n[pP j ]l + N ∑\ni=1\nc[pPu+1]i + V (l)(pPu+1)\n)\n.\nThus, the sleeping policy here is obtained locally for each sensor by solving the above Bellman equation for each sensor, with a strong approximation on the state evolution. Note that our algorithms make no such assumptions and attempt to find the optimal sleeping policy in the global sense (i.e., considering all the sensors) and not in the local sense (i.e., treating the sensors individually).\nQMDP : In this approach, the decomposition into the per sensor problem is the same as in FCR. However here, the underlying assumption is that the location of the object will always be known in the future. Thus, instead of (3), the state evolves here according to pk+1 = elk+1P . The objective function for a sensor l, given the state component p, is given by\nV (l)(p) =min\nu\n(\nu ∑\nj=1\n[pP j ]l +\nN ∑\ni=1\nc[pPu+1]i +\nN ∑\ni=1\n[pPu+1]iV (l)(ei)\n)\n.\nThe difference between the above and the corresponding equation for FCR is in the third term on the right hand side representing the future cost. In the case of QMDP , the future cost is the conditional expectation of the cost incurred from the object location after u time units given the current distribution as its location. Thus, one can solve V (l)(p) for any p once V (l)(ei), 1 ≤ i ≤ N are known. The QMDP algorithm then attempts to find a solution using the well-known dynamic programming procedure - policy iteration for MDPs. However, an important drawback with the dynamic programming approaches is the curse of dimensionality (i.e., the computational complexity with solving the associated Markov decision process increases exponentially with the dimension and cardinality of the state and action spaces). RL algorithms that incorporate function approximation techniques alleviate this problem and make the computational complexity manageable, while still ensuring that these algorithms converge to a ‘good enough’ policy.\nThe simulations were conducted for 6000 cycles for all algorithms. We set the single-stage cost component\nc to 0.1. The step-sizes are chosen as follows: For QSA-A, we set a(n) = 1\nn , n ≥ 1 and for TQSA-A, we set\nb(n) = 1 n , a(n) = 1 n0.55 , n ≥ 1, respectively. Further, for TQSA-A/TQSA-D, we set δ = 0.001. For QSA-A, we have taken the fixed state s as 〈p0, r〉 where p0 is the initial distribution of pk and r is a random sleep time vector. For this choice of the special state s, it can be seen that there is a positive probability of the underlying MDP visiting s. This is because the intruder stays in the starting location for at least one time step and the exploration of actions initially results in a positive probability of a random action being chosen. The fact that a state satisfying such a criterion can be used as the fixed state has been established in Bertsekas [2007]."
    }, {
      "heading" : "7.2 Results",
      "text" : "We use the number of sensors awake and the number of detects per time step as the performance metrics for comparing the various sleep/wake algorithms. While the former metric is the ratio of the total number of sensors in the wake state to the number of time-steps, the latter is the ratio of the number of successful detects of the intruder to the number of time-steps. Fig. 4 presents the number of sensors awake and the number of detects per time step, for each of the algorithms studied in the average cost setting, while Fig. 7 presents similar results for the algorithms in the discounted cost setting.\nWe observe that in comparison to the QMDP algorithm, our algorithms attain a slightly higher tracking accuracy at the cost of a few additional sensors in the wake state. On the other hand, our algorithms exhibit better tradeoff between energy cost and tracking accuracy in comparison to the FCR algorithm. Amongst our algorithms, we observe that the two timescale variant TQSA-A performs better that the Q-learning based QSA-A, since TQSA-A results in a tracking accuracy similar to QSA-A with lesser number of sensors awake. A similar observation holds in the discounted cost setting as well.\nFurther, as evident in the tradeoff plot in Fig. 4, the QMDP algorithm exhibits fluctuating behaviour with a significant number of outliers that show poor tradeoffs. This, we suspect, is due to the underlying requirement of complete future observations in QMDP . Further, QMDP (and even FCR) is not a learning algorithm that stabilizes the number of sensors awake and the tracking errors in the long-term. This is because, at each instant, QMDP attempts to solve the Bellman equation in an approximate fashion and no information about the solution thus obtained is carried forward to the future instants.\nOn the contrary, our algorithms learn a good enough sleep/wake scheduling policy for the individual sensors with contextual information being carried forward from one time step to the next. This results in a stable regime for the number of sensors awake and the tracking accuracy, unlike QMDP . While the number of sensors awake for the FCR algorithm is less than that for our algorithms, the tracking accuracy is significantly lower in comparison. For critical tracking systems, where failing to track has higher penalty, our proposed algorithms (esp. TQSA-A)\nwill be able to achieve greater performance (tracking accuracy) at the cost of only a few additional sensors in the wake state.\nFrom Fig. 6a, we observe that the policy parameter θ converges. This is a significant feature of the TQSA-A algorithm as it possesses theoretical convergence guarantees, unlike QSA-A, which may not converge in some settings. Further, it can also be seen that the transient period when the policy parameter θ has not converged is short for each of our algorithms.\nFig. 5 presents the results obtained from the experiments with TQSA-A combined with the mobility model estimation procedure (28). We observe that even for the case when the intruder’s mobility model is not known, our algorithm TQSA-A shows performance on par with the original TQSA-A, i.e., the one with known P . In contrast, the QMDP algorithm requires full knowledge of the distribution of the intruder movement and hence, cannot be applied in the setting of unknown P . We also observe that in the TQSA-A algorithm, the estimate Pk of the transition probability matrix P converges to the true P and this is illustrated by the convergence plots in Fig. 6b. In particular, Fig. 6b shows that the value of Pk(i, j), where i corresponds to the (6, 6)th cell and j corresponds to (6, 5)th cell converges to the actual P (i, j) value of 0.11."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We studied the problem of optimizing sleep times in a sensor network for intrusion detection. Following a POMDP formulation similar to the one in Fuemmeler et al. [2011], our aim in this paper was to minimize certain long-run average and discounted cost objectives. This in turn allowed us to study both transient as well as steady state system\nbehavior. We proposed two novel reinforcement learning based algorithms for both the settings considered. All our algorithms are model-free, online and easy to implement. The first algorithm in either setting is based on the well-known Q-learning algorithm with function approximation. On the other hand, the second algorithm in either setting is a two-timescale Q-learning algorithm which possesses theoretical convergence guarantees, unlike the first algorithm. Next, we extended these algorithms to a setting where the intruder’s mobility model is not known. Empirically, we demonstrated the usefulness of our algorithms on a simple two-dimensional network setting.\nAs future work, one could extend these algorithms to settings where multiple intruders have to be detected. This would involve the conflicting objectives of keeping less number of sensors awake and at the same time, detecting as many intruders as possible. Another interesting direction of future research is to develop intruder detection algorithms in a decentralized setting, i.e., a setting where the individual sensors collaborate in the absence of a central controller. Decentralized variants of our proposed algorithms can be developed in the following manner: Each sensor runs an RL algorithm to decide on the sleep times in a manner similar to the algorithms we propose. However, this would require the knowledge of pk (distribution of the intruder’s location) at each sensor and this can be obtained by means of a message passing scheme between the individual sensors. However, this approach of exchanging message between every pair of sensors may increase the load on the network. A practical alternative is to form (possibly dynamic) groups of sensors, within which the message regarding the intruder’s location (or pk) is exchanged and the individual sensors decide on the sleep times using this local information and an update rule similar to the algorithms we propose (for instance, according to (23))."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Convergence analysis for TQSA-A\nThe ODE approach is adopted for analyzing the convergence of θ and w recursions (23) in the main paper. In essence, the two-timescale stochastic approximation architecture employed in the TQSA-A algorithm allows (i) the faster timescale analysis of the w-recursion in (23) in the main paper assuming that the slower θ-recursion is constant (quasi-static), and (ii) the slower timescale analysis of the θ-recursion in (23) in the main paper assuming that the faster w-recursion has converged, for any given θ.\nThe convergence analysis comprises of the following important steps: (i) Theorem 2, in effect, states that the w-recursion performs a gradient descent using one-simulation SPSA and converges to a set of points in the neighborhood of the local minimum of the approximate Q-value functionR(θ, w) (defined below). Note that this analysis is for the w-recursion on the faster timescale, assuming the Q-value function parameter θ to be a constant. (ii) Analyzing θ-recursion on the slower timescale, Theorem 4 claims that the iterate θ asymptotically converges to a closed connected internally chain transitive invariant set associated with a corresponding differential inclusion (DI).\nA.1.1 Analysis of the w-recursion\nWe present below the precise statements of these results. Let C(D) denote the space of all continuous functions from D to Rd. We define the operator Γ̂2 : C(C) → C(RN ) as follows:\nΓ̂2(v(w)) = lim α↓0\n(\nΓ2(w + αv(w)) − w\nα\n)\n.\nConsider the ODE associated with the w-recursion on the faster timescale, assuming θ(t) ≡ θ (a constant independent of t):\nẇ(t) = Γ̂2 (−∇wR(θ, w(t))) , (29)\nwith R(θ, w) △ =\n∑\ni∈S,a∈A(i)\nfw(i, a)θ Tσi,a,\nwhere fw(i, a) are the stationary probabilities fw(i, a) = dπw(i)πw(i, a), i ∈ S, a ∈ A(i) for the joint process {(Xn, Zn)}, obtained from the state-action tuples at each instant. Here dπw (i) is the stationary probability distribution for the Markov chain {Xn} under policy πw being in state i ∈ S. The ergodicity of the joint process {(Xn, Zn)} and the existence of stationary distribution fw(i, a) follows from the proposition below:\nProposition 1. Under (A1) and (A2), the process (Xn, Zn), n ≥ 0 with Zn, n ≥ 0 obtained from the SRP πw, for any given w ∈ C, is an ergodic Markov process.\nProof. See [Bhatnagar and Lakshmanan, 2012, Proposition 1, Section 3].\nLemma 1. Under (A1) and (A2), the stationary probabilities fw(i, a), i ∈ S, a ∈ A(i) are continuously differentiable in the parameter w ∈ C.\nProof. See [Bhatnagar and Lakshmanan, 2012, Lemma 1, Section 3].\nLet Kθ denote the set of asymptotically stable equilibria of (29), i.e., the local minima of the function R(θ, ·) within the constraint set C. Given ǫ > 0, let Kǫθ denote the closed ǫ-neighborhood of Kθ, i.e.,\nKǫθ = {w ∈ C |‖ w − w0 ‖≤ ǫ, w0 ∈ Kθ}.\nThe following result establishes that the w-recursion tracks the ODE (29).\nTheorem 2. Let θn ≡ θ, ∀n, for some θ ∈ D ⊂ Rd. Then, given ǫ > 0, there exists δ0 > 0 such that for all δ ∈ (0, δ0], {wn} governed by (23) in the main paper converges to a point w∗ ∈ Kǫθ almost surely.\nProof. See [Bhatnagar and Lakshmanan, 2012, Theorem 2, Section 3].\nProposition 2. The set Kǫθ is a compact subset of R N for any θ and ǫ > 0.\nProof. Follows in a similar manner as [Bhatnagar and Lakshmanan, 2012, Corollary 2].\nA.1.2 Analysis of the θ-recursion\nWe now analyze the θ-recursion, which is the slower recursion in (23) in the main paper. We first show that the estimate Ĵn tracks the average cost J(πwn) corresponding to the policy parameter wn ≡ w(θn).\nLemma 3. With probability one, |Ĵn−J(πwn)| → 0 as n → ∞, where J(πwn) is the average reward under πwn .\nProof. The Ĵ update can be re-written as\nĴn+1 = Ĵn + b(n) ( J(πwn) + ξn − Ĵn +Mn+1 ) . (30)\nIn the above,\n• Fn = σ(wm, θn, ξm,Mm;m ≤ n), n ≥ 0 is a set of σ-fields.\n• ξn = (E[g(sn, an)|Fn−1]− J(πwn)), n ≥ 0 and\n• Mn+1 = g(sn, an)− E[g(sn, an)|Fn−1], n ≥ 0 is a martingale difference sequence.\nLet Nm = ∑m\nn=0 c(n)Mn+1. Clearly, (Nm,Fm),m ≥ 0 is a square-integrable and almost surely convergent martingale. Further, from Proposition 1, |ξn| → 0 almost surely on the ‘natural timescale’, as n → ∞. The ‘natural timescale’ is faster than the algorithm’s timescale and hence ξn vanishes asymptotically, almost surely, see [Borkar, 2008, Chapter 6.2] for detailed treatment of natural timescale algorithms.\nThe ODE associated with (30) is\n˙̂ J(t) = J(πw(t))− Ĵ(t) △ = H(Ĵ(t)). (31)\nLet H∞(Ĵ(t)) = limc→∞ H(cĴ(t))\nc = −Ĵ(t). Note that the ODE\n˙̂ J(t) = −Ĵ(t)\nhas the origin as its unique globally asymptotically stable equilibrium. Further, the ODE (31) has Ĵ∗ = J(πwn) as its unique asymptotically stable equilibrium. The claim follows from Lemma 7 - Corollary 8 on pp. 74 and Theorem 9 on pp. 75 of Borkar [2008].\nLet Tw : R|S×A(S)| → R|S×A(S)| be the operator given by\nTw(J)(i, a) = g(i, a)− J(πw)e+ ∑\nj∈S,b∈A(j)\npw(i, a; j, b)J(j, b), (32)\nor in more compact notation Tw(J) = G− J(πw)e+ PwJ,\nwhere G is the column vector with components g(i, a), i ∈ S, a ∈ A(i) and Pw is the transition probability matrix with components pw(i, a; j, b) that denote the transition probabilities of the joint process {(Xn, Zn)}.\nThe differential inclusion associated with the θ-recursion of (23) in the main paper corresponds to\nθ̇(t) ∈ Γ̂θ (h(θ)) , (33)\nwhere h(θ) is the set-valued map, defined in compact notation as follows:\nh(θ) △ = {ΦTFw(θ(t))(Tw(θ(t))(Φθ(t)) − Φθ(t) | w ∈ K ǫ θ}.\nIn the above, Fw denotes the diagonal matrix with elements along the diagonal being fw(i, a), i ∈ S, a ∈ A(i). Also, Φ denotes the matrix with rows σTs,a, s ∈ S, a ∈ A(s). The number of rows of this matrix is thus |S×A(S)|, while the number of columns is N . Thus, Φ = (Φ(i), i = 1, . . . , N) where Φ(i) is the column vector\nΦ(i) = (σs,a(i), s ∈ S, a ∈ A(s)) T , i = 1, . . . , N.\nFurther, the projection operator Γ̂θ is defined as\nΓ̂θ △ = ∩ǫ>0ch ( ∪{‖β−θ‖<ǫ}{γ1(β; y + Y ) | y ∈ h(β), Y ∈ A(β)} ) , where\n• ch(S) denotes the closed convex hull of the set S;\n• yn is defined as follows:\nyn △ =\n∑\n(i,a)\nfw(θn)(i, a) ( g(i, a)− J(πw) + θn T ∑\n(j,b)\npw(θn)(i, a; j, b)σj,b − θn Tσi,a\n)\n,\nwith wn ∈ Kǫθ;\n• Y (n+ 1) is defined as follows:\nY (n+ 1) △ = (\ng(Xn, Zn)− J(πwn) + θ T nσXn+1,Zn+1 − θ T nσXn,Zn\n)\nσXn,Zn\n−E [( g(Xn, Zn)− J(πwn) + θ T nσXn+1,Zn+1 − θ T nσXn,Zn ) σXn,Zn | G(n) ] ,\nwhere G(n) = σ(θr, Xr, Zr, r ≤ n), n ≥ 0 is a sequence of associated sigma fields.\nThe main result is then given as follows:\nTheorem 4. The iterate θn, n ≥ 0 governed by (23) in the main paper, converges a.s to a closed connected internally chain transitive invariant set of (33).\nProof. Let N(n) = ∑n−1\nm=0 b(m)Y (m + 1). It is easy to see that N(n), n ≥ 0 is a martingale sequence. Further, (N(n),G(n)), n ≥ 0 is a square-integrable and almost surely convergent martingale, owing to the following facts: (i) sup\n(i,a)∈S×A(S) ‖ σi,a ‖< ∞ and sup (i,a)∈S×A(S) |g(i, a)| < ∞ since S ×A(S) is a finite set.\n(ii) Since we project the iterate θ using Γ1 onto a compact and convex set C, we have sup n ‖ θn ‖< ∞. (iii) By assumption, the step-size sequence b(n), n ≥ 0 satisfies ∑\nn\nb(n)2 < ∞.\nThus, the θ-recursion (23) in the main paper can be re-written as follows:\nθn+1 =Γ1\n(\nθn + b(n)yn + b(n)Y (n+ 1)\n)\n, (34)\nFollowing the technique in [Borkar, 2008, Chapter 5.4], one can rewrite the above as follows:\nθn+1 =θn + b(n)\n(\nΓ1(θn + b(n)(yn + Y (n+ 1)))− θn b(n)\n)\n,\n=θn + b(n) (γ1(θn; yn + Y (n+ 1)) + o(b(n))) , (35)\nwhere γ1(θ; y) = limη↓0\n(\nΓ1(θn + ηy)− θ\nη\n)\n.\nLet zn △ = E[γ1(θn; yn + Y (n+ 1)) | G(n)] and Y̌ (n+ 1) △ = γ1(θn; yn + Y (n+ 1))− zn. Then, it is easy to see that (35) is equivalent to\nθn+1 =θn + b(n) ( zn + Y̌ (n+ 1) + o(b(n)) ) . (36)\nUsing similar arguments as in [Bhatnagar and Lakshmanan, 2012, Proposition 3], it can be seen that Γ̂(h(θ)) satisfies the conditions stipulated in [Borkar, 2008, Section 5.1]. These conditions ensure that Γ̂(h(θ)) is compact, convex valued and upper-semicontinuous with bounded range on compacts.\nSince we use Γ1 operator to ensure θ is bounded, the general result of [Borkar, 2008, Corollary 4, Chapter 5] can be applied to see that θn converges a.s. to a closed connected internally chain transitive invariant set of (33). The claim follows."
    } ],
    "references" : [ {
      "title" : "Learning algorithms for Markov decision processes with average cost",
      "author" : [ "J. Abounadi", "D. Bertsekas", "V.S. Borkar" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Abounadi et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Abounadi et al\\.",
      "year" : 2002
    }, {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L. Baird" ],
      "venue" : "In ICML, pages 30–37,",
      "citeRegEx" : "Baird.,? \\Q1995\\E",
      "shortCiteRegEx" : "Baird.",
      "year" : 1995
    }, {
      "title" : "Multiple abstraction levels in performance analysis of wsn monitoring systems",
      "author" : [ "M. Beccuti", "D. Codetta-Raiteri", "G. Franceschinis" ],
      "venue" : "In International ICST Conference on Performance Evaluation Methodologies and Tools,",
      "citeRegEx" : "Beccuti et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beccuti et al\\.",
      "year" : 2009
    }, {
      "title" : "Dynamic Programming and Optimal Control, vol. II, 3rd edition",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2007
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "Dimitri P. Bertsekas", "John N. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 1996
    }, {
      "title" : "Two-timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences",
      "author" : [ "S. Bhatnagar", "M.C. Fu", "S.I. Marcus", "I. Wang" ],
      "venue" : "ACM Transactions on Modeling and Computer Simulation (TOMACS),",
      "citeRegEx" : "Bhatnagar et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bhatnagar et al\\.",
      "year" : 2003
    }, {
      "title" : "Stochastic Approximation: A Dynamical Systems Viewpoint",
      "author" : [ "V.S. Borkar" ],
      "venue" : null,
      "citeRegEx" : "Borkar.,? \\Q2008\\E",
      "shortCiteRegEx" : "Borkar.",
      "year" : 2008
    }, {
      "title" : "Smart sleeping policies for energy efficient tracking in sensor networks",
      "author" : [ "J.A. Fuemmeler", "V.V. Veeravalli" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Fuemmeler and Veeravalli.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fuemmeler and Veeravalli.",
      "year" : 2008
    }, {
      "title" : "Sleep control for tracking in sensor networks",
      "author" : [ "J.A. Fuemmeler", "G.K. Atia", "V.V. Veeravalli" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Fuemmeler et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Fuemmeler et al\\.",
      "year" : 2011
    }, {
      "title" : "Power conservation and quality of surveillance in target tracking sensor networks",
      "author" : [ "C. Gui", "P. Mohapatra" ],
      "venue" : "In Proceedings of the international conference on mobile computing and networking,",
      "citeRegEx" : "Gui and Mohapatra.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gui and Mohapatra.",
      "year" : 2004
    }, {
      "title" : "Energy efficient sleep scheduling based on moving directions in target tracking sensor network",
      "author" : [ "B. Jiang", "K. Han", "B. Ravindran", "H. Cho" ],
      "venue" : "In IEEE International Symposium on Parallel and Distributed Processing,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2008
    }, {
      "title" : "Dynamic clustering for object tracking in wireless sensor networks",
      "author" : [ "G. Jin", "X. Lu", "M.S. Park" ],
      "venue" : "Ubiquitous Computing Systems,",
      "citeRegEx" : "Jin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2006
    }, {
      "title" : "Reinforcement learning with function approximation for traffic signal control",
      "author" : [ "L.A. Prashanth", "S. Bhatnagar" ],
      "venue" : "IEEE Transactions on Intelligent Transportation Systems,",
      "citeRegEx" : "Prashanth and Bhatnagar.,? \\Q2011\\E",
      "shortCiteRegEx" : "Prashanth and Bhatnagar.",
      "year" : 2011
    }, {
      "title" : "Reinforcement learning with average cost for adaptive control of traffic lights at intersections",
      "author" : [ "L.A. Prashanth", "S. Bhatnagar" ],
      "venue" : "In 14th International IEEE Conference on Intelligent Transportation Systems (ITSC),",
      "citeRegEx" : "Prashanth and Bhatnagar.,? \\Q2011\\E",
      "shortCiteRegEx" : "Prashanth and Bhatnagar.",
      "year" : 2011
    }, {
      "title" : "Adaptive sleep-wake control using reinforcement learning in sensor networks",
      "author" : [ "L.A. Prashanth", "Abhranil Chatterjee", "Shalabh Bhatnagar" ],
      "venue" : "In Sixth International Conference on Communication Systems and Networks (COMSNETS). IEEE,",
      "citeRegEx" : "Prashanth et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Prashanth et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal sleep–wake scheduling for quickest intrusion detection using sensor networks",
      "author" : [ "K. Premkumar", "A. Kumar" ],
      "venue" : "IEEE INFOCOM, Arizona,",
      "citeRegEx" : "Premkumar and Kumar.,? \\Q2008\\E",
      "shortCiteRegEx" : "Premkumar and Kumar.",
      "year" : 2008
    }, {
      "title" : "Markov decision processes: Discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 1994
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "An Analysis of Temporal Difference Learning with Function Approximation",
      "author" : [ "John N Tsitsiklis", "Benjamin Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Tsitsiklis and Roy.,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "We formulate this problem as a partially-observable Markov decision process (POMDP) in a manner similar to the one considered in Fuemmeler and Veeravalli [2008]. However, unlike their total cost objective, in this paper we consider infinite horizon average as well as discounted cost objectives.",
      "startOffset" : 129,
      "endOffset" : 161
    }, {
      "referenceID" : 6,
      "context" : "The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material. Next, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation.",
      "startOffset" : 137,
      "endOffset" : 742
    }, {
      "referenceID" : 6,
      "context" : "The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material. Next, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation. Finally, we also consider a setting where the mobility model of the intruder is not available and develop an online scheme that estimates the same. We combine this estimation procedure with the sleep-wake scheduling algorithms mentioned above using a multi-timescale scheme. We study our algorithms on a simple two-dimensional network setting (see Fig. 1) and compare their performance with the QMDP and FCR algorithms from Fuemmeler and Veeravalli [2008]. Our algorithms are seen to be easily implementable, converge rapidly with a short transient period and provide more consistent results than the QMDP and FCR algorithms.",
      "startOffset" : 137,
      "endOffset" : 1228
    }, {
      "referenceID" : 6,
      "context" : "The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material. Next, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation. Finally, we also consider a setting where the mobility model of the intruder is not available and develop an online scheme that estimates the same. We combine this estimation procedure with the sleep-wake scheduling algorithms mentioned above using a multi-timescale scheme. We study our algorithms on a simple two-dimensional network setting (see Fig. 1) and compare their performance with the QMDP and FCR algorithms from Fuemmeler and Veeravalli [2008]. Our algorithms are seen to be easily implementable, converge rapidly with a short transient period and provide more consistent results than the QMDP and FCR algorithms. Further, we observe that the procedure for estimating the mobility model of the intruder converges empirically to the true model. A short version of this paper with only the average cost criterion and without the convergence proofs is available in Prashanth et al. [2014].",
      "startOffset" : 137,
      "endOffset" : 1670
    }, {
      "referenceID" : 6,
      "context" : "2 Literature Review Sleep-wake scheduling: A Markov decision process (MDP) model for intrusion detection has also been formulated in Premkumar and Kumar [2008], where the authors present three sleep/wake scheduling algorithms to control the number of sensors in the wake state.",
      "startOffset" : 133,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking. However, unlike us, their algorithms are under the waking channel assumption, i.e., a setting where the central controller can communicate with a sensor that is in the sleep state. In Jiang et al. [2008], a sleep/wake scheduling algorithm based on the target’s moving direction has been proposed.",
      "startOffset" : 3,
      "endOffset" : 318
    }, {
      "referenceID" : 3,
      "context" : "In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking. However, unlike us, their algorithms are under the waking channel assumption, i.e., a setting where the central controller can communicate with a sensor that is in the sleep state. In Jiang et al. [2008], a sleep/wake scheduling algorithm based on the target’s moving direction has been proposed. In Jin et al. [2006], the authors present a heuristic algorithm that uses dynamic clustering of sensors to balance energy cost and tracking error.",
      "startOffset" : 3,
      "endOffset" : 432
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al.",
      "startOffset" : 3,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution.",
      "startOffset" : 3,
      "endOffset" : 233
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants.",
      "startOffset" : 3,
      "endOffset" : 700
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a ‘good enough’ policy that minimizes the long-run average sum of this cost. The term ‘good enough’ here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL.",
      "startOffset" : 3,
      "endOffset" : 1849
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a ‘good enough’ policy that minimizes the long-run average sum of this cost. The term ‘good enough’ here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL.",
      "startOffset" : 3,
      "endOffset" : 1874
    }, {
      "referenceID" : 1,
      "context" : "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a ‘good enough’ policy that minimizes the long-run average sum of this cost. The term ‘good enough’ here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL. Q-learning Watkins and Dayan [1992] is a well-known RL algorithm for finding the optimal policy.",
      "startOffset" : 3,
      "endOffset" : 1962
    }, {
      "referenceID" : 1,
      "context" : "However, for problems involving high-dimensional state spaces, the Q-learning algorithm with function approximation may diverge or may show large oscillations, Baird [1995]. This is primarily due to the inherent nonlinearity in the Q-learning update rule resulting from the explicit maximization/minimization in the update procedure.",
      "startOffset" : 160,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "However, for problems involving high-dimensional state spaces, the Q-learning algorithm with function approximation may diverge or may show large oscillations, Baird [1995]. This is primarily due to the inherent nonlinearity in the Q-learning update rule resulting from the explicit maximization/minimization in the update procedure. A two-timescale variant of the Q-learning algorithm, proposed in Bhatnagar and Lakshmanan [2012], avoids this problem by using two timescales, where on the faster timescale the policy parameter is tuned in the negative gradient direction using SPSA estimates of the gradient and on the slower timescale, a TD-like update for the parameters is performed.",
      "startOffset" : 160,
      "endOffset" : 431
    }, {
      "referenceID" : 7,
      "context" : "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1.",
      "startOffset" : 37,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value ǫ otherwise.",
      "startOffset" : 37,
      "endOffset" : 661
    }, {
      "referenceID" : 7,
      "context" : "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value ǫ otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak−1), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is ŝk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above.",
      "startOffset" : 37,
      "endOffset" : 1164
    }, {
      "referenceID" : 7,
      "context" : "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value ǫ otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak−1), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is ŝk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above. Note that pk = (pk(1), ..., pk(N)) is the distribution at time step k of the object being in one of the locations 1, 2, ..., N and evolves according to pk+1 = elk+1I{rk+1(lk+1)=0} + pkPI{rk+1(lk+1)>0}, (3) where ei denotes an N -dimensional unit vector with 1 in the ith position and 0 elsewhere. The idea behind the evolution of pk is as follows: (i) the first term refers to the case when the location of the intruder is known, i.e., the sensor at lk+1 is in the wake state; (ii) the second term refers to the case when intruder’s location is not known and hence, the intruder transitions to the next distribution pk+1 from the current pk via the transition probability matrix P . Note that the evolution of pk in our setting differs from Fuemmeler and Veeravalli [2008], as we do not have the termination state.",
      "startOffset" : 37,
      "endOffset" : 2082
    }, {
      "referenceID" : 7,
      "context" : "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value ǫ otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak−1), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is ŝk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above. Note that pk = (pk(1), ..., pk(N)) is the distribution at time step k of the object being in one of the locations 1, 2, ..., N and evolves according to pk+1 = elk+1I{rk+1(lk+1)=0} + pkPI{rk+1(lk+1)>0}, (3) where ei denotes an N -dimensional unit vector with 1 in the ith position and 0 elsewhere. The idea behind the evolution of pk is as follows: (i) the first term refers to the case when the location of the intruder is known, i.e., the sensor at lk+1 is in the wake state; (ii) the second term refers to the case when intruder’s location is not known and hence, the intruder transitions to the next distribution pk+1 from the current pk via the transition probability matrix P . Note that the evolution of pk in our setting differs from Fuemmeler and Veeravalli [2008], as we do not have the termination state. With an abuse of terminology, henceforth we shall refer to the sufficient statistic ŝk as the state vector in the algorithms we propose next. Further, we would like to emphasize here that our algorithms do not require full observation of the state vector. Instead, by an intelligent choice of features that rely only on pk, the algorithms obtain a sleeping policy that works well. 1Since we study long-run average sum of (2) (see (4) below), we can consider the problem of tracking an intruder in an infinite horizon, whereas a termination state in Fuemmeler and Veeravalli [2008] was made necessary as they considered a total cost objective.",
      "startOffset" : 37,
      "endOffset" : 2705
    }, {
      "referenceID" : 3,
      "context" : "The following Bellman equation is satisfied (Chapter 4 of Bertsekas [2007]): J + h(i) = min a (g(i, a) + ∑",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "It is well known, see Puterman [1994], that the optimal value function V (·) satisfies the following Bellman equation of optimality in the discounted cost case: V (i) = min a∈A(i) ",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : ", those with i ∈ S and a ∈ A(i) using the stochastic approximation version of (13) (see Abounadi et al. [2002]).",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "we develop a feature based Q-learning algorithm as in Prashanth and Bhatnagar [2011a]. While the full state Qlearning algorithm in (14) cannot be used on even moderately sized sensing regions, its function approximation based variant can be used over larger network settings.",
      "startOffset" : 54,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "We first present a sleep–wake scheduling algorithm which is the function approximation analogue of the Qlearning with average cost algorithm proposed in Abounadi et al. [2002]. While this algorithm that we refer to as QSA-A is shown to work well in the numerical experiments, it does not possess theoretical convergence guarantees.",
      "startOffset" : 153,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : "Prashanth and Bhatnagar [2011b]).",
      "startOffset" : 0,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "Prashanth and Bhatnagar [2011b]). A possible reason behind this problem is the off-policy characteristic of QSA-A accompanied by the resolution problem introduced by the feature based Q-learning with function approximation. The off-policy problem here arises because of the presence of the min operation in the Qlearning algorithm that introduces nonlinearity in the update rule. Note that if instead of the min operation, actions are selected according to a given policy, then the Q-learning update would resemble a temporal difference (TD) learning update for the joint (state-action) Markov chain. It has been shown in Tsitsiklis and Van Roy [1997] that",
      "startOffset" : 0,
      "endOffset" : 652
    }, {
      "referenceID" : 5,
      "context" : "3 of Bhatnagar et al. [2003] for details of the construction).",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "3 of Bhatnagar et al. [2003] for details of the construction). Given the output from the perturbed simulation, the gradient of the approximate Q-value function Q(s, a) ≈ θσs,a is estimated as: ∇wQ(s, a) ≈ θσs,a δ ∆. (22) It has been shown in Bhatnagar et al. [2003] that an incremental stochastic recursive algorithm that incorporates the RHS of (22) as its update direction essentially performs a search in the gradient direction when δ is small.",
      "startOffset" : 5,
      "endOffset" : 266
    }, {
      "referenceID" : 5,
      "context" : "This algorithm uses two-timescale simultaneous perturbation stochastic approximation (SPSA) with Hadamard matrix based deterministic perturbation sequences Bhatnagar et al. [2003]. The TQSA-D algorithm is a two timescale stochastic approximation algorithm that employs a linear approximation architecture and parameterizes the policy.",
      "startOffset" : 156,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "For the sake of comparison, we also implemented the FCR and QMDP algorithms proposed in Fuemmeler and Veeravalli [2008]. Note that for each of these algorithms, the knowledge of the mobility model of the intruder is assumed.",
      "startOffset" : 88,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "The fact that a state satisfying such a criterion can be used as the fixed state has been established in Bertsekas [2007].",
      "startOffset" : 105,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "Following a POMDP formulation similar to the one in Fuemmeler et al. [2011], our aim in this paper was to minimize certain long-run average and discounted cost objectives.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "75 of Borkar [2008]. Let Tw : R → R be the operator given by Tw(J)(i, a) = g(i, a)− J(πw)e+ ∑",
      "startOffset" : 6,
      "endOffset" : 20
    } ],
    "year" : 2013,
    "abstractText" : "The aim in this paper is to allocate the ‘sleep time’ of the individual sensors in an intrusion detection application so that the energy consumption from the sensors is reduced, while keeping the tracking error to a minimum. We propose two novel reinforcement learning (RL) based algorithms with both infinite horizon discounted and long-run average cost objectives for solving this problem. All our algorithms incorporate feature-based representations to handle the curse of dimensionality associated with the underlying partially-observable Markov decision process (POMDP). Further, the feature selection scheme used in our algorithms intelligently manages the energy cost and tracking cost factors, which in turn assists the search for the optimal sleeping policy. The first algorithm in either (discounted or average) setting is based on Q-learning, while the second algorithm is a novel two-timescale algorithm that performs on-policy Q-learning. The latter possesses theoretical convergence guarantees, unlike the former Q-learning based algorithm. We also extend these algorithms to a setting where the intruder’s mobility model is not known by incorporating a stochastic iterative scheme for estimating the mobility model. The simulation results on a synthetic 2-d network setting suggest that our proposed algorithms result in better tracking accuracy at the cost of a few additional sensors, in comparison to a recent prior work. We also observe empirically that the proposed model estimation scheme converges to the true model.",
    "creator" : "gnuplot 4.2 patchlevel 6 "
  }
}