{
  "name" : "1602.02442.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Simple Practical Accelerated Method for Finite Sums",
    "authors" : [ "Aaron Defazio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied.\nIntroduction\nA large body of recent developments in optimization have focused on minimization of convex finite sums of the form:\nf(x) = 1\nn n∑ i=1 fi(x),\na very general class of problems including the empirical risk minimization (ERM) framework as a special case. Any function h can be written in this form by setting f1(x) = h(x) and fi = 0 for i 6= 1, however when each fi is sufficiently regular in a way that can be made precise, it is possible to optimize such sums more efficiently than by treating them as black box functions.\nIn most cases recently developed methods such as SAG [Schmidt et al., 2013] can find an -minimum faster than either stochastic gradient descent or accelerated black-box approaches, both in theory and in practice. We call this class of methods fast incremental gradient methods (FIG).\nFIG methods are randomized methods similar to SGD, however unlike SGD they are able to achieve linear convergence rates under Lipschitz-smooth and strong convexity conditions [Mairal, 2014, Defazio et al., 2014b, Johnson and Zhang, 2013, Konečný and Richtárik, 2013]. The linear rate in the first wave of FIG methods directly depended on the condition number (L/µ) of the problem, whereas recently several methods have been developed that depend on the square-root of the condition number [Lan and Zhou, 2015, Lin et al., 2015, Shalev-Shwartz and Zhang, 2013c, Nitanda, 2014]. Analogous to the black-box case, these methods are known as accelerated methods.\nIn this work we develop another accelerated method, which is conceptually simpler and requires less tuning than existing accelerated methods. The method we give is a primal approach, however it makes use of a proximal operator oracle for each fi instead of a gradient oracle, unlike other primal approaches. The proximal operator is also used by dual methods such as some variants of SDCA [Shalev-Shwartz and Zhang, 2013a].\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 2.\n02 44\n2v 2\n[ st\nat .M\nL ]\n2 7\nO ct\n2 01\nAlgorithm 1 Pick some starting point x0 and step size γ. Initialize each g0i = f ′ i(x 0), where f ′i(x 0) is any gradient/subgradient at x0. Then at step k + 1:\n1. Pick index j from 1 to n uniformly at random. 2. Update x:\nzkj = x k + γ [ gkj − 1\nn n∑ i=1 gki\n] ,\nxk+1 = proxγj ( zkj ) .\n3. Update the gradient table: Set gk+1j = 1 γ ( zkj − xk+1 ) , and leave the rest of the entries\nunchanged (gk+1i = g k i for i 6= j)."
    }, {
      "heading" : "1 Algorithm",
      "text" : "Our algorithm’s main step makes use of the proximal operator for a randomly chosen fi. For convenience, we use the following compact notation:\nproxγi (x) = argminy { γfi(y) + 1\n2 ‖x− y‖2\n} .\nThis proximal operator can be computed efficiently or in closed form in many cases, see Section 4 for details. Like SAGA, we also maintain a table of gradients gi, one for each function fi. We denote the state of gi at the end of step k by gki . The iterate (our guess at the solution) at the end of step k is denoted xk. The starting iterate x0 may be chosen arbitrarily.\nThe full algorithm is given as Algorithm 1. The sum of gradients 1n ∑n i=1 g k i can be cached and updated efficiently at each step, and in most cases instead of storing a full vector for each gi, only a single real value needs to be stored. This is the case for linear regression or binary classification with logistic loss or hinge loss, in precisely the same way as for standard SAGA. A discussion of further implementation details is given in Section 4.\nWith step size\nγ =\n√ (n− 1)2 + 4nLµ\n2Ln − 1− 1n 2L ,\nthe expected convergence rate in terms of squared distance to the solution is given by: E ∥∥xk − x∗∥∥2 ≤ (1− µγ\n1 + µγ\n)k µ+ L\nµ ∥∥x0 − x∗∥∥2 , when each fi : Rd → R is L-smooth and µ-strongly convex. See Nesterov [1998] for definitions of these conditions. Using big-O notation, the number of steps required to reduce the distance to the solution by a factor is:\nk = O\n((√ nL\nµ + n\n) log ( 1 )) ,\nas → 0. This rate matches the lower bound known for this problem [Lan and Zhou, 2015] under the gradient oracle. We conjecture that this rate is optimal under the proximal operator oracle as well. Unlike other accelerated approaches though, we have only a single tunable parameter (the step size γ), and the algorithm doesn’t need knowledge of L or µ except for their appearance in the step size.\nCompared to the O ((L/µ+ n) log (1/ )) rate for SAGA and other non-accelerated FIG methods, accelerated FIG methods are significantly faster when n is small compared to L/µ, however for n ≥ L/µ the performance is essentially the same. All known FIG methods hit a kind of wall at n ≈ L/µ, where they decrease the error at each step by no more than 1− 1n . Indeed, when n ≥ L/µ the problem is so well conditioned so as to be easy for any FIG method to solve it efficiently. This is sometimes called the big data setting [Defazio et al., 2014b].\nOur convergence rate can also be compared to that of optimal first-order black box methods, which have rates of the form k = O ((√ L/µ ) log (1/ ) ) per epoch equivalent. We are able to achieve a √ n speedup on a per-epoch basis, for n not too large. Of course, all of the mentioned rates are significantly better than the O ((L/µ) log (1/ )) rate of gradient descent.\nFor non-smooth but strongly convex problems, we prove a 1/ -type rate under a standard iterate averaging scheme. This rate does not require the use of decreasing step sizes, so our algorithm requires less tuning than other primal approaches on non-smooth problems."
    }, {
      "heading" : "2 Relation to other approaches",
      "text" : "Our method is most closely related to the SAGA method. To make the relation clear, we may write our method’s main step as:\nxk+1 = xk − γ [ f ′j(x k+1)− gkj + 1\nn n∑ i=1 gki\n] ,\nwhereas SAGA has a step of the form:\nxk+1 = xk − γ [ f ′j(x k)− gkj + 1\nn n∑ i=1 gki\n] .\nThe difference is the point at which the gradient of fj is evaluated at. The proximal operator has the effect of evaluating the gradient at xk+1 instead of xk. While a small difference on the surface, this change has profound effects. It allows the method to be applied directly to non-smooth problems using fixed step sizes, a property not shared by SAGA or other primal FIG methods. Additionally, it allows for much larger step sizes to be used, which is why the method is able to achieve an accelerated rate.\nIt is also illustrative to look at how the methods behave at n = 1. SAGA degenerates into regular gradient descent, whereas our method becomes the proximal-point method [Rockafellar, 1976]:\nxk+1 = proxγf (x k).\nThe proximal point method has quite remarkable properties. For strongly convex problems, it converges for any γ > 0 at a linear rate. The downside being the inherent difficulty of evaluating the proximal operator. For the n = 2 case, if each term is an indicator function for a convex set, our algorithm matches Dykstra’s projection algorithm if we take γ = 2 and use cyclic instead of random steps.\nAccelerated incremental gradient methods\nSeveral acceleration schemes have been recently developed as extensions of non-accelerated FIG methods. The earliest approach developed was the ASDCA algorithm [Shalev-Shwartz and Zhang, 2013b,c]. The general approach of applying the proximal-point method as the outer-loop of a doubleloop scheme has been dubbed the Catalyst algorithm Lin et al. [2015]. It can be applied to accelerate any FIG method. Recently a very interesting primal-dual approach has been proposed by Lan and Zhou [2015]. All of the prior accelerated methods are significantly more complex than the approach we propose, and have more complex proofs."
    }, {
      "heading" : "3 Theory",
      "text" : ""
    }, {
      "heading" : "3.1 Proximal operator bounds",
      "text" : "In this section we rehash some simple bounds from proximal operator theory that we will use in this work. Define the short-hand pγf (x) = proxγf (x), and let gγf (x) = 1 γ (x− pγf (x)), so that pγf (x) = x− γgγf (x). Note that gγf (x) is a subgradient of f at the point pγf (x). This relation is known as the optimality condition of the proximal operator. Note that proofs for the following two propositions are in the supplementary material.\nProposition 1. (Strengthening firm non-expansiveness under strong convexity) For any x, y ∈ Rd, and any convex function f : Rd → R with strong convexity constant µ ≥ 0,\n〈x− y, pγf (x)− pγf (y)〉 ≥ (1 + µγ) ‖pγf (x)− pγf (y)‖2 ."
    }, {
      "heading" : "In operator theory this property is known as (1 + µγ)-cocoerciveness of pγf .",
      "text" : "Proposition 2. (Moreau decomposition) For any x ∈ Rd, and any convex function f : Rd → R with Fenchel conjugate f∗ :\npγf (x) = x− γp 1 γ f ∗(x/γ). (1)\nRecall our definition of gγf (x) = 1γ (x− pγf (x)) also. After combining, the following relation thus holds between the proximal operator of the conjugate f∗ and gγf :\np 1 γ f\n∗(x/γ) = 1\nγ (x− pγf (x)) = gγf (x). (2)\nTheorem 3. For any x, y ∈ Rd, and any convex L-smooth function f : Rd → R:\n〈gγf (x)− gγf (y), x− y〉 ≥ γ ( 1 + 1\nLγ\n) ‖gγf (x)− gγf (y)‖2 ,\nProof. We will apply cocoerciveness of the proximal operator of f∗ as it appears in the decomposition. Note that L-smoothness of f implies 1/L-strong convexity of f∗. In particular we apply it to the points 1γx and\n1 γ y:〈\np 1 γ f\n∗( 1\nγ x)− p 1 γ f ∗(\n1 γ y), 1 γ x− 1 γ y\n〉 ≥ ( 1 + 1\nLγ )∥∥∥∥p 1γ f∗( 1γ x)− p 1γ f∗( 1γ y) ∥∥∥∥2 .\nPulling 1γ from the right side of the inner product out, and plugging in Equation 9, gives the result."
    }, {
      "heading" : "3.2 Notation",
      "text" : "Let x∗ be the unique minimizer (due to strong convexity) of f . In addition to the notation used in the description of the algorithm, we also fix a set of subgradients g∗j , one for each of fj at x\n∗, chosen such that ∑n j=1 g ∗ j = 0. We also define vj = x ∗ + γg∗j . Note that at the solution x ∗, we want to apply a proximal step for component j of the form:\nx∗ = proxγj ( x∗ + γg∗j ) = proxγj (vj) .\nLemma 4. (Technical lemma needed by main proof) Under Algorithm 1, taking the expectation over the random choice of j, conditioning on xk and each gki , allows us to bound the following inner product at step k:\nE 〈 γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j , ( xk − x∗ ) + γ [ gkj − 1 n n∑ i=1 gki ] − γg∗j 〉\n≤ γ2 1 n n∑ i=1 ∥∥gki − g∗i ∥∥2 . The proof is in the supplementary material."
    }, {
      "heading" : "3.3 Main result",
      "text" : "Theorem 5. (single step Lyapunov descent) We define the Lyapunov function T k of our algorithm (Point-SAGA) at step k as:\nT k = c\nn n∑ i=1 ∥∥gki − g∗i ∥∥2 + ∥∥xk − x∗∥∥2 , for c = 1/µL. Then using step size γ = √ (n−1)2+4nLµ\n2Ln − 1− 1n 2L , the expectation of T\nk+1, over the random choice of j, conditioning on xk and each gki , is:\nE [ T k+1 ] ≤ (1− κ)T k for κ = µγ\n1 + µγ ,\nwhen each fi : Rd → R is L-smooth and µ-strongly convex and 0 < µ < L. This is the same Lyapunov function as used by Hofmann et al. [2015].\nProof. Term 1 of T k+1 is straight-forward to simplify:\nc n E n∑ i=1 ∥∥gk+1i − g∗i ∥∥2 = (1− 1n ) c n n∑ i=1 ∥∥gki − g∗i ∥∥2 + cnE ∥∥gk+1j − g∗j∥∥2 . For term 2 of T k+1 we start by applying cocoerciveness (Theorem 11):\n(1 + µγ)E ∥∥xk+1 − x∗∥∥2\n= (1 + µγ)E ∥∥proxγj (zkj )− proxγj (vj)∥∥2\n≤ E 〈 proxγj (z k j )− prox γ j (vj), z k j − vj 〉 = E 〈 xk+1 − x∗ , zkj − vj 〉 .\nNow we add and subtract xk : = E 〈 xk+1 − xk + xk − x∗ , zkj − vj 〉 = E 〈 xk − x∗ , zkj − vj 〉 + E 〈 xk+1 − xk , zkj − vj\n〉 =\n∥∥xk − x∗∥∥2 + E 〈xk+1 − xk , zkj − vj〉 , where we have pulled out the quadratic term by using E[zkj − vj ] = xk − x∗ (we can take the expectation since the left hand side of the inner product doesn’t depend on j). We now expand E 〈 xk+1 − xk , zkj − vj 〉 further:\nE 〈 xk+1 − xk , zkj − vj 〉 = E 〈 xk+1 − γg∗j + γg∗j − xk , zkj − vj\n〉 = E 〈 xk − γgk+1j + γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j + γg∗j − xk,\n( xk − x∗ ) + γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j 〉 . (3)\nWe further split the left side of the inner product to give two separate inner products:\n= E 〈 γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j , ( xk − x∗ ) + γ [ gkj − 1 n n∑ i=1 gki ] − γg∗j 〉\n+ E 〈 γg∗j − γgk+1j , ( xk − x∗ ) + γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j 〉 . (4)\nThe first inner product in Equation 4 is the quantity we bounded in Lemma 8 by γ2 1n ∑n i=1 ∥∥gki − g∗i ∥∥2. The second inner product in Equation 4, can be simplified using Theorem 3 (note the right side of the inner product is equal to zkj − vj):\n−γE 〈 gk+1j − g ∗ j , z k j − vj 〉 ≤ −γ2 ( 1 + 1\nLγ\n) E ∥∥gk+1j − g∗j∥∥2 .\nCombing these gives the following bound on (1 + µγ)E ∥∥xk+1 − x∗∥∥2: (1+µγ)E ∥∥xk+1 − x∗∥∥2 ≤ ∥∥xk − x∗∥∥2+γ2 1\nn n∑ i=1 ∥∥gki − g∗i ∥∥2−γ2(1 + 1Lγ ) E ∥∥gk+1j − g∗j∥∥2 .\nDefine α = 11+µγ = 1− κ, where κ = µγ 1+µγ . Now we multiply the above inequality through by α and combine with the rest of the Lyapunov function, giving:\nE [ T k+1 ] ≤ T k + ( αγ2 − c\nn ) 1 n n∑ i ∥∥gki − g∗i ∥∥2 + ( c n − αγ2 − αγ L ) E ∥∥gk+1j − g∗j∥∥2 − κE ∥∥xk − x∗∥∥2 .\nWe want an α convergence rate, so we pull out the required terms:\nE [ T k+1 ] ≤ αT k + ( αγ2 + κc− c\nn ) 1 n n∑ i ∥∥gki − g∗i ∥∥2 + ( c n − αγ2 − αγ L ) E ∥∥gk+1j − g∗j∥∥2 .\nNow to complete the proof we note that c = 1/µL and γ =\n√ (n−1)2+4nLµ\n2Ln − 1− 1n 2L ensure that both\nterms inside the round brackets are non-positive, giving ET k+1 ≤ αT k. These constants were found by equating the equations in the brackets to zero, and solving with respect to the two unknowns, γ and c. It is easy to verify that γ is always positive, as a consequence of the condition number L/µ always being at least 1.\nCorollary 6. (Smooth case) Chaining Theorem 5 gives a convergence rate for Point-SAGA at step k under the constants given in Theorem 5 of:\nE ∥∥xk − x∗∥∥2 ≤ (1− κ)k µ+ L\nµ ∥∥x0 − x∗∥∥2 , if each fi : Rd → R is L-smooth and µ-strongly convex. Theorem 7. (Non-smooth case) Suppose each fi : Rd → R is µ-strongly convex,\n∥∥g0i − g∗i ∥∥ ≤ B and\n∥∥x0 − x∗∥∥ ≤ R. Then after k iterations of Point-SAGA with step size γ = R/B√n: E ∥∥x̄k − x∗∥∥2 ≤ 2√n (1 + µ (R/B√n))\nµk RB,\nwhere x̄k = 1kE ∑k t=1 x t. The proof of this theorem is included in the supplementary material."
    }, {
      "heading" : "4 Implementation",
      "text" : "Care must be taken for efficient implementation, particularly in the sparse gradient case. We discuss the key points below. A fast Cython implementation is available on the author’s website incorporating these techniques. Proximal operators For the most common binary classification and regression methods, imple-\nmenting the proximal operator is straight-forward. We include details of the computation of the proximal operators for the hinge, square and logistic losses in the supplementary material. The logistic loss does not have a closed form proximal operator, however it may be computed very efficiently in practice using Newton’s method on a 1D subproblem. For problems of a non-trivial dimensionality the cost of the dot products in the main step is much greater than the cost of the proximal operator evaluation. We also detail how to handle a quadratic regularizer within each term’s prox operator, which has a closed form in terms of the unregularized prox operator.\nInitialization Instead of setting g0i = f ′i(x0) before commencing the algorithm, we recommend using g0i = 0 instead. This avoids the cost of a initial pass over the data. In practical effect this is similar to the SDCA initialization of each dual variable to 0."
    }, {
      "heading" : "5 Experiments",
      "text" : "We tested our algorithm which we call Point-SAGA against SAGA [Defazio et al., 2014a], SDCA [Shalev-Shwartz and Zhang, 2013a], Pegasos/SGD [Shalev-Shwartz et al., 2011] and the catalyst acceleration scheme [Lin et al., 2015]. SDCA was chosen as the inner algorithm for the catalyst scheme as it doesn’t require a step-size, making it the most practical of the variants. Catalyst applied to SDCA is essentially the same algorithm as proposed in Shalev-Shwartz and Zhang [2013c]. A single inner epoch was used for each SDCA invocation. Accelerated MISO as well as the primal-dual FIG method [Lan and Zhou, 2015] were excluded as we wanted to test on sparse problems and they are not designed to take advantage of sparsity. The step-size parameter for each method (κ for catalyst-SDCA) was chosen using a grid search of powers of 2. The step size that gives the lowest error at the final epoch is used for each method.\nWe selected a set of commonly used datasets from the LIBSVM repository [Chang and Lin, 2011]. The pre-scaled versions were used when available. Logistic regression with L2 regularization was applied to each problem. The L2 regularization constant for each problem was set by hand to ensure f was not in the big data regime n ≥ L/µ; as noted above, all the methods perform essentially the same when n ≥ L/µ. The constant used is noted beneath each plot. Open source code to exactly replicate the experimental results is available at https://github.com/adefazio/point-saga.\nAlgorithm scaling with respect to n The key property that distinguishes accelerated FIG methods from their non-accelerated counterparts is their performance scaling with respect to the dataset size. For large datasets on well-conditioned problems we expect from the theory to see little difference between the methods. To this end, we ran experiments including versions of the datasets subsampled randomly without replacement in 10% and 5% increments, in order to show the scaling with n empirically. The same amount of regularization was used for each subset.\nFigure 1 shows the function value sub-optimality for each dataset-subset combination. We see that in general accelerated methods dominate the performance of their non-accelerated counter-parts. Both SDCA and SAGA are much slower on some datasets comparatively than others. For example, SDCA is very slow on the 5 and 10% COVTYPE datasets, whereas both SAGA and SDCA are much slower than the accelerated methods on the AUSTRALIAN dataset. These differences reflect known properties of the two methods. SAGA is able to adapt to inherent strong convexity while SDCA can be faster on very well-conditioned problems.\nThere is no clear winner between the two accelerated methods, each gives excellent results on each problem. The Pegasos (stochastic gradient descent) algorithm with its slower than linear rate is a clear loser on each problem, almost appearing as an almost horizontal line on the log scale of these plots.\nNon-smooth problems We also tested the RCV1 dataset on the hinge loss. In general we did not expect an accelerated rate for this problem, and indeed we observe that Point-SAGA is roughly as fast as SDCA across the different dataset sizes."
    }, {
      "heading" : "A Proximal operators",
      "text" : "For the most common binary classification and regression methods, implementing the proximal operator is straight-forward. In this section let yj be the label or target for regression, and Xj the data instance vector. We assume for binary classification that yj ∈ {−1, 1}.\nHinge loss: fj(z) = l(z; yj , Xj) = max {0, 1− yj 〈z,Xj〉} .\nThe proximal operator has a closed form expression:\nproxγfj (z) = z − γyjνXj ,\nwhere:\ns = 1− yj 〈z,Xj〉 γ ‖Xj‖2 .\nν =  −1 s ≥ 1 0 s ≤ 0 −s otherwise .\nLogistic loss: fj(z) = l(z; yj , Xj) = log ( 1 + exp ( −yjXTj z )) .\nThere is no closed form expression, however it can be computed very efficiently using Newton iteration, since it can be reduced to a 1D minimization problem. In particular, let c0 = 0, γ′ = γ ‖Xj‖2, and a = 〈z,Xj〉. Then iterate until convergence:\nsk = −yj\n1 + exp (yjck) ,\nck+1 = ck − γ ′sk + ck − a\n1− y′sk − γ′sksk .\nThe prox operator is then proxγfj (z) = z − ( a− ck ) Xj/ ‖Xj‖2. Three iterations are generally enough, but ill-conditioned problems or large step sizes may require up to 12. Correct initialization is important, as it will diverge when initialized with a point on the opposite side of 0 from the solution.\nSquared loss:\nfj(z) = l(z; yj , Xj) = 1\n2\n( XTj z − yj )2 .\nLet γ′ = γ ‖Xj‖2 and a = 〈z,Xj〉. Define:\nc = a+ γ′y\n1 + γ′ .\nThen proxγfj (z) = z − (a− c)Xj/ ‖Xj‖ 2 ."
    }, {
      "heading" : "L2 regularization",
      "text" : "Including a regularizer within each fi, i.e. Fi(x) = fi(x) + µ2 ‖x‖ 2 , can be done using the proximal operator of fi. Define the scaling factor:\nρ = 1− µγ 1 + µγ .\nThen proxγFi(z) = proxργfi(ρz)."
    }, {
      "heading" : "B Proofs",
      "text" : "Lemma 8. Under Algorithm 1, taking the expectation over the random choice of j, conditioning on xk and each gki , allows us to bound the following inner product at step k:\nE 〈 γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j , ( xk − x∗ ) + γ [ gkj − 1 n n∑ i=1 gki ] − γg∗j 〉\n≤ γ2 1 n n∑ i=1 ∥∥gki − g∗i ∥∥2 . Proof. We start by splitting on the right hand side of the inner product:\n= E 〈 γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j , xk − x∗ 〉\n+E 〈 γ [ gkj − 1\nn n∑ i=1 gki ] − γg∗j , γ [ gkj − 1 n n∑ i=1 gki ] − γg∗j 〉 (5)\nThe first inner product has expectation 0 on the left hand side (Recall that E[g∗j ] = 0), so it’s simply 0 in expectation (we may take expectation on the left since the right doesn’t depend on j). The second inner product is the same on both sides, so we may convert it to a norm-squared term. So we have:\n= γ2E ∥∥∥∥∥gkj − 1n n∑ i=1 gki − g∗j ∥∥∥∥∥ 2\n≤ γ2E ∥∥gkj − g∗j∥∥2 = γ2 1n n∑ i=1 ∥∥gki − g∗i ∥∥2 . The inequality used is just an application of the variance formula E[(X − E[X])2] = E[X2] − E[X]2 ≤ E[X2].\nCorollary 9. Chaining the main theorem gives a convergence rate for point-saga at step k under the constants given in of:\nE ∥∥xk − x∗∥∥2 ≤ (1− κ)k µ+ L\nµ ∥∥x0 − x∗∥∥2 , if each fi : Rd → R is L-smooth and µ-strongly convex.\nProof. First we simplify T 0 using c = 1/µL and use Lipschitz smoothness:\nT 0 = 1 µL · 1 n ∑ i ∥∥g0i − g∗i ∥∥2 + ∥∥x0 − x∗∥∥2 ≤ L\nµ · ∥∥x0 − x∗∥∥2 + ∥∥x0 − x∗∥∥2\n= µ+ L\nµ ∥∥x0 − x∗∥∥2 . Now recall that the main theorem gives a bound E [ T k+1 ] ≤ (1− κ)T k where the expectation is\nconditional on xk and each gki from step k, taking expectation over the randomness in the choice of j. We can further take expectation with respect to xk and each gki , giving the unconditional bound:\nE [ T k+1 ] ≤ (1− κ)E [ T k ] .\nChaining over k gives the result.\nTheorem 10. Suppose each fi : Rd → R is µ-strongly convex, ∥∥g0i − g∗i ∥∥ ≤ B and ∥∥x0 − x∗∥∥ ≤ R.\nThen after k iterations of Point-SAGA with step size γ = R/B √ n:\nE ∥∥x̄k − x∗∥∥2 ≤ 2√n (1 + µ (R/B√n))\nµk RB,\nwhere x̄k = 1kE ∑k t=1 x t.\nProof. Recall the bound on the Lyapunov function established in the main theorem:\nE [ T k+1 ] ≤ T k + ( αγ2 − c\nn ) 1 n n∑ i ∥∥gki − g∗i ∥∥2 + ( c n − αγ2 − αγ L ) E ∥∥gk+1j − g∗j∥∥2\n− κE ∥∥xk − x∗∥∥2 .\nIn the non-smooth case this holds with L =∞. In particular, if we take c = αγ2n, then:\n−κE ∥∥xk+1 − x∗∥∥2 ≥ E [T k+1]− T k.\nRecall that this expectation is (implicitly) conditional on xk and each gki from step k, Taking expectation over the randomness in the choice of j. We can further take expectation with respect to xk and each gki , and negate the inequality, giving the unconditional bound:\nκE ∥∥xk+1 − x∗∥∥2 ≤ E [T k]− E [T k+1] .\nWe now sum this over t = 0 . . . k:\nκE k∑ t=1 ∥∥xt − x∗∥∥2 ≤ T 0 − E [T k] . We can drop the −E [ T k ] since it is always negative. Dividing through by k:\n1 k E k∑ t=1 ∥∥xt − x∗∥∥2 ≤ 1 κk T 0.\nNow using Jensen’s inequality on the left gives:\nE ∥∥x̄k − x∗∥∥2 ≤ 1\nκk T 0,\nwhere x̄k = 1kE ∑k t=1 x t. Now we plug in T 0 = cn ∑ i ∥∥g0i − g∗i ∥∥2+∥∥x0 − x∗∥∥2 with c = αγ2n ≤ γ2n:\nE ∥∥x̄k − x∗∥∥2 ≤ γ2n\nκk\n1\nn ∑ i ∥∥g0i − g∗i ∥∥2 + 1κk ∥∥x0 − x∗∥∥2 . Now we plug in the bounds in terms of B and R:\nE ∥∥x̄k − x∗∥∥2 ≤ γ2n\nκk B2 +\n1\nκk R2.\nIn order to balance the terms on the right, we need:\nγ2n\nκk B2 =\n1\nκk R2,\n∴ γ2nB2 = R2,\n∴ γ2 = R2\nnB2 .\nSo we can take γ = R/B √ n, giving a rate of:\nE ∥∥x̄k − x∗∥∥2 ≤ 2\nκk R2\n= 2 1 + µγ\nµγk R2\n= 2\n√ n (1 + µ (R/B √ n))\nµk RB."
    }, {
      "heading" : "C Proximal operator bounds with proofs",
      "text" : "In this section we prove some simple bounds from proximal operator theory that we will use in this work. Define the short-hand pγf (x) = proxγf (x), and let gγf (x) = 1 γ (x− pγf (x)), so that pγf (x) = x− γgγf (x). Note that gγf (x) is a subgradient of f at the point pγf (x). This relation is known as the optimality condition of the proximal operator.\nWe will also use a few standard convexity bounds without proof. Let f : Rd → R be a convex function with strong convexity constant µ ≥ 0 and Lipschitz smoothness constant L. Let x∗ be the minimizer of f , then for any x, y ∈ Rd:\n〈f ′(x)− f ′(y), x− y〉 ≥ µ ‖x− y‖2 , (6)\n‖f ′(x)− f ′(y)‖2 ≤ L2 ‖x− y‖2 . (7) Proposition 11. (Firm non-expansiveness) For any x, y ∈ Rd, and any convex function f : Rd → R with strong convexity constant µ ≥ 0,\n〈x− y, pγf (x)− pγf (y)〉 ≥ (1 + µγ) ‖pγf (x)− pγf (y)‖2 .\nProof. Using strong convexity of f, we apply Equation 6 at the (sub-)gradients gγf (x) and gγf (y), and their corresponding points pγf (x) and pγf (y):\n〈gγf (x)− gγf (y), pγf (x)− pγf (y)〉 ≥ µ ‖pγf (x)− pγf (y)‖2 .\nWe now multiply both sides by γ, then add ‖pγf (x)− pγf (y)‖2 to both sides:\n〈pγf (x) + γgγf (x)− pγf (y)− γgγf (y), pγf (x)− pγf (y)〉 ≥ (1 + µγ) ∥∥pγf (x)− pγf (y)∥∥2 ,\nleading to the bound by using the optimality condition: pγf (x) + γgγf (x) = x.\nProposition 12. (Moreau decomposition) For any x ∈ Rd, and any convex function f : Rd → R with Fenchel conjugate f∗ :\npγf (x) = x− γp 1 γ f ∗(x/γ). (8)\nRecall our definition of gγf (x) = 1γ (x− pγf (x)) also. After combining, the following relation thus holds between the proximal operator of the conjugate f∗ and gγf :\np 1 γ f\n∗(x/γ) = 1\nγ (x− pγf (x)) = gγf (x). (9)\nProof. Let u = pγf (x), and v = 1γ (x− u). Then v ∈ ∂f(u) by the optimality condition of the proximal operator of f (namely if u = pγf (x) then u = x − γv ⇔ v ∈ ∂f(u)). It follows by conjugacy of f that u ∈ ∂f∗(v). Thus we may interpret v = 1γ (x− u) as the optimality condition of a proximal operator of f∗ :\nv = p 1 γ f\n∗( 1\nγ x).\nPlugging in the definition of v then gives:\n1 γ (x− u) = p 1 γ f ∗( 1 γ x).\nFurther plugging in u = pγf (x) and rearranging gives the result."
    } ],
    "references" : [ {
      "title" : "Libsvm : a library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Finito: A faster, permutable incremental gradient method for big data problems",
      "author" : [ "Aaron Defazio", "Tiberio Caetano", "Justin Domke" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Variance reduced stochastic gradient descent with neighbors",
      "author" : [ "Thomas Hofmann", "Aurelien Lucchi", "Simon Lacoste-Julien", "Brian McWilliams" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hofmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : null,
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "Semi-Stochastic Gradient Descent Methods",
      "author" : [ "Jakub Konečný", "Peter Richtárik" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Konečný and Richtárik.,? \\Q2013\\E",
      "shortCiteRegEx" : "Konečný and Richtárik.",
      "year" : 2013
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "G. Lan", "Y. Zhou" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Lan and Zhou.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lan and Zhou.",
      "year" : 2015
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Incremental majorization-minimization optimization with application to large-scale machine learning",
      "author" : [ "Julien Mairal" ],
      "venue" : "Technical report, INRIA Grenoble Rhône-Alpes / LJK Laboratoire Jean Kuntzmann,",
      "citeRegEx" : "Mairal.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mairal.",
      "year" : 2014
    }, {
      "title" : "Stochastic proximal gradient descent with acceleration techniques",
      "author" : [ "Atsushi Nitanda" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Nitanda.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nitanda.",
      "year" : 2014
    }, {
      "title" : "Monotone operators and the proximal point algorithm",
      "author" : [ "R Tyrrell Rockafellar" ],
      "venue" : "SIAM journal on control and optimization,",
      "citeRegEx" : "Rockafellar.,? \\Q1976\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1976
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2013
    }, {
      "title" : "Accelerated mini-batch stochastic dual coordinate ascent",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2013
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "In most cases recently developed methods such as SAG [Schmidt et al., 2013] can find an -minimum faster than either stochastic gradient descent or accelerated black-box approaches, both in theory and in practice.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "This rate matches the lower bound known for this problem [Lan and Zhou, 2015] under the gradient oracle.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "SAGA degenerates into regular gradient descent, whereas our method becomes the proximal-point method [Rockafellar, 1976]: x = proxγf (x ).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "The general approach of applying the proximal-point method as the outer-loop of a doubleloop scheme has been dubbed the Catalyst algorithm Lin et al. [2015]. It can be applied to accelerate any FIG method.",
      "startOffset" : 139,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Recently a very interesting primal-dual approach has been proposed by Lan and Zhou [2015]. All of the prior accelerated methods are significantly more complex than the approach we propose, and have more complex proofs.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "This is the same Lyapunov function as used by Hofmann et al. [2015]. Proof.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : ", 2014a], SDCA [Shalev-Shwartz and Zhang, 2013a], Pegasos/SGD [Shalev-Shwartz et al., 2011] and the catalyst acceleration scheme [Lin et al.",
      "startOffset" : 62,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : ", 2011] and the catalyst acceleration scheme [Lin et al., 2015].",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Accelerated MISO as well as the primal-dual FIG method [Lan and Zhou, 2015] were excluded as we wanted to test on sparse problems and they are not designed to take advantage of sparsity.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "We selected a set of commonly used datasets from the LIBSVM repository [Chang and Lin, 2011].",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "5 Experiments We tested our algorithm which we call Point-SAGA against SAGA [Defazio et al., 2014a], SDCA [Shalev-Shwartz and Zhang, 2013a], Pegasos/SGD [Shalev-Shwartz et al., 2011] and the catalyst acceleration scheme [Lin et al., 2015]. SDCA was chosen as the inner algorithm for the catalyst scheme as it doesn’t require a step-size, making it the most practical of the variants. Catalyst applied to SDCA is essentially the same algorithm as proposed in Shalev-Shwartz and Zhang [2013c]. A single inner epoch was used for each SDCA invocation.",
      "startOffset" : 77,
      "endOffset" : 491
    } ],
    "year" : 2016,
    "abstractText" : "We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied.",
    "creator" : "LaTeX with hyperref package"
  }
}