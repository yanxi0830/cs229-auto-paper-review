{
  "name" : "1705.09518.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Sampling Theory Perspective of Graph-based Semi-supervised Learning",
    "authors" : [ "Aamir Anis", "Aly El Gamal" ],
    "emails" : [ "aanis@usc.edu,", "avestimehr@ee.usc.edu,", "ortega@sipi.usc.edu).", "elgamala@purdue.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 A Sampling Theory Perspective of Graph-based Semi-supervised Learning\nAamir Anis, Student Member, IEEE, Aly El Gamal, Member, IEEE, Salman Avestimehr, Senior Member, IEEE, and Antonio Ortega, Fellow, IEEE\nAbstract—Graph-based methods have been quite successful in solving unsupervised and semi-supervised learning problems, as they provide a means to capture the underlying geometry of the dataset. It is often desirable for the constructed graph to satisfy two properties: first, data points that are similar in the feature space should be strongly connected on the graph, and second, the class label information should vary smoothly with respect to the graph, where smoothness is measured using the spectral properties of the graph Laplacian matrix. Recent works have justified some of these smoothness conditions by showing that they are strongly linked to the semi-supervised smoothness assumption and its variants. In this work, we reinforce this connection by viewing the problem from a graph sampling theoretic perspective, where class indicator functions are treated as bandlimited graph signals (in the eigenvector basis of the graph Laplacian) and label prediction as a bandlimited reconstruction problem. Our approach involves analyzing the bandwidth of class indicator signals generated from statistical data models with separable and nonseparable classes. These models are quite general and mimic the nature of most real-world datasets. Our results show that in the asymptotic limit, the bandwidth of any class indicator is also closely related to the geometry of the dataset. This allows one to theoretically justify the assumption of bandlimitedness of class indicator signals, thereby providing a sampling theoretic interpretation of graph-based semi-supervised classification.\nI. INTRODUCTION The abundance of unlabeled data in various machine learning applications, along with the prohibitive cost of labeling, has led to growing interest in semi-supervised learning. This paradigm deals with the task of classifying data points in the presence of very little labeling information by relying on the geometry of the dataset. Assuming that the features are well-chosen, a natural assumption in this setting is to consider the marginal density p(x) of the feature vectors to be informative about the labeling function f(x) defined on the points. This assumption is fundamental to the semi-supervised learning problem both in the classification and the regression settings, and is also known as the semi-supervised smoothness\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nThis work is supported in part by NSF under grants CCF-1410009, CCF1527874, CCF-1408639, NETS-1419632 and by AFRL and DARPA under grant 108818.\nA. Anis, S. Avestimehr and A. Ortega are with the Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, 90089 USA (e-mail: aanis@usc.edu, avestimehr@ee.usc.edu, ortega@sipi.usc.edu).\nA. El Gamal is with the Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, 479007 USA (e-mail: elgamala@purdue.edu).\nassumption [1], which states that the label function is smoother in regions of high data density. There also exist other similar variants of this assumption specialized for the classification setting, namely, the cluster assumption [2] (points in a cluster are likely to have the same class label) or the low density separation assumption [3] (decision boundaries pass through regions of low data density). Most present day algorithms for semi-supervised learning rely on one or more of these assumptions to predict the unknown labels.\nIn practice, graph-based methods have been found to be quite suitable for geometry-based learning tasks, primarily because they provide an easy way of exploiting information from the geometry of the dataset. These methods involve constructing a distance-based similarity graph whose vertices represent the data points and whose edge weights are in general a decreasing function of distance between them. The key assumption here is that the label function is “smooth” over the graph, in the sense that labels of vertices do not vary much over edges with high weights (i.e., edges that connect close or similar points). There are numerous ways of quantitatively imposing smoothness constraints over label functions defined on vertices of a similarity graph. Most graph-based semisupervised classification algorithms incorporate one of these criteria as a penalty against the fitting error in a regularization problem, or as a constraint term while minimizing the fitting error in an optimization problem. For example, a commonly used measure of smoothness for a label function f is the graph Laplacian regularizer fTLf (L being the graph Laplacian), and many algorithms involve minimizing this quadratic energy function while ensuring that f satisfies the known set of labels [2], [4]. There also exist higher-order variants of this measure known as iterated graph Laplacian regularizers fTLmf , that have been shown to make the problem more well-behaved [5]. On the other hand, a spectral theory based classification algorithm restricts f to be spanned by the first few eigenvectors of the graph Laplacian [6], [7], that are known to form a representation basis for smooth functions on the graph. In each of the examples, the criterion enforces smoothness of the labels over the graph – a lower value of the regularizer fTLf , and a smaller number of leading eigenvectors to model f imply that vertices that are close neighbors on the graph are more likely to have the same label.\nA more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]–[12]. It involves treating the class label function f as a bandlimited graph signal, and\nar X\niv :1\n70 5.\n09 51\n8v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 7\n2 label prediction as a bandlimited reconstruction problem. The advantage of this approach is that one can also analyze, using sampling theory, the label complexity of semi-supervised classification, that is, the minimum fraction of labeled examples required for perfect prediction of the unknown labels. A key ingredient in this formulation is the bandwidth ω(f) of signals on the graph, which is defined as the largest Laplacian eigenvalue for which the projection of the signal over the corresponding eigenvector is non-zero. Signals with lower bandwidth tend to be smoother on the graph, and are useful for modeling label functions over similarity graphs. Label prediction using bandlimited reconstruction then involves estimating a label function that minimizes prediction error on the known set under a bandwidth constraint. This can also be carried out without explicitly computing the eigenvectors of the Laplacian, and has been shown to be quite competitive in comparison to state-of-the-art graph-based semi-supervised learning methods [13].\nAlthough graph-based semi-supervised learning methods are well-motivated, their connection to the underlying geometry of the dataset is not clearly understood so far in a theoretical sense. Recent works focused on justifying these approaches by exploring their geometrical interpretation in the limit of infinitely available unlabeled data. This is typically done by assuming a probabilistic generative model for the dataset and analyzing the graph smoothness criteria in the asymptotic setting for certain commonly-used graph construction schemes. For example, it has been shown that for data points drawn from a smooth distribution with an associated smooth label function (i.e., the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]–[17]. A similar connection ensues for semi-supervised learning problems in the classification setting (i.e., when labels are discrete in the feature space). If points drawn from a smooth distrbution are separated by a smooth boundary into two classes, then the graph cut for the partition converges to a weighted volume of the boundary [3], [18]. This is consistent with the low density separation assumption – a low value of the graph cut implies that the boundary passes through regions of low data density.\nHowever, these results cannot be applied for providing an interpretation of the sampling theoretic approach to learning, since we lack knowledge about the convergence behavior of the bandwidth of class indicator signals. An interpretation of this approach would help complete our theoretical understanding of graph-based semi-supervised learning approaches and strengthen their link with the semi-supervised smoothness assumption and its variants. Therefore, in this work, we seek answers for the following questions: • What is the connection between the bandwidth of class\nindicator signals over the similarity graph and the underlying geometry of the data set? • What is the interpretation of the bandlimited recontruction approach for label prediction? • How many labeled examples does one require for perfect prediction?\nTo answer these questions, our work analyzes the asymptotic behavior of an iterated Laplacian-based bandwidth estimator for class indicator signals on similarity graphs constructed from a statistical model for the feature vectors. To make our analysis as general as possible, we consider two data models: separable and nonseparable. These generative models are quite practical and can be used to mimic most datasets in the real world. The separable model assumes that data points are independently drawn from an underlying probability distribution in the feature space and each class is separated from the others by a smooth boundary. On the other hand, the nonseparable model assumes a mixture distribution for the data where the data points are drawn independently with certain probability from separate class conditional distributions. We also introduce a notion of “boundaries” for classes in the nonseparable model in the form of overlap regions (i.e., the region of ambiguity), defined as the set of points where the probability of belonging and not belonging to a class are both non-zero. This definition is quite practical and useful for characterizing the geometry of such datasets.\nUsing the data points, we consider a specific graph construction scheme that applies the Gaussian kernel over Euclidean distances between feature vectors for computing their similarities (our analysis can be generalized easily to arbitrary kernels under simple assumptions). In order to compute the bandwidth of any signal on the graph, we define an estimator based on the iterated Laplacian regularizer. A significant portion of this paper focuses on analyzing the stochastic convergence of this bandwidth estimate (using variance-bias decomposition) in the limit of infinite data points for any class indicator signal on the graph. The analysis in our work suggests a novel sampling theoretic interpretation of graph-based semi-supervised learning and the main contributions can be summarized as follows:\n• Relationship between bandwidth and data geometry. For the separable model, we show that under certain rate conditions, the bandwidth estimate for any class converges to the supremum of the data density over the class boundary. Similarly, for the nonseparable model, we show that the bandwidth estimate converges to the supremum of the density over the overlap region. • Interpretation of bandlimited reconstruction. Using the geometrical interpretation of the bandwidth, we conclude that bandlimited reconstruction allows one to choose the complexity of the hypothesis space while predicting unknown labels (i.e., a larger bandwidth allows more complex class boundaries). • Quantification of label complexity. We also show that the bandwidth of class indicator signals is closely linked theoretically to the fraction of labeled points required for perfect classification which is in turn related to the geometry of the data.\nOur analysis has significant implications: Firstly, class indicator signals have a low bandwidth if class boundaries lie in regions of low data densities, that is, the semi-supervised assumption holds for graph-based methods. And secondly, our analysis also helps quantify the impact of bandwidth and data geometry in semi-supervised learning problems. Specifically, it\n3 enables us to theoretically assert that for the sampling theoretic approach to graph-based semi-supervised learning, the label complexity (minimum fraction of labeled points required) of learning classifiers matches the theoretical estimate and is indeed lower if the boundary lies in regions of low data density, as demonstrated empirically in earlier works [6], [7].\nThe rest of this paper is organized as follows: In Section II, we formally introduce the statistical data models and the graph construction scheme for analysis. In Section III, we review prior work and underline their connections with our work. In Section IV, we state our main results and outline their implications. In Section V, we prove the major building blocks for our results and leave the details of smaller components to the Appendix. We finally conclude with numerical validation in Section VI, followed by discussion and an outline of future work in Section VII. It is worth noting that the bandwidth convergence result for the separable model and an interpretation of bandlimited reconstruction were given in our preliminary work [19]. This paper presents complete formal proofs for those results, extends them to the nonseparable model, and also considers the label complexity problem."
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : ""
    }, {
      "heading" : "A. Data models",
      "text" : "1) The separable model: In this model, we assume that the dataset consists of a pool of n random, d-dimensional feature vectors X = {X1,X2, . . . ,Xn} drawn independently from some probability density function p(x) supported on Rd (this is assumed for simplicity, the analysis can be extended to arbitrary manifolds M ⊂ Rd, but would more technically involved). To simplify our analysis, we also assume that p(x) is bounded from above, Lipschitz continuous and twice differentiable. We assume that a smooth hypersurface ∂S, with radius of curvature lower bounded by a constant τ , splits Rd into two disjoint classes S and Sc, with indicator functions 1S(x) : Rd → {0, 1} and 1Sc(x) : Rd → {0, 1}. This is illustrated in Figure 1a. Thus, the n-dimensional class indicator signal for class S is given by 1S ∈ {0, 1}n such that 1S(i) = 1S(Xi), i.e., the ith entry of 1S is 1 if Xi ∈ S and 0 otherwise.\n2) The nonseparable model: In this model, we assume that each class has its own conditional distribution supported on Rd (that may or may not overlap with other distributions of other classes). The data set consists of a pool of n random and independent d-dimensional feature vectors X = {X1,X2, . . . ,Xn} drawn independently from any of the distributions pi(x) with probabilities αi, such that ∑ i αi = 1. For our analysis, we consider a class denoted by an index A with selection probability αA, class conditional distribution pA(x) and an n-dimensional indicator vector 1A whose ith component takes value 1 if Xi is drawn from class A. This model is illustrated in Figure 1b. Further, we denote by αAc = 1 − αA the probability that a point does not belong to A and by pAc(x) = ∑ i 6=A αipi(x)/αAc the density of all such points. The marginal distribution of data points is then given by the mixture density\np(x) = αApA(x) + αAcpAc(x). (1)\nOnce again, to simplify our analysis, we assume that all distributions are Lipschitz continuous, bounded from above and twice differentiable in Rd. Next, we introduce the notion of a “boundary” for classes in the nonseparable model as follows: for class A, we define its overlap region ∂A as\n∂A = {x ∈ Rd | pA(x)pAc(x) > 0} (2)\nIntuitively, ∂A can be considered as the region of ambiguity, where both points belonging and not belonging to A co-exist. In other words, ∂A can be thought of as a “boundary” that separates the region where points can only belong to A from the region where points can never belong to A. Since class indicator signals on graphs will change values only within the overlap region, one would expect that the indicators will be smoother if there are fewer data points within this region. We shall show later that this is indeed the case, both theoretically and experimentally. Note that the definition of the boundary is not very meaningful for class conditional distributions with decaying tails, such as the Gaussian, since the boundary encompasses the entire feature space. However, in such cases, one can approximate the boundary with appropriate thresholds in the definition and this approximation can also be formalized for distributions with exponentially decaying tails."
    }, {
      "heading" : "B. Graph construction",
      "text" : "Using the n feature vectors, we construct an undirected distance-based similarity graph where nodes represent the data points and edge weights are proportional to their similarity, given by the Gaussian kernel:\nwij = Kσ2(Xi,Xj) = 1\n(2πσ2)d/2 e−‖Xi−Xj‖ 2/2σ2 , (3)\nwhere σ is the variance (bandwidth) of the Gaussian kernel. Further, we assume wii = 0, i.e., the graph does not have self-loops. The adjacency matrix of the graph W is an n× n symmetric matrix with elements wij , while the degree matrix is a diagonal matrix with elements Dii = ∑ j wij . We define the graph Laplacian as L = 1n (D −W). Normalization by n ensures that the norm of L is stochastically bounded as n grows. Since the graph is undirected, L is a symmetric matrix with non-negative eigenvalues 0 ≤ λ1 ≤ · · · ≤ λn and an orthogonal set of corresponding eigenvectors {u1, . . . ,un}. It is known that for a larger eigenvalue λ, the corresponding eigenvector u exhibits greater variation when plotted over the nodes of the graph [8]."
    }, {
      "heading" : "C. Bandwidth of graph signals",
      "text" : "The bandwidth ω(f) of any signal f on the graph is defined as the largest eigenvalue for which the projection of the signal on the corresponding eigenvector is non-zero, i.e.,\nω(f) = max i {λi | uTi f > 0}. (4)\nIdeally, finding the bandwidth ω(f) of a graph signal f requires computing the eigenvectors {ui} and the corresponding projections f̃i = uTi f . However, analyzing the convergence\n4 (a) (b)\nFig. 1: Statistical models of data considered in this work: (a) The separable model, (b) The nonseparable model. Darker shades indicate regions of higher density.\nof these coefficients is technically challenging. Therefore, we resort to the following estimate of the bandwidth [12]:\nωm(f) =\n( fTLmf\nfT f\n)1/m , (5)\nwhere we call ωm(f) the mth-order bandwidth estimate. It can be shown that the bandwidth estimates satisfy the property: for all 0 < m1 < m2, ωm1(f) ≤ ωm2(f) ≤ ω(f). In other words, {ωm(f)} forms a monotonically improving sequence of estimates of the true bandwidth ω(f). Thus, we have the following relation:\n∀f , ω(f) = lim m→∞ ωm(f). (6)\nAnalyzing the convergence of ωm(1S) and ωm(1A) as n → ∞, σ → 0 and m → ∞ constitutes the main subject for the rest of this paper. Specifically, we relate these quantities to the underlying data distribution p(x) and class boundaries (the hypersurface ∂S in the separable case and the overlap region ∂A in the nonseparable case).\nNote that the limit in (6) holds in a point-wise sense. This means that analyzing the convergence of the bandwidth estimates ωm(1S) and ωm(1A) as n→∞ and then applying the limit m→∞ gives only an idea about the convergence of actual bandwidths ω(1S) and ω(1A) as n→∞. Specifically, it does not imply convergence of ω(1S) and ω(1A) to the same values as ωm(1S) and ωm(1A), since the limits are not interchangeable unless (6) holds in a uniform sense. However, based on our experiments and results on label complexity, we believe that our intuition is accurate, i.e., the convergence results hold for the actual bandwidths, not only their estimates. We leave the analysis of this intricacy for future work."
    }, {
      "heading" : "D. Bandlimited interpolation for classification",
      "text" : "Bandwidth plays an important role in the sampling theoretic approach for semi-supervised learning. In this approach, one finds a label assignment by minimizing the error over the known set, while ensuring that the resulting class indicator vector is bandlimited over the similarity graph, i.e,\nmin f\n‖f(L)− y(L)‖2 subject to ω(f) < ωL (7)\nwhere L denotes the set of known labels, y denotes the true class labels and f(L) and y(L) denote the values of f and y on the set L respectively. The constraint restricts the hypothesis space by constraining it to a set of bandlimited signals with\nbandwidth less than ωL, which is equivalent to enforcing smoothness of the labels over the graph. One can also use sampling theory to set ωL as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].\nNote that the bandwidth-based approach for semisupervised learning extends the Fourier eigenvector approach suggested in [6], [7] by allowing the estimation of the complexity of the bandlimited space via ωL. Further, the label prediction can be carried out without explicitly computing the eigenvectors of L. It is based on iterative and alternate projections onto convex sets and can be implemented in an efficient manner via graph filtering operations [21]."
    }, {
      "heading" : "III. RELATED WORK AND CONNECTIONS",
      "text" : "Existing graph-based semi-supervised learning and spectral clustering methods have been justified by analyzing the convergence of graph-based smoothness measures (such as the graph cut and the Laplacian regularizer) for various graph construction schemes in two different settings – classification and regression. The classification setting assumes that labels indicate class memberships and are discrete, typically with 1/0 values. Note that both the separable and nonseparable data models considered in our paper are in the classification setting. On the other hand, in the regression setting, one allows the class label signal f to be smooth and continuous with soft values, i.e, f ∈ Rn and later applies some thresholding mechanism to infer class memberships. For example, in the two class problem, one can assign +1 and −1 to the two classes and threshold f at 0. Convergence analysis of smoothness measures in this setting requires different scaling conditions than the classification setting, and leads to fundamentally different limiting quantities that require differentiability of the label functions. A summary of convergence results in the literature for both settings is presented in Table I. Although these results do not focus on analyzing the bandwidth of class indicator signals, the proof techniques used in this paper are inspired by some of these works. We review them in this section and discuss their connections to our work."
    }, {
      "heading" : "A. Classification setting",
      "text" : "Prior work under this setting assumes the separable data model where the feature space is partitioned by smooth decision boundaries into different classes. When m = 1, the bandwidth estimate ωm(1S) for the separable model in our\n5\nwork reduces (within a scaling factor) to the empirical graph cut for the partitions S and Sc of the feature space, i.e.,\nCut(S, Sc) = ∑\nXi∈S,Xj∈Sc wij = 1\nT SL1S . (8)\nConvergence of this quantity has been studied before in the context of spectral clustering, where one tries to minimize it across the two partitions of the nodes. It has been shown in [18] that the cut formed by a hyperplane ∂S in Rd converges with some scaling under the rate conditions σ → 0 and nσd+1 →∞ as\n1\nnσ 1TSL1S p.−→ 1√ 2π ∫ ∂S p2(s)ds, (9)\nwhere ds ranges over all (d−1)-dimensional volume elements tangent to the hyperplane ∂S, and p. denotes convergence in probability. The analysis is also extended to other graph construction schemes such as the k-nearest neighbor graph and the r-neighborhood graph, both weighted and unweighted. The condition σ → 0 in (9) is required to have a clear and well-defined limit on the right hand side. We borrow this convergence regime in our work, since it allows a succinct interpretation of the bandwidth of class indicator signals. Intuitively, it enforces sparsity in the similarity matrix W by shrinking the neighborhood volume as the number of data points increases. As a result, one can ensure that the graph remains sparse even as the number of points goes to infinity.\nA similar result for a similarity graph constructed with normalized weights w′ij = wij/ √ didj was shown earlier for an arbitrary hypersurface ∂S in [3], where di denotes the degree of node i. In this case, normalization of the graph weights results in convergence to 1√\n2π\n∫ ∂S p(s)ds. The results in [3],\n[18] aim to provide an interpretation for spectral clustering – up to some scaling, the empirical cut value converges to a weighted volume of the boundary. Thus, spectral clustering is a means of performing low density separation on a finite sample drawn from a distribution in feature space.\nAlthough these works provide inspiration for the proof techniques used for analyzing the separable model in this paper, they cannot be directly used in the convergence analysis of ωm(1S) for m > 1, which is the main focus of our paper. Additionally, our work is the first to propose and analyze the nonseparable model in the classification setting, i.e., convergence results for ωm(1A)."
    }, {
      "heading" : "B. Regression setting",
      "text" : "To predict the labels of unknown samples in the regression setting, one generally minimizes the graph Laplacian regularizer fTLf subject to the known label constraints [4]:\nmin f\nfTLf such that f(L) = y(L), (10)\nOne particular convergence result in this setting assumes that n data points are drawn i.i.d. from p(x) and are labeled by sampling a smooth function f(x) on Rd. Here, the graph\n6 Laplacian regularizer fTLf can be shown to converge in the asymptotic limit under the conditions σ → 0 and nσd → ∞ as in [14], [15]:\n1\nnσ2 fTLf p.−−→ C ∫ Rd ‖∇f(x)‖2p2(x)dx, (11)\nwhere for each n, f is the n-dimensional label vector representing the values of f(x) at the n sample points, ∇ is the gradient operator and C is a constant factor independent of n and σ. The right hand side of the result above is a weighted Dirichlet energy functional that penalizes variation in the label function weighted by the data distribution. Similar to the justification of spectral clustering, this result justifies the formulation in (10) for semi-supervised classification: given label constraints, the predicted label function must vary little in regions of high density. The work of [15], [23] also generalizes the result for arbitrary kernel functions used in defining graph weights, and data distributions defined over arbitrary manifolds in Rd. Similar convergence results have also been derived for the higher-order Laplacian regularizer fTLmf obtained from uniformly distributed data [5]. In this case, it was shown that for data points obtained from a uniform distribution on a ddimensional submanifold M ⊂ RN such that Vol(M) = 1 and 2m-differentiable functions f(x), one has as n → ∞, σ → 0,\n1\nnσmn fTLmf p.−−→ C ∫ M f(x)∆mf(x)dx, (12)\nwhere ∆ is the Laplace operator and σn = n−1/(2d+4+α) is a vanishing sequence with α > 0. Extensions for non-uniform probability distributions p(x) over the manifold can be obtained using the weighted Laplace-Beltrami operator [16], [17]. More recently, an `p-based Laplacian regularization has been proposed for imposing smoothness constraints in semisupervised learning problems [22]. This is similar to higherorder regularization but is defined as Jp(f) = ∑ i,j∈E w p ij |fi− fj |p, where wij = K(‖Xi −Xj‖/σ) and K(.) is a smoothly decaying Kernel function. It has been shown for a bounded density p(x) defined on [0, 1]d that for every p ≥ 2, as n→∞, σ → 0,\n1\nn2σp+d Jp(f) p.−−→ C ∫ [0,1]d ‖∇f(x)‖pp2(x)dx. (13)\nAlthough our work also uses higher powers of L in the expressions for ωm(1S) and ωm(1A), we cannot use the convergence results in (12) and (13) since they are only applicable for smooth functions (i.e., differentiable upto certain order) on Rd. Specifically, these results cannot be applied for the bandwidth of discrete 0/1 class indicator functions.\nTo summarize, the results in the literature mostly pertain to convergence analysis of variants of the graph cut or the graph Laplacian regularizer for different models of data and graph construction schemes, and do not provide insight into the convergence of bandwidths of discrete 0/1 class indicator signals. In contrast, we analyze bandwidth expressions involving these class indicator signals and higher powers of L, and for the first time, extend it to a nonseparable data model. As opposed to other smoothness measures considered earlier,\nanalyzing the bandwidth allows us to interpret graph-based semi-supervised learning using the sampling theorem [12] and provide a quantitative insight into label complexity based on data geometry."
    }, {
      "heading" : "IV. MAIN RESULTS AND DISCUSSION",
      "text" : "A. Interpretation of bandwidth and bandlimited reconstruction\nWe first show that under certain conditions, the bandwidth estimates of class indicator signals, over the distance-based similarity graph described earlier, converge to quantities that are functions of the underlying distribution and the class boundary for both data models. This convergence is achieved under the following asymptotic regime:\n1) Increasing size of dataset: n→∞. 2) Shrinking neighborhood volume: σ → 0. 3) Improving bandwidth estimates: m→∞.\nNote that an increasing size of the dataset (Condition 1) is required for the stochastic convergence of the bandwidth estimate. Condition 2 ensures that the limiting values are concise and have a simple interpretation in terms of the data geometry. Intuitively, Condition 2 ensures that as the number of data points increases, one looks at a smaller neighborhood around each data point, as a result, the degree of each node in the graph does not blow up. Finally, Condition 3 leads to improving values of the bandwidth estimate. The convergence results are precisely stated in the theorems below:\nTheorem 1. If n→∞, σ → 0 and m→∞ while satisfying the following rate conditions\n1) (nσmd)/(mCm)→∞, where C = 2/(2π)d/2, 2) m/nσ → 0, 3) mσ2 → 0, 4) σ1/m → 1,\nthen for the separable model, one has\nωm(1S) p.−−→ sup\ns∈∂S p(s), (14)\nwhere “p.” denotes convergence in probability.\nTheorem 2. If n→∞, σ → 0 and m→∞ while satisfying the following rate conditions\n1) (nσmd)/(mCm)→∞, where C = 2/(2π)d/2, 2) m/n→ 0, 3) mσ2 → 0,\nthen for the non-separable model, one has\nωm(1A) p.−−→ sup\nx∈∂A p(x). (15)\nThe dependence of the results on the rate conditions will be explained later in the proofs section. An example of parameter choices that allow all the scaling laws to hold simultaneously is illustrated in the following corollary:\nCorollary 1. Equations (14) and (15) hold if for each value of n, we choose m and σ as follows:\nm = [m0 (log n) y], (16)\nσ = σ0 n −x/md, (17)\n7 for constants m0, σ0 > 0, 1/2 < y < 1 and 0 < x < 1. [ . ] indicates taking the nearest integer value.\nTheorems 1 and 2 give an explicit connection between bandwidth estimates of class indicator signals and the underlying geometry of the dataset. This interpretation forms the basis of justifying the choice of bandwidth as a smoothness constraint in graph-based learning algorithms. Theorem 1 suggests that for the separable model, if the boundary ∂S passes through regions of low probability density, then the bandwidth of the corresponding class indicator vector ω(1S) is low. A similar conclusion is suggested for the nonseparable model from Theorem 2, i.e., if the density of data points in the overlap region ∂A is low, then the bandwidth ω(1A) is low. In other words, low density of data in the boundary regions leads to smooth indicator functions.\nFrom our results, we also get an intuition behind the smoothness constraint imposed in the bandlimited reconstruction approach (7) for semi-supervised learning. Basically, enforcing smoothness on classes in terms of indicator bandwidth ensures that the learning algorithm chooses a boundary passing through regions of low data density in the separable case. Similarly, in the nonseparable case, it ensures that variations in labels occur in regions of low density. Further, the bandwidth cutoff ωL effectively imposes a constraint on the complexity of the hypothesis space – a larger value increases the size of the hypothesis space and results in choosing more complex boundaries.\nAs a special case of our analysis, we also get a convergence result for the graph cut in the nonseparable model analogous to the results of [18] for the separable model. Note that the cut in this case equals the sum of weights of edges connecting points that belong to class A to points that do not belong to class A, i.e.,\nCut(A,Ac) = ∑\nXi∈A,Xj∈Ac wij = 1\nt AL1A. (18)\nWith this definition, we have the following result:\nTheorem 3. If n→∞, σ → 0 and nσd+1 →∞, then 1\nn Cut(A,Ac)\np.−−→ ∫ αAαAcpA(x)pAc(x)dx (19)\nThe result above indicates that if the overlap between the conditional distributions of a particular class and its compliment is low, then the value of the graph cut is lower. This justifies the use of spectral clustering in the context of nonseparable models."
    }, {
      "heading" : "B. Label complexity of SSL",
      "text" : "In the context of semi-supervised learning, we define the label complexity as the minimum fraction of labeled examples required for perfectly predicting the labels of the unlabeled data points. This quantity essentially is an indicator of how “good” the semi-supervised problem is, i.e., how much help do we get from geometry while learning. A low label complexity is indicative of a favorable situation, where one is able to learn from only a few known labels by exploiting data geometry. In the following discussion, we first estimate the theoretical\nlabel complexities of the data models we consider, and then show that the expected label complexity of the sampling theoretic approach to learning exactly matches these values in the asymptotic limit.\nTheoretical label complexities: A simple way to compute the label complexity, for the data models we consider, is to find the fraction of points belonging to a region that fully encompasses the boundary. To formalize this, let us define the following two regions in Rd:\nXS = {x : p(x) ≤ sup s∈∂S p(s)}, (20) XA = {x : p(x) ≤ sup x∈∂A p(x)} (21)\nNote that by definition, ∂S is fully contained in XS and ∂A is fully contained in XA (see Figure 2 for an example in R1). To perfectly reconstruct 1S and 1A, it is sufficient to know the labels of all points in XS and XA respectively, as this strategy removes all ambiguity in labeling the two classes. Based on this, we arrive at the following conclusions:\nObservation 1. The theoretical label complexity of learning 1S and 1A in the asymptotic limit are P (XS) and P (XA) respectively, where P (E) = ∫ E p(x)dx.\nLabel complexity of graph-based learning: Using our results, we can show that the same label complexities hold for the graph-based sampling theoretic approach to semisupervised classification. In this context, label complexity can be seen as the fraction of samples required for perfectly reconstructing a signal on the similarity graph. It is known that the fraction of samples required for perfectly reconstructing a bandlimited signal cannot be more than the fraction of eigenvalues of the Laplacian below the signal’s bandwidth [12]. Since our bandwidth convergence results relate the bandwidth of indicators for the two data models with data geoemtry, we only need to asymptotically relate the number of eigenvalues of L below any constant in terms of data geometry. This is achieved through the following result:\nTheorem 4. Let NL(t) be the number of eigenvalues of L below a constant t. Then, as n→∞ and σ → 0, we have\nE { 1\nn NL(t)\n} −→ P ({x : p(x) ≤ t}) , (22)\nProof. See Section V-D.\nSubstituting the bandwidth convergence results from Theorems 1 and 2 (i.e., t = ωm(1S) and t = ωm(1A)), we immediately get the desired value of the expected label complexity of graph-based semi-supervised learning:\nTheorem 5. If the conditions in Theorems 1 and 2 hold, then the expected label complexities of bandlimited reconstruction for the separable and nonseparable models are given as\nlim 1\nn E {NL(ωm(1S)} → P (XS), and (23)\nlim 1\nn E {NL(ωm(1A)} → P (XA). (24)\nThe following remarks are in order:\n8 (a) (b)\nFig. 2: 1-D example illustrating the theoretical label complexity for (a) the separable model, (b) the nonseparable model. Note that labeling all points where density is lower than supremum density over the boundary resolves all ambiguity and results in perfect prediction.\n1) Note that Theorem 4 and Theorem 5 can be strengthened by proving convergence of 1nNL(t) rather than its expected value. This requires further analysis, which we leave for future work. The result in Theorem 5 also encourages us to conjecture that the convergence results for bandwidth estimates also hold for the convergence of the bandwidth itself. 2) This result further strengthens the connection between graph-based learning methods and the semi-supervised smoothness assumption, since one can conclude that the number of labeled examples required for perfect prediction depends on the geometry of the data around the boundary. A low value of the density at the boundary results in a lower label complexity. 3) One might ask what is the advantage of using graphbased methods for semi-supervised learning, if we can predict the class labels by the simple labeling strategy used to compute label complexities in Observation 1. Note that our definition of label complexity is an ideal one which aims for perfect reconstruction. The power of graph-based methods would be more evident for a more practical definition of label complexity, where one tries to find the number of labels required for achieving a certain error tolerance. We leave this issue for future work."
    }, {
      "heading" : "V. PROOFS",
      "text" : "We now present the proofs of Theorems 1 and 2 through a sequence of lemmas1. The main idea is to perform a variancebias decomposition of the bandwidth estimate and then prove the convergence of each term independently. Specifically, for the indicator vector 1R ∈ {0, 1}n of any region R ⊂ Rn, one can consider the random variable:\n(ωm(1R)) m = 1TRL m1R 1TR1R = 1 n1 T RL m1R 1 n1 T R1R . (25)\nWe study the convergence of this quantity by considering the numerator and denominator separately (it is easy to show that the fraction converges if both the numerator and denominator\n1A partial sketch of the proof for the separable model is also provided in our parallel work [19]; here we provide the complete proof.\nconverge). By the strong law of large numbers, we conclude the following for the denominator as n→∞:\n1 n 1TR1R a.s.−−→ ∫ x∈R p(x)dx, (26)\nwhere a.s. denotes almost sure convergence. For the numerator, we decompose it into two parts – a variance term for which we show stochastic convergence using a concentration inequality, and a bias term for which we prove deterministic convergence."
    }, {
      "heading" : "A. Convergence of variance terms",
      "text" : "Let V = 1n1 T RL m1R, then we have the following concentration result:\nLemma 1 (Concentration). For every > 0, we have:\nPr (|V − E {V }| > ) ≤ 2 exp ( −[n/(m+ 1)]σmd 2\n2CmE {V }+ 23 |Cm − σmdE {V }|\n) , (27)\nwhere C = 2/(2π)d/2.\nProof. Recalling that wi,j = Kσ2(Xi,Xj), we begin by explicitly expanding V = 1n1 T R(D−W)m1R into the following summation\nV = 1\nnm+1 ∑ i1,i2,...,im+1 g ( Xi1 ,Xi2 , . . . ,Xim+1 ) . (28)\nThe above expansion has the form of a V-statistic. Details on how to explicitly write the summation are given in Appendix A. Note that g is composed of a sum of 2m terms; each a product of m kernel functions that are non-negative. Therefore,\ng ≤ 2m‖K‖m∞ = (\n2\n(2πσ2)d/2\n)m = Cm\nσmd . (29)\nIn order to apply a concentration inequality for V, we first re-write it in the form of a U-statistic by regrouping terms in the summation in order to remove repeated indices, as given in [24]:\nV = 1\nn(m+1) ∑ (n,m+1) g∗ ( Xi1 ,Xi2 , . . . ,Xim+1 ) , (30)\n9 where ∑\n(n,m+1) denotes summation over all ordered (m+1)tuples of distinct indices taken from the set {1, . . . , n}, n(m+1) = n.(n − 1) . . . (n − m) is the number of (m+1)permutations of n and g∗ is a convex combination of specific instances of g that absorbs repeating indices (see supplementary material for a complete expansion):\ng∗ (x1,x2, . . . ,xm+1) = n(m+1)\nnm+1 g (x1,x2, . . . ,xm+1)\n+ (terms with repeated indices). (31)\nTherefore, g∗ has the same upper bound as that of g derived in (29). Moreover, using the fact that E {V } = E {g∗}, we can bound the variance of g∗ as\nVar {g∗} ≤ ‖g∗‖∞E {g∗} = Cm\nσmd E {V } . (32)\nFinally, plugging in the bound and variance of g∗ in Bernstein’s inequality for U-statistics as stated in [23], [24], we arrive at the desired result of (27).\nNote that as n → 0 and σ → 0 with rates satisfying (nσmd)/(mCm) → ∞, we have P (|V − E {V } | > ) → 0 for all > 0. The continuous mapping theorem then allows us to conclude that V 1/m p.−→ (E {V })1/m."
    }, {
      "heading" : "B. Convergence of the bias term for the separable model",
      "text" : "To evaluate the convergence of bias terms, we shall require the following properties of the d-dimensional Gaussian kernel:\nLemma 2. If p(x) is twice differentiable, then∫ Kσ2(x,y)p(y)dy = p(x) +O ( σ2 ) . (33)\nProof. Using the substitution y = x+ t followed by a Taylor series expansion about x, we have∫\nKσ2(x,y)p(y)dy\n=\n∫ 1\n(2πσ2)d/2 e−‖t‖ 2/2σ2p(x + t)dt\n=\n∫ 1\n(2πσ2)d/2 e−‖t‖\n2/2σ2 ( p(x) + tT∇p(x)\n+ 1\n2 tT∇2p(x)t + . . .\n) dt\n= p(x) + 0 + σ2\n2 Tr(∇2p(x)) + . . .\n= p(x) +O(σ2),\nwhere Tr(.) denotes the trace of a matrix, and the third step follows from simple component-wise integration.\nLemma 3. If p(x) is twice differentiable, then∫ Kaσ2(x, z)Kbσ2(z,y)p(z)dz\n= K(a+b)σ2(x,y)\n( p ( bx + ay\na+ b\n) +O ( σ2 )) . (34)\nProof. Note that\nKaσ2(x, z)Kbσ2(z,y)\n= 1\n(2πaσ2) d 2\ne− ‖x−z‖2 2aσ2 1\n(2πbσ2) d 2\ne− ‖z−y‖2 2bσ2\n= 1\n(2π(a+ b)σ2) d 2\ne − ‖x−y‖\n2\n2(a+b)σ2 1\n(2π aba+bσ 2) d 2\ne − ‖z− bx+ay a+b ‖2 2( ab a+b )σ2\n= K(a+b)σ2(x,y) K ab a+bσ 2\n( bx + ay\na+ b , z\n) .\nTherefore, we have∫ Kaσ2(x, z)Kbσ2(z,y)p(z)dz\n= K(a+b)σ2(x,y)\n∫ K ab\na+bσ 2\n( bx + ay\na+ b , z\n) p(z)dz\n= K(a+b)σ2(x,y)\n( p ( bx + ay\na+ b\n) +O ( σ2 )) ,\nwhere the last step follows from Lemma 2.\nIn order to prove convergence for the separable model, we need the following results:\nLemma 4. If p(x) is Lipschitz continuous, then for a smooth hypersurface ∂S that divides Rd into S1 and S2, and whose radius has curvature that is bounded by τ > 0,\nlim σ→0\n1\nσ ∫ S1 ∫ S2 Kσ2(x1,x2)p α(x1)p β(x2)dx1dx2 = 1√ 2π ∫ ∂S pα+β(s)ds, (35)\nwhere α and β are positive integers. Moreover, for positive integers a, b, and α, β, α′, β′ such that α+ β = α′ + β′ = γ, we have:\nlim σ→0\n1\nσ ∫ S1 ∫ S1 [ Kaσ2(x1,x2)p α(x1)p β(x2)\n−Kbσ2(x1,x2)pα ′ (x1)p β′(x2) ] dx1dx2\n= √ b− √ a√\n2π ∫ ∂S pγ(s)ds. (36)\nProof. See Appendix B.\nWe now prove the deterministic convergence of E {\n1 n1 T SL m1S } in the following lemma:\nLemma 5. As n→∞, σ → 0 such that m/n→ 0,mσ2 → 0, we have\n1 σ E { 1 n 1TSL m1S } → t(m)√\n2π ∫ ∂S pm+1(s)ds, (37)\nwhere t(m) = ∑m−1 r=1 ( m−1 r ) (−1)r( √ r + 1− √ r).\nProof. We evaluate E {\n1 n1 T SL m1S }\nterm by term by expanding Lm as (D−W)m−1(D−W). Details on the intermediate\n10\nsteps of this expansion are given in Appendix A. Using (33) repeatedly, we have for the first two terms of the expansion:\n1 σ E { 1 n 1TSD . . .D(D−W)1S } = 1\nσ ∫ S ∫ Sc Kσ2(x,y)p m(x)p(y)dxdy\n+O (σ) +O ( m nσ ) . (38)\nFor the rest of the terms, we also require the use of (34). However, in this case, we encounter several terms of the form p(θx+(1−θ)y) for some θ ∈ [0, 1]. Since mσ2 → 0 and p(x) is assumed to be Lipschitz continuous, we can approximate such terms by p(x) or p(y). Therefore, for all terms in the expansion of (D −W)m−1 containing r > 1 occurrences of W (there are ( m−1 r ) such terms), repeated use of (33), (34) gives:\n1 σ E { 1 n 1TS [D m−1−r,Wr](D−W)1S }\n= 1\nσ [ ∫ S ∫ S Krσ2(x,y)p α(x)pβ(y)dxdy\n− ∫ S ∫ S K(r+1)σ2(x,y)p α′(x)pβ ′ (y)dxdy ] +O(σ) +O ( m nσ ) , (39)\nwhere α+β = α′+β′ = m+1 and [Dm−1−r,Wr] denotes an expression containing r occurrences of W and m−1−r occurrences of D. Now, using Lemma 4, we conclude that the right hand sides of (38) and (39) converge to 1√\n2π\n∫ ∂S pm+1(s)ds\nand √ r+1− √ r√\n2π\n∫ ∂S pm+1(s)ds, respectively, as σ → 0 and\nm/nσ → 0. Putting everything together in the expansion of E {\n1 n1 T SL m1S } , we get the desired result.\nSince σ1/m → 1, we have( E { 1\nn 1TSL m1S\n})1/m = σ1/m ( 1 σ E { 1 n 1TSL m1S })1/m → ( t(m)√\n2π ∫ ∂S pm+1(s)ds )1/m .\n(40)\nFinally, we note that as m→∞, we have t(m)√2π ∫∂S pm+1(s)ds∫ S p(x)dx 1/m −→ sup s∈∂S p(s). (41)\nTherefore, we conclude for the separable model\nωm(1S)→ sup s∈∂S p(s) (42)"
    }, {
      "heading" : "C. Convergence of bias term for the nonseparable model",
      "text" : "For the nonseparable model, we need to prove convergence of E { 1 n1 T AL m1A } . This is illustrated in the following lemma:\nLemma 6. As n→∞, σ → 0 such that m/n→ 0,mσ2 → 0, we have\nE { 1\nn 1TAL m1A\n} → ∫ αAαAcpA(x)pAc(x)p m−1(x)dx.\n(43)\nProof. Similar to the proof of Lemma 5, we evaluate E {\n1 n1 T AL m1A }\nterm by term by expanding Lm as (D − W)m−1(D −W). Using (33) repeatedly, we have for the first two terms of the expansion:\nE { 1\nn 1TAD . . .D(D−W)1A } = ∫ αAαAcpA(x)pAc(x)p m−1(x)dx +O ( σ2 ) +O (m n ) .\n(44)\nFurther, for all terms in the expansion of (D − W)m−1 containing r > 1 occurrences of W (there are ( m−1 r ) such terms), repeated use of (33), (34) gives:\nE { 1\nn 1TA[D\nm−1−r,Wr](D−W)1A } = O ( σ2 ) +O (m n ) .\n(45)\nTherefore, as σ → 0, m/n→ 0, we get the desired result.\nWe finally note that as m→∞, we have(∫ αAαAcpA(x)pAc(x)p\nm−1(x)dx∫ A p(x)dx\n)1/m s.−−→ sup\nx∈∂A p(x).\n(46) Therefore, we conclude for the nonseparable model\nωm(1A)→ sup x∈∂A p(x). (47)\nNote that Lemma 6 for the special case of m = 1 yields\n1 n 1TAL1A →\n∫ αAαAcpA(x)pAc(x)dx, (48)\nwhich proves Theorem 3."
    }, {
      "heading" : "D. Proof of Theorem 4",
      "text" : "We begin by recalling the definition of the empirical spectral distribution (ESD) of L: µn(x) = 1n ∑n i=1 δ(x − λi), where {λi} are the eigenvalues of L. For each x, µn(x) is a function of X1, . . . ,Xn, and thus a random variable. Note that the fraction of eigenvalues of L below a constant t, and its expected value can be computed from the ESD as\n1 n NL(t) = ∫ t 0 µn(x)dx, (49)\nE { 1\nn NL(t)\n} = ∫ t 0 E {µn(x)} dx. (50)\nTherefore, to understand the behavior of the expected fraction of eigenvalues of L below t, we need to analyze the convergence of the expected ESD in the asymptotic limit. The idea is to show the convergence of the moments of E {µn(x)} to the moments of a limiting distribution µ(x). Then, by a standard convergence result, E {µn(I)} → µ(I) for intervals I . More precisely, let the⇒ symbol denote weak convergence\n11\nof measures, then we use the following result that follows from the Weierstrass approximation theorem:\nLemma 7. Let µn be a sequence of probability measures and µ be a compactly supported probability measure. If∫ xmµn(dx)→ ∫ xmµ(dx) for all m ≥ 1, then µn ⇒ µ.\nWe then use the following result on equivalence of different notions of weak convergence of measures [25, Theorem 25.2] in order to prove our result for cumulative distribution functions.\nLemma 8. µn ⇒ µ if and only if µn(A) → µ(A) for every µ-continuity set A.\nWe begin by writing the mth moment of E {µn(x)}:∫ xmE {µn(x)} dx = 1\nn n∑ i=1 E {λmi } = E { 1 n Tr (Lm) } .\n(51) Now, note that Lm = (D − W)m = Dm +∑m k=1 ( m k ) [Dm−k,Wk], where [Dm−k,Wk] denotes product terms with m−k occurrences of D and k occurrences of W. Therefore, we have for the right hand side of (51):\nE { 1\nn Tr (Lm) } = ∫ (∫ K(xi1 ,xi2)p(xi2)dxi2\n) . . . (52)(∫\nK(xi1 ,xim+1)p(xim+1)dxim+1 ) p(xi1)dxi1\n+O (m n ) (expected value of other terms).\nUsing (33) repeatedly in the equation above, we get: E { 1\nn Tr (Lm)\n} = ∫ pm+1(x)dx+O (m n ) +O ( σ2 ) . (53)\nTherefore, as n→∞ and σ → 0, we have:∫ xmE {µn(x)} dx = ∫ pm(x)p(x)dx. (54)\nFrom the right hand side of the equation above, we conclude that the mth moment of the expected ESD of L converges to the mth moment of the distribution of a random variable Y = p(X), where p(x) is the probabilty density function of X. Moreover, since pY (y) has compact support, E {µn(x)} converges weakly to the probability density function of pY (y). Hence, the following can be said about the expected fraction of eigenvalues of L:\nE { 1\nn NL(t)\n} = ∫ t 0 E {µn(x)} dx\ns.−−→ ∫ t 0 pY (y)dy = ∫ p(x)≤t p(x)dx. (55)\nThis proves our claim in Theorem 4. Note that, to prove the stochastic convergence of the fraction itself rather than its expected value, we would need a condition similar to those in Theorems 1 and 2 to hold for each moment. In that case, σ will go to 0 in a prohibitively slow fashion. We believe that this is an artifact of the methods we employ for proving the result.\nHence, our conjecture is that the convergence result holds for 1 nNL(t) itself, and we leave the analysis of this statement for future work."
    }, {
      "heading" : "VI. NUMERICAL VALIDATION",
      "text" : "We now present simple numerical analyses to validate our results and demonstrate their usefulness in practice. For simulating the separable model, we first consider a data distribution based on a 2D Gaussian Mixture Model (GMM) with two Gaussians: µ1 = [−1 0],Σ1 = 0.25I and µ2 = [1 0],Σ2 = 0.16I, and mixing proportions α1 = 0.4 and α2 = 0.6 respectively. The probability density function is illustrated in Figure 3. Next, we evaluate the claim of Theorem 1 on five boundaries, described in Table II. These boundaries are depicted in Figure 4 and are illustrative of typical separation assumptions such as linear or non-linear and low or high density.\nFor simulating the nonseparable model, we first construct the following smooth (twice-differentiable) 2D probability density function\nq(x, y) =\n{ 3 π [ 1− (x2 + y2) ]2 , x2 + y2 ≤ 1\n0, x2 + y2 > 1 (56)\nNote that datapoints (X,Y ) can be sampled from this distribution by setting the coordinates X = √ 1− U1/4 cos(2πV ), Y = √\n1− U1/4 sin(2πV ), where U, V ∼ Uniform(0, 1). We then use q(x, y) to define a nonseparable 2D model with mixture density p(x, y) = αApA(x, y) + αAcpAc(x, y), where pA(x, y) = q(x − 0.75, y), pAc(x, y) = q(x + 0.75, y) and αA = αAc = 0.5. The probability density function is illustrated in Figure 3. The overlap region or boundary ∂A for this model is given by\n∂A = {\n(x, y) : (x− 0.75)2 + y2 < 1 and (x+ 0.75)2 + y2 < 1 } . (57)\nFurther, for this model, we have sup∂A p(x) = 0.2517. In our first experiment, we validate the statements of Theorems 1 and 2 by comparing the left and right hand sides of (14) and (15) for corresponding boundaries. This is carried out in the following way: we draw n = 2500 points from each model and construct the corresponding similarity graphs using σ = 0.1. Then, for the boundaries ∂Si in the separable model and ∂A in the nonseparable model, we carry out the following steps:\n12\n1) We first construct the indicator functions 1Si and 1A on the corresonding graphs. 2) We then compute the empirical bandwidth ω(1Si) and ω(1A) in a manner that takes care of numerical error: we first obtain the eigenvectors of the corresponding L, then set ω(1Si) and ω(1A) to be ν for which energy contained in the Fourier coefficients corresponding to eigenvalues λj > ν is at most 0.01%, i.e.,\nω(1Si) = min { ν ∣∣ ∑ j:λj>ν ( uTj 1Si )2 ≤ 10−4} (58) ω(1A) = min\n{ ν ∣∣ ∑ j:λj>ν ( uTj 1A )2 ≤ 10−4}. (59) The procedure above is repeated 100 times and the mean of ω(1Si) and ω(1A) are compared with sups∈∂Si p(s) and supx∈∂A p(x) respectively. The result is plotted in Figure 5. We observe that the empirical bandwidth is close to the theoretically predicted value and has a very low standard deviation. This supports our conjecture that stochastic convergence should hold for the bandwidth. To further justify this claim, we study the behavior of the standard deviation of ω(1Si)\n13\nand ω(1A) as a function of n in Figure 6, where we observe a decreasing trend consistent with our result.\nFor our second experiment, we validate the bound on the label complexity of graph-based semi-supervised learning in Theorem 5 by reconstructing the indicator function corresponding to ∂S3 and ∂A from a fraction of labeled examples on the corresponding graphs. This is carried out as follows: For a given budget B, we find the set of points to label pivoted column-wise Gaussian elimination on the eigenvector matrix U of L. This method ensures that the obtained labeled set guarantees perfect recovery for signals spanned by the first B eigenvectors of L [12]. We then recover the indicator functions from these labeled sets by solving the least squares problem in (7) followed by thresholding. The mean reconstruction error is defined as\nEmean = No. of mismatches on unlabeled set\nSize of unlabeled set . (60)\nWe repeat the experiment 100 times by generating different graphs and plot the averaged Emean against the fraction of labeled examples. The result is illustrated in Figure 7. We observe that the error goes to zero as the fraction of labeled points goes beyond the theoretically predicted label complexity as predicted. This reinforces the intuition that the bandwidth of class indicators is closely linked with the inherent geometry of the data."
    }, {
      "heading" : "VII. DISCUSSIONS AND FUTURE WORK",
      "text" : "In this paper, we provided an interpretation of the graph sampling theoretic approach to semi-supervised learning. Our work analyzed the bandwidth of class indicator signals with respect to the Laplacian eigenvector basis and revealed its connection to the underlying geometry of the dataset. This connection is useful in justifying graph-based approaches for semi-supervised and unsupervised learning problems, and provides a geometrical interpretation of the smoothness assumptions imposed in the bandlimited reconstruction approach. Specifically, our results have shown that an estimate of the bandwidth of class indicators converges to the supremum of the probability density on the class boundaries for the separable model, and on the overlap regions for the nonseparable model. This quantifies the connection between the assumptions of smoothness (in terms of bandlimitedness) and low density separation, since boundaries passing through regions of low data density result in lower bandwidth of the class indicator signals. We numerically validated these results through various experiments.\nThere are several directions in which our results can be extended. In this paper we only considered Gaussian-weighted graphs, an immediate extension would be to consider arbitrary kernel functions for computing graph weights, or density dependent edge-connections such as k-nearest neighbors. Another possibility is to consider data defined on a subset of the d-dimensional Euclidean space.\nOur analysis also sheds light on the label complexity of graph-based semi-supervised learning problems. We showed that perfect prediction from a few labeled examples using a graph-based bandlimited interpolation approach requires the\nsame amount of labeling as one would need to completely encompass the boundary or region of ambiguity. This indicates that graph-based approaches achieve the theoretical label complexity as dictated by the underlying geometry of the problem. We believe that the main potential of graph-based methods will be apparent in situations where one can tolerate a certain amount of prediction error, in which case such approaches shall require fewer labeled data. We plan to investigate this as part of future work."
    }, {
      "heading" : "APPENDIX A",
      "text" : "EXPANSION OF 1TSL m1S AND E { 1 n1 T SL m1S }\nTo expand 1TSL m1S in terms of the elements wij of W, we first write the expression for each product term. Since Lm = 1nm (D −W)\nm, there are 2m such terms. Let us first introduce the following notation: [D,W]m denotes a product term containing the matrices D and W, such that there are m matrices in the product. Note that Lm is essentially a summation of all possible [D,W]m with appropriate signs.\nNow, the explicit expression for 1TS [D,W]m1S can be obtained using the following procedure: 1) All product terms have a form defined by the following\ntemplate:\n1TS [D,W]m1S = ∑\ni1,...,im+1\n(1S)i1wi1i2w∗i3 . . . w∗imw∗im+1(1S)∗, (61)\nwhere the locations with ∗ need to be filled with appropriate indices depending on the product term. Note that each wij is contributed by either a D or W depending on its location in the expression. 2) We fill the locations one-by-one from left to right, using the following set of rules. Let wab be the term preceding w∗c, then • If wab is contributed by D, then ∗ = a. • If wab is contributed by W, then ∗ = b. 3) Let waim+1 denote the term preceding (1S)∗. Then, we have the following rule: • If waim+1 is contributed by D, then ∗ = a. • If waim+1 is contributed by W, then ∗ = im+1. The expansion of 1TSL m1S can be found by summing up the expansions of the individual product terms 1TS [D,W]m1S . Recalling that wij = K(Xi,Xj), we conclude\n1 n 1TSL m1S = 1 nm+1 ∑ i1,...,im+1 g(Xi1 ,Xi2 , . . . ,Xim+1).\n(62)\nThe expression for E {\n1 n1 T SL m1S }\ncan be evaluated in a similar fashion, except that the summations are replaced by integrals. We first evaluate the expected value of individual product terms E { 1 n1 T S [D,W]m1S } by the following rules: 1) The template for the expected value of any product term can be expressed through the following template:\nE { 1\nn 1TS [D,W]m1S } = ∫ · · · ∫ (\n1S(x1)K(x1,x2)K(x∗,x3)\n. . .K(x∗,xm+1)1S(x∗) ) p(x1)dx1 . . . p(xm+1)dxm+1,\n(63)\nwhere the locations with ∗ need to be filled with appropriate indices depending on the product term. Once again, each K(xi,xj) is contributed by either a D or a W. 2) We fill the locations one-by-one from left to right, using the following set of rules. Let K(xa,xb) be the term preceding K(x∗,xc). Then • If K(xa,xb) is contributed by D, then ∗ = a. • If K(xa,xb) is contributed by W, then ∗ = b. 3) Further, let K(xa,xm+1) be the term preceding 1S(x∗). Then • If K(xa,xm+1) is contributed by D, then ∗ = a. • If K(xa,xm+1) is contributed by W, then ∗ = m+ 1."
    }, {
      "heading" : "APPENDIX B PROOF OF LEMMA 4",
      "text" : "The key ingredient required for evaluating the integrals in Lemma 4 involves selecting a radius R (< τ ) as a function of σ that satisfies the following properties as σ → 0:\n1) R→ 0, 2) R/σ →∞, 3) R2/σ → 0, 4) R/σ → 0, where R := ∫ ‖z‖>RKσ2(0, z)dz.\nA particular choice of R is given by R = √ dσ2 log (1/σ2). Note that R→ 0 as σ → 0. Further, R\nσ = √ d log (1/σ2), (64)\nR2\nσ = dσ log (1/σ2). (65)\nHence, R/σ → ∞ and R2/σ → 0 as σ → 0. Additionally, substituting the expression for R in the tail bound for the norm of a d-dimensional Gaussian vector gives us:\nR σ\n= 1\nσ ∫ ‖z‖>R Kσ2(0, z)dz\n≤ 1 σ\n( σ2d\nR2\n)−d/2 e− R2 2σ2 + d2\n= 1\nσ\n( eσ2 log(1/σ2) )d/2 . (66)\nTherefore, for d > 1, R/σ → 0 as σ → 0. Further, it is easy to ensure R < τ for the regime of σ in our proofs.\n15\nWe now consider the proof of equation (35), let\nI := 1\nσ ∫ S1 ∫ S2 Kσ2(x1,x2)p α(x1)p β(x2)dx1dx2. (67)\nFurther, let [S1]R indicate a tubular region of thickness R adjacent to the boundary ∂S in S1, i.e., the set of points in S1 at a distance ≤ R from the boundary. Then, we have\nI = 1\nσ ∫ [S1]R pα(x1) ∫ S2\nKσ2(x1,x2)p β(x2)dx2 dx1︸ ︷︷ ︸\nI1\n+ 1\nσ ∫ [S1]cR pα(x1) ∫ S2 Kσ2(x1,x2)p β(x2)dx2 dx1︸ ︷︷ ︸\nE1\n. (68)\nE1 is the error associated with approximating I by I1 and exhibits the following behavior\nLemma 9. limσ→0E1 = 0.\nProof. Note that\nE1 ≤ 1\nσ (pmax) β ∫ [S1]cR pα(x1) (∫ S2 Kσ2(x1,x2)dx2 ) dx1\n≤ 1 σ (pmax) β ∫ [S1]cR pα(x1) (∫ ‖z‖>R Kσ2(0, z)dz ) dx1\n= R σ (pmax) β ∫ [S1]cR pα(x1)dx1 ≤ R σ (pmax) α+β . (69)\nUsing limσ→∞ R/σ = 0, we get the desired result.\nIn order to analyze I1, we need to define certain geometrical constructions (illustrated in Figure 8) as follows:\nDefinition 1. 1) For each x1 ∈ [S1]R, we define a transformation of coordinates as:\nx1 = s1 + r1n(s1), (70)\nwhere s1 is the foot of the perpendicular dropped from x1 onto ∂S, r1 is the distance between s1 and x1, and n(s1) is the surface normal at s1 (towards the direction of x1). Since the minimum radius of curvature of ∂S is τ and R < τ , this mapping is injective. 2) For each s1 ∈ ∂S, let H+s1 denote the halfspace created by the plane tangent on s1 and on the side of S2. Similarly, let H−s1 denote the halfspace on the side of S1, that is, H−s1 = R\nd \\H+s1 . 3) Let W+s1(x) denote an infinite slab of thickness x tangent\nto ∂S at s1 and towards the side of S2. Let W−s1(y) denote a similar slab of thickness y on the side of S1. 4) Finally, for any x, let B(x, R) denote the Euclidean ball of radius R centered at x.\nWe now consider I1, the main idea here is to approximate the integral over S2 by an integral over the halfspace H+s1 . Hence, we have:\nI1 = 1\nσ ∫ [S1]R pα(x1) ∫ H+s1 Kσ2(x1,x2)p β(x2)dx2dx1︸ ︷︷ ︸\nI2\n+ 1\nσ ∫ [S1]R pα(x1) ∫ S2−H+s1 Kσ2(x1,x2)p β(x2)dx2dx1︸ ︷︷ ︸\nE2\n,\n(71)\nwhere E2 is the error associated with the approximation. Therefore, we have\nI = I2 + E2 + E1. (72) We now show that as σ → 0, I2 → 1√2π ∫ ∂S pα+β(s)ds, and E2 → 0.\nLemma 10. limσ→0 I2 = 1√2π ∫ ∂S pα+β(s)ds.\nProof. Using the change of coordinates x1 = s1 + r1n(s1), we have\nI2 = 1\nσ ∫ ∂S ∫ R 0\npα(s1 + r1n(s1))(∫ H+s1 Kσ2(s1 + r1n(s1),x2)p β(x2)dx2 ) |detJ(s1, r1)|ds1dr1, (73)\nwhere J(s1, r1) denotes the Jacobian of the transformation. Now, an arc P̂Q of length ds at a distance r1 away from ∂S gets mapped to an arc P̂ ′Q′ on ∂S whose length lies in the interval [ds(1− r1τ ), ds(1 + r1 τ )]. Therefore, for all points within [S1]R, we have( 1− R\nτ\n)d−1 ≤ |detJ(s1, r1)| ≤ ( 1 + R\nτ\n)d−1 . (74)\nFurther, since p(x) is Lipschitz continuous with constant Lp, pα(x) is also Lipschitz continuous with constant Lp,α. There-\n16\nfore, for any x1 ∈ [S1]R, we have pα(x1) = pα(s1) +Lp,αR. This leads to the following simplification for I2:\nI2 = ( 1 +O(Rd−1) ) ∫ ∂S pα(s1)I3(s1)ds1\n+O(Rd) ∫ ∂S I3(s1)ds1, (75)\nwhere we defined\nI3(s1) := 1\nσ ∫ R 0 ∫ H+s1 Kσ2(s1 + r1n(s1),x2)p β(x2)dx2dr1.\n(76) Note that every x2 ∈ H+s1 can be written as s2 + r2n(s2), where n(s2) = −n(s1). Hence, we get\nI3(s1) = ∫ Rd−1\n1\n(2πσ2) d−1 2\ne− ‖s1−s2‖\n2\n2σ2 pβ(s2 − r2n(s1))ds2\n× 1 σ ∫ R 0 ∫ ∞ 0 1√ 2πσ2 e− (r1+r2) 2 2σ2 dr1dr2\n= (∫ Rd−1\n1\n(2πσ2) d−1 2\ne− ‖s1−s2‖\n2\n2σ2 pβ(s2)ds2 +O(R)\n)\n× 1 σ ∫ R 0 ∫ ∞ 0 1√ 2πσ2 e− (r1+r2) 2 2σ2 dr1dr2\n= ( pβ(s1) +O(σ 2) +O(R) ) ×\n1\nσ ∫ R 0 ∫ ∞ 0 1√ 2πσ2 e− (r1+r2) 2 2σ2 dr1dr2, (77)\nwhere we used Lipschitz continuity of pβ(x) in the second equality and applied Lemma 2 to arrive at the last step. Further, using the definition of the Q-function and integration by parts, we note that\n1\nσ ∫ R 0 ∫ ∞ 0 1√ 2πσ2 e− (r1+r2) 2 2σ2 dr1dr2\n= ∫ R/σ 0 ∫ ∞ 0 1√ 2π e− (x+y)2 2 dxdy\n= ∫ R/σ 0 Q(y)dy\n= yQ(y) ∣∣∣∣R/σ 0 − ∫ R/σ 0 Q′(y)dy = R\nσ Q\n( R\nσ\n) +\n1√ 2π\n( 1− e−R 2/2σ2 ) .\nTherefore, I3(s1) = ( pβ(s1) +O(σ 2) +O(R) ) ×(\nR σ Q\n( R\nσ\n) +\n1√ 2π\n( 1− e−R 2/2σ2 )) . (78)\nCombining (75) and (78) and using the fact that R/σ → ∞ as σ → 0 (from the definition of R), we get\nlim σ→0 I2 = 1√ 2π ∫ ∂S pα+β(s)ds, (79)\nwhich concludes the proof.\nWe now consider the error term E2 and prove the following result:\nLemma 11. limσ→0E2 = 0.\nProof. Let us first rewrite E2 as follows:\nE2 = 1\nσ ∫ [S1]R pα(x1)I4(x1)dx1, (80)\nwhere we defined\nI4(x1) := ∫ S2−H+s1 Kσ2(x1,x2)p β(x2)dx2. (81)\nThe key idea is to lower and upper bound I4(x1) for all x1 using worst case scenarios and evaluate the limits of the bounds. Note that I4(x1) is largest in magnitude when S1 or S2 is a sphere of radius τ , as illustrated in Figures 9a and 9b. We now make certain geometrical observations. For any x1 = s1 + r1n(s1) ∈ [S1]R, we observe from Figure 9b that\nI4(x1) ≤ ∫ W−s1 ( R2−r21 2(τ−r1)\n)Kσ2(x1,x2)pβ(x2)dx2 +\n∫ B(x1,R)c Kσ2(x1,x2)p β(x2)dx2\n≤ ∫ W−s1 (R ′) Kσ2(x1,x2)p β(x2)dx2 + p β max R. (82)\nwhere R′ = R 2\n2(τ−R) . Similarly, from Figure 9a, we observe that\nI4(x1) ≥ − [ ∫\nW+s1 ( R2−r21 2(τ+r1) )Kσ2(x1,x2)pβ(x2)dx2 +\n∫ B(x1,R)c Kσ2(x1,x2)p β(x2)dx2 ] ≥ −\n[ ∫ W+s1 (R ′) Kσ2(x1,x2)p β(x2)dx2 + p β max R ] .\n(83)\nSubstituting these in (80) and using a simplification similar to that of I2 in (75), we get\nE2 ≤ ( 1 +O(Rd−1) ) ∫ ∂S pα(s1)I − 5 (s1)ds1\n+O(Rd) ∫ ∂S I−5 (s1)ds1 + R σ pα+βmax , (84)\nE2 ≥ − ( 1 +O(Rd−1) ) ∫ ∂S pα(s1)I + 5 (s1)ds1\n−O(Rd) ∫ ∂S I+5 (s1)ds1 − R σ pα+βmax , (85)\nwhere we defined\nI−5 (s1) := 1\nσ ∫ R 0 ∫ W−s1 (R ′) Kσ2(s1 + r1n(s1),x2)\npβ(x2)dx2dr1, (86)\nI+5 (s1) := 1\nσ ∫ R 0 ∫ W+s1 (R ′) Kσ2(s1 + r1n(s1),x2)\npβ(x2)dx2dr1. (87)\n17\nSimilar to the evaluation of I3(s1) in (77), we have I+5 (s1) = ( pβ(s1) +O(σ 2) +O(R) ) ×\n1\nσ ∫ R 0 ∫ R′ 0 1√ 2πσ2 e− (r1+r2) 2 2σ2 dr1dr2, (88)\nI−5 (s1) = ( pβ(s1) +O(σ 2) +O(R) ) ×\n1\nσ ∫ R 0 ∫ R′ 0 1√ 2πσ2 e− (r1−r2) 2 2σ2 dr1dr2. (89)\nWe now evaluate the two 1-D integrals as follows:\n1\nσ ∫ R 0 ∫ R′ 0 1√ 2πσ2 e− (r1+r2) 2 2σ2 dr1dr2\n= ∫ R/σ 0 ∫ R′/σ 0 1√ 2π e− (x+y)2 2 dxdy\n= ∫ R/σ 0 ( Q(y)−Q ( y + R′ σ )) dy\n= ∫ R/σ 0 Q(y)dy + ∫ R′/σ 0 Q(y)dy − ∫ R+R′ σ 0 Q(y)dy = R\nσ Q\n( R\nσ\n) + R\nσ Q\n( R′\nσ\n) − R+R ′\nσ Q\n( R+R′\nσ ) 1√ 2π ( 1− e− R2 2σ2 − e− R′2 2σ2 + e− (R+R′)2 2σ2 ) .\nSimilarly,\n1\nσ ∫ R 0 ∫ R′ 0 1√ 2πσ2 e− (r1−r2) 2 2σ2 dr1dr2\n= ∫ R/σ 0 ∫ R′/σ 0 1√ 2π e− (x−y)2 2 dxdy\n= ∫ R/σ 0 ( Q ( y − R ′ σ ) −Q(y) ) dy\n= ∫ 0 −R′/σ Q(y)dy + ∫ R−R′ σ 0 Q(y)dy − ∫ R/σ 0 Q(y)dy = R′\nσ Q\n( −R ′\nσ\n) + R−R′\nσ Q\n( R−R′\nσ ) − R σ Q ( R σ ) 1√ 2π ( e− R′2 2σ2 − 1 + e− (R+R′)2 2σ2 − e− R2 2σ2 ) .\nNoting that as σ → 0, R/σ →∞ and R′/σ → 0, we conclude that limσ→0E2 = 0.\nThe proof of (36) proceeds in a similar fashion by approximating the inner integral using hyperplanes. Specifically, similar to the proof of (35), we can show that the integral on the left hand side can be written as I + E, where\nI := 1\nσ ∫ [S1]R ∫ H−s1 [ Kaσ2(x1,x2)p α(x1)p β(x2)\n−Kbσ2(x1,x2)pα ′ (x1)p β′(x2) ] dx1dx2, (90)\nand E is the residual associated with the approximation that can be shown to go to zero as σ → 0 (we skip this proof since it is quite similar to the analysis for (35)). In order to evaluate I , we perform a change of coordinates x1 = s1 + r1n(s1) as before to obtain\nI = 1\nσ ∫ ∂S ∫ R 0 [ pα(s1 + r1n(s1))(∫\nH−s1\nKaσ2(s1 + r1n(s1),x2)p β(x2)dx2 ) − pα\n′ (s1 + r1n(s1))(∫\nH−s1\nKbσ2(s1 + r1n(s1),x2)p β′(x2)dx2 )] |detJ(s1, r1)|ds1dr1\n= ∫ ∂S pα(s1)Iβ(s1)ds1 − ∫ ∂S pα ′ (s1)Iβ′(s1)ds1 +O ( Rd ) ,\n(91)\nwhere we defined\nIβ(s1) := 1\nσ ∫ R 0 ∫ H−s1 Kaσ2(s1 + r1n(s1),x2)\npβ(x2)dx2dr1,\nIβ′(s1) := 1\nσ ∫ R 0 ∫ H−s1 Kbσ2(s1 + r1n(s1),x2)\npβ ′ (x2)dx2dr1.\n18\nBy using a change of coordinates for x2 similar to the steps in (77), we obtain\nIβ(s1) = ( pβ(s1) +O(σ 2) +O(R) ) ×\n1\nσ ∫ R 0 ∫ ∞ 0 1√ 2πaσ2 e− (r1−r2) 2 2aσ2 dr1dr2, (92)\nIβ′(s1) = ( pβ ′ (s1) +O(σ 2) +O(R) ) ×\n1\nσ ∫ R 0 ∫ ∞ 0 1√ 2πbσ2 e− (r1−r2) 2 2bσ2 dr1dr2. (93)\nThe 1-D integrals can be evaluated as follows:\n1\nσ ∫ R 0 ∫ ∞ 0 1√ 2πaσ2 e− (r1−r2) 2 2aσ2 dr1dr2\n= √ a ∫ R/√aσ 0 ∫ ∞ 0 1√ 2π e− (x−y)2 2 dxdy = √ a\n∫ R/√aσ 0 Q(−y)dy\n= √ a ∫ R/√aσ 0 (1−Q(y))dy = R\nσ − R σ Q ( R√ aσ ) − √ a√\n2π\n( 1− e−R 2/2aσ2 ) ,\n1\nσ ∫ R 0 ∫ ∞ 0 1√ 2πbσ2 e− (r1−r2) 2 2bσ2 dr1dr2\n= R σ − R σ Q ( R√ bσ ) − √ b√ 2π ( 1− e−R 2/2bσ2 ) .\nUsing the fact that α+ β = α′ + β′ = γ, and taking the limit σ → 0 after putting everything together, we conclude\nlim σ→0 I =\n√ b− √ a√\n2π ∫ ∂S pγ(s)ds. (94)"
    } ],
    "references" : [ {
      "title" : "Supervised Learning (Adaptive Computation and Machine Learning)",
      "author" : [ "Olivier Chapelle", "Bernhard Schölkopf", "Alexander Zien. Semi" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "Denny Zhou", "Olivier Bousquet", "Thomas N. Lal", "Jason Weston", "Bernhard Schölkopf" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "On the relation between low density separation, spectral clustering and graph cuts",
      "author" : [ "Hariharan Narayanan", "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty" ],
      "venue" : "In IN ICML,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Semi-supervised learning by higher order regularization",
      "author" : [ "Xueyuan Zhou", "Mikhail Belkin" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Using manifold stucture for partially labeled classification",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Semi-supervised learning on riemannian manifolds",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "author" : [ "D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Towards a sampling theorem for signals on arbitrary graphs",
      "author" : [ "A Anis", "A Gadde", "A Ortega" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Sampling large data on graphs",
      "author" : [ "H. Shomorony", "A.S. Avestimehr" ],
      "venue" : "In Signal and Information Processing (GlobalSIP),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Discrete signal processing on graphs: Sampling theory",
      "author" : [ "S. Chen", "R. Varma", "A. Sandryhaila", "J. Kovačević" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Efficient sampling set selection for bandlimited graph signals using graph spectral proxies",
      "author" : [ "A. Anis", "A. Gadde", "A. Ortega" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Active semisupervised learning using sampling theory for graph signals",
      "author" : [ "Akshay Gadde", "Aamir Anis", "Antonio Ortega" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Measure based regularization",
      "author" : [ "Olivier Bousquet", "Olivier Chapelle", "Matthias Hein" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Uniform Convergence of Adaptive Graph-Based Regularization, pages 50–64",
      "author" : [ "Matthias Hein" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Towards a theoretical foundation for laplacian-based manifold methods",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "An iterated graph laplacian approach for ranking on manifolds",
      "author" : [ "Xueyuan Zhou", "Mikhail Belkin", "Nathan Srebro" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "How the result of graph clustering methods depends on the construction of the graph",
      "author" : [ "Markus Maier", "Ulrike von Luxburg", "Matthias Hein" ],
      "venue" : "ESAIM: Probability and Statistics, 17:370–418,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Asymptotic justification of bandlimited interpolation of graph signals for semi-supervised learning",
      "author" : [ "A Anis", "A El Gamal", "S Avestimehr", "A Ortega" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Signal processing techniques for interpolation in graph structured data",
      "author" : [ "S.K. Narang", "A Gadde", "A Ortega" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Localized iterative methods for interpolation in graph structured data",
      "author" : [ "S.K. Narang", "A Gadde", "E. Sanou", "A Ortega" ],
      "venue" : "In Global Conference on Signal and Information Processing (GlobalSIP),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Asymptotic behavior of `p-based laplacian regularization in semi-supervised learning",
      "author" : [ "Ahmed El Alaoui" ],
      "venue" : "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Geometrical aspects of statistical learning theory",
      "author" : [ "Matthias Hein" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "Wassily Hoeffding" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1963
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "assumption [1], which states that the label function is smoother in regions of high data density.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "There also exist other similar variants of this assumption specialized for the classification setting, namely, the cluster assumption [2] (points in a cluster are likely to have the same class label) or the low density separation assumption [3] (decision boundaries pass through regions of low data density).",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "There also exist other similar variants of this assumption specialized for the classification setting, namely, the cluster assumption [2] (points in a cluster are likely to have the same class label) or the low density separation assumption [3] (decision boundaries pass through regions of low data density).",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 1,
      "context" : "For example, a commonly used measure of smoothness for a label function f is the graph Laplacian regularizer fLf (L being the graph Laplacian), and many algorithms involve minimizing this quadratic energy function while ensuring that f satisfies the known set of labels [2], [4].",
      "startOffset" : 270,
      "endOffset" : 273
    }, {
      "referenceID" : 3,
      "context" : "For example, a commonly used measure of smoothness for a label function f is the graph Laplacian regularizer fLf (L being the graph Laplacian), and many algorithms involve minimizing this quadratic energy function while ensuring that f satisfies the known set of labels [2], [4].",
      "startOffset" : 275,
      "endOffset" : 278
    }, {
      "referenceID" : 4,
      "context" : "There also exist higher-order variants of this measure known as iterated graph Laplacian regularizers fLf , that have been shown to make the problem more well-behaved [5].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "On the other hand, a spectral theory based classification algorithm restricts f to be spanned by the first few eigenvectors of the graph Laplacian [6], [7], that are known to form a representation basis for smooth functions on the graph.",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "On the other hand, a spectral theory based classification algorithm restricts f to be spanned by the first few eigenvectors of the graph Laplacian [6], [7], that are known to form a representation basis for smooth functions on the graph.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "A more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]–[12].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "A more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]–[12].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 11,
      "context" : "A more recent approach, derived from the emerging field of Graph Signal Processing (GSP) [8], considers the semisupervised learning problem from the perspective of sampling theory for graph signals [9]–[12].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "This can also be carried out without explicitly computing the eigenvectors of the Laplacian, and has been shown to be quite competitive in comparison to state-of-the-art graph-based semi-supervised learning methods [13].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 4,
      "context" : ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]–[17].",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 6,
      "context" : ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]–[17].",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 13,
      "context" : ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]–[17].",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 16,
      "context" : ", the regression setting), the graph Laplacian regularizer converges in the limit of infinite data points to a density-weighted variational energy functional that penalizes large variations of the labels in high density regions [5], [7], [14]–[17].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 2,
      "context" : "If points drawn from a smooth distrbution are separated by a smooth boundary into two classes, then the graph cut for the partition converges to a weighted volume of the boundary [3], [18].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : "If points drawn from a smooth distrbution are separated by a smooth boundary into two classes, then the graph cut for the partition converges to a weighted volume of the boundary [3], [18].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "enables us to theoretically assert that for the sampling theoretic approach to graph-based semi-supervised learning, the label complexity (minimum fraction of labeled points required) of learning classifiers matches the theoretical estimate and is indeed lower if the boundary lies in regions of low data density, as demonstrated empirically in earlier works [6], [7].",
      "startOffset" : 359,
      "endOffset" : 362
    }, {
      "referenceID" : 6,
      "context" : "enables us to theoretically assert that for the sampling theoretic approach to graph-based semi-supervised learning, the label complexity (minimum fraction of labeled points required) of learning classifiers matches the theoretical estimate and is indeed lower if the boundary lies in regions of low data density, as demonstrated empirically in earlier works [6], [7].",
      "startOffset" : 364,
      "endOffset" : 367
    }, {
      "referenceID" : 18,
      "context" : "It is worth noting that the bandwidth convergence result for the separable model and an interpretation of bandlimited reconstruction were given in our preliminary work [19].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "It is known that for a larger eigenvalue λ, the corresponding eigenvector u exhibits greater variation when plotted over the nodes of the graph [8].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "Therefore, we resort to the following estimate of the bandwidth [12]:",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "One can also use sampling theory to set ωL as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "One can also use sampling theory to set ωL as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "One can also use sampling theory to set ωL as the cutoff frequency associated with the sampling set [12], as considered in [13], [20].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Note that the bandwidth-based approach for semisupervised learning extends the Fourier eigenvector approach suggested in [6], [7] by allowing the estimation of the complexity of the bandlimited space via ωL.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "Note that the bandwidth-based approach for semisupervised learning extends the Fourier eigenvector approach suggested in [6], [7] by allowing the estimation of the complexity of the bandlimited space via ωL.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "It is based on iterative and alternate projections onto convex sets and can be implemented in an efficient manner via graph filtering operations [21].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "[18] also studies convergence of graph cuts for weighted k-nearest neighbor and r-neighborhood graphs which we do not include for brevity.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "Narayanan et al [3] p(x) supported on manifold M ⊂ Rd, separated into S and Sc by smooth hypersurface ∂S Normalized Gaussian weights w′ ij = wij √ didj 1 nσ 1SL1S n→∞, σ → 0 ∫ ∂S p(s)ds",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "Maier et al [18] p(x) supported onM⊂ Rd, separated into S and Sc by hyperplane ∂S r-neighborhood, unweighted 1 nrd+1 1SL1S n→∞, r → 0 ∫ ∂S p 2(s)ds",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "k-nn, unweighted, t = (k/n)1/d 1 ntd+1 1SL1S n→∞, t→ 0 ∫ ∂S p 1−1/d(s)ds fully-connected, Gaussian weights 1 nσ 1SL1S n→∞, σ → 0 ∫ ∂S p 2(s)ds Bousquet et al [14], Hein [15] p(x) and f(x) supported on Rd fully-connected, weights wij = 1 nσd K ( ‖Xi−Xj‖ σ2 ) , where K(.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "k-nn, unweighted, t = (k/n)1/d 1 ntd+1 1SL1S n→∞, t→ 0 ∫ ∂S p 1−1/d(s)ds fully-connected, Gaussian weights 1 nσ 1SL1S n→∞, σ → 0 ∫ ∂S p 2(s)ds Bousquet et al [14], Hein [15] p(x) and f(x) supported on Rd fully-connected, weights wij = 1 nσd K ( ‖Xi−Xj‖ σ2 ) , where K(.",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "Zhou et al [5] Uniformly distributed on d-dim.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "El Alaoui et al [22] p(x) supported on [0, 1]d fully-connected, weights wij = K ( ‖Xi−Xj‖ σ ) , where K(.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "El Alaoui et al [22] p(x) supported on [0, 1]d fully-connected, weights wij = K ( ‖Xi−Xj‖ σ ) , where K(.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "It has been shown in [18] that the cut formed by a hyperplane ∂S in R converges with some scaling under the rate conditions σ → 0 and nσ →∞ as",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "A similar result for a similarity graph constructed with normalized weights w′ ij = wij/ √ didj was shown earlier for an arbitrary hypersurface ∂S in [3], where di denotes the degree of node i.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "The results in [3], [18] aim to provide an interpretation for spectral clustering – up to some scaling, the empirical cut value converges to a weighted volume of the boundary.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 17,
      "context" : "The results in [3], [18] aim to provide an interpretation for spectral clustering – up to some scaling, the empirical cut value converges to a weighted volume of the boundary.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Regression setting To predict the labels of unknown samples in the regression setting, one generally minimizes the graph Laplacian regularizer fLf subject to the known label constraints [4]:",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "Laplacian regularizer fLf can be shown to converge in the asymptotic limit under the conditions σ → 0 and nσ → ∞ as in [14], [15]: 1 nσ2 fLf p.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "Laplacian regularizer fLf can be shown to converge in the asymptotic limit under the conditions σ → 0 and nσ → ∞ as in [14], [15]: 1 nσ2 fLf p.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "The work of [15], [23] also generalizes the result for arbitrary kernel functions used in defining graph weights, and data distributions defined over arbitrary manifolds in R.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "The work of [15], [23] also generalizes the result for arbitrary kernel functions used in defining graph weights, and data distributions defined over arbitrary manifolds in R.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Similar convergence results have also been derived for the higher-order Laplacian regularizer fLf obtained from uniformly distributed data [5].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "Extensions for non-uniform probability distributions p(x) over the manifold can be obtained using the weighted Laplace-Beltrami operator [16], [17].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Extensions for non-uniform probability distributions p(x) over the manifold can be obtained using the weighted Laplace-Beltrami operator [16], [17].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "More recently, an `p-based Laplacian regularization has been proposed for imposing smoothness constraints in semisupervised learning problems [22].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "It has been shown for a bounded density p(x) defined on [0, 1] that for every p ≥ 2, as n→∞, σ → 0, 1 n2σp+d Jp(f) p.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "[0,1]d ‖∇f(x)‖p(x)dx.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 11,
      "context" : "As opposed to other smoothness measures considered earlier, analyzing the bandwidth allows us to interpret graph-based semi-supervised learning using the sampling theorem [12] and provide a quantitative insight into label complexity based on data geometry.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 17,
      "context" : "As a special case of our analysis, we also get a convergence result for the graph cut in the nonseparable model analogous to the results of [18] for the separable model.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : "It is known that the fraction of samples required for perfectly reconstructing a bandlimited signal cannot be more than the fraction of eigenvalues of the Laplacian below the signal’s bandwidth [12].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : "1A partial sketch of the proof for the separable model is also provided in our parallel work [19]; here we provide the complete proof.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 23,
      "context" : "In order to apply a concentration inequality for V, we first re-write it in the form of a U-statistic by regrouping terms in the summation in order to remove repeated indices, as given in [24]:",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "(32) Finally, plugging in the bound and variance of g∗ in Bernstein’s inequality for U-statistics as stated in [23], [24], we arrive at the desired result of (27).",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "(32) Finally, plugging in the bound and variance of g∗ in Bernstein’s inequality for U-statistics as stated in [23], [24], we arrive at the desired result of (27).",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "However, in this case, we encounter several terms of the form p(θx+(1−θ)y) for some θ ∈ [0, 1].",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "25I and μ2 = [1 0],Σ2 = 0.",
      "startOffset" : 13,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : "This method ensures that the obtained labeled set guarantees perfect recovery for signals spanned by the first B eigenvectors of L [12].",
      "startOffset" : 131,
      "endOffset" : 135
    } ],
    "year" : 2017,
    "abstractText" : "Graph-based methods have been quite successful in solving unsupervised and semi-supervised learning problems, as they provide a means to capture the underlying geometry of the dataset. It is often desirable for the constructed graph to satisfy two properties: first, data points that are similar in the feature space should be strongly connected on the graph, and second, the class label information should vary smoothly with respect to the graph, where smoothness is measured using the spectral properties of the graph Laplacian matrix. Recent works have justified some of these smoothness conditions by showing that they are strongly linked to the semi-supervised smoothness assumption and its variants. In this work, we reinforce this connection by viewing the problem from a graph sampling theoretic perspective, where class indicator functions are treated as bandlimited graph signals (in the eigenvector basis of the graph Laplacian) and label prediction as a bandlimited reconstruction problem. Our approach involves analyzing the bandwidth of class indicator signals generated from statistical data models with separable and nonseparable classes. These models are quite general and mimic the nature of most real-world datasets. Our results show that in the asymptotic limit, the bandwidth of any class indicator is also closely related to the geometry of the dataset. This allows one to theoretically justify the assumption of bandlimitedness of class indicator signals, thereby providing a sampling theoretic interpretation of graph-based semi-supervised classification.",
    "creator" : "LaTeX with hyperref package"
  }
}