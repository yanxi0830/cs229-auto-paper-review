{
  "name" : "1205.2151.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination",
    "authors" : [ "Andri Mirzal" ],
    "emails" : [ "andrimirzal@utm.my" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—converged algorithm, inverse problems, L-curve, nonnegative matrix factorization, Tikhonov regularization.\nI. INTRODUCTION\nTHE nonnegative matrix factorization (NMF) is a tech-nique that decomposes a nonnegative matrix into a pair of other nonnegative matrices. Given a nonnegative matrix A, the NMF seeks to find two nonnegative matrices B and C such that:\nA ≈ BC, (1)\nwhere A ∈ RM×N+ = [a1, . . . , aN ], B ∈ R M×R + = [b1, . . . ,bR], C ∈ R R×N + = [c1, . . . , cN ], R denotes the number of factors which usually is chosen so that R ≪ min(M,N), and RM×N+ denotes M by N matrix with nonnegative entries. The conventional method in computing B and C is by minimizing the distance between A and BC in Frobenius norm,\nmin B,C\nJ(B,C) = 1\n2 ‖A−BC‖2F s.t. B ≥ 0,C ≥ 0. (2)\nIn addition, other criteria like Kullback-Leibler divergence [1], [2] and Csiszárs ϕ-divergence [3] can also be used.\nPreviously, the NMF was studied under the term positive matrix factorization by Paatero et al. [4], [5]. The popularity of the NMF is due to the work of Lee and Seung [6] in which they introduced a simple yet powerful NMF algorithm, and then show its applicability in image processing and text analysis.\nA. Mirzal is with the Faculty of Computer Science and Information Systems, University of Technology Malaysia, 81310 Johor Bahru, Malaysia e-mail: andrimirzal@utm.my\nIn addition, the algorithm also produces sparser factors (thus requires less storage) [6]–[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9]. Due to these reasons, many works explored the possibility of applying the NMF in some problem domains, e.g., document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.\nRecently, various works have been conducted to extend the standard NMF formulation (eq. 2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24]. These constraints are usually formulated based on inherent properties of the data and prior knowledge about the applications, so that computed solutions can be directed to have desired characteristics. Algorithms for solving the problems are mainly based on multiplicative update rules algorithms. This is due to the convenience of deriving algorithm directly from corresponding objective function. However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem. Note that even though some alternating nonnegativity-constrained least square based NMF algorithms (e.g., projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it’s not always clear how to incorporate those auxiliary constraints into the algorithms.\nIn this work, we propose a converged algorithm for Tikhonov regularized NMF using additive update rules. The additive update rules based algorithm for standard NMF first appeared in the work of Lee & Seung [1], but the convergence proof was given by Lin in ref. [18]. As in the multiplicative update rules version, the additive update rules based algorithm can also be derived directly from corresponding NMF objective, thus providing a convenient way in deriving converged algorithms for various NMF objectives.\nWe choose Tikhonov regularization as the auxiliary constraint because this constraint has been used in many applications, e.g., text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.e., the Tikhonov regularization, instead of L1\n3 norm—the more appropriate constraint for enforcing sparseness [31]), and showed that it can offer better results compared to the results of standard NMF. This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13]. In addition, from the inverse problem study, it is known that Tikhonov regularized least square (LS) is the more preferable form in solving inverse problems because solutions of the conventional LS tend to be unstable and dominated by data and rounding errors [32]–[34]. Moreover, in the presence of noise, frequently the conventional LS solutions are rather undesirable as it leads to amplification of noise in the direction of singular vectors with small singular values [35]. Since LS is the building block of the NMF,\n‖A−BC‖2F = N ∑\nn=1\n‖an −Bcn‖ 2 F , (3)\nthen it can be expected that Tikhonov regularized NMF will be the more appropriate approach to solving NMF problem in eq. 1.\nIntroducing Tikhonov regularization into the NMF brings the issue of how to properly determine the regularization parameters. From the inverse problems study it is known that the effectiveness of regularization methods depends strongly on the parameters; too much regularization creates a loss of information, and too little regularization leads to a solution that is dominated by noise components and has similar problems as in the unregularized solutions. There are two methods that are usually be used in determining an appropriate value for the regularization parameter: the L-curve and Morozov discrepancy principles. In this paper we will utilize the L-curve since the Morozov discrepancy principles require knowledge of the error level in the data which is often inaccessible [36].\nThe L-curve is a graphical tool that displays the tradeoff between approximation error and solution size as the regularization parameter varies. In this curve, the proper value for the regularization parameter is the value associated with corner of the curve where both solution and approximation error have minimum norms [32], [34]. There are some methods proposed to find this L-corner, e.g., [32], [34], [37], [38]. We will use a method proposed by Oraintara et al. in ref. [34] in which they defined L-corner to be the point of tangency between L-curve with positive curvature and a straight line of negative slope. Fig. 1 shows such condition for L-corner. We choose this method because it has convergence guarantee and is relatively faster to compute than the standard method; the maximum curvature approach [32].\nII. TIKHONOV REGULARIZED NMF\nTikhonov regularization is a method for regularizing solutions of linear inverse problems in order to enhance stability of the solutions and reduce the observational errors. The method was developed independently by Phillips [39] and Tikhonov [40]. In statistic it is also known as ridge regression. Because Tikhonov regularization can smooth the solutions, it is often used as a smoothness constraint.\nGiven a linear inverse problem,\ny = Ax (4)\nFig. 1. The generic L-curve with positive curvature; a is the L-corner.\nwhere x ∈ RM denotes unknown vector to be estimated, y ∈ RN denotes observation data, and A ∈ RN×M denotes distortion matrix. The classical approach is to use standard LS approach to estimate x,\nx = argmin x\n‖y −Ax‖2F . (5)\nTo improve the solution, usually Tikhonov regularized LS is used instead [32]:\nxλ = argmin x\n‖y −Ax‖2F + λ‖x‖ 2 F (6)\nwhere λ denotes nonnegative regularization parameter, ‖y − Ax‖2F denotes approximation error, and ‖x‖ 2 F denotes solution size. In [34], Oraintara et al. proposed an iterative algorithm to compute λ based on the L-curve criterion depicted in fig. 1. In summary, x and λ can be computed using algorithm 1:\nAlgorithm 1 Iterative algorithm for computing x and λ.\nInitialize x(0) and λ(0). Set k ← 0 repeat\nk ← k + 1\nx(k) ← argmin x ‖y −Ax‖2F + λ (k−1)‖x‖2F (7) λ(k) ← |γ| ‖y−Ax(k)‖2F\n‖x(k)‖2F (8)\nerror ← ‖x(k) − x(k−1)‖\nx(k−1)\nuntil error ≤ ǫ\nwhere γ is the slope of the straight line that is tangent to the Lcurve and ǫ is a small nonnegative number that is set to 0.001 in the authors’ work [34]. Note that the value of γ doesn’t influence convergence property of sequence x(k) and λ(k), and as long as λ(0) is sufficiently small then λ(k) converges to a stationary point [34]. The similar method for computing λ can also be found in ref. [33], but the authors fixed γ value to one.\nWe will now derive formulation for Tikhonov regularized NMF. The NMF problem in eq. 2 is known to be nonconvex\n4 and may have several local mimima [25]. The common practice to deal with the nonconvexity of an optimization problem is by transforming it into convex subproblems [41]. In the case of the NMF, this can be done by employing the alternating strategy; fixing one matrix while solving for the other [25] (apparently, all NMF algorithms utilizing alternating strategy). This strategy transforms an NMF problem into a pair of convex subproblems. The following equations give convex subproblems of the NMF,\nB = argmin B≥0\n1 2 ‖A−BC‖2F (9)\nC = argmin C≥0\n1 2 ‖A−BC‖2F . (10)\nAlternatingly solving eq. 9 for B and eq. 10 for C is known as alternating nonnegativity-constrained LS (ANLS), and usually each of these subproblems is solved by decomposing it into a series of corresponding nonnegativity-constraint LS (NNLS) problems. The following equations are the NNLS versions of eq. 9 and eq. 10.\nbTm = arg min bm≥0\n1 2 ‖aTm −C T bTm‖ 2 F , ∀m (11)\ncn = arg min cn≥0\n1 2 ‖an −Bcn‖ 2 F , ∀n, (12)\nwhere bm and am denotes the m-th row of B and A respectively.\nAs shown, each of these NNLS problems is exactly the standard LS problem with additional nonnegativity constraint. Accordingly, Tikhonov regularization can be employed to improve the solutions. The following equations give Tikhonov regularized version of the above NNLS problems.\nbTm = arg min bm≥0\n1 2 ‖aTm −C T bTm‖ 2 F + 1 2 βm‖b T m‖ 2 F ∀m, (13)\ncn = arg min cn≥0\n1 2 ‖an −Bcn‖ 2 F + 1 2 αn‖cn‖ 2 F , ∀n, (14)\nwhere αn and βm denotes the corresponding nonnegative regularization parameters. By rearranging rows of B and columns of C back, Tikhonov regularized NMF can be written as:\nB = argmin B≥0\n1 2 ‖A−BC‖2F + 1 2 ‖ √ βB‖2F , (15)\nC = argmin C≥0\n1 2 ‖A−BC‖2F + 1 2 ‖C √ α‖2F , (16)\nwhere β = diag(β1, . . . , βM ) and α = diag(α1, . . . , αN ). The following gives a generic algorithm for Tikhonov regularized NMF where update rules for αn and βm are derived based on the work of Oraintara et al. [34], γBm and γ C n are defined similarly as in algorithm 1, and ǫ denotes small positive number.\nIII. A CONVERGED ALGORITHM FOR TIKHONOV REGULARIZED NMF\nWe will now present a converged algorithm for Tikhonov regularized NMF based on additive update rules. By combining update rules for B and C in eq. 15 and eq. 16, we define\nAlgorithm 2 A generic algorithm for Tikhonov regularized NMF.\nInitialize B(0), C(0), α(0), and β(0). Set k ← 0 repeat\nB(k+1) ← argmin B≥0\n1 2 ‖A−BC(k)‖2F + 1 2 ‖ √ β (k) B‖2F\nC(k+1) ← arg min C≥0\n1 2 ‖A−B(k+1)C‖2F + 1 2 ‖C √ α (k) ‖2F\nβ(k+1)m ← |γ B m|\n‖aTm −C (k)T b (k+1)T m ‖2F\n‖b (k+1)T m ‖2F\n, ∀m\nα(k+1)n ← |γ C n |\n‖an −B (k+1)c (k+1) n ‖2F\n‖c (k+1) n ‖2F\n, ∀n\nk ← k + 1 until\nmax ( ∇BJ ( B ) ⊙B ) ≤ ǫ &max ( ∇CJ ( C ) ⊙C ) ≤ ǫ\nobjective function for Tikhonov regularized NMF as follows:\nmin B,C\nJ(B,C) = 1\n2 ‖A−BC‖2F +\n1 2 ‖ √ βB‖2F + 1 2 ‖C √ α‖2F\n(17)\ns.t. B ≥ 0,C ≥ 0.\nThe Karush-Kuhn-Tucker (KKT) function of the objective can be written as:\nL(B,C) = J(B,C)− tr ( ΓBB T ) − tr ( ΓCC ) .\nwhere ΓB ∈ RM×R and ΓC ∈ RN×R denotes the KKT multipliers. Partial derivatives of L with respect to B and C are:\n∇BL(B) = ∇BJ(B)− ΓB, ∇CL(C) = ∇CJ(C)− Γ T C,\nwith\n∇BJ(B) = BCC T −ACT + βB, ∇CJ(C) = B TBC−BTA+Cα.\nBy results from optimization studies, (B∗,C∗) is a stationary point of eq. 17 if it satisfies the KKT optimality conditions [42], i.e.,\nB∗ ≥ 0, C∗ ≥ 0, (18)\n∇BJ(B ∗) = ΓB ≥ 0, ∇CJ(C ∗) = ΓTC ≥ 0, (19)\n∇BJ(B ∗)⊙B∗ = 0, ∇CJ(C ∗)⊙C∗ = 0, (20)\nwhere ⊙ denotes Hadamard products (component-wise multiplications), and eq. 20 is known as the complementary slackness.\n5 Multiplicative update rules based algorithm for Tikhonov regularized NMF can be derived by utilizing the complementary slackness:\n( BCCT −ACT + βB )\n⊙B = 0, (\nBTBC−BTA+Cα ) ⊙C = 0.\nThese equations lead to the following update rules:\nbmr ←− bmr\n( ACT )\nmr (\nBCCT + βB )\nmr\n∀m, r (21)\ncrn ←− crn\n( BTA )\nrn (\nBTBC+Cα )\nrn\n∀r, n (22)\nwhere bmr and crn denote (m, r) entry of B and (r, n) entry of C respectively.\nAs stated by Lin [18], the above multiplicative update rules based algorithm can be modified into an equivalent converged algorithm by (1) using additive update rules, and (2) replacing zero entries that do not satisfy the KKT conditions with a small positive number to escape the zero locking.\nThe additive update rules version of the algorithm can be written as:\nbmr ←− bmr − bmr (\nBCCT + βB )\nmr\n∇BJ(B)mr,\ncrn ←− crn − crn (\nBTBC+Cα )\nrn\n∇CJ(C)rn.\nBy inspection it is clear that this algorithm also suffers from the zero locking, i.e.:\n∇BJ(B)mr < 0 & bmr = 0, or\n∇CJ(C)rn < 0 & crn = 0,\nsuch that when these conditions happened, the algorithm can no longer update the corresponding entries even though those entries haven’t satisfied the KKT optimality conditions in eq. 19.\nAlgorithm 3 gives necessary modifications to avoid zero locking and—as will be shown later—also has convergence guarantee, where ⊘ denotes component-wise division,\nb̄(k)mr ≡\n{\nb (k) mr if ∇BJ ( B(k),C(k) )\nmr ≥ 0\nmax(b (k) mr, σ) if ∇BJ ( B(k+1),C(k) )\nmr < 0\n,\n(23)\nc̄(k)rn ≡\n{\nc (k) rn if ∇CJ ( B(k+1),C(k) )\nrn ≥ 0\nmax(c (k) rn , σ) if ∇CJ ( B(k+1),C(k) )\nrn < 0\n,\n(24)\ndenote the modifications to avoid the zero locking with σ is a small positive number, δB and δC denote small positive numbers that introduced to avoid division by zeros, B̄ and C̄ denote matrices that contain b̄mr and c̄rn respectively, and\n∇BJ(B (k),C(k)) = B(k)C(k)C(k)T −AC(k)T\n+ β(k)B(k),\n∇CJ(B (k+1),C(k)) = B(k+1)TB(k+1)C(k) −B(k+1)TA\n+C(k)α(k).\nAlgorithm 3 A converged algorithm for Tikhonov regularized NMF.\nInitialization B(0) ≥ 0, C(0) ≥ 0, β(0)m ≥ 0 ∀m, and α (0) n ≥ 0 ∀n. k ← 0 repeat\nB(k+1) ← B(k) − B̄(k) ⊙∇BJ(B (k),C(k))⊘\n( B̄(k)C(k)C(k)T + β(k)B̄(k) + δB )\n(25)\nC(k+1) ← C(k) − C̄(k) ⊙∇CJ(B (k+1),C(k))⊘\n( B(k+1)TB(k+1)C̄(k) + C̄(k)α(k) + δC ) (26)\nβ(k+1)m ← |γ B m|\n‖aTm −C (k)T b (k+1)T m ‖2F\n‖b (k+1)T m ‖2F + δB\n, ∀m (27)\nα(k+1)n ← |γ C n |\n‖an −B (k+1)c (k+1) n ‖2F\n‖c (k+1) n ‖2F + δC\n, ∀n (28)\nk ← k + 1 until\nmax ( ∇BJ ( B ) ⊙B ) ≤ ǫ &max ( ∇CJ ( C ) ⊙C ) ≤ ǫ\nwhere 0 < step < 1. Note that since algorithm 3 is free from the zero locking, B and C can be initialized using nonnegative matrices. Theorem 1 explains this formally.\nTheorem 1: If B0 > 0 and C0 > 0, then Bk > 0 and Ck > 0, ∀k ≥ 0. And if B0 ≥ 0 and C0 ≥ 0, then Bk ≥ 0 and Ck ≥ 0, ∀k ≥ 0.\nProof: This statement is clear for k = 0, so we need only to prove for k > 0. Case 1: ∇BJmr ≥ 0 ⇒ b̄mr = bmr (see b̄mr definition in eq. 23).\nb(k+1)mr =\n( B(k)C(k)C(k)T + β(k)B(k) ) mr b (k) mr + δBb k mr\n( B(k)C(k)C(k)T + β(k)B(k) )\nmr + δB\n−\n( B(k)C(k)C(k)T −AC(k)T + β(k)B(k) ) mr b (k) mr\n( B(k)C(k)C(k)T + β(k)B(k) )\nmr + δB\n=\n(\nδB +AC (k)T\n) mr b (k) mr\n( B(k)C(k)C(k)T + β(k)B(k) )\nmr + δB\n.\nThus ∀k > 0, b(k)mr > 0 ⇒ b (k+1) mr > 0 ∀m, r, and b (k) mr ≥ 0 ⇒ b (k+1) mr ≥ 0 ∀m, r. Case 2: ∇BJmr < 0 ⇒ b̄mr 6= bmr.\nb(k+1)mr = b (k) mr −\nmax ( b (k) mr, σ ) ∇BJ ( B(k),C(k) )\nmr (\nB̄(k)C(k)C(k)T + β(k)B̄(k) )\nmr + δB\n.\nBecause max ( b (k) mr, σ ) > 0 and ∇BJ ( B(k),C(k) )\nmr < 0,\nthen ∀k > 0, b(k)mr > 0 ⇒ b (k+1) mr > 0 ∀m, r, and b (k) mr ≥ 0 ⇒ b (k+1) mr > 0 ∀m, r.\n6 Case 3: ∇CJrn ≥ 0 ⇒ c̄rn = crn.\nc(k+1)rn =\n( B(k+1)TB(k+1)C(k) +C(k)α(k) ) rn c (k) rn + δCc (k) rn\n( B(k+1)TB(k+1)C(k) +C(k)α(k) )\nrn + δC\n−\n( B(k+1)TB(k+1)C(k) −B(k+1)TA+C(k)α(k) ) rn c (k) rn\n( B(k+1)TB(k+1)C(k) +C(k)α(k) )\nrn + δC\n=\n(\nδC +B (k+1)TA\n) rn c (k) rn\n( B(k+1)TB(k+1)C(k) +C(k)α(k) )\nrn + δC\n,\nThus ∀k > 0, ckrn > 0 ⇒ c (k+1) rn > 0 ∀r, n, and ckrn ≥ 0 ⇒ c (k+1) rn ≥ 0 ∀r, n. Case 4: ∇CJrn < 0 ⇒ c̄rn 6= crn.\nc(k+1)rn = c (k) rn −\nmax ( c (k) rn , σ ) ∇CJ ( B(k+1),C(k) )\nrn (\nB(k+1)TB(k+1)C̄(k) + C̄(k)α(k) )\nrn + δC\n.\nBecause max ( c (k) rn , σ ) > 0 and ∇CJ ( B(k+1),C(k) )\nrn < 0,\nthen ∀k > 0, c(k)rn > 0 ⇒ c (k+1) rn > 0 ∀r, n, and c (k) rn ≥ 0 ⇒ c (k+1) rn > 0 ∀r, n.\nBy combining results for k = 0 and k > 0 in case 1-4, the proof is completed.\nAppendix A gives Matlab/Octave codes for implementing algorithm 3.\nIV. CONVERGENCE ANALYSIS\nThere are two type of update rules in algorithm 3. The first is the update rules for B(k) and C(k), and the second is the update rules for β(k) and α(k). Since the algorithm 3 uses alternating strategy in updating these variables, the convergence analysis can be carried out separately. This approach is known as the block-coordinate descent method [42].\nTo derive the convergence guarantee of solution sequence {\nB(k),C(k),β(k),α(k) }\n, we will first show the convergence of sequence { B(k),C(k) } , and then sequence { β(k),α(k) }\n. From convergence analysis study, the following conditions must be satisfied for sequence { B(k),C(k) }\nto have convergence guarantee [18], [25], [44].\n1) The nonincreasing property of sequence J ( B(k),C(k) )\n, i.e.,\na) J ( B(k+1) ) ≤ J ( B(k) )\nand b) J ( C(k+1) ) ≤ J ( C(k) ) .\n2) Any limit point of sequence { B(k),C(k) }\ngenerated by algorithm 3 is a stationary point.\n3) Sequence { B(k),C(k) } has at least one limit point.\nA. The nonincreasing property of sequence J ( B(k) )\nWe will utilize the auxiliary function approach introduced in [1] to prove this property. By using the auxiliary function as an intermediate function, the nonincreasing property of J ( B(k) ) can be restated with:\nJ ( B(k+1) ) = G ( B(k+1),B(k+1) ) ≤ G ( B(k+1),B(k) )\n≤ G ( B(k),B(k) ) = J ( B(k) ) .\nTo define G, let’s rearrange B into:\nBT ≡\n\n    \nbT1\nbT2\n. . .\nbTM\n\n    \nwhere bm denotes the m-th row of B. And also let’s define:\n∇BT J ( BkT ) ≡\n\n   \n∇BJ ( B(k) )T\n1\n. . .\n∇BJ ( B(k) )T\nM\n\n   \nwhere ∇BJ ( B(k) ) m denotes the m-th row of ∇BJ(B(k)) = B(k)C(k)C(k)T −AC(k)T + β(k)B(k). Then define,\nD ≡ diag ( D1, . . . ,DM )\nwhere Dm denotes a diagonal matrix with its diagonal entries defined as:\ndmrr ≡\n\n\n\n( B̄(k)C(k)C(k)T +β(k)B̄(k) )\nmr +δB\nb̄ (k) mr\nif r ∈ Im\n⋆ if r /∈ Im\nwith\nIm ≡ { r|b(k)mr > 0, ∇BJ ( B(k) ) mr 6= 0, or\nb(k)mr = 0, ∇BJ ( B(k) ) mr < 0 }\ndenotes the set of non-KKT indices in m-th row of B(k), and ⋆ is defined so that ⋆ ≡ 0 and ⋆−1 ≡ 0.\nThe auxiliary function G can be defined as:\nG ( BT ,B(k)T ) ≡ J ( B(k)T ) + tr {( B−B(k) ) ∇BT J ( B(k)T )}\n+ 1\n2 tr\n{( B−B(k) ) D ( B−B(k) )T} .\n(29)\nNote that J and G are equivalent to J and G with B is rearranged into BT , and other variables are reordered accordingly. And also whenever X(k+1) is a variable, we remove (k + 1) sign. And:\n∇BTG ( B T ,B(k)T ) = D ( B−B(k) )T +∇BT J ( B (k)T ) .\nBy definition D is positive definite for all B(k) not satisfy the KKT conditions and positive semidefinite if and only if B(k) satisfies the KKT conditions. Thus G ( BT ,B(k)T )\nis a strict convex function, and consequently has a unique minimum, so that:\nD ( B−B(k) )T +∇BT J ( B(k)T ) = 0, (30)\nBT = B(k)T −D−1∇BT J ( B(k)T ) ,\nwhich is exactly the update rule for B in eq. 25. Lemma 1: J ( BT ) can be rewritten as:\nJ ( BT ) = J ( B(k)T ) + tr {( B−B(k) ) ∇BT J ( B(k)T )}\n+ 1\n2 tr\n{( B−B(k) ) ∇2BJ ( B(k) )( B−B(k) )T} .\n(31)\n7 where\n∇2BJ ( Bk ) ≡\n\n  \nC(k)C(k)T + β1I\n. . .\nC(k)C(k)T + βMI\n\n  \nand I denotes corresponding compatible identity matrix. Proof: Let decompose J ( B ) into:\nJ ( B )\nm =\n1 2 ‖aTm −C (k)T bTm‖ 2 F + 1 2 βm‖b T m‖ 2 F ∀m,\nso that J ( B ) = J ( B )\n1 . . . J\n( B )\nM . Then,\n∂J ( B )\nm\n∂bm = −amC\n(k)T + bmC (k)C(k)T + βmbm\nand ∂2J ( B )\nm ∂b2m = C(k)C(k)T + βmI.\nBy using the Taylor series expansion, J ( B )\nm can be rewritten\nas:\nJ ( B )\nm = J\n( B(k) ) m + ( bm − b (k) m )\n(\n∂J ( B )\nm\n∂bm\n)T\n+ 1\n2\n(\nbm − b (k) m\n)\n(\n∂2J ( B )\nm\n∂b2m\n)\n(\nbm − b (k) m )T ,\nwhich is the m-th row of J ( BT )\n. To prove the nonincreasing property of J ( B(k) )\n, the following statements must be shown:\n1) G ( BT ,BT ) = J ( BT )\n, 2) G ( BkT ,BkT ) = J ( BkT )\n, 3) G ( BT ,BT ) ≤ G ( BT ,BkT )\n, and 4) G ( BT ,BkT ) ≤ G ( BkT ,BkT ) .\nThe first and second will be proven in theorem 2, the third in theorem 3, and the fourth in theorem 4.\nTheorem 2: G ( BT ,BT ) = J ( BT ) and G ( B(k)T , B(k)T )\n= J ( B(k)T )\n. Proof: These are obvious from the definition of G in\neq. 29. Theorem 3: G ( BT ,BT ) ≤ G ( BT ,B(k)T )\n. Moreover if and only if Bk satisfies the KKT conditions in eq. 18–20, then G ( BT ,BT ) = G ( BT ,B(k)T )\n. Proof:\nG ( BT ,B(k)T ) −G ( BT ,BT ) =\n1 2 tr { ( B−B(k) ) ( D−∇2BJ ( B(k) ) ) ( B−B(k) )T } =\n1\n2\nM ∑\nm=1\n[\n(\nbm − b (k) m\n)\n(\nDm − ∂2J\n( B )\nm\n∂b2m\n)\n(\nbm − b (k) m\n)T\n]\nIf Dm − ∂2J\n( B )\nm\n∂b2 m\n∀m are all positive definite, then the\ninequality always holds except when bm = bkm ∀m, where based on theorem 1 and update rule eq. 25 happened if and only if the point has reach a stationary point—a point where the KKT conditions are satisfied. Thus, it is sufficient to prove the positive definiteness of Dm −∇2BJ ( Bk ) m ∀m.\nLet vTm = bm − b (k) m 6= 0, then we must prove:\nvTm\n(\nDm − ∂2J\n( B )\nm\n∂b2m\n)\nvm > 0.\nNote that\ndmrr ≡\n\n\n\n(\nb̄(k) m X(k) m\n)\nr +δB\nb̄ (k) mr\nif r ∈ Im\n⋆ if r /∈ Im\nwhere b̄(k)m denotes the m-th row of B̄(k); and X (k) m = C(k)C(k)T + βmI and Dm are both symmetric matrix. Thus,\nvTm\n(\nDm − ∂2J\n( B )\nm\n∂b2m\n)\nvm =\nR ∑\nr=1\nv2r δB\nb̄ (k) mr\n+ R ∑\nr=1\nv2r\n( X(k)b̄ (k)T m )\nr\nb̄ (k) mr\n− R ∑\nr,s=1\nvrvsx (k) rs >\nR ∑\nr=1\nv2r\n∑R s=1 x (k) rs ( b̄ (k) m )\ns\nb̄ (k) mr\n−\nR ∑\nr=1\nR ∑\ns=1\nvrvsx (k) rs =\n1\n2\nR ∑\nr=1\nR ∑\ns=1\nv2r x (k) rs ( b̄ (k) m ) s\nb̄ (k) mr\n+ 1\n2\nR ∑\ns=1\nR ∑\nr=1\nv2s x (k) sr ( b̄ (k) m ) r\nb̄ (k) ms\n−\nR ∑\nr=1\nR ∑\ns=1\nvrvsx (k) rs =\n1\n2\nR ∑\nr=1\nR ∑\ns=1\nx(k)rs\n\n\n√\nb̄ (k) ms b̄ (k) mr vr −\n√\nb̄ (k) mr b̄ (k) ms vs\n\n\n2\n≥ 0\nwhere vr denotes the r-th entry of vm and x (k) rs denotes the (r, s) entry of X(k). Therefore, Dm − ∂2J ( B ) m\n∂b2 m ∀m are all positive definite\nTheorem 4: G ( BT ,B(k)T ) ≤ G ( B(k)T ,B(k)T )\n. Moreover, if and only if B satisfies the KKT conditions, then G ( BT ,B(k)T ) = G ( B(k)T ,B(k)T )\n. Proof:\nG\n( B(k)T ,B(k)T ) −G ( BT ,B(k)T ) =\n− tr { ( B−B(k) ) ∇BT J ( B(k)T ) }\n− 1\n2 tr\n{\n( B−B(k) ) D ( B−B(k) )T\n}\n.\nBy using eq. 30, and the fact that D is positive semi-definite:\nG\n( B(k)T ,B(k)T ) −G ( BT ,B(k)T ) =\n1 2 tr { ( B−B(k) ) D ( B−B(k) )T } ≥ 0,\nwe proved that G ( BT ,B(k)T ) ≤ G ( B(k)T ,B(k)T )\n. Now, let’s prove the second part of the theorem. By the update rule eq. 25, if B(k) satisfies the KKT conditions, then B will be equal to B(k), and thus the equality holds. Now we need to prove that if the equality holds, then B(k) satisfies the KKT conditions. To prove this, let consider a contradiction where the equality holds but B(k) does not satisfy the KKT\n8 conditions. In this case, there exists at least an index (m, r) such that:\nbmr 6= b (k) mr and d m rr =\n(\nb̄ (k) m X (k) m\n)\nr + δB\nb̄ (k) mr\n≥ δB\nb̄ (k) mr\n.\nNote that by the definition in eq. 23, if b̄(k)mr = 0, then it satisfies the KKT conditions. Accordingly, bmr = b (k) mr which violates the condition for contradiction. So, b̄kmr cannot be equal to zero, and thus dmrr is well defined. Consequently,\nG\n( B(k)T ,B(k)T ) −G ( BT ,B(k)T ) ≥\n(\nbmr − b (k) mr\n)2\nδB\nb̄ (k) mr\n> 0,\nwhich violates the condition for contradiction. Thus, it is proven that if the equality holds, then B(k) satisfies the KKT conditions.\nThe following theorem summarizes the nonincreasing property of sequence J ( B(k) )\nTheorem 5: J ( B(k+1) ) ≤ J ( B(k) )\n∀k ≥ 0 under update rule eq. 25 with the equality happens if and only if B(k) satisfies the KKT optimality conditions in eq. 18-20. Proof: This is the results of theorem 2, 3, and 4.\nB. The nonincreasing property of sequence J ( C(k) )\nTheorem 6: J ( C(k+1) ) ≤ J ( C(k) )\n∀k ≥ 0 under update rule eq. 26 with the equality happens if and only if C(k) satisfies the KKT optimality conditions in eq. 18-20. Proof: This theorem can be proven similarly as in J ( B(k) ) case.\nC. The nonincreasing property of sequence J ( B(k),C(k) )\nTheorem 7: J ( B(k+1), C(k+1) ) ≤ J ( B(k+1), C(k) )\n≤ J ( B(k), C(k) )\n∀k ≥ 0 under update rule eq. 25 and 26 with the equality happens if and only if ( B(k),C(k) )\nsatisfies the KKT optimality conditions in eq. 18-20.\nProof: This statement can be proven by combining the results in theorem 5 and 6.\nBy this theorem, we have proven the first conditions for the algorithm 3 to have convergence guarantee. The next subsection will give proofs for the second and the third condition.\nD. Limit points of sequence { B(k),C(k) }\nTheorem 8: Any limit point of sequence { B(k),C(k) }\ngenerated by algorithm 3 is a stationary point\nProof: By theorem 7, algorithm 3 produces strictly decreasing sequence J ( B(k),C(k) )\nuntil reaching a point that satisfies the KKT conditions. By update rules in eq. 25 and 26, after a point satisfies the KKT conditions, the algorithm will stop updating ( B(k),C(k) )\n, i.e., B(k+1) = B(k) and C(k+1) = C(k) ∀k ≥ ∗, where ∗ is the iteration where the KKT conditions are satisfied.\nTheorem 9: Sequence { B(k),C(k) }\nhas at least one limit point.\nProof: As stated by Lin [18], it suffices to prove that {\nB(k),C(k) } is in a closed and bounded set. The boundedness\nof this sequence is clear by the objective eq. 17; if there exists l such that lim blmr → ∞ or lim c l rn → ∞, then lim J (\nB(l), C(l) ) → ∞ > J ( B(0), C(0) )\nwhich violates theorem 7. With nonnegativity guarantee from theorem 1, it is proven that { B(k),C(k) } is in closed and bounded set.\nE. The convergence of sequence { β(k),α(k) }\nThe convergence guarantee of this sequence has been established by Oraintara et al. in ref. [34], [43]. Here we will adopt their works and summarize the results in accord to our notations.\nTheorem 10 (Oraintara et al. [34]): The optimum βm corresponding to the L-corner must satisfy\nβm ∥ ∥bTm ∥ ∥ 2 F = ∣ ∣γBm ∣ ∣ ∥ ∥aTm −C T bTm ∥ ∥ 2 F .\nThe similar condition can also derived for αn. Since the update rules for β(k)m ∀m and α (k) n ∀n are derived from this theorem, it implies that the update rules find the optimal solutions of these parameters for each iteration.\nNext we state the monotonicity property of sequence β(k)m and α(k)m .\nLemma 2 (Oraintara et al. [34]): The values of β(k)m ∀m either strictly increase or decrease under update rule eq. 27 unless converged to a limit point. The similar conditions also apply to α(k)n ∀n.\nAnd, the following theorem summarizes the convergence property of sequence β(k)m and α (k) n .\nTheorem 11 (Oraintara et al. [34]): If the update rule eq. 27 converges, then it converges to the corresponding Lcorner. The same condition also applies to the update rule eq. 28\nThus, lemma 2 and theorem 11 state that while update rules eq. 27 and 28 generate monotonic updated values for β(k)m ∀m and α(k)n ∀n which if the sequences converge, then they converge to the corresponding L-corner; there is no guarantee that the sequences will converge. Fortunately, the convergence can be established by choosing appropriate initial values.\nDirectly from ref. [34], the followings summarize the strategy in choosing the initial values.\n1) If an is not in the range B, then choosing α (0) n = 0 will\nproduce an increasing sequence of α(k)n converging to the nearest L-corner. 2) If an is in the range of B, then it is always possible to choose α(0)n > 0 small enough so that J ( α (0) n )\nn > α\n(0) n\nand convergence occurs. 3) More generally, if limαn→∞ J ( αn ) n /αn < 1 and\nJ ( 0 ) > 0, any initial value α(0)n produces a converging\nsequence α(k)n . 4) When limαn→∞ J ( αn ) n /αn > 1, the only case when\nthe update rule eq. 28 will not generate a converged sequence is when α(0)n is chosen larger than the last intersection between J ( αn )\nn and the straight line αn =\nJ ( αn )\nn .\nThe same conditions can also be derived for sequence β (k) m ∀m. Thus, by simply choosing αn = 0 ∀n and βm = 0 ∀m, the update rules eq. 27 and 28 will generate sequence\n9 β (k) m ∀m and α (k) n ∀n that converge to the corresponding Lcorners.\nF. The convergence of the solution sequence\nThe following theorem gives the convergence guarantee of the solution sequence { B(k),C(k),β(k),α(k) }\ngenerated by algorithm 3\nTheorem 12: By choosing appropriate initial values β (0) m ∀m and α (0) n ∀n, algorithm 3 generates sequence {\nB(k),C(k) }\nthat converges to a point that satisfies the KKT optimality conditions (a stationary point), and sequence {\nβ(k),α(k) }\nthat converges to the corresponding L-corners. Proof: By the results of theorem 7, 8, and 9, we know that\nsequence { B(k),C(k) }\nconverges to a point that satisfies the KKT conditions. By discussion in subsection IV-E we know that it is always possible to choose appropriate initial values for β (0) m ∀m and α (0) n ∀n so that sequence { β (k),α(k) }\nconverges to the corresponding L-corners.\nV. DISCUSSION ON THE CONVERGENCE OF ALGORITHM 3\nAlgorithm 3 converges when it stops updating both sequence { B(k),C(k) } and sequence { β (k),α(k) }\n. And as shown in eq. 27 and eq. 28, when { B(k),C(k) }\nhas converged then { β(k),α(k) }\nwould also have converged. With the boundedness of B(k) and C(k), β(k) and α(k) will also be bounded. Thus, algorithm 3 will be well-behaved through the update steps. This is an important fact since even though sequence J ( B(k),C(k) )\nhas the nonincreasing property, due to lemma 2, algorithm 3 may produce nondecreasing J (\nB(k), C(k), β(k), α(k) ) (we will refer this sequence as J ( · )\n). This is because the nonincreasing property of J ( · )\nshould be shown by proofing that:\n1) J ( B(k+1) ) ≤ J ( B(k) )\n, 2) J ( C(k+1) ) ≤ J ( C(k) )\n, 3) J ( β(k+1) ) ≤ J ( β(k) )\n, and 4) J ( α(k+1) ) ≤ J ( α(k) ) .\nThe first and second have been proven in theorem 5 and 6 respectively. But as stated in lemma 2, sequence βm ∀m and αn ∀n can either strictly increase or decrease until reaching the limit points, thus J ( β(k+1) ) > J ( β(k) ) and/or J ( α(k+1) ) > J ( α(k) )\ncases can occur. This, however, will not affect the convergence of algorithm 3 since as long as sequence {\nβ(k),α(k) }\nconverges, then the update rules eq. 25 and 26 will eventually find the stationary point for { B(k),C(k) }\n. Numerically, it seems that γBm and γ C n play the key role in\ndetermining whether sequence { β (k) m } and { α (k) n }\nwill strictly increase or decrease with the big values lead to the increasing sequences and vice versa.\nVI. CONCLUSION\nWe have presented a converged algorithm for NMF with Tikhonov regularization on its factor. There are two contributions that can be pointed out. The first is to show the connection between Tikhonov regularized linear inverse problems with constraint NMF which naturally leads to a\nmechanisme for determining the regularization parameter in the NMF automatically. And, the second is the development of a converged algorithm for NMF with Tikhonov regularized constraints.\nAPPENDIX A OCTAVE/MATLAB CODES FOR ALGORITHM 3\nfunction [B, C, newalpha, newbeta, iteration, olderror, errordiff, maxNablaB, maxNablaC, resNorm, solNorm] = TikhonovNMF3(A, r, B0, C0, oldalpha, oldbeta, gammaB, gammaC, maxiter, tol)\n%The converged version of the algorithm %Use complementary slackness as stopping\ncriterion\nformat long; %%Check the input matrix if min(min(A)) < 0 error(’Input matrix cannot contain negative\nentries’); return\nend\n[m,n] = size(A);\n%%Check input arguments\nif ˜exist(’A’) error(’incorrect inputs!’) end if ˜exist(’r’) error(’incorrect inputs!’) end if ˜exist(’B0’) B0 = rand(m,r); end if ˜exist(’C0’) C0 = rand(r,n); end if ˜exist(’alpha0’) oldalpha = zeros(n,1); end if ˜exist(’beta0’) oldbeta = zeros(m,1); end if ˜exist(’gammaB’) gammaB = ones(m,1)*0.1; %small values lead to better convergence property end if ˜exist(’gammaC’) gammaC = ones(n,1)*0.1; %small values lead to better convergence property end if ˜exist(’maxiter’) maxiter = 1000; end if ˜exist(’tol’) tol = 1.0e-9; end\nB = B0; clear B0; C = C0; clear C0; newalpha = oldalpha; newbeta = oldbeta; trAtA = trace(A’*A);\n10\nolderror = zeros(maxiter+1,1);\nolderror(1) = 0.5*trAtA - trace(C’*(B’*A)) + 0.5*trace(C’*(B’*B*C)) + 0.5*trace(B’*diag(newbeta)*B) + 0.5*trace(C’*(C*diag(newalpha)));\nsigma = 1.0e-9; delta = sigma;\nfor iteration=1:maxiter CCt = C*C’; gradB = B*CCt-A*C’+diag(newbeta)*B; Bm = max(B,(gradB<0)*sigma); B = B - (Bm.*gradB./(Bm*CCt +\ndiag(newbeta)*Bm + delta));\nBtB = B’*B; gradC = BtB*C-B’*A+C*diag(newalpha); Cm = max(C,(gradC<0)*sigma); C = C - (Cm.*gradC./(BtB*Cm +\nCm*diag(newalpha) + delta));\nfor i = 1:m newbeta(i) = gammaB(i)*norm((A(i,:) -\nB(i,:)*C),’fro’)ˆ2/(norm(B(i,:),’fro’)ˆ2 + delta);\nend\nfor i = 1:n newalpha(i) = gammaC(i)*norm((A(:,i) -\nB*C(:,i)),’fro’)ˆ2/(norm(C(:,i),’fro’)ˆ2 + delta);\nend\nnewerror = 0.5*trAtA - trace(C’*(B’*A)) + 0.5*trace(C’*(B’*B*C)) + 0.5*trace(B’*diag(newbeta)*B) + 0.5*trace(C’*(C*diag(newalpha)));\nerrordiff = abs(newerror - olderror(iteration)); olderror(iteration+1) = newerror; NablaB = (B*C*C’ - A*C’ +\ndiag(newbeta)*B).*B; NablaC = (B’*B*C - B’*A +\nC*diag(newalpha)).*C;\nmaxNablaB = max(max(abs(NablaB))); maxNablaC = max(max(abs(NablaC))); if(maxNablaB < tol && maxNablaC < tol) break; end\nend resNorm = norm((A-B*C),’fro’)ˆ2; solNorm(1) = norm(B,’fro’)ˆ2; solNorm(2) = norm(C, ’fro’)ˆ2;\nREFERENCES\n[1] D. Lee and H. Seung, “Algorithms for non-negative matrix factorization,” Proc. Advances in Neural Processing Information Systems, pp. 556-62, 2000. [2] I.S. Dhillon and S. Sra, “Generalized nonnegative matrix approximation with Bregman divergences,” UTCS Technical Reports, The University of Texas at Austin, 2005. [3] A. Cichocki, R. Zdunek, and S. Amari, “Csiszárs divergences for nonnegative matrix factorization: Family of new algorithms,” Proc. 6th Int’l\nConf. on Independent Component Analysis and Blind Signal Separation, 2006. [4] P. Paatero and U. Tapper,“Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values,” Environmetrics, Vol. 5, pp. 111-26, 1994. [5] P. Anttila, P. Paatero, and U. Tapper, “Source identification of bulk wet deposition in finland by positive matrix factorization,” Atmospheric Environment, Vol. 29, No. 14, pp. 1705-18, 1995. [6] D. Lee and H. Seung, “Learning the parts of objects by non-negative matrix factorization,” Nature, 401(6755), pp. 788-91, 1999. [7] P.O. Hoyer, “Non-negative matrix factorization with sparseness constraints,” The Journal of Machine Learning Research, Vol. 5, pp. 1457-69, 2004. [8] S.Z. Li, X.W. Hou, H.J. Zhang, and Q.S. Cheng, “Learning spatially localized, parts-based representation,” Proc. IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition, pp. 207-12, 2001. [9] M. Berry, M. Brown, A. Langville, P. Pauca, and R.J. Plemmons, “Algorithms and applications for approximate nonnegative matrix factorization,” Computational Statistics and Data Analysis, 2006. [10] F. Shahnaz, M.W. Berry, V. Pauca, and R.J. Plemmons, “Document clustering using nonnegative matrix factorization,” Information Processing & Management, Vol. 42, No. 2, pp. 373-86, 2006. [11] W. Xu, X. Liu and Y. Gong, “Document clustering based on nonnegative matrix factorization,” Proc. ACM SIGIR, pp. 267-73, 2003. [12] V.P. Pauca, J. Piper, and R.J. Plemmons, “Nonnegative matrix factorization for spectral data analysis,” Linear Algebra and Its Applications, Vol. 416, No. 1, pp. 29-47, 2006. [13] S. Jia and Y. Qian, “Constrained Nonnegative Matrix factorization for hyperspectral unmixing,” IEEE Transactions on Geoscience and Remote Sensing, Vol. 47, No. 1, pp. 161-73, 2009. [14] A. Cichocki, S. Amari, R. Zdunek, R. Kompass, G. Hori, and Z. He, “Extended SMART algorithms for non-negative matrix factorization,” Lecture Notes in Computer Science, Vol. 4029, pp. 548-62, 2006. [15] J.P. Brunet, P. Tamayo, T.R. Golub, and J.P. Mesirov, “Metagenes and molecular pattern discovery using matrix factorization,” Proc. Natl Acad. Sci. USA, Vol. 101, No. 12, pp. 4164-9, 2003. [16] Y. Gao and G. Church, “Improving Molecular cancer class discovery through sparse non-negative matrix factorization,” Bioinformatics, Vol. 21, No. 21, pp. 3970-5, 2005. [17] H. Kim and H. Park, “Sparse non-negative matrix factorizations via alternating non-negativity constrained least squares for microarray data analysis,” Bioinformatics, Vol. 23, No. 12, pp. 1495-502, 2007. [18] C.J. Lin, “On the convergence of multiplicative update algorithms for nonnegative matrix factorization,” IEEE Transactions on Neural Networks, Vol. 18, No. 6, pp. 1589-96, 2007. [19] A. Mirzal, Nonnegative Matrix Factorizations for Clustering and LSI, LAP Lambert Academic Publishing, 2011, chapter 4. [20] C. Ding, T. Li, W. Peng, and H. Park, “Orthogonal nonnegative matrix t-factorizations for clustering,” Proc. 12th ACM SIGKDD Int’l Conf. on Knowledge Discovery and Data Mining, pp. 126-35, 2006. [21] J. Yoo and S. Choi, “Orthogonal nonnegative matrix factorization: Multiplicative updates on Stiefel manifolds,” Proc. 9th Int’l Conf. Intelligent Data Engineering and Automated Learning, pp. 140-7, 2008. [22] J. Yoo and S. Choi, “Orthogonal nonnegative matrix tri-factorization for co-clustering: Multiplicative updates on Stiefel manifolds,” Information Processing & Management, Vol. 46, No. 5, pp. 559-70, 2010. [23] S. Choi, “Algorithms for orthogonal nonnegative matrix factorization,” Proc. IEEE Int’l Joint Conf. on Neural Networks, pp. 1828-32, 2008. [24] H. Li, T. Adali, W. Wang, and D. Emge, “Non-Negative Matrix Factorization with Orthogonality Constraints for Chemical Agent Detection in Raman Spectra,” Proc. IEEE Workshop on Machine Learning for Signal Processing, pp. 253-8, 2005. [25] C.J. Lin, “Projected gradient methods for non-negative matrix factorization,” Technical Report ISSTECH-95-013, Department of CS, National Taiwan University, 2005. [26] E.F. Gonzales and Y. Zhang, “Accelerating the Lee-Seung algorithm for non-negative matrix factorization,” Technical Report, Dept. Comput. Appl. Math., Rice Univ., Houston, 2005. [27] D. Kim, S. Sra, and I.S. Dhillon, “Fast projection-based methods for the least squares nonnegative matrix approximation problem,” Stat. Anal. Data Min., Vol. 1, No. 1, pp. 38-51, 2008. [28] D. Kim, S. Sra, I.S. Dhillon, “Fast newton-type methods for the least squares nonnegative matrix approximation problem,” Proc. SIAM Conference on Data Mining, pp. 343-54, 2007. [29] H. Kim and H. Park, “Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method,” SIAM. J. Matrix Anal. & Appl., Vol. 30, No. 2, pp. 713-30, 2008.\n11\n[30] J. Kim and H. Park, “Toward faster nonnegative matrix factorization: A new algorithm and comparisons,” Proc. 8th IEEE International Conference on Data Mining, pp. 353-62, 2008. [31] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. Royal Statistical Society B, Vol. 58, Issue 1, pp. 267-88, 1996. [32] P.C. Hansen, The L-curve and its use in the numerical treatment of inverse problems, Computational Inverse Problems in Electrocardiology, WIT Press, 2000, pp. 119-142. [33] P.R. Johnston and R.M. Gulrajani, “A new method for regularization parameter determination in the inverse problem of electrocardiography,” IEEE Transaction on Biomedical Engineering, Vol. 44, No. 1, pp. 19-39, 1997. [34] S. Oraintara, W.C. Karl, D.A. Castanon, and T.Q. Nguyen, “A method for choosing the regularization parameter in generalized tikhonov regularized linear inverse problems,” Proc. International Conference of Image Processing, pp. 93-96, 2000. [35] S.M. Tan and C. Fox, “Regularization methods for linear inverse problems,” Lecture Note: PHYSICS 707 Inverse Problems, Chapter 3, pp. 1-15, 1998. [36] K. Kunisch and J. Zhou, “Iterative choices of regularization parameters in linear inverse problems,” Inverse Problems, Vol. 14, No. 5, pp. 1247- 64, 1998. [37] M. Belge, E. Miller, and M. Kilmer, “Simultaneous multiple regulariza-\ntion parameter selection by means of the L-hypersurface with applications to linear inverse problems posed in the wavelet transform domain,” SPIE Int’l Symposium on Optical Science, Engineering, and Instrumentation: Bayesian Inference for Inverse Problems, pp. 328-36, 1998. [38] T. Reginska, “A regularization parameter in discrete ill-posed problems,” SIAM Journal of Scientific Computing, Vol. 17, No. 3, pp. 740-9, 1996. [39] D.L. Phillips, “A technique for the numerical solution of certain integral equations of the first kind,” J. Association for Computing Machinery, Vol. 9, Issue 1, pp. 84-97, 1997. [40] A.N. Tikhonov, “Solution of incorrectly formulated problems and the regularization method,” Soviet Math. Dokl. 4, pp. 1035-8, 1963. English translation of Dokl. Akad. Nauk. SSSR, 151, pp. 501-4, 1963. [41] H. Hindi, “A tutorial on convex optimization,” Proc. American Control Conference, pp. 3252-65, 2004. [42] D.P. Bertsekas, “Nonlinear Programming 2nd Ed.,” Athena Scientific, 1999. [43] S. Oraintara, “Regular Linear Phase Perfect Reconstruction Filter Banks for Image Compression,” PhD Thesis, Boston University, 2000, Appendix A. [44] L. Grippo and M. Sciandrone, “On the convergence of the block nonlinear Gauss-Seidel method under convex constraints,” Operation Research Letters, Vol. 26, pp. 127-36, 2000."
    } ],
    "references" : [ {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "D. Lee", "H. Seung" ],
      "venue" : "Proc. Advances in Neural Processing Information Systems, pp. 556-62, 2000.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Generalized nonnegative matrix approximation with Bregman divergences",
      "author" : [ "I.S. Dhillon", "S. Sra" ],
      "venue" : "UTCS Technical Reports, The University of Texas at Austin, 2005.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Csiszárs divergences for nonnegative matrix factorization: Family of new algorithms",
      "author" : [ "A. Cichocki", "R. Zdunek", "S. Amari" ],
      "venue" : "Proc. 6th Int’l  Conf. on Independent Component Analysis and Blind Signal Separation, 2006.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Tapper,“Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values,",
      "author" : [ "U.P. Paatero" ],
      "venue" : "Environmetrics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1994
    }, {
      "title" : "Source identification of bulk wet deposition in finland by positive matrix factorization",
      "author" : [ "P. Anttila", "P. Paatero", "U. Tapper" ],
      "venue" : "Atmospheric Environment, Vol. 29, No. 14, pp. 1705-18, 1995.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "D. Lee", "H. Seung" ],
      "venue" : "Nature, 401(6755), pp. 788-91, 1999.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Non-negative matrix factorization with sparseness constraints",
      "author" : [ "P.O. Hoyer" ],
      "venue" : "The Journal of Machine Learning Research, Vol. 5, pp. 1457-69, 2004.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning spatially localized, parts-based representation",
      "author" : [ "S.Z. Li", "X.W. Hou", "H.J. Zhang", "Q.S. Cheng" ],
      "venue" : "Proc. IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition, pp. 207-12, 2001.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Algorithms and applications for approximate nonnegative matrix factorization",
      "author" : [ "M. Berry", "M. Brown", "A. Langville", "P. Pauca", "R.J. Plemmons" ],
      "venue" : "Computational Statistics and Data Analysis, 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Document clustering using nonnegative matrix factorization",
      "author" : [ "F. Shahnaz", "M.W. Berry", "V. Pauca", "R.J. Plemmons" ],
      "venue" : "Information Processing & Management, Vol. 42, No. 2, pp. 373-86, 2006.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Document clustering based on nonnegative matrix factorization",
      "author" : [ "W. Xu", "X. Liu", "Y. Gong" ],
      "venue" : "Proc. ACM SIGIR, pp. 267-73, 2003.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Nonnegative matrix factorization for spectral data analysis",
      "author" : [ "V.P. Pauca", "J. Piper", "R.J. Plemmons" ],
      "venue" : "Linear Algebra and Its Applications, Vol. 416, No. 1, pp. 29-47, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Constrained Nonnegative Matrix factorization for hyperspectral unmixing",
      "author" : [ "S. Jia", "Y. Qian" ],
      "venue" : "IEEE Transactions on Geoscience and Remote Sensing, Vol. 47, No. 1, pp. 161-73, 2009.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Extended SMART algorithms for non-negative matrix factorization",
      "author" : [ "A. Cichocki", "S. Amari", "R. Zdunek", "R. Kompass", "G. Hori", "Z. He" ],
      "venue" : "Lecture Notes in Computer Science, Vol. 4029, pp. 548-62, 2006.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Metagenes and molecular pattern discovery using matrix factorization",
      "author" : [ "J.P. Brunet", "P. Tamayo", "T.R. Golub", "J.P. Mesirov" ],
      "venue" : "Proc. Natl Acad. Sci. USA, Vol. 101, No. 12, pp. 4164-9, 2003.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Improving Molecular cancer class discovery through sparse non-negative matrix factorization",
      "author" : [ "Y. Gao", "G. Church" ],
      "venue" : "Bioinformatics, Vol. 21, No. 21, pp. 3970-5, 2005.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Sparse non-negative matrix factorizations via alternating non-negativity constrained least squares for microarray data analysis",
      "author" : [ "H. Kim", "H. Park" ],
      "venue" : "Bioinformatics, Vol. 23, No. 12, pp. 1495-502, 2007.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On the convergence of multiplicative update algorithms for nonnegative matrix factorization",
      "author" : [ "C.J. Lin" ],
      "venue" : "IEEE Transactions on Neural Networks, Vol. 18, No. 6, pp. 1589-96, 2007.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Nonnegative Matrix Factorizations for Clustering and LSI",
      "author" : [ "A. Mirzal" ],
      "venue" : "LAP Lambert Academic Publishing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Orthogonal nonnegative matrix t-factorizations for clustering",
      "author" : [ "C. Ding", "T. Li", "W. Peng", "H. Park" ],
      "venue" : "Proc. 12th ACM SIGKDD Int’l Conf. on Knowledge Discovery and Data Mining, pp. 126-35, 2006.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Orthogonal nonnegative matrix factorization: Multiplicative updates on Stiefel manifolds",
      "author" : [ "J. Yoo", "S. Choi" ],
      "venue" : "Proc. 9th Int’l Conf. Intelligent Data Engineering and Automated Learning, pp. 140-7, 2008.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Orthogonal nonnegative matrix tri-factorization for co-clustering: Multiplicative updates on Stiefel manifolds",
      "author" : [ "J. Yoo", "S. Choi" ],
      "venue" : "Information Processing & Management, Vol. 46, No. 5, pp. 559-70, 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Algorithms for orthogonal nonnegative matrix factorization",
      "author" : [ "S. Choi" ],
      "venue" : "Proc. IEEE Int’l Joint Conf. on Neural Networks, pp. 1828-32, 2008.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1828
    }, {
      "title" : "Non-Negative Matrix Factorization with Orthogonality Constraints for Chemical Agent Detection in Raman Spectra",
      "author" : [ "H. Li", "T. Adali", "W. Wang", "D. Emge" ],
      "venue" : "Proc. IEEE Workshop on Machine Learning for Signal Processing, pp. 253-8, 2005.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Projected gradient methods for non-negative matrix factorization",
      "author" : [ "C.J. Lin" ],
      "venue" : "Technical Report ISSTECH-95-013, Department of CS, National Taiwan University, 2005.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Accelerating the Lee-Seung algorithm for non-negative matrix factorization",
      "author" : [ "E.F. Gonzales", "Y. Zhang" ],
      "venue" : "Technical Report, Dept. Comput. Appl. Math., Rice Univ., Houston, 2005.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Fast projection-based methods for the least squares nonnegative matrix approximation problem",
      "author" : [ "D. Kim", "S. Sra", "I.S. Dhillon" ],
      "venue" : "Stat. Anal. Data Min., Vol. 1, No. 1, pp. 38-51, 2008.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fast newton-type methods for the least squares nonnegative matrix approximation problem",
      "author" : [ "D. Kim", "S. Sra", "I.S. Dhillon" ],
      "venue" : "Proc. SIAM Conference on Data Mining, pp. 343-54, 2007.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method",
      "author" : [ "H. Kim", "H. Park" ],
      "venue" : "SIAM. J. Matrix Anal. & Appl., Vol. 30, No. 2, pp. 713-30, 2008.  11",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Toward faster nonnegative matrix factorization: A new algorithm and comparisons",
      "author" : [ "J. Kim", "H. Park" ],
      "venue" : "Proc. 8th IEEE International Conference on Data Mining, pp. 353-62, 2008.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. Royal Statistical Society B, Vol. 58, Issue 1, pp. 267-88, 1996.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "The L-curve and its use in the numerical treatment of inverse problems, Computational Inverse Problems in Electrocardiology",
      "author" : [ "P.C. Hansen" ],
      "venue" : "WIT Press,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2000
    }, {
      "title" : "A new method for regularization parameter determination in the inverse problem of electrocardiography",
      "author" : [ "P.R. Johnston", "R.M. Gulrajani" ],
      "venue" : "IEEE Transaction on Biomedical Engineering, Vol. 44, No. 1, pp. 19-39, 1997.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A method for choosing the regularization parameter in generalized tikhonov regularized linear inverse problems",
      "author" : [ "S. Oraintara", "W.C. Karl", "D.A. Castanon", "T.Q. Nguyen" ],
      "venue" : "Proc. International Conference of Image Processing, pp. 93-96, 2000.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Regularization methods for linear inverse problems",
      "author" : [ "S.M. Tan", "C. Fox" ],
      "venue" : "Lecture Note: PHYSICS 707 Inverse Problems, Chapter 3, pp. 1-15, 1998.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Iterative choices of regularization parameters in linear inverse problems",
      "author" : [ "K. Kunisch", "J. Zhou" ],
      "venue" : "Inverse Problems, Vol. 14, No. 5, pp. 1247- 64, 1998.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Simultaneous multiple regulariza-  tion parameter selection by means of the L-hypersurface with applications to linear inverse problems posed in the wavelet transform domain",
      "author" : [ "M. Belge", "E. Miller", "M. Kilmer" ],
      "venue" : "SPIE Int’l Symposium on Optical Science, Engineering, and Instrumentation: Bayesian Inference for Inverse Problems, pp. 328-36, 1998.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A regularization parameter in discrete ill-posed problems",
      "author" : [ "T. Reginska" ],
      "venue" : "SIAM Journal of Scientific Computing, Vol. 17, No. 3, pp. 740-9, 1996.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "A technique for the numerical solution of certain integral equations of the first kind",
      "author" : [ "D.L. Phillips" ],
      "venue" : "J. Association for Computing Machinery, Vol. 9, Issue 1, pp. 84-97, 1997.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Solution of incorrectly formulated problems and the regularization method",
      "author" : [ "A.N. Tikhonov" ],
      "venue" : "Soviet Math. Dokl. 4, pp. 1035-8, 1963. English translation of Dokl. Akad. Nauk. SSSR, 151, pp. 501-4, 1963.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "A tutorial on convex optimization",
      "author" : [ "H. Hindi" ],
      "venue" : "Proc. American Control Conference, pp. 3252-65, 2004.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Nonlinear Programming 2nd Ed",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific, 1999.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Regular Linear Phase Perfect Reconstruction Filter Banks for Image Compression",
      "author" : [ "S. Oraintara" ],
      "venue" : "PhD Thesis, Boston University, 2000, Appendix A.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints",
      "author" : [ "L. Grippo", "M. Sciandrone" ],
      "venue" : "Operation Research Letters, Vol. 26, pp. 127-36, 2000.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "(2) In addition, other criteria like Kullback-Leibler divergence [1], [2] and Csiszárs φ-divergence [3] can also be used.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "(2) In addition, other criteria like Kullback-Leibler divergence [1], [2] and Csiszárs φ-divergence [3] can also be used.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "(2) In addition, other criteria like Kullback-Leibler divergence [1], [2] and Csiszárs φ-divergence [3] can also be used.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "[4], [5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[4], [5].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "The popularity of the NMF is due to the work of Lee and Seung [6] in which they introduced a simple yet powerful NMF algorithm, and then show its applicability in image processing and text analysis.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]–[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]–[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]–[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].",
      "startOffset" : 266,
      "endOffset" : 269
    }, {
      "referenceID" : 6,
      "context" : "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]–[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].",
      "startOffset" : 271,
      "endOffset" : 274
    }, {
      "referenceID" : 8,
      "context" : "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]–[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].",
      "startOffset" : 276,
      "endOffset" : 279
    }, {
      "referenceID" : 9,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 11,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]–[17], and showed that the NMF can give better results.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 6,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]–[24].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 24,
      "context" : "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it’s not always clear how to incorporate those auxiliary constraints into the algorithms.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 26,
      "context" : ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it’s not always clear how to incorporate those auxiliary constraints into the algorithms.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it’s not always clear how to incorporate those auxiliary constraints into the algorithms.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it’s not always clear how to incorporate those auxiliary constraints into the algorithms.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it’s not always clear how to incorporate those auxiliary constraints into the algorithms.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "The additive update rules based algorithm for standard NMF first appeared in the work of Lee & Seung [1], but the convergence proof was given by Lin in ref.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 28,
      "context" : ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "norm—the more appropriate constraint for enforcing sparseness [31]), and showed that it can offer better results compared to the results of standard NMF.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 31,
      "context" : "In addition, from the inverse problem study, it is known that Tikhonov regularized least square (LS) is the more preferable form in solving inverse problems because solutions of the conventional LS tend to be unstable and dominated by data and rounding errors [32]–[34].",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 33,
      "context" : "In addition, from the inverse problem study, it is known that Tikhonov regularized least square (LS) is the more preferable form in solving inverse problems because solutions of the conventional LS tend to be unstable and dominated by data and rounding errors [32]–[34].",
      "startOffset" : 265,
      "endOffset" : 269
    }, {
      "referenceID" : 34,
      "context" : "Moreover, in the presence of noise, frequently the conventional LS solutions are rather undesirable as it leads to amplification of noise in the direction of singular vectors with small singular values [35].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 35,
      "context" : "In this paper we will utilize the L-curve since the Morozov discrepancy principles require knowledge of the error level in the data which is often inaccessible [36].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 31,
      "context" : "In this curve, the proper value for the regularization parameter is the value associated with corner of the curve where both solution and approximation error have minimum norms [32], [34].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 33,
      "context" : "In this curve, the proper value for the regularization parameter is the value associated with corner of the curve where both solution and approximation error have minimum norms [32], [34].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 31,
      "context" : ", [32], [34], [37], [38].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 33,
      "context" : ", [32], [34], [37], [38].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 36,
      "context" : ", [32], [34], [37], [38].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 37,
      "context" : ", [32], [34], [37], [38].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 33,
      "context" : "[34] in which they defined L-corner to be the point of tangency between L-curve with positive curvature and a straight line of negative slope.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "We choose this method because it has convergence guarantee and is relatively faster to compute than the standard method; the maximum curvature approach [32].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 38,
      "context" : "The method was developed independently by Phillips [39] and Tikhonov [40].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 39,
      "context" : "The method was developed independently by Phillips [39] and Tikhonov [40].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "(5) To improve the solution, usually Tikhonov regularized LS is used instead [32]: xλ = argmin x ‖y −Ax‖F + λ‖x‖ 2 F (6) where λ denotes nonnegative regularization parameter, ‖y − Ax‖F denotes approximation error, and ‖x‖ 2 F denotes solution size.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : "In [34], Oraintara et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 33,
      "context" : "001 in the authors’ work [34].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 33,
      "context" : "Note that the value of γ doesn’t influence convergence property of sequence x and λ, and as long as λ is sufficiently small then λ converges to a stationary point [34].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 32,
      "context" : "[33], but the authors fixed γ value to one.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "and may have several local mimima [25].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 40,
      "context" : "The common practice to deal with the nonconvexity of an optimization problem is by transforming it into convex subproblems [41].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "In the case of the NMF, this can be done by employing the alternating strategy; fixing one matrix while solving for the other [25] (apparently, all NMF algorithms utilizing alternating strategy).",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "[34], γ m and γ C n are defined similarly as in algorithm 1, and ǫ denotes small positive number.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "17 if it satisfies the KKT optimality conditions [42], i.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "As stated by Lin [18], the above multiplicative update rules based algorithm can be modified into an equivalent converged algorithm by (1) using additive update rules, and (2) replacing zero entries that do not satisfy the KKT conditions with a small positive number to escape the zero locking.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 41,
      "context" : "This approach is known as the block-coordinate descent method [42].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "From convergence analysis study, the following conditions must be satisfied for sequence { B,C } to have convergence guarantee [18], [25], [44].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "From convergence analysis study, the following conditions must be satisfied for sequence { B,C } to have convergence guarantee [18], [25], [44].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 43,
      "context" : "From convergence analysis study, the following conditions must be satisfied for sequence { B,C } to have convergence guarantee [18], [25], [44].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "We will utilize the auxiliary function approach introduced in [1] to prove this property.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "Proof: As stated by Lin [18], it suffices to prove that",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 33,
      "context" : "[34], [43].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 42,
      "context" : "[34], [43].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 33,
      "context" : "[34]): The optimum βm corresponding to the L-corner must satisfy βm ∥ bTm ∥",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34]): The values of β m ∀m either strictly increase or decrease under update rule eq.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34]): If the update rule eq.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34], the followings summarize the strategy in choosing the initial values.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2012,
    "abstractText" : "We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination.",
    "creator" : "LaTeX with hyperref package"
  }
}