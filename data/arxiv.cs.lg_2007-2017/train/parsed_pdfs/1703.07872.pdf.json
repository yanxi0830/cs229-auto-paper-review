{
  "name" : "1703.07872.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Random Features for Compositional Kernels",
    "authors" : [ "Amit Daniely", "Roy Frostig", "Vineet Gupta", "Yoram Singer" ],
    "emails" : [ "singer}@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗Google Brain, {amitdaniely, frostig, vineet, singer}@google.com\nar X\niv :1\n70 3.\n07 87\n2v 1\n[ cs\n.L G\n] 2\n2 M\nar 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks. Learning using kernel representations amounts to convex optimization with provable convergence guarantees. The first generation of kernel functions in machine learning were oblivious to spatial or temporal characteristics of input spaces such as text, speech, and images. A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9]. Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].\nWhile the “kernel trick” unleashes the power of convex optimization, it comes with a large computational cost as it requires storing or repeatedly computing kernel products between pairs of examples. Rahimi and Recht [16] described and analyzed an elegant and computationally effective way that mitigates this problem by generating random features that approximate certain kernels. Their work was extended to various other kernels [10, 15, 2, 1].\nIn this paper we describe and analyze a simple random feature generation scheme from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of neural networks. The kernels’ definition and the connection to networks was developed in [6, 5]. Our feature map construction has several benefits over previous ones. It naturally exploits hierarchical structure in terms of representation power. The random feature generation is computationally efficient. More importantly, computing the feature map is efficient and often can be performed in time linear in the embedding dimension. Last but not least, computing the feature map requires highly sparse access patterns to the input, implying low memory requirements in the process.\nThe course of the paper is as follows. After a brief background, we start the paper by recapping in Sec. 3 the notion of random features schemes (RFS). Informally speaking, a random feature scheme is an embedding from an input space into the real or complex numbers. The scheme is random such that multiple instantiations result in different mappings. Standard inner products in the embedded space emulate a kernel function and converge to the inner product that the kernel defines. We conclude the section with a derivation of concentration bounds for kernel approximation by RFS and a generalization bound for learning with RFS.\nThe subsequent sections provide the algorithmic core of the paper. In Sec. 4 we describe RFS for basic spaces such as {−1,+1}, [n], and T. We show that the standard inner product on the sphere in one and two dimensions admits an effective norm-efficient RFS. However, any RFS for Sd−1 where d ≥ 3 is norm-deficient. In Sec. 5, we discuss how to build random feature schemes from compositional kernels that are described by a computation skeleton, which is an annotated directed acyclic graph. The base spaces constitute the initial nodes of the skeleton. As the name implies, a compositional kernel consists of a succession of compositions of prior constructed kernels, each of which is by itself a compositional kernel or a base kernel. We conclude the section with run-time and sparsity-level analysis.\nThe end result of our construction is a lightweight yet flexible feature generation procedure. Each random feature can be represented as an algebraic expression over of a small number of (random) paths in a composition tree. Thus, compositional random features\ncan be stored very compactly. The discrete nature of the generation process enables deduplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. The latter property cannot be directly achieved by previously studied random feature schemes. We would like to emphasize that our approach does not stand in contrast to, but rather complements, prior work. Indeed, the base kernels of a compositional kernel can be non-elementary such as the Gaussian kernel, and hence our RFS can be used in conjunction with the well-studied RFS of [16] for Gaussian kernels. One can also envision a hybrid structure where the base kernels are defined, for example, through a PSD matrix obtained by metric learning on the original input space."
    }, {
      "heading" : "2 Background and notation",
      "text" : "We start with a few notational conventions used throughout the paper. The Hilbert spaces we consider are over the reals. This includes spaces that are usually treated as complex Hilbert spaces. For example, for z = a+ ib, z′ = a′ + ib′ ∈ C we denote 〈z, z′〉 = Re(zz̄′) = aa′ + bb′ (rather than the more standard 〈z, z′〉 = zz̄′). Likewise, for z, z′ ∈ Cq we denote 〈z, z′〉 =∑q\ni=1〈zi, z′i〉. For a measure space (Ω, µ), L2(Ω) denotes the space of square integrable functions f : Ω → C. For f, g ∈ L2(Ω) we denote 〈f, g〉L2(Ω) = ∫ Ω 〈f(x), g(x)〉dµ(x). For all the measurable spaces we consider, we assume that singletons are measurable. We denote T = {z ∈ C : |z| = 1}.\nNext, we introduce the notation for kernel spaces and recap some of their properties. A kernel is a function k : X × X → R such that for every x1, . . . ,xm ∈ X the matrix {k(xi,xj)}i,j is positive semi-definite. We say that k is D-bounded if k(x,x) ≤ D2 for every x ∈ X . We call k a normalized kernel if k(x,x) = 1 for every x ∈ X . We will always assume that kernels are normalized. A kernel space is a Hilbert space H of functions from X to R such that for every x ∈ X the linear functional f ∈ H 7→ f(x) is bounded. The following theorem describes a one-to-one correspondence between kernels and kernel spaces.\nTheorem 1. For every kernel k there exists a unique kernel space Hk such that for every x,x′ ∈ X , k(x,x′) = 〈k(·,x), k(·,x′)〉Hk . Likewise, for every kernel space H there is a kernel k for which H = Hk.\nThe following theorem underscores a tight connection between kernels and embeddings of X into Hilbert spaces.\nTheorem 2. A function k : X × X → R is a kernel if and only if there exists a mapping φ : X → H to some Hilbert space for which k(x,x′) = 〈φ(x), φ(x′)〉H. In this case, Hk = {fv | v ∈ H} where fv(x) = 〈v, φ(x)〉H. Furthermore, ‖f‖Hk = min{‖v‖H | f = fv} and the minimizer is unique.\nWe say that f : [−1, 1]→ R is a normalized positive semi-definite (PSD) function if\nf(ρ) = ∞∑ i=0 aiρ i where ∞∑ i=0 ai = 1, ∀i : ai ≥ 0 .\nNote that f is PSD if and only if f ◦ k is a normalized kernel for any normalized kernel k."
    }, {
      "heading" : "3 Random feature schemes",
      "text" : "Let X be a measurable space and let k : X × X → R be a normalized kernel. A random features scheme (RFS) for k is a pair (ψ, µ) where µ is a probability measure on a measurable space Ω, and ψ : Ω×X → C is a measurable function, such that\n∀x,x′ ∈ X , k(x,x′) = E ω∼µ\n[ ψ(ω,x)ψ(ω,x′) ] .\nSince the kernel is real valued, we have in this case that,\nk(x,x′) = Re (k(x,x′))\n= E ω∼µ\n[ Re ( ψ(ω,x)ψ(ω,x′) )] = E\nω∼µ 〈ψ(ω,x), ψ(ω,x′)〉 . (1)\nWe often refer to ψ as a random feature scheme. We define the norm of ψ as ‖ψ‖ = supω,x |ψ(ω,x)|. We say that ψ is C-bounded if ‖ψ‖ ≤ C. As the kernels are normalized, (1) implies that ‖ψ‖ ≥ 1 always. In light of this, we say that an RFS ψ is norm-efficient if it is 1-bounded. Note that in this case, since the kernel is normalized, it holds that |ψ(ω,x)| = 1 for almost every ω as otherwise we would obtain that k(x,x) < 1. Hence, we can assume w.l.o.g. that the range of norm-efficient RFSs is T.\nComment 1 (From complex to real RFSs). While complex-valued features would simplify the analysis of random feature schemes, it often favorable to work in practice with real-valued features. Let ψ(ω,x) := Rω(x)e iθω(x) be a C-bounded RFS for k. Consider the RFS\nψ′((ω, b),x) := √\n2Rω(x) cos (θω(x) + b) , where ω ∼ µ, and b ∈ {\n0, π 2\n} is distributed uniformly and independently from ω. It is not\ndifficult to verify that ψ′ is √ 2C-bounded RFS for k.\nA random feature generated from ψ is a random function ψ(ω, ·) from X to C where ω ∼ µ. A random q-embedding generated from ψ is the random mapping\nΨω(x) def = (ψ(ω1,x), . . . , ψ(ωq,x))√ q ,\nwhere ω1, . . . , ωq ∼ µ are i.i.d. The random q-kernel corresponding to Ψω is kω(x,x′) = 〈Ψω(x),Ψω(x′)〉. Likewise, the random q-kernel space corresponding to Ψω is Hkω . For the rest of this section, let us fix a C-bounded RFS ψ for a normalized kernel k and a random q embedding Ψω. For every x,x ′ ∈ X\nkω(x,x ′) =\n1\nq q∑ i=1 〈ψ(ωi,x), ψ(ωi,x′)〉\nis an average of q independent random variables whose expectation is k(x,x′). By Hoeffding’s bound we have the following theorem.\nTheorem 3 (Kernel Approximation). Assume that q ≥ 2C 4 log( 2δ ) 2\n, then for every x,x′ ∈ X we have Pr (|kω(x,x′)− k(x,x′)| ≥ ) ≤ δ.\nWe next discuss approximation of functions in Hk by functions in Hkω . It would be useful to consider the embedding\nx 7→ Ψx where Ψx def= ψ(·,x) ∈ L2(Ω) . (2)\nFrom (1) it holds that for any x,x′ ∈ X , k(x,x′) = 〈Ψx,Ψx′〉L2(Ω). In particular, from Theorem 2, for every f ∈ Hk there is a unique function f̌ ∈ L2(Ω) such that ‖f̌‖L2(Ω) = ‖f‖Hk and for every x ∈ X ,\nf(x) = 〈f̌ ,Ψx〉L2(Ω) = E ω∼µ 〈f̌(ω), ψ(ω,x)〉 . (3)\nLet us denote fω(x) = 1 q ∑q i=1〈f̌(ωi), ψ(ωi,x)〉. From (3) we have that Eω [fω(x)] = f(x). Furthermore, for every x, the variance of fω(x) is at most\n1 q E ω∼µ ∣∣〈f̌(ω), ψ(ω,x)〉∣∣2 ≤ C2 q E ω∼µ ∣∣f̌(ω)∣∣2 =\nC2‖f‖2Hk q .\nAn immediate consequence is the following corollary.\nCorollary 4 (Function Approximation). For all x ∈ X , Eω |f(x)− fω(x)|2 ≤ C2‖f‖2Hk\nq .\nAs a result, if χ is a distribution on X , we have\nE ω ‖f − fω‖2,χ = E ω √ E χ |f(x)− fω(x)|2\n≤ √\nE ω E χ |f(x)− fω(x)|2 = √\nE χ E ω |f(x)− fω(x)|2\n≤ C‖f‖Hk√ q .\nWe next consider supervised learning with RFS. Let Y be a target (output) space and let ` : Rt×Y → R+ be a ρ-Lipschitz loss function, i.e. for every y ∈ Y , |`(y1, y)−`(y2, y)| ≤ ρ|y1−y2|. Let D be a distribution on X × Y . We define the loss of a (prediction) function f : X → Rt as LD(f) = E(x,y)∼D `(f(x), y). Let S = {(x1, y1), . . . , (xm, ym)} denote m i.i.d. examples sampled from D. We denote by Htk the space of all functions f = (f1, . . . , ft) : X → Rt where fi ∈ Hk for every i. Htk is a Hilbert space with the inner product 〈f ,g〉Htk = ∑t i=1〈fi, gi〉Hk . Let f̂ be the function in Htk that minimizes the regularized empirical loss,\nLλS(f) = 1\nm m∑ i=1 `(f(xi), yi) + λ‖f‖2Htk ,\nover all functions in Htk. It is well established (see e.g. Corollary 13.8 in [20]) that for every f? ∈ Htk,\nE S LD(f̂) ≤ LD (f?) + λ‖f?‖2Htk +\n2ρ2 λm . (4)\nIf we further assume that ‖f?‖Htk ≤ B, for B > 0, and set λ = √ 2ρ√ mB , we obtain that\nE S LD(f̂) ≤ inf\n‖f?‖Ht k ≤B\nLD (f ?) + √ 8ρB√ m . (5)\nThe additive term in (5) is optimal, up to a constant factor. We would like to obtain similar bounds for an algorithm that minimizes the regularized loss w.r.t. the embedding Ψω. Let f̂ω be the function that minimizes,\nLλS(f) = 1\nm m∑ i=1 `(f(xi), yi) + λ‖f‖2Htkω , (6)\nover all functions in Htkω . Note that in most settings f̂ω can be found efficiently by defining a matrix V ∈ Ct×q whose i’th row is vi, and rewriting f̂ω as,\nf̂ω(x) = (〈v1,Ψω(x)〉, . . . , 〈vt,Ψω(x)〉) def = VΨω(x) .\nWe now can recast the empirical risk minimization of (6) as,\nLλS(V ) = 1\nm m∑ i=1 `(VΨω(xi), yi) + λ‖V ‖2F .\nTheorem 5 (Learning with RFS). For every f? ∈ Htk,\nE ω E S LD(f̂ω) ≤ LD (f?) + λ‖f?‖2Htk +\n2ρ2C2 λm + ρ‖f?‖HtkC√ q . (7)\nIf we additionally impose the constraint ‖f?‖Htk ≤ B for B > 0 and set λ = √ 2ρC√ mB we have,\nE ω E S LD(f̂ω) ≤ inf\n‖f?‖Ht k ≤B\nLD (f ?) + √ 8ρBC√ m + ρBC √ q . (8)\nWe note that for norm-efficient RFS (i.e. when C = 1), if the number of random features is proportional to the number of examples, then the error terms in the bounds (8) and (5) are the same up to a multiplicative factor. Since the error term in (5) is optimal up to a constant factor, we get that the same holds true for (8).\nProof. For simplicity, we analyze the case t = 1. Since kω is C-bonded, we have from (4) that,\nE S LD (f) ≤ LD (f ?ω) + λ‖f ?ω‖2Hkω +\n2ρ2C2\nλm .\nHence, it is enough to show that Eω ‖f ?ω‖2Hkω ≤ ‖f ?‖2Hk and Eω LD (f ? ω) ≤ LD (f ?)+ ρ‖f?‖HkC√ q . Indeed, since\nf ?ω(x) =\n〈 (f̌ ?(ω1), . . . , f̌\n?(ωq))√ q ,Ψω(x)\n〉 ,\nwe have, by Theorem 2,\nE ω ‖f ?ω‖2Hkω ≤ Eω\n[∑q i=1 ∣∣f̌ ?(ωi)∣∣2 q ]\n= 1\nq q∑ i=1 E ωi ∣∣f̌ ?(ωi)∣∣2 = ‖f ?‖2Hk ,\nand similarly,\nE ω LD (f ? ω) = E ω E D l(f ?ω(x), y) = ED Eω l(f ? ω(x), y) .\nNow, from the ρ-Lipschitzness of ` and Theorem (4) we obtain,\nE ω `(f ?ω(x), y) ≤ `(f ?(x), y) + ρE ω |f ?(x)− f ?ω(x)| ≤ `(f ?(x), y) + ρ √\nE ω |f ?(x)− f ?ω(x)|2\n≤ `(f ?(x), y) + ρ‖f ?‖HkC√ q ,\nconcluding the proof."
    }, {
      "heading" : "4 Random feature schemes for basic spaces",
      "text" : "In order to apply Theorem 5, we need to control the boundedness of the generated features. Consider the RFS generation procedure, given in Algorithm 1, which employs multiplications of features generated from basic RFSs. If each basic RFS is C-bounded, then every feature that is a multiplication of t basic features is Ct-bounded. In light of this, we would like have RFSs for the basic spaces whose norm is as small as possible. The best we can hope for is norm-efficient RFSs—namely, RFSs with norm of 1. We first describe such RFSs for several kernels including the Gaussian kernel on Rd, and the standard inner product on S0 and S1. Then, we discuss the standard inner product on Sd−1 for d ≥ 3. In this case, we show that the smallest possible norm for an RFS is √ d/2. Hence, if the basic spaces are Sd−1 for d ≥ 3, one might prefer to use other kernels such as the Gaussian kernel.\nExample 1 (Binary coordinates). Let X = {±1} and k(x, x′) = xx′. In this case the deterministic identity RFS ψ(ω, x) = x is norm-efficient.\nExample 2 (One dimensional sphere). Let X = T and k(z, z′) = 〈z, z′〉. Let ψ(ω, z) = zω, where ω is either −1 or +1 with equal probability. Then, ψ is a norm-efficient RFS since\nE ω∼µ\nψ(ω, z)ψ(ω, z′) = zz′ + zz′\n2 = 〈z, z′〉 .\nExample 3 (Gaussian kernel). Let X = Rd and k(x,x′) = e− a2‖x−x′‖2\n2 , where a > 0 is a constant. The Gaussian RFS is ψ(ω,x) = eia〈ω,x〉, where ω ∈ Rd is the standard normal distribution. Then, ψ is a norm-efficient RFS, as implied by [16].\nExample 4 (Categorical coordinates). Let X = [n] and define k(x, x′) = 1 if x = x′ and 0 otherwise. In this case ψ(ω, x) = e iωx 2πn , where ω is distributed uniformly over [n], is a norm-efficient RFS since,\nE ω∼µ ψ(ω, x)ψ(ω, x′) = E ω e iω(x−x′) 2πn =\n{ 1 x = x′\n0 x 6= x′ .\nExamples 1 and 2 show that the standard inner product on the sphere in one and two dimensions admits a norm-efficient RFS. We next examine the sphere Sd−1 for d ≥ 3. In this case, we show a construction of a √ d/2-bounded RFS. Furthermore, we show that it is the best attainable bound. Namely, any RFS for Sd−1 will necessarily have a norm of at least√ d/2. In particular, there does not exist a norm-efficient RFS when d ≥ 3.\nExample 5 (Sd−1 for d ≥ 3). Let µ be the uniform distribution on Ω = [d] × {−1,+1}. Define ψ : Ω × Sd−1 → C for ω = (j, b) as ψ(ω,x) = √ d/2(xj + ibxj+1), where we use the\nconvention xd+1 := x1. Now, ψ is a √ d/2-bounded RFS as,\nE (j,b)∼µ ψ((j, b),x)ψ((j, b),x′)\n= d 2\n∑d j=1 [ (xj + ixj+1)(x ′ j − ix′j+1) + (xj − ixj+1)(x′j + ix′j+1) ] 2d\n=\n∑d j=1 [ xjx ′ j + xj+1x ′ j+1 ] 2\n= 〈x,x′〉 .\nWe find it instructive to compare the RFS above to the following √ d-bounded RFSs.\nExample 6. Let µ be the uniform distribution on Ω = Sd−1 and define ψ : Ω × Sd−1 → R as ψ(w,x) = √ d〈w,x〉. We get that,\nE w∼µ [ψ(w,x)ψ(w,x′)] = d E w∼µ 〈w,x〉〈w,x′〉 = d 〈x,Wx′〉 ,\nwhere Wi,j = Ew∼µ [wiwj]. Since w is distributed uniformly on Sd−1, E [wiwj] = 0 for i 6= j and E [w2i ] = 1/d. Thus, we have W = (1/d)I and therefore Ew∼µ [ψ(w,x)ψ(w,x′)] = 〈x,x′〉. A similar result still holds when ψ(ω,x) = √ d xω where ω ∈ [d] is distributed uniformly.\nTo conclude the section, we prove that √ d/2-boundedness is optimal for RFS on Sd−1.\nTheorem 6. Let d ≥ 1 and > 0. There does not exist a ( √ d/2− )-bounded RFS for the kernel k(x,x′) = 〈x,x′〉 on Sd−1.\nBefore proving the theorem, we need the following lemmas.\nLemma 7. Let z ∈ Cd. There exists a ∈ Sd−1 such that ∣∣∣∑j ajzj∣∣∣2 ≥ 12‖z‖2.\nProof. Let us write z = α + iβ where α,β ∈ Rd. We thus have ‖z‖2 = ‖α‖2 + ‖β‖2. We can further assume that ‖α‖2 ≥ 1\n2 ‖z‖2 as otherwise we can replace z with iz. Let us define\na as α/‖α‖. We now obtain that,∣∣∣∣∣∑ j ajzj ∣∣∣∣∣ 2 ≥ 〈a,α〉2 = ‖α‖2 ≥ 1 2 ‖z‖2 ,\nwhich concludes the proof.\nLemma 8. Let (ψ, µ) be an RFS for the kernel k(x,x′) = 〈x,x′〉 on Sd−1 and let a ∈ Sd−1. Then, for almost all ω we have ψ(ω, a) = ∑d j=1 ajψ(ω, ej).\nProof. Let us examine the difference between Ψa def = ψ(·, a) and ∑ j ajΨ ej def= ∑\nj ajψ(·, ej),∥∥∥∥∥Ψa − d∑ i=1 aiΨ ei ∥∥∥∥∥ 2\nL2(Ω) = 〈Ψa,Ψa〉L2 + d∑\ni,j=1 aiaj〈Ψei ,Ψej〉L2 − 2 d∑ i=1 ai〈Ψa,Ψei〉L2\n= 〈a, a〉+ d∑\ni,j=1 aiaj〈ei, ej〉 − 2 d∑ i=1 ai〈a, ei〉\n= ∥∥∥∥∥a− d∑ i=1 aiei ∥∥∥∥∥ 2 = 0.\nProof of Theorem 6. Let ψ : Sd−1 × Ω → C be an RFS for k(x,x′) = 〈x,x′〉 and let > 0. We next show that ψ is not √ d/2− - bounded. Let A ⊂ Sd−1 be a dense and countable set. From Lemma (8) and the fact that sets of measure zero are closed under countable union, it follows that for almost every ω and all a ∈ A we have ψ(ω, a) = ∑d i=1 aiψ(ω, ei). Using the linearity of expectation we know that,\nE ω∼µ d∑ i=1 |ψ(ω, ei)|2 = d∑ i=1 E ω∼µ |ψ(ω, ei)|2 = d∑ i=1 〈ei, ei〉 = d .\nHence, with a non-zero probability we get,\nd∑ i=1 |ψ(ω, ei)|2 > d− , (9)\nand,\n∀a ∈ A, ψ(ω, a) = d∑ i=1 aiψ(ω, ei) . (10)\nLet us now fix ω for which (9) holds. From Lemma (7) there exists ã ∈ Sd−1 such that | ∑ i ãiψ(ω, ei)| 2 ≥ d− 2 . Since A is dense in Sd−1 there is a vector a ∈ A for which\n| ∑\ni aiψ(ω, ei)| 2 ≥ d 2 − . Finally, from (9) it follows that |ψ(ω, a)|2 ≥ d 2 − ."
    }, {
      "heading" : "5 Compositional random feature schemes",
      "text" : "Compositional kernels are obtained by sequentially multiplying and averaging kernels. Hence, it will be useful to have RFSs for multiplications and averages of kernels. The proofs of Lemmas 9 and 10 below are direct consequences of properties of kernel spaces and RFSs and thus omitted.\nLemma 9. Let (ψ1, µ1), (ψ2, µ2), . . . be RFSs for the kernels k1, k2, . . . and let (αi) ∞ i=1 be a sequence of non-negative numbers that sum to one. Then, the following procedure defines an RFS for the kernel k(x,x′) = ∑n i=1 αik i(x,x′).\n1. Sample i with probability αi\n2. Choose ω ∼ µi\n3. Generate the feature x 7→ ψiω(x)\nLemma 10. Let (ψ1, µ1), . . . , (ψn, µn) be RFSs for the kernels k1, . . . , kn. The following scheme is an RFS for the kernel k(x,x′) = ∏n i=1 k\ni(x,x′). Sample ω1, . . . , ωn ∼ µ1× . . .×µn and generate the feature x 7→ ∏n i=1 ψ i ωi (x).\nRandom feature schemes from computation skeletons. We next describe and analyze the case where the compositional kernel is defined recursively using a concrete computation graph defined below. Let X1, . . . ,Xn be measurable spaces with corresponding normalized kernels k1, . . . , kn and RFSs ψ1, . . . , ψn. We refer to these spaces, kernels, and RFS as base spaces, kernels and RFSs. We also denote X = X1 × . . .×Xn. The base spaces (and correspondingly kernels, and RFSs) often adhere to a simple form. For example, for real-valued input, feature i is represented as Xi = T, where ki(z, z′) = 〈z, z′〉, ψiω(z) = zω, and ω is distributed uniformly in {±1}.\nWe next discuss the procedure for generating compositional RFSs using a structure termed computation skeleton, or skeleton for short. A skeleton S is a DAG with m := |S| nodes. S has a single node whose out degree is zero, termed the output node and denoted out(S), see Figure 1 for an illustration. The nodes indexed 1 through n are input nodes, each of which is associated with a base space. We refer to non-input nodes as internal nodes. Thus, the indices of internal nodes are in {n + 1, . . . ,m}. An internal node v is associated with a PSD function (called a conjugate activation function [6][Sec. 5]), σ̂v(ρ) = ∑∞ i=0 a v i ρ i. For every node v we denote by Sv the subgraph of S rooted at v. This sub-graph defines a compositional kernel through all the nodes nodes with a directed path to v. By definition it holds that out(Sv) = v and Sout(S) = S. We denote by in(v) the set of nodes with a directed edge into v. Each skeleton defines a kernel kS : X × X → R according to the following recurrence,\nkS(x,x ′) =\n{ kv(x,x\n′) v ∈ [n] σ̂v\n(∑ u∈in(v) kS(u)(x,x ′)\n|in(v)|\n) v 6∈ [n] for v = out(S) .\nIn Figure 1 we give the pseudocode describing the RFS for the kernel kS . We call the routine RFSS as a shorthand for a Random Feature Scheme for a Skeleton. The correctness of the algorithm is a direct consequence of Lemmas 9 and 10.\nAlgorithm 1 RFSS(S) Let v = out(S) if v ∈ [n] then\nReturn x 7→ ψv(x) else\nSample l ∈ {0, 1, 2, . . . , } according to (avi )∞i=0 for j = 1, . . . , l do\nChoose u ∈ in(v) at random Call RFSS(Su) and get x 7→ ψωj(x)\nend for Return x 7→ ∏l j=1 ψωj(x)\nend if\nWe next present a simple running time analysis of Algorithm 1 and the sparsity of the generated random features. Note that a compositional random feature is a multiplication of base random features. Thus the amortized time it takes to generate a compositional random\nfeature and its sparsity amount to the expected number of recursive calls made by RFSS. For a given node v the expected number of recursive calls emanating from v is,\nE l∼(avj ) [ l ] = ∞∑ j=0 j aj = σ̂ ′(1) .\nWe now define the complexity of a skeleton as,\nC(S) = { 1 out(S) ∈ [n] σ̂′v(1) ∑ u∈in(v) C(S(u)) |in(v)| otherwise . (11)\nIt is immediate to verify that C(S) is the expected value of the number of recursive calls and the sparsity of a random feature. When all conjugate activations are the same (11) implies that\nC(S) ≤ (σ̂′(1))depth(S) ,\nwhere equality holds when the skeleton is layered. For activations such as ReLU, σ(x) = max(0, x), and exponential, σ(x) = ex, we have σ̂′(1) = 1, and thus C(S) = 1. Hence, it takes constant time in expectation to generate a random feature, which in turn has a constant number of multiplications of base random features. For example, let us assume that the basic spaces are S1 with the standard inner product, and that the skeleton has a single non-input node, equipped with the exponential dual activation σ̂(ρ) = eρ. In this case, the resulting kernel is the Gaussian kernel and C(S) = 1. Therefore, the computational cost of storing and evaluating each feature is constant. This is in contrast to the Rahimi and Recht scheme [16], in which the cost is linear in n."
    }, {
      "heading" : "6 Empirical Evaluation",
      "text" : "Accuracy of kernel approximation. We empirically evaluated the kernel approximation of our random feature scheme under kernels of varying structure and depth. Our concern is efficiency of random features: how does the quality of approximation fare in response to increasing the target dimension of the feature map? Theorem 3 already establishes an upper bound for the approximation error (in high probability), but overlooks a few practical advantages of our construction that are illustrated in the experiments that follow. Primarily, when one repeatedly executes Algorithm 1 in order to generate features, duplicates may arise. It is straightforward to merge them and hence afford to generate more under the target feature budget. We use the CIFAR-10 dataset for visual object recognition [11].\nWe considered two kernel structures, one shallow and another deep. Figure 2 caricatures both. For visual clarity, it oversimplifies convolutions and the original image to onedimensional objects, considers only five input pixels, and disregards the true size and stride of convolutions used.\nFollowing Daniely et al. [6], for a scalar function σ : R → R termed an activation, let σ̂(ρ) = E(X,Y )∼Nρ [σ(X)σ(Y )] be its conjugate activation (shown in the original to be a PSD function). Our shallow structure is simply a Gaussian (RBF) kernel with scale 0.5.\nshallow deep\nEquivalently, again borrowing the terminology of Daniely et al. [6], it corresponds to a singlelayer fully-connected skeleton, having a single internal node v to which all input nodes point. The node v is labeled with a conjugate activation σ̂v(ρ) = exp((ρ− 1)/4).\nThe deep structure comprises a layer of 5x5 convolutions at stride 2 with a conjugate activation of σ̂1(ρ) = exp((ρ − 1)/4), followed by a layer of 4x4 convolutions at stride 2 with the conjugate activation σ̂2 corresponding to the ReLU activation σ2(t) = max{0, t}, followed by a fully-connected layer again with the ReLU’s conjugate activation.\nIn each setting, we compare to a natural baseline built (in part, where possible) on the scheme of Rahimi and Recht [16] for Gaussian kernels. In the shallow setting, doing so is straightforward, as their scheme applies directly. As their scheme is not defined for compositional kernels, our baseline in the deep setting is a hybrid construction. A single random features is generated according to the recursive procedure of Algorithm 1, until an internal node is reached in the bottom-most convolutional layer. Each such node corresponds to a Gaussian kernel, and so we apply the scheme Rahimi and Recht [16] to approximate the kernel of that node.\nFor each configuration of true compositional kernel and feature budget, we repeat the following ten times: draw a batch of 128 data points at random (each center-cropped to 24x24 image pixels), generate a randomized feature map, and compute 1282 kernel evaluations. The result is 10·1282 corresponding evaluations of the true kernel k and an empirical kernel k̂. We compare error measures and correlation between the two kernels using (i) RFS inner products as the empirical kernel and (ii) inner products from the baseline feature map. Figure 3 plots the comparison.\nStructural effects and relation to neural networks. In connection to deep neural networks, we experiment with the effect of compositional kernel architecture on learning. We explore two questions: (i) Is a convolutional structure effective in a classifier built on random features? (ii) What is the relation between learning a compositional kernel and a neural network corresponding to its skeleton? The second question is motivated by the connection that Daniely et al. [6] establish between a compositional kernel and neural networks\ndescribed by its skeleton. In particular, we first explore the difference in classification accuracy between the kernel and the network. Then, since training under the kernel is simply a convex problem, we ask whether its relative accuracy across architectures predicts well the relative performance of analogous fully-trained networks.\nFor the experiment, we considered several structures and trained both a corresponding networks and the compositional kernel through an RFS approximation. We again use the CIFAR-10 dataset, with a standard data augmentation pipeline [12]: random 24x24 pixel crop, random horizontal flip, random brightness, saturation, and contrast delta, per-image whitening, and per-patch PCA. In the test set, no data augmentation is applied, and images are center-cropped to 24x24. We generated 106 random features, and trained for 120 epochs with AdaGrad [7] and a manually tuned initial learning rate among {25, 50, 100, 200}.\nThe convolutional architectures are of the same kind as the deep variety in Sec. 6, i.e. two convolutions with a size between 4x4 and 6x6 and a stride of 2, followed by a fully-connected layer. The fully-connected architecture is the shallow structure described in Section 6. All activations are ReLU. The per-patch PCA preprocessing step projects patches to the top q principal components where q is 10, 12, and 16, respectively, for first-layer convolutions of size 4, 5, and 6, respectively. The neural networks are generated from the kernel’s skeleton by node replication (i.e. producing channels) at a rate of 64 neurons per skeleton node for convolutions and 384 for fully-connected layers. We use the typical random Gaussian initialization [8], 200 epochs of AdaGrad with a manually tuned initial learning rate among {.0002, .0005, .001, .002, .005}.\nTest set accuracies are given in Table 1, annotated with how well approximate RFS learning accuracies rank those of the trained networks. We would like to underscore that, for convolutional kernels, networks consistently outperformed the kernel. Meanwhile, the fullyconnected kernel actually outperforms the corresponding network. RFS learning moreover\nranks the top two networks correctly (as well as the bottom three). This observation is qualitatively in line with earlier findings of Saxe et al. [17], who final-layer training of a randomly initialized network to rank fully-trained networks. Indeed, from Daniely et al. [6], we know that the random network approach is an alternative random feature map for the compositional kernel.\nVisualization of hierarchical random features. In Figure 4, we illustrate how our random feature scheme is able to accommodate local structures that are fundamental to image data. We chose 4 different networks of varying depths, and generated 500,000 random features for each. We then computed for each pixel, the probability that it co-occurs in a random feature with any other pixel, by measuring the correlation between their occurrences. As expected, for the flat kernel, the correlation between any pair of pixels was the same. However for deeper kernels, nearby pixels co-occur more often in the random features, and the degree of this co-occurrence is shown in the figure. As we increase the depth, different tiers of locality appear. The most intense correlations share a common first-layer convolution, while moderate correlation share only a second-layer convolution. Lastly, mild correlation share no convolutions."
    } ],
    "references" : [ {
      "title" : "Breaking the curse of dimensionality with convex neural networks",
      "author" : [ "F. Bach" ],
      "venue" : "arXiv:1412.8690,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the equivalence between quadrature rules and random features",
      "author" : [ "F.R. Bach" ],
      "venue" : "CoRR, abs/1502.06800,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Object recognition with hierarchical kernel descriptors",
      "author" : [ "L. Bo", "K. Lai", "X. Ren", "D. Fox" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729–1736. IEEE,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Y. Cho", "L.K. Saul" ],
      "venue" : "Advances in neural information processing systems, pages 342–350,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sgd learns the conjugate kernel class of the network",
      "author" : [ "Amit Daniely" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
      "author" : [ "Amit Daniely", "Roy Frostig", "Yoram Singer" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "International conference on artificial intelligence and statistics, pages 249–256,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The pyramid match kernel: Discriminative classification with sets of image features",
      "author" : [ "K. Grauman", "T. Darrell" ],
      "venue" : "Tenth IEEE International Conference on Computer Vision, volume 2, pages 1458–1465,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Random feature maps for dot product kernels",
      "author" : [ "P. Kar", "H. Karnick" ],
      "venue" : "arXiv:1201.6530,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "Technical report, University of Toronto,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks",
      "author" : [ "J. Mairal" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Convolutional kernel networks",
      "author" : [ "J. Mairal", "P. Koniusz", "Z. Harchaoui", "Cordelia Schmid" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Spherical random features for polynomial kernels",
      "author" : [ "J. Pennington", "F. Yu", "S. Kumar" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1837–1845,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS, pages 1177–1184,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On random weights and unsupervised feature learning",
      "author" : [ "A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1089–1096,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Advances in Kernel Methods - Support Vector Learning",
      "author" : [ "B. Schölkopf", "C. Burges", "A. Smola", "editors" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Prior knowledge in support vector kernels",
      "author" : [ "B. Schölkopf", "P. Simard", "A. Smola", "V. Vapnik" ],
      "venue" : "Advances in Neural Information Processing Systems 10, pages 640–646. MIT Press,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "S. Shalev-Shwartz", "S. Ben-David" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "Wiley,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "Springer,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].",
      "startOffset" : 197,
      "endOffset" : 211
    }, {
      "referenceID" : 2,
      "context" : "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].",
      "startOffset" : 197,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].",
      "startOffset" : 197,
      "endOffset" : 211
    }, {
      "referenceID" : 12,
      "context" : "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].",
      "startOffset" : 197,
      "endOffset" : 211
    }, {
      "referenceID" : 15,
      "context" : "Rahimi and Recht [16] described and analyzed an elegant and computationally effective way that mitigates this problem by generating random features that approximate certain kernels.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Their work was extended to various other kernels [10, 15, 2, 1].",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "Their work was extended to various other kernels [10, 15, 2, 1].",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Their work was extended to various other kernels [10, 15, 2, 1].",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Their work was extended to various other kernels [10, 15, 2, 1].",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "The kernels’ definition and the connection to networks was developed in [6, 5].",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "The kernels’ definition and the connection to networks was developed in [6, 5].",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "Indeed, the base kernels of a compositional kernel can be non-elementary such as the Gaussian kernel, and hence our RFS can be used in conjunction with the well-studied RFS of [16] for Gaussian kernels.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 19,
      "context" : "8 in [20]) that for every f ∈ H k, E S LD(f̂) ≤ LD (f) + λ‖f?‖2Ht k + 2ρ λm .",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 15,
      "context" : "Then, ψ is a norm-efficient RFS, as implied by [16].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "An internal node v is associated with a PSD function (called a conjugate activation function [6][Sec.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "This is in contrast to the Rahimi and Recht scheme [16], in which the cost is linear in n.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "We use the CIFAR-10 dataset for visual object recognition [11].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "[6], for a scalar function σ : R → R termed an activation, let σ̂(ρ) = E(X,Y )∼Nρ [σ(X)σ(Y )] be its conjugate activation (shown in the original to be a PSD function).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6], it corresponds to a singlelayer fully-connected skeleton, having a single internal node v to which all input nodes point.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "In each setting, we compare to a natural baseline built (in part, where possible) on the scheme of Rahimi and Recht [16] for Gaussian kernels.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "Each such node corresponds to a Gaussian kernel, and so we apply the scheme Rahimi and Recht [16] to approximate the kernel of that node.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "[6] establish between a compositional kernel and neural networks",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "In the shallow setting, the baseline (RR) is the scheme of Rahimi and Recht [16] for Gaussian kernels.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "We again use the CIFAR-10 dataset, with a standard data augmentation pipeline [12]: random 24x24 pixel crop, random horizontal flip, random brightness, saturation, and contrast delta, per-image whitening, and per-patch PCA.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "We generated 10 random features, and trained for 120 epochs with AdaGrad [7] and a manually tuned initial learning rate among {25, 50, 100, 200}.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "We use the typical random Gaussian initialization [8], 200 epochs of AdaGrad with a manually tuned initial learning rate among {.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "[17], who final-layer training of a randomly initialized network to rank fully-trained networks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6], we know that the random network approach is an alternative random feature map for the compositional kernel.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2017,
    "abstractText" : "We describe and analyze a simple random feature scheme (RFS) from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of convolutional neural networks and kernels. The resulting scheme yields sparse and efficiently computable features. Each random feature can be represented as an algebraic expression over a small number of (random) paths in a composition tree. Thus, compositional random features can be stored compactly. The discrete nature of the generation process enables de-duplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. Our approach complements and can be combined with previous random feature schemes. ∗Google Brain, {amitdaniely, frostig, vineet, singer}@google.com ar X iv :1 70 3. 07 87 2v 1 [ cs .L G ] 2 2 M ar 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}