{
  "name" : "1511.08486.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed Machine Learning via Sufficient Factor Broadcasting",
    "authors" : [ "Pengtao Xie", "Jin Kyu Kim", "Yi Zhou", "Qirong Ho", "Abhimanu Kumar", "Eric Xing" ],
    "emails" : [ "pengtaox@cs.cmu.edu", "jinkyuk@andrew.cmu.edu", "yzhou35@syr.edu", "hoqirong@gmail.com", "abhimanyu.kumar@gmail.com", "epxing@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix\nW. For example, in MLR, rows of W represent the classification coefficient vectors corresponding to different classes; whereas in SC rows of W correspond to the basis vectors used for reconstructing the observed data. A learning algorithm, such as stochastic gradient descent (SGD), would iteratively compute an update ∆W from data, to be aggregated with the current version of W. We call such models matrix-parameterized models (MPMs).\nLearning MPMs in large scale ML problems is challenging: ML application scales have risen dramatically, a good example being the ImageNet [12] compendium with millions of images grouped into tens of thousands of classes. To ensure fast running times when scaling up MPMs to such large problems, it is desirable to turn to distributed computation; however, a unique challenge to MPMs is that the parameter matrix grows rapidly with problem size, causing straightforward parallelization strategies to perform less ideally. Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters — a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14]. However, this synchronization can quickly become a bottleneck: take MLR for example, in which the parameter matrix W is of size J ×D, where J is the number of classes and D is the feature dimensionality. In one application of MLR to Wikipedia [26], J = 325k and D > 10, 000, thus W contains several billion entries (tens of GBs of memory). Because typical computer cluster networks can only transfer a few GBs per second at the most, inter-machine synchronization of W can dominate and bottleneck the actual algorithmic computation. In recent years, many distributed frameworks have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous keyvalue stores such as Yahoo LDA[2], DistBelief[10], PetuumPS [15], Project Adam [7] and [22]. When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates ∆W between machines, usually in a server-client style [11, 10, 29, 14, 7,\nar X\niv :1\n51 1.\n08 48\n6v 1\n[ cs\n.L G\n] 2\n6 N\n21]. As the matrices become larger due to increasing problem sizes, so do communication costs and synchronization delays — hence, reducing such costs is a key priority when using these frameworks.\nIn this paper, we investigate the structure of matrix parameterized models, in order to design efficient communication strategies that can be realized in distributed ML frameworks. We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update 4W computed over one (or a few) data sample(s) is of low-rank, e.g . it can be written as the outer product of two vectors u and v: 4W = uv>. The vectors u and v are sufficient factors (SF, meaning that they are sufficient to reconstruct the update matrix 4W). A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector. Similarly, when solving an `2 regularized MLR problem using SDCA, the update matrix 4W also admits such as a structure, where u is the update vector of a dual variable and v is the feature vector. Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].\nLeveraging this property, we propose a computation model called Sufficient Factor Broadcasting (SFB), and evaluate its effectiveness in a peer-to-peer implementation (while noting that SFB can also be used in other distributed frameworks). SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21]. The basic idea is as follows: since 4W can be exactly constructed from the sufficient factors, rather than communicating the full (update) matrix between workers, we can instead broadcast only the sufficient factors and have workers reconstruct the updates. SFB is thus highly communication-efficient; transmission costs are linear in the dimensions of the parameter matrix, and the resulting faster communication greatly reduces waiting time in synchronous systems (e.g. Hadoop and Spark), or improves parameter freshness in (bounded) asynchronous systems (e.g. GraphLab, Petuum-PS and [22]). SFs have been used to speed up some (but not all) network communication in deep learning [7]; our work differs primarily in that we always transmit SFs, never full matrices.\nSFB does not impose strong requirements on the distributed system — it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy. We provide theoretical analysis of SFB under synchronous and boundedasync consistency, and demonstrate that SFB learning of matrix-parametrized models significantly outperforms strategies that communicate the full parameter/update matrix, on a variety of applications including distance metric learning [35], sparse coding [24] and unregularized/`2-regularized multiclass logistic regression. Using our own C++ implementations of each application, our experiments show that, for parameter matrices with 5-10 billion entries, replacing full-matrix communication with SFB improves convergence times by 3-4 fold. Notably, our SFB implementation of `2- MLR is approximately 9 times faster than the Spark v1.3.1\nimplementation. We expect the performance benefit of SFB (versus full matrix communication) to improve with even larger matrix sizes.\nThe major contributions of this paper are summarized as follows:\n• We identify the sufficient factor property of a large family of matrix-parametrized models when solved with two popular algorithms: stochastic gradient descent and stochastic dual coordinate ascent.\n• In light of the sufficient factor property, we propose a sufficient factor broadcasting model of computation, which can greatly reduce the communication complexity without losing computational correctness.\n• We provide an efficient implementation of SFB, with flexible consistency models and easy-to-use programming interface.\n• We analyze the communication and computation costs of SFB and provide a convergence guarantee of SFB based minibatch SGD algorithm.\n• We perform extensive evaluation of SFB on four popular models and corroborate the efficiency and low communication complexity of SFB.\nThe rest of the paper is organized as follows. In Section 2 and 3, we introduce the sufficient factor property of matrix-parametrized models and propose the sufficient factor broadcasting computation model, respectively. Section 4 presents SFBroadcaster, an implementation of SFB. Section 5 analyzes the costs and convergence behavior of SFB. Section 6 gives experimental results. Section 7 reviews related works and Section 8 concludes the paper."
    }, {
      "heading" : "2. SUFFICIENT FACTOR PROPERTY OF MATRIX-PARAMETRIZED MODELS",
      "text" : "The core goal of Sufficient Factor Broadcasting (SFB) is to reduce network communication costs for matrix-parametrized models; specifically, those that follow an optimization formulation\n(P) min W\n1 N N∑ i=1 fi(Wai) + h(W) (1)\nwhere the model is parametrized by a matrix W ∈ RJ×D. The loss function fi(·) is typically defined over a set of training samples {(ai,bi)}Ni=1, with the dependence on bi being suppressed. We allow fi(·) to be either convex or nonconvex, smooth or nonsmooth (with subgradient everywhere); examples include `2 loss and multiclass logistic loss, amongst others. The regularizer h(W) is assumed to admit an efficient proximal operator proxh(·) [3]. For example, h(·) could be an indicator function of convex constraints, `1-, `2-, tracenorm, to name a few. The vectors ai and bi can represent observed features, supervised information (e.g., class labels in classification, response values in regression), or even unobserved auxiliary information (such as sparse codes in sparse coding [24]) associated with data sample i. The key property we exploit below ranges from the matrix-vector multiplication Wai. This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following:\nDistance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality. The vector ai is the difference of the feature vectors in the ith data pair and fi(·) can be either a quadratic function or a hinge loss function, depending on the similarity/dissimilarity label bi of the data pair. In both cases, h(·) can be an `1-, `2-, trace-norm regularizer or simply h(·) = 0 (no regularization). Sparse coding (SC) [24] learns a dictionary of basis from data, so that the data can be re-represented sparsely (and thus efficiently) in terms of the dictionary. In SC, W is the dictionary matrix, ai are the sparse codes, bi is the input feature vector and fi(·) is a quadratic function [24]. To prevent the entries in W from becoming too large, each column Wk must satisfy ‖Wk‖2 ≤ 1. In this case, h(W) is an indicator function which equals 0 if W satisfies the constraints and equals ∞ otherwise."
    }, {
      "heading" : "2.1 Optimization via proximal SGD and SDCA",
      "text" : "To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and wellestablished parallel optimization techniques. Proximal SGD: In proximal SGD, a stochastic estimate of the gradient, 4W, is first computed over one data sample (or a mini-batch of samples), in order to update W via W← W−η4W (where η is the learning rate). Following this, the proximal operator proxηh(·) is applied to W. Notably, the stochastic gradient 4W in (P) can be written as the outer product of two vectors 4W = uv>, where u = ∂f(Wai,bi)\n∂(Wai) ,\nv = ai, according to the chain rule. Later, we will show that this low rank structure of4W can greatly reduce interworker communication. Stochastic DCA: SDCA applies to problems (P) where fi(·) is convex and h(·) is strongly convex [36] (e.g . when h(·) contains the squared `2 norm); it solves the dual problem of (P), via stochastic coordinate ascent on the dual variables. More specifically, introducing the dual matrix U = [u1, . . . ,uN ] ∈ RJ×N and the data matrix A = [a1, . . . ,aN ] ∈ RD×N , we can write the dual problem of (P) as\n(D) min U\n1 N N∑ i=1 f∗i (−ui) + h∗( 1NUA >) (2)\nwhere f∗i (·) and h∗(·) are the Fenchel conjugate functions of fi(·) and h(·), respectively. The primal-dual matrices W and U are connected by1 W = ∇h∗(Z), where the auxiliary matrix Z := 1\nN UA>. Algorithmically, we need to update\nthe dual matrix U, the primal matrix W, and the auxiliary matrix Z: every iteration, we pick a random data sample i, and compute the stochastic update 4ui by minimizing (D) while holding {uj}j 6=i fixed. The dual variable is updated via ui ← ui − 4ui, the auxiliary variable via Z ← Z − 4uia>i , and the primal variable via W← ∇h∗(Z). Similar to SGD, the update of the SDCA auxiliary variable Z is also the outer product of two vectors: 4ui and ai, which can be exploited to reduce communication cost.\n1The strong convexity of h is equivalent to the smoothness of the conjugate function h∗.\nSufficient Factor property in SGD and SDCA: In both SGD and SDCA, the parameter matrix update can be computed as the outer product of two vectors — we call these sufficient factors (SFs). This property can be leveraged to improve the communication efficiency of distributed ML systems: instead of communicating parameter/update matrices among machines, we can communicate the SFs and reconstruct the update matrices locally at each machine. Because the SFs are much smaller in size, synchronization costs can be dramatically reduced. See ?? below for a detailed analysis. Low-rank Extensions: More generally, the update matrix 4W may not be exactly rank-1, but still of very low rank. For example, when each machine uses a mini-batch of size K, 4W is of rank at most K; in Restricted Boltzmann Machines [30], the update of the weight matrix is computed from four vectors u1,v1,u2,v2 as u1v > 1 −u2v>2 , i.e. rank-2; for the BFGS algorithm [4], the update of the inverse Hessian is computed from two vectors u,v as αuu>−β(uv>+ vu>), i.e. rank-3. Even when the update matrix 4W is not genuinely low-rank, to reduce communication cost, it might still make sense to send only a certain low-rank approximation. We intend to investigate these possibilities in future work."
    }, {
      "heading" : "3. SUFFICIENT FACTOR BROADCASTING",
      "text" : "Leveraging the SF property of the update matrix in problems (P) and (D), we propose a Sufficient Factor Broadcasting (SFB) model of computation, that supports efficient (low-communication) distributed learning of the parameter matrix W. We assume a setting with P workers, each of which holds a data shard and a copy of the parameter matrix2 W. Stochastic updates to W are generated via proximal SGD or SDCA, and communicated between machines to ensure parameter consistency. In proximal SGD, on every iteration, each worker p computes SFs (up,vp), based on one data sample xi = (ai,bi) in the worker’s data shard. The worker then broadcasts (up,vp) to all other workers; once all P workers have performed their broadcast (and have thus received all SFs), they re-construct the P update matrices (one per data sample) from the P SFs, and apply them to update their local copy of W. Finally, each worker applies the proximal operator proxh(·). When using SDCA, the above procedure is instead used to broadcast SFs for the auxiliary\n2For simplicity, we assume each worker has enough memory to hold a full copy of the parameter matrix W. If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.e. out of core operation). We plan to investigate these strategies as future work.\nmatrix Z, which is then used to obtain the primal matrix W = ∇h∗(Z). ?? illustrates SFB operation: 4 workers compute their respective SFs (u1,v1), . . . , (u4,v4), which are then broadcast to the other 3 workers. Each worker p uses all 4 SFs (u1,v1), . . . , (u4,v4) to exactly reconstruct the update matrices 4Wp = upv>p , and update their local copy of the parameter matrix: Wp ←Wp − ∑4 q=1 uqv > q . While the above description reflects synchronous execution, asynchronous and bounded-asynchronous extensions are also possible (Section 4). SFB vs client-server architectures: The SFB peer-topeer topology can be contrasted with a “full-matrix” clientserver architecture for parameter synchronization, e.g. as used by Project Adam [7] to learn neural networks: there, a centralized server maintains the global parameter matrix, and each client keeps a local copy. Clients compute sufficient factors and send them to the server, which uses the SFs to update the global parameter matrix; the server then sends the full, updated parameter matrix back to clients. Although client-to-server costs are reduced (by sending SFs), server-to-client costs are still expensive because full parameter matrices need to be sent. In contrast, the peer-to-peer SFB topology never sends full matrices; only SFs are sent over the network. We also note that under SFB, the update matrices are reconstructed at each of the P machines, rather than once at a central server (for full-matrix architectures). Our experiments show that the time taken for update reconstruction is empirically negligible compared to communication and SF computation. Mini-batch proximal SGD/SDCA: SFB can also be used in mini-batch proximal SGD/SDCA; every iteration, each worker samples a mini-batch of K data points, and computes K pairs of sufficient factors {(ui,vi)}Ki=1. These K pairs are broadcast to all other workers, which reconstruct the originating worker’s update matrix as 4W = 1 K ∑K i=1 uiv T i ."
    }, {
      "heading" : "4. SUFFICIENT FACTOR BROADCASTER:",
      "text" : "AN IMPLEMENTATION\nIn this section, we present Sufficient Factor Broadcaster (SFBcaster) — an implementation of SFB — including consistency models, programing interface and implementation details. We stress that SFB does not require a special system; it can be implemented on top of existing distributed frameworks, using any suitable communication topology — such as star3, ring, tree, fully-connected and Halton-sequence [21]."
    }, {
      "heading" : "4.1 Flexible Consistency Models",
      "text" : "Our SFBcaster implementation supports three consistency models: Bulk Synchronous Parallel (BSP-SFB), Asynchronous Parallel (ASP-SFB), and Stale Synchronous Parallel (SSPSFB), and we provide theoretical convergence guarantees for BSP-SFB and SSP-SFB in the next section. BSP-SFB: Under BSP [11, 23, 38], an end-of-iteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration. BSP is a strong consistency model, that guarantees the same computational outcome (and thus algorithm convergence) each time.\n3For example, each worker sends the SFs to a hub machine, which re-broadcasts them to all other workers.\nASP-SFB: BSP can be sensitive to stragglers (slow workers) [15, 31], limiting the distributed system to the speed of the slowest worker.\nThe Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others. ASP is efficient in terms of iteration throughput, but carries the risk that worker parameter copies can end up greatly out of synchronization, which can lead to algorithm divergence [15]. SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a bounded-asynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s. SSP alleviates the straggler issue while guaranteeing algorithm convergence [15, 31]. Under SSP-SFB, each worker p tracks the number of SF pairs computed by itself, tp, versus the number τ q p (tp) of SF pairs received from each worker q. If there exists a worker q such that tp − τ qp (tp) > s (i.e. some worker q is likely more than s iterations behind worker p), then worker p pauses until q is no longer s iterations or more behind. When s = 0, SSP-SFB reduces to BSP-SFB [11, 38], and when s =∞, SSP-SFB becomes ASP-SFB."
    }, {
      "heading" : "4.2 Programming Interface",
      "text" : "The SFBcaster programming interface is simple; users need to provide a SF computation function to specify how to compute the sufficient factors. To send out SF pairs (u,v), the user adds them to a buffer object sv list, via: write u(vec u), write v(vec v), which set i-th SF u or v to vec u or vec v. All SF pairs are sent out at the end of an iteration, which is signaled by commit(). Finally, in order to choose between BSP, ASP and SSP consistency, users simply set staleness to an appropriate value (0 for BSP, ∞ for ASP, all other values for SSP). SFBcaster automatically updates each worker’s local parameter matrix using all SF pairs — including both locally computed SF pairs added to sv list, as well as SF pairs received from other workers.\nFigure 2 shows SFBcaster pseudocode for multiclass logistic regression. For proximal SGD/SDCA algorithms, SFBcaster requires users to write an additional function, prox(mat), which applies the proximal operator proxh(·) (or the SDCA dual operator h∗(·)) to the parameter matrix mat. Figure 3 shows the sample code of implementing sparse coding in SFB. D is the feature dimensionality of data and J is the\ndictionary size. Users write a SF computation function to specify how to compute the sufficient factors: for each data sample xi, we first compute its sparse code a based on the dictionary B stored in the parameter matrix sc.para mat. Given a, the sufficient factor u can be computed as Ba−xi and the sufficient factor v is simply a. In addition, users provide a proximal operator function to specify how to project B to the `2 ball constraint set."
    }, {
      "heading" : "4.3 Halton Sequence Broadcast",
      "text" : "In previous sections, we assume the sufficient factors generated by each worker are broadcasted to all other workers. Such a full broadcast pattern incurs a communication cost O(P 2), which grows quadratically with the number of workers P . In data center scale clusters, P can reach several thousand [10, 22], inwhere a full broadcast scheme would be too costly. To address this issue, we borrow the Haltonsequence idea proposed in [21], where each machine connects with and broadcasts messages to a subset of Q machines rather than all other machines. Q is in the scale of logP , thereby, the communication cost can be greatly reduced. The basic idea of Halton sequence broadcast (HSB) [21] works as follows: given a constructed Halton sequence4\n4http://en.wikipedia.org/wiki/Halton_sequence\n{P/2, P/4, 3P/4, P/8, 3P/8, · · · }, each worker p sends the sufficient vectors to Q machines with IDs {(p+ bP/2c)%P , (p + bP/4c)%P , (p + b3P/4c)%P , (p + bP/8c)%P , (p + b3P/8c)%P , · · · }. Figure 4 gives an illustration. In this example, we have 6 machines and set Q = 2 ≈ log2(6). According to the connection pattern rule, machine p should broadcast messages to machine (p+3)%6 and (p+1)%6. For example, machine 1 broadcasts messages to machine 2 and 4; machine 5 broadcasts messages to machine 2 and 6. HSB loses per-iteration consistency of different parameter copies since each worker only broadcasts the sufficient vectors to part of peers. However, it pursues eventual consistency in the sense that over a period of time all the workers can see the effects of updates from every other worker directly or indirectly via an intermediate worker."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "Figure 5 shows the implementation details on each worker in SFBcaster. Each worker maintains three threads: SF computing thread, parameter update thread and communication thread. Each worker holds a local copy of the parameter matrix and a partition of the training data. It also maintains an input SF queue which stores the sufficient factors computed locally and received remotely and an output SF queue which stores SFs to be sent to other workers. In each iteration, the SF computing thread checks the consistency policy detailed in Section 4 in the main paper. If permitted, this thread randomly chooses a minibatch of samples from the training data, computes the SFs and pushes them to the input and output SF queue. The parameter update thread fetches SFs from the input SF queue and uses them to update the parameter matrix. In proximal-SGD/SDCA, the proximal/dual operator function (provided by the user) is automatically called by this thread as a function pointer. The communication thread receives SFs from other workers and pushes them into the input SF queue and sends SFs in the output SF queue to other workers, either in a full broadcasting scheme or a Halton sequence based partial broadcasting scheme. One worker is in charge of measuring the objective value. Once the algorithm converges, this worker notifies all other workers to terminate the job. We implemented SFBcaster in C++. OpenMPI was used for communication between workers and OpenMP was used for multicore parallelization within each machine.\nThe decentralized architecture of SFBcaster makes it robust to machine failures. If one worker fails, the rest of workers can continue to compute and broadcast the sufficient factors among themselves. In addition, SFBcaster possesses high elasticity [22]: new workers can be added and existing workers can be taken offline, without restarting the running framework. A thorough study of fault tolerance and elasticity will be left for future work."
    }, {
      "heading" : "5. COST ANALYSIS AND THEORY",
      "text" : "We now examine the costs and convergence behavior of SFB under synchronous and bounded-async (e.g. SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes."
    }, {
      "heading" : "5.1 Cost Analysis",
      "text" : "Table 6 compares the communications, space and time (to apply updates to W) costs of peer-to-peer SFB, against full matrix synchronization (FMS) under a client-server architecture [7]. For SFB with a full broadcasting scheme, in each minibatch, every worker broadcasts K SF pairs (u,v) to P − 1 other workers, i.e. O(P 2K(J + D)) values are sent per iteration — linear in matrix dimensions J,D, and quadratic in P . For SFB with a Halton sequence broadcasting scheme, every worker communicates SF pairs with Q = O(P ) peers, hence the communication cost is reduced to O(P logPK(J + D)). Because SF pairs cannot be aggregated before transmission, the cost has a dependency on K. In contrast, the communication cost in FMS is O(PJD), linear in P , quadratic in matrix dimensions, and independent of K. For both SFB and FMS, the cost of storing W is O(JD) on every machine. As for the time taken to update W per iteration, FMS costs O(PJD) at the server (to aggregate P client update matrices) and O(PKJD) at the P clients (to aggregate K updates into one update matrix for the server). By comparison, SFB bears a cost of O(P 2KJD) under full broadcasting and O(P logPKJD) under Halton sequence broadcasting due to the additional overhead of reconstructing each update matrix P times.\nCompared with FMS, SFB achieves communication savings by paying an extra computation cost. In a number of practical scenarios, such a tradeoff is worthwhile. Consider large problem scales where min(J,D) ≥ 10000, and moderate minibatch sizes 1 ≤ K ≤ 1000 (as studied in this paper); when using a moderate number of machines (around 10-100), the O(P 2K(J+D)) communications cost of SFB is lower than the O(PJD) cost for FMS, and the relative benefit of SFB improves as the dimensions J,D of W grow. In data center scale computing environments with thousands of machines, we can adopt the Halton sequence broadcasting scheme under which the communication cost is linearithmic (O(P logP )) in P . As for the time needed to apply updates to W, it turns out that the additional cost of reconstructing each update matrix P times in SFB is negligible in practice — we have observed in our experiments that the time spent computing SFs, as well as communicating SFs over the network, greatly dominates the cost of reconstructing update matrices using SFs. Overall, the communication savings dominate the added computational overhead, which we validated in experiments (Section 6).\n5.2 Convergence Analysis\nWe study the convergence of minibatch SGD under full broadcasting SFB (with extensions to proximal-SGD, SDCA and Halton sequence broadcasting being a topic for future study). Since SFB is a peer-to-peer decentralized computation model, we need to show the parameter copies on different workers converge to the same limiting point. This is different from the analyses of centralized parameter server systems [15, 8, 1], which show convergence of global parameters on the central server.\nWe wish to solve the optimization problem minW ∑M m=1\nfm(W), whereM is the number of training data minibatches, and fm corresponds to the loss function on the m-th minibatch. Assume the training data minibatches {1, ...,M} are divided into P disjoint subsets {S1, ..., SP } with |Sp| denoting the number of minibatches in Sp. Denote F = ∑M m=1 fm\nas the total loss, and for p = 1, . . . , P , Fp := ∑ j∈Sp fj is the loss on Sp (residing on the p-th machine). Consider a distributed system with P machines. Each machine p keeps a local variable Wp and the training data in Sp. At each iteration, machine p draws one minibatch Ip uniformly at random from partition Sp, and computes the partial gradient ∑ j∈Ip ∇fj(Wp). Each machine updates its local variable by accumulating partial updates from all machines. Denote ηc as the learning rate at c-th iteration on every machine. The partial update generated by machine p at its c-th iteration is denoted as Up(W c p, I c p) =\n−ηc|Sp| ∑ j∈Icp ∇fj(Wcp). Note that Icp is random and the factor |Sp| is to restore unbiasedness in expectation. Then the local update rule of machine p is\nWcp = W 0+ ∑P q=1 ∑τqp (c) t=0 Uq(W t q, I t q), 0 ≤ (c−1)−τ qp (c) ≤ s (3) where W0 is the common initializer for all P machines, and τ qp (c) is the number of iterations machine q has transmitted to machine p when machine p conducts its c-th iteration. Clearly, τpp (c) = c. Note that we also require τ q p (c) ≤ c− 1, i.e., machine p will not use any partial updates of machine q that are too fast forward. This is to avoid correlation in the theoretical analysis. Hence, machine p (at its c-th iteration) accumulates updates generated by machine q up to iteration τ qp (c), which is restricted to be at most s iterations behind. Such bounded-asynchronous communication addresses the slow-worker problem caused by bulk synchronous execution, while ensuring that the updates accumulated by each machine are not too outdated. The following standard assumptions are needed for our theoretical analysis:\nAssumption 1. (1) For all j, fj is continuously differentiable and F is bounded from below; (2) ∇F , ∇Fp are Lipschitz continuous with constants LF and Lp, respectively,\nand let L = ∑P p=1 Lp; (3) There exists B, σ 2 such that for all p and c, we have (almost surely) ‖Wcp‖ ≤ B and E‖ |Sp| ∑ j∈Ip ∇fj(W)−∇Fp(W) ‖ 2 2 ≤ σ2.\nOur analysis is based on the following auxiliary update Wc = W0 + ∑P q=1 ∑c−1 t=0 Uq(W t q, I t q), (4)\nCompare to the local update (??) on machine p, essentially this auxiliary update accumulates all c−1 updates generated by all machines, instead of the τ qp (c) updates that machine p has access to. We show that all local machine parameter sequences are asymptotically consistent with this auxiliary sequence:\nTheorem 1. Let {Wcp}, p = 1, . . . , P , and {Wc} be the local sequences and the auxiliary sequence generated by SFB for problem (P) (with h ≡ 0), respectively. Under ?? and set the learning rate η−1c = LF 2 + 2sL+ √ c, then we have • lim inf c→∞\nE‖∇F (Wc)‖ = 0, hence there exists a subsequence of ∇F (Wc) that almost surely vanishes; • lim c→∞\nmaxp ‖Wc−Wcp‖ = 0, i.e. the maximal disagreement between all local sequences and the auxiliary sequence converges to 0 (almost surely); • There exists a common subsequence of {Wcp} and {Wc}\nthat converges almost surely to a stationary point of F ,\nwith the rate min c≤C\nE‖ ∑P p=1∇Fp(W c p)‖22 ≤ O ( (L+LF )σ 2Ps logC√ C ) Intuitively, Theorem 1 says that, given a properly-chosen learning rate, all local worker parameters {Wcp} eventually converge to stationary points (i.e. local minima) of the objective function F , despite the fact that SF transmission can be delayed by up to s iterations. Thus, SFB learning is robust even under bounded-asynchronous communication (such as SSP). Our analysis differs from [5] in two ways: (1) [5] explicitly maintains a consensus model which would require transmitting the parameter matrix among worker machines — a communication bottleneck that we were able to avoid; (2) we allow subsampling in each worker machine. Accordingly, our theoretical guarantee is probabilistic, instead of the deterministic one in [5]."
    }, {
      "heading" : "6. EXPERIMENTS",
      "text" : "We demonstrate how four popular models can be efficiently learnt using SFB: (1) multiclass logistic regression (MLR) and distance metric learning (DML)5 based on SGD; (2) sparse coding (SC) based on proximal SGD; (3) `2 regularized multiclass logistic regression (L2-MLR) based on SDCA. For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models. In FMS, workers send update matrices to the central server, which then sends up-to-date parameter\n5For DML, we use the parametrization proposed in [34], which learns a linear projection matrix L ∈ Rd×k, where d is the feature dimension and k is the latent dimension.\nmatrices to workers6. Due to data sparsity, both the update matrices and sufficient factors are sparse; we use this fact to reduce communication and computation costs. Our experiments used a 12-machine cluster; each machine has 64 2.1GHz AMD cores, 128G memory, and a 10Gbps network interface."
    }, {
      "heading" : "6.1 Datasets and Experimental Setup",
      "text" : "We used two datasets for our experiments: (1) ImageNet [12] ILSFRC2012 dataset, which contains 1.2 million images from 1000 categories; the images are represented with LLC features [33], whose dimensionality is 172k. (2) Wikipedia [26] dataset, which contains 2.4 million documents from 325k categories; documents are represented with tf-idf, with a dimensionality of 20k. We ran MLR, DML, SC, L2-MLR on the Wikipedia, ImageNet, ImageNet, Wikipedia datasets respectively, and the parameter matrices contained up to 6.5b, 8.6b, 8.6b, 6.5b entries respectively (the largest latent dimension for DML and largest dictionary size for SC were both 50k). The tradeoff parameters in SC and L2-MLR were set to 0.001 and 0.1. We tuned the minibatch size, and found that K = 100 was near-ideal for all experiments. All experiments used the same constant learning rate (tuned in the range [10−5, 1]). In all experiments except those in Section 6.6, we adopted a full broadcasting scheme."
    }, {
      "heading" : "6.2 Convergence Speed and Quality",
      "text" : "6This has the same communication complexity as [7], which sends SFs from clients to servers, but sends full matrices from servers to clients (which dominates the overall cost).\nFigure 7 shows the time taken to reach a fixed objective value, for different model sizes, using BSP consistency. Figure 8 shows the results under SSP with staleness=20. SFB converges faster than FMS, as well as Spark v1.3.17. This is because SFB has lower communication costs, hence a greater proportion of running time gets spent on computation rather than network waiting. This is shown in Figure 9, which plots data samples processed per second8 (throughput) and algorithm progress per sample for MLR, under BSP consistency and varying minibatch sizes. The middle graph shows that SFB processes far more samples per second than FMS, while the rightmost graph shows that SFB and FMS produce exactly the same algorithm progress per sample under BSP. For this experiment, minibatch sizes between K = 10 and 100 performed the best as indicated by the leftmost graph. We point out that larger model sizes should further improve SFB’s advantage over FMS, because SFB has linear communications cost in the matrix dimensions, whereas FMS has quadratic costs. Under a large model size (e.g., 325K classes in MLR), the communication cost becomes the bottleneck in FMS and causes prolonged network waiting time and considerable parameter synchronization delays, while the cost is moderate in SFB.\nWe also compared the iteration throughput and quality of SFB and FMS under the SSP consistency model. Figure 10 shows the iteration throughput (left) and iteration quality (right) for MLR, under SSP (staleness=20). The minibatch size was set to 100 for both SFB and FMS. As can be seen from the right graph, SFB has a slightly worse iteration quality than FMS. The reason we conjecture is the centralized architecture of FMS is more robust and stable than the decentralized architecture of SFB. On the other hand, the iteration throughput of SFB is much higher than FMS as shown in the left graph. Overall, SFB outperforms FMS in total convergence time."
    }, {
      "heading" : "6.3 Scalability",
      "text" : "In all experiments that follow, we set the number of (L2)MLR classes, DML latent dimension, SC dictionary size to 325k, 50k, 50k respectively. Figure 11 shows SFB scalability\n7Spark is about 2x slower than PS [15, 22] based C++ implementation of FMS, due to JVM and RDD overheads. 8We use samples per second instead of iterations, so different minibatch sizes can be compared.\nwith varying machines under BSP, for MLR, DML, SC, L2MLR. Figure 12 shows how SFB scales with machine count, under SSP with staleness=20. In general, we observed close to linear (ideal) speedup, with a slight drop at 12 machines."
    }, {
      "heading" : "6.4 Computation Time vs Network Waiting Time",
      "text" : "Figure 13 shows the total computation and network time required for SFB and FMS to converge, across a range of SSP staleness values9 — in general, higher communication cost and lower staleness induce more network waiting. In the figure, the horizontal axis corresponds to different staleness values. The blue solid bar, blue checker board, red solid bar and red checker board correspond to the network waiting time of FMS, computation time of FMS, network waiting time of SFB and computation time of SFB respectively. For all staleness values, SFB requires far less network waiting (because SFs are much smaller than full matrices in FMS). Computation time for SFB is slightly longer than FMS because (1) update matrices must be reconstructed on each SFB worker, and (2) SFB requires a few more iterations for convergence, because peer-to-peer communication causes a slightly more parameter inconsistency under staleness. Overall, the SFB reduction in network waiting time remains far greater than the added computation time, and outperforms FMS in total time.\nAnother observation is that increasing staleness value can reduce the network waiting time for both FMS and SFB, thus allows more iterations to be performed in a given time interval. This is because a larger staleness gives each worker more flexibility to proceed at its own pace without waiting for others. On the other hand, as staleness increases, the computation time grows. If the staleness is too large (e.g., 50), the total convergence time is prolonged due to the rapid increasing of computation time. This is due to that the parameter copies on different workers bear a higher risk to be out of synchronization (thus inconsistent) under a larger staleness, which hurts the convergence quality of each iteration and requires more iterations to achieve the same objective value. The best tradeoff happens when staleness is around 10-20 in our experiments.\n9The Spark implementation does not easily permit this time breakdown, so we omit it."
    }, {
      "heading" : "6.5 Communication Cost",
      "text" : "Figure 14 shows the communication volume of four models under BSP. As shown in the figure, the communication volume of SFB is significantly lower than FMS and Spark. Under the BSP consistency model, SFB and FMS share the same iteration quality, hence need the same number of iterations to converge. Within each iteration, SFB communicates vectors while FMS transmits matrices. As a result, the communication volume of SFB is much lower than FMS."
    }, {
      "heading" : "6.6 Halton Sequence Broadcasting",
      "text" : "We studied how the parameterQ in Halton sequence broadcasting (HSB) affects the convergence speed of SFB. Figure 15 and 16 show the convergence time of MLR and L2-MLR versus varying Q, under BSP and SSP (staleness=20) respectively. As observed in these two figures, a smaller Q incurs longer convergence time. This is because a smaller Q is more likely to cause the parameter copies on different workers to be out of synchronization and degrade iteration quality. However, as long as Q is not too small, the convergence speed of HSB is comparable with a full broadcasting scheme. As shown in the figures, for Q ≥ 4 ≈ log2 12,\nthe convergence time of HSB is very close to full broadcasting (where Q = 12). This demonstrates that using HSB, we can reduce the communication cost from O(P 2) to O(PQ) ≈ O(P logP ) with slight sacrifice of the convergence speed."
    }, {
      "heading" : "7. RELATED WORKS",
      "text" : "A number of system and algorithmic solutions have been proposed to reduce communication cost in distributed ML. On the system side, [10] proposed to reduce communication overhead by reducing the frequency of parameter/gradient exchanges between workers and the central server. [22] used filters to select part of “important” parameters/updates for transmission to reduce the number of data entries to be communicated. On the algorithm side, [32] and [36] studied the tradeoffs between communication and computation in distributed dual averaging and distributed stochastic dual coordinate ascent respectively. [28] proposed an approximate Newton-type method to achieve communication efficiency in distributed optimization. SFB is orthogonal to these existing approaches and be potentially combined with them to further reduce communication cost.\nPeer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21]. Our SFBcaster system also adopt such an architecture, but with the specific purpose of supporting the SFB computation model, which is not explored by existing peer-to-peer ML frameworks."
    }, {
      "heading" : "8. CONCLUSIONS AND FUTURE WORKS",
      "text" : "In this paper, we identify the sufficient factor property of a large set of matrix-parametrized models: when these models are optimized with stochastic gradient descent or stochastic dual coordinate ascent, the update matrices are of low-rank. Leveraging this property, we propose a sufficient factor broadcasting computation model to efficiently handle the learning of these models with low communication cost. We analyze the cost and convergence property of SFB and provide an efficient implementation and empirical evaluations.\nFor very large models, the size of the local parameter matrix W may exceed each machine’s memory capacity — to address this issue, we would like to investigate partitioning\nW over a small number of nearby machines, or using outof-core (disk-based) storage to hold W in future work.\nFinally, a promising extension to the SF idea is to reparameterize the model W completely in terms of SFs, rather than just the updates. For example, if we initialize the parameter matrix W to be of low rankR, i.e., W0 = ∑R j=1 ujv > j ,\nafter I iterations (updates), WI = ∑R+I j=1 ujv > j . Leveraging this fact, for SGD without proximal operation and SDCA where h(·) is a `2 regularizer, we can re-parametrize WI using a set of SFs {(uj ,vj)}R+Ij=1 , rather than maintaining W explicitly. This re-parametrization can possibly reduce both computation and storage cost, which we will investigate in the future."
    }, {
      "heading" : "9. ADDITIONAL AUTHORS",
      "text" : "Additional authors: Yaoliang Yu (Machine Learning Department, Carnegie Mellon University yaoliang@cs.cmu.edu)"
    }, {
      "heading" : "10. REFERENCES",
      "text" : "[1] A. Agarwal and J. C. Duchi. Distributed delayed\nstochastic optimization. In Conference on Neural Information Processing Systems, 2011.\n[2] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable inference in latent variable models. In ACM International Conference on Web Search and Data Mining, 2012.\n[3] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2009.\n[4] D. P. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.\n[5] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, 1989.\n[6] K. Bhaduri, R. Wolff, C. Giannella, and H. Kargupta. Distributed decision-tree induction in peer-to-peer systems. Statistical Analysis and Data Mining: The ASA Data Science Journal, 2008.\n[7] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam: building an efficient and scalable deep learning training system. In USENIX Symposium on Operating Systems Design and Implementation, 2014.\n[8] W. Dai, A. Kumar, J. Wei, Q. Ho, G. Gibson, and E. P. Xing. High-performance distributed ml at scale through parameter server consistency models. In AAAI Conference on Artificial Intelligence, 2015.\n[9] K. Das, K. Bhaduri, and H. Kargupta. A local asynchronous distributed privacy preserving feature selection algorithm for large peer-to-peer networks. Knowledge and information systems, 2010.\n[10] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker, K. Yang, Q. V. Le, et al. Large scale distributed deep networks. In Conference on Neural Information Processing Systems, 2012.\n[11] J. Dean and S. Ghemawat. Mapreduce: simplified data processing on large clusters. Communication of the ACM, 2008.\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.\n[13] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. Powergraph: distributed graph-parallel computation on natural graphs. In USENIX Symposium on Operating Systems Design and Implementation, 2012.\n[14] S. Gopal and Y. Yang. Distributed training of large-scale logistic models. In International Conference on Machine Learning, 2013.\n[15] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. Xing. More effective distributed ml via a stale synchronous parallel parameter server. In Conference on Neural Information Processing Systems, 2013.\n[16] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In International Conference on Machine Learning, 2008.\n[17] C.-J. Hsieh, H.-F. Yu, and I. S. Dhillon. Passcode: Parallel asynchronous stochastic dual co-ordinate descent. In International Conference on Machine Learning, 2015.\n[18] M. Jaggi, V. Smith, M. Takác, J. Terhorst, S. Krishnan, T. Hofmann, and M. I. Jordan. Communication-efficient distributed dual coordinate\nascent. In Conference on Neural Information Processing Systems, 2014.\n[19] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 1999.\n[20] S. Lee, J. K. Kim, X. Zheng, Q. Ho, G. A. Gibson, and E. P. Xing. On model parallelization and scheduling strategies for distributed machine learning. In Conference on Neural Information Processing Systems, 2014.\n[21] H. Li, A. Kadav, E. Kruus, and C. Ungureanu. Malt: distributed data-parallelism for existing ml applications. In Proceedings of the Tenth European Conference on Computer Systems, 2015.\n[22] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling distributed machine learning with the parameter server. In USENIX Symposium on Operating Systems Design and Implementation, 2014.\n[23] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In ACM SIGMOD International Conference on Management of data, 2010.\n[24] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 1997.\n[25] R. Ormándi, I. Hegedűs, and M. Jelasity. Gossip learning with linear models on fully distributed data. Concurrency and Computation: Practice and Experience, 2013.\n[26] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres, G. Paliouras, E. Gaussier, I. Androutsopoulos, M.-R. Amini, and P. Galinari. Lshtc: A benchmark for large-scale text classification. arXiv:1503.08581 [cs.IR], 2015.\n[27] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. Journal of Machine Learning Research, 2013.\n[28] O. Shamir, N. Srebro, and T. Zhang. Communication efficient distributed optimization using an approximate newton-type method. In International Conference on Machine Learning, 2014.\n[29] V. Sindhwani and A. Ghoting. Large-scale distributed non-negative sparse coding and sparse dictionary learning. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2012.\n[30] P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. 1986.\n[31] D. Terry. Replicated data consistency explained through baseball. Communication of the ACM, 2013.\n[32] K. Tsianos, S. Lawlor, and M. G. Rabbat. Communication/computation tradeoffs in consensus-based distributed optimization. In Conference on Neural Information Processing Systems, 2012.\n[33] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained linear coding for image classification. In IEEE Conference on Computer Vision and Pattern Recognition, 2010.\n[34] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance\nmetric learning for large margin nearest neighbor classification. In Conference on Neural Information Processing Systems, 2005.\n[35] E. P. Xing, M. I. Jordan, S. Russell, and A. Y. Ng. Distance metric learning with application to clustering with side-information. In Conference on Neural Information Processing Systems, 2002.\n[36] T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In Conference on Neural Information Processing Systems, 2013.\n[37] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2006.\n[38] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In USENIX Symposium on Networked Systems Design and Implementation, 2012.\nAPPENDIX"
    }, {
      "heading" : "A. PROOF OF THEOREM 1",
      "text" : "Proof. Let Fc := σ{Iτp : p = 1, . . . , P, τ = 1, . . . , c} be the filtration generated by the random samplings Iτp up to iteration counter c, i.e., the information up to iteration c. Note that for all p and c, Wcp and W\nc are Fc−1 measurable (since τ qp (c) ≤ c−1 by assumption), and Icp is independent of Fc−1. Recall that the partial update generated by machine p at its c-th iteration is\nUp(W c p, I c p) = −ηc|Sp| ∑ j∈Icp ∇fj(Wcp)\nThen it holds that\nUp(W c p) = E[Up(Wcp, Icp)|Fc−1] = −ηc∇Fp(Wcp)\n(Note that we have suppressed the dependence of Up on the iteration counter c.)\nThen, we have E [∑P\np=1 Up(W c p, I c p) | Fc−1 ] = ∑P p=1 E[Up(W c p, I c p) | Fc−1]\n= ∑P p=1 Up(W c p)\n(5) Similarly we have\nE [∥∥∑P\np=1 Up(W c p, I c p) ∥∥2 2 | Fc−1 ] = ∑P p,q=1 E[〈Up(W c p, I c p), Uq(W c q, I c q )〉 | Fc−1]\n= ∑P p,q=1〈Up(W c q), Uq(W c q)〉\n+ ∑P p=1 E [ ‖Up(Wcp, Icp)− Up(Wcp)‖22 | Fc−1 ] (6) The variance term in the above equality can be bounded as∑P\np=1 E [ ‖Up(Wcp, Icp)− Up(Wcp)‖22 | Fc−1 ] = η2c\nP∑ p=1 E ‖|Sp|∑ j∈Icp ∇fj(Wcp)−∇Fp(Wcp)‖22 | Fc−1 \n︸ ︷︷ ︸ σ̂2P\n≤ η2c σ̂2P (7)\nNow use the update rule Wc+1p = W c p + ∑P p=1 Up(W c p, I c p) and the descent lemma [5], we have\nF (Wc+1)− F (Wc) ≤ 〈Wc+1 −Wc,∇F (Wc)〉+ LF\n2 ‖Wc+1 −Wc‖22 = 〈 ∑P p=1 Up(W c p, I c p),∇F (Wc)〉+ LF2 ‖ ∑P p=1 Up(W c p, I\nc p)‖22 (8)\nThen take expectation on both sides, we obtain E [ F (Wc+1)− F (Wc) | Fc−1 ]\n≤ 〈 ∑P p=1 Up(W c p),∇F (Wc)〉+ LF η 2 c σ̂ 2P 2\n+LF 2 ‖ ∑P p=1 Up(W c p)‖22 = (LF 2 − η−1c )‖ ∑P p=1 Up(W c p)‖22 + LF η 2 c σ̂ 2P 2\n−η−1c 〈 ∑P p=1 Up(W c p), ∑P p=1[Up(W\nc)− Up(Wcp)]〉 ≤ (LF\n2 − η−1c )‖ ∑P p=1 Up(W c p)‖22 + LF η 2 c σ̂ 2P 2\n+‖ ∑P p=1 Up(W c p)‖ ∑P p=1 Lp‖W c −Wcp‖\n(9)\nNow take expectation w.r.t all random variables, we obtain E [ F (Wc+1)− F (Wc) ] ≤ (LF\n2 − η−1c )E [ ‖ ∑P p=1 Up(W c p)‖22 ] + ∑P p=1 LpE [ ‖ ∑P p=1 Up(W c p)‖‖Wc −Wcp‖ ] + LF η 2 cσ 2P 2\n(10) Next we proceed to bound the term E‖ ∑P p=1 Up(W c p)‖‖Wc−\nWcp‖. We list the auxiliary update rule and the local update rule here for convenience.\nWc = W0 + ∑P q=1 ∑c−1 t=0 Uq(W t q, I t q), Wcp = W 0 + ∑P q=1 ∑τqp (c) t=0 Uq(W t q, I t q).\n(11)\nNow subtract the above two and use the bounded delay assumption 0 ≤ (c− 1)− τ qp (c) ≤ s, we obtain\n‖Wc −Wcp‖ = ‖ ∑P q=1 ∑c−1 t=τ q p (c)+1 Uq(W t q, I t q)‖\n≤ ‖ ∑P q=1 ∑c−1 t=c−s Uq(W t q, I t q)‖+ ‖ ∑P q=1 ∑τqp (c) t=c−s Uq(W t q, I t q)‖\n≤ ∑c−1 t=c−s ‖ ∑P q=1 Uq(W t q, I t q)‖+ ηc−sG\n(12) where the last inequality follows from the facts that ηc is\nstrictly decreasing, and ‖ ∑P q=1 ∑τqp (c) t=c−s∇Fq(Wtq, Itq)‖ is bounded by some constant G since ∇Fq is continuous and all the sequences Wcp are bounded. Thus by taking expectation, we obtain\nE [ ‖ ∑P p=1 Up(W c p)‖ ‖Wc −Wcp‖ ] ≤ E [ ‖ ∑P p=1 Up(W c p)‖ (∑c−1 t=c−s ‖ ∑P q=1 Uq(W t q, I t q)‖+ ηc−sG\n)] = ∑c−1 t=c−s E [ ‖ ∑P p=1 Up(W c p)‖‖ ∑P q=1 Uq(W t q, I t q)‖ ]\n+ηc−sG · E [ ‖ ∑P p=1 Up(W c p)‖ ]\n≤ ∑c−1 t=c−s E [ ‖ ∑P p=1 Up(W c p)‖22 + ‖ ∑P q=1 Uq(W t q, I t q)‖22 ] +E‖ ∑P p=1 Up(W c p)‖22 + η2c−sG2\n≤ (s+ 1)E‖ ∑P p=1 Up(W c p)‖22 + η2c−sG2\n+ ∑c−1 t=c−s [ E‖ ∑P q=1 Uq(W t q)‖22 + η2t σ2P ] (13)\nNow plug this into the previous result in (??):\nEF (Wc+1)− EF (Wc) ≤ (LF\n2 − η−1c )E‖ ∑P p=1 Up(W c p)‖22\n+(s+ 1)LE‖ ∑P p=1 Up(W c p)‖22 + η2c−sG2L\n+ ∑c−1 t=c−s [ LE‖ ∑P p=1 Up(W c p)‖22 + η2tLσ2P ] + LF η 2 cσ 2P 2 = (LF 2 + (s+ 1)L− η−1c )E‖ ∑P p=1 Up(W c p)‖22 + η2c−sG2L\n+ ∑c−1 t=c−s [ LE‖ ∑P p=1 Up(W c p)‖22 + η2tLσ2P ] + LF η 2 cσ 2P 2\n(14) Sum both sides over c = 0, ..., C:\nEF (WC+1)− EF (W0) ≤ ∑C c=0 [ (LF 2 + (2s+ 1)L− η−1c )E‖ ∑P p=1 Up(W c p)‖22 ] +(Lσ2Ps+ LF σ\n2P 2 ) ∑C c=0 η 2 c +G 2L ∑C c=0 η 2 c−s\n(15) After rearranging terms we finally obtain∑C\nc=0\n[ η2c (η −1 c − LF2 − 2(s+ 1)L)E‖ ∑P p=1 ∇Fp(W c p)‖22 ] ≤ EF (W0)− EF (WC+1) + (Lσ2Ps+ LF σ\n2P 2 ) ∑C c=0 η 2 c\n+G2L ∑C c=0 η 2 c−s\n(16)\nNow set η−1c = LF 2\n+2sL+ √ c. Then, the above inequality\nbecomes (ignoring some universal constants):∑C c=0 [ 1√ c E‖ ∑P p=1∇Fp(W c p)‖22 ] ≤ O (( (L+ LF )σ 2Ps )∑C\nc=0 1 c\n) .\n(17)\nSince ∑C c=0 1 c = o( ∑C c=0 1√ c ), we must have\nlim inf c→∞\nE‖ ∑P p=1∇Fp(W c p)‖ = 0 (18)\nproving the first claim. On the other hand, the bound of ‖Wc−Wcp‖ in (??) gives\n‖Wc −Wcp‖ ≤ ∑c−1 t=c−s ηt‖ ∑P q=1 |Sq| ∑ j∈Itq ∇fj(Wtq)‖+ ηc−sG\n(19) By assumption the sequences {Wcp}p,c and {Wc}c are bounded and the gradient of fj is continuous, thus∇fj(Wtq) is bounded. Now take c → ∞ in the above inequality and notice that lim c→∞ ηc = 0, we have lim c→∞ ‖Wc −Wcp‖ = 0 almost surely, proving the second claim. Lastly, the Lipschitz continuity of ∇Fp further implies\n0 = lim inf c→∞\nE‖ ∑P p=1∇Fp(W c p)‖ ≥ lim inf c→∞ E‖ ∑P p=1∇Fp(W\nc)‖ = lim inf\nc→∞ E‖∇F (Wc)‖ = 0\n(20) Thus there exists a common limit point of Wc,Wcp that is a stationary point almost surely. From (??) and use the\nestimate ∑C c=1 1 c ≈ logC, we have\nminc=1,...,C E [ ‖ ∑P p=1∇Fp(W c p)‖22 ] ≤ O ( (L+LF )σ 2Ps logC√ C ) .\n(21) The proof is now complete."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Matrix-parametrized models, including multiclass logistic<lb>regression and sparse coding, are used in machine learning<lb>(ML) applications ranging from computer vision to compu-<lb>tational biology. When these models are applied to large-<lb>scale ML problems starting at millions of samples and tens of<lb>thousands of classes, their parameter matrix can grow at an<lb>unexpected rate, resulting in high parameter synchroniza-<lb>tion costs that greatly slow down distributed learning. To<lb>address this issue, we propose a Sufficient Factor Broadcast-<lb>ing (SFB) computation model for efficient distributed learn-<lb>ing of a large family of matrix-parameterized models, which<lb>share the following property: the parameter update com-<lb>puted on each data sample is a rank-1 matrix, i.e. the outer<lb>product of two “sufficient factors” (SFs). By broadcast-<lb>ing the SFs among worker machines and reconstructing the<lb>update matrices locally at each worker, SFB improves com-<lb>munication efficiency — communication costs are linear in<lb>the parameter matrix’s dimensions, rather than quadratic —<lb>without affecting computational correctness. We present a<lb>theoretical convergence analysis of SFB, and empirically cor-<lb>roborate its efficiency on four different matrix-parametrized<lb>ML models.",
    "creator" : "LaTeX with hyperref package"
  }
}