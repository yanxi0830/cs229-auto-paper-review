{
  "name" : "1206.4651.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Is margin preserved after random projection?",
    "authors" : [ "Qinfeng Shi", "Chunhua Shen", "Anton van den Hengel" ],
    "emails" : [ "javen.shi@adelaide.edu.au", "chunhua.shen@adelaide.edu.au", "rhys.hill@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The margin separating classes of data is a key concept in many existing classification algorithms including support vector machines (SVMs) (Cortes & Vapnik, 1995; Crammer & Singer, 2001) and Boosting (Schapire & Freund, 1998). These classifiers are fundamentally involved in identifying and characterising such margins, and are described in terms of the accuracy and generality with which they do so.\nRandom projections have attracted much attention within a range of fields including signal processing (Donoho, 2006; Baraniuk et al., 2007), and clustering (Schulman, 2000), largely due to the fact that distances are preserved under such transformations in certain circumstances (Dasgupta & Gupta, 2002). Random projections have also been applied to classification for a variety of purposes (Balcan et al., 2006; Duarte et al., 2007; Shi et al., 2009a;b; 2010). However, whether margin is preserved has not been well studied.\nOur primary contributions here are to establish the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nconditions under which margins are preserved after random projection, and to show that error free margins are preserved for both binary and multiclass problems if these conditions are met. We also demonstrate that our results hold for one-parameter multiclass classification, which explains the approach used in Shi et al. (2009a;b).\nIn this vein we build upon the work of Balcan et al. (2006) which provided a lower bound on the number of dimensions required if a random projection was to have a given probability of maintaining half of the original margin in the data. Although an important step, Balcan et al. (2006) do not solve the problem because the resulting formulation demands infinite many projections in order to guarantee the preservation of an error free margin."
    }, {
      "heading" : "2. Motivation and Definitions",
      "text" : "A typical definition of margin for a binary classification problem is as follows.\nDefinition 1 (Margin) The dataset S = {(xi ∈ Rd, yi ∈ {−1,+1})}mi=1 is said linearly separable by margin γ if there exists a unit length u ∈ Rd such that for all (x, y) ∈ S,\ny〈u,x〉 ≥ γ.\nThe maximum (among all γ) smallest (among all data) margin is\nγ∗ = min (x,y)∈S y〈u,x〉. (1)\nUnfortunately this margin is not preserved after random projection, which we demonstrate by showing a counter-example, depicted in Figure 1. We construct a dataset of 4 data points (x1, y1), (x2, y2), (x3, y3), (x4, y4), where y1 = y2 = +1 and y3 = y4 = −1, and x1 = (−1, 1),x2 = (1, 1),x3 = (−1,−1),x3 = (1,−1). Let R ∈ R1,2 be a random matrix that maps x onto a 1 dimensional subspace. If the projected data R x are to be linearly separable then the subspace onto which they are projected (a line) must lie within the grey area between the two dashed lines in Figure 1(a). If R is chosen randomly from a uniform distribution then probability that the projected data is linearly separable (by a positive margin) is the angle separating the dashed lines divide by π. If we expand x along horizontal line (pushing x1 and x3 to the left, x2 and x4 to the right while keeping their vertical coordinates unchanged), the grey area angle shrinks as shown in Figure 1(b). In fact, if we push x to infinitely far away, the angle reduces to 0 while keeping the original margin γ unchanged. This means that there exist data with positive margin, such that the chance of the projected data being linearly separable is close to zero."
    }, {
      "heading" : "2.1. Error-allowed margin",
      "text" : "Balcan et al. (2006) studied the problem of margin preservation under random projection for binary classification. In doing so they derived a formula for the probability that a margin would be decreased by less than half under a particular projection. They provided two margin definitions below, using dataset S as in Definition 1 and data distribution D.\nDefinition 2 (Normalised Margin) A dataset S is linearly separable by margin γ if there exists u ∈ Rd, such that for all (x, y) ∈ S,\ny 〈u,x〉 ‖u ‖‖x ‖ ≥ γ.\nDefinition 3 (Error-allowed Margin) A data distribution D is linearly separable by margin γ with error ρ, if there exists u ∈ Rd, such that\nPr (x,y)∼D ( y 〈u,x〉 ‖u ‖‖x ‖ < γ ) ≤ ρ.\nDefinition 2 describes a normalised version of the more traditional margin, and it is this normalised version which we refer to as the margin henceforth.\nDefinition 3 describes a margin over a distribution rather than a dataset. Balcan et al. (2006) showed that if the original data has normalised margin γ then as long as the number of projections\nn ≥ c γ2 ln 1 ρδ , (2)\nfor an appropriate constant c, the projected data (now n dimensional) has margin γ/2 with error ρ, with probability at least 1 − δ. Definition 3 shows that a positive margin implies ρ = 0, which by (2) implies that n = +∞. Thus in order to preserve a positive margin in the projected data one needs infinitely many random projections."
    }, {
      "heading" : "3. Margin Distortion and Preservation",
      "text" : "In this section, we will establish the conditions for error free margin preservation. Extension to error allowed margin will be briefly discussed at the end of this section.\nIn order to give an indication that angles and margins might be preserved we first tackle the simpler question of whether the mean is preserved.\nLemma 4 (Mean preservation) For any w,x ∈ Rd, any random Gaussian matrix R ∈ Rn,d whose entries R(i, j) = 1√\nn rij where the rijs are i.i.d. random\nvariables from N(0, 1), we have\nE(〈R w,R x〉) = 〈w,x〉 . (3)\nProof of Lemma 4.\nE(〈R w,R x〉)\n= 1 n E [ n∑ `=1 ( d∑ j=1 r`jwj d∑ i=1 r`ixi )]\n= 1\nn n∑ `=1 ( d∑ j=1 E(r2`j)wjxj\n+ d∑ j=1 E(r`j)wj d∑ i 6=j:i=1 E(r`i)xi ) .\n= 〈w,x〉 .\nThe above is based solely on the fact that the {rij} are independent with zero mean and unit variance.\nThe fact that the mean is preserved is a necessary condition for margin preservation, although the following proves that angle and margin are preserved do not depend on it. Due to the 2-stability of the Gaussian distribution, we know that ∑d j=1 r`jwj = ‖w ‖z`\nand ∑d j=1 r`jxj = ‖x ‖z′`, where z` and z′` ∼ N(0, 1).\nWe thus see that 〈R w,R x〉 = 1n‖w ‖‖x ‖ ∑n `=1 z`z ′ `. There are two possible cases:\n• If w = x, then ∑n `=1 z 2 ` has a chi-square distribu-\ntion with n-degrees freedom. Applying chi-square distribution tail bound (Achlioptas, 2003) implies tight bounds on Pr ( ‖R x‖2 ≤ (1 − )‖x‖2 ) and\nPr ( ‖R x‖2 ≤ (1 + )‖x‖2 ) .\n• If w 6= x, ∑n `=1 z`z ′ ` is a sum of product normal\ndistributed variables. One approach might be to compute the variance using Chebyshev’s inequality, but the resulting bound would be very loose. Our first main result will show that if the angle between w and x is small, the inner product is well preserved.\nTheorem 5 (Angle preservation) For any w,x ∈ Rd, any random Gaussian matrix R ∈ Rn,d as defined above, for any ∈ (0, 1), if 〈w,x〉 > 0, then with probability at least\n1− 6 exp (−n 2 ( 2 2 − 3 3 )),\nthe following holds\n(1 + ) (1− ) 〈w,x〉 ‖w ‖‖x ‖ − 2 (1− ) ≤ 〈R w,R x〉 ‖R w‖‖R x‖\n≤ 1− √\n(1− 2) (1 + ) + (1 + ) + (1− ) (1 + ) 〈w,x〉 ‖w ‖‖x ‖ . (4)\nWith angle preservation, one can easily show inner product preservation by multiplying ‖R w‖‖R x‖ in (4) which, however, results in a looser bound.\nThis theorem is one of our main results, and underpins the analysis to follow. The proof is deferred to Section 4. This theorem shares similar insight as Magen (2007), in which Magen showed that random projections preserve volumes and distances to affine spaces. A similar result on inner product is obtained in Arriaga & Vempala (2006, Corollary 2).\nSince acute angles are provably preserved, we are now ready to see whether the margin is also preserved. Proving margin preservation requires proving\nthat there exists a parameter vector v ∈ Rn such that the dataset S after random projection can still be separated by a certain margin. The proof is achieved essentially by showing that R u is one such parameter vector v. Using angle preservation from Theorem 5 and the union bound yields the following theorem.\nTheorem 6 (Binary preservation) Given any random Gaussian matrix R ∈ Rn,d as defined above, if the dataset S = {(xi ∈ Rd, yi ∈ {−1,+1})}mi=1 is linearly separable by margin (the normalised margin in Definition 2) γ ∈ (0, 1], then for any δ, ∈ (0, 1) and any\nn > 12\n(3 2 − 2 3) ln\n6m\nδ ,\nwith probability at least 1−δ, the dataset S′ = {(R xi ∈ Rn, yi ∈ {−1,+1})}mi=1 is linearly separable by margin γ − 2 (1− ) .\nProof of Theorem 6. By definition, for all (x, y) ∈ S, there exists u, such that 〈u,y x〉‖u‖‖x‖ ≥ γ. Applying Theorem 5 and the union bound, we have\nPr ( ∃(x, y) ∈S, 〈R u, yR x〉\n‖R u ‖‖R x ‖ ≤ γ − 2 (1− ) ) ≤6m exp (−n\n2 ( 2 2 − 3 3 )).\nLet δ = 6m exp (−n2 ( 2 2 − 3\n3 )), then solving for n gives the required bound on n.\nNote that in Theorem 6, the lower bound of the margin after random projection can become negative for certain values of . A negative margin implies that the projected data are not linearly separable. Since it is a lower bound (not a upper bound), the implication in this case is only that the separability of the projected data can no longer be guaranteed with high probability.\nWhen the lower bound is positive, Theorem 6 indicates that margin separability for binary classification is preserved with high probability under random projection. This may seem at odds with our initial counter-example, but the difference lies in the distinction between Definition 1 and Definition 2. In the counter-example, pushing xs apart reduces the probability of achieving a separable projection (as indicated by the margin defined in Definition 1) to zero. However, during this process the margin as defined in Definition 2 is also shrinking to zero, and thus that it is only this diminished margin which need be preserved.\nWe now consider the multiclass case, and specifically the widely accepted definition of the multiclass margin from Crammer & Singer (2001). The straightforward\nmulticlass extension of the counter-example shown in Figure 1 shows that the multiclass margin is not always preserved under random projection. As in the binary classification case, we thus introduce the normalised multiclass margin.\nDefinition 7 (Normalised Multiclass Margin) The multiclass dataset S = {(xi ∈ Rd, yi ∈ Y = {1, . . . , L})}mi=1 is linearly separable by margin γ ∈ (0, 1], if there exists {uy ∈ Rd}y∈Y, such that for all (x, y) ∈ S\n〈uy,x〉 ‖uy ‖‖x ‖ −max y′ 6=y 〈uy′ ,x〉 ‖uy′ ‖‖x ‖ ≥ γ.\nFor the purposes of the discussion to follow we will assume the multiclass dataset S to be as in Definition 7.\nThe smallest maximum γ can be found via\nγ∗ = min (x,y)∈S ( 〈uy,x〉 ‖uy ‖‖x ‖ −max y′ 6=y 〈uy′ ,x〉 ‖uy′ ‖‖x ‖ ) . (5)\nAs we will see a special case of the above is used in the one-parameter method in Shi et al. (2009a;b) whereby uy = uy′ for all y, y\n′ ∈ Y. Thus the set {uy ∈ Rd}y∈Y reduces to a single vector.\nTheorem 8 (Multiclass margin preservation) For any multiclass dataset S and any Gaussian random matrix R, if S is linearly separable by margin γ ∈ (0, 1], then for any δ, ∈ (0, 1) and any\nn > 12\n(3 2 − 2 3) ln\n6Lm\nδ ,\nwith probability at least 1 − δ, the dataset S′ = {(R xi ∈ Rn, yi ∈ Y}mi=1 is linearly separable by margin\n− (1+3 )(1− 2) + √ (1− 2) (1+ ) + (1+ ) (1− )γ.\nProof of Theorem 8. By the margin definition, for all (x, y) ∈ S\n〈uy,x〉 ‖uy ‖‖x ‖ −max y′ 6=y 〈uy′ ,x〉 ‖uy′ ‖‖x ‖ ≥ γ.\nTake any single (x, y) ∈ S, we have by Theorem 5 and union bound that\nPr ( 〈R uy,R x〉 ‖R uy‖‖R x‖ ≥ 1− (1 + ) (1− ) (1− 〈uy,x〉 ‖uy‖‖x‖ ) )\n≥ 1− 6 exp (−n 2 ( 2 2 − 3 3 ))\nPr ( ∀y′ 6= y, 〈R uy\n′ ,R x〉 ‖R uy′‖‖R x‖\n≤ 1− √\n(1− 2) (1 + )\n+ (1 + ) + (1− ) (1 + ) 〈uy′ ,x〉 ‖uy′‖‖x‖ ) ≥ 1− 6(L− 1) exp (−n\n2 ( 2 2 − 3 3 )).\nBy the union bound, with probability at least 1 − 6Lm exp (−n2 ( 2 2 − 3 3 )), for all (x, y) ∈ S, we have\n〈R uy,R x〉 ‖R uy‖‖R x‖ −max y′ 6=y 〈R uy′ ,R x〉 ‖R uy′‖‖R x‖\n≥ √\n(1− 2) (1 + ) − (1 + 3 ) (1− 2) + (1 + ) (1− ) γ.\nIf we let δ = 6Lm exp (−n2 ( 2 2 − 3\n3 )), we get the required lower bound on n.\nNote the above result can be easily extended to the widely accepted definition of the multiclass unnormalised margin from Crammer & Singer (2001) by bounding the distortion on ‖R uy‖‖R x‖ and ‖R uy′‖‖R x‖. However, larger multiclass unnormalised margin does not mean smaller angle (i.e. larger normalised margin), thus may not provide tighter preservation.\nThe definition of the multiclass margin in Definition 7 assumes the existence of a set {uy ∈ Rd}y∈Y. We now consider whether the margin is preserved in the case where there exists only a single parameter vector u.\nTheorem 9 (One-parameter method) For any multiclass dataset S, and any random Gaussian matrix R, denote by Ry ∈ Rn,d the y-th sub-matrix of R, that is R = [R1, · · · ,Ry, · · · ,RL]. If S is linearly separable by margin γ ∈ (0, 1], then for any δ, ∈ (0, 1] and any\nn > 12\n(3 2 − 2 3) ln 6m(L− 1) δ ,\nthere exists a parameter vector v ∈ Rn, such that Pr ( ∀(x, y) ∈ S, ∀y′ 6= y,\n〈v,Ry x〉 − 〈v,Ry′ x〉 ‖v ‖ √ ‖Ry x ‖2 + ‖Ry′ x ‖2 ≥ −2 1− + 1 + √ 2L(1− ) γ ) ≥ 1− δ. (6)\nProof of Theorem 9. By the margin definition there exists {wy ∈ Rd}y∈Y, such that for all (x, y) ∈ S,\n〈wy,x〉 ‖wy‖‖x‖ − 〈wy ′ ,x〉 ‖wy′‖‖x‖ ≥ γ,∀y′ 6= y.\nWithout loss of generality we assume that wy has unit length1 for all y. So now\n〈wy,x〉 − 〈wy′ ,x〉 ≥ γ‖x‖,∀y′ 6= y. 1This can be achieved by normalisation.\nThis can be rewritten as\n〈u,x⊗ ey〉 − 〈u,x⊗ey′〉 = 〈x⊗ ey −x⊗ ey′ ,u〉 ≥ γ‖x‖,\nwhere u is a concatenation of all wy i.e. u = [wT1 , · · · ,wTy , · · ·wTL]T, ey is a vector ∈ RL with 1 at the y-th location and zeros in all others, and ⊗ is the tensor product. Define zx,y′ = x⊗ ey −x⊗ ey′ . Applying Theorem 5 to u and zx,y′ , we have for a given (x, y) and a fixed y′ 6= y, with probability at least 1−6 exp (−n2 ( 2 2 − 3 3 )), that the following holds,\n〈R u,R zx,y′〉 ‖R u ‖‖R zx,y′ ‖ ≥ 1− 1 + 1− (1− 〈x⊗ ey −x⊗ ey ′ ,u〉√ 2‖u ‖‖x ‖ )\n= 1− 1 + 1− + 1 + √ 2(1− ) ( 〈wy,x〉 ‖u ‖‖x ‖ − 〈wy ′ ,x〉 ‖u ‖‖x ‖ )\n≥ 1− 1 + 1− + 1 + √ 2L(1− ) γ = −2 1− + 1 + √ 2L(1− ) γ\nBy the union bound over m samples and the L− 1 y′s\nPr ( ∃(x, y) ∈ S, ∃y′ 6= y,\n〈R u,R zx,y′〉 ‖R u ‖‖R zx,y′ ‖ < −2 1− + 1 + √ 2L(1− ) γ )\n≤ 6m(L− 1) exp (−n 2 ( 2 2 − 3 3 )).\nLetting v = R u, we have\n〈R u,R zx,y′〉 = 〈v,Ry x−Ry′ x〉 .\nSetting δ = 6m(L− 1) exp (−n2 ( 2 2 − 3\n3 )) gives the required bound on n.\nTheorem 9 shows the existence of a parameter vector under which the margin is preserved up to an order O(γ/ √ 2L) term where n increases logarithmically with L. The gain is thus that the memory requirement is independent of the number of classes.\nError allowed margin The error free margin preservation results presented above apply only to linearly separable data. Real data are often not linearly separable, but the result applies none the less to any linearly separable subset of the data. Given that what has been developed is a theoretical result intended to guide the selection of an appropriate projection dimension the fact that it applies to every linearly separable subset of the data is likely to suffice in most cases. Our\nresults can be easily extended to error allowed margin, however, in both binary and multiclass cases, by the addition of a controllable tolerance . The probability of preserving the error allowed margin can be bounded below by bounding above the chance of the subset of data being not linearly separable (thus error allowed) and the chance of the complement subset being linearly separable under projection."
    }, {
      "heading" : "4. Angle Preservation",
      "text" : "To prove angle preservation as in Theorem 5, we will use the following tail bound, which also appears in a different form in the simplified proof of the JohnsonLindestrauss Lemma in Dasgupta & Gupta (2002).\nLemma 10 (Tail bound) For any x ∈ Rd, any random Gaussian matrix R ∈ Rn,d as defined above, for any ∈ (0, 1),\nPr ( (1− ) ≤ ‖R x‖ 2\n‖x ‖2 ≤ (1 + ) ) ≥ 1− 2 exp (−n\n2 ( 2 2 − 3 3 )).\nProof of Theorem 5. From Lemma 10 and the union bound, we know that\n(1− ) ≤ ‖R x‖ 2 ‖x ‖2 ≤ (1 + ), (1− ) ≤ ‖R w‖ 2 ‖w ‖2 ≤ (1 + )\n(7)\nholds with probability at least 1−4 exp (−n2 ( 2 2 − 3\n3 )). When (7) holds, due to the fact that increasing the length of two unit length vectors (i.e. from Rx‖Rx ‖ and Rw‖Rw ‖ to Rx√ (1− )‖x ‖ and Rw√ (1− )‖w ‖ ) increases the norm of their difference2, we have\n‖ R x ‖R x ‖ − R w ‖R w ‖ ‖2\n≤‖ R x√ (1− )‖x ‖ − R w√ (1− )‖w ‖ ‖2. (8)\nWe thus see that\n‖ R x ‖x ‖ − R w ‖w ‖ ‖2\n≤‖ √\n(1− ) R x ‖R x ‖\n− √ (1 + ) R w\n‖R w ‖ ‖2\n≤‖ √ (1 + )( R x\n‖R x ‖ − R w ‖R w ‖ )‖2\n+ ( √ (1 + )− √\n(1− ))2. (9) 2Note that the opposite does not hold in general.\nThe first inequality is due to (7), the second inequality is a property of any acute angle. Applying Lemma 10 to the vector ( x‖x ‖ − w ‖w ‖ ), we see that\n(1− )‖ x ‖x ‖ − w ‖w ‖ ‖2 ≤ ‖ R x ‖x ‖ − R w ‖w ‖ ‖2\n≤ (1 + )‖ x ‖x ‖ − w ‖w ‖ ‖2 (10)\nholds with a given probability.\nLetting β denote the angle between w and x we have\nγ = 〈w,x〉 ‖w‖‖x‖ = cos(β) = 1− 2 sin2(β 2 )\n= 1− 1 2 ‖ x ‖x ‖ − w ‖w ‖ ‖2. (11)\nSimilarly\n〈R w,R x〉 ‖R w‖‖R x‖ = 1− 1 2 ‖ R x ‖R x ‖ − R w ‖R w ‖ ‖2. (12)\nUsing (10), (8) and (9) we see that ‖ Rx‖Rx ‖ − Rw ‖Rw ‖‖ 2 is bounded below and above by two terms involving ‖ x‖x ‖ − w ‖w ‖‖ 2. Plugging (11) and (12) into the two side bounds, we get (4). Here we have applied Lemma 10 to 3 vectors, namely x, w, and ( x‖x ‖ − w ‖w ‖ ), thus by the union bound, the probability that the above holds is at least 1− 6 exp (−n2 ( 2 2 − 3 3 )).\nFor completeness, we show the proof of Lemma 10 below.\nProof of Lemma 10. By Lemma 4 and letting w = x, we have E(‖R x ‖2) = ‖x ‖2. Due to the 2-stability of the Gaussian distribution, we know∑d j=1 r`jxj = ‖x ‖z`, where z` ∼ N(0, 1). We thus\nhave ‖R x ‖2 = 1n‖x ‖ 2 ∑n `=1 z 2 ` . Here ∑n `=1 z 2 ` is chisquare distributed with n-degrees of freedom. Applying the standard tail bound of the chi-square distribution, we have\nPr ( ‖R x ‖2 ≤ (1− )‖x ‖2 ) ≤ exp (n 2 (1− (1− ) + ln(1− )) ) ≤ exp (−n 4 2).\nHere we used the inequality ln(1 − ) ≤ − − 2/2. Similarly, we have\nPr ( ‖R x ‖2 ≤ (1 + )‖x ‖2 ) ≤ exp (n 2 (1− (1 + ) + ln(1 + )) ) ≤ exp (−n 2 ( 2 2 − 3 3 )).\nHere we used the inequality ln(1+ ) ≤ − 2/2+ 3/3."
    }, {
      "heading" : "5. Experiments",
      "text" : "The experiments detailed below offer an empirical validation in support of the theoretical analysis above, and a demonstration of its application to SVMs."
    }, {
      "heading" : "5.1. Angle and inner product preservation condition",
      "text" : "Figure 2 shows the results of simulations whereby we randomly generate two vectors w and x ∈ Rd, d = 300. We then generate 2, 000 random Gaussian matrices of the form specified in Theorem 5. Each such matrix is used to project the data into n dimensions where n = {30, 60, 90, . . . , 300}. We vary ∈ {0.1, 0.3} and compute the empirical rejection probability for angle preservation\nP1 = 1−Pr (\n(1− ) ≤ 〈R w,R x〉 ‖w ‖‖x ‖ ‖R w‖‖R x‖〈w,x〉\n< (1+ ) ) ,\nand the empirical rejection probability for the inner product preservation\nP2 = 1− Pr (\n(1− ) ≤ 〈R w,R x〉 〈w,x〉\n≤ (1 + ) ) .\nThe simulations cover two cases, first where the vectors x and w are separated by an acute angle (i.e. γ = 〈w,x〉 ‖w ‖‖x ‖ > 0), and second where the angle is obtuse (i.e. γ < 0).\nIn the acute angle (γ > 0) case we generate two pairs of vectors with 〈w1,x1〉 = 0.827 and 〈w2,x2〉 = 0.527 and plot the empirical rejection probability in Figure 2. In Figure 2(a), we can clearly see that the rejection probability more rapidly approaches zero for {w1,x1} than {w2,x2}. This aligns with the theoretical analysis above in that we we would expect a more acute angle to imply a better angle preservation under random projection, and thus that fewer projections would be required to achieve a reasonable separation. It is also expected that the rejection probability decreases as and n increase.\nLikewise, the inner product is preserved under random projection if the angle is acute as is visible in Figure 2(b). It is interesting to see that for the same γ, and n, the empirical rejection probability for the angle preservation is significantly smaller than that for inner product preservation.\nIn the obtuse angle (i.e. γ < 0) case, we generate two pairs of vectors with 〈w3,x3〉) = −0.062 and 〈w4,x4〉) = −0.0165 and plot the empirical rejection probability in Figure 2 (c) and (d). Clearly the empirical rejection probability does not shrink towards zero, thus both the angle and the inner product are not preserved."
    }, {
      "heading" : "5.2. Margin preservation",
      "text" : "Margins We generated L parallel hyperplanes, where the L is the number of classes. Each class consists of 5 data points x ∈ R100 from a hyperplane. We then generated 100 random Gaussian matrices. We used the random matrices to project the data, and then computed both the normalised margin and unnormalised margin. The empirical rejection probability\nP = 1− Pr ( (1− ) ≤ γ ′\nγ < (1 + )\n) ,\nwhere γ′ is the new margin and γ is the original margin, was also computed. We show the plots for both binary and multiclass (L = 3) cases. As we can see in Figure 3, that the empirical rejection probability decreases ( i.e. margins are preserved with higher probability) as the number of projection n increases.\nImplications for SVMs As has been shown, the results above can be applied even in the case where the data are linearly inseparable, as is often the case in real classification problems. Testing of this method shows that it exhibits a smaller testing error on in the TiCC handwritten digit dataset (van der Maaten,\n2009), for example, than the multiclass SVM (Crammer & Singer, 2001) algorithm in liblinear (Fan et al., 2008). We conjecture that this is due to the significantly reduced dimensionality specifically as a result of the application of the one-parameter method. Projecting features to a lower dimensional space can significantly reduce the model capacity such as VC dimension (Vapnik, 1995). Thus the consequent generalisation bounds can be reduced if the margin preservation is good."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have provided an analysis of margin distortion under random projections, described the conditions under which margins are preserved, and given bounds on the margin distortion. We have shown particularly that margin preservation is closely related to acute angle (cosine) preservation and inner product preservation. In doing so we saw that the smaller acute angle, the better the preservation of the angle and the inner product. When the angle is well preserved, the margin is well preserved too. Because of this, the normalised margin is more informative than the unnormalised margin. We have also provided a theoretical underpinning for classification methods which use random projection to achieve multiclass classification with a single model parameter vector.\nIn contrast to previous work in the area (Balcan et al., 2006) we have shown that it is possible to provide bounds on error free margin preservation without requiring an infinite number of projections, and have done so for arbitrary tolerances, rather than only for half of the original margin. In addition, all of the above has been achieved for multiclass rather than solely binary classifiers. It is worth pointing out that our error free margin is defined on a dataset as traditional margin concepts whereas Balcan et al. (2006)’s error allowed margin is defined on a data distribution.\nThough we only showed results for random Gaussian matrix, similar bounds can be achieved for subGaussian distribution as long as a tail bound similar to Lemma 10 holds (see Achlioptas, 2003).\nThe bounds derived above are conservative, however, as they are based on the union bound over all of the data. The margin is primarily determined by the data on the boundary, however. Even a small distortion of data near the boundary may change the margin significantly, whereas distortion of the data far from the boundary is far less likely to do so. It thus seems likely that the margin bound can be further tightened by taking into account the data distribution."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the Australian Research Council DECRA grant DE120101161. We thank Anders Eriksson for discussion on the counter-example, and Maria-Florina Balcan and Avrim Blum for discussion on error-allowed margin."
    } ],
    "references" : [ {
      "title" : "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
      "author" : [ "D. Achlioptas" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Achlioptas,? \\Q2003\\E",
      "shortCiteRegEx" : "Achlioptas",
      "year" : 2003
    }, {
      "title" : "An algorithmic theory of learning: Robust concepts and random projection",
      "author" : [ "Arriaga", "Rosa I", "Vempala", "Santosh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Arriaga et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Arriaga et al\\.",
      "year" : 2006
    }, {
      "title" : "Kernels as features: On kernels, margins, and low-dimensional mappings",
      "author" : [ "Balcan", "M.-F", "A. Blum", "S. Vempala" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2006
    }, {
      "title" : "A simple proof of the restricted isometry principle for random matrices",
      "author" : [ "R.G. Baraniuk", "M. Davenport", "R. DeVore", "M.B. Wakin" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Baraniuk et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Baraniuk et al\\.",
      "year" : 2007
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Crammer and Singer,? \\Q2001\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2001
    }, {
      "title" : "An elementary proof of a theorem of johnson and lindenstrauss",
      "author" : [ "S. Dasgupta", "A. Gupta" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "Dasgupta and Gupta,? \\Q2002\\E",
      "shortCiteRegEx" : "Dasgupta and Gupta",
      "year" : 2002
    }, {
      "title" : "Multiscale random projections for compressive classification",
      "author" : [ "M.F. Duarte", "M.A. Davenport", "M.B. Wakin", "J.N. Laska", "D. Takhar", "K.F. Kelly", "R.G. Baraniuk" ],
      "venue" : "In Proc. IEEE Int. Conf. Image Processing,",
      "citeRegEx" : "Duarte et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Duarte et al\\.",
      "year" : 2007
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Fan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications",
      "author" : [ "A. Magen" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "Magen,? \\Q2007\\E",
      "shortCiteRegEx" : "Magen",
      "year" : 2007
    }, {
      "title" : "Boosting the margin: a new explanation for the effectiveness of voting methods",
      "author" : [ "R.E. Schapire", "Y. Freund" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Schapire and Freund,? \\Q1998\\E",
      "shortCiteRegEx" : "Schapire and Freund",
      "year" : 1998
    }, {
      "title" : "Clustering for edge-cost minimization",
      "author" : [ "L.J. Schulman" ],
      "venue" : "In Proc. Annual ACM Symp. Theory of Computing,",
      "citeRegEx" : "Schulman,? \\Q2000\\E",
      "shortCiteRegEx" : "Schulman",
      "year" : 2000
    }, {
      "title" : "S.V.N. Hash kernels for structured data",
      "author" : [ "Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A.J. Smola", "Vishwanathan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Shi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2009
    }, {
      "title" : "Rapid face recognition using hashing",
      "author" : [ "Q. Shi", "H. Li", "C. Shen" ],
      "venue" : "In Proc. IEEE Conf. Computer Vision & Pattern Recognition, San Francisco,",
      "citeRegEx" : "Shi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2010
    }, {
      "title" : "A new benchmark dataset for handwritten character recognition",
      "author" : [ "L.J.P. van der Maaten" ],
      "venue" : "Technical Report TiCC TR 2009-002,",
      "citeRegEx" : "Maaten,? \\Q2009\\E",
      "shortCiteRegEx" : "Maaten",
      "year" : 2009
    }, {
      "title" : "The Nature of Statistical Learning",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik,? \\Q1995\\E",
      "shortCiteRegEx" : "Vapnik",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Random projections have attracted much attention within a range of fields including signal processing (Donoho, 2006; Baraniuk et al., 2007), and clustering (Schulman, 2000), largely due to the fact that distances are preserved under such transformations in certain circumstances (Dasgupta & Gupta, 2002).",
      "startOffset" : 102,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : ", 2007), and clustering (Schulman, 2000), largely due to the fact that distances are preserved under such transformations in certain circumstances (Dasgupta & Gupta, 2002).",
      "startOffset" : 24,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Random projections have also been applied to classification for a variety of purposes (Balcan et al., 2006; Duarte et al., 2007; Shi et al., 2009a;b; 2010).",
      "startOffset" : 86,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "Random projections have also been applied to classification for a variety of purposes (Balcan et al., 2006; Duarte et al., 2007; Shi et al., 2009a;b; 2010).",
      "startOffset" : 86,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "In this vein we build upon the work of Balcan et al. (2006) which provided a lower bound on the number of dimensions required if a random projection was to have a given probability of maintaining half of the original margin in the data.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "In this vein we build upon the work of Balcan et al. (2006) which provided a lower bound on the number of dimensions required if a random projection was to have a given probability of maintaining half of the original margin in the data. Although an important step, Balcan et al. (2006) do not solve the problem because the resulting formulation demands infinite many projections in order to guarantee the preservation of an error free margin.",
      "startOffset" : 39,
      "endOffset" : 286
    }, {
      "referenceID" : 2,
      "context" : "Balcan et al. (2006) showed that if the original data has normalised margin γ then as long as the number of projections n ≥ c γ2 ln 1 ρδ , (2)",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Applying chi-square distribution tail bound (Achlioptas, 2003) implies tight bounds on Pr ( ‖R x‖ ≤ (1 − )‖x‖ ) and",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "This theorem shares similar insight as Magen (2007), in which Magen showed that random projections preserve volumes and distances to affine spaces.",
      "startOffset" : 39,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "2009), for example, than the multiclass SVM (Crammer & Singer, 2001) algorithm in liblinear (Fan et al., 2008).",
      "startOffset" : 92,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "Projecting features to a lower dimensional space can significantly reduce the model capacity such as VC dimension (Vapnik, 1995).",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "In contrast to previous work in the area (Balcan et al., 2006) we have shown that it is possible to provide bounds on error free margin preservation without requiring an infinite number of projections, and have done so for arbitrary tolerances, rather than only for half of the original margin.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "In contrast to previous work in the area (Balcan et al., 2006) we have shown that it is possible to provide bounds on error free margin preservation without requiring an infinite number of projections, and have done so for arbitrary tolerances, rather than only for half of the original margin. In addition, all of the above has been achieved for multiclass rather than solely binary classifiers. It is worth pointing out that our error free margin is defined on a dataset as traditional margin concepts whereas Balcan et al. (2006)’s error allowed margin is defined on a data distribution.",
      "startOffset" : 42,
      "endOffset" : 533
    } ],
    "year" : 2012,
    "abstractText" : "Random projections have been applied in many machine learning algorithms. However, whether margin is preserved after random projection is non-trivial and not well studied. In this paper we analyse margin distortion after random projection, and give the conditions of margin preservation for binary classification problems. We also extend our analysis to margin for multiclass problems, and provide theoretical bounds on multiclass margin on the projected data.",
    "creator" : "LaTeX with hyperref package"
  }
}