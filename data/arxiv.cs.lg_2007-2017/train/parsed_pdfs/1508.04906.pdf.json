{
  "name" : "1508.04906.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised Learning with Regularized Laplacian",
    "authors" : [ "K. Avrachenkov", "P. Chebotarev", "A. Mishenin" ],
    "emails" : [ "k.avrachenkov@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 8.\n04 90\n6v 1\n[ cs\n.L G\n] 2\n0 A\nug 2\n01 5\nIS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 87\n65 --\nF R\n+ E\nN G\nRESEARCH REPORT\nN° 8765 July 2015\nProject-Team Maestro"
    }, {
      "heading" : "Semi-supervised",
      "text" : ""
    }, {
      "heading" : "Learning with",
      "text" : ""
    }, {
      "heading" : "Regularized Laplacian",
      "text" : "K. Avrachenkov, P. Chebotarev, A. Mishenin\nRESEARCH CENTRE SOPHIA ANTIPOLIS – MÉDITERRANÉE\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nSemi-supervised Learning with\nRegularized Laplacian\nK. Avrachenkov∗, P. Chebotarev†, A. Mishenin‡ §\nProject-Team Maestro\nResearch Report n° 8765 — July 2015 — 19 pages\nAbstract: We study a semi-supervised learning method based on the similarity graph and Regularized Laplacian. We give convenient optimization formulation of the Regularized Laplacian method and establish its various properties. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods.\nKey-words: Semi-supervised learning, Graph-based learning, Regularized Laplacian, Proximity measure, Wikipedia article classification\n∗ Corresponding author. K. Avrachenkov is with Inria Sophia Antipolis, 2004 Route des Lucioles, 06902, Sophia Antipolis, France k.avrachenkov@inria.fr\n† P. Chebotarev is with Trapeznikov Institute of Control Sciences of the Russian Academy of Sciences, 65 Profsoyuznaya Str., Moscow, 117997, Russia\n‡ A. Mishenin is with St. Petersburg State University, Faculty of Applied Mathematics and Control Processes, Peterhof, 198504, Russia\n§ This work was partially supported by Campus France, Alcatel-Lucent Inria Joint Lab, EU Project Congas FP7-ICT-2011-8-317672, and RFBR grant No. 13-07-00990."
    }, {
      "heading" : "L’Apprentissage Semi-supervisé avec Laplacian Régularisé",
      "text" : "Résumé : Nous étudions une méthode d’apprentissage semi-supervisé, basé sur le graphe de similarité et Laplacian régularisé. Nous formalisons la méthode comme un problème d’optimisation convexe et quadratique et nous établissons ses diverses propriétés. En particulier, nous montrons que le noyau de la méthode peut être interprété en termes des marches aléatoires en temps discret et continu et possède plusieurs propriétés importantes des mesures de proximité. Les techniques d’optimisation ainsi que les techniques d’algébre linéaire peuvent être utilisé pour un calcul efficace des fonctions de classification. Nous démontrons sur des exemples numériques que la méthode de Laplacian régularisé est concurrentiel par rapport aux autres état de l’art méthodes d’apprentissage semi-supervisé.\nMots-clés : Apprentissage Semi-supervisé, Apprentissage basé sur le graphe de similarité, Laplacian régularisé, mesure de proximité, classification des articles Wikipedia"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 3",
      "text" : ""
    }, {
      "heading" : "1 Introduction",
      "text" : "Graph-based semi-supervised learning methods have the following three principles at their foundation. The first principle is to use a few labelled points (points with known classification) together with the unlabelled data to tune the classifier. In contrast with the supervised machine learning, the semi-supervised learning creates a synergy between the training data and classification data. This drastically reduces the size of the training set and hence significantly reduces the cost of experts’ work. The second principal idea of the semi-supervised learning methods is to use a (weighted) similarity graph. If two data points are connected by an edge, this indicates some similarity of these points. Then, the weight of the edge, if present, reflects the degree of similarity. The result of classification is given in the form of classification functions. Each class has its own classification function defined over all data points. An element of a classification function gives a degree of relevance to the class for each data point. Then, the third principal idea of the semi-supervised learning methods is that the classification function should change smoothly over the similarity graph. Intuitively, nodes of the similarity graph that are closer together in some sense are more likely to belong to the same class. This idea of classification function smoothness can naturally be expressed using graph Laplacian or its modification.\nThe work [37] seems to be the first work where the graph-based semi-supervised learning was introduced. The authors of [37] formulated the semi-supervised learning method as a constrained optimization problem involving graph Laplacian. Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian. In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36]. In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5]. We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning. An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].\nIn the present work we study in detail a semi-supervised learning method based on the Regularized Laplacian. To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15]. In [23] the authors compared experimentally many graph-based semi-supervised learning methods on several datasets and their conclusion was that the semisupervised learning method based on the Regularized Laplacian kernel demonstrates one of the best performances on nearly all datasets. In [8] the authors studied a semi-supervised learning method based on the Normalized Laplacian graph kernel which also shows good performance. Interestingly, as we show below, if we choose Markovian Laplacian as a weight matrix, several known semi-supervised learning methods reduce to the Regularized Laplacian method. In this work we formulate the Regularized Laplacian method as a convex quadratic optimization problem which helps to design easily parallelizable numerical methods. In fact, the Regularized Laplacian method can be regarded as a Lagrangian relaxation of the method proposed in [37]. Of course, this is a more flexible formulation, since by choosing an appropriate value for the Lagrange multiplier one can always retrieve the method of [37] as a particular case. We establish various properties of the Regularized Laplacian method. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We discuss advantages and disadvantages of various numerical approaches. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods.\nRR n° 8765"
    }, {
      "heading" : "4 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : "The paper is organized as follows: In the next section we formally define the Regularized Laplacian method. In Section 3 we discuss several related graph-based semi-supervised methods and graph kernels. In Section 4 we present insightful interpretations and properties of the Regularized Laplacian method. We analyse important limiting cases in Section 5. Then, in Section 6 we discuss various numerical approaches to compute the classification functions and show by numerical examples that the performance of the Regularized Laplacian method is better or comparable with the leading semi-supervised methods. Section 7 concludes the paper with directions for future research."
    }, {
      "heading" : "2 Notations and method formulation",
      "text" : "Suppose one needs to classify N data points (nodes) into K classes and assume P data points are labelled. That is, we know the class to which each labelled point belongs. Denote by Vk the set of labelled points in class k = 1, ...,K. Of course, |V1|+ ...+ |VK | = P .\nThe graph-based semi-supervised learning approach uses a weighted graph G = (V,A) connecting data points, where V , |V | = N , denotes the set of nodes and A denotes the weight (similarity) matrix. In this work we assume that A is symmetric and the underlying graph is connected. Each element aij represents the degree of similarity between data points i and j. Denote by D the diagonal matrix with its (i, i)-element equal to the sum of the i-th row of matrix A: di = ∑N\nj=1 aij . We denote by L = D − A the Standard (Combinatorial) Laplacian associated with the graph G.\nDefine an N ×K matrix Y as\nYik =\n{\n1, if i ∈ Vk, i.e., point i is labelled as a class k point,\n0, otherwise.\nWe refer to each column Y∗k of matrix Y as a labeling function. Also define an N ×K matrix F and call its columns F∗k classification functions. The general idea of the graph-based semisupervised learning is to find classification functions so that on the one hand they are close to the corresponding labeling function and on the other hand they change smoothly over the graph associated with the similarity matrix. This general idea can be expressed by means of the following particular optimization problem:\nmin F\n{\nK ∑\nk=1\n(F∗k − Y∗k) T (F∗k − Y∗k) + β\nK ∑\nk=1\nFT∗kLF∗k\n}\n, (1)\nwhere β ∈ (0,∞) is a regularization parameter. The regularization parameter β represents a trade-off between the closeness of the classification function to the labeling function and its smoothness.\nSince the Laplacian L is positive-semidefinite and the second term in (1) is strictly convex, the optimization problem (1) has a unique solution determined by the stationarity condition\n2(F∗k − Y∗k) T + 2βFT∗kL = 0, k = 1, ...,K,\nwhich gives F∗k = (I + βL) −1Y∗k, k = 1, ...,K. (2)\nThe matrix Qβ = (I + βL) −1 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1]. The classification functions F∗k, k = 1, ...,K, can be obtained either by numerical linear algebra methods\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 5",
      "text" : "(e.g., power iterations) applied to (2) or by numerical optimization methods applied to (1). We elaborate on numerical methods in Section 6. Once the classification functions are obtained, the points are classified according to the rule\nFik > Fik′ , ∀k ′ 6= k ⇒ Point i is classified into class k.\nThe ties can be broken in arbitrary fashion."
    }, {
      "heading" : "3 Related approaches",
      "text" : "Let us discuss a number of related approaches. First, we discuss formal relations and in the numerical examples section we compare the approaches on some benchmark examples."
    }, {
      "heading" : "3.1 Relation to heat kernels",
      "text" : "The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian. Specifically, they introduced the kernel\nH(t) = exp(−tL), (3)\nwhere L = D−1/2LD−1/2\nis the normalized Laplacian. Let us refer to H(t) as the normalized heat kernel. Note that the normalized heat kernel can be obtained as a solution of the following differential equation\nḢ(t) = −LH(t),\nwith the initial condition H(0) = I. Then, in [19] the PageRank heat kernel was introduced\nΠ(t) = exp(−t(I − P )), (4)\nwhere P = D−1A, (5)\nis the transition probability matrix of the standard random walk on the graph. In [20] the PageRank heat kernel was applied to local graph partitioning.\nIn [28] the heat kernel based on the standard Laplacian\nH(t) = exp(−tL), (6)\nwith L = D − A, was proposed as a kernel in the support vector machine learning method. Then, in [37] the authors proposed a semi-supervised learning method based on the solution of a heat diffusion equation with Dirichlet boundary conditions. Equivalently, the method of [37] can be viewed as the minimization of the second term in (1) with the values of the classification functions F∗k fixed on the labelled points. Thus, the proposed approach (1) is more general as it can be viewed as a Lagrangian relaxation of [37]. The results of the method in [37] can be retrieved with a particular choice of the regularization parameter.\nRR n° 8765"
    }, {
      "heading" : "6 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : ""
    }, {
      "heading" : "3.2 Relation to the generalized semi-supervised learning method",
      "text" : "In [4] the authors proposed a generalized optimization framework for graph based semi-supervised learning methods\nmin F\n\n\n\nN ∑\ni=1\nN ∑\nj=1\nwij‖di σ−1Fi∗ − dj σ−1Fj∗‖ 2 + µ\nN ∑\ni=1\ndi 2σ−1‖Fi∗ − Yi∗‖ 2\n\n\n\n, (7)\nwhere wij are the entries of a weight matrix W = (wij) which is a function of A (in particular, one can also take W = A).\nIn particular, with σ = 1 we retrieve the transductive semi-supervised learning method [35], with σ = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with σ = 0 we retrieve the PageRank based method [3].\nThe classification functions of the generalized graph based semi-supervised learning are given by\nF∗k = µ\n2 + µ\n(\nI − 2\n2 + µ D−σWDσ−1\n)−1\nY∗k, k = 1, ...,K.\nNow taking as the weight matrix W = I − τL = I − τ(D−A) (note that with this choice of the weight matrix, the generalized degree matrix D′ = diag(W1) becomes the identity matrix), the above equation transforms to\nF∗k =\n(\nI + 2τ\nµ L\n)−1\nY∗k, k = 1, ...,K,\nwhich is (2) with β = 2τ/µ. It is very interesting to observe that with the proposed choice of the weight matrix all the semi-supervised learning methods defined by various σ’s coincide."
    }, {
      "heading" : "4 Properties and interpretations of the Regularized Lapla-",
      "text" : "cian method\nThere is a number of interesting interpretations and characterizations which we can provide for the classification functions (2). These interpretations and characterizations will give different insights about the Regularized Laplacian kernel Qβ and the classification functions (2)."
    }, {
      "heading" : "4.1 Discrete-time random walk interpretation",
      "text" : "The Regularized Laplacian kernel Qβ = (I + βL) −1 can be interpreted as the overall transition matrix of a random walk on the similarity graph G with a geometrically distributed number of steps. Namely, consider a Markov chain whose states are our data points and the probabilities of transitions between distinct states are proportional to the corresponding entries of the similarity matrix A:\np̂ij = τaij , i, j = 1, . . . , N, i 6= j, (8)\nwhere τ > 0 is a sufficiently small parameter. Then the diagonal elements of the transition matrix P̂ = (p̂ij) are\np̂ii = 1− ∑\nj 6=i\nτaij , i = 1, . . . , N (9)\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 7",
      "text" : "or, in the matrix form,\nP̂ = I − τL. (10)\nThe matrix P̂ determines a random walk on G which differs from the “standard” one defined by (5) and related to the PageRank heat kernel (4). As distinct from (5), the transition matrix (10) is symmetric for every undirected graph; in general, it has a nonzero diagonal. It is interesting to observe that P̂ coincides with the weight matrix W used for transformation of Subsection 3.2.\nConsider a sequence of independent Bernoulli trials indexed by 0, 1, 2, . . . with a certain success probability q. Assume that the number of steps, K, in a random walk is equal to the trial number of the first success. And let Xk be the state of the Markov chain at step k. Then, K is distributed geometrically:\nPr{K = k} = q(1 − q)k, k = 0, 1, 2, . . . ,\nand the transition matrix of the overall random walk after a random number of stepsK, Z = (zij), zij = Pr{XK = j | X0 = i}, i, j = 1, . . . , N, is given by\nZ = q ∞ ∑\nk=0\n(1 − q)kP̂ k = q ∞ ∑\nk=0\n(1− q)k(I − τL)k\n= q (I − (1 − q)(I − τL)) −1 = ( I + τ(q−1 − 1)L )−1 .\nThus, Z = Qβ = (I + βL) −1 with β = τ(q−1 − 1).\nThis means that the i-th component of the classification function can be interpreted as the probability of finding the discrete-time random walk with transition matrix (10) in node i after the geometrically distributed number of steps with parameter q, given the random walk started with the distribution Y∗k/(1 TY∗k)."
    }, {
      "heading" : "4.2 Continuous-time random walk interpretation",
      "text" : "Consider the differential equation\nḢ(t) = −LH(t), (11)\nwith the initial condition H(0) = I. Also consider the standard continuous-time random walk that spends exponentially distributed time in node k with the expected duration 1/dk and after the exponentially distributed time moves to a new node l with probability akl/dk. Then, the solution hij(t) = exp(−tL) of the differential equation (11) can be interpreted as a probability to find the standard continuous-time random walk in node j given the random walk started from node i. By taking the Laplace transform of (11) we obtain\nH(s) = (sI + L)−1 = s−1(I + s−1L)−1. (12)\nThus, the classification function (2) can be interpreted as the Laplace transform divided by 1/s, or equivalently the i-th component of the classification function can be interpreted as a quantity proportional to the probability of finding the random walk in node i after exponentially distributed time with mean β = 1/s given the random walk started with the distribution Y∗k/(1 TY∗k).\nRR n° 8765"
    }, {
      "heading" : "8 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : ""
    }, {
      "heading" : "4.3 Proximity and distance properties",
      "text" : "As before, let Qβ=(q β ij)N×N be the Regularized Laplacian kernel (I + βL) −1 of (2).\nQβ determines a positive 1-proximity measure [14] s(i, j) := q β ij , i.e., it satisfies [13] the\nfollowing conditions: (1) for any i ∈ V, ∑\nk∈V q β ik = 1 and\n(2) for any i, j, k ∈ V, qβji + q β jk − q β ik ≤ q β jj with a strict inequality whenever i = k and i 6= j\n(the triangle inequality for proximities).\nThis implies [14] the following two important properties: (a) qβii > q β ij for all i, j ∈ V such\nthat i 6= j (egocentrism property); (b) ρβij := β(q β ii + q β jj − q β ij − q β ji) is 1 a distance on V. Because of the forest interpretation of Qβ (see Section 4.4), it is called the adjusted forest distance. The distances ρβij have a twofold connection with the resistance distance ρ̃ij on G [16]. First, limβ→∞ ρ β ij = ρ̃ij , i, j ∈ V. Second, let G β be the weighted graph such that: V (Gβ) = V (G)∪{0}, the restriction of Gβ to V (G) coincides with G, and Gβ additionally contains an edge (i, 0) of weight 1/β for each node i ∈ V (G). Then it follows that ρβij(G) = ρ̃ij(G β), i, j ∈ V. In the electrical interpretation of G, the weight 1/β of the edges (i, 0) is treated as conductivity, i.e., the lines connecting each node to the “hub” 0 have resistance β. An interested reader can find more properties of the proximity measures determined by Qβ in [13].\nFurthermore, every Qβ, β > 0 determines a transitional measure on V, which means [12] that:\nqβij q β jk ≤ q β ik q β jj for all i, j, k ∈ V with q β ij q β jk = q β ik q β jj if and only if every path in G from i to k visits j.\nIt follows that dβij := − ln\n(\nqβij/ √ qβiiq β jj\n)\nprovides a distance on V. This distance is cutpoint\nadditive, that is, dβij + d β jk = d β ik if and only if every path in G from i to k visits j. In the asymptotics, dβij becomes proportional to the shortest path distance and the resistance distance as β → 0 and β → ∞, respectively."
    }, {
      "heading" : "4.4 Matrix forest characterization",
      "text" : "By the matrix forest theorem [13, 1], each entry qβij of Qβ is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.\nMore specifically, qβij = F β i⊣j/F β, where Fβ is the total β-weight of all spanning rooted forests of G, Fβi⊣j being the total β-weight of such of them that have node i in a tree rooted at j. Here, the β-weight of a forest stands for the product of its edges weights, each multiplied by β.\nLet us mention a closely related interpretation of the Regularized Laplacian kernel Qβ in terms of information dissemination [11]. Suppose that an information unit (an idea) must be transmitted through G. A plan of information transmission is a spanning rooted forest F in G: the information unit is initially injected into the roots of F; after that it comes to the other nodes along the edges of F. Suppose that a plan is chosen at random: the probability of every choice is proportional to the β-weight of the corresponding forest. Then by the matrix forest theorem, the probability that the information unit arrives at i from root j equals qβij = F β i⊣j/F\nβ. This interpretation is particularly helpful in the context of machine learning for social networks.\n1Cf. the cosine law [21] and the inverse covariance mapping [22, Section 5.2].\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 9",
      "text" : ""
    }, {
      "heading" : "4.5 Statistical characterization",
      "text" : "Consider the problem of attribute evaluation from paired comparisons.\nSuppose that each data point (node) i has a value parameter vi, and a series of paired comparisons rij between the points is performed. Let the result of i in a comparison with j obey the Scheffé linear statistical model [32]\nE(rij) = vi − vj , (13)\nwhere E(·) is the mathematical expectation. The matrix form of (13) applied to an experiment is\nE(r) = Xv,\nwhere v = (v1, . . . , vN ) T , and r is the vector of comparison results, X being the incidence matrix (design matrix , in terms of statistics): if the kth element of r is a comparison result of i confronted to j, then, in accordance with (13), xki = 1, xkj = −1, and xkl = 0 for l 6∈ {i, j}.\nSuppose that X is known, r being a sample, and the problem is to estimate v up to a shift [10, Section 4]. Then\nṽ(λ) = (λI +XTX)−1XTr (14)\nis the well-known ridge estimate of v, where λ > 0 is the ridge parameter. Denoting β = λ−1 and XTX = L (it is easily verified that XTX is a Laplacian matrix whose (i, j)-entry with j 6= i is minus the number of comparisons between i and j) one has\nṽ(λ) = (I + βL)−1βXTr, (15)\ni.e., the solution is provided by the same transformation based on the Regularized Laplacian kernel as in (2) (cf. also (12)). Here, the weight matrix A of G contains the numbers of comparisons between nodes; s = XTr is the vector of the sums of comparison results of the nodes: si = ∑ j rij − ∑\nj rji, where rij and rji are taken from r, which has one entry (either rij or rji) for each comparison result.\nSuppose now that value parameter vi (belonging to an interval centered at zero) is a positive or negative intensity of some property, and thus, vi can be treated as a signed membership of data point i in the corresponding class. The pairwise comparisons r are performed with respect to this property. Then βXTr = βs is a kind of labeling function or a crude correlate of membership in the above class, whereas (15) provides a refined measure of membership which takes into account proximity. Along these lines, (15) can be considered as a procedure of semi-supervised learning.\nA Bayesian version of the model (13) enables one to interpret and estimate the ridge parameter λ = 1/β. Namely, assume that: (i) the parameters v1, . . . , vN chosen at random from the universal set are independent random variables with zero mean and variance σ21 and (ii) for any vector v, the errors in (13) are independent and have zero mean, their unconditional variance being σ22 .\nIt can be shown [10, Proposition 4.2] that under these conditions, the best linear predictors for the parameters v are the ridge estimators (15) with β = σ21/σ 2 2 .\nThe best linear predictors for v are the ṽi’s that minimize E(ṽi − vi) 2 among all statistics of\nthe form ṽi = ci + C T i r satisfying E(ṽi − vi) = 0.\nThe variances σ21 and σ 2 2 can be estimated from the experiment. In fact, there are many\napproaches to choosing the ridge parameter, see, e.g., [24, 29] and the references therein.\nRR n° 8765"
    }, {
      "heading" : "10 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : ""
    }, {
      "heading" : "5 Limiting cases",
      "text" : "Let us analyse the formula (2) in two limiting cases: β → 0 and β → ∞. If β → 0, we have\nF∗k = (I − βL)Y∗k + o(β).\nThus, for very small values of β, the method resembles the nearest neighbour method with the weight matrix W = I −βL. If there are many points situated more than one hop away from any labelled point, the method cannot produce good classification with very small values of β. This will be illustrated by the numerical experiments in Section 6.\nNow consider the other case β → ∞. We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (λI + L)−1 with λ = 1/β\n(I + βL)−1 = λ(λI + L)−1\n= λ\n(\n1\nλ\n1\nN 11T +H − λH2 + ...\n)\n, (16)\nwhere H = (L + 1N 11 T )−1 − 1N 11 T is the generalized (group) inverse of the Laplacian. Since the first term in (16) gives the same value for all classes if 1TY∗k = 1 TY∗l, k 6= l (which is typically the case), the classification will depend on the entries of the matrix H and finally, of the matrix (L+ 1N 11\nT )−1. Note that the matrix (L+α11T )−1, with a sufficiently small positive α, determines a proximity measure called accessibility via dense forests. Its properties are listed in [15, Proposition 10]. An interpretation of H in terms of spanning forests can be found in [15, Theorem 3]; see also [26].\nThe accessibility via dense forests violates a natural monotonicity condition, as distinct from (I + βL)−1 with a finite β. Thus, a better performance of the regularized Laplacian proximity measure with finite values of β can be expected.\nFor the sake of comparison, let us analyse the limiting behaviour of the heat kernels. For instance, let us consider the Standard Laplacian heat kernel (6), since it is also based on the Standard Laplacian. In fact, it is immediate to see that the Standard Laplacian heat kernel has the same asymptotic as the Regularized Laplacian kernel. Namely, if t → 0,\nH(t) = exp(−tL) = I − tL+ o(t).\nSimilar expressions hold for the other heat kernels. Thus, for small values of t, the semi-supervised learning methods based on heat kernels should behave as the nearest neighbour method.\nNext consider the Standard Laplacian heat kernel when t → ∞. Recall that the Laplacian L = D−A is a positive definite symmetric matrix. Without the loss of generality, we can denote and rearrange the eigenvalues of the Laplacian as 0 = λ1 ≤ λ2 ≤ ... and the corresponding eigenvectors as u1, ..., un. Note that u1 = 1. Thus, we can write\nH(t) = u1u T 1 +\nN ∑\ni=2\nexp(−λit)uiu T i .\nWe can see that for large values of t the first term in the above expression is non-informative as in the case of the Regularized Laplacian method and we need to look for the second order term. However, in contrast to the Regularized Laplacian kernel, the second order term exp(−λ2t)u2u T 2 is a rank-one term and cannot in principle give correct classification in the case of more than two classes. The second term of the Regularized Laplacian kernel H is not a rank-one matrix and as mentioned above can be interpreted in terms of proximity measures.\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 11",
      "text" : ""
    }, {
      "heading" : "6 Numerical methods and examples",
      "text" : "Let us first discuss various approaches for the numerical computation of the classification functions (2). Broadly speaking, the approaches can be divided into linear algebra methods and optimization methods. One of the basic linear algebra methods is the power iteration method. Similarly to the power iteration method described in [6], we can write\nF∗k = (I + βD − βA) −1Y∗k,\nF∗k = (I − β(I + βD) −1A)−1(I + βD)−1Y∗k,\nF∗k = (I − β(I + βD) −1DD−1A)−1(I + βD)−1Y∗k.\nNow denoting B := β(I + βD)−1D and C := (I + βD)−1, we can propose the following power iteration method to compute the classification functions\nF (s+1) ∗k = BD −1AF (s) ∗k + CY∗k, s = 0, 1, ... , (17)\nwith F (0) ∗k = Y∗k. Since B is a diagonal matrix with the diagonal entries less than one, the matrix BD−1A is substochastic with the spectral radius less than one and the power iterations (17) are convergent. However, for large values of β and di, the matrix BD\n−1A can be very close to stochastic and hence the convergence rate of the power iterations can be very slow. Therefore, unless the value of β is small, we recommend to use the other methods from numerical linear algebra for the solution of linear systems with symmetric matrices (recall that L is a symmetric positive semi-definite matrix in the case of undirected graphs). In particular, we tried the Cholesky decomposition method and the conjugate gradient method. Both methods appeared to be very efficient for the problems with tens of thousands of variables. Actually, the conjugate gradient method can also be viewed as an optimization method for the respective convex quadratic optimization problem such as (1) and (7). A very convenient property of optimization formulations (1) and (7) is that the objective, and consequently, the gradient, can be written in terms of a sum over the edges of the underlying graph. This allows a very simple (and with some software packages even automatic) parallelization of the optimization methods based on the gradient. For instance, we have used the parallel implementation of the gradient based methods provided by the NVIDIA CUDA sparse matrix library (cuSPARSE) [39] and it showed excellent performance.\nLet us now illustrate the Regularized Laplacian method and compare it with some other state of the art semi-supervised learning methods on two datasets: Les Miselables and Wikipedia Mathematical Articles.\nThe first dataset represents the network of interactions between major characters in the novel Les Miserables. If two characters participate in one or more scenes, there is a link between these two characters. We consider the links to be unweighted and undirected. The network of the interactions of Les Miserables characters has been compiled by Knuth [27]. There are 77 nodes and 508 edges in the graph. Using the betweenness based algorithm of Newman and Girvan [30] we obtain 6 clusters which can be identified with the main characters: Valjean (17), Myriel (10), Gavroche (18), Cosette (10), Thenardier (12), Fantine (10), where in brackets we give the number of nodes in the respective cluster. First, we generate randomly (100 times) labeled points (two labeled points per class). In Figure 1 we plot average precision as a function of parameter β. In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking σ = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5]. Thus, we compare the Regularized Laplacian method with the PageRank based method. As we can see for Figure 1.(a), the performance\nRR n° 8765"
    }, {
      "heading" : "12 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : "of the Regularized Laplacian method is comparable to that of the PageRank based method on Les Miserables dataset. The horizontal line in Figure 1.(a) corresponds to the PageRank based method with the best choice of the regularization parameter or the restart probability in the context of PageRank. Since the Regularized Laplacian method is based on graph Laplacian, we also compare it in Figure 1.(b) with the three heat kernel methods derived from variations of the graph Laplacian. Specifically, we consider the three time-domain kernels based on various Laplacians: Standard Heat kernel (6), Normalized Heat kernel (3), and PageRank Heat kernel (4). For instance, in the case of the Standard Heat kernel the classification functions are given by F∗k = H(t)Y∗k. It turns out that all the three time-domain heat kernels are very sensitive to the value of the chosen time, t. Even though there are parameter settings that give similar performances of Heat kernel methods and the Regularized Laplacian method, the Regularized Laplacian method has a large plateau for values of β where the good performance of the method is assured. Thus, the Regularized Laplacian method is more robust with respect to the parameter setting than the heat kernel methods.\nTo see better the behaviour of the heat kernel methods for large values of t, we have chosen a larger interval for t in Figure 2. The performance of the heat kernel methods degrades quite significantly for large values of t. This is actually predicted by the asymptotics given in Section 5. Since we have more than two classes, the heat kernels with rank-one second order asymptotics are not able to distinguish among the classes. All heat kernel methods as well as the Regularized Laplacian method show a deterioration in performance for small values of t and β. This was predicted in Section 5, as all the methods start to behave like the nearest neighbour method. In particular, as follows from the asymptotics of Section 5 and can be observed in the figures the Standard Laplacian heat kernel method and the Regularized Laplacian method shows exactly the same performance when t → 0 and β → 0.\nIt was observed in [5] that taking labelled data points with large (weighted) degree is typically beneficial for the semi-supervised learning methods. Thus, we now label randomly two points out of three points with maximal degree for each class. The average precision is given in Figure 3.(a). We also test heat kernel based methods with the same labelled points, see Figure 3.(b). One can see that if we choose the labelled points with large degree, the Regularized Laplacian Method outperforms the PageRank based method. Some heat kernel based methods with large degree labelled points also outperform the PageRank based method but their performance is much less stable with respect to the value of parameter t.\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 13",
      "text" : "RR n° 8765"
    }, {
      "heading" : "14 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : "Next, we consider the second dataset consisting of Wikipedia mathematical articles. This dataset is derived from the English language Wikipedia snapshot (dump) from January 30, 20102. The similarity graph is constructed by a slight modification of the hyper-text graph. Each Wikipedia article typically contains links to other Wikipedia articles which are used to explain specific terms and concepts. Thus, Wikipedia forms a graph whose nodes represent articles and whose edges represent hyper-text inter-article links. The links to special pages (categories, portals, etc.) have been ignored. In the present experiment we did not use the information about the direction of links, so the similarity graph in our experiments is undirected. Then we have built a subgraph with mathematics related articles, a list of which was obtained from “List of mathematics articles” page from the same dump. In the present experiments we have chosen the following three mathematical classes: “Discrete mathematics” (DM), “Mathematical analysis” (MA), “Applied mathematics” (AM). With the help of AMS MSC Classification3 and experts we have classified relatedWikipedia mathematical articles into the three above mentioned classes. As a result, we obtained three imbalanced classes DM (106), MA (368) and AM (435). The subgraph induced by these three topics is connected and contains 909 articles. Then, the similarity matrix A is just the adjacency matrix of this subgraph.\nFirst, we have chosen uniformly at random 100 times 5 labeled nodes for each class. The average precisions corresponding to the Regularized Laplacian method and the PageRank based method are plotted in Figure 4.(a). We also provide the results for the three heat kernel based methods in Figure 4.(b). As one can see, the results of Wikipedia Mathematical articles dataset are consistent with the results of Les Miserables dataset.\nThen, for each class out of 10 data points with largest degrees we choose 5 points and average the results. The average precisions for the Regularized Laplacian method, PageRank based method and for the three heat kernel based methods are plotted in Figure 5. The results are again consistent with the corresponding results for Les Miserables dataset. We would like to mention that for the computations in the Wiki Math dataset with many parameter settings and extensive averaging using NVIDIA CUDA sparse matrix library (cuSPARSE) [39] were noticeably faster than using numpy.linalg.solve calling LAPACK routine _gesv.\nFinally, we would like to recall from Subsection 4.5 that a good value of β can be provided\n2http://download.wikimedia.org/enwiki/20100130 3http://www.ams.org/mathscinet/msc/msc2010.html\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 15",
      "text" : "by the ratio σ21/σ 2 2 , where σ 2 1 is the variance related to the data points and σ 2 2 is the variance related to the paired comparison between points. We can argue that σ21 is naturally large and the paired comparisons between points can be performed with much more certainty, and hence, σ22 is small. This gives a statistical explanation why it is good to take relatively large values for the parameter β in the Regularized Laplacian method."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have studied in detail the semi-supervised learning method based on the Regularized Laplacian. The method admits both linear algebraic and optimization formulations. The optimization formulation appears to be particularly well suited for parallel implementation. We have provided various interpretations and proximity-distance properties of the Regularized Laplacian graph kernel. We have also shown that the method is related to the Scheffé linear statistical model. The method was tested and compared with the other state of the art semi-supervised learning methods on two datasets. The results from the two datasets are consistent. In particular, we can conclude that the Regularized Laplacian method is comparable in performance with the PageRank based method and outperforms the related heat kernel based methods in terms of robustness.\nSeveral interesting research directions remain open for investigation. It will be interesting to compare the Regularized Laplacian method with the other semi-supervised methods on a very large dataset. We are currently working in this direction. We observe that there is a large plateau of β values for which the Regularized Laplacian method performs very well. It will be very useful to characterize this plateau analytically. Also, it will be interesting to understand analytically why the Regularized Laplacian method performs better when the labelled points with large degree are chosen."
    }, {
      "heading" : "16 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : "[2] Andersen, R., Chung, F., and Lang, K. (2006). “Local graph partitioning using pagerank vectors”. In Proceedings of IEEE FOCS 2006, pp. 475–486.\n[3] Avrachenkov, K., Dobrynin, V., Nemirovsky, D., Pham, S.K., and Smirnova, E. (2008). “Pagerank based clustering of hypertext document collections”. In Proceedings of ACM SIGIR 2008, pp. 873–874.\n[4] Avrachenkov, K., Gonçalves, P., Mishenin, A., and Sokol, M. (2012). “Generalized optimization framework for graph-based semi-supervised learning”. In Proceedings of SIAM Conference on Data Mining (SDM 2012) (Vol. 9).\n[5] Avrachenkov, K., Gonçalves, P., and Sokol, M. (2013). “On the choice of kernel and labelled data in semi-supervised learning methods”. In Algorithms and Models for the Web Graph, WAW 2013, also LNCS, Vol. 8305, pp. 56–67.\n[6] Avrachenkov, K., Mazalov, V. and Tsynguev, B. (2015) “Beta Current Flow Centrality for Weighted Networks”. In Proceedings of the 4th International Conference on Computational Social Networks (CSoNet 2015), also LNCS 9197, Chapter 19.\n[7] Blackwell, D. (1962). “Discrete dynamic programming”. The Annals of Mathematical Statistics, 33, pp. 719–726.\n[8] Callut, J., Françoisse, K., Saerens, M., and Dupont, P. (2008) “Semi-supervised classification from discriminative random walks”. In Machine Learning and Knowledge Discovery in Databases European Conference, ECML PKDD 2008, Antwerp, Belgium, September 15– 19, 2008, Proceedings, Part I, ser. Lecture Notes in Computer Science / Lecture Notes on Artificial Intelligence, W. Daelemans, B. Goethals, and K. Morik, Eds., vol. 5211. BerlinHeidelberg: Springer, pp. 162–177.\n[9] Chapelle, O., Schölkopf, B. and Zien A. (2006). Semi-supervised learning, MIT Press.\n[10] Chebotarev, P. Y. (1994). “Aggregation of preferences by the generalized row sum method”. Mathematical Social Sciences, 27, pp. 293–320.\n[11] Chebotarev, P. (2008). “Spanning forests and the golden ratio”. Discrete Applied Mathematics, 156(5), pp. 813–821.\n[12] P. Chebotarev (2011). “The graph bottleneck identity”. Advances in Applied Mathematics, 47(3), pp. 403–413.\n[13] Chebotarev, P. Yu., and Shamis, E. V. (1997). “The matrix-forest theorem and measuring relations in small social groups”. Automation and Remote Control, 58(9), pp. 1505–1514.\n[14] Chebotarev, P. Yu., and Shamis, E. V. (1998). “On a duality between metrics and Σproximities”. Automation and Remote Control, 59(4), pp. 608–612.\n[15] Chebotarev, P. Yu., and Shamis, E. V. (1998). “On proximity measures for graph vertices”. Automation and Remote Control, 59(10), pp. 1443–1459.\n[16] Chebotarev, P. Yu., and Shamis, E. V. (2000). “The forest metrics of a graph and their properties”. Automation and Remote Control, 61(8), pp. 1364–1373.\n[17] Chung, F., and Yau, S. T. (1999). “Coverings, heat kernels and spanning trees”. Electronic Journal of Combinatorics, 6, R12.\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 17",
      "text" : "[18] Chung, F., and Yau, S. T. (2000). “Discrete Green’s functions”. Journal of Combinatorial Theory, Series A, 91(1), pp. 191–214.\n[19] Chung, F. (2007). “The heat kernel as the pagerank of a graph”. PNAS, 105(50), pp. 19735– 19740.\n[20] Chung, F. (2009). “A local graph partitioning algorithm using heat kernel pagerank”. In Proceedings of WAW 2009, LNCS 5427, pp. 62–75.\n[21] Critchley, F. (1988) “On certain linear mappings between inner-product and squareddistance matrices”. Linear Algebra and its Applications, 105, pp. 91–107.\n[22] Deza, M. M., and Laurent, M. (1997) Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics. Berlin: Springer.\n[23] Fouss, F., Francoisse, K., Yen, L., Pirotte A., and Saerens, M. (2012) “An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification”. Neural Networks, 31, pp. 53–72.\n[24] Dorugade, A. V. (2014) “New ridge parameters for ridge regression”. Journal of the Association of Arab Universities for Basic and Applied Sciences, 15, pp. 94–99.\n[25] Fouss, F., Yen, L., Pirotte, A., and Saerens, M. (2006) “An experimental investigation of graph kernels on a collaborative recommendation task”. In Sixth International Conference on Data Mining (ICDM’06), pp. 863–868.\n[26] Kirkland, S. J., Neumann, M., and Shader, B. L. (1997) “Distances in weighted trees and group inverse of Laplacian matrices”. SIAM J. Matrix Anal. Appl., 18, pp.827–841.\n[27] Knuth, D. E. (1993). The Stanford GraphBase: a platform for combinatorial computing. ACM, New York, NY, USA.\n[28] Kondor, R. I., and Lafferty, J. (2002). “Diffusion kernels on graphs and other discrete input spaces”. In Proceedings of ICML, 2, pp. 315–322.\n[29] Muniz, G. and Kibria, B. M. G. (2009) “On some ridge regression estimators: An empirical comparisons”. Communications in Statistics – Simulation and Computation, 38(3), pp. 621– 630.\n[30] Newman, M. E. J. and Girvan, M. (2004). “Finding and evaluating community structure in networks”. Phys. Rev. E, 69(2):026113.\n[31] Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming, John Wiley & Sons.\n[32] Scheffé, H. (1952) “An analysis of variance for paired comparisons”. Journal of the American Statistical Association, 47(259), pp. 381-400.\n[33] Smola, A. J., and Kondor, R. I. (2003) “Kernels and regularization of graphs”. In Proceedings of the 16th Annual Conference on Learning Theory, pp. 144–158.\n[34] Yen, L., Saerens, M., Mantrach, A., and Shimbo, M. (2008) “A family of dissimilarity measures between nodes generalizing both the shortest-path and the commutetime distances”. In 14th ACM SIGKDD Intern. Conf. on Knowledge Discovery and Data Mining, pp. 785–793.\nRR n° 8765"
    }, {
      "heading" : "18 K. Avrachenkov & P. Chebotarev & A. Mishenin",
      "text" : "[35] Zhou, D., and Burges, C. J. C. (2007) “Spectral clustering and transductive learning with multiple views”. In Proceedings of ICML 2007, pp. 1159–1166.\n[36] Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., Schölkopf, B. (2004). “Learning with local and global consistency”. In: Advances in Neural Information Processing Systems, 16, pp. 321–328.\n[37] Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). “Semi-supervised learning using Gaussian fields and harmonic functions”. In Proceedings of ICML 2003, Vol. 3, pp. 912–919.\n[38] Zhu, X. (2005). “Semi-supervised learning literature survey”. University of WisconsinMadison Research Report TR 1530.\n[39] The NVIDIA CUDA Sparse Matrix library (cuSPARSE), https://developer.nvidia.com/cuSPARSE\nInria"
    }, {
      "heading" : "Semi-supervised Learning with Regularized Laplacian 19",
      "text" : ""
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 3",
      "text" : ""
    }, {
      "heading" : "2 Notations and method formulation 4",
      "text" : ""
    }, {
      "heading" : "3 Related approaches 5",
      "text" : "3.1 Relation to heat kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Relation to the generalized semi-supervised learning\nmethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    }, {
      "heading" : "4 Properties and interpretations of the Regularized Laplacian method 6",
      "text" : "4.1 Discrete-time random walk interpretation . . . . . . . . . . . . . . . . . . . . . . 6 4.2 Continuous-time random walk interpretation . . . . . . . . . . . . . . . . . . . . 7 4.3 Proximity and distance properties . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.4 Matrix forest characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.5 Statistical characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"
    }, {
      "heading" : "5 Limiting cases 10",
      "text" : ""
    }, {
      "heading" : "6 Numerical methods and examples 11",
      "text" : ""
    }, {
      "heading" : "7 Conclusions 15",
      "text" : "RR n° 8765\nRESEARCH CENTRE SOPHIA ANTIPOLIS – MÉDITERRANÉE\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399\nThis figure \"logo-inria.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1508.04906v1\nThis figure \"pagei.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1508.04906v1\nThis figure \"rrpage1.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1508.04906v1"
    } ],
    "references" : [ {
      "title" : "Spanning forests of a digraph and their applications",
      "author" : [ "R.P. Agaev", "P.Y. Chebotarev" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Local graph partitioning using pagerank vectors",
      "author" : [ "R. Andersen", "F. Chung", "K. Lang" ],
      "venue" : "In Proceedings of IEEE FOCS",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Pagerank based clustering of hypertext document collections",
      "author" : [ "K. Avrachenkov", "V. Dobrynin", "D. Nemirovsky", "S.K. Pham", "E. Smirnova" ],
      "venue" : "In Proceedings of ACM SI- GIR",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Generalized optimization framework for graph-based semi-supervised learning",
      "author" : [ "K. Avrachenkov", "P. Gonçalves", "A. Mishenin", "M. Sokol" ],
      "venue" : "In Proceedings of SIAM Conference on Data Mining (SDM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "On the choice of kernel and labelled data in semi-supervised learning methods",
      "author" : [ "K. Avrachenkov", "P. Gonçalves", "M. Sokol" ],
      "venue" : "WAW 2013, also LNCS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Beta Current Flow Centrality for Weighted Networks",
      "author" : [ "K. Avrachenkov", "V. Mazalov", "B. Tsynguev" ],
      "venue" : "In Proceedings of the 4th International Conference on Computational Social Networks (CSoNet 2015),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Discrete dynamic programming",
      "author" : [ "D. Blackwell" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1962
    }, {
      "title" : "Semi-supervised classification from discriminative random walks",
      "author" : [ "J. Callut", "K. Françoisse", "M. Saerens", "P. Dupont" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases European Conference,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Aggregation of preferences by the generalized row sum method",
      "author" : [ "P.Y. Chebotarev" ],
      "venue" : "Mathematical Social Sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1994
    }, {
      "title" : "Spanning forests and the golden ratio",
      "author" : [ "P. Chebotarev" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "The graph bottleneck identity",
      "author" : [ "P. Chebotarev" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "The matrix-forest theorem and measuring relations in small social groups",
      "author" : [ "Chebotarev", "P. Yu", "E.V. Shamis" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1997
    }, {
      "title" : "On a duality between metrics and Σproximities",
      "author" : [ "Chebotarev", "P. Yu", "E.V. Shamis" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "On proximity measures for graph vertices",
      "author" : [ "Chebotarev", "P. Yu", "E.V. Shamis" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1998
    }, {
      "title" : "The forest metrics of a graph and their properties",
      "author" : [ "Chebotarev", "P. Yu", "E.V. Shamis" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Coverings, heat kernels and spanning trees",
      "author" : [ "F. Chung", "S.T. Yau" ],
      "venue" : "Electronic Journal of Combinatorics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1999
    }, {
      "title" : "Discrete Green’s functions",
      "author" : [ "F. Chung", "S.T. Yau" ],
      "venue" : "Journal of Combinatorial Theory, Series A,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "The heat kernel as the pagerank of a graph",
      "author" : [ "F. Chung" ],
      "venue" : "PNAS, 105(50),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "A local graph partitioning algorithm using heat kernel pagerank",
      "author" : [ "F. Chung" ],
      "venue" : "In Proceedings of WAW 2009,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "On certain linear mappings between inner-product and squareddistance matrices",
      "author" : [ "F. Critchley" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1988
    }, {
      "title" : "Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics",
      "author" : [ "M.M. Deza", "M. Laurent" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1997
    }, {
      "title" : "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification",
      "author" : [ "F. Fouss", "K. Francoisse", "L. Yen", "Pirotte A", "M. Saerens" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "New ridge parameters for ridge regression",
      "author" : [ "A.V. Dorugade" ],
      "venue" : "Journal of the Association of Arab Universities for Basic and Applied Sciences,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "An experimental investigation of graph kernels on a collaborative recommendation task",
      "author" : [ "F. Fouss", "L. Yen", "A. Pirotte", "M. Saerens" ],
      "venue" : "In Sixth International Conference on Data Mining (ICDM’06),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Distances in weighted trees and group inverse of Laplacian matrices",
      "author" : [ "S.J. Kirkland", "M. Neumann", "B.L. Shader" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1997
    }, {
      "title" : "The Stanford GraphBase: a platform for combinatorial computing",
      "author" : [ "D.E. Knuth" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1993
    }, {
      "title" : "Diffusion kernels on graphs and other discrete input spaces",
      "author" : [ "R.I. Kondor", "J. Lafferty" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    }, {
      "title" : "On some ridge regression estimators: An empirical comparisons",
      "author" : [ "G. Muniz", "B.M.G. Kibria" ],
      "venue" : "Communications in Statistics – Simulation and Computation,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "Finding and evaluating community structure in networks",
      "author" : [ "M.E.J. Newman", "M. Girvan" ],
      "venue" : "Phys. Rev. E,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2004
    }, {
      "title" : "Markov decision processes: Discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1994
    }, {
      "title" : "An analysis of variance for paired comparisons",
      "author" : [ "H. Scheffé" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1952
    }, {
      "title" : "Kernels and regularization of graphs",
      "author" : [ "A.J. Smola", "R.I. Kondor" ],
      "venue" : "In Proceedings of the 16th Annual Conference on Learning Theory,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "A family of dissimilarity measures between nodes generalizing both the shortest-path and the commutetime distances",
      "author" : [ "L. Yen", "M. Saerens", "A. Mantrach", "M. Shimbo" ],
      "venue" : "In 14th ACM SIGKDD Intern. Conf. on Knowledge Discovery and Data Mining, pp. 785–793",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "Spectral clustering and transductive learning with multiple views",
      "author" : [ "D. Zhou", "C.J.C. Burges" ],
      "venue" : "In Proceedings of ICML",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2007
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T. Navin Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2004
    }, {
      "title" : "Semi-supervised learning using Gaussian fields and harmonic functions",
      "author" : [ "X. Zhu", "Z. Ghahramani", "J. Lafferty" ],
      "venue" : "In Proceedings of ICML 2003,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2003
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "X. Zhu" ],
      "venue" : "University of Wisconsin- Madison Research Report TR 1530",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "The work [37] seems to be the first work where the graph-based semi-supervised learning was introduced.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 35,
      "context" : "The authors of [37] formulated the semi-supervised learning method as a constrained optimization problem involving graph Laplacian.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 34,
      "context" : "Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 33,
      "context" : "Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 33,
      "context" : "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 34,
      "context" : "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning.",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning.",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].",
      "startOffset" : 115,
      "endOffset" : 126
    }, {
      "referenceID" : 36,
      "context" : "An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].",
      "startOffset" : 115,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "In [23] the authors compared experimentally many graph-based semi-supervised learning methods on several datasets and their conclusion was that the semisupervised learning method based on the Regularized Laplacian kernel demonstrates one of the best performances on nearly all datasets.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "In [8] the authors studied a semi-supervised learning method based on the Normalized Laplacian graph kernel which also shows good performance.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 35,
      "context" : "In fact, the Regularized Laplacian method can be regarded as a Lagrangian relaxation of the method proposed in [37].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 35,
      "context" : "Of course, this is a more flexible formulation, since by choosing an appropriate value for the Lagrange multiplier one can always retrieve the method of [37] as a particular case.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "(2) The matrix Qβ = (I + βL) −1 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : "(2) The matrix Qβ = (I + βL) −1 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "(2) The matrix Qβ = (I + βL) −1 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "(2) The matrix Qβ = (I + βL) −1 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "(2) The matrix Qβ = (I + βL) −1 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "1 Relation to heat kernels The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian.",
      "startOffset" : 42,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "1 Relation to heat kernels The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian.",
      "startOffset" : 42,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "Then, in [19] the PageRank heat kernel was introduced",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 18,
      "context" : "In [20] the PageRank heat kernel was applied to local graph partitioning.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "In [28] the heat kernel based on the standard Laplacian H(t) = exp(−tL), (6)",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 35,
      "context" : "Then, in [37] the authors proposed a semi-supervised learning method based on the solution of a heat diffusion equation with Dirichlet boundary conditions.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 35,
      "context" : "Equivalently, the method of [37] can be viewed as the minimization of the second term in (1) with the values of the classification functions F∗k fixed on the labelled points.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 35,
      "context" : "Thus, the proposed approach (1) is more general as it can be viewed as a Lagrangian relaxation of [37].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "The results of the method in [37] can be retrieved with a particular choice of the regularization parameter.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "2 Relation to the generalized semi-supervised learning method In [4] the authors proposed a generalized optimization framework for graph based semi-supervised learning methods",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : "In particular, with σ = 1 we retrieve the transductive semi-supervised learning method [35], with σ = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with σ = 0 we retrieve the PageRank based method [3].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 34,
      "context" : "In particular, with σ = 1 we retrieve the transductive semi-supervised learning method [35], with σ = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with σ = 0 we retrieve the PageRank based method [3].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "In particular, with σ = 1 we retrieve the transductive semi-supervised learning method [35], with σ = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with σ = 0 we retrieve the PageRank based method [3].",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 12,
      "context" : "Qβ determines a positive 1-proximity measure [14] s(i, j) := q β ij , i.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : ", it satisfies [13] the following conditions: (1) for any i ∈ V, ∑ k∈V q β ik = 1 and (2) for any i, j, k ∈ V, q ji + q β jk − q β ik ≤ q β jj with a strict inequality whenever i = k and i 6= j (the triangle inequality for proximities).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "This implies [14] the following two important properties: (a) q ii > q β ij for all i, j ∈ V such that i 6= j (egocentrism property); (b) ρβij := β(q β ii + q β jj − q β ij − q β ji) is 1 a distance on V.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "The distances ρβij have a twofold connection with the resistance distance ρ̃ij on G [16].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "An interested reader can find more properties of the proximity measures determined by Qβ in [13].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, every Qβ, β > 0 determines a transitional measure on V, which means [12] that: q ij q β jk ≤ q β ik q β jj for all i, j, k ∈ V with q β ij q β jk = q β ik q β jj if and only if every path in G from i to k visits j.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "4 Matrix forest characterization By the matrix forest theorem [13, 1], each entry q ij of Qβ is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "4 Matrix forest characterization By the matrix forest theorem [13, 1], each entry q ij of Qβ is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "Let us mention a closely related interpretation of the Regularized Laplacian kernel Qβ in terms of information dissemination [11].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "the cosine law [21] and the inverse covariance mapping [22, Section 5.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 30,
      "context" : "Let the result of i in a comparison with j obey the Scheffé linear statistical model [32]",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : ", [24, 29] and the references therein.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : ", [24, 29] and the references therein.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (λI + L) with λ = 1/β (I + βL) = λ(λI + L) = λ (",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 29,
      "context" : "We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (λI + L) with λ = 1/β (I + βL) = λ(λI + L) = λ (",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "An interpretation of H in terms of spanning forests can be found in [15, Theorem 3]; see also [26].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "Similarly to the power iteration method described in [6], we can write",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "The network of the interactions of Les Miserables characters has been compiled by Knuth [27].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 28,
      "context" : "Using the betweenness based algorithm of Newman and Girvan [30] we obtain 6 clusters which can be identified with the main characters: Valjean (17), Myriel (10), Gavroche (18), Cosette (10), Thenardier (12), Fantine (10), where in brackets we give the number of nodes in the respective cluster.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking σ = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking σ = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking σ = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].",
      "startOffset" : 225,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking σ = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].",
      "startOffset" : 225,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking σ = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].",
      "startOffset" : 225,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "It was observed in [5] that taking labelled data points with large (weighted) degree is typically beneficial for the semi-supervised learning methods.",
      "startOffset" : 19,
      "endOffset" : 22
    } ],
    "year" : 2015,
    "abstractText" : "We study a semi-supervised learning method based on the similarity graph and Regularized Laplacian. We give convenient optimization formulation of the Regularized Laplacian method and establish its various properties. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods. Key-words: Semi-supervised learning, Graph-based learning, Regularized Laplacian, Proximity measure, Wikipedia article classification ∗ Corresponding author. K. Avrachenkov is with Inria Sophia Antipolis, 2004 Route des Lucioles, 06902, Sophia Antipolis, France k.avrachenkov@inria.fr † P. Chebotarev is with Trapeznikov Institute of Control Sciences of the Russian Academy of Sciences, 65 Profsoyuznaya Str., Moscow, 117997, Russia ‡ A. Mishenin is with St. Petersburg State University, Faculty of Applied Mathematics and Control Processes, Peterhof, 198504, Russia § This work was partially supported by Campus France, Alcatel-Lucent Inria Joint Lab, EU Project Congas FP7-ICT-2011-8-317672, and RFBR grant No. 13-07-00990. L’Apprentissage Semi-supervisé avec Laplacian Régularisé Résumé : Nous étudions une méthode d’apprentissage semi-supervisé, basé sur le graphe de similarité et Laplacian régularisé. Nous formalisons la méthode comme un problème d’optimisation convexe et quadratique et nous établissons ses diverses propriétés. En particulier, nous montrons que le noyau de la méthode peut être interprété en termes des marches aléatoires en temps discret et continu et possède plusieurs propriétés importantes des mesures de proximité. Les techniques d’optimisation ainsi que les techniques d’algébre linéaire peuvent être utilisé pour un calcul efficace des fonctions de classification. Nous démontrons sur des exemples numériques que la méthode de Laplacian régularisé est concurrentiel par rapport aux autres état de l’art méthodes d’apprentissage semi-supervisé. Mots-clés : Apprentissage Semi-supervisé, Apprentissage basé sur le graphe de similarité, Laplacian régularisé, mesure de proximité, classification des articles Wikipedia Semi-supervised Learning with Regularized Laplacian 3",
    "creator" : "LaTeX with hyperref package"
  }
}