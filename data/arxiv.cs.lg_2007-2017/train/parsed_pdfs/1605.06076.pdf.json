{
  "name" : "1605.06076.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On a convergent off -policy temporal difference learning algorithm in on-line learning environment",
    "authors" : [ "Prasenjit Karmakar", "Rajkumar Maity", "Shalabh Bhatnagar" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of estimating the value function corresponding to a target policy given the realization of a finite state Markov decision process under a behaviour policy which is different from the target policy. This is well known in literature as the off-policy evaluation problem. A solution for this problem might allow one to learn about the optimal policy while behaving according to an exploratory policy. See [7] for additional uses.\nIt is well-known that for this problem the standard temporal difference learning with linear function approximation may diverge ([6], [8, Section 3]). Further, the usual single time-scale stochastic approximation kind of argument may not be useful as the associated ordinary differential equation (o.d.e) may not have the TD(0) solution as its globally asymptotically stable equilibrium. In [9, 10, 2] the gradient temporal difference learning (GTD) algorithms were proposed to solve this problem. The per time-step computational complexity for these algorithms scales only linearly in the size d of the function approximator. However, the authors make the assumption that either\n1. one uses “sub-sampling” (see [2, Section 4.1],[9] for details) to filter the data relevant to target policy given the trajectory corresponding to behaviour policy, or\n2. the data itself is available in the off-policy setting i.e. one has direct access to quadruples of the form (state, action, reward, next state) where the first component of the quadruples are sampled independently from the stationary distribution of\nar X\niv :1\n60 5.\n06 07\n6v 1\n[ cs\n.L G\n] 1\nthe underlying Markov chain corresponding to the behaviour policy and the quadruples are formed according to the target policy.\nAmongst all algorithms with the above assumptions, the TDC (temporal difference learning with gradient correction) algorithm was empirically found to be most efficient in terms of the rate of convergence. It was shown in [10] that such an algorithm can be proved to be convergent using the classical convergence proof for two timescale stochastic approximation with martingale difference noise [11]. The reason for using two time-scale framework for the TDC algorithm is to make sure that the O.D.E’s have globally asymptotically stable equilibrium. However, one can prove the convergence using single time-scale convergence analysis as in [2, Theorem 3]; however the extra condition on the step-size ratio η mentioned there is hard to verify as stationary distribution will be unknown.\nNote that such works incorporate the off-policy issue into the data as they don’t take the full behvaiour trajectory as input to the algorithm. The assumptions used in the aforementioned reference on off-policy algorithms are highly restrictive as\n1. although in the first case Markov chain sampled at increasing stopping times is time-homogeneous Markov, its transition probability will be different from the same of the Markov chain corresponding to behaviour policy. Further, we are interested in an “online” learning scheme. Also,\n2. the second situation is not realistic too as the aforementioned stationary distribution will be unknown; one has access to only the trajectory corresponding to behaviour policy from which the goal is to evaluate the target policy.\nKeeping this in mind, another algorithm introduced in [2], namely, TDC with importance weighting solves the above off-policy evaluation problem in a more realistic scenario. The idea is to handle the off-policy issue in the algorithm rather than in the data by weighting the updates by the likelihood of action taken by the target policy (as opposed to the behavior policy). The advantage is that, unlike sub-sampling, here all the data from the given trajectory corresponding to the behaviour policy is used which is necessary in an online learning scenario. Another advantage of this method is that we can allow both the behaviour and target policies to be be randomized unlike the sub-sampling scenario where one can use only deterministic policy as a target policy. However, to the best of our knowledge, both its theoretical and empirical convergence properties have not yet been analyzed. Note that one cannot represent the algorithm in the usual two time-scale stochastic approximation framework to prove its convergence and needs to extend such a framework to non-additive Markov noise and additive martingale difference noise. The Markov noise appears in the algorithm as the full trajectory of the realization of the underlying Markov decision process corresponding to the behaviour policy is taken as input to the algorithm.\nIn this work we give a rigorous almost sure convergence proof of TDC algorithm with importance weighting by formulating it into the two time-scale stochastic approximation framework with non-additive Markov noise and additive martingale difference noise. To the best of our knowledge this is the first time an almost sure convergence proof of off-policy temporal difference learning algorithm with linear function approximation is\npresented for step-sizes satisfying Robbins-Monro conditions. We also support these theoretical results by providing empirical results. Our results show that due to the above-mentioned importance weighting factor, online TDC with importance weighting performs much better than the sub-sampling version of TDC for standard off-policy counterexamples when the behaviour policy is much different from the target policy.\nRecently, emphatic temporal difference learning has been introduced in [8] to solve the off-policy evaluation problem. However, such algorithms are proven to be almost surely convergent for special step-size sequences and weakly convergent for a large range of step-sizes [5].\nAnother related work is the much complex off-policy learning algorithms that obtain the benefits of weighted importance sampling (to reduce variance) with O(d) computational complexity [1]. However, nothing is known about the convergence of such algorithms. In this context, we empirically show that in the case of TDC with importance weighting the variance of the difference between true value function and the estimated one for standard off-policy counterexamples such as [6] becomes very small eventually.\nThe organization of the paper is as follows: Section 2 describes the TDC algorithm with importance weighting. Section 3 gives the rigorous convergence proof of the algorithm. Section 4 shows empirical results supporting our theoretical results. Finally we conclude by providing some interesting future directions."
    }, {
      "heading" : "2 Background and description of TDC with importance weighting",
      "text" : "We need to estimate the value function for a target policy π given the continuing evolution of the underlying MDP (with finite state and action spaces S and A respectively, specified by expected reward r(·, ·, ·) and transition probability kernel p(·|·, ·)) for a behaviour policy πb with π 6= πb. Suppose, the above-mentioned on-policy trajectory is (Xn, An, Rn, Xn+1), n ≥ 0 where {Xn} is a time-homogeneous irreducible Markov chain with unique stationary distribution ν and generated from the behavior policy πb. Here the quadruplet (s, a, r, s′) represents (current state, action, reward, next state). Also, assume that πb(a|s) > 0 ∀s ∈ S, a ∈ A. We need to find the solution θ∗ for the following:\n0 = ∑ s,a,s′ ν(s)π(a|s)p(s′|s, a)δ(θ; s, a, s′)φ(s) = E[ρX,AδX,R,Y (θ)φ(X)]\n= b−Aθ, (1)\nwhere\n(i) θ ∈ Rd is the parameter for value function,\n(ii) φ : S → Rd is a vector of state features,\n(iii) X ∼ ν,\n(iv) 0 < γ < 1 is the discount factor, (v) E[R|X = s, Y = s′] = ∑ a∈A πb(a|s)r(s, a, s′),\n(vi) P (Y = s′|X = s) = ∑ a∈A πb(a|s)p(s′|s, a),\n(vii) δ(θ; s, a, s′) = r(s, a, s′) + γθTφ(s′)− θTφ(s) is the temporal difference term with expected single-stage reward,\n(viii) ρX,A = π(A|X) πb(A|X) ,\n(ix) δX,R,Y = R+ γθTφ(Y )− θTφ(X),\n(x) A = E[ρX,Aφ(X)(φ(X)− γφ(Y ))T ], b = E[ρX,ARφ(X)].\nThe desired approximate value function under the target policy π is V ∗π = θ ∗Tφ. Let Vθ = θ Tφ. It is well-known ([2]) that θ∗ (solution to (1)) satisfies the projected fixed point equation namely Vθ = ΠG,νT πVθ,\nwhere ΠG,ν V̂ = arg min\nf∈G (‖V̂ − f‖ν),\nwith G = {Vθ|θ ∈ Rd} and the Bellman operator\nTπVθ(s) = ∑ s′∈S ∑ a∈A π(a|s)p(s′|s, a) [γVθ(s′) + r(s, a, s′)] .\nHere ‖ · ‖ν is the weighted Euclidean norm defined by ‖f‖2ν = ∑ s∈S f(s)\n2ν(s), Therefore to find θ∗, the idea is to minimize the mean square projected Bellman error (MSPBE) J(θ) = ‖Vθ − ΠG,νTπVθ‖2ν using stochastic gradient descent. It can be shown that the expression of gradient contains product of multiple expectations. Such framework can be modelled by two time-scale stochastic approximation where one iterate stores the quasi-stationary estimates of some of the expectations and the other iterate is used for sampling.\nWe consider the TDC (Temporal Difference with Correction) algorithm with importanceweighting from Sections 4.2 and 5.2 of [2]. The gradient in this case can be shown to satisfy\n−1 2 ∇J(θ) = E[ρX,AδX,R,Y (θ)φ(X)]− γE[ρX,Aφ(Y )φ(X)T ]w(θ),\nw(θ) = E[φ(X)φ(X)T ]−1E[ρX,AδX,R,Y (θ)φ(X)].\nDefine φn = φ(Xn), φ′n = φ(Xn+1), δn(θ) = δXn,Rn,Xn+1(θ) and ρn = ρXn,An . Therefore the associated iterations in this algorithm are:\nθn+1 = θn + a(n)ρn [ δn(θn)φn − γφ′nφTnwn ] , (2)\nwn+1 = wn + b(n) [ (ρnδn(θn)− φTnwn)φn ] , (3)\nwith {a(n)}, {b(n)} satisfying conditions which will be specified later. Note that the second term inside bracket in (2) is essentially an adjustment or correction of the TD update so that it follows the gradient of the MSPBE objective function thus helping in the desired convergence.\nNote that the sub-sampling version of TDC algorithm (therefore the offline version of TDC algorithm) can be written in the following way:\nθn+1 = θn + a(n)I{An=π(Xn)} [ δn(θn)φn − γφ′nφTnwn ] ,\nwn+1 = wn + b(n)I{An=π(Xn)} [ (δn(θn)− φTnwn)φn ] ,\nwhere I{An=π(Xn)} = 1 if An = π(Xn) and 0 otherwise. In the rest of the paper both the above algorithms will be denoted by ONTDC and OFFTDC respectively except the figures in Section 4 where we mention the full name."
    }, {
      "heading" : "3 Almost sure convergence proof of ONTDC",
      "text" : "As mentioned earlier, to analyze the convergence of the iterations (2) and (3) one has to first extend the classic two time-scale stochastic approximation framework of Borkar [11] to a setting with Markov noise. The full extension is shown in the Appendix. We only state here a special case of this theory which will be sufficient for us. Hence we start with this extension and then later show how the TDC iterations can be cast into this framework and proven to be convergent."
    }, {
      "heading" : "3.1 Two timescale stochastic approximation with Markov noise",
      "text" : "Our goal is to perform an asymptotic analysis of the following coupled recursions:\nθn+1 = θn + a(n) [ h(θn, wn, Z (1) n ) +M (1) n+1 ] , (4)\nwn+1 = wn + b(n) [ g(θn, wn, Z (2) n ) +M (2) n+1 ] , (5)\nwhere θn ∈ Rd, wn ∈ Rk, n ≥ 0 and {Z(i)n }, {M (i)n }, i = 1, 2 are random processes that we describe below.\nWe make the following assumptions:\n(A1) {Z(i)n } takes values in a compact metric space S(i), i = 1, 2. Additionally, the processes {Z(i)n }, i = 1, 2 are Markov processes with their individual dynamics specified by\nP (Z (i) n+1 ∈ B(i)|Z(i)m ,m ≤ n) = ∫ B(i) p(i)(dy|Z(i)n ), n ≥ 0,\nfor B(i) Borel in S(i), i = 1, 2, respectively.\n(A2) h : Rd+k × S(1) → Rd is jointly continuous as well as Lipschitz in its first two arguments uniformly w.r.t the third. The latter condition means that\n∀z(1) ∈ S(1), ‖h(θ, w, z(1))− h(θ′, w′, z(1))‖ ≤ L(1)(‖θ − θ′‖+ ‖w − w′‖).\nSame thing is also true for g where the Lipschitz constant is L(2). Note that the Lipschitz constant L(i) does not depend on z(i) for i = 1, 2.\n(A3) {M (i)n }, i = 1, 2 are martingale difference sequences w.r.t the increasing σ-fields\nFn = σ(θm, wm,M (i)m , Z(i)m ,m ≤ n, i = 1, 2), n ≥ 0,\nsatisfying\nE[‖M (i)n+1‖2|Fn] ≤ K(1 + ‖θn‖2 + ‖wn‖2), i = 1, 2,\nfor n ≥ 0 and a given constant K > 0.\n(A4) The stepsizes {a(n)}, {b(n)} are positive scalars satisfying∑ n a(n) = ∑ n b(n) =∞, ∑ n (a(n)2 + b(n)2) <∞, a(n) b(n) → 0.\nMoreover, a(n), b(n), n ≥ 0 are non-increasing.\n(A5) The map S(i) 3 z(i) → p(i)(dy|z(i)) ∈ P(S(i)) is continuous. (A6) The function ĝ(θ, w) = ∫ g(θ, w, z)Γ(2)(dz) is Lipschitz continuous where Γ(2)\nis the unique stationary distribution of Z(2)n . Further, for all θ ∈ Rd, the o.d.e\nẇ(t) = ĝ(θ, w(t)) (6)\nhas globally asymptotically stable equilibrium λ(θ) where λ : Rd → Rk is a Lipschitz map with constant K. Moreover, the function V ′ : Rd+k → [0,∞) defined by V ′(θ, w) = Vθ(w) is continuously differentiable where Vθ(.) is the Lyapunov function for λ(θ). This extra condition is needed so that the set graph(λ):={(θ, λ(θ)) : θ ∈ Rd} becomes a globally asymptotically stable set of the coupled o.d.e\nẇ(t) = ĝ(θ(t), w(t)), θ̇(t) = 0. (A7) Let ĥ(θ) = ∫ h(θ, λ(θ), z)Γ(1)(dz) where Γ(1) is the unique stationary distribu-\ntion of the Markov process Z(1). Then the o.d.e\nθ̇(t) = ĥ(θ(t))), (7)\nhas a globally asymptotically stable equilibrium θ∗.\n(A8) Stability of the iterates: supn(‖θn‖+ ‖wn‖) <∞ a.s. The following theorem is our main result:\nTheorem 3.1 (Slower timescale result). Under assumptions (A1)-(A8),\n(θn, wn)→ (θ∗, λ(θ∗))a.s. as n→∞.\nWe call (6) and (7) as the faster and slower o.d.e to correspond with faster and slower recursions, respectively."
    }, {
      "heading" : "3.2 Convergence Proof of ONTDC",
      "text" : "Theorem 3.2. Consider the iterations (2) and (3) of the TDC. Assume the following:\n(i) {a(n)}, {b(n)} satisfy (A4).\n(ii) {(Xn, Rn, Xn+1), n ≥ 0} is such that {Xn} is a time-homogeneous finite state irreducible Markov chain generated from the behavior policy πb with unique stationary distribution ν. E[Rn|Xn = s,Xn+1 = s′] = ∑ a∈A πb(a|s)r(s, a, s′)\nand P (Xn+1 = s′|Xn = s) = ∑ a∈A πb(a|s)p(s′|s, a) where πb is the behaviour policy, π 6= πb. Also, E[R2n|Xn, Xn+1] < ∞ for all n almost surely, and\n(iii) C = E[φ(X)φ(X)T ] and A = E[ρX,Aφ(X)(φ(X) − γφ(Y ))T ] are nonsingular where X ∼ ν.\n(iv) πb(a|s) > 0 for all s ∈ S, a ∈ A.\n(v) supn(‖θn‖+ ‖wn‖) <∞ w.p. 1.\nThen the parameter vector θn converges with probability one as n→∞ to the TD(0) solution (1).\nProof 1. The iterations (2) and (3) can be cast into the framework of Section 3.1 with\n(i) Z(i)n = Xn−1,\n(ii) h(θ, w, z) = E[(ρn(δn(θn)φn − γφ′nφTnwn))|Xn−1 = z, θn = θ, wn = w],\n(iii) g(θ, w, z) = E[((ρnδn(θn)− φTnwn)φn)|Xn−1 = z, θn = θ, wn = w],\n(iv) M (1)n+1 = ρn(δn(θn)φn−γφ′nφTnwn)−E[ρn(δn(θn)φn−γφ′nφTnwn)|Xn−1, θn, wn],\n(v) M (2)n+1 = (ρnδn(θn)− φTnwn)φn − E[(ρnδn(θn)− φn Twn)φn|Xn−1, θn, wn],\n(vi) Fn = σ(θm, wm, Rm−1, Xm−1, Am−1,m ≤ n, i = 1, 2), n ≥ 0.\nNote that in (ii) and (iii) we can define h and g independent of n due to time-homogeneity of {Xn}.\nNow, we verify the assumptions (A1)-(A8) (mentioned in Section 3.1) for our application:\n(i) (A1): Z(i)n ,∀n, i = 1, 2 takes values in compact metric space as {Xn} is a finite state Markov chain.\n(ii) (A5): Continuity of transition kernel follows trivially from the fact that we have a finite state MDP.\n(iii) (A2)\n‖h(θ, w, z)− h(θ′, w′, z)‖ = ‖E[ρn(θ − θ′)T (γφ(Xn+1)− φ(Xn))φ(Xn) − γρnφ(Xn+1)φ(Xn)T (w − w′)|Xn−1 = z]‖ ≤ L(2‖θ − θ′‖M2 + ‖w − w′‖M2),\nwhere M = maxs∈S ‖φ(s)‖ with S being the state space of the MDP and L = max(s,a)∈(S×A) π(a|s) πb(a|s) . Hence h is Lipschitz continuous in the first two arguments uniformly w.r.t the third. In the last inequality above, we use the Cauchy-Schwarz inequality. As with the case of h, g can be shown to be Lipschitz continuous in the first two arguments uniformly w.r.t the third. Joint continuity of h and g follows from the above as well as the finiteness of S.\n(iv) (A3): Clearly, {M (i)n+1}, i = 1, 2 are martingale difference sequences w.r.t. increasing σ-fields Fn. Note that E[‖M (i)n+1‖2|Fn] ≤ K(1 + ‖θn‖2 + ‖wn‖2) a.s., n ≥ 0 since E[R2n|Xn, Xn+1] <∞ for all n almost surely and S is finite.\n(v) (A4): This follows from the conditions (i) in the statement of Theorem 3.2.\nNow, one can see that the faster o.d.e. becomes\nẇ(t) = E[ρX,AδX,R,Y (θ)φ(X)]− E[φ(X)φ(X)T ]w(t).\nClearly, C−1E[ρX,AδX,R,Y (θ)φ(X)] is the globally asymptotically stable equilibrium of the o.d.e. The corresponding Lyapunov function V (θ, w) = 12‖Cw−E[ρX,AδX,R,Y (θ)φ(X)]‖ 2 is continuously differentiable. Additionally, λ(θ) = C−1E[ρX,AδX,R,Y (θ)φ(X)] and it is Lipschitz continuous in θ, verifying (A6). , Further, A−1E[ρX,ARφ(X)] is the globally asymptotically stable equilibrium of the slower o.d.e., verifying (A7). Also, (A8) is (v) in the statement of Theorem 3.2. Therefore the assumptions (A1)− (A8) are verified. The proof then follows from Theorem 3.1.\nRemark 1. Because of the fact that the gradient is a product of two expectations the scheme is a “pseudo”-gradient descent which helps to find the global minimum here.\nRemark 2. Here we assume the stability of the iterates (2) and (3). Certain sufficient conditions have been sketched for showing stability of single timescale stochastic recursions with controlled Markov noise [12, p. 75, Theorem 9]. This subsequently needs to be extended to the case of two time-scale recursions. In this context we mention that the way single timescale Borkar-Meyn theorem was used in [10] to prove stability of two time-scale recursions is not a proper way to prove the same.\nRemark 3. Convergence analysis for ONTDC along with eligibility traces cf. [2, p. 74] where it is called GTD(λ) can be done similarly using our results. The main advantage is that it works for λ < 1Lγ (λ ∈ [0, 1] being the eligibility function) whereas the analysis in [4] is shown only for λ very close to 1."
    }, {
      "heading" : "4 Empirical results",
      "text" : "For the assessment of the algorithm experimentally we have compared the result on a variation of the classic Baird’s off-policy counter-example [2, Fig. 2.4] and θ → 2θ problem [8, Section 3]. In both cases, we compare the TD(0), OFFTDC and ONTDC. Unlike [10] where updating was done synchronously in dynamic-programming-like sweeps through the state space, we consider the usual stochastic approximation scenario where only simulated sample trajectories are taken as input to the algorithms i.e. the algorithms do not use any knowledge of the probabilities for the underlying Markov decision process. For Baird’s problem our performance metric is Root Mean Squared Error (RMSE) defined to be the square root of the average of the square of the deviation between true value function and the estimated value function. For θ → 2θ problem the y-axis is θ itself. The average is taken over 1000 simulation run and the metric is plotted against the number of times θn is updated. While the analysis has been shown for the diminishing step-size case, we implement here the algorithm with constant step-sizes as in [2, 10].\nThe θ → 2θ problem consists of only 2 states where θ and 2θ are the estimated value of the states. According to its behavior policy with probability p = 12 it stays on the same state and chooses the other state. The target policy is to choose the action that accesses the second state with probability 1 (See Fig. 1 in [8, Section 3] for details). The constant step-sizes are chosen as a(n) = .075; b(n) = .05 for the two time-scale algorithms and α = .075 for single timescale algorithms. The simulations are run for 1000 different sample paths. Rewards in all transitions are zero. The initial values are θ = 1 and w = 0. The results are summarized in Figure 2.\nNext we consider the ’7-star’ version of Baird’s counter example from [2, p .17] All the rewards in transitions are zero and true value function for each state is zero. The value functions are approximated as V (s) = 2θ(s) + θ0 ∀s ∈ {1, 2 . . . 6} and V (7) = θ(7) + 2θ0. The behaviour policy is to choose the state 7 with probability\nq = 17 and choose uniformly states 1 − 6 with probability (1 − q) = 6 7 . The target policy is to choose the state 7 with probability 1. The step size chosen for this setting is a = .005, b = .05. The initial parameters are θ = (1, 1, 1, 1, 1, 1, 10, 1) and w = 0. The results in this case are summarized in Figure 1.\nIn both cases (Fig. 2 and 1) ONTDC performs better than the OFFTDC. The difference becomes more apparent when behaviour policy differs significantly from the target policy (Fig 3 and 4). The intuition is that in case of OFFTDC the TD update is weighted by only step-size whereas in case of ONTDC it is additionally weighted by ρn. Therefore by changing the behaviour policy one can improve the rate of convergence of the algorithm. In the case of on-policy learning for the θ → 2θ problem, Figure 5 shows that with eligibility traces the performance of ONTDC is much closer to TD(λ) compared to the case with λ = 0.\nAlthough ONTDC uses importance weighting in its update, this is not importance sampling used in Monte-Carlo algorithms which is the source of high variance. Further, ONTDC does not have any follow-on trace like emphatic TD which has a high variance. We show in Fig. 6 that the variances of the performance metric for the ONTDC is negligible eventually for the two standard counterexamples.\nFor both the aforementioned examples the results for the extension to eligibility\ntraces (the algorithm is called GTD(λ) or TDC(λ)) can be seen in Fig. 7 with λ = 0.1. Fig. 8 shows the results of experiments where the step-size sequences obey the requirements in (A5). We observe good convergence behaviour in this case that is also better when compared with the case of constant step-sizes as considered in the main paper."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented almost sure convergence proof for an off-policy temporal difference learning algorithm that is also extendible to eligibility traces (for a sufficiently large range of λ) with linear function approximation under the assumption that the “on-policy” trajectory for a behaviour policy is only available. This has previously not been done to\nour knowledge. A future direction would be to similarly extend algorithms for off-policy control ([3]) to the more realistic settings as we consider in this paper."
    } ],
    "references" : [ {
      "title" : "Off-policy learning based on weighted importance sampling with linear computational complexity",
      "author" : [ "A.R.Mahmood", "R.S.Sutton" ],
      "venue" : "Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Toward off-policy learning control with function approximation",
      "author" : [ "H.R.Maei", "C.Szepesvári", "S.Bhatnagar", "R.S.Sutton" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S.Sutton", "A.G.Barto" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "An emphatic approach to the problem of offpolicy temporal-difference learning",
      "author" : [ "R.S.Sutton", "A.R.Mahmood", "M.White" ],
      "venue" : "Technical report, University of Alberta,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Fast gradientdescent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S.Sutton", "H.R.Maei", "D.Precup", "S.Bhatnagar", "D.Silver", "E.Wiewiora" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "See [7] for additional uses.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "In [9, 10, 2] the gradient temporal difference learning (GTD) algorithms were proposed to solve this problem.",
      "startOffset" : 3,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "It was shown in [10] that such an algorithm can be proved to be convergent using the classical convergence proof for two timescale stochastic approximation with martingale difference noise [11].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Recently, emphatic temporal difference learning has been introduced in [8] to solve the off-policy evaluation problem.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Another related work is the much complex off-policy learning algorithms that obtain the benefits of weighted importance sampling (to reduce variance) with O(d) computational complexity [1].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "In this context we mention that the way single timescale Borkar-Meyn theorem was used in [10] to prove stability of two time-scale recursions is not a proper way to prove the same.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "The main advantage is that it works for λ < 1 Lγ (λ ∈ [0, 1] being the eligibility function) whereas the analysis in [4] is shown only for λ very close to 1.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "Unlike [10] where updating was done synchronously in dynamic-programming-like sweeps through the state space, we consider the usual stochastic approximation scenario where only simulated sample trajectories are taken as input to the algorithms i.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "While the analysis has been shown for the diminishing step-size case, we implement here the algorithm with constant step-sizes as in [2, 10].",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "A future direction would be to similarly extend algorithms for off-policy control ([3]) to the more realistic settings as we consider in this paper.",
      "startOffset" : 83,
      "endOffset" : 86
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we provide a rigorous convergence analysis of a “off”-policy temporal difference learning algorithm with linear function approximation and per time-step linear computational complexity in “online” learning environment. The algorithm considered here is TDC with importance weighting introduced by Maei et al. We support our theoretical results by providing suitable empirical results for standard off-policy counterexamples.",
    "creator" : "LaTeX with hyperref package"
  }
}