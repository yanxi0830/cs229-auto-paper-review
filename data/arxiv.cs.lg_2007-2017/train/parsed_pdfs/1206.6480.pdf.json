{
  "name" : "1206.6480.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Dantzig Selector Approach to Temporal Difference Learning",
    "authors" : [ "Matthieu Geist", "Bruno Scherrer", "Alessandro Lazaric", "Mohammad Ghavamzadeh" ],
    "emails" : [ "matthieu.geist@supelec.fr", "bruno.scherrer@inria.fr", "firstname.lastname@inria.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "An important problem in reinforcement learning (RL) (Sutton & Barto, 1998) is to estimate the quality of a given policy through the computation of its value function (e.g., in the policy evaluation step of a policy iteration). Oftentimes, the state space is to large, and thus, approximation schemes must be used to represent the value function. Furthermore, whenever the model (reward function and probability transitions) is unknown, the best approximation should be computed using a set of sampled transitions. Many algorithms\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nhave been designed to solve this approximation problem. Among them, LSTD (Least-Squares Temporal Differences) (Bradtke & Barto, 1996) is the most popular. Using a linear parametric representation, LSTD computes the fixed-point of the Bellman operator composed with the orthogonal projection.\nIn many practical scenarios, the number of features of the linear approximation is much larger than the number of available samples. For example, one may want to consider a very rich function space, such that the actual value function lies in it. Unfortunately, in this case, learning is prone to overfitting. A standard approach to face this problem is to introduce some form of regularization. While LSTD has been often paired with `2-regularization, only recently `1-regularization (see Sec. 2.2 for a thorough review of the main `1- regularized algorithms) has been considered to deal with high–dimensional problems. This approach is particularly appealing since `1-regularization implicitly performs feature selection and targets sparse solutions. In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero. However, LASSO-TD is not derived from a proper convex optimization problem, and thus, it requires some assumptions that might not hold in an off-policy setting. Although other algorithms have been proposed to overcome these drawbacks (e.g., `1-PBR by Geist & Scherrer (2011)), other disadvantages may appear.\nThis paper introduces a new algorithm, Dantzig-LSTD (D-LSTD for short, see Sec. 3), which extends the Dantzig Selector (DS) (Candes & Tao, 2007) to temporal difference learning. Instead of solving a fixed-point problem as in LASSO-TD, it can simply be cast as a linear program, thus allowing to use any off-the-shelf solver. Furthermore, since the underlying optimiza-\ntion problem is convex, it can handle off-policy learning in a principled way. Yet, when LASSO-TD is well defined, both algorithms provide similar solutions (see Prop. 2), as DS does w.r.t. LASSO. We show that for some oracle choice of the regularization factor, the DLSTD solution converges quickly to the LSTD solution (at a rate depending only logarithmically on the number of features), as shown in Theorem 1. This new algorithm also opens some issues, namely how well is the true value function estimated and how to efficiently choose the regularization factor. These points are discussed in Sec. 4. Finally, we report some illustrative empirical results in Sec. 5."
    }, {
      "heading" : "2. LSTD and Related Work",
      "text" : "A Markov reward process1 (MRP) is a tuple {S, P,R, γ}, where S is a finite state space, P = (p(s′|s))1≤s,s′≤|S| is the transition matrix, R = (r(s))1≤s≤|S| with ‖R‖∞ ≤ rmax is the reward vector, and γ is a discount factor. The value function V is defined as the expected cumulative reward from a given state s, V (s) = E[ ∑∞ t=0 γ\ntrt|s0 = s]. It is the unique fixed-point of the Bellman operator T : V → R+γPV .\nIn many practical applications, the model of the MRP (i.e., the reward R and transitions P ) is unknown and only a set of n transitions {(si, ri, s′i)1≤i≤n} is available. In general, we assume that states s1, . . . , sn are sampled from a sampling distribution µ (not necessarily the stationary distribution of the MRP) and the next states s′1, . . . , s ′ n are generated according to the transition probabilities p(·|si). Whenever the state space is too large, the value function cannot be computed exactly at each state and a function approximation scheme is needed. We consider value functions V̂θ defined as a linear combination of p basis functions φi(s), that is V̂θ(s) = ∑p i=1 θiφi(s) = θ\n>φ(s). We denote by Φ ∈ R|S|×p the feature matrix whose rows contain the feature vectors φ(s)> for any s ∈ S. This defines a hypothesis space H = {Φθ|θ ∈ Rp}, which contains all the value functions that can be represented by the features φ. The objective is to find the function V̂θ∗ that approximates V the best."
    }, {
      "heading" : "2.1. LSTD",
      "text" : "Let Πµ denote the orthogonal projection onto H w.r.t. the sampling distribution µ. If Dµ is the diagonal matrix with elements µ(s) and Mµ = Φ>DµΦ is the Gram matrix, then the projection operator is Πµ = ΦM−1µ Φ >Dµ. Motivated by the fact that the value\n1This can easily be extended to Markovian decision processes that reduce to MRPs for fixed policies.\nfunction is the fixed point of the Bellman operator T , the LSTD algorithm computes the fixed–point of the joint ΠµT operator: V̂θ∗ = ΠµT V̂θ∗ . Let us define A ∈ Rp×p and b ∈ Rp as A = Φ>Dµ(I−γP )Φ and b = Φ>DµR. In the following we assume that A and Mµ are invertible. It can be shown through simple algebra that V̂θ∗ is the fixed-point of ΠµT if and only if θ∗ is the (unique) solution to Aθ∗ = b. This relationship is particularly interesting since it shows that computing the fixed point ΠµT is equivalent to solving a linear system of equations defined by A and b.\nSince P and R are not usually known, we have to rely on sample–based estimates. In particular, we define Φ̃ (resp. Φ̃′) ∈ Rn×p the empirical feature matrices whose rows contain the feature vectors φ(si)> (resp φ(s′i) >), and R̃ ∈ Rn the reward vector of rowcomponents ri. The random matrices Ã and b̃ are then defined as Ã = 1n Φ̃ >∆Φ̃ and b̃ = 1n Φ̃ >R̃ with ∆Φ̃ = Φ̃ − γΦ̃′. LSTD computes the solution θ0 of the sample–based linear system Ãθ0 = b̃. We notice that both Ã and b̃ are unbiased estimators of the model–based matrices A and b (i.e., E[Ã] = A and E[b̃] = b), thus suggesting that as the number of samples increases, the solution of LSTD θ0 converges to the model–based solution θ∗. Since LSTD computes the fixed point of the joint operator ΠµT , then the sample–based LSTD solution can also be formulated in an equivalent form as the solution of two nested optimization problems:{\nωθ = argminω ‖R̃+ γΦ̃′θ − Φ̃ω‖22 θ0 = argminθ ‖Φ̃θ − Φ̃ωθ‖22 , (1)\nwhere the first equation projects the image of the estimated value function V̂θ under the Bellman operator T onto the hypothesis space H, and the second one solves the related fixed-point problem."
    }, {
      "heading" : "2.2. Related Work",
      "text" : "When the number of samples is close or smaller than the number of features, the matrix Ã is ill–conditioned and some form of regularization should be employed to solve the LSTD problem. In this section, we review the state–of–the–art regularized LSTD algorithms.\nThe formulation of LSTD in Eq. 1 is particularly helpful in understanding the different regularization schemes that could be applied to LSTD. In particular, each of the minimizations relative to the operators Πµ and T can be regularized, thus obtaining:{\nωθ = argminω ‖R̃+ γΦ̃′θ − Φ̃ω‖22 + λ1pen1(ω) θλ1,λ2 = argminθ ‖Φ̃θ − Φ̃ωθ‖22 + λ2pen2(θ) .\nWith this formulation, all the regularization schemes for LSTD (except `1-LSTD, which we discuss at the end of this section) can be summarized as in Tab. 1.\nRidge regression (i.e., `2-regularization) is the most common form of regularization and it simply adds a term λI to Ã. This corresponds to λ1pen1(ω) = λ‖ω‖22 and λ2 = 0 and it has been generalized by Farahmand et al. (2008) with `2,2-LSTD, where both penalty terms use an `2-norm regularization. Although these approaches can help in dealing with ill–defined Ã matrices, they are not specifically designed for the case of n p, where the optimal solution is sparse. In fact, it is well-known that, unlike `1–regularization, `2 does not promote sparsity, and thus, it might fail when the number of samples is much smaller than the number of features.\nThe `1-regularization has been introduced more recently with LASSO-TD, where the projection is replaced by an `1-penalized projection. In this case, the nested optimization problem in Eq. 1 reduces to solving the fixed-point problem (if well defined): θl,λ = argminθ ‖R̃+γΦ̃′θl,λ−Φ̃θ‖22+λ‖θ‖1. This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad–hoc variation of the LARS algorithm (Efron et al., 2004). For LARS-TD to find a solution, Ã must be a P-matrix.2 Unfortunately, this may not be true when the sampling and stationary distributions are different (off-policy learning). Although this does not always affect the performance in practice (see some of the experiments reported in Kolter & Ng 2009), it would be desirable to remove or relax this condition. The LARSTD idea is further developed by Johns et al. (2010), where LASSO-TD is reframed as a linear complementary problem. This allows using any off-the-shelf LCP solver (notably some of them allow warm-starts, which may be of interest in a policy iteration context), but the P-matrix condition is still required, since it is inherent to the optimization problem and not to how it is actually solved. Finally, the theoretical properties of LASSO-TD were analyzed by Ghavamzadeh et al.\n2A P-matrix is a matrix that all its principal minors are positive (generalizing positive definite matrices).\n(2011), who provided prediction error bounds in the on-policy fixed design setting (i.e., the performance is evaluated on the points in the training set). In particular, they show that, similarly to LASSO in regression, the prediction error depends on the sparsity of the projection of the value function (i.e., the `0-norm of the θ parameter of ΠµV ), and it scales only logarithmically with the number of features. This implies that even if the dimensionality of H is much larger than the number of samples, the LASSO-TD accurately approximates the true value function in the on– policy setting. In order to alleviate the P-matrix problem, the `1-PBR (Projected Bellman residual) (Geist & Scherrer, 2011) and the `2,1-LSTD (Hoffman et al., 2011) algorithms have been proposed. The idea is to place the `1-regularization term in the fixed-point equation instead of the projection equation. This corresponds to adding an `1-penalty term to the projected Bellman residual minimization (writing Π̃ the empirical projection and T̃ the sampled Bellman operator): θpbr,λ = argminθ ‖Π̃(Φ̃θ−T̃ (Φ̃θ))‖2+λ‖θ‖1. Since this is a convex optimization problem, there is no problem for Ã not being a P-matrix, and off-the-shelf LASSO solvers can be used. However, this comes at the cost of a high computational cost if n p (notably in the computation the empirical projection, which could be as bad as O(p3)), and there is no theoretical analysis.\nFinally, a novel approach has been introduced by Pires (2011). The idea is to consider the linear system formulation of LSTD (i.e., Aθ = b) and to add an `1- penalty term to it: θ1,λ = argminθ ‖Ãθ− b̃‖22 + λ‖θ‖1. We refer to this algorithm as `1-LSTD. Being defined as a proper convex optimization problem, it does not have theoretical problems in the off-policy setting and any standard solver can be used. Notice that for γ = 0, `1-LSTD does not reduce to a known algorithm."
    }, {
      "heading" : "3. Dantzig-LSTD",
      "text" : "The Dantzig-LSTD (D-LSTD for short) algorithm that we propose in this paper returns an estimate θd,λ (i.e., a value function Vθd,λ) with a low `1-norm under the constraint that the Bellman residual (R̃ + γΦ̃′θ − Φ̃θ), namely the correlated Bellman residual (Φ̃>(R̃ + γΦ̃′θ − Φ̃θ) = b̃− Ãθ), is smaller than a parameter λ. Formally, D-LSTD solves:\nθd,λ = argmin θ∈Rp ‖θ‖1 subject to ‖Ãθ − b̃‖∞ ≤ λ. (2)\nThis optimization problem is convex and can be easily recast as a linear program (LP):\nmin u,θ∈Rp\n1>u subject to { −u ≤ θ ≤ u −λ1 ≤ Ãθ − b̃ ≤ λ1 .\nThis algorithm is closely related to DS (Candes & Tao, 2007), to which it reduces when γ = 0. Being a convex optimization problem, it does not require Ã to be a P-matrix and it can be solved using any LP solver (notably the efficient primal–dual interior point method of Candes & Tao 2007, which makes use of the Woodbury matrix identity when n p)."
    }, {
      "heading" : "3.1. A Finite Sample Analysis",
      "text" : "In this section we study how well the D-LSTD solution θd,λ compares to θ∗, i.e., the model–based LSTD solution satisfying Aθ∗ = b. The analysis follows similar steps as in Pires (2011) for `1-LSTD. In the following, we use the assumption that the samples are generated i.i.d. from an arbitrary sampling distribution µ. We leave as future work the extension to Markov design (i.e., when the samples are generated from a single trajectory of the policy under evaluation).\nTheorem 1. Let B∞,φ = maxs∈S ‖φ(s)‖∞, the DLSTD solution θd,λ (Eq. 2) satisfies\ninf λ ‖Aθd,λ − b‖∞ ≤ (3)\n2 (‖θ∗‖1(1 + γ)B∞,φ + rmax)B∞,φ √ 4 n ln 8p δ ,\nwith probability at least 1− δ.\nProof. (sketch) We first need a concentration result for the `∞-norm. Let x1, . . . , xn be i.i.d. random vectors with mean x̄ ∈ Rd and bounded by ‖xi‖∞ ≤ B. Using Hoeffding inequality and a union bound, it is easy to show that with probability greater that\n1 − δ, one has ‖ 1n ∑n i=1 xi − x̄‖∞ ≤ B √ 2 n ln 2d δ . Let ∆A,max = ‖A − Ã‖max (entrywise max norm) and ∆b,max = ‖b − b̃‖∞. We have the following consistency inequality: ‖Aθ‖∞ ≤ ‖A‖max‖θ‖1. Combined with the triangle inequality, this gives: |‖Aθ − b‖∞ − ‖Ãθ − b̃‖∞| ≤ ∆A,max‖θ‖1 + ∆b,max. Let us choose λ = ∆A,max‖θ∗‖1 + ∆b,max. The previous inequality implies that ‖Ãθ∗ − b̃‖∞ ≤ λ (recall that Aθ∗ = b). Combined with the fact that θd,λ minimizes Eq. 2, we have that ‖θd,λ‖1 ≤ ‖θ∗‖1. Combining the previous results, we obtain ‖Aθd,λ − b‖∞ ≤ 2∆A,max‖θ∗‖1 + 2∆b,max. The concentration result for ‖.‖∞ can be used to bound ∆A,max and ∆b,max, which gives the stated result, using the fact that ‖φ(si)(φ(si) − γφ(s′i))T ‖max ≤ B2∞,φ(1 + γ) and that ‖φ(si)ri‖∞ ≤ B∞,φrmax.\nSince the algorithm is specifically designed for the high–dimensional setting (n p), it is critical to study\nthe dependency of the performance on n and p. Up to constant terms, the previous bound can be written as\ninf λ ‖Aθd,λ − b‖∞ ≤ O\n( ‖θ∗‖1 √ 1 n ln p δ ) .\nFirst we notice that as the number of samples increases, the error of θd,λ tends to zero, thus implying that it matches the performance of the model–based LSTD solution θ∗. Furthermore, the dependency on the number of features p is just logarithmic, while the `1-norm of θ∗ is assumed to be small whenever the solution is sparse. This suggests that D-LSTD could work well even in the case n p whenever the problem admits a sparse LSTD solution. Finally, we also notice that there is no specific assumption regarding the learning setting, except that A should be invertible. This is particularly important because it means that, unlike most of the other results available for LSTD (e.g., see Ghavamzadeh et al. 2011), this result holds also in the off-policy setting. The main drawback of this analysis is that it holds for an oracle choice of λ. We postpone a discussion about how to choose the regularizer in practice to Sec. 4."
    }, {
      "heading" : "3.2. Comparison to Other Algorithms",
      "text" : "Similar to `1-PBR and `2,1-LSTD, D-LSTD is based on a well-defined standard convex optimization problem, which does not require Ã to be a P-matrix (unlike LASSO-TD) and that can be solved using any off-theshelf solvers. Nonetheless, D-LSTD has only one metaparameter (instead of two), and in general, it has a smaller computational cost w.r.t. solving the nested optimization problems of `1-PBR and `2,1-LSTD.\nD-LSTD is also related to LASSO-TD: Proposition 2. The LASSO-TD solution θl,λ (if it exists) satisfies the D-LSTD constraints:\n‖Ãθl,λ − b̃‖∞ ≤ λ.\nProof. The optimality conditions of LASSO-TD can be obtained by ensuring that 0 belongs to the subgradient of 12‖Φ̃θ − (R̃ + γΦ̃\n′θl,λ)‖22 + λ‖θ‖1 and then substituting θ by θl,λ (Kolter & Ng, 2009). This notably implies that for all 1 ≤ i ≤ p, we have −λ ≤ (b̃− Ãθl,λ)i ≤ λ, which is the stated result.\nTherefore, D-LSTD and LASSO-TD satisfy the same constraints, but ‖θl,λ‖1 ≥ ‖θd,λ‖1, thus suggesting a more sparse solution. This is not surprising, since DLSTD relates to LASSO-TD in a similar way as DS does to LASSO (Bickel et al., 2009). However, thanks to its definition as a convex optimization problem, DLSTD avoids the main drawbacks of LASSO-TD (notably the P-matrix requirement).\nSimilar to `1-LSTD, D-LSTD is built on the linear system of equations formulation of LSTD. Both approaches relax the condition Ãθ = b̃ (using an `2- norm of the error for `1-LSTD and an `∞-norm for D-LSTD) while penalizing model complexity through the `1-norm of the parameter vector. Both algorithms have the same advantages compared to LASSO-TD and to `1-PBR/`2,1-LSTD. Their main difference lies in their convergence rate. A result similar to Theorem 1 exists for `1-LSTD (Pires, 2011):\ninf λ ‖Aθ1,λ − b‖2 ≤ O\n( ‖θ∗‖1 √ p2\nn ln 1 δ\n) .\nAlthough controlling the `2-norm (in `1-LSTD) may be harder than the `∞-norm (as in D-LSTD), `1-LSTD has a very poor dependency on p, which makes the bound not informative as n p. On the other hand, D-LSTD just has a logarithmic dependency on p."
    }, {
      "heading" : "4. Discussion",
      "text" : "In this section, we discuss how the error ||Aθ − b|| relates to the value function prediction error and how to choose the regularizer λ in practice."
    }, {
      "heading" : "4.1. From the Parameters to the Value",
      "text" : "Similar to Yu & Bertsekas (2010), we can link V − V̂θ to Aθ − b as in the next theorem. Theorem 3. For any V̂θ = Φθ, we have the component–wise equality:\nV − V̂θ = (I − γΠµP )−1((V −ΠµV ) + ΦM−1µ (Aθ̂ − b)).\nProof. Recall that V = TV (for the true value function) and that V̂θ = ΠµV̂θ (for an estimate V̂θ = Φθ belonging to the hypothesis space). We have that:\nV −ΠµV = V −ΠµTV − (V̂θ −ΠµT V̂θ) + (V̂θ −ΠµT V̂θ) = (I − γΠµP )(V − V̂θ) + Πµ(V̂θ − T V̂θ),\nV − V̂θ = (I − γΠµP )−1 ` (V −ΠµV ) + Πµ(T V̂θ − V̂θ) ´ .\nNote that Πµ(T V̂θ − V̂θ) = ΦM−1µ (b − Aθ) gives the result.\nIn order to have the final prediction error, we apply the `∞-norm to Theorem 3. Let Lφµ = maxs ‖M−1µ φ(s)‖1, using Theorem 1, we obtain\ninf λ ‖V − V̂θd,λ‖∞ ≤ ‖(I − γΠµP )−1‖∞×( ‖V −ΠµV ‖∞ +O ( ‖θ∗‖1Lφµ √ 1 n ln p δ )) .\nIn general, the previous expression cannot be simplified any further. Nonetheless, under a high– dimensional assumption ΠµP = P and ΠµR = R. Therefore, the hypothesis space H is stable by the Bellman operator T and V ∈ H. In this case, we have ‖V − ΠµV ‖∞ = 0 and it can be shown that ‖(I − γΠµP )−1‖∞ = 11−γ . Thus, we obtain the bound (valid also in the off-policy case):\ninf λ ‖V − V̂θd,λ‖∞ ≤ O ( ‖θ∗‖1Lφµ 1− γ √ 1 n ln p δ ) .\nThe main critical term in this bound is Lφµ, which might hide a dependency on the number of features p. In fact, although the specific value of Lφµ depends on the feature space, it is possible to find cases when it grows as √ p (consider an orthonormal basis), thus potentially neutralizing the low dependency on p in Theorem 1. It is an open question whether this dependency on p is intrinsic to the algorithm or is an artifact of the proof. In fact, if θd,λ solves the linear system of equations accurately, then we expect that the corresponding function V̂θd,λ performs almost as well as the model–based solution V̂θ∗ . The experiments of Section 5 seem to confirm this conjecture."
    }, {
      "heading" : "4.2. Cross Validation",
      "text" : "The result of Theorem 1 holds for an oracle value of λ. In practice, the choice of λ can only be directed by the available data. This issue is of great practical importance, though not often discussed in the RL literature (especially for `1-penalized LSTD variations). In supervised learning, algorithms minimize a risk being defined as the (empirical) expectation of some loss function. Cross-validation consists in using an independent sample to estimate the true risk function, and the meta-parameter is selected as the one minimizing the estimated true risk. However, for value function estimation, there is no such risk, and crossvalidation cannot be used. A general model selection method has been derived for value function estimation by Farahmand & Szepesvári (2011). However, we may devise an ad–hoc (and simple) solution for D-LSTD. Since D-LSTD is defined as a proper convex optimization problem (which reduces to a supervised learning problem when γ tends to 0), one may be tempted to use standard cross-validation. Unfortunately, this is not directly possible. Indeed, ‖Ãθ − b̃‖∞ is the loss (‖.‖∞) of an empirical average (Ã and b̃) rather than the empirical expectation of a loss. However, we can still consider some heuristics. Assume that we want to estimate ‖Aθ − b‖∞ for some fixed parameter vector θ. Let Ã, b̃ be unbiased estimates of A, b, then\n‖Aθ−b‖∞ ≤ E[‖Ãθ−b̃‖∞] (Jensen’s inequality). Thus, given an independent set of samples, we have access to an unbiased estimate of an upper–bound of ‖Aθ−b‖∞. Based on this evidence, we propose a K-fold crossvalidation-based heuristic for D-LSTD. Assume that the training set is split in K folds Fk. Let θ(−k)d,λ denote the estimate trained without Fk, and ÃFk and b̃Fk be the quantities computed with only the samples in Fk. A heuristic is to choose the λ that minimizes\nJ1(λ) = 1 K K∑ i=1 ‖ÃFkθ (−k) d,λ − b̃Fk‖∞. (4)\nHowever, since we are interested in the case n p and the estimate ÃFk is computed with n K samples, it may have a high variance. An alternative (which we empirically found to be more efficient), at the cost of adding some bias, is to choose λ by minimizing\nJ2(λ) = 1 K K∑ i=1 ‖Ãθ(−k)d,λ − b̃‖∞. (5)\nA similar heuristic can be devised for `1-LSTD. Although the previous heuristic worked well in our experiments, it does not have any theoretical guarantees. A different model selection strategy has been devised for `1-LSTD by Pires (2011). It consists in choosing λ̂ = argmin[a,b] ‖Ãθ1,λ − b̃‖22 + λ′‖θ1,λ‖1 with [a, b] an exponential grid and λ′ can be computed from data (no oracle choice). This does not require splitting the learning set while ensuring a bound for ‖Aθ1,λ̂ − b‖2. We leave the adaptation of this model selection strategy to D-LSTD for future work."
    }, {
      "heading" : "5. Illustration and Experiment",
      "text" : "Sec. 5.1 presents an example that shows D-LSTD alleviates the potential problem of off-policy learning. Sec. 5.2 reports a more complex corrupted chain illustrating the case of n p, in an on- and off-policy setting, and studies (heuristic) cross-validation."
    }, {
      "heading" : "5.1. A Pathological MDP",
      "text" : "We consider a simple two-state MDP (e.g., see Kolter & Ng 2009). The transition matrix and reward vector are P = ( 0 1 0 1 ) and R = ( 0 −1\n)>. The optimal value function is therefore V = −11−γ ( γ 1\n)> with γ the discount factor. Let us consider the one-feature approximation Φ = ( 1 2\n)>. We compare the (asymptotic) regularization paths of LASSO-TD (Kolter & Ng, 2009), `1-LSTD and D-LSTD, in the on-policy and off-policy cases (where LASSO-TD fails).\nOn-policy Case. In the on-policy case, the sampling distribution is µ> = ( 0 1 ) . The regularization paths for each algorithm can be computed easily by solving analytically the optimality conditions (there is only one parameter) and they are reported in Fig. 1, top panels. LASSO-TD and D-LSTD have the same regularization path. This was expected, as there is only one parameter, but this is not true in general (recall that LASSO-TD and D-LSTD inherit the same differences as LASSO and DS).\nOff-policy Case. Let us now consider the uniform distribution µ> = ( 1 2 1 2 ) . For γ > 56 , A is not a Pmatrix and LASSO-TD does not have a unique solution, nor a piecewise linear regularization path. Paths are shown on Fig. 1, bottom panels. The `1-LSTD’s path is still well-defined. LASSO-TD has more than one solution. The interesting fact here is that DLSTD’s path is well-defined, there is always a unique solution, and the path is piecewise linear. Note that both in the on- and off-policy cases all the algorithms provide the LSTD solution for λ = 0."
    }, {
      "heading" : "5.2. Corrupted Chain",
      "text" : "We consider the same chain problem as in Kolter & Ng (2009) and Hoffman et al. (2011). The state s has s̄ + 1 components si. The first one is an integer (s1 ∈ {1 . . . 20}) that evolves according to a 20-state, 2-action MDP (states are connected by a chain, the action chooses the direction, and the probability of success is 0.9). All other state components are random Gaussian noises si+1t ∼ N (0, 1). The reward is +1 if s1t = 1 or 20. The feature vector φ(s) ∈ Rs̄+6 consists of an intercept (constant function), 5 radial basis functions corresponding to the first state component, and s̄ identity functions corresponding to the irrelevant components: φ(s) = ( 1 RBF1(s1) . . .RBF5(s1) s2 . . . ss̄+1 )> .We\ncompare LASSO-TD (with its LARS-like implementation), `1-LSTD, and D-LSTD (for which we used `1-magic (Romberg, 2005)).3 We standardize the data by removing the intercept, centering the observations, centering and standardizing the features Φ̃, and applying the same transformation (computed from Φ̃) to Φ̃′. The intercept can be computed analytically, it is the mean Bellman error (without regularization, this allows recovering the LSTD solution). We also consider `2,∅-LSTD, i.e., the standard `2-penalized LSTD.\nOn-policy Evaluation. We first study the on-policy problem. The evaluated policy is the optimal one (going left if s1 ≤ 10, and right otherwise). We sample 400 transitions (20 trajectories of length 20 started randomly on {1 . . . 20}) and vary the number s̄ of irrelevant features between 800 and 1400. Results are presented in Fig. 2, averaged over 20 independent runs. For LARS-TD, we computed the whole regularization path (at least until too many features are added) and trained the other algorithms for a set of regularization parameters (logarithmically spaced between 10−3 and 10). Each time, we report the best prediction error (on 500 test points, such that the first state component is uniformly sampled from {1 . . . 20}), computed w.r.t. the true value function (therefore, this is an oracle choice). All `1-penalized approaches perform significantly better than the `2-penalization ones, showing that their performance have only a very mild dependency on the dimensionality p (as predicted by Theorem 1). Among them, LASSO-TD seems to be consistently better, followed closely by D-LSTD and `1- LSTD. For LASSO-TD and `2,∅-LSTD, these results are consistent with those published by Hoffman et al. (2011). Notice that there was more choice of regularization parameters for LASSO-TD, as the whole regularization path was computed. This may explain the\n3We also considered `1-PBR/`2,1-LSTD. Results are not reported for the sake of clarity, but they behave like `1- LSTD, so worse than LASSO-TD/D-LSTD.\nbetter results of LASSO-TD compared to D-LSTD.\nHeuristic Cross-validation. All results of Fig. 2 require an oracle to choose the right regularization parameter. This is not practical in a real setting. As explained in Sec. 4, `1-LSTD and D-LSTD can benefit from a heuristic cross-validation scheme. We tested K-fold cross-validation (with K = 5) on this problem, with the schemes J1 (Eq. 4) and J2 (Eq. 5) for n = 400 training samples and s̄ = 800 irrelevant features (results averaged over 20 independent runs). Results are reported in Tab. 2. The error is computed as before, but here it is not used to choose the regularization parameter. The results for J1 are quite bad, probably due to the high variance of the related estimator (still, for D-LSTD, the right regularization parameter is often chosen, apart from a few outliers). The J2 scheme is much better, comparable to the oracle scheme (see Fig. 2). Comparing the results of the J2 heuristic using a Behrens-Fisher t-test, `1-LSTD and LASSO-TD are different (5% risk), but not D-LSTD and LASSO-TD.\nOff-policy Evaluation. Here we test the off-policy evaluation problem. Let πopt be the optimal policy (going left if s1 ≤ 10 and right otherwise) and πworst = 1 − πopt (going right if s1 ≤ 10 and left otherwise). We define πα = (1 − α)πopt + απworst, with α ∈ [0, 12 ]. Let also µα be the corresponding stationary distribution and recall V is the true value function. We consider the same problem as before, with s̄ = 800. For values of α varying from 0 to 0.5, we sample n = 400 chain states according to µα as well as the associated transitions according to the optimal policy. The regularization parameter is chosen to minimize the error between the true value function and the\nestimated one on the training set (thus, an oracle-like selection procedure), for all algorithms. Results are averaged over 50 independent runs. Fig. 3 shows the error ‖V̂α − V ‖µα as a function of α. The term 0 corresponds to the zero prediction, that is ‖V ‖µα . In all cases, D-LSTD seems to be slightly better than the others, and things get worse as going away from the stationary distribution (as α increases). In no case LASSO-TD seems to suffer from off-policy learning, suggesting that in this case the P -matrix condition is satisfied. Also, the difference between `2- and `1- schemes decreases as α increases. An `1-schemes may help when there are much more features than samples, but there is little to do when the mismatch between distributions increases. Even if not reported, all approaches performed equally bad when α tends to one, since there is no more valuable information in the data."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we introduced the Dantzig-LSTD algorithm with the objective of removing the drawbacks of existing `1-schemes for temporal difference learning. Since D-LSTD is defined as a standard linear program, it does not require Ã to be a P-matrix and can be computed using any LP solver. The D-LSTD estimate is a good approximation of the asymptotic LSTD solution in the sense of Theorem 1. It is also close to the LASSO-TD estimate (whenever well defined) in the sense of Prop. 2. In fact, D-LSTD inherits the same difference that the Dantzig selector has w.r.t. LASSO. Also, our preliminary experiments show that D-LSTD performs at least as well as LASSO-TD.\nThere are still a number of issues that need further investigation. As discussed in Sec. 4, when moving from the linear system of equations to the prediction error, an additional dependency on the number of features seems to appear. To which extent this dependency is an artifact of the proof or a characteristic of the algorithm is not fully clear yet. As for the choice of the regularization parameter, we plan to adapt the model selection scheme of `1-LSTD (Pires, 2011) to D-LSTD and test it. Finally, we plan to test D-LSTD in control schemes (i.e., policy iteration).\nAcknowledgments The first author thanks the Région Lorraine for financial support. The third and fourth authors would like to thank French National Research Agency (ANR) under project LAMPADA n◦ ANR-09-EMER-007, European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement n◦ 231495, and PASCAL2 European Network of Excellence for supporting their research."
    }, {
      "heading" : "Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous",
      "text" : "analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.\nBradtke, S. J. and Barto, A. G. Linear Least-Squares algorithms for temporal difference learning. Machine Learning, 22:33–57, 1996.\nCandes, E. and Tao, T. The Dantzig selector: statistical estimation when p is much larger than n. Annals of Statistics, 35(6):2313–2351, 2007."
    }, {
      "heading" : "Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R.",
      "text" : "Least Angle Regression. Annals of Statistics, 32(2):407– 499, 2004."
    }, {
      "heading" : "Farahmand, A., Ghavamzadeh, M., Szepesvári, C., and",
      "text" : "Mannor, S. Regularized Policy Iteration. In Proc. of NIPS 21, 2008."
    }, {
      "heading" : "Farahmand, A. M. and Szepesvári, C. Model selection in",
      "text" : "reinforcement learning. Machine Learning Journal, 85 (3):299–332, 2011.\nGeist, M. and Scherrer, B. `1-penalized projected Bellman residual. In Proc. of EWRL 9, 2011."
    }, {
      "heading" : "Ghavamzadeh, M., Lazaric, A., Munos, R., and Hoffman,",
      "text" : "M. Finite-Sample Analysis of Lasso-TD. In Proc. of ICML, 2011."
    }, {
      "heading" : "Hoffman, M. W., Lazaric, A., Ghavamzadeh, M., and",
      "text" : "Munos, R. Regularized Least Squares Temporal Difference learning with nested `2 and `1 penalization. In Proc. of EWRL 9, 2011.\nJohns, J., Painter-Wakefield, C., and Parr, R. Linear Complementarity for Regularized Policy Evaluation and Improvement. In Proc. of NIPS 23, 2010."
    }, {
      "heading" : "Kolter, J. Z. and Ng, A. Y. Regularization and Feature Selection in Least-Squares Temporal Difference Learning.",
      "text" : "In Proc. of ICML, 2009.\nPires, B. A. Statistical analysis of `1-penalized linear estimation with applications. Master’s thesis, University of Alberta, 2011.\nRomberg, J. `1-magic matlab library. http://users.ece. gatech.edu/~justin/l1magic/, 2005.\nSutton, R. S. and Barto, A. G. Reinforcement Learning: an Introduction. The MIT Press, 1998."
    }, {
      "heading" : "Tibshirani, R. Regression Shrinkage and Selection via the",
      "text" : "Lasso. Journal of the Royal Statistical Society, 58(1): 267–288, 1996.\nYu, H. and Bertsekas, D. P. Error Bounds for Approximations from Projected Linear Equations. Mathematics of Operations Research, 35:306–329, 2010."
    } ],
    "references" : [ {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "Linear Least-Squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bradtke and Barto,? \\Q1996\\E",
      "shortCiteRegEx" : "Bradtke and Barto",
      "year" : 1996
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "E. Candes", "T. Tao" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Candes and Tao,? \\Q2007\\E",
      "shortCiteRegEx" : "Candes and Tao",
      "year" : 2007
    }, {
      "title" : "Least Angle Regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Efron et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Efron et al\\.",
      "year" : 2004
    }, {
      "title" : "Regularized Policy Iteration",
      "author" : [ "A. Farahmand", "M. Ghavamzadeh", "C. Szepesvári", "S. Mannor" ],
      "venue" : "In Proc. of NIPS",
      "citeRegEx" : "Farahmand et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2008
    }, {
      "title" : "Model selection in reinforcement learning",
      "author" : [ "A.M. Farahmand", "C. Szepesvári" ],
      "venue" : "Machine Learning Journal,",
      "citeRegEx" : "Farahmand and Szepesvári,? \\Q2011\\E",
      "shortCiteRegEx" : "Farahmand and Szepesvári",
      "year" : 2011
    }, {
      "title" : "`1-penalized projected Bellman residual",
      "author" : [ "M. Geist", "B. Scherrer" ],
      "venue" : "In Proc. of EWRL",
      "citeRegEx" : "Geist and Scherrer,? \\Q2011\\E",
      "shortCiteRegEx" : "Geist and Scherrer",
      "year" : 2011
    }, {
      "title" : "Finite-Sample Analysis of Lasso-TD",
      "author" : [ "M. Ghavamzadeh", "A. Lazaric", "R. Munos", "M. Hoffman" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "Ghavamzadeh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ghavamzadeh et al\\.",
      "year" : 2011
    }, {
      "title" : "Regularized Least Squares Temporal Difference learning with nested `2 and `1 penalization",
      "author" : [ "M.W. Hoffman", "A. Lazaric", "M. Ghavamzadeh", "R. Munos" ],
      "venue" : "In Proc. of EWRL",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2011
    }, {
      "title" : "Linear Complementarity for Regularized Policy Evaluation and Improvement",
      "author" : [ "J. Johns", "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "In Proc. of NIPS",
      "citeRegEx" : "Johns et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Johns et al\\.",
      "year" : 2010
    }, {
      "title" : "Regularization and Feature Selection in Least-Squares Temporal Difference Learning",
      "author" : [ "J.Z. Kolter", "A.Y. Ng" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "Kolter and Ng,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolter and Ng",
      "year" : 2009
    }, {
      "title" : "Statistical analysis of `1-penalized linear estimation with applications",
      "author" : [ "B.A. Pires" ],
      "venue" : "Master’s thesis, University of Alberta,",
      "citeRegEx" : "Pires,? \\Q2011\\E",
      "shortCiteRegEx" : "Pires",
      "year" : 2011
    }, {
      "title" : "Reinforcement Learning: an Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Regression Shrinkage and Selection via the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "Error Bounds for Approximations from Projected Linear Equations",
      "author" : [ "H. Yu", "D.P. Bertsekas" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Yu and Bertsekas,? \\Q2010\\E",
      "shortCiteRegEx" : "Yu and Bertsekas",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero. However, LASSO-TD is not derived from a proper convex optimization problem, and thus, it requires some assumptions that might not hold in an off-policy setting. Although other algorithms have been proposed to overcome these drawbacks (e.g., `1-PBR by Geist & Scherrer (2011)), other disadvantages may appear.",
      "startOffset" : 82,
      "endOffset" : 470
    }, {
      "referenceID" : 4,
      "context" : "This corresponds to λ1pen1(ω) = λ‖ω‖2 and λ2 = 0 and it has been generalized by Farahmand et al. (2008) with `2,2-LSTD, where both penalty terms use an `2-norm regularization.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad–hoc variation of the LARS algorithm (Efron et al., 2004).",
      "startOffset" : 154,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad–hoc variation of the LARS algorithm (Efron et al., 2004). For LARS-TD to find a solution, Ã must be a P-matrix. Unfortunately, this may not be true when the sampling and stationary distributions are different (off-policy learning). Although this does not always affect the performance in practice (see some of the experiments reported in Kolter & Ng 2009), it would be desirable to remove or relax this condition. The LARSTD idea is further developed by Johns et al. (2010), where LASSO-TD is reframed as a linear complementary problem.",
      "startOffset" : 155,
      "endOffset" : 592
    }, {
      "referenceID" : 8,
      "context" : "In order to alleviate the P-matrix problem, the `1-PBR (Projected Bellman residual) (Geist & Scherrer, 2011) and the `2,1-LSTD (Hoffman et al., 2011) algorithms have been proposed.",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : "Finally, a novel approach has been introduced by Pires (2011). The idea is to consider the linear system formulation of LSTD (i.",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "The analysis follows similar steps as in Pires (2011) for `1-LSTD.",
      "startOffset" : 41,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "This is not surprising, since DLSTD relates to LASSO-TD in a similar way as DS does to LASSO (Bickel et al., 2009).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "A result similar to Theorem 1 exists for `1-LSTD (Pires, 2011):",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "A different model selection strategy has been devised for `1-LSTD by Pires (2011). It consists in choosing λ̂ = argmin[a,b] ‖Ãθ1,λ − b̃‖2 + λ‖θ1,λ‖1 with [a, b] an exponential grid and λ′ can be computed from data (no oracle choice).",
      "startOffset" : 69,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "We consider the same chain problem as in Kolter & Ng (2009) and Hoffman et al. (2011). The state s has s̄ + 1 components s.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "For LASSO-TD and `2,∅-LSTD, these results are consistent with those published by Hoffman et al. (2011). Notice that there was more choice of regularization parameters for LASSO-TD, as the whole regularization path was computed.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "As for the choice of the regularization parameter, we plan to adapt the model selection scheme of `1-LSTD (Pires, 2011) to D-LSTD and test it.",
      "startOffset" : 106,
      "endOffset" : 119
    } ],
    "year" : 2012,
    "abstractText" : "LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, `1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are wellsuited for high–dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed–point problem, its integration with `1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.",
    "creator" : "LaTeX with hyperref package"
  }
}