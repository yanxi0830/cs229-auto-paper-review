{
  "name" : "1703.00522.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Understanding Synthetic Gradients and Decoupled Neural Interfaces",
    "authors" : [ "Wojciech Marian Czarnecki", "Grzegorz Świrszcz", "Max Jaderberg", "Simon Osindero", "Oriol Vinyals", "Koray Kavukcuoglu" ],
    "emails" : [ "<lejlot@google.com>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Neural networks can be represented as a graph of computational modules, and training these networks amounts to optimising the weights associated with the modules of this graph to minimise a loss. At present, training is usually performed with first-order gradient descent style algorithms, where the weights are adjusted along the direction of the negative gradient of the loss. In order to compute the gra-\n1DeepMind, London, United Kingdom. Correspondence to: WM Czarnecki <lejlot@google.com>.\nfi\nfi+1\nfi+2\n…\n…\n…\n…\nfi\nfi+1\nfi+2\n…\n…\n…\n…\nMi+1\ni ̂i\nMi+2\n̂i+1 i+1\n(a) (b) (c)\nDifferentiable\nLegend:\nfi\nfi+1\nfi+2\n…\n…\n…\n…\nMi+1\ni ̂i\n(b)\nx y\nL\nh SG\nLSG\nx y\nL\nh\nForward connection, differentiable\nForward connection, non-differentiable\nError gradient, non-differentiable\nSynthetic error gradient, differentiable\nLegend:\nSynthetic error gradient, nondifferentiable\nNon-differentiable\nForward connection\nError gradient\nSynthetic error gradient\nFigure 1. Visualisation of SG-based learning (b) vs. regular backpropagation (a).\ndient of the loss with respect to the weights of a module, one performs backpropagation (Williams & Hinton, 1986) – sequentially applying the chain rule to compute the exact gradient of the loss with respect to a module. However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015). In particular, backpropagation results in locking – the weights of a network module can only be updated after a full forwards propagation of the data through the network, followed by loss evaluation, then finally after waiting for the backpropagation of error gradients. This locking constrains us to updating neural network modules in a sequential, synchronous manner.\nOne way of overcoming this issue is to apply Synthetic Gradients (SGs) to build Decoupled Neural Interfaces (DNIs) (Jaderberg et al., 2016). In this approach, models of error gradients are used to approximate the true error gradient. These models of error gradients are local to the network modules they are predicting the error gradient for, so that an update to the module can be computed by using the predicted, synthetic gradients, thus bypassing the need for subsequent forward execution, loss evaluation, and backpropagation. The gradient models themselves are trained at the same time as the modules they are feeding synthetic gradients to are trained. The result is effectively a complex\nar X\niv :1\n70 3.\n00 52\n2v 1\n[ cs\n.L G\n] 1\nM ar\n2 01\n7\nUnderstanding Synthetic Gradients and DNIs\ndynamical system composed of multiple sub-networks cooperating to minimise the loss.\nThere is a very appealing potential of using DNIs e.g. the potential to distribute and parallelise training of networks across multiple GPUs and machines, the ability to asynchronously train multi-network systems, and the ability to extend the temporal modelling capabilities of recurrent networks. However, it is not clear that introducing DNIs and SGs into a learning system will not negatively impact the learning dynamics and solutions found. While the empirical evidence in Jaderberg et al. (2016) suggests that SGs do not have a negative impact and that this potential is attainable, this paper will dig deeper and analyse the result of using SGs to accurately answer the question of the impact of synthetic gradients on learning systems.\nIn particular, we address the following questions, using feed-forward networks as our probe network architecture: Does introducing SGs change the critical points of the neural network learning system? In Section 3 we show that the critical points of the original optimisation problem are maintained when using SGs. Can we characterise the convergence and learning dynamics for systems that use synthetic gradients in place of true gradients? Section 4 gives first convergence proofs when using synthetic gradients and empirical expositions of the impact of SGs on learning. What is the difference in the representations and functional decomposition of networks learnt with synthetic gradients compared to backpropagation? Through experiments on deep neural networks in Section 5, we find that while functionally the networks perform identically trained with backpropagation or synthetic gradients, the layer-wise functional decomposition is markedly different due to SGs.\nIn addition, in Section 6 we look at formalising the connection between SGs and other forms of approximate error propagation such as Feedback Alignment (Lillicrap et al., 2016), Direct Feedback Alignment (Nøkland, 2016; Baldi et al., 2016), and Kickback (Balduzzi et al., 2014), and show that all these error approximation schemes can be captured in a unified framework, but crucially only using synthetic gradients can one achieve unlocked training."
    }, {
      "heading" : "2. DNI using Synthetic Gradients",
      "text" : "The key idea of synthetic gradients and DNI is to approximate the true gradient of the loss with a learnt model which predicts gradients without performing full backpropagation.\nConsider a feed-forward network consisting of N layers fn, n ∈ {1, . . . , N}, each taking an input hn−1i and producing an output hni = fn(h n−1 i ), where h 0 i = xi is the input data point xi. A loss is defined on the output of the net-\nwork Li = L(hNi , yi) where yi is the given label or supervision for xi (which comes from some unknown P (y|x)). Each layer fn has parameters θn that can be trained jointly to minimise Li with the gradient-based update rule\nθn ← θn − α ∂L(hNi , yi)\n∂hni\n∂hni ∂θn\nwhere α is the learning rate and ∂Li/∂hni is computed with backpropagation.\nThe reliance on ∂Li/∂hNi means that an update to layer i can only occur after every subsequent layer fj , j ∈ {i + 1, . . . , N} has been computed, the loss Li has been computed, and the error gradient ∂L/∂hNi backpropgated to get ∂Li/∂h N i . An update rule such as this is update locked as it depends on computing Li, and also backwards locked as it depends on backpropagation to form ∂Li/∂hni .\nJaderberg et al. (2016) introduces a learnt prediction of the error gradient, the synthetic gradient SG(hni , yi) = ̂∂Li/∂hni ' ∂Li/∂hni resulting in the update\nθk ← θk − α SG(hni , yi) ∂hni ∂θk ∀k ≤ n\nThis approximation to the true loss gradient allows us to have both update and backwards unlocking – the update to layer n can be applied without any other network computation as soon as hni has been computed, since the SG module is not a function of the rest of the network (unlike ∂Li/∂hi). Furthermore, note that since the true ∂Li/∂hni can be described completely as a function of just hni and yi, from a mathematical perspective this approximation is sufficiently parameterised.\nThe synthetic gradient module SG(hni , yi) has parameters θSG which must themselves be trained to accurately predict the true gradient by minimising the L2 loss LSGi = ‖SG(hni , yi)− ∂Li/∂hni ‖2.\nThe resulting learning system consists of three decoupled parts: first, the part of the network above the SG module which minimises L wrt. to its parameters {θn+1, ..., θN}, then the SG module that minimises the LSG wrt. to θSG. Finally the part of the network below the SG module which uses SG(h, y) as the learning signal to train {θ1, ...θn}, thus it is minimising the loss modeled internally by SG."
    }, {
      "heading" : "Assumptions and notation",
      "text" : "Throughout the remainder of this paper, we consider the use of a single synthetic gradient module at a single layer k and for a generic data sample j and so refer to h = hj = hkj ; unless specified we drop the superscript k and subscript j. This model is shown in Figure 1 (b). We also focus on SG modules which take the point’s true label/value as conditioning SG(h, y) as opposed to SG(h). Note that without label conditioning, a SG module is trying to approximate\nUnderstanding Synthetic Gradients and DNIs\nnot ∂L/∂h but rather EP (y|x)∂L/∂h since L is a function of both input and label. In theory, the lack of label is a sufficient parametrisation but learning becomes harder, since the SG module has to additionally learn P (y|x).\nWe also focus most of our attention on models that employ linear SG modules, SG(h, y) = hA+ yB +C. Such modules have been shown to work well in practice, and furthermore are more tractable to analyse.\nAs a shorthand, we denote θ<h to denote the subset of the parameters contained in modules up to h (and symmetrically θ>h), i.e. if h is the kth layer then θ<h = {θ1 . . . , θk}."
    }, {
      "heading" : "Synthetic gradients in operation",
      "text" : "Consider an N -layer feed-forward network with a single SG module at layer k. This network can be decomposed into two sub-networks: the first takes an input x and produces an output h = Fh(x) = fk(fk−1(. . . (f1(x)))), while the second network takes h as an input, produces an output p = Fp(h) = fN (. . . (fk+1(h))) and incurs a loss L = L(p, y) based on a label y.\nWith regular backpropagation, the learning signal for the first network Fh is ∂L/∂h, which is a signal that specifies how the input to Fp should be changed in order to reduce the loss. When we attach a linear SG between these two networks, the first sub-network Fh no longer receives the exact learning signal from Fp, but an approximation SG(h, y), which implies that Fh will be minimising an approximation of the loss, because it is using approximate error gradients. Since the SG module is a linear model of ∂L/∂h, the approximation of the true loss that Fh is being optimised for will be a quadratic function of h and y. Note that this is not what a second order method does when a function is locally approximated with a quadratic and used for optimisation – here we are approximating the current loss, which is a function of parameters θ with a quadratic which is a function of h. Three appealing properties of an approximation based on h is that h already encapsulates a lot of non-linearities due to the processing of Fh, h is usually vastly lower dimensional than θ<h which makes learning more tractable, and the error only depends on quantities (h) which are local to this part of the network rather than θ which requires knowledge of the entire network.\nWith the SG module in place, the learning system decomposes into two tasks: the second sub-network Fp tasked with minimising L given inputs h, while the first subnetwork Fh is tasked with pre-processing x in such a way that the best fitted quadratic approximator of L (wrt. h) is minimised. In addition, the SG module is tasked with best approximating L.\nThe approximations and changing of learning objectives (described above) that are imposed by using synthetic gradients may appear to be extremely limiting. However, in\nboth the theoretical and empirical sections of this paper we show that SG models can, and do, learn solutions to highly non-linear problems (such as memorising noise).\nThe crucial mechanism that allows such rich behaviour is to remember that the implicit quadratic approximation to the loss implied by the SG module is local (per data point) and non-stationary – it is continually trained itself. It is not a single quadratic fit to the true loss over the entire optimisation landscape, but a local quadratic approximation specific to each instantaneous moment in optimisation. In addition, because the quadratic approximation is a function only of h and not θ, the loss approximation is still highly non-linear w.r.t. θ.\nIf, instead of a linear SG module, one uses a more complex function approximator of gradients such as an MLP, the loss is effectively approximated by the integral of the MLP. More formally, the loss implied by the SG module in hypotheses spaceH is of class {l : ∃g ∈ H : ∂l/∂h = g}1. In particular, this shows an attractive mathematical benefit over predicting loss directly: by modelling gradients rather than losses, we get to implicitly model higher order loss functions."
    }, {
      "heading" : "3. Critical points",
      "text" : "We now consider the effect SG has on critical points of the optimisation problem. Concretely, it seems natural to ask whether a model augmented with SG is capable of learning the same functions as the original model. We ask this question under the assumption of a locally converging training method, such that we always end up in a critical point. In the case of a SG-based model this implies a set of parameters θ such that ∂L/∂θ>h = 0, SG(h, y)∂h/∂θ<h = 0 and ∂LSG/∂θSG = 0. In other words we are trying to establish whether SG introduces regularisation to the model class, which changes the critical points, or whether it merely introduces a modification to learning dynamics, but retains the same set of critical points.\nIn general, the answer is positive: SG does induce a regularisation effect. However, in the presence of additional assumptions, we can show families of models and losses for which the original critical points are not affected.\nProposition 1. Every critical point of the original optimisation problem where SG can produce ∂L/∂hi has a corresponding critical point of the SG-based model. Proof. Directly from the assumption we have that there exists a set of SG parameters such that the loss is minimal, thus ∂LSG/∂θSG = 0 and also SG(h, y) = ∂L/∂h and SG(h, y)∂h/∂θ<h = 0.\nThe assumptions of this proposition are true for example when L = 0 (one attains global minimum), when\n1We mean equality for all points where ∂l/∂h is defined.\nUnderstanding Synthetic Gradients and DNIs\n∂L/∂hi = 0 or a network is a deep linear model trained with MSE and SG is linear.\nIn particular, this shows that for a large enough SG module all the critical points of the original problem have a corresponding critical point in the SG-based model. Limiting the space of SG hypotheses leads to inevitable reduction of number of original critical points, thus acting as a regulariser. At first this might look like a somewhat negative result, since in practice we rarely use a SG module capable of exactly producing true gradients. However, there are three important observations to make: (1) Our previous observation reflects having an exact representation of the gradient at the critical point, not in the whole parameter space. (2) One does preserve all the critical points where the loss is zero, and given current neural network training paradigms these critical points are important. For such cases even if SG is linear the critical points are preserved. (3) In practice one rarely optimises to absolute convergence regardless of the approach taken; rather we obtain numerical convergence meaning that ‖∂L/∂θ‖ is small enough. Thus, all one needs from SG-based model is to have small enough ‖(∂L/∂h+e)∂h/∂θ<h‖ ≤ ‖∂L/∂θ<h‖+‖e‖‖∂h/∂θ<h‖, implying that the approximation error at a critical point just has to be small wrt to ‖∂h/∂θ<h‖ and need not be 0.\nTo recap: so far we have shown that SG can preserve critical points of the optimisation problem. However, SG can also introduce new critical points, leading to premature convergence and spurious additional solutions. As with our previous observation, this does not effect SG modules which are able to represent gradients exactly. But if the SG hypothesis space does not include a good approximator2 of the true gradient, then we can get new critical points which end up being an equilibrium state between SG modules and the original network. We provide an example of such an equilibrium in the Supplementary Materials Section A."
    }, {
      "heading" : "4. Learning dynamics",
      "text" : "Having demonstrated that important critical points are preserved and also that new ones might get created, we need a better characterisation of the basins of attraction, and to understand when, in both theory and practice, one can expect convergence to a good solution."
    }, {
      "heading" : "Artificial Data",
      "text" : "We conduct an empirical analysis of the learning dynamics on easily analysable artificial data. We create 2 and 100 dimensional versions of four basic datasets (details in the Supplementary Materials Section C) and train four simple models (a linear model and a deep linear one with 10 hidden layers, trained to minimise MSE and log loss) with regular backprop and with a SG-based alternative to see\n2In this case, our gradient approximation needs to be reasonable at every point through optimisation, not just the critical ones.\nwhether it (numerically) converges to the same solution.\nFor MSE and both shallow and deep linear architectures the SG-based model converges to the global optimum (exact numerical results provided in Supplementary Material Table 2). However, this is not the case for logistic regression. This effect is a direct consequence of a linear SG module being unable to model ∂L/∂p3 (where p = xW + b is the output of logistic regression), which often approaches the step function (when data is linearly separable), and cannot be well approximated with a linear function SG(h, y) = hA+ yB+C. Once one moves towards problems without this characteristic (e.g. random labeling) the problem vanishes, since now ∂L/∂p can be approximated much better. While this may not seem particularly significant, it illustrates an important characteristic of SG in the context of the log loss – it will struggle to overfit to training data, since it requires modeling step function type shapes, which is not possible with a linear model. In particular this means that for best performance one should adapt the SG module architecture to the loss function used —for MSE linear SG is a reasonable choice, however for log loss one should use architectures including a sigmoid σ applied pointwise to a linear SG, such as SG(h, y) = dσ(hA) + yB + C.\nAs described in Section 2, using a linear SG module makes the implicit assumption that loss is a quadratic function of the activations. Furthermore, in such setting we can actually reconstruct the loss being used up to some additive constant since ∂L/∂h = hA + yB + C implies that L(h) = 12hAh\nT + (yB + C)hT + const. If we now construct a 2-dimensional dataset, where data points are arranged in a 2D grid, we can visualise the loss implicitly predicted by the SG module and compare it with the true loss for each point.\nFigure 2 shows the results of such an experiment when learning a highly non-linear model (5-hidden layer relu network). As one can see, the quality of the loss approximation has two main components to its dynamics. First, it is better in layers closer to the true loss (i.e. the topmost layers), which matches observations from Jaderberg et al. (2016) and the intuition that the lower layers solve a more complex problem (since they bootstrap their targets). Second, the loss is better approximated at the very beginning of the training and the quality of the approximation degrades slowly towards the end. This is a consequence of the fact that close to the end of training, the highly nonlinear model has quite complex derivatives which cannot be well represented in a space of linear functions. It is worth noting, that in these experiments, the quality of the loss approximation dropped significantly when the true loss was around 0.001, thus it created good approximations for the majority of the learning process. There is also an empirical\n3∂L/∂p = exp(xW + b)/(1 + exp(xW + b))− y\nconfirmation of the previous claim, that with log loss and data that can be separated, linear SGs will have problems modeling this relation close to the end of training (Figure 2 (b) left), while there is no such problem for MSE loss (Figure 2 (a) left)."
    }, {
      "heading" : "Convergence",
      "text" : "It is trivial to note that if a SG module used is globally convergent to the true gradient, and we only use its output after it converges, then the whole model behaves like the one trained with regular backprop. However, in practice we never do this, and instead train the two models in parallel without waiting for convergence of the SG module. We now discuss some of the consequences of this, and begin by showing that as long as a synthetic gradient produced is close enough to the true one we still get convergence to the true critical points. Namely, only if the error introduced by SG, backpropagated to all the parameters, is consistently smaller than the norm of true gradient multiplied by some positive constant smaller than one, the whole system converges. Thus, we essentially need the SG error to vanish around critical points.\nProposition 2. Let us assume that a SG module is trained in each iteration in such a way that it -tracks true gradient, i.e. that ‖SG(h, y)− ∂L/∂h‖ ≤ . If ‖∂h/∂θ<h‖ is upper bounded by some K and there exists a constant δ ∈ (0, 1) such that in every iteration K ≤ ‖∂L/∂θ<h‖ 1−δ1+δ , then the whole training process converges to the solution of the\noriginal problem. Proof. Proof follows from showing that, under the assumptions, effectively we are training with noisy gradients, where the noise is small enough for convergence guarantees given by Zoutendijk (1970); Gratton et al. (2011) to apply. Details are provided in the Supplementary Materials Section B.\nAs a consequence of Proposition 2 we can show that with specifically chosen learning rates (not merely ones that are small enough) we obtain convergence for deep linear models.\nCorollary 1. For a deep linear model minimising MSE, trained with a linear SG module attached between two of its hidden layers, there exist learning rates in each iteration such that it converges to the critical point of the original problem. Proof. Proof follows directly from Propositions 1 and 2. Full proof is given in Supplementary Materials Section B.\nFor a shallow model we can guarantee convergence to the global solution provided we have a small enough learning rate, which is the main theoretical result of this paper.\nTheorem 1. Let us consider linear regression trained with a linear SG module attached between its output and the loss. If one chooses the learning rate of the SG module using line search, then in every iteration there exists small\nenough, positive learning rate of the main network such that it converges to the global solution.\nProof. The general idea (full proof in the Supplementary Materials Section B) is to show that with assumed learning rates the sum of norms of network error and SG error decreases in every iteration.\nDespite covering a quite limited class of models, these are the very first convergence results for SG-based learning. Unfortunately, they do not seem to easily generalise to the non-linear cases, which we leave for future research."
    }, {
      "heading" : "5. Trained models",
      "text" : "We now shift our attention to more realistic data. We train deep relu networks of varied depth (up to 50 hidden layers) with batch-normalisation and with two different activation functions on MNIST and compare models trained with full backpropagation to variants that employ a SG module in the middle of the hidden stack.\nFigure 4 shows, that SG-based architectures converge well even if there are many hidden layers both below and above the module. Interestingly, SG-based models actually seem to converge faster (compare for example 20- or 50 layer deep relu network). We believe this may be due to some amount of loss function smoothing since, as described in Section 2, a linear SG module effectively models the loss function to be quadratic – thus the lower network has a simpler optimisation task and makes faster learning progress.\nObtaining similar errors on MNIST does not necessarily mean that trained models are the same or even similar. Since the use of synthetic gradients can alter learning dynamics and introduce new critical points, they might converge to different types of models. Assessing the representational similarity between different models is difficult, however. One approach is to compute and visualise Representational Dissimilarity Matrices (Kriegeskorte et al., 2008) for our data. We sample a subset of 400 points from MNIST, order them by label, and then record activations on each hidden layer when the network is presented with these points. We plot the pairwise correlation matrix for each layer, as shown in Figure 3. This representation is permutation invariant, and thus the emergence of a block-diagonal correlation matrix means that at a given layer, points from the same class already have very correlated representations.\nUnder such visualisations one can notice qualitative differences between the representations developed under standard backpropagation training versus those delivered by a SG-based model. In particular, in the MNIST model with 20 hidden layers trained with standard backpropagation we see that the representation covariance after 9 layers is nearly the same as the final layer’s representation. However, by contrast, if we consider the same architecture but with a SG module in the middle we see that the layers before the SG module develop a qualitatively different style of representation. Note: this does not mean that layers before SG do not learn anything useful. To confirm this, we also introduced linear classifier probes (Alain & Bengio, 2016) and observed that, as with the pure backpropagation trained model, such probes can achieve 100% training accuracy after the first two hidden-layers of the SGbased model, as shown in Supplementary Material’s Figure 8. With 20 SG modules (one between every pair of layers), the representation is scattered even more broadly: we see rather different learning dynamics, with each layer contributing a small amount to the final solution, and there is no longer a point in the progression of layers where the representation is more or less static in terms of correlation structure (see Figure 3).\nUnderstanding Synthetic Gradients and DNIs\nAnother way to investigate whether the trained models are qualitatively similar is to examine the norms of the weight matrices connecting consecutive hidden layers, and to assess whether the general shape of such norms are similar. While this does not definitively say anything about how much of the original classification is being solved in each hidden layer, it is a reasonable surrogate for how much computation is being performed in each layer4. According\nto our experiments (see Figure 5 for visualisation of one of the runs), models trained with backpropagation on MNIST tend to have norms slowly increasing towards the output of the network (with some fluctuations and differences coming from activation functions, random initialisations, etc.). If we now put a SG in between every two hidden layers, we get norms that start high, and then decrease towards the output of the network (with much more variance now). Finally, if we have a single SG module we can observe that the behaviour after the SG module resembles, at least to some degree, the distributions of norms obtained with backpropagation, while before the SG it is more chaotic, with some similarities to the distribution of weights with SGs in-between every two layers.\nThese observations match the results of the previous experiment and the qualitative differences observed. When synthetic gradients are used to deliver full unlocking we obtain a very basic model at the lowest layers and then see iterative corrections in deeper layers. For a one-point unlocked model with a single SG module, we have two slightly separated models where one behaves similarly to backprop, and the other supports it. Finally, a fully locked model (i.e. traditional backprop) solves the task relatively early on, and later just increases its confidence.\n4We train with a small L2 penalty added to weights to make norm correspond roughly to amount of computation.\nWe note that the results of this section support our previous notion that we are effectively dealing with a multi-agent system, which looks for coordination/equilibrium between components, rather than a single model which simply has some small noise injected into the gradients (and this is especially true for more complex models)."
    }, {
      "heading" : "6. SG and conspiring networks",
      "text" : "We now shift our attention and consider a unified view of several different learning principles that work by replacing true gradients with surrogates. We focus on three such approaches: Feedback Alignment (FA) (Lillicrap et al., 2016), Direct Feedback Alignment (DFA) (Nøkland, 2016), and Kickback (KB) (Balduzzi et al., 2014). FA effectively uses a fixed random matrix during backpropagation, rather than the transpose of the weight matrix used in the forward pass. DFA does the same, except each layer directly uses the learning signal from the output layer rather than the subsequent local one. KB also pushes the output learning signal directly but through a predefined matrix instead of a random one. By making appropriate choices for targets, losses, and model structure we can cast all of these methods in the SG framework, and view them as comprising two networks with a SG module in between them, wherein the first module builds a representation which makes the task of the SG predictions easier.\nWe begin by noting that in the SG models described thus far we do not backpropagate the SG error back into the part of the main network preceding the SG module (i.e. we assume ∂LSG/∂h = 0). However, if we relax this restriction, we can use this signal (perhaps with some scaling factor α) and obtain what we will refer to as a SG + prop model. Intuitively, this additional learning signal adds capacity to our SG model and forces both the main network and the SG module to “conspire” towards a common goal of making better gradient predictions. From a practical perspective, according to our experiments, this additional signal heavily stabilises learning system5. However, this comes at the cost of no longer being unlocked.\nOur main observation in this section is that FA, DFA, and KB can be expressed in the language of “conspiring” networks (see Table 1), of two-network systems that use a SG module. The only difference between these approaches is how one parametrises SG and what target we attempt to fit it to. This comes directly from the construction of these\n5 In fact, ignoring the gradients predicted by SG and only using the derivative of the SG loss, i.e. ∂LSG/∂h, still provides enough learning signal to converge to a solution for the original task in the simple classification problems we considered. We posit a simple rationale for this: if one can predict gradients well using a simple transformation of network activations (e.g. a linear mapping), this suggests that the loss itself can be predicted well too, and thus (implicitly) so can the correct outputs.\nUnderstanding Synthetic Gradients and DNIs\nNetwork\nfi\nfi+1 fi+2\n…\n…\n… … fi fi+1 fi+2\n…\n…\n… …\nMi+1\ni ̂i\nMi+2 ̂i+1 i+1\n(a) (b) (c)\nDifferentiable Legend:\nx y\nL h SG LSG\nx y\nL h Forward connection, differentiable Forward connection, non-differentiable Error gradient, non-differentiable Synthetic error gradient, differentiable Legend: Synthetic error gradient, nondifferentiable\nNon-differentiable Forward connection\nError gradient\nSynthetic error gradient\nL\nh SG\nLSG\nSG\np\nL\nh h\nLSG\nBprop\np\nL\nh SG\nLSG\nSG + prop\np\nL\nh hA\nDFA\np\nL\nh hA\nLSG\nFA\np\ng=hW\nL\nh h1\nLSG\nKickback\np\nLSG\nMethod\n∂̂L/∂h SG(h, y) SG(h, y) + α∂LSG∂h ∂L/∂h (∂L/∂p)A T (∂L/∂g)AT (∂L/∂p)1T SG(h, y) SG(h, y) SG(h, y) h hA hA h1 SG trains yes yes no no no no SG target ∂L/∂h ∂L/∂h −∂L/∂h −∂L/∂p −∂L/∂g −∂L/∂p LSG(t, s) ‖t− s‖2 ‖t− s‖2 −〈t, s〉 −〈t, s〉 −〈t, s〉 −〈t, s〉 Update locked no yes* yes yes yes yes Backw. locked no yes* yes no yes no Direct error no no no yes no yes\nTable 1. Unified view of “conspiring” gradients methods, including backpropagation, synthetic gradients are other error propagating methods. For each of them, one still trains with regular backpropagation (chain rule) however ∂L/∂h is substituted with a particular ∂̂L/∂h. Black lines are forward signals, blue ones are synthetic gradients, and green ones are true gradients. Dotted lines represent non-differentiable operations. The grey modules are not trainable. A is a fixed, random matrix and 1 is a matrix of ones of an appropriate dimension. * In SG+Prop the network is locked if there is a single SG module, however if we have multiple ones, then propagating error signal only locks a module with the next one, not with the entire network. Direct error means that a model tries to solve classification problem directly at layer h.\nsystems, and the fact that if we treat our targets as constants (as we do in SG methods), then the backpropagated error from each SG module (∂LSG/∂h) matches the prescribed update rule of each of these methods (∂̂L/∂h). One direct result from this perspective is the fact that Kickback is essentially DFA with A = 1. For completeness, we note that regular backpropagation can also be expressed in this unified view – to do so, we construct a SG module such that the gradients it produces attempt to align the layer activations with the negation of the true learning signal (−∂L/∂h). In addition to unifying several different approaches, our mapping also illustrates the potential utility and diversity in the generic idea of predicting gradients."
    }, {
      "heading" : "7. Conclusions",
      "text" : "This paper has presented new theory and analysis for the behaviour of synthetic gradients in feed forward models. Firstly, we showed that introducing SG does not necessarily change the critical points of the original problem, however at the same time it can introduce new critical points into the learning process. This is an important result showing that SG does not act like a typical regulariser despite simplifying the error signals. Secondly, we showed that (despite modifying learning dynamics) SG-based models converge\nto analogous solutions to the true model under some additional assumptions. We proved exact convergence for a simple class of models, and for more complex situations we demonstrated that the implicit loss model captures the characteristics of the true loss surface. It remains an open question how to characterise the learning dynamics in more general cases. Thirdly, we showed that despite these convergence properties the trained networks can be qualitatively different from the ones trained with backpropagation. While not necessarily a drawback, this is an important consequence one should be aware of when using synthetic gradients in practice. Finally, we provided a unified framework that can be used to describe alternative learning methods such as Synthetic Gradients, FA, DFA, and Kickback, as well as standard Backprop. The approach taken shows that the language of predicting gradients is suprisingly universal and provides additional intuitions and insights into the models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank James Martens and Ross Goroshin for their valuable remarks and discussions.\nUnderstanding Synthetic Gradients and DNIs"
    }, {
      "heading" : "Supplementary Materials",
      "text" : ""
    }, {
      "heading" : "A. Additional examples",
      "text" : ""
    }, {
      "heading" : "Critical points",
      "text" : "We can show an example of SG introducing new critical points. Consider a small one-dimensional training dataset {−2,−1, 1, 2} ⊂ R, and let us consider a simple system where the model f : R → R is parametrised with two scalars, a and b and produces ax + b. We train it to minimise L(a, b) = ∑4 i=1 |axi + b|. This has a unique minimum which is obtained for a = b = 0, and standard gradient based methods will converge to this solution. Let us now attach a SG module betweenf and L. This module produces a (trainable) scalar value c ∈ R (thus it produces a single number, independent from the input). Regardless of the value of a, we have a critical point of the SG module when b = 0 and c = 0. However, solutions with a = 1 and c = 0 are clearly not critical points of the original system. Figure 6 shows the loss surface and the fitting of SG module when it introduces new critical point."
    }, {
      "heading" : "B. Proofs",
      "text" : "Theorem 1 Let us consider linear regression trained with a linear SG module attached between its output and the loss. If one chooses the learning rate of the SG module using line search, then in every iteration there exists small\nUnderstanding Synthetic Gradients and DNIs\nenough, positive learning rate of the main network such that it converges to the global solution.\nProof. Let X = {xs}Ss=1 ∈ Rd×S be the data, let {ys}Ss=1 ∈ R1×S be the labels. Throughout the proof k will be the iteration of training. We denote by 1 ∈ R1×S a row vector in which every element is 1. We also follow the standard convention of including the bias in the weight matrix by augmenting the data X with one extra coordinate always equal to 1. Thus, we denote X̄ = (XT |1T )T , X̄ ∈ R(d+1)×S and x̄s-the columns of X̄. Using that convention, the weight matrix is Wk ∈ R1×(d+1). We have\npsk := Wkx̄ s,\nL = 1\n2 S∑ s=1 (ys − psk) 2 = 1 2 n∑ i=1 (ys −Wkx̄s)2 .\nOur aim is to find arg min\nW,b L.\nWe use\n∂L ∂W = ∂L ∂p ∂p ∂W = S∑ s=1 ∂L ∂ps ∂ps ∂W =\nS∑ s=1 ∂L ∂ps x̄s = S∑ s=1 (ys −Wkx̄s) (x̄s)T\n∂L ∂p = ( p1 − y1, . . . , pS − yS ) We will use the following parametrization of the synthetic gradient ∇̃Lk = (αk+1)pk−(βk+1)y+γk1. The reason for using this form instead of simply akpk + bky + ck1 is that we are going to show that under DNI this synthetic gradient will converge to the “real gradient” ∂L∂p , which means showing that lim\nk→∞ (αk, βk, γk) = (0, 0, 0). Thanks to this\nchoice of parameters αk, βk, γk we have the simple expression for the error\nEk = ∥∥∥∥∇̃Lk − ∂L∂p ∥∥∥∥2 2 =\n‖(αk + 1)pk − (βk + 1)y + γk1−( p1k − y1, . . . , pSk − yS )∥∥2 2\n=∥∥(αkp1k − βky1 + γk, . . . , αkpSk − βkyS + γk)∥∥22 Parameters αk, βk, γk will be updated using the gradient descent minimizing the error E. We have\n∂E ∂α = S∑ s=1 (αkp s k − βkys + γk)psk\n∂E ∂β = − S∑ s=1 (αkp s k − βkys + γk)ys\n∂E ∂γ = S∑ s=1 (αkp s k − βkys + γk).\nAs prescribed in Jaderberg et al. (2016), we start our iterative procedure from the synthetic gradient being equal to zero and we update the parameters by adding the (negative) gradient multiplied by a learning rate ν. This means that we apply the iterative procedure:\nα0 = −1, β0 = −1, γ0 = 0\nWk+1 =Wk − µ S∑ s=1 ((αk + 1)p s k−\n(βk + 1)y s + γk) (x̄ s)T\nαk+1 =αk − ν S∑ s=1 (αkp s k − βkys + γk)psk\nβk+1 =βk + ν S∑ s=1 (αkp s k − βkys + γk)ys\nγk+1 =γk − ν S∑ s=1 (αkp s k − βkys + γk).\nUsing matrix notation\nWk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk1)X̄T αk+1 = αk − ν ( αk‖pk‖22 − βk〈y,pk〉+ γk〈1,pk〉 ) βk+1 = βk + ν ( αk〈pk,y〉 − βk‖y‖22 + γk〈1,y〉\n) γk+1 = γk − ν (αk〈1,pk〉 − βk〈1,y〉+ Sγk)\nNote, that the subspace given by α = β = γ = 0 is invariant under this mapping. As noted before, this corresponds to the synthetic gradient being equal to the real gradient. Proving the convergence of SG means showing, that a trajectory starting from α0 = −1, β0 = −1, γ0 = 0 converges to W = W0, α = β = γ = 0, where W0 are the “true” weigts of the linear regression. We are actually going to prove more, we will show that W = W0, α = β = γ = 0 is in fact a global attractor, i.e. that any trajectory converges to that point. Denoting ω = (α, β, γ)t we get\nWk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk1)X̄T ωk+1 = ωk − ν [ pTk | − yT |1T ]T [ pTk | − yT |1T ] ωk\nWk+1 = Wk − µ(pk − y)X̄T − µωTk [ pTk | − yT |1T ]T X̄T\nωk+1 = ωk − ν [ pTk | − yT |1T ]T [ pTk | − yT |1T ] ωk.\nDenoting by Ak = [ pTk | − yT |1T ] we get\nWk+1 = Wk − µ(pk − y)X̄T − µωTATk X̄T\nωk+1 = ωk − νATkAkωk.\nUnderstanding Synthetic Gradients and DNIs\nMultiplying both sides of the first equation by X̄ we obtain\nWk+1X̄ = WkX̄− µ(pk − y)X̄T X̄− µωTATk X̄T X̄ ωk+1 = ωk − νATkAkωk.\nDenote B = X̄T X̄. We get\npk+1 = pk − µpkB + µyB− µωTkATkB ωk+1 = ωk − νATkAkωk.\nDenoting ek = (y − pk)T we get\nek+1 = ek − µBek + µBAkωk ωk+1 = ωk − νATkAkωk.\nWe will use the symbol ξ = Akωk. Then\nek+1 = ek − µBek + µBξk ξk+1 = ξk − νAkATk ξk.\n(1)\nEvery vector v can be uniquely expressed as a sum v = v⊥ + v‖ with X̄v⊥ = 0 and v‖ = X̄T θ for some θ (v‖ is a projection of v onto the linear subspace spanned by the columns of X̄). Applying this decomposition to ek = e⊥k + e ‖ k we get\ne⊥k+1 = e ⊥ k − µ(Bek)⊥ + µ(Bξk)⊥ e ‖ k+1 = e ‖ k − µ(Bek) ‖ + µ(Bξk) ‖ ξk+1 = ξk − νAkATk ξk.\nNote now, that as B = X̄T X̄, for any vector v there is (Bv)⊥ = 0, and (Bv)‖ = Bv (because the operator v 7→ v‖ is a projection). Moreover, Bv = Bv‖. Therefore\ne⊥k+1 = e ⊥ k e ‖ k+1 = e ‖ k − µ(Be ‖ k) + µ(Bξk) ‖ ξk+1 = ξk − νAkATk ξk.\nThe value e⊥k does not change. Thus, we will be omitting the first equation. Note, that e⊥k is “the residue”, the smallest error that can be obtained by a linear regression. For the sake of visual appeal we will denote f = e‖k\nfk+1 = fk − µBfk + µBξk ξk+1 = ξk − νAkATk ξk.\nTaking norms and using ‖u+ v‖ ≤ ‖u‖+ ‖v‖ we obtain\n‖fk+1‖2 ≤ ‖fk − µBfk‖2 + µ‖Bξk‖2 ‖ξk+1‖22 = ‖ξk‖22 − 2ν‖ATk ξk‖22 + ν2‖AkATk ξk‖22.\nObserve that ‖fk − µBfk‖22 = ‖fk‖22 − 2µfkBfk + µ2‖Bfk‖22. As B is a constant matrix, there exists a constant b > 0 such that vTBv ≥ b‖v‖22 for any v satisfying\nv‖ = v. Therefore ‖fk − µBfk‖22 ≤ ‖fk‖22 − 2µb‖fk‖22 + µ2‖B‖2‖fk‖22. Using that and ‖Bξk‖2 ≤ ‖B‖‖ξk‖2 we get\n‖fk+1‖2 ≤ √\n1− 2µb+ µ2‖B‖2‖fk‖2 + µ‖B‖‖ξk‖2 ‖ξk+1‖22 = ‖ξk‖22 − 2ν‖ATk ξk‖22 + ν2‖AkATk ξk‖22.\nLet us assume that AkATk ξk 6= 0. In that case the righthand side of the second equation is a quadratic function is ν, whose minimum value is attained for ν = ‖A T k ξk‖ 2 2\n‖AkATk ξk‖ 2 2 . For so-chosen ν we have\n‖fk+1‖2 ≤ √ 1− 2µb+ µ2‖B‖2‖fk‖2 + µ‖B‖‖ξk‖2\n‖ξk+1‖22 = ( 1− ‖A T k ξk‖22\n‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22\n) ‖ξk‖22.\nConsider a space {f}⊕{ξ} (concatenation of vectors) with a norm ‖{f} ⊕ {ξ}‖⊕ = ‖f‖2 + ‖ξ‖2.\n‖{fk+1} ⊕ {ξk+1}‖⊕ ≤√ 1− 2µb+ µ2‖B‖2‖fk‖2 + µ‖B‖‖ξk‖2 +√\n1− ‖ATk ξk‖22 ‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22 ‖ξk‖2 ≤\nUsing √\n1− h ≤ 1− 12h we get\n‖{fk+1} ⊕ {ξk+1}‖⊕ ≤ √\n1− 2µb+ µ2‖B‖2‖fk‖2+( 1− ‖A T k ξk‖22\n2‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22 + µ\n) ‖ξk‖2\nNote, that √\n1− 2µb+ µ2‖B‖2 < 1 for 0 < µ ≤ b‖B‖2 . Thus, for\nµ < min\n{ b\n‖B‖2 , 1− ‖A T k ξk‖22 2‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22\n} ,\nfor every pair {fk+1} ⊕ {ξk+1} 6= {0} ⊕ {0} (and if they are zeros then we already converged) there is\n‖{fk+1} ⊕ {ξk+1}‖⊕ < ‖{fk} ⊕ {ξk}‖⊕.\nTherefore, by Theorem 2, the error pair {fk+1} ⊕ {ξk+1} has to converge to 0, which ends the proof in the case AkA T k ξk 6= 0. It remains to investigate what happens if AkA T k ξk = 0.\nWe start by observing that either ξk = 0 or ATk ξk 6= 0 and AkA T k ξk 6= 0. This follows directly from the definition ξk = Akωk. Indeed, if ξk 6= 0 there is 0 < ‖Akωk‖22 = ωTkA T k ξk and analogously 0 < ‖ATk ξk‖ = ξTkAkATk ξk.\nIn case ξk = 0 there is ‖{fk+1} ⊕ {ξk+1}‖⊕ = ‖ fk+1‖2 < √ 1− 2µb+ µ2‖B‖2‖fk‖2 =√\n1− 2µb+ µ2‖B‖2‖{fk} ⊕ {ξk}‖⊕ and the theorem follows.\nUnderstanding Synthetic Gradients and DNIs\nTheorem 2. Let B be a finite-dimensional Banach space. Let f : B → B be a continuous map such that for every x ∈ B there is ‖f(x)‖ < ‖x‖. Then for every x there is lim n→∞ fn(x) = 0.\nProof. Let ω(x) = {y : ∃i1<i2<... lim n→∞ f in(x) = y}. Because ‖f(x)‖ < ‖x‖, the sequence x, f(x), f2(x), . . . is contained in a ball of a radius ‖x‖, which due to a finite dimensionality of B is a compact set. Thus, ω(x) is nonempty. Moreover, from the definition, ω(x) is a closed set, and therefore it is a compact set. Let y0 = infy∈ω(x) ‖y‖ – which we know exists, due to the compactness of ω(x) and the continuity of ‖ · ‖ (Weierstraß theorem). But for every y ∈ ω(x) there is f(y) ∈ ω(x), thus there must be y0 = 0. By definition, for every ε, there exists n0 such that ‖fn0(x)‖ < ε. Therefore, for n > n0 ‖fn(x)‖ < ε. Therefore, fn(x) must converge to 0.\nProposition 2. Let us assume that a SG module is trained in each iteration in such a way that it -tracks true gradient, i.e. that ‖SG(h, y)− ∂L/∂h‖ ≤ . If ‖∂h/∂θ<h‖ is upper bounded by some K and there exists a constant δ ∈ (0, 1) such that in every iteration K ≤ ‖∂L/∂θ<h‖ 1−δ1+δ , then the whole training process converges to the solution of the original problem.\nProof. Directly from construction we get that ‖∂L/∂θ<h− ∂̂L/∂̂θ<h‖ = ‖(∂L/∂h−SG(h, y))∂h/∂θ<h‖ ≤ K thus in each iteration there exists such a vector e, that ‖e‖ ≤ K and ∂̂L/∂̂θ<h = ∂L/∂θ<h + e. Consequently, we get a model trained with noisy gradients, where the noise of the gradient is bounded in norm by K so, directly from assumptions, it is also upper bounded by ‖∂L/∂θ<h‖ 1−δ1+δ and we we get that the direction followed is sufficient for convergence as this means that cosine between true gradient and synthetic gradient is uniformly bounded away (by δ) from zero (Zoutendijk, 1970; Gratton et al., 2011). At the same time, due to Proposition 1, we know that the assumptions do not form an empty set as the SG module can stay in an neighborhood of the gradient, and both norm of the synthetic gradient and ‖∂h/∂θ<h‖ can go to zero around the true critical point.\nCorollary 1. For a deep linear model and an MSE objective, trained with a linear SG module attached between two of its hidden layers, there exist learning rates in each iteration such that it converges to the critical point of the original problem.\nProof. Denote the learning rate of the main model by µ and learning rate of the SG module by ν > 0 and put µ = max(0, ‖e‖ − 1/(3‖∂h/∂θ<h‖)‖∂L/∂θ<h‖), where is a small learning rate (for example found using line search)\nand e is the error SG will make in the next iteration. The constant 1/3 appears here as it is equal to (1− δ)/(1 + δ) for δ = 0.5 which is a constant from Proposition 2, which we will need later on. Norm of e consists of the error fitting term LSG which we know, and the term depending on the previous µ value, since this is how much the solution for the SG problem evolved over last iteration. In such a setting, the main model changes iff\n‖e‖‖∂h/∂θ<h‖ < 1/3‖∂L/∂θ<h‖. (2)\nFirst of all, this takes place as long as ν is small enough since the linear SG is enough to represent ∂L/∂h with arbitrary precision (Proposition 1) and it is trained to do so in a way that always converges (as it is a linear regression fitted to a linear function). So in the worst case scenario for a few first iterations we choose very small µ (it always exists since in the worst case scenario µ = 0 agrees with the inequality). Furthermore, once this happens we follow true gradient on θ>h and a noisy gradient on θ<h. Since the noise is equal to e∂h/∂θ<h we get that\n‖e∂h/∂θ<h‖ ≤ ‖e‖‖∂h/∂θ<h‖ < 1/3‖∂L/∂θ<h‖,\nwhich is equivalent to error for θ<h being upper bounded by (1 − δ)/(1 + δ)‖∂L/∂h‖ for δ = 0.5 which matches assumptions of Proposition 2, thus leading to the convergence of the model considered. If at any moment we lose track of the gradient again – the same mechanism kicks in - µ goes down for as long as the inequality (2) does not hold again (and it has to at some point, given ν is positive and small enough)."
    }, {
      "heading" : "C. Technical details",
      "text" : "All experiments were performed using TensorFlow (Abadi et al., 2016). In all the experiments SG loss is the MSE between synthetic and true gradients. Since all SGs considered were linear, weights were initialized to zeros so initially SG produces zero gradients, and it does not affect convergence (since linear regression is convex)."
    }, {
      "heading" : "Datasets",
      "text" : "Each of the artificial datasets is a classification problem, consisting of X sampled from k-dimensional Gaussian distribution with zero mean and unit standard deviation. For k = 2 we sample 100 points and for k = 100 we sample 1000. Labels y are generated in a way depending on the dataset name:\n• lineark - we randomly sample an origin-crossing hyperplane (by sampling its parameters from standard Gaussians) and label points accordingly,\n• noisyk - we label points according to lineark and then randomly swap labels of 10% of samples,\n• randomk - points are labeled completely randomly.\nWe used one-hot encoding of binary labels to retain compatibility with softmax-based models, which is consistent with the rest of experiments. However we also tested the same things with a single output neuron and regular sigmoid-based network and obtained analogous results."
    }, {
      "heading" : "Optimisation",
      "text" : "Optimisation is performed using the Adam optimiser (Kingma & Ba, 2014) with a learning rate of 3e−5. This applies to both main model and to SG module."
    }, {
      "heading" : "Artificial datasets",
      "text" : "Table 2 shows results for training linear regression (shallow MSE), 10 hidden layer deep linear regression (deep MSE), logistic regression (shallow log loss) and 10 hidden layer deep linear classifier (deep log loss). Since all these problems (after proper initialisation) converge to the global optima, we report the difference between final loss obtained for SG enriched models and the true global optimum."
    }, {
      "heading" : "MNIST experiments",
      "text" : "Networks used are simple feed forward networks with h layers of 512 hidden relu units followed by batch normalisation layers. The final layer is a regular 10-class softmax layer. Inputs were scaled to [0, 1] interval, besides that there was no preprocessing applied."
    }, {
      "heading" : "Representational Dissimilarity Matrices",
      "text" : "In order to build RSMs for a layer h we sample 400 points (sorted according to their label) from the MNIST dataset, {xi}400i=1 and record activations on each of these points, hi = h(xi). Then we compute a matrix RSM such that RSMij = 1 − corr(hi, hj). Consequently a perfect RSM is a block diagonal matrix, thus elements of the same class have a representation with high correlation and the representations of points from two distinct classes are not correlated. Figure 7 is the extended version of the analogous Figure 3 from the main paper where we show RDMs for backpropagation, a single SG, SG in-between every two layers, and also the DFA model, when training 20 hidden layer deep relu network.\nUnderstanding Synthetic Gradients and DNIs\ndataset model MSE log loss\nlinear2 shallow 0.00000 0.03842 linear100 shallow 0.00002 0.08554 noisy2 shallow 0.00000 0.00036 noisy100 shallow 0.00002 0.00442 random2 shallow 0.00000 0.00000 random100 shallow 0.00004 0.00003 noisy2 deep 0.00000 0.00000 noisy100 deep 0.00001 0.00293 random2 deep 0.00000 0.00000 random100 deep 0.00001 0.00004\nTable 2. Differences in final losses obtained for various models/datasets when trained with SG as compared to model trained with backpropagation. Bolded entries denote experiments which converged to a different solution. lineark is k dimensional, linearly separable dataset, noisy is linearly separable up to 10% label noise, and random has completely random labeling. Shallow models means linear ones, while deep means 10 hidden layer deep linear models. Reported differences are averaged across 10 different datasets from the same distributions."
    }, {
      "heading" : "Linear classifier/regression probes",
      "text" : "One way of checking the degree to which the actual classification problem is solved at every layer of a feedforward network is to attach linear classifiers to every hidden layer and train them on the main task without backpropagating through the rest of the network. This way we can make a plot of train accuracy obtained from the representation at each layer. As seen in Figure 8 (left) there is not much of the difference between such analysis for backpropagation and a single SG module, confirming our claim in the paper that despite different representations in both sections of SG based module - they are both good enough to solve the main problem. We can also that DFA tries to solve the classification problem bottom-up as opposed to up-bottom – notice that for DFA we can have 100% accuracy after the very first hidden layer, which is not true even for backpropagation.\nWe also introduced a new kind of linear probe, which tries to capture how much computation (non-linear transformations) are being used in each layer. To achieve this, we at-\ntach a linear regressor module after each hidden layer and regress it (with MSE) to the input of the network. This is obviously label agnostic approach, but measures how non-linear the transformations are up to the given hidden layer. Figure 8 (right) again confirms that with a single SG we have two parts of the network (thus results are similar to RDM experiments) which do have slightly different behaviour, and again show clearly that DFA performs lots of non-linear transformations very early on compared to all other methods."
    }, {
      "heading" : "Loss estimation",
      "text" : "In the main paper we show how SG modules using both activations and labels are able to implicitly describe the loss surface reasonably well for most of the training, with different datasets and losses. For completeness, we also include the same experiment for SG modules which do not use label information (Figure 9 (a) - (d)) as well as a module which does not use activations at all6 (Figure 9 (e) - (h))). There are two important observations here: Firstly, none of these two approaches provide a loss estimation fidelity comparable with the full SG (conditioned on both activations and labels). This gives another empirical confirmation for correct conditioning of the module. Secondly, models which used only labels did not converge to a good solutions after 100k iterations, while without the label SG was able to do so (however it took much longer and was far noisier).\n6This is more similar to a per-label stale gradient model."
    } ],
    "references" : [ {
      "title" : "Understanding intermediate layers using linear classifier probes",
      "author" : [ "Alain", "Guillaume", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1610.01644,",
      "citeRegEx" : "Alain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alain et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning in the machine: Random backpropagation and the learning channel",
      "author" : [ "Baldi", "Pierre", "Sadowski", "Peter", "Lu", "Zhiqin" ],
      "venue" : "arXiv preprint arXiv:1612.02734,",
      "citeRegEx" : "Baldi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Baldi et al\\.",
      "year" : 2016
    }, {
      "title" : "Kickback cuts backprop’s red-tape: Biologically plausible credit assignment in neural networks",
      "author" : [ "Balduzzi", "David", "Vanchinathan", "Hastagiri", "Buhmann", "Joachim" ],
      "venue" : "arXiv preprint arXiv:1411.6191,",
      "citeRegEx" : "Balduzzi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balduzzi et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards biologically plausible deep learning",
      "author" : [ "Bengio", "Yoshua", "Lee", "Dong-Hyun", "Bornschein", "Jorg", "Mesnard", "Thomas", "Lin", "Zhouhan" ],
      "venue" : "arXiv preprint arXiv:1502.04156,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "How much gradient noise does a gradient-based linesearch method tolerate",
      "author" : [ "Gratton", "Serge", "Toint", "Philippe L", "Tröltzsch", "Anke" ],
      "venue" : "Technical report, Citeseer,",
      "citeRegEx" : "Gratton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gratton et al\\.",
      "year" : 2011
    }, {
      "title" : "Decoupled neural interfaces using synthetic gradients",
      "author" : [ "Jaderberg", "Max", "Czarnecki", "Wojciech Marian", "Osindero", "Simon", "Vinyals", "Oriol", "Graves", "Alex", "Kavukcuoglu", "Koray" ],
      "venue" : "arXiv preprint arXiv:1608.05343,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Representational similarity analysis-connecting the branches of systems neuroscience",
      "author" : [ "Kriegeskorte", "Nikolaus", "Mur", "Marieke", "Bandettini", "Peter A" ],
      "venue" : "Frontiers in systems neuroscience,",
      "citeRegEx" : "Kriegeskorte et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kriegeskorte et al\\.",
      "year" : 2008
    }, {
      "title" : "Random synaptic feedback weights support error backpropagation for deep learning",
      "author" : [ "Lillicrap", "Timothy P", "Cownden", "Daniel", "Tweed", "Douglas B", "Akerman", "Colin J" ],
      "venue" : "Nature Communications,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Toward an integration of deep learning and neuroscience",
      "author" : [ "Marblestone", "Adam H", "Wayne", "Greg", "Kording", "Konrad P" ],
      "venue" : "Frontiers in Computational Neuroscience,",
      "citeRegEx" : "Marblestone et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Marblestone et al\\.",
      "year" : 2016
    }, {
      "title" : "Direct feedback alignment provides learning in deep neural networks",
      "author" : [ "Nøkland", "Arild" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Nøkland and Arild.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nøkland and Arild.",
      "year" : 2016
    }, {
      "title" : "βky + γk)",
      "author" : [ "Jaderberg" ],
      "venue" : null,
      "citeRegEx" : "Jaderberg,? \\Q2016\\E",
      "shortCiteRegEx" : "Jaderberg",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view.",
      "startOffset" : 159,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "One way of overcoming this issue is to apply Synthetic Gradients (SGs) to build Decoupled Neural Interfaces (DNIs) (Jaderberg et al., 2016).",
      "startOffset" : 115,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "While the empirical evidence in Jaderberg et al. (2016) suggests that SGs do not have a negative impact and that this potential is attainable, this paper will dig deeper and analyse the result of using SGs to accurately answer the question of the impact of synthetic gradients on learning systems.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "In addition, in Section 6 we look at formalising the connection between SGs and other forms of approximate error propagation such as Feedback Alignment (Lillicrap et al., 2016), Direct Feedback Alignment (Nøkland, 2016; Baldi et al.",
      "startOffset" : 152,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : ", 2016), Direct Feedback Alignment (Nøkland, 2016; Baldi et al., 2016), and Kickback (Balduzzi et al.",
      "startOffset" : 35,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : ", 2016), and Kickback (Balduzzi et al., 2014), and show that all these error approximation schemes can be captured in a unified framework, but crucially only using synthetic gradients can one achieve unlocked training.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Jaderberg et al. (2016) introduces a learnt prediction of the error gradient, the synthetic gradient SG(hi , yi) = ̂ ∂Li/∂hi ' ∂Li/∂hi resulting in the update",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "the topmost layers), which matches observations from Jaderberg et al. (2016) and the intuition that the lower layers solve a more complex problem (since they bootstrap their targets).",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Proof follows from showing that, under the assumptions, effectively we are training with noisy gradients, where the noise is small enough for convergence guarantees given by Zoutendijk (1970); Gratton et al. (2011) to apply.",
      "startOffset" : 193,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "One approach is to compute and visualise Representational Dissimilarity Matrices (Kriegeskorte et al., 2008) for our data.",
      "startOffset" : 81,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "We focus on three such approaches: Feedback Alignment (FA) (Lillicrap et al., 2016), Direct Feedback Alignment (DFA) (Nøkland, 2016), and Kickback (KB) (Balduzzi et al.",
      "startOffset" : 59,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : ", 2016), Direct Feedback Alignment (DFA) (Nøkland, 2016), and Kickback (KB) (Balduzzi et al., 2014).",
      "startOffset" : 76,
      "endOffset" : 99
    } ],
    "year" : 2017,
    "abstractText" : "When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backpropagated – resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.",
    "creator" : "LaTeX with hyperref package"
  }
}