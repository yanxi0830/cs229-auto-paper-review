{
  "name" : "1606.08061.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exact gradient updates in time independent of output size for the spherical loss family",
    "authors" : [ "Pascal Vincent", "Alexandre de Brébisson", "Xavier Bouthillier" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "4d , i.e. two orders of magnitude for typical sizes, for that critical part\nof the computations that often dominates the training time in this kind of network architecture.\n1 Introduction Many modern applications of neural networks have to deal with data represented, or representable, as very large sparse vectors. Such representations arise in natural language related tasks, where the dimension D of that vector is typically (a multiple of) the size of the vocabulary, but also in the sparse user-item matrices of collaborativefiltering applications. It is trivial to handle very large sparse inputs to a neural network in a computationally efficient manner: the forward propagation and update to the input weight matrix after backpropagation are correspondingly sparse. By contrast, training\nar X\niv :1\n60 6.\n08 06\n1v 1\n[ cs\n.N E\n] 2\nwith very large sparse prediction targets is problematic: even if the target is sparse, the computation of the equally large network output and the corresponding gradient update to the huge output weight matrix are not sparse and thus computationally prohibitive. This has been a practical problem ever since Bengio et al. [1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2]. Several approaches have been proposed to attempt to address this difficulty essentially by sidestepping it. They fall in two categories: • Sampling or selection based approximations consider and compute only a tiny frac-\ntion of the output’s dimensions sampled at random or heuristically chosen. The reconstruction sampling of Dauphin et al. [3], the efficient use of biased importance sampling in Jean et al. [4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al. [7] all fall under this category. As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset. • Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class. Compared to the initial problem of considering all D output dimensions, both kinds of approaches are crude approximations. In the present work, we will instead investigate a way to actually perform the exact gradient update that corresponds to considering all D outputs, but do so implicitly, in a computationally efficient manner, without actually computing the D outputs. This approach works for a relatively restricted class of loss functions, that we call the spherical family, its simplest member being linear output with squared error (a natural choice for sparse real-valued regression targets). For simplicity and clarity we will begin with this squared error case, presenting the computational challenge that arises in the standard naive approach in Section 2 and deriving our algorithmic solution in Section 3. We will then extend our approach to the more general case of loss functions in the spherical family in Section 4. In Section 5 we will discuss numerical stability issues that may arise and detail our numerical stabilization strategy. Section 6 presents experimental validation focusing on timings obtained with our CPU and GPU implementations of our algorithm relative to the naive update algorithm.\n2 The problem\n2.1 Problem definition and setup We are concerned with gradient-descent based training of a deep feed-forward neural network with target vectors of very high dimension D (e.g. D = 200 000) but that are sparse, i.e. a comparatively small number, at most K D, of the elements of the target vector are non-zero. Such a K-sparse vector will typically be stored and represented compactly as 2K numbers corresponding to pairs (index, value). A network to be trained with such targets will naturally have an equally large output layer of dimension D. We can also optionally allow the input to the network to be a similarly\nhigh dimensional sparse vector of dimensionDin. Between the large sparse target, output, and (optionally large sparse) input, we suppose the network’s intermediate hidden layers to be of smaller, more typically manageable, dimension d D (e.g. d = 500)1.\nMathematical notation:\n• Vectors are denoted using lower-case letters, e.g. h, and are considered columnvectors; corresponding row vectors are denoted with a transpose, e.g. hT . • Matrices are denoted using upper-case letters, e.g. W , withWT the transpose ofW . • The jth column of W is denoted Wj , and its ith row Wi• (both viewed as a column\nvector). • U−T = ( U−1 )T denotes the transpose of the inverse of a square matrix. • 1D denotes a D-dimensional column vector filled with ones. • 1i∈A(y) denotes an indicator function whose value will be 1 if i ∈ A(y) and 0\notherwise. • onehotD(j) = {1i=j}Di=1 is the D-dimensional column vector filled with zeros\nexcept at index j where its value is 1. • Id is the d× d identity matrix.\nNetwork architecture\nWe consider a standard feed forward neural network architecture as depicted in Figure 1. An input vector x ∈ RDin is linearly transformed into a linear activation a(1) = W (1)Tx + b(1) through a Din × d input weight matrix W (1) (and an optional bias vector b(1) ∈ Rd). This is typically followed by a non-linear transformation s to yield the representation of the first hidden layer h(1) = s(a(1)). This first hidden layer representation is then similarly transformed through a number of subsequent non-linear layers (that can be of any usual kind amenable to backpropagation) e.g. h(k) = s(a(k)) with a(k) = W (k)Th(k−1) + b(k) until we obtain last hidden layer representation h = h(m). We then obtain the final D-dimensional network output as o = Wh where W is a D × d output weight matrix, which will be our main focus in this work. Finally, the network’s D-dimensional output o is compared to the D-dimensional target vector y associated with input x using squared error, yielding loss L = ‖o− y‖2.\nTraining procedure\nThis architecture is a typical (possibly deep) multi-layer feed forward neural network architecture with a linear output layer and squared error loss. Its parameters (weight matrices and bias vectors) will be trained by gradient descent, using gradient backpropagation Rumelhart et al. [11], LeCun [12, 13] to efficiently compute the gradients. The procedure is shown in Figure 1. Given an example from the training set as an (input,target) pair (x, y), a pass of forward propagation proceeds as outlined above, computing the hidden representation of each hidden layer in turn based on the previous one, and finally the network’s predicted output o and associated loss\n1Our approach does not impose any restriction on the architecture nor size of the hidden layers, as long as they are amenable to usual gradient backpropagation.\nEfficient Exact Gradient Update for Training Deep Networks with Very Large Sparse Targets Pascal Vincent * Alexandre de Brébisson Xavier Bouthillier Abstract An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 500 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D ! d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. Training time is thus independent of output-layer size (or number of classes). Compared to naive backprop, the proposed algorithm is expected to yield an actual speedup of at least D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computation that often dominates the training time in this kind of network architecture.\nThe Problem ‣ Training deep neural networks with very large sparse targets is an important problem ‣ Arises e.g. in Neural Language Models [1] with large vocabulary size (e.g. D = 500 000 one-hot target). ‣ Efficient handling of large sparse inputs is trivial. ‣ But backprop training with large sparse targets is prohibitively expensive. ‣ Focus on output layer: maps last hidden representation h of reasonable dimension d (e.g. 500)\nExperimental validation Timing of output layer computations, for CPU implementation on 2 GHz Intel Core i7. Minibatch size m =10. Both naive backprop version and the proposed factorised parameter version learn the same actual W.\nDetailed algorithm, benefits and limitations Accepted as a workshop contribution at ICLR 2015 3.5 PUTTING IT ALL TOGETHER: ALGORITHM FOR COMPUTING THE COST L, GRADIENT ON h, AND UPDATING U AND V Efficient computation of cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U−T and Q. The following table describes the algorithmic steps that we put together from the equations derived above. Step # Operation Computational complexity Number of multiply-adds 1: ĥ = Qh O(d2) d2 2: ŷ = UT (V T y) O(Kd + d2) Kd + d2 3: ẑ = ĥ− ŷ O(d) d 4: ∇h = 2ẑ O(d) d 5: L = hT ĥ− 2hT ŷ + yT y O(2d + K) 2d + K + 1 6: Unew = U − 2η(Uh)hT O(d2) 2d2 + d 7: U−Tnew =\nU−T + 2η 1−2ηh2 (U\n−T h)hT O(d2) 2d2 + 2d + 3\n8: Vnew = V + 2ηy(U−Tnewh) T O(d2 + Kd) d2 + K + Kd 9: Qnew = Q− 2η  hẑT + ẑhT  +\n(4η2L)hhT\nO(d2) 4 + 2d + 3d2\n4 DISCUSSION: EXPECTED BENEFITS, EXTENSIONS AND LIMITATIONS\nHaving K  d  D we see that the proposed algorithm requires O(d2) operations whereas the standard approach required O(Dd) operations. If we take K ≈ d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will requires roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm change corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the expected speedup is thus 100.\nNote that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and cha ge ll D × d elements of matrix W , whereas the proposed approach only accesses the much smaller number K×d element of V as well as the three d× d matrices U , U−T , and Q. So overall we have a much faster algorithm, which while doing so implicitly, will however perform the exact same gradient update as the standard approach. We want to emphasize here that what we are doing is not at all the same as simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to Uand V would not be equivalent to the ordinary gradient update to W = V U .\nOur algorithm can be straightforwardly extended to the minibatch case, and is expected to yield the same speedup factor compared to the standard approach. But one needs to be careful in order to keep the computation of U−T h reasonably efficient. Indeed, depending on the size of the minibatch m, it may be more efficient to resolve the correpsonding linear equation for each minibatch from scratch rather than updating U−T with the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1). This approach that we detailed for linear output and squared error can easily be extended to slightly more exotic loss functions: basically any loss function that can be expressed using only the oc associated to non-zero yc and o2 =  j o 2 j the squared norm of the whole output vector, which we can compute cheaply. This family of loss functions does not include the standard softmax, but includes the so-called spherical softmax: log o 2 c\nj o 2 j (where c is the correct class label). It remains to be seen in practice how this approach performs computationally, and whether we lose something due to using this more limited family of loss functions.\n7\n* and CIFAR\nProposed approach We can do much better than O( Dd ). We can compute ! loss L ! gradient w.r.t. last hidden layer !h ! exact same gradient update to W all in O(d2) without ever computing full output o=Wh ! First trick: L and !h can be computed efficiently if we keep an up-to-date d x d matrix Q = WTW\nSecond trick: represent W implicitly as factorization and update U and V instead\n5.1 Computing the squared error loss efficiently Suppose we have, for a network input example x, computed last hidden representation h ∈ Rd through forward propagation. The network’s D dimensional output o = Wh is then in principle compared to high dimensional target y ∈ RD. The corresponding squared error loss is L = Wh− y2. As we have seen in Section 3.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output Wh with a full D×d matrix W and a typically non-sparse h is O(Dd). Note however that we can rewrite this as: L = Wh− y2 = (Wh− y)T (Wh− y) = hT WT Wh− yT Wh− hT WT y + yT y = hT Qh− 2hT (WT y) + yT y = hT Qh− 2hT UT V T y + yT y = hT (Qh)− 2hT (UT (V T y)) + yT y = hT ( Qh\nĥ\n−2(UT (V T y)   ŷ ) + yT y\nSHORT IDEA FORMULATION FOR SLIDES:\nL =  O(Dd) Wh −y2\n= (Wh− y)T (Wh− y) = hT WT Wh− 2hT (WT y) + yT y = hT ( Qh\nO(d2) −2(WT y)   O(Kd) ) + yT y O(K)\nwith Q = WT W Supposing we have maintained an up-to-date Q = WT W , which is a compact d×d matrix (we will see how we update Q cheaply in section ??????), computing ĥ = Qh has a complexity of O(d2). Thanks to the K−sparsity and sparse representation of y, computing V T y is O(Kd) and results in a d−dimensional vector, so that computing ŷ = UT (V T y) is O(Kd + d2) . The last term is O(K). So the overall computational complexity for computing L in this way is O(Kd+d2) = O((K +d)d). With K  D and d  D this can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.\nIf we define intermediate vectors ĥ = Qh and ŷ = WT y = UT (V T y) the computation of L can be rewritten a little more compactly as\nL = hT (ĥ− 2ŷ) + y2\n5\nthis is O(Kd +d2 +K) = O(d2)\nComputing loss L\n5.2 Computing the gradient on h efficiently To backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last hidden layer representation h. This is ∇h = ∂L∂h = ∂Wh−y2 ∂h = 2W T (Wh − y). Again, if we were to compute it directly in this manner the computational complexity would be a prohibitive O(Dd). But we can instea rewrite it as ∇h = ∂L ∂h = ∂ Wh− y2 ∂h = 2WT (Wh− y) = 2  WT Wh−WT y  = 2  Qh− UT V T y  = 2  Qh− UT (V T y)  = 2(ĥ− ŷ) Again, supposing we have maintained an up-to-date Q (we will see how we update Q cheaply in section ?????) computing ∂L∂h this way is O(Kd + d 2) = O((K + d)d), much ch a er than the O(Dd) of the direct approach.\nHORT IDEA FORMULATION FOR SLIDES:\n∇h = ∂L\n∂h = ∂Wh− y2 ∂h\n= 2WT (Wh− y) = 2( Qh\nO(d2) −WT y   O(Kd) )\n5.3 Efficient gradient update of W The gradient of the squared error loss with respect to output layer weight matrix W is ∂L∂W = ∂Wh−y2 ∂W = 2(Wh−y)hT . And the corresponding gradient descent update to W would be Wnew ← W − 2η(Wh − y)hT where η is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residue Wh − y, and then to update all the Dd elements of W (since generally neither Wh− y nor h will be sparse). To overcome this difficulty let us first rewrite the update as\nWnew = W − 2η(Wh− y)hT = W − 2ηWhhT + 2ηyhT\nNote that we can decompose this update into two consecutive update steps:\n6\nthis is O(Kd +d2) = O(d2)\nProvided w maintain an up-to-date Q = WTW (achievable cheaply)\nComputing gradient !h w.r.t. last hidden layer\nW D×d = V D×d U d×d\n5.2 Computing the gradient n h efficiently To backpropagate the gradient through the network, we need to comput the gradient of loss L with respect to last hidden layer representation h. This is ∇h = ∂L∂h = ∂Wh−y2 ∂h = 2W T (Wh − y). Again, if we were to compute it directly in this manner the computational complexity would be a prohibitive O(Dd). But we can instead rewrite it as ∇h = ∂L ∂h = ∂ Wh− y2 ∂h = 2WT (Wh− y) = 2  WT Wh−WT y  = 2  Qh− UT V T y  = 2  Qh− UT (V T y)  = 2(ĥ− ŷ)\nAgain, supposing we have maintained an up-to-date Q we will see how we update Q cheaply in section ?????) computing ∂L∂h this way is O(Kd + d\n2) = O((K + d)d), much cheaper than the O(Dd) of the direct approach.\nSHORT IDEA FORMULATION FOR SLIDES:\n∇h = ∂L\n∂h = ∂Wh− y2 ∂h\n= 2WT (Wh− y) = 2( Qh\nO(d2) −WT y   O(Kd) )\n5.3 Efficient gradient update of W The gradient of the squared error loss with respect to output layer weight matrix W is ∂L∂W = ∂Wh−y2 ∂W = 2(Wh−y)hT . And the corresponding gradient descent update to W would be Wnew ← W − 2η(Wh − y)hT where η is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residue Wh − y, and then to update all the Dd elements of W (since generally neither Wh− y nor h will be sparse). To overcome this difficulty let us first rewrite the update as\nWnew = W − 2η(Wh− y)hT = W − 2ηWhhT + 2ηyhT\nNote that we can decompose this update into two consecutive update steps:\n6\nNaive gadient update is a rank-one update to W (all Dd elements of W modified!)\nEquivalently decomposed in 2 sequential steps: O( Dd )\na) W ←W − 2ηWhhT\nb) W ←W + 2ηyhT\nWe will now see how we can perform each of these updates implicitly by updating only U and V respectively, as well as how we maintain correspondingly up-todate versio s of Q = V T V (needed to effic ently compute cost L and gradient on h in Equations ???? and ???? above) and U−T = (U−1)T (that will be needed for update b) ).\nSolution:\na) Unew = U − 2η(Uh)hT b) Vnew = V + 2ηy(U−Tnewh)T Proof:\nVnewUnew = (V + 2ηy(U −T newh) T ) Unew\n= V Unew + 2ηy(U −T newh) T Unew = V Unew + 2ηyh T U−1newUnew = V (U − 2η(Uh)hT ) + 2ηyhT (U−1newUnew) = V U − 2ηV UhhT + 2ηyhT = V U − 2η(V Uh− y)hT = W − 2η(Wh− y)T hT = Wnew\na) First update of the form W ← W − 2ηWhhT This can be achieved implicitly by updating only U as follows:\nUnew = U − 2η(Uh)hT\nProof:\nWnew = V Unew = V (U − 2η(Uh)hT ) = V U − 2ηV UhhT = W − 2ηWhhT\nChanging U doesn’t change Q = V T V . But we will need an up-to-date U−T in the second update b).\nProvided we already have U−T this can be achieved cheaply by using the Sherman-Morisson formula for the rank-one update to the inverse of U :\n(U + uvT )−1 = U−1 − 1 1 + vT U−1u U−1uvT U−1\n7\na) W ←W − 2ηWhhT\nb) W ←W + 2ηyhT\nWe will now see how we can perform each of these updates implicitly by updating only U and V respectively, as well as how we maintain correspondingly up-todate versions of Q = V T V (needed to efficiently compute cost L and gradient on h in Equations ???? and ???? above) and U−T = (U−1)T (that will be needed for update b) ).\nSolution:\na) Unew = U − 2η(Uh)hT b) Vnew = V + 2ηy(U−Tnewh)T Proof:\nVnewUnew = (V + 2ηy(U −T newh) T ) Unew\n= V Unew + 2ηy(U −T newh) T Unew = V Unew + 2ηyh T U−1newUnew = V (U − 2η(Uh)hT ) + 2ηyhT (U−1newUnew) = V U − 2ηV UhhT + 2ηyhT = V U − 2η(V Uh− y)hT = W − 2η(Wh− y)T hT = Wnew\na) First update of the form W ← W − 2ηWhhT This can be achieved implicitly by updating only U as follows:\nUnew = U − 2η(Uh)hT\nProof:\nWnew = V Unew = V (U − 2η(Uh)hT ) = V U − 2ηV UhhT = W − 2ηWhhT\nChanging U doesn’t change Q = V T V . But we will need an up-to-date U−T in the second update b).\nProvided we already have U−T this can be achieved cheaply by using the Sherman-Morisson formula for the rank-one update to the inverse of U :\n(U + uvT )−1 = U−1 − 1 1 + vT U−1u U−1uvT U−1\n7\nThat can be performed implic ty through U and V:\nrank-1 update to U: O(d2)\nO(Kd) O(d2) provided we updated U-1 cheaply using Sherman-Morrison\nSparse update: only K rows of V instead of all D rows of W !\nO( Dd )\nProof:\nAccepted as a workshop contribution at ICLR 2015\na) W ← W − 2ηWhhT b) W ← W + 2ηyhT\nNotice that we can perform each of these updates implicitly by updating only U and V respectively.:\na) Unew = U − 2η(Uh)hT (4)\nb) Vnew = V + 2ηy(U −T newh) T (5)\nThis results in implicitly updating W as we did explicitly in the naive approach of Eq. 3.\nProof:\nVnewUnew = (V + 2ηy(U −T newh) T ) Unew\n= V Unew + 2ηy(U −T newh) T Unew = V Unew + 2ηyh T U−1newUnew = V (U − 2η(Uh)hT ) + 2ηyhT ( −1newUnew) = V U − 2ηV UhhT + 2ηyhT = V U − 2η(V Uh− y)hT = W − 2η(Wh− y)T hT = Wnew\nWe see that the update of U in Eq. 4 is a simple O(d2) operation. Following this simple rank-one update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U−T which will also be O(d2):\nU−Tnew = U −T +\n2η\n1− 2η h2 (U−T h)hT (6)\nIt is then easy to compute the U−Tnewh, an O(d 2) operation needed in Eq. 5, and the ensuing rank-one update of V , thanks to the K-sparsity of y is only O(Kd). Thanks to the K−sparsity and sparse representation of y, computing ŷ = V T y is O(Kd) and t2 is O(K). Computation of ĥ = U−T h is O(d2). Given these, the update of Q is O(d2) and the rank-one update of V , thanks to the K-sparsity of y is O(Kd). So these operations together have computational complexity of O(Kd + d2) = O((K + d)d), which is much cheaper than the prohibitive O(Dd) of the direct approach.\n3.4 BOOKKEEPING: KEEPING AN UP-TO-DATE Q AND U−T We have already seen, in Eq. 6, how we can cheaply maintain an up-to-date U−T following our update of U . Similarly, following our updates to U and V , we need to keep an up-to-date Q = WT W which is needed to efficiently compute the loss L (Eq. 1) and gradient ∇h (Eq. 2). The updates to and V in Equations 4 and 5 are equivalent to implicitly updating W as in Eq. 3, and this translates into the following update to Q = WT W :\nẑ = Qh− UT (V T y) Qnew = Q− 2η  hẑT + ẑhT  + (4η2L)hhT (7)\nProof is straightforward but not provided here due to space constraints.\n6\nBookkeeping operations as we update U and V: ! Using factored representation of W=VU does not change the complexity of the computation of L and !h .\n! Need to maintain an up-to-date U-1 following rank-1 update to U. \" achieved in O(d2) through Sherman-Morrison formula.\n! Need to maintain an up-to-date Q = WTW following updates to U and V. \" achieved in O(d2) as follows:\na) W ←W − 2ηWhhT\nb) W ←W + 2ηyhT\nWe will now see how we can perform each of these updates implicitly by updating only U and V respectively, as well as how we maintain correspondingly up-todate versions of Q = V T V (needed to efficiently compute cost L and gradient on h in Equations ???? and ???? above) and U−T = (U−1)T (that will be needed for update b) ).\nSolution:\na) Unew = U − 2η(Uh)hT b) Vnew = V + 2ηy(U−Tnewh)T\nProof:\nVnewUnew = (V + 2ηy(U −T newh) T ) Unew\n= V Unew + 2ηy(U −T newh) T Unew = V Unew + 2ηyh T U−1newUnew = V (U − 2η(Uh)hT ) + 2ηyhT (U−1newUnew) = V U − 2ηV UhhT + 2ηyhT = V U − 2η(V Uh− y)hT = W − 2η(Wh− y)T hT = Wnew\nSHORT FORMULATION FOR SLIDES OF UPDATE OF Q IN ONLINE CASE:\nẑ = Qh− UT (V T y) Qnew = Q− 2η  hẑT + ẑhT  + (4η2L)hhT\na) First update of the form W ← W − 2ηWhhT This can be achieved implicitly by updating only U as follows:\nUnew = U − 2η(Uh)hT\nProof:\nWnew = V Unew = V (U − 2η(Uh)hT ) = V U − 2ηV UhhT = W − 2ηWhhT\n7\nNote: this is NOT th same as a ordinary backprop update on two consecutive layers U and V which would still be O( Dd ). Altogether: O( d2 ) we suppose K << d << D\nwe suppose K << d << D\n‣ Sampling based approximations compute only a tiny fraction of the output’s dimensions sampled at random.\nReconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.\n‣ Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.\n[1] Bengio, Y., Ducharme, R., and Vincent, P. (2001). A neural probabilistic language model. NIPS 2000.\n[2] Dauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with reconstruction sampling. ICML 2011.\n[5] Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation. NIPS 2013. [6] Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language model. AISTATS 2005. [3] Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. AISTATS 2010. [4] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. ICLR 2013 workshop track.\nwe suppose K << d << D Full algorithm (online version): ! Computation: O(12 d2) v.s. O(3 Dd) \" speedup of D/4d for typical sizes: between 50 and 300\n! Memory access: for each example access only Kd elements of V and d2 elements of U, U-1 and Q v.s. Dd elements of W.\nAnticipated benefits:\n! Approach limited to loss functions expressible using ||o||2 and the oc associated to non-zero yc only: ✓ linear output + squared error # not regular log softmax ✓ linear+spherical softmax:\n! Step 6 can lead over time to ill conditioning \" must periodically apply numerical stabilization strategy.\nLimitations\nExtension for minibatch of size m: ! Straightforward except for step 7: ! Update of U-T no longer with simple Sherman-Morrison. ! Several possibilities: Woodbury identity (must invert m x m matrix), or iterated\nSherman-Morrison, or solving UTx = h each time. Best choice will depends on m. ! \" complexity remains O(d2) per example.\nAccepted as a workshop contribution at ICLR 2015 3.5 PUTTING IT ALL TOGETHER: ALGORITHM FOR COMPUTING THE COST L, GRADIENT ON h, AND UPDATING U AND V Efficient computation of cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U−T and Q. The following table describes the algorithmic steps that we put together from the equations derived above. Step # Operation Computational complexity Number of multiply-adds 1: ĥ = Qh O(d2) d2 2: ŷ = UT (V T y) O(Kd + d2) Kd + d2 3: ẑ = ĥ− ŷ O(d) d 4: ∇h = 2ẑ O(d) d 5: L = hT ĥ− 2hT ŷ + yT y O(2d + K) 2d + K + 1 6: Unew = U − 2η(Uh)hT O(d2) 2d2 + d 7: U−Tnew = U−T + 2η 1−2ηh2 (U −T h)hT O(d2) 2d2 + 2d + 3 8: Vnew = V + 2ηy(U−Tnewh) T O(d2 + Kd) d2 + K + Kd 9: Qnew = Q− 2η  hẑT + ẑhT  + (4η2L)hhT O(d2) 4 + 2d + 3d2 4 DISCUSSION: EXPECTED BENEFITS, EXTENSIONS AND LIMITATIONS Having K  d  D we see that the proposed algorithm requires O(d2) operations whereas the standard approach required O(Dd) operations. If we take K ≈ d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will requires roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm change corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the ex ected speedup is thus 100. Note that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change all D × d elements of matrix W , whereas the proposed approach only accesses the much smaller number K×d element of V as well as the three d× d matrices U , U−T , and Q. So overall we have a much faster algorithm, which while doing so implicitly, will however perform the exact same gradient update as the standard approach. We want to emphasize here that what we are doing is not at all the same as simply chaining 2 linear layers U and V and performing or inary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to Uand V would not be equivalent to the ordinary gradient update to W = V U . Our algorithm can be straightforwardly extended to the minibatch case, and is expected to yield the same speedup factor compared to the standard approach. But one needs to be careful in order to keep the computation of U−T h reasonably efficient. Indeed, depending on the size of the minibatch m, it may be more efficient to resolve the correpsonding linear equation for each minibatch from scratch rather than updating U−T with the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1). This approach that we detailed for linear output and squared error can easily be extended to slightly more exotic loss functions: basically any loss function that can be expressed using only the oc associated to non-zero yc and o2 =  j o 2 j the squared norm of the whole output vector, which we can compute cheaply. This family of loss functions does not include the standard softmax, but includes the so-called spherical softmax: log o 2 c\nj o 2 j (where c is the correct class label). It remains to be seen in practice how this approach performs computationally, and whether we lose something due to using this more limited family of loss functions.\n7\nTime taken by naive backprop (dotted lines) and the proposed factorised parameter version (full lines).\nSpeedup of factorised parameter version v.s. naive backprop (theoretical and experimentally measured).\nConclusion and future work ‣ We developed an original algorithm that yields a huge speedup for performing a full exact gradient update in networks with very large\nsparse targets: remarkably time is independent of output size (number of classes).\n‣ Gain is from a fundamental algorithmic computational complexity improvement, not from low-level hardware-specific tricks or tuning. ‣ Future: GPU implementation; spherical softmax cost; compare quality of word embeddings learned with these costs to standard softmax.\nReferences:\nL. A pass of gradient backpropagation then works in the opposite direction, starting from ∇o = ∂L∂o = 2(o − y) and propagating back the gradients ∇h(k) = ∂L∂h(k) and ∇a(k) = ∂L∂a(k) upstream through the network. The corresponding gradient contributions on parameters (weights and biases), collected along the way, are straightforward once we have the associated ∇a(k) . Specifically they are ∇b(k) = ∇a(k) and ∇W (k) = h(k−1)(∇a(k))T . Similarly for the input layer ∇W (1) = x(∇a(1))T , and for the output layer ∇W = (o − y)hT . Parameters are then updated through a gradient descent step W (k) ← W (k) − η∇W (k) and b(k) ← b(k) − η∇b(k) , where η is a positive learning-rate. Similarly for the output layer which will be our main focus here: W ←W − η∇W .\n2.2 The easy part: input layer forward propagation and weight update\nIt is easy and straightforward to efficiently compute the forward propagation, and the backpropagation and weight update part for the input layer when we have a very large Din-dimensional but K−sparse input vector x with appropriate sparse representation. Specifically we suppose that x is represented as a pair of vectors u, v of length (at most) K, where u contains integer indexes and v the associated real values of the elements of x such that xi = 0 if i /∈ u, and xuk = vk. • Forward propagation through the input layer: The sparse representation of x as\nthe positions of K elements together with their value makes it cheap to compute W (1)Tx. Even though W (1) may be a huge full Din × d matrix, only K of its rows (those corresponding to the non-zero entries of x) need to be visited and summed to computeW (1)Tx. Precisely, with our (u, v) sparse representation of x this operation can be written asW (1)Tx = ∑K k=1 vkW (1) :uk where each W (1) :uk is a d-dimensional\nvector, making this an O(Kd) operation rather than O(Dd). • Gradient and update through input layer: Let us for now suppose that we were\nable to get gradients (through backpropagation) up to the first hidden layer activations a(1) ∈ Rd in the form of gradient vector ∇a(1) = ∂L∂a(1) . The corresponding gradient-based update to input layer weights W (1) is simply W (1) ← W (1)−ηx(∇a(1))T . This is a rank-one update toW (1). Here again, we see that only the K rows of W (1) associated to the (at most) K non-zero entries of x need to be modified. Precisely this operation can be written as:W (1):uk ←W (1):uk−ηvk∇a(1) ∀k ∈ {1, . . . ,K} making this again a O(Kd) operation rather than O(Dd).\n2.3 The hard part: output layer propagation and weight update Given some network input x we suppose we can compute without difficulty through forward propagation the associated last hidden layer representation h ∈ Rd. From then on: • Computing the final output o = Wh incurs a prohibitive computational cost of O(Dd) since W is a full D × d matrix. Note that there is a-priori no reason for representation h to be sparse (e.g. with a sigmoid non-linearity) but even if it was, this would not fundamentally change the problem since it is D that is extremely large, and we supposed d reasonably sized already. Computing the residual (o− y) and associated squared error loss ‖o− y‖2 incurs an additional O(D) cost. • The gradient on h that we need to backpropagate to lower layers is ∇h = ∂L∂h = 2WT (o− y) which is another O(Dd) matrix-vector product. • Finally, when performing the corresponding output weight update W ←W −η(o− y)hT we see that it is a rank-one update that updates allD×d elements ofW , which again incurs a prohibitive O(Dd) computational cost. For very large D, all these three O(Dd) operations are prohibitive, and the fact that y is sparse, seen from this perspective, doesn’t help, since neither o nor o − y will be sparse.\n3 A computationally efficient algorithm for performing the exact online gradient update\nPreviously proposed workarounds are approximate or use stochastic sampling. We propose a different approach that results in the exact same, yet efficient gradient update, remarkably without ever having to compute large output o.\n3.1 Computing the squared error loss L and the gradient with respect to h efficiently\nSuppose that, we have, for a network input example x, computed the last hidden representation h ∈ Rd through forward propagation. The network’s D dimensional output o = Wh is then in principle compared to the high dimensional target y ∈ RD. The corresponding squared error loss is L = ‖Wh− y‖2. As we saw in Section 2.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output Wh with a full D × d matrix W and a typically non-sparse h is O(Dd). Similarly, to backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last hidden layer representation h. This is ∇h = ∂L∂h = ∂‖Wh−y‖2 ∂h = 2W\nT (Wh− y). So again, if we were to compute it directly in this manner, the computational complexity would be a prohibitive O(Dd). Provided we have maintained an up-to-date matrix Q = WTW , which is of reasonable size d × d and can be cheaply maintained as we will see in Section 3.4, we can rewrite these two operations so as to perform them in O(d2):\nLoss computation:\nL = ‖ O(Dd)︷︸︸︷ Wh −y‖2\n= (Wh− y)T (Wh− y) = hTWTWh− yTWh− hTWT y + yT y = hTQh− 2hT (WT y) + yT y = hT ( Qh︸︷︷︸\nO(d2) −2 WT y︸ ︷︷ ︸ O(Kd) ) + yT y︸︷︷︸ O(K)\n(1)\nGradient on h:\n∇h = ∂L\n∂h = ∂‖Wh− y‖2 ∂h\n= 2WT (Wh− y) = 2 ( WTWh−WT y )\n= 2( Qh︸︷︷︸ O(d2) −WT y︸ ︷︷ ︸ O(Kd) ) (2)\nThe terms in O(Kd) and O(K) are due to leveraging the K-sparse representation of target vector y. WithK D and d D, we get altogether a computational cost of O(d2) which can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.\n3.2 Efficient gradient update of W The gradient of the squared error loss with respect to output layer weight matrix W is ∂L ∂W = ∂‖Wh−y‖2 ∂W = 2(Wh−y)hT . And the corresponding gradient descent update to W would beWnew ←W −2η(Wh−y)hT , where η is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residual Wh − y, and then to update all the Dd elements of W (since generally neither Wh − y nor h will be sparse). All D × d elements of W must be accessed during this update. On the surface this seems hopeless. But we will now see how we can achieve the exact same update on W in O(d2). The trick is to represent W implicitly as the factorization2 W︸︷︷︸\nD×d = V︸︷︷︸ D×d U︸︷︷︸ d×d and update U and V\ninstead:\na) Unew = U − 2η(Uh)hT (3) b) Vnew = V + 2ηy(U −T newh) T (4)\nThis results in implicitly updating W as we did explicitly in the naive approach as we now prove:\nVnewUnew = (V + 2ηy(U −T newh) T )Unew\n= V Unew + 2ηy(U −T newh) TUnew = V Unew + 2ηyh TU−1newUnew = V (U − 2η(Uh)hT ) + 2ηyhT (U−1newUnew) = V U − 2ηV UhhT + 2ηyhT = V U − 2η(V Uh− y)hT = W − 2η(Wh− y)ThT = Wnew\nWe see that the update of U in Eq. 3 is a simple O(d2) operation. Following this simple rank-one update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U−T which will also be O(d2):\nU−Tnew = U −T +\n2η\n1− 2η ‖h‖2 (U−Th)hT (5)\nIt is then easy to compute the U−Tnewh, anO(d 2) operation needed in Eq. 4. The ensuing rank-one update of V in Eq 4, thanks to the K-sparsity of y is only O(Kd): only theK rows V associated to non-zero elements in y are accessed and updated, sited of all D rows of W we had to modify in the naive update!\n2Note that we never factorize a pre-exisitng arbitrary W , which would be prohibitive as W is huge. We will no longer store a W nor work on it explicitly, but only matrices V and U which implicitly represent W .\n3.3 Adapting the computation of L and ∇h to the factored representation of W\nWith the factored representation of W as V U , we only have W implicitly, so the WT y terms that entered in the computation of L and ∇h in the previous section (Eq. 1 on page 6 and 2 on page 6) need to be adapted slightly as ŷ = WT y = UT (V T y), which becomesO(d2+Kd) rather thanO(Kd) in computational complexity. But this doesn’t change the overall O(d2) complexity of these computations.\nThe adapted update computation of L and ∇h can thus be expressed simply as:\n∇h = 2 (Qh︸︷︷︸ ĥ −UT (V T y)︸ ︷︷ ︸ ŷ )\n︸ ︷︷ ︸ ẑ\n(6)\nand L = hT (Qh︸︷︷︸\nĥ\n−2UT (V T y)︸ ︷︷ ︸ ŷ ) + yT y (7)\n3.4 Bookkeeping: keeping an up-to-date Q and U−T\nWe have already seen, in Eq. 5, how we can cheaply maintain an up-to-date U−T following our update of U . Similarly, following our updates to U and V , we need to keep an up-to-date Q = WTW which is needed to efficiently compute the loss L (Eq. 1) and gradient∇h (Eq. 2). We have shown that updates to U and V in equations 3 and 4 are equivalent to implicitly updating W as Wnew ← W − 2η(Wh− y)hT , and this translates into the following update to Q = WTW :\nQnew = Q− η ( h∇Th +∇hhT ) + (4η2L)hhT (8)\nOne can see that this last bookkeeping operation also has a O(d2) computational complexity.\nProof that this update to Q corresponds to the update Wnew ← 2(Wh− y)hT\nWTnewWnew = ( W − 2η(Wh− y)hT )T ( W − 2η(Wh− y)hT ) WTnewWnew = W TW − 2ηh(Wh− y)TW − 2ηWT (Wh− y)hT +4η2h(Wh− y)T (Wh− y)hT WTnewWnew = Q− 2η ( hhTWTW − hyTW ) − 2η ( WTWhhT −WT yhT ) +4η2h(hTWTWh− hTWT y − yTWh+ yT y)hT WTnewWnew = Q− 2η ( hhTQ− h(WT y)T ) − 2η ( QhhT − (WT y)hT ) +4η2h(hTQh− hT (WT y)− (WT y)Th+ yT y)hT WTnewWnew = Q− 2ηh ( hTQ− (WT y)T ) − 2η ( Qh−WT y ) hT\n+4η2h(hTQh− 2hTWT y + yT y)hT\nWTnewWnew = Q− ηh(2(Qh−WT y)︸ ︷︷ ︸ ∇h )T − η(2(Qh−WT y)︸ ︷︷ ︸ ∇h )hT\n+4η2h (hT (Qh− 2WT y) + yT y)︸ ︷︷ ︸ L hT\nwhere we see that the last term uses the expression of L from Eq. 1 on page 6 and the first two terms uses the expression of∇h from Eq. 6: ∇h = 2(Qh−UT (V T y)) = 2(Qh−WT y). Thus we have shown that\nWTnewWnew = Q− ηh∇Th − 2η∇hhT + 4η2hLhT = Q− η ( h∇Th +∇hhT ) + (4η2L)hhT\nwhich is the update Qnew that we gave in Eq. 8 above.\n3.5 Putting it all together: detailed online update algorithm and expected benefits\nWe have seen that we can efficiently compute cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping forU−T andQ. Here we put everything together. The parameters of the output layer that we will learn are V,U and implicitly representW asW = V U . We first need to initialize these parameter matrices, as well as bookkeeping matrices Q and U−T in a consistent way, as explained in Algo. 1. We then iterate over the following: • pick a next input,target example x, y (where y is K-sparse and uses an appropriate\nsparse representation) • perform forward propagation through all layers of the network up to the last hidden\nlayer, to compute last hidden layer representation h = h(x), that should include a constant 1 first element. • execute Algo. 2, that we put together from the equations derived above, and that will: compute the associated squared error lossL, perform an implicit gradient update step on W by correspondingly updating V and U in a computationally efficient manner, update bookkeeping matrices Q and U−T accordingly, and compute and return the gradient of the loss with respect to the last hidden layer∇h • having ∇h, further backpropagate the gradients upstream, and use them to update the parameters of all other layers Having K d D we see that the update algorithm we developed requires O(d2) operations, whereas the standard approach required O(Dd) operations. If we take K ≈ d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will require roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm\nchange corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the expected speedup is thus 100. Note that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change allD×d elements of matrixW , whereas the proposed approach only accesses the much smaller number K × d elements of V as well as the three d× d matrices U , U−T , and Q. So overall we have a substantially faster algorithm whose complexity is independent of D, which, while doing so implicitly, will nevertheless perform the exact same gradient update as the standard O(Dd) approach. We want to emphasize here that this approach is entirely different from simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to U and V would not be equivalent to the ordinary gradient update to W = V U .\nAlgorithm 1 Initialization of output layer parameters V,U and bookkeeping matrices Q,U−T • we can initialize D × d matrix V randomly as we would have initialized W so that we initially have V = W . Alternatively we can initialize V to 0 (there won’t be symmetry breaking issues with having W initially be 0 provided the other layers are initialized randomly, since varying inputs and targets will naturally break symmetry for the output layer) • initialize Q ← V TV (or more cheaply initialize Q ← 0 if we have initialized V to 0). • we initialize U to the identity: U ← Id so that, trivially, we initially have V U = W . • initialize U−T ← Id\n3.6 Minibatch version of the algorithm for squared error The algorithm we derived for online gradient is relatively straightforward to extend to the case of minibatches containing m examples. We iniialize parameters as in the online case follpwing Algo. 1 and apply the same training procedure outlined in Section. 3.5, but now using minibatches containing m examples, rather than a single example vector. The corresponding update and gradient computation is given in Algorithm 3 which follows equivalent steps to the online version of Algorithm 2, but using matrices with m columns in place of single column vectors. For example step 3 which in the online algorithm was ∇h = 2(ĥ − ŷ) using d−dimensional vectors becomes in the minibatch version∇H = 2(Ĥ − Ŷ ) using d×m matrices instead.\nNote that in the minibatch version, in step 6, we update U−T based on the Woodbury equation, which generalizes the Sheman-Morrison formula for m > 1 and involves inverting an m × m matrix, an O(m3) operation. But depending on the size of the minibatch m, it may become more efficient to solve the corresponding linear equations for each minibatch from scratch every time, rather than inverting that m×m matrix. In which case we won’t need to maintain an U−T at all. Or in cases of minibatches containing more than d examples, it may even become more efficient to invert U from scratch every time.\nAlgorithm 2 Efficient computation of cost L, gradient ∇h, and update to parameters U and V for squared error, in the online case Inputs (besides above parameters V,U,Q,U−T ): • h ∈ Rd hidden representation vector for one example h ∈ Rd • y ∈ RD associated K-sparse target vector stored using a sparse representation (in-\ndices and values of non-zero elements) • η ∈ R+learning rate for the update Outputs: • L ∈ R the squared error loss for this example • updated parameters and bookkeeping matrices Unew, Vnew, Qnew, U−Tnew • ∇h ∈ Rd the gradient of the loss with respect to h, to further backpropagate up-\nstream. Algorithm:\nStep # Operation Computational complexity Approximate number of elementary operations (multiplyadds) 1: ĥ = Qh O(d2) d2 2: ŷ = UT (V T y) O(Kd+ d2) Kd+ d2 3: ∇h = 2(ĥ− ŷ) O(d) d 4: L = hT ĥ− 2hT ŷ + yT y O(2d+K) 2d+K + 1 5: U ← U − 2η(Uh)hT O(d2) 2d2 + d 6: U−T ← U−T +\n2η 1−2η‖h‖2 (U −Th)hT [ from Sherman-Morrison formula ]\nO(d2) 2d2 + 2d+ 3\n7: V ← V + 2ηy(U−Th)T where we must use the freshly updated U−T\nresulting from step 6)\nO(d2 +Kd) d2+K+Kd\n8: Q← Q− η ( h∇Th +∇hhT ) +\n(4η2L)hhT\nO(d2) 4 + 2d+ 3d2\nAltogether: O(d2) provided K < d\nD\n≈ 12d2 elementary operations\nIn step 9, the updateQnew forQ corresponds to the implicit weight updateWnew ← W − 2η(WH − Y )HT as we now prove:\nWe will use the following precomputed quantities: Q = WTW , Ĥ = QH and Ŷ = WTY = UT (V TY ) and ∇H = 2(Ĥ − Ŷ ).\nQnew = W T newWnew\n= ( W − 2η(WH − Y )HT )T ( W − 2η(WH − Y )HT )\n= WTW − 2ηH(WH − Y )TW − 2ηWT (WH − Y )HT +4η2H(WH − Y )T (WH − Y )HT = Q− 2η ( HHTWTW −HY TW ) − 2η ( WTWHHT −WTY HT )\n+4η2H(HTWTWH −HTWTY − Y TWH + Y TY )HT = Q− 2η ( HHTQ−H(WTY )T ) − 2η ( QHHT − (WTY )HT )\n+4η2H(HTQH −HT (WTY )− (WTY )TH + Y TY )HT\n= Q− 2η ( HĤT −HŶ T + ĤHT − Ŷ HT )\n+4η2H(HT Ĥ −HT Ŷ − Ŷ TH + Y TY )HT\n= Q− 2η ( H(Ĥ − Ŷ )T + (Ĥ − Ŷ )HT ) + 4η2H(HT (Ĥ − Ŷ )− Ŷ TH + Y TY )HT = Q− η ( H(2(Ĥ − Ŷ ))T + (2(Ĥ − Ŷ ))HT ) + 4η2H(HT (Ĥ − Ŷ )− Ŷ TH + Y TY )HT = Q− η ( H∇TH +∇HHT ) + 4η2H ( HT Ẑ − Ŷ TH + Y TY )\n︸ ︷︷ ︸ M\nHT\nwhich is the update of Q we use in in step 8 of Algorithm on the previous page.\nAlgorithm 3 Minibatch version of the update algorithm for squared error Inputs (besides above parameters V,U,Q,U−T ): • parameters and bookkeeping matrices: U, V, Q, U−T • H : a d ×m matrix whose m columns contain the last hidden layer representation\nvectors form example (with an appended constant 1 element to account for an output bias). • Y : aD×m sparse target matrix. Each of itsm columns is theK-sparse target vector associated to one example of the minibatch, stored using a sparse representation (indices and values of non-zero elements). • η ∈ R+learning rate for the update Updates: • parameters and bookkeeping matrices: U, V, Q, U−T Outputs: • L ∈ R the sum of squared error losses for the m examples of the minibatch • ∇H a d ×m matrix whose m columns contain the gradient of the loss with respect\nto H , to further backpropagate upstream. Algorithm:\nStep # Operation Computation complexity Approximate number of elementary operations (multiply-adds)\n1: Ĥ = QH O(md2) md2 2: Ŷ = UT (V TY ) O(mKd+ md2) mKd+md2 3: ∇H = 2(Ĥ − Ŷ ) O(md) md 4a: M =\nHT Ĥ − (Ŷ TH +HT Ŷ ) + Y TY O(m2d+ m2K)\n2m2d+m2K\n4b: L = Tr(M) O(m) m 5: U ← U − 2η(UH)HT O(md2) 2md2 +md 6: U−T ← U−T −\n(U−TH) ( (HTH − 12η Im)−1HT ) [ from Woodbury identity ] O(m2d+ m3 +md2) 2md2 +m+ 23m 3 +m2d (we count 23m 3\noperations for inversion of a m×m matrix)\n7: V ← V + 2ηY (U−TH)T where we must use the freshly updated U−T resulting from step 6)\nO(md2 + mKd)\nmd2 +mK +mKd\n8: Q← Q− η ( H∇TH +∇HHT ) +\n4η2(HM)HT O(md2 + m2d)\nm2d+ 3md2 + 2d2\nAltogether: O(md2) provided K < m < d D.\n≈ 10md2 + 3m2d+m3 elementary operations when K = 1\nNote that if we chose m > d we will not perform step 7 based on the Woodbury identity, which would be wasteful, but instead directly recompute the inverse of Unew in O(d3). The overall complexity remains O(md2) in this case also.\n4 Generalizing to a broader family of loss functions Let o = Wh the linear activations computed at the output layer. The approach that we detailed for linear output and squared error can be extended to a more general family of loss functions: basically any loss function ` that can be expressed using only the oc associated to non-zero yc together with q = ‖o‖2 = ∑ j o 2 j the squared norm of the\nwhole output vector, and optionally s = sum(o) = ∑ j oj which we will see that we can both compute cheaply. We call this family of loss functions the spherical family of loss functions or in short spherical losses, defined more formally as the family of losses that can be expressed as:\nL = `( ‖o‖2, sum(o), K, oK, yK)\nwhere K denotes the vector of indices of y of cardinality at most K D that is associated to non-zero elements of y in a sparse representation ofy; yK is the corresponding vector of values of y at positions K, i.e. yK = (y(K1), . . . , y(K|K|))T ; similarly oK is the vector of values of linear activation o at positions K, i.e. oK = (o(K1), . . . , o(K|K|))\nT . Note that the squared error loss belongs to this family as\n`squared =\nD∑\nj=1\n(oj − yj)2\n=\nD∑\nj=1\no2j − 2ojyj + y2j\n=\n  D∑\nj=1\no2j\n − 2   D∑\nj=1\nojyj\n +   D∑\nj=1\ny2j\n \n= ‖o‖2 − 2\n ∑\nj∈K ojyj\n +  ∑\nj∈K y2j\n  since for j /∈ K we have yj = 0\n= ‖o‖2 − 2oTKyK + ‖yK‖2 = `squared( ‖o‖2, sum(o), K, oK, yK)\nwhere `squared in particular doesn’t use sum(o). The spherical family of loss functions does not include the standard log of softmax, but it includes possible alternatives, such as the spherical softmax and Taylor-softmax that we will introduce in a later section. Let us detail the steps for computing such a spherical loss from last hidden layer representation h: • o = Wh • q = ‖o‖2 = ∑ o2i • s = sum(o) = ∑ oi • L = `(q, s, K, oK, yK)\nThe gradient of the loss may be backpropagated and the parameters updated in the usual naive way with the following steps: • compute scalars ∂`∂q (q, s, K, oK, yK) and ∂`∂s (q, s, K, oK, yK) as well asK-dimensional gradient vector ∂`∂oK (q, s, K, oK, yK)• clear D-dimensional gradient vector∇o ← 0 • update (∇o)K ← ∂`∂oK • update ∇o ← ∇o + ∂`∂q ∂q\n∂o︸︷︷︸ 2o\n• update ∇o ← ∇o + ∂`∂s ∂s\n∂o︸︷︷︸ 1D\n• backpropagate ∇h = WT∇o • update W ←W − η∇ohT where η is a scalar learning rate. Here again, as in the squared error case, we see that the computation of o in the forward pass and backpropagation of the gradient to ∇h would both require multiplication by the D × d matrix W , and that the update to W will generally be a non-sparse rank-1 update that requires modifying all itsDd elements. Each of these three operations have a O(Dd) complexity.\nWe will now follow the same logical steps as in the simpler squared error case to derive an efficient algorithm for the spherical loss family.\n4.1 Efficient computation of the loss Let us name the formal parameters of ` more clearly as follows:\n`(q, s,K,a, t) where q ands are scalars that will receive ‖o‖2 and sum(o) respectively; K is a vector that will contain the list of at mostK indices that correspond to non-zero elements of sparse y; a = oK and t = yK.\n4.1.1 Computing q = ‖o‖2\nq = ‖o‖2 = ‖ O(Dd)︷︸︸︷ Wh ‖2\n= (Wh) T (Wh)\n= hTWTWh\n= hT ( Qh︸︷︷︸ O(d2) ) (9)\nsupposing we have maintained an up-to date Q = WTW . Derivative:\n∂q ∂o = 2o\n4.1.2 Computing s = sum(o)\ns = sum(o) = sum( O(Dd)︷︸︸︷ Wh )\n=\nD∑\ni=1\n  d∑\nj=1\nhjWj   i\n=\nD∑\ni=1\nd∑\nj=1\nhjWij\n=\nd∑\nj=1\n( hj D∑\ni=1\nWij\n)\n=\nd∑\nj=1 hj sum(Wj)︸ ︷︷ ︸ w̄j\n= w̄Th\n= hT w̄ (10)\nThis is an O(d) operation, provided we have maintained an up-to-date vector w̄ = (sum(W1), . . . , sum(Wd)) = W T1D.\n∂s ∂o = 1D\n4.1.3 Computing specific ok\nWe will also need to compute the specific ok for the few k ∈ K.\nok = (Wh)k\n= hTWk•\nwhich gives\na = oK = (o(K1), . . . , o(K|K|)) T\n= (hTWK1•, . . . , h TWK|K|•) T (11)\nwe then have all we need to pass to loss function ` to compute the associated loss\nL = `(q, s, K, oK, yK) = `(q, s, K, a, t) (12)\n4.1.4 Corresponding equations for the minibatch case\nIn the minibatch case, rather than having the hidden representation of a single example as a vector h we suppose we receive m hidden representations in the m columns of a d ×m matrix H . The associated sparse target is D ×m matrix Y whose m columns contain each at mostK non-zero elements. Y will be stored using sparse representation (K, T ) whereK is now a K×m matrix of indices and T is a K×m matrix containing the corresponding values of Y such that Tkj = YKkj ,j for k ∈ {1, . . . ,K} and j ∈ {1, . . . ,m}.\nThe above equations given for the online case, can easily be adapted to the minibatch case as follows:\nLet O = WH the D ×m matrix of linear outputs whose jth column will contain the output vector of the jth example of the minibatch. The specific outputs associated to non-zero target values in Y (whose indexes are in K) will be collected in K × m matrix A (the minibatch version of vector a of Equation 11 such that\nAkj = OKkj ,j = (Hj) TWKkj• (13)\nAdapting Equation 9 to the minibatch case, the squared norm of the m output vectors is obtained in m-dimensional vector q as\nq = diag(HT QH︸︷︷︸ Ĥ︸ ︷︷ ︸\nM̂\n) (14)\nAdapting Equation 10 to the minibatch case, the sum of each of the m output vectors is obtained in m-dimensional vector s as\ns = HT w̄ (15)\nAdapting Equation 12 the corresponding vector of m individual losses for the m examples of the minibatch is\n~L = [`(qj , sj ,Kj,, Aj , Tj)]j=1...m (16)\nand the total loss for the minibatch is\nL = sum(~L) (17)\n4.2 Gradient of loss L with respect to h Online case:\nTo backpropagate the gradients through the network, we first need the gradients with respect to linear activations o: ∇o = ∂L∂o .\nThere will be three types of contributions to this gradient: contribution due to q, contribution due to s, and contribution due to direct influence on the loss of the ok for k ∈ K.\n∇o = ∂L\n∂o =\n∂`\n∂q\n∂q ∂o + ∂` ∂s ∂s ∂o +\nK∑\nk=1\n∂`\n∂ak\n∂ak ∂o\nWe have ∂q∂o = 2o, ∂s ∂o = 1D and ∂ak ∂o = onehotD(Kk) because ak = oKk so this\nbecomes\n∇o = 2o ∂`\n∂q + 1D\n∂` ∂s +\nK∑\nk=1\n∂`\n∂ak onehotD(Kk)\n= 2o ∂`\n∂q + 1D\n∂` ∂s + ẙ (18)\nwhere we have defined vector ẙ = ∑K k=1 ∂` ∂ak\nonehotD(Kk) as a sparse vector, having value at position kj equal ∂`∂aj . It will, like y, be stored in K − sparse representation, with the indexes given by k and the corresponding values in ∂`∂aj .\nGradient with respect to h:\n∇h = ∂o\n∂h\n∂L\n∂o\n= WT∇o = WT ( 2o ∂`\n∂q + 1D\n∂` ∂s + ẙ\n)\n= 2WT o ∂`\n∂q +WT1D\n∂` ∂s +WT ẙ\n= 2WTWh ∂`\n∂q + w̄\n∂` ∂s +WT ẙ\n= 2Qh ∂`\n∂q + w̄\n∂` ∂s +WT\nK∑\nk=1\n∂`\n∂ak onehotD(Kk)\n= 2Qh ∂`\n∂q + w̄\n∂` ∂s +\nK∑\nk=1\n∂`\n∂ak WT onehotD(Kk)\n= 2Qh ∂`\n∂q + w̄\n∂` ∂s +\nK∑\nk=1\n∂`\n∂ak WKk•\nMinibatch case:\nWe now consider a minibatch of m examples whose corresponding linear outputs are in a D ×m matrix O = WH . Let us also denote the vectors of gradients of the loss with respect to q and s as:\n∇q = [ ∂`\n∂q (qj , sj ,Kj,, Aj , Tj)\n]\nj=1...m\n∇s = [ ∂`\n∂s (qj , sj ,Kj,, Aj , Tj)\n]\nj=1...m\nLet us also define\n∇A = [ ∂`\n∂ak (qj , sj ,Kj,, Aj , Tj)\n]\nk=1...K, j=1...m\nand Y̊ as the sparse D ×m whose column j is defined as\nY̊j = K∑\nk=1\n∂`\n∂ak (qj , sj ,Kj,, Aj , Tj) onehotD(Kkj)\n=\nK∑\nk=1\n(∇A)kj onehotD(Kkj)\nwhich may be summarize asY̊Kj = (∇A)j Equation 18 then becomes in the minibatch case:\n∇Oj = 2Oj (∇q)j + 1D (∇s)j + Y̊j or in matrix form\n∇O = 2O diag(∇q) + 1D∇Ts + Y̊ (19) and the gradient with respect to H is:\n∇H = ∂L\n∂H = ∂O\n∂H\n∂L\n∂O\n= WT∇O (20) = WT ( 2O diag(∇q) + 1D∇Ts + Y̊ )\n= 2WTO diag(∇q) +WT1D∇Ts +WT Y̊ = 2WTWH diag(∇q) + w̄∇Ts +WT Y̊ = 2QH diag(∇q) + w̄∇Ts +WT Y̊︸ ︷︷ ︸\nẐ\n(21)\nwhere we define the d×m matrix Ẑ as Ẑ = w̄∇Ts +WT Y̊ (22)\n4.3 Standard naive gradient update of parameters W The gradient of the loss with respect to output layer weight matrix W is\n∂L\n∂W =\n∂L\n∂O\n∂O\n∂W\n= ∇OHT = ( 2O diag(∇q) + 1D∇Ts + Y̊ ) HT = ( 2WH diag(∇q) + 1D∇Ts + Y̊ ) HT\nAnd the corresponding gradient descent update to W would thus be\nWnew = W − η ( 2WH diag(∇q) + 1D∇Ts + Y̊ ) HT (23)\nwhere η is a positive learning rate. Computed in this manner, this induces a prohibitive O(mDd) computational complexity, first to compute WH , and then to update all the Dd elements of W . Note that all D × d elements of W must be accessed during this update. On the surface this seems hopeless. But we will see in the next section how we can achieve the exact same update of W in O(md2).\n4.4 Efficient gradient update of parameters using a factored representation of W\nFirst note that the update of W given in equation 23 can be decomposed in 3 consecutive updates:\na) W ← W − 2η(WH) diag(∇q)HT b) W ← W − η1D∇Ts HT c) W ← W − ηY̊ HT\nIn doing this we haven’t yet changed anything to the O(mDd) complexity of this update. Note that update a) can also be seen as W ←W ( I− 2ηH diag(∇q)HT ) .\nThe trick now is to represent W implicitly as3:\nW︸︷︷︸ D×d = V︸︷︷︸ D×d U︸︷︷︸ d×d\n+1Dω T (24)\nwhere ω is a d-dimensional vector. In this case the following updates to V,U, ω respectively will implicitly update the implicit W in the exact same way as the above 3 updates:\n3Note that we never actually factorize an arbitrary pre-exisitng W , which would be prohibitive as W is huge. We will no longer store or update a W , but onlyV, U, ω which implicitly represent W .\na) Unew = U ( I− 2ηH diag(∇q)HT )\n= U − 2ηUH diag(∇q)HT (25) b) ωnew = ( I− 2ηH diag(∇q)HT )T ω − ηH∇s\n= ω − 2ηH diag(∇q)HTω − ηH∇s = ω − ηH ( 2 diag(∇q)HTω +∇s ) (26)\nc) Vnew = V − ηY̊ (U−TnewH)T (27)\nBut, with this formulation, provided we keep an up-to-date U−T (which we will see we can do cheaply using the Woodbury identity), the whole update to V,U, ω is now O(md2) rather than the equivalent naive O(mDd) update of Eq. 23 to an explicit W .\nIndeed, step a) and b) involve only multiplications between matrices of dimensions d×m and d×d (matricesH andU ). As for step c) it involves anO(md2) multiplication of U−T by H , followed by a sparse update of V . Since Y̊ is an extremely sparse D ×m matrix whose m columns each contain at most K non-zero elements, update c) will touch at most Km rows of V , yielding an O(Kmd) operation. This is to be contrasted with the standard, equivalent but naive update of Eq. 23 to an explicit W , which requires accessing and modifying all D×d elements of W for every update and yields an overall O(mDd) computational complexity.\nProof that this sequence of updates yields the update of W given above:\nVnewUnew + 1Dω T new\n= ( V − ηY̊ (U−TnewH)T ) Unew + 1D ( ω − ηH ( 2 diag(∇q)HTω +∇s ))T = ( V − ηY̊ HTU−1new ) Unew + 1D ( ωT − η ( 2 diag(∇q)HTω +∇s )T HT ) = V Unew − ηY̊ HTU−1newUnew + 1DωT − η1D ( 2 diag(∇q)HTω +∇s )T HT = V Unew − ηY̊ HT + 1DωT − η1D ( 2 diag(∇q)HTω +∇s )T HT = V ( U − 2ηUH diag(∇q)HT ) − ηY̊ HT + 1DωT − η1D ( 2 diag(∇q)HTω +∇s )T HT = V U − 2ηV UH diag(∇q)HT − ηY̊ HT + 1DωT − η1D ( 2 ωTH diag(∇q) +∇Ts ) HT\n= (V U + 1Dω T )− 2ηV UH diag(∇q)HT − ηY̊ HT − η1D ( 2 ωTH diag(∇q) +∇Ts ) HT\n= W − 2ηV UH diag(∇q)HT − ηY̊ HT − 2η1DωTH diag(∇q)HT − η1D∇Ts HT = W − 2ηV UH diag(∇q)HT − 2η1DωTH diag(∇q)HT − η1D∇Ts HT − ηY̊ HT = W − 2η ( V UH diag(∇q)HT + 1DωTH diag(∇q)HT ) − η1D∇Ts HT − ηY̊ HT = W − 2η ( V U + 1Dω T ) H diag(∇q)HT − η1D∇Ts HT − ηY̊ HT\n= W − 2ηWH diag(∇q)HT − η1D∇Ts HT − ηY̊ HT\n= W − η ( 2WH diag(∇q) + 1D∇Ts + Y̊ ) HT\n= Wnew\n4.5 Adapting the computation of loss L and gradient ∇H to the factorized representation\nLet us now adapt the computation of loss L and gradient ∇H now that we no longer have an explicit W but rather store it implicitly as W = V U + 1DωT .\n4.5.1 Loss L\nComputing the total loss L over a minibatch implies computing L = sum(~L) = sum ( [`(qj , sj ,Kj,, Aj , Tj)]j=1...m ) as previously seen in Eq. 16 and Eq. 17. Index matrix K and associated target matrix T are the same as before. Vectors q and s can be computed cheaply as previously using Eq. 14 and 15 provided we have kept an up-to-date Q and w̄ (we shall see how to update them effectively in the next section). So to be able to compute loss L using this factored representation of W it remains only to adapt the computation of K × m matrix A. This matrix was defined in Eq. 13 as Akj = OKkj ,j = (Hj) TWKkj•. Replacing W by its factored expression we can write\nAkj = (Hj) T ( V U + 1Dω T ) Kkj•\n= (Hj) T (V U)Kkj• + (Hj)\nT ( 1Dω T ) Kkj•\n= (Hj) T (V U)Kkj• + (Hj) Tω = (Hj) T ( (V U)T ) Kkj + (Hj) Tω = (Hj) T ( UTV T ) Kkj + (Hj) Tω = (Hj) TUT ( V T ) Kkj + (Hj) Tω = (UHj) T ( V T ) Kkj + (Hj) Tω = ((UH)j) TVKkj• + (Hj) Tω\n= ((UH︸︷︷︸ H̃\n)j) TVKkj• + (H Tω︸ ︷︷ ︸ h̃ )j\nIn summary, having computed\nH̃ = UH (28)\nand\nh̃ = HTω (29)\nwe can efficiently compute the elements ofK×mmatrixA by accessing only the rows of V whose indexes are in Kas follows:\nAkj = (H̃j) TVKkj• + h̃j (30)\n4.5.2 Gradient ∇H Let us now adapt the computation of the gradient with respect to H , starting from previous Eq. 21 i.e.∇H = 2QH diag(∇q) + Ẑ with Ẑ = w̄∇Ts +WT Y̊ .\nSupposing we have kept an up-to-date Q and w̄ (we shall see how to update them effectively in the section 4.6), we are left with only adapting the computation of the WT Y̊ term to use the factored representation of W :\nẐ = w̄∇Ts +WT Y̊ = w̄∇Ts + ( V U + 1Dω T )T Y̊\n= w̄∇Ts + UTV T Y̊ + ω1TDY̊ = w̄∇Ts + UT (V T Y̊ ) + ω(Y̊ T1D)T = w̄∇Ts + UT (V T Y̊ ) + ωȳT (31)\nprovided we defined\nȳ = Y̊ T1D = rowsum(Y̊ ) = rowsum(∇A) (32)\nWe see that computing d × m matrix Ẑ in this manner can be achieved efficiently using our factored representation V,U and ω. Note that computing V T Y̊ is a multiplication by sparse matrix Y̊ which will have a computational complexity of O(Kdm), and yield a d × m matrix. The computation of Ẑ in this manner thus has aO(dm+ d2m+Kdm+ dm) complexity.\nWe can then proceed to computing∇H as in Eq. 21:\n∇H = 2QH︸︷︷︸ Ĥ diag(∇q) + Ẑ (33)\n4.6 Bookkeeping operations: keeping up-to-date w̄ and Q We have shown in section 4.4 that our updates to V,U, ω (Eq. 27,25,26) achieve the same update on (an implicit)W as Eq. 23, i.e. Wnew = W−η ( 2WH diag(∇q) + 1D∇Ts + Y̊ ) HT . The efficient computation of loss L and gradient∇H seen in Section 4.5 relies on having an up-to-dateQ = WTW and w̄ = rowsum(W ) = (sum(W1), . . . , sum(Wd)) = WT1D. In this section, we derive efficient updates to w̄ and Q that reflect the update to W .\n4.6.1 Update of w̄\nw̄new = W T new1D\n= ( W − η ( 2WH diag(∇q) + 1D∇Ts + Y̊ ) HT )T 1D = WT1D − ηH ( 2WH diag(∇q) + 1D∇Ts + Y̊ )T 1D\n= w̄ − ηH ( 2diag(∇q)HTWT +∇s1TD + Y̊ T ) 1D\n= w̄ − 2ηHdiag(∇q)HTWT1D − ηH∇s1TD1D − ηH Y̊ T1D︸ ︷︷ ︸ ȳ\n= w̄ − 2ηHdiag(∇q)HT w̄ − ηDH∇s − ηHȳ = w̄ − ηH ( 2diag(∇q)HT w̄ − ηD∇s − ηȳ ) (34)\n4.6.2 Update of Q\nQnew = W T newWnew\n= ( W − η∇OHT )T ( W − η∇OHT ) = WTW −WT ( η∇OHT ) − ( η∇OHT )T W + η2 ( ∇OHT )T ∇OHT\n= WTW︸ ︷︷ ︸ Q −ηWT∇O︸ ︷︷ ︸ ∇H HT − η(WT∇O︸ ︷︷ ︸ ∇H HT )T + η2H∇TO∇OHT\nQnew = Q− η ( ∇HHT ) − η ( ∇HHT )T + η2H(∇TO∇O︸ ︷︷ ︸\nM\n)HT (35)\nwhere we used the fact that WTW = Q and∇H = WT∇O. Note that while computing ∇OHT would be a prohibitive O(mDd) computation (in addition to requiring to explicitly compute ∇O in the first place), computing ∇HHT is a comparatively cheap O(md2) operation.\nIt remains to derive a way to efficiently compute m × m matrix M = ∇TO∇O without explicitly computing O nor resorting to explicit W . Substituting ∇O by its expression from Eq. 19 i.e. ∇O = 2O diag(∇q) + 1D∇Ts + Y̊ yields\nM = ∇TO∇O M = ( 2O diag(∇q) + 1D∇Ts + Y̊ )T ( 2O diag(∇q) + 1D∇Ts + Y̊ )\nM = ( (2O diag(∇q))T + ( 1D∇Ts + Y̊ )T)( (2O diag(∇q)) + ( 1D∇Ts + Y̊ )) M = (2O diag(∇q))T (2O diag(∇q)) + ( 1D∇Ts + Y̊ )T ( 1D∇Ts + Y̊ )\n+ (2O diag(∇q))T ( 1D∇Ts + Y̊ ) + ( 1D∇Ts + Y̊ )T (2O diag(∇q))\nM = ( 4diag(∇q)OTO diag(∇q) ) + ( ∇s1TD1D∇Ts + Y̊ T Y̊ +∇s1TDY̊ + Y̊ T1D∇Ts )\n+ (2O diag(∇q))T ( 1D∇Ts + Y̊ ) + ( 1D∇Ts + Y̊ )T (2O diag(∇q))\nM = 4diag(∇q)OTO diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+ (2O diag(∇q))T ( ∇s1TD + Y̊ T )T + ( ∇s1TD + Y̊ T ) (2O diag(∇q))\nM = 4diag(∇q)OTO diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+ (( ∇s1TD + Y̊ T ) (2O diag(∇q)) )T + (( ∇s1TD + Y̊ T ) (2O diag(∇q)) )\nM = 4diag(∇q)OTO diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+ ( 2∇s1TDO diag(∇q) + 2Y̊ TO diag(∇q) )T + ( 2∇s1TDO diag(∇q) + 2Y̊ TO diag(∇q) )\nSinceO = WH we haveOTO = HTWTWH = HTQH and 1TDO = 1 T DWH =\nw̄TH . Substituting these in the above expression of M we obtain\nM = 4diag(∇q) HTQH︷ ︸︸ ︷ OTO diag(∇q) + (D∇s∇Ts + M̊︷ ︸︸ ︷ Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+(2∇s 1TDO︸ ︷︷ ︸ w̄TH diag(∇q) + 2Y̊ T O︸︷︷︸ WH diag(∇q))T + (2∇s 1TDO︸ ︷︷ ︸ w̄TH diag(∇q) + 2Y̊ T O︸︷︷︸ WH diag(∇q))\nM = 4diag(∇q)HTQH diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+2 ( ∇sw̄TH diag(∇q) + Y̊ TWH diag(∇q) )T + 2 ( ∇sw̄TH diag(∇q) + Y̊ TWH diag(∇q) )\nM = 4diag(∇q)HTQH diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+2 (( ∇sw̄T + Y̊ TW ) H diag(∇q) )T + 2 (( ∇sw̄T + Y̊ TW ) H diag(∇q) )\nM = 4diag(∇q)HTQH diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+2(diag(∇q)HT (w̄∇Ts +WT Y̊ )︸ ︷︷ ︸ Ẑ +2(diag(∇q)HT (w̄∇Ts +WT Y̊ )︸ ︷︷ ︸ Ẑ )T .\nReusing previously defined Ẑ = w̄∇Ts +WT Y̊ that were already part of the computation of ∇H (see Eq. 33 in section 4.5.2), we can thus compute M efficiently as\nM = 4diag(∇q) M̂︷ ︸︸ ︷ HTQH diag(∇q) + ( D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts )\n+2 ( diag(∇q)HT Ẑ ) + 2 ( diag(∇q)HT Ẑ )T\n(36)\nNote that computing M requires computing Y̊ T Y̊ , a m×m matrix, each element of which is the dot product between two K − sparse columns of sparse matrix Y̊ so that it can be computed in O(m2K).\nHaving M we can then update Q using Eq. 35.\n4.7 Bookkeeping operations: tracking U−T\nWe can updateU−T to reflect our rank-m update of U in step a), using the Woodbury identity.\n4.8 Putting it all together In this section, we put together all the operations that we have derived to write the minibatch version of the update algorithm for general spherical losses.\nThe parameters of the output layer that we will learn are V,U, ω and implicitly represent W as W = V U + 1DωT .\nThe algorithm will work for any spherical loss function ` in canonical form that computes `(q, s,K,,a, t) and for which we can compute gradients with respect to its parameters.\nInitialization • we can initialize D × d matrix V randomly as we would have initialized W so that\nwe initially have V = W . Alternatively we can initialize V to 0 (there won’t be symmetry breaking issues with having W initially be 0 provided the other layers are initialized randomly, since varying inputs and targets will naturally break symmetry for the output layer) • we initialize U to the identity: U ← Id • and ω to zero ω ← 0d so that, trivially, we initially have V U + 1DωT = W . • initialize U−T ← Id • initialize Q ← WTW = V TV (or more cheaply initialize Q ← 0 if we have\ninitialized V to 0). • initialize w̄ = WT1D = rowsum(W ) = rowsum(V ) (or more cheaply w̄ ← 0 if\nwe have initialized V to 0).\nMinibatch update algorithm for arbitrary spherical loss Inputs (besides above parameters V,U, ω and bookkeeping variables Q,U−T , w̄): • H : a d ×m matrix whose m columns contain the last hidden layer representation\nvectors form example (with an appended constant 1 element to account for an output bias). • Y : a D × m sparse target matrix that uses sparse representation (K, T ) so that YKkj ,j = Tkj for k ∈ {1, . . . ,K} and j ∈ {1, . . . ,m}. Each of the m columns of Y is the K-sparse target vector associated to one example of the minibatch. • η ∈ R+learning rate for the update Updates: • parameters and bookkeeping matrices U, V, ω,Q,U−T , w̄ Returns: • L ∈ R the sum of squared error losses for the m examples of the minibatch • ∇H a d ×m matrix whose m columns contain the gradient of the loss with respect\nto H , to further backpropagate upstream.\nThe detailed algorithm is given as Algorithm 4 Counting the total number of basic operations of the update algorithm yields roughly 8md2 +m3 + 7m2d+ 2mKd+ 3d2 ≈ 17md2 operations. Comparing this17md2 to the 3Dm of the naive update, the expected theoretical speedup is approximately 3D18d = 1 6 D d\nFor d = 512 and D = 793471 this yields a theoretical speedup of 258 Note that in the special cases where the specific loss function ` does not depend on the sum of outputs s (as is the case e.g. of the squared error) then we don’t need to compute s, and can use aω that is always 0 so there’s a lot we don’t need to compute and update.\n5 Controlling numerical stability The update of U may over time lead to U becoming ill-conditioned. Simultaneously, as we update U and U−T (using Sherman-Morrison or Woodbury) our updated U−Tmay over time start to diverge from the true U−T due to numerical precision. It is thus important to prevent both of these form happening, i.e. make sure U stays well conditioned, to ensure the numerical stability of the algorithm. We present here progressively refined strategies for achieving this.\n5.1 Restoring the system in a pristine stable state One simple way to ensure numerical stability is to once in a while restore the system in its pristine state where V = W and U = Id = U−T . This is easily achieved as follows:\nV ← V U U ← Id\nU−T ← Id.\nThis operation doesn’t affects the product V U , so the implicit matrix W remains unchanged, nor does it affect Q = WTW . And it does restore U to a perfectly well conditioned identity matrix. But computing V U is an extremely costly O(Dd2) operation, so if possible we want to avoid it (except maybe once at the very end of training, if we want to compute the actual W ). In the next paragraphs we develop a more efficient strategy.\n5.2 Stabilizing only problematic singular values U becoming ill-conditioned is due to its singular values over time becoming too large and/or too small. Let use define σ1, . . . , σd as the singular values of U ordered in decreasing order. The conditioning number of U is defined as σ1σd and it can become overly large when σ1 becomes too large and/or when σd becomes too small. Restoring\nAlgorithm 4 Minibatch version of the update algorithm for general spherical loss FUNCTION spherical_minibatch_fbprop_update:\nInputs: hidden layer minibatch︷︸︸︷ H , sparse target︷︸︸︷ K, T , learning rate︷︸︸︷ η , layer parameters︷ ︸︸ ︷ V,U, ω , bookkeeping variables︷ ︸︸ ︷ Q, w̄, U−T Updates: V,U, ω,Q, w̄, U−T Returns: loss L, gradient∇H to backpropagate further upstream Operations main\ntext Eq. result dims\n# ops\nĤ = QH Eq. 14 d×m md2 M̂ = HT Ĥ Eq. 14 m×m q = diag(M̂ ) Eq. 14 m m\ns = HT w̄ Eq. 15 m md H̃ = UH Eq. 28 d×m md2 h̃ = HTω Eq. 29 m md MatrixA: Akj = (H̃j)TVKkj• + h̃j Eq. 30 K ×m mKd ~L = [`(qj , sj ,Kj,, Aj , Tj)]j=1...m Eq. 16 m typically\nO(Km)\nL = sum(~L) 1 m ∇q = [ ∂` ∂q (qj , sj ,Kj,, Aj , Tj) ] j=1...m m\n∇s = [ ∂` ∂s (qj , sj ,Kj,, Aj , Tj) ] j=1...m\nm\n∇A =[ ∂` ∂ak (qj , sj ,Kj,, Aj , Tj) ] k=1...K, j=1...m\nK ×m\nY̊ = sparsematD,m(K,∇A) D ×m (Ksparse) ȳ = Y̊ T1D = rowsum(∇A) Eq. 32 m Km Ẑ = w̄∇Ts + UT (V T Y̊ ) + ωȳT Eq. 31 d×m md ∇H = 2Ĥ diag(∇q) + Ẑ Eq. 33 d×m md U ← U − 2η(UH︸︷︷︸\nH̃\n) diag(∇q)HT Eq. 25 d× d md2\nU−T ← ... use Woodbury Identity to update it. d× d 2m2d+ m3 +\n2md2\nω ← ω − ηH(2diag(∇q)HTω︸ ︷︷ ︸ h̃ +∇s) Eq. 26 d 2md+3d V ← V − ηY̊ (U−TnewH)T Eq. 27 D × d md2 + mKd\nw̄ ← w̄ − ηH ( 2diag(∇q)HT w̄ +D∇s + ȳ ) Eq. 34 d 2md+4d M = 4diag(∇q)M̂ diag(∇q) +D∇s∇Ts + Y̊ T Y̊ +∇sȳT + ȳ∇Ts +2 ( diag(∇q)HT Ẑ ) +2 ( diag(∇q)HT Ẑ )T Eq. 36 m×m 2m2d+ (5 + K)m2 +\nd2\nQ← Q− η∇HHT − ηH∇TH + η2(HM)HT Eq. 35 d× d md2 + 2m2d+\n2d2\nRETURN L,∇H 28\nthe system in its pristine state, as shown in the previous paragraph, in effect brings back all singular values of U back to 1 (since it brings back U to being the identity). It is instead possible, and computationally far less costly, to correct when needed only for the singular values of U that fall outside a safe range. Most often we will only need to occasionally correct for one singular value (usually the smallest, and only when it becomes too small). Once we have determined the offending singular value and its corresponding singular vectors, correcting for that singular value, i.e. effectively bringing it back to 1, will be a O(Dd) operation. The point is to apply corrective steps only on the problematic singular values and only when needed, rather than blindly, needlessly and inefficiently correcting for all of them through the basic O(Dd2) full restoration explained in the previous paragraph.\nHere is the detailed algorithm that achieves this:\nAlgorithm 5 Numerical stabilization procedure for problematic singular values • The chosen safe range for singular values is [σlow, σhigh] (ex: [0.001, 100] ) • The procedures given below act on output layer parameters U , U−T and V . • For concision, we do not enlist these parameters explicitly in their parameter list. • Procedure SINGULAR-STABILIZE gets called after every ncheck gradient updates (ex: ncheck = 100). procedure SINGULAR-STABILIZE( )\nŪ, σ, V̄ = SVD(U ) . Computes singular value decomposition of U as U = Ū diag(σ) V̄T\nfor all k ∈ {1, . . . , d} do if σk < σlow OR σk > σhigh then\nFIX-SINGULAR-VALUE(σk, Ūk, 1) end if\nend for end procedure\nThe following procedure will change singular value σ of U associated to singular vector u to become target singular value σ∗ (typically 1). It doesn’t change U ’s singular vectors, only that one singular value. It also changes V symetrically (with a rank-one update) in such a way that W = V U remains unchanged.\nprocedure FIX-SINGULAR-VALUE(σ, u, σ∗) α = σ\n∗−σ σ\nβ = − α1+α U ← U + αu(UTu)T V ← V + β(V u)uT U−T ← U−T + βu(U−1u)T . Where U−1 is obtained as the transpose of U−T . But we may instead of this prefer to recompute U−T from scratch by inverting U to ensure it doesn’t stray too much due to numerical imprecisions. end procedure\nProof that W = V U is left unchanged by FIX-SINGULAR-VALUE\nVnewUnew = (V + β(V u)u T ) (U + αu(UTu)T )\n= V (Id + βuu T ) (U + αuuTU) = V (Id + βuu T ) (Id + αuu T )U = V (I2d + βuu T + αuuT + βαuuTuuT )U = V (I2d + (α+ β)uu T + βαu(uTu)uT )U = V (Id + (α+ β)uu T + βαuuT )U = V (Id + (α− α\n1 + α + α −α 1 + α )uuT )U\n= V (Id + (α− α 1 + α − α\n2\n1 + α )uuT )U\n= V (Id + (α− α+ α2\n1 + α )uuT )U\n= V (Id + (α− α(1 + α)\n1 + α )uuT )U\n= V (Id + (α− α)uuT )U = V IdU\n= V U\n5.3 Avoiding the cost of a full singular-value decomposition Computing the SVD of d× d matrix U as required above, costs roughly 25d3 elementary operations (use the so-called R-SVD algorithm). But since the offending singular values will typically be only the smallest or the largest, it is wasteful to compute all d singular values every time. A possibly cheaper alternative is to use the power iteration method with U to find its largest singular value and associated singular vector, and similarly with U−1to obtain the smallest singular value of U (which corresponds to the inverse of the largest singular value of U−1). Each iteration of the power iteration method requires only O(d2) operations, and a few iterations may suffice. In our experiments we fixed it to 100 power iterations. Also it is probably not critical if the power iteration method is not run fully to convergence, as correcting along an approximate offending singular vector direction may be sufficient for the purpose of ensuring numerical stability.\nWith this refinement, we loop over finding the smallest singular value with the power iteration method, correcting for it to be 1 by calling FIX-SINGULAR-VALUE if it is too small, and we repeat this until we find the now smallest singular value to be inside the acceptable range. Similarly for the largest singular values.\nNote that while in principle we may not need to ever invert U from scratch (as we provided update formulas of U−T with every change we make to U ), it nevertheless proved to be necessary to do so regularly to ensure U−T doesn’t stray too much from the correct value due to numerical imprecisions. Inverting U using Gaussianelimination costs roughly d3 operations, so it is very reasonable and won’t affect the\ncomputational complexity if we do it no more often than every d training examples (which will typically correspond to less than 10 minibatches of size 128). In practice, we recompute U−T from scratch every time before we run this check for singular value stabilization.\n6 Experimental validation We implemented both a CPU version using blas and a parallel GPU (Cuda) version using cublas of the proposed algorithm4. We evaluated the GPU and CPU implementations by training word embeddings with simple neural language models, in which a probability map of the next word given its preceding n-gram is learned by a neural network. We used a Nvidia Titan Black GPU and a i7-4820K @ 3.70GHz CPU and ran experiments on the one billion word dataset[? ], which is composed of 0.8 billions words belonging to a vocabulary of 0.8 millions words. We evaluated the resulting word embeddings with the recently introduced Simlex-999 score [? ], which measures the similarity between words. We also compared our approach to unfactorised versions and to a two-layer hierarchical softmax. Figure 2 and 3 (left) illustrate the practical speedup of our approach for the output layer only. Figure 3(right) shows that the LST (Large Sparse Target) models are much faster to train than the softmax models and converge to only slightly lower Simlex-999 scores. Table 1 summarizes the speedups for the different output layers we tried, both on CPU and GPU. We also emprically verified that our proposed factored algorithm learns the exact same model weights (V U) as the corresponding naive unfactored algorithm’s W , as it theoretically should (up to negligible numerical precision differences), and followed the exact same learning curves (as a function of number of iterations, not time!).\n7 Conclusion and future work We introduced a new algorithmic approach to efficiently compute the exact gradient updates for training deep networks with very large sparse targets. Remarkably the complexity of the algorithm is independent of the target size, which allows tackling\n4Open source code will be released upon official publication of this research.\nvery large problems. Our CPU and GPU implementation yield similar speedups to the theoretical one and can thus be used in practical applications, which could be explored in further work. In particular, neural language models seem good candidates. But it remains unclear how using a loss function other than log-softmax may affect the quality of the resulting word embeddings and further research should be carried out in this direction. While restricted, the spherical family of loss functions, offers opportunities to explore alternatives to the ubiquitous softmax, that thanks to the algorithm presented here, could scale computationally to extremely large output spaces.\nAcknowledgements We would like to thank the developers of Theano [14, 15] and Blocks [16].\nThis research is supported by NSERC and Ubisoft.\nReferences [1] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language\nmodel. In NIPS’00, pages 932–938. MIT Press, 2001.\n[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12: 2493–2537, 2011.\n[3] Y. Dauphin, X. Glorot, and Y. Bengio. Large-scale learning of embeddings with reconstruction sampling. In Proceedings of the 28th International Conference on Machine learning, ICML ’11, 2011.\n[4] Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In ACL-IJCNLP’2015, 2015. arXiv:1412.2007.\n[5] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS’10), 2010.\n[6] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noisecontrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265– 2273. Curran Associates, Inc., 2013.\n[7] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS’2013, pages 3111–3119. 2013.\n[8] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2321–2329. Curran Associates, Inc., 2014.\n[9] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large output spaces. arxiv:1412.7479, 2014.\n[10] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 246–252. Society for Artificial Intelligence and Statistics, 2005.\n[11] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning representations by backpropagating errors. Nature, 323:533–536, 1986.\n[12] Yann LeCun. Une procédure d’apprentissage pour Réseau à seuil assymétrique. In Cognitiva 85: A la Frontière de l’Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences, pages 599–604, Paris 1985, 1985. CESTA, Paris.\n[13] Yann LeCun. Learning processes in an asymmetric threshold network. In E. Bienenstock, F. Fogelman-Soulié, and G. Weisbuch, editors, Disordered Systems and Biological Organization, pages 233–240. Springer-Verlag, Berlin, Les Houches 1985, 1986.\n[14] James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), 2010. Oral Presentation.\n[15] Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.\n[16] B. van Merriënboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-Farley, J. Chorowski, and Y. Bengio. Blocks and Fuel: Frameworks for deep learning. ArXiv e-prints, jun 2015."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent" ],
      "venue" : "In NIPS’00,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Large-scale learning of embeddings with reconstruction sampling",
      "author" : [ "Y. Dauphin", "X. Glorot", "Y. Bengio" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "In ACL-IJCNLP’2015,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "M. Gutmann", "A. Hyvarinen" ],
      "venue" : "In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS’10),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Learning word embeddings efficiently with noisecontrastive estimation",
      "author" : [ "Andriy Mnih", "Koray Kavukcuoglu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "In NIPS’2013,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)",
      "author" : [ "Anshumali Shrivastava", "Ping Li" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Deep networks with large output spaces",
      "author" : [ "Sudheendra Vijayanarasimhan", "Jonathon Shlens", "Rajat Monga", "Jay Yagnik" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Frederic Morin", "Yoshua Bengio" ],
      "venue" : "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Learning representations by backpropagating",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1986
    }, {
      "title" : "Une procédure d’apprentissage pour Réseau à seuil assymétrique",
      "author" : [ "Yann LeCun" ],
      "venue" : "In Cognitiva 85: A la Frontière de l’Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1985
    }, {
      "title" : "Learning processes in an asymmetric threshold network",
      "author" : [ "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1985
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "James Bergstra", "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Python for Scientific Computing Conference (SciPy),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Blocks and Fuel: Frameworks for deep learning",
      "author" : [ "B. van Merriënboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2].",
      "startOffset" : 264,
      "endOffset" : 267
    }, {
      "referenceID" : 2,
      "context" : "[3], the efficient use of biased importance sampling in Jean et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "[7] all fall under this category.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "• Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "• Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "[11], LeCun [12, 13] to efficiently compute the gradients.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[11], LeCun [12, 13] to efficiently compute the gradients.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "[11], LeCun [12, 13] to efficiently compute the gradients.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "in Neural Language Models [1] with large vocabulary size (e.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "‣ Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "‣ Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "[1] Bengio, Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Dauphin, Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Mnih, A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Morin, F.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Gutmann, M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Mikolov, T.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "We would like to thank the developers of Theano [14, 15] and Blocks [16].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "We would like to thank the developers of Theano [14, 15] and Blocks [16].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "We would like to thank the developers of Theano [14, 15] and Blocks [16].",
      "startOffset" : 68,
      "endOffset" : 72
    } ],
    "year" : 2016,
    "abstractText" : "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of sizeD (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating theD× d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d) per example instead of O(Dd), remarkably without ever computing theD-dimensional output. The proposed algorithm yields a speedup of D 4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.",
    "creator" : "LaTeX with hyperref package"
  }
}