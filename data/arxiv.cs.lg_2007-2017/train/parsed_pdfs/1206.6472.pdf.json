{
  "name" : "1206.6472.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An Efficient Approach to Sparse Linear Discriminant Analysis",
    "authors" : [ "Luis Francisco Sánchez", "Yves Grandvalet" ],
    "emails" : [ "luis-francisco.sanchez@utc.fr", "yves.grandvalet@utc.fr", "gerard.govaert@utc.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Linear Discriminant Analysis (LDA) aims at finding the “best” separation of a set of observations into known classes. It is used for two main purposes: to classify future observations or to describe the essential differences between classes, either by providing a visual representation of data, or by revealing the combinations of features that discriminate between classes. An additional widely employed practice is to extract features by LDA, which is then used as a simple dimensionality reduction method taking into account the discriminant information.\nLDA originates from the analysis of the within and be-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntween groups variances (Fisher, 1936), leading to the maximization of a separability criterion. It can be related to the estimation of the conditional probabilities of classes given the observations (without modelling their distribution), via a connection with the regression of class indicators. Finally, it can also be derived as a plug-in classifier under the assumption of normal distributions for the classes, with different means but a common variance matrix.\nSparse LDA refers here to formulations revealing discriminant directions that only involve a few variables. Besides cases where the sparsity of the true discriminants is assumed, such as most genetic analyses, sparse classification methods may be motivated by interpretability, robustness of the solution, or computational restraints for evaluation in prediction.\nThe most common approach to sparse LDA consists in performing variable selection in a separate step, before classification. Variable selection is then usually based on univariate statistics, which are fast and convenient to compute, but whose very partial view of the overall classification problem may lead to dramatic information loss. As a result, several approaches have been devised in the recent years to construct LDA with embedded feature selection capabilities.\nOur Group-Lasso Optimal Scoring Solver (GLOSS) addresses a sparse LDA problem globally, through the regression approach of LDA. Our analysis formally relates GLOSS to Fisher’s discriminant analysis, and also enables to derive variants, such that LDA assuming diagonal within-class covariance structure (Bickel & Levina, 2004). The group-Lasso selects the same features in all discriminant directions, which provide a more interpretable low-dimensional representation of data. Compared to the competing approaches, the models are extremely parsimonious without compromising prediction performances. Our algorithm efficiently processes medium to large number of variables, and is thus particularly well suited to the analysis of gene expression data.\nThis paper is organized as follows. Section 2 introduces the basic notations that are necessary for stating Fisher’s discriminant problem. Section 3 reviews the main approaches that have been followed to perform sparse LDA via regression. We then derive a connection between sparse optimal scoring and sparse LDA in Section 4. The GLOSS algorithm is described in Section 5 and experimental results follow in Section 6, before our final concluding remarks of Section 7."
    }, {
      "heading" : "2. Primary Notations and Definitions",
      "text" : "Vectors are denoted by lowercase letters in bold font and matrices by uppercase letters in bold font. Unless otherwise stated, vectors are column vectors and parentheses are used to build line vectors from commaseparated lists of scalars, or to build matrices from comma-separated lists of column vectors.\nThe data consist of a set of n labeled examples, with observations xi ∈ Rp comprising p features, and label yi ∈ {0, 1}K indicating the exclusive assignment of observation xi to one of the K classes. It will be convenient to gather the observations in the n×p matrix X = (xᵀ1 , . . . ,x ᵀ n) ᵀ\nand the corresponding labels in the n×K matrix Y = (yᵀ1 , . . . ,yᵀn) ᵀ .\nThe two main components of LDA are the within-class and between-class covariance matrices. We consider centered observations, so that the global sample mean is null, and S = n−1XᵀX is the sample covariance matrix. The cardinality of class k in the training sample is nk, and the sample mean for class k is µ̂k. The between-class sample covariance matrix is\nSb = 1\nn K∑ k=1 nk µ̂kµ̂ ᵀ k\n= n−1XᵀPYX ,\nwhere PY = Y (Y ᵀY)\n−1 Yᵀ projects onto the span of\nY. The within-class sample covariance matrix is\nSw = 1\nn K∑ k=1 ∑ {i:yik=1} (xi − µ̂k) (xi − µ̂k) ᵀ\n= n−1Xᵀ (In −PY) X , (1)\nwhere In is the identity matrix of size n.\nThe Fisher discriminant was originally defined in binary classification, as the linear projection that “best” separates the observations from each class, that is, maximizing the between-class variance relative to the within-class variance in the considered projection. This objective may also be stated as the maximization\nof between-class variance subject to unitary withinclass variance, so as to define a unique maximizer.\nWhen considering several projections, there are several equivalent formulations of LDA, which may differ once a penalization scheme is applied. Here, we use a definition based on subspace projection, where the discriminant directions maximally separate the class means subject to orthonormal constraints:\nBLDA = argmax B∈Rp×M\ntr(BᵀSbB)\ns. t. BᵀSwB = IM , (2)\nwhere BLDA = (β1, . . . ,βM ) gathers the M ≤ K − 1 discriminant directions, and tr(·) is the trace operator. This definition was chosen here for its succinctness, but it is incomplete with regard to the usual textbook LDA procedure where the discriminant directions are computed iteratively so as to maximally separate the class means subject to orthonormality constraints: here, the discriminant directions are defined up to an orthonormal transformation. Note that this is a notational problem and that we can recover such an ordering in the GLOSS procedure proposed here."
    }, {
      "heading" : "3. Sparse LDA",
      "text" : "LDA is often used as a data reduction technique, where the K − 1 discriminant directions summarize the p original variables. However, all variables intervene in the definition of these discriminant directions, and this trait may be troublesome in some applications.\nSeveral modifications of LDA have been proposed to generate sparse discriminant directions. They can be categorized according to the LDA formulation that provides the basis to the sparsity inducing extension, that is, either Fisher’s Discriminant Analysis (variance-based), Gaussian mixture model (based on the assumption of the normality of classes) or regression-based. We briefly review here the approaches belonging to the last category to introduce the method proposed in this paper.\nFisher (1936) introduced what is now known as Fisher’s linear discriminant analysis in his analysis of the famous iris dataset, and discussed its analogy with the linear regression of the scaled class indicators. This route was further developed, for more than two classes, by Breiman & Ihaka (1984) as an inspiration for a non-linear extension of discriminant analysis using additive models. They named their approach optimal scaling, for it optimizes the scaling of the indicators of classes together with the discriminant functions. Their approach later disseminated under the name optimal scoring (OS) by Hastie et al. (1994), who proposed\nseveral extensions of LDA, either aiming at constructing more flexible discriminants (Hastie & Tibshirani, 1996) or more conservative ones (Hastie et al., 1995).\nSeveral sparse LDA have been derived using sparsityinducing penalties on the OS regression problem (Leng, 2008; Grosenick et al., 2008; Clemmensen et al., 2011). These proposals are motivated by the equivalence between penalized OS and penalized LDA, but since sparsity-inducing penalties are non-quadratic, they fall beyond the realm of the equivalence stated by Hastie et al. (1995). Until now, no rigorous link was derived between sparse OS and sparse LDA.\nIn this paper, we demonstrate that the equivalence between penalized OS and penalized LDA is preserved for the non-quadratic Lasso penalty for binary classification. However, the connection collapses in the general multi-class setting. We thus propose GLOSS, based on the group-Lasso, that preserves the equivalence between sparse OS and sparse LDA in the general multi-class situation."
    }, {
      "heading" : "4. From Sparse OS to Sparse LDA",
      "text" : "We relate here a sparse OS problem, penalized by the group-Lasso, with a sparse LDA problem. Our derivation uses a variational formulation of the group-Lasso to generalize the equivalence drawn by Hastie et al. (1995) for quadratic penalties."
    }, {
      "heading" : "4.1. A Variational Form of the Group-Lasso",
      "text" : "Quadratic variational forms of the Lasso and groupLasso have been proposed shortly after the original Lasso paper of Tibshirani (1996), as a means to address optimization issues, but also as an inspiration for generalizing the Lasso penalty (Grandvalet, 1998; Grandvalet & Canu, 1999). The algorithms based on these quadratic variational forms iteratively reweight a quadratic penalty. They are now often outperformed by more efficient strategies (Bach et al., 2012)."
    }, {
      "heading" : "4.1.1. A Quadratic Variational Form",
      "text" : "We now present a handy convex quadratic variational form of the group-Lasso. Let B ∈ Rp×M be a matrix composed of row vectors βj ∈ RM , B = ( β1 ᵀ , . . . ,βp ᵀ)ᵀ .We consider the following problem:\nmin τ∈Rp min B∈Rp×M J(B) + λ p∑ j=1 w2j\n∥∥βj∥∥2 2\nτj (3a)\ns. t. ∑ j τj − ∑ j wj ∥∥βj∥∥ 2 ≤ 0 (3b)\nτj ≥ 0 , j = 1, . . . , p . (3c)\nwhere wj are predefined weights. Here and in what follows, b/τ is defined by continuation at zero as b/0 = ∞ if b 6= 0 and 0/0 = 0. Note that variants of (3) have been proposed elsewhere (see e.g. Grandvalet & Canu, 1999; Bach et al., 2012, and references therein).\nLemma 1. The quadratic penalty in βj in (3) acts as the group-Lasso penalty λ ∑p j=1 wj ∥∥βj∥∥ 2 .\nThis equivalence is crucial to the derivation of the link between sparse OS and sparse LDA; it furthermore suggests a convenient implementation. We sketch below some properties that are instrumental in the implementation of the active-set described in Section 5."
    }, {
      "heading" : "4.1.2. Useful Properties",
      "text" : "The first property states that the quadratic formulation is convex when J is convex, thus providing an easy control of optimality and convergence.\nLemma 2. If J is convex, Problem (3) is convex.\nIn what follows, J will be a convex quadratic (hence smooth) function, in which case a necessary and sufficient optimality condition is that zero belongs to the subdifferential of the objective function. This condition results in an equality for the “active” non-zero vectors βj , and an inequality for the other ones, which both provide essential building blocks of our algorithm.\nLemma 3. Problem (3) admits at least one solution, which is unique if J is strictly convex. All critical points B of the objective function verifying the following conditions are global minima.\n∀j ∈ S , ∂J(B) ∂βj\n+ λwj ∥∥βj∥∥−1\n2 βj = 0 , (4a)\n∀j ∈ Sc , ∥∥∥∥∂J(B)∂βj ∥∥∥∥ 2 ≤ λ . (4b)\nwhere S ⊆ {1, . . . , p} denotes the set of non-zero row vectors βj and Sc(B) is its complement.\nLemma 3 provides a simple appraisal of the support of the solution, which would not be as easily handled with the direct analysis of the variational problem (3)."
    }, {
      "heading" : "4.2. Penalized Optimal Scoring",
      "text" : "In binary classification, the regression of (scaled) class indicators enables to recover the LDA discriminant direction. For more than two classes, this approach is impaired by the masking effect (Hastie et al., 1994), where the scores assigned to a class situated between two other ones may never dominate. Optimal scoring\n(OS) circumvents the problem by assigning “optimal scores” to the classes.\nHastie et al. (1995) proposed to incorporate a smoothness prior on the discriminant directions in the OS problem through a positive-definite penalty matrix Ω, leading to a problem expressed in compact form as\nmin Θ,B\n‖YΘ−XB‖2F + λ tr(B ᵀΩB) (5)\ns. t. ΘᵀYᵀYΘ = IK−1 ,\nwhere Θ are the class scores, B the regression coefficients, and ‖·‖F is the Frobenius norm.\nHastie et al. (1995) proved that, under the assumption that YᵀY and XᵀX + λΩ are full rank (which is fulfilled when there are no empty class and Ω is positive definite), Problem (5) is equivalent to a penalized LDA problem, where the sample within-class covariance Sw defined in (1) is replaced in (2) by the “shrunken” estimate\nΣ̂w = n −1 (Xᵀ (In −PY) X + λΩ)\n= Sw + n −1λΩ . (6)\nNote that Σ̂w has larger eigenvalues than Sw; shrinkage refers here to the bias of Σ̂w towards Ω. Linear discriminant classifiers are invariant to the “size” of Σ̂w, that is, not modified by an overall scaling of Σ̂w.\nThe equivalence states that the solutions in B to the OS problem can be mapped to the solutions of the corresponding penalized LDA problem. The parameters of this mapping are furthermore computed when solving the OS optimization problem.\nThough non-convex, the OS problem is readily solved by a decomposition in Θ and B: the optimal B? does not intervene in the optimality conditions with respect to Θ and the optimization with respect to B is obtained in a closed form as a linear combination of the optimal scores Θ? (Hastie et al., 1995). The algorithm may seem a bit tortuous considering the properties mentioned above, as it proceeds in four steps:\n1. initialize Θ to Θ0 such that Θ0 ᵀ YᵀYΘ0 = IK−1;\n2. compute B = (XᵀX + λΩ) −1 XᵀYΘ0;\n3. set Θ? to be the K − 1 leading eigenvectors of YᵀX (XᵀX + λΩ) −1 XᵀY;\n4. compute the optimal regression coefficients\nB? = (XᵀX + λΩ) −1 XᵀYΘ? . (7)\nDefining Θ0 in Step 1, instead of using directly Θ? as expressed in Step 3, drastically reduces the computational burden of the eigenanalysis: the latter is performed on Θ0 ᵀ YᵀX (XᵀX + λΩ) −1 XᵀYΘ0, which is computed as Θ0 ᵀ YᵀXB, thus avoiding a costly matrix inversion. Finally, note that (Θ?,B?) are uniquely defined up to sign swaps and column permutations, and that all critical points are global optima."
    }, {
      "heading" : "4.3. Group-Lasso OS as Penalized LDA",
      "text" : "We now have all the necessary ingredients to introduce our Group-Lasso Optimal Scoring Solver for performing sparse LDA.\nProposition 1. The Group-Lasso OS problem\nB? = argmin B∈Rp×M min Θ∈RK×M\n1 2 ‖YΘ−XB‖2F + λ p∑ j=1 ∥∥βj∥∥ 2\ns. t. ΘᵀYᵀYΘ = IM ,\nwith M = K − 1, is equivalent to the penalized LDA problem\nBLDA = argmax B∈Rp×M\ntr(BᵀSbB)\ns. t. Bᵀ(Sw + n −1λΩ)B = IM ,\nthat is, BLDA = B ? diag ( (α−1k (1− α 2 k) −1/2) ) , where αk ∈ (0, 1) is the kth leading eigenvalue of\nM = n−1YᵀX (XᵀX + λΩ) −1 XᵀY ,\nwith Ω = diag (∥∥β1?∥∥−1\n2 , . . . , ∥∥βp?∥∥−1 2 ) , using again\nthe convention that ∥∥βj?∥∥\n2 null implies that the jth\nrow of BLDA is null.\nProof. The proof simply consists in applying the result of Hastie et al. (1995), which holds for quadratic penalties, to the quadratic variational form of the grouplasso.\nThe proposition applies in particular to the Lassobased OS approaches to sparse LDA (Grosenick et al., 2008; Clemmensen et al., 2011) for K = 2, that is, for binary classification or more generally for a single discriminant direction. Note however that it leads to a slightly different decision rule if the decision threshold is chosen a priori according to the Gaussian assumption for the features. For more than one discriminant direction, the equivalence does not hold any more, since the Lasso penalty does not result in an equivalent quadratic penalty in the simple form tr(BᵀΩB).\nAlgorithm 1 Adaptively Penalized Optimal Scoring\nInput: X, Y, B, λ Initialize: S ← { j ∈ {1, . . . , p} : ∥∥βj∥∥ 2 > 0 } ,\nΘ0 : Θ0 ᵀ YᵀYΘ0 = IK−1, convergence ← false repeat // Step 1: solve (3) in B assuming S optimal repeat\nΩ← diag(ωS) , with ωj ← ∥∥βj∥∥−1\n2\nBS ← (Xᵀ SX S + λΩ) −1 Xᵀ SYΘ 0\nuntil conditions (4) hold for all j ∈ S // Step 2: identify inactivated variables\nfor {j ∈ S : ∥∥βj∥∥\n2 = 0} do\nif optimality condition (4b) holds then S ← S\\{j}\nend if end for // Step 3: check for optimality of set S j? ← argmax\nj∈Sc\n∥∥∂J/∂βj∥∥ 2\nif ∥∥∥∂J/∂βj?∥∥∥\n2 < λ then\nconvergence ← true // B is optimal else S ← S ∪ {j?}\nend if until convergence (s,V)←eigenanalyze(Θ0ᵀYᵀX SB), that is,\nΘ0 ᵀ YᵀX SBvk = skvk k = 1, . . .K − 1\nΘ? ← Θ0V, B? ← BV, α?k ← n−1/2s 1/2 k\nOutput: Θ?, B?, α?"
    }, {
      "heading" : "5. GLOSS Algorithm",
      "text" : "The efficient approaches developed for the Lasso take advantage of the sparsity of the solution by solving a series of small linear systems, whose sizes are incrementally increased/decreased (Osborne et al., 2000). This approach was pursued for the group-Lasso (Roth & Fischer, 2008) in its standard formulation. We adapt this algorithmic framework to the variational form (3), with J(B) = 1/2 ‖YΘ−XB‖2F .\nThe algorithm starts from a sparse initial guess, say B = 0, thus defining the set S of “active” variables, currently identified as non-zero. Then, it iterates the three steps summarized in Algorithm 1."
    }, {
      "heading" : "5.1. OS Regression Coefficients Updates",
      "text" : "Step 1 of Algorithm 1 updates the coefficient matrix B within the current active set S. The quadratic variational form of the problem suggests a blockwise optimization strategy consisting in solving (K − 1) independent card(S)-dimensional problems instead of a\nsingle (K − 1) × card(S)-dimensional problem. The interaction between the (K − 1) problems is relegated to the common adaptive quadratic penalty Ω. This decomposition is especially attractive as we then solve (K − 1) similar systems:\n(Xᵀ SX S + λΩ)βk = X ᵀ SYθ 0 k ,\nwhere X S denotes the columns of X indexed by S and βk and θ 0 k denote the kth column of B and Θ 0 respectively. These linear systems only differ in the right-hand-side term, so that a single Cholesky decomposition is necessary to solve all systems, whereas a blockwise Newton-Raphson method based on the standard group-Lasso formulation would result in different “penalties” Ω for each system."
    }, {
      "heading" : "5.2. Solution Path",
      "text" : "Finally, note that our default strategy is to compute a series of solutions along the regularization path, defined by a series of penalties λ1 = λmax > · · · > λt > · · · > λT = λmin ≥ 0 such that B?(λmax) = 0, that is λmax = maxj∈{1,...,p}\n∥∥Xᵀj YΘ0∥∥2 , where Xj is the jth column of X. Then, we regularly decrease the penalty λt+1 = λt/2 and use a warm-start strategy, where the feasible initial guess for B(λt+1) is initialized with B(λt). The final penalty parameter λmin is specified in the optimization process when the maximum number of desired active variables is attained (by default the minimum of n and p)."
    }, {
      "heading" : "5.3. Diagonal LDA Variant",
      "text" : "We motivated the group-Lasso penalty by sparsity requisites, but robustness considerations could also drive its usage, since LDA is known to be unstable when the number of examples is small compared to the number of variables. In this context, LDA has been experimentally observed to benefit from unrealistic assumptions on the form of the estimated within-class covariance matrix. Indeed, the diagonal approximation that ignores correlations between genes may lead to better classification in microarray analysis.Bickel & Levina (2004) shown that this crude approximation provides a classifier with best worst-case performances than the LDA decision rule in small sample size regimes, even if variables are correlated.\nThe equivalence proof between penalized OS and penalized LDA of Hastie et al. (1995) reveals that quadratic penalties in the OS problem are equivalent to penalties on the within-class covariance matrix in the LDA formulation. This proof suggests a slight variant of penalized OS (5) corresponding to penalized LDA with diagonal within-class covariance ma-\ntrix, where the least square problems\n‖YΘ−XB‖2F = tr(Θ ᵀYᵀYΘ− 2ΘᵀXB + nBᵀSB)\nare replaced by\ntr(ΘᵀYᵀYΘ− 2ΘᵀXB + nBᵀ(Sb + diag( Sw))B)\nNote that this variant requires diag( Sw) + Ω − S to be positive definite."
    }, {
      "heading" : "6. Experimental Results",
      "text" : "This section presents some experimental results comparing our proposed algorithm, GLOSS, to two other sparse linear classifiers recently proposed to perform sparse LDA, namely the Penalized LDA (PLDA) of Witten & Tibshirani (2011), which applies a Lasso penalty in Fisher’s LDA framework, and the Sparse Linear Discriminant Analysis (SLDA) of Clemmensen et al. (2011), which applies an elastic net penalty to the OS problem. The latter was used with a single tuning coefficient, without any quadratic term in the penalty, so that the elastic net reduces to the Lasso. The two competitors where implemented with the code publicly available from the authors in R and MATLAB respectively (Witten, 2011; Clemmensen, 2008). All results have been computed using the same training, validation and test sets. Note that they differ significantly from the ones of Witten & Tibshirani (2011) in Simulation 4 for which there was a typo in their paper."
    }, {
      "heading" : "6.1. Simulated Data",
      "text" : "We first compare the three techniques in the simulation study of Witten & Tibshirani (2011), which considers four setups with 1200 examples equally distributed between classes. They are split in a training set of size n = 100, a validation set of size 100, and a test set of size 1000. We are in the small sample regime, with p = 500 variables, out of which 100 differ between classes. Independent variables are generated for all simulations except for simulation 2 where they are slightly correlated. In simulation 2 and 3, classes are optimally separated by a single projection of the original variables, while the two other scenarios require three discriminant directions. The Bayes’ error was estimated to be respectively 1.7%, 6.7%, 7.3% and 30.0%. We follow all the other details of the simulation protocol of Witten & Tibshirani (2011).\nNote that this protocol is detrimental to GLOSS as each relevant variable only affects a single class mean out of K. The setup is favorable to PLDA in the sense that most within-class covariance matrix are diagonal. We thus tested the diagonal GLOSS variant discussed in Section 5.3.\nThe results are summarized in Table 1. Overall, the best predictions are performed by PLDA and GLOS-D that both benefit of the knowledge of the true withinclass covariance structure. Then, among SLDA and GLOSS that both ignore this structure, our proposal has a clear edge. The error rates are far away from the Bayes’ error rates, but the sample size is small with regard to the number of relevant variables. Regarding sparsity, the clear overall winner is GLOSS, followed far away by SLDA, which is the only method that do not succeed in uncovering a low-dimensional representation in Simulation 3. The adequacy of the selected features was assessed by the True Positive Rate (TPR) and the False Positive Rate (FPR). PLDA has the best TPR but a terrible FPR, except in simulation 3 where it dominates all the other methods. GLOSS has by far the best FPR with overall TPR slightly below SLDA."
    }, {
      "heading" : "6.2. Gene Expression Data",
      "text" : "We now compare GLOSS to PLDA and SLDA on three genomic datasets. The Nakayama dataset contains 105 examples of 22,283 gene expressions for categorizing 10 soft tissue tumors. It was reduced to the 86 examples belonging to the 5 dominant categories (Witten & Tibshirani, 2011). The Ramaswamy dataset contains 198 examples of 16,063 gene expressions for cat-\negorizing 14 classes of cancer. Finally, the Sun dataset contains 180 examples of 54,613 gene expressions for categorizing 4 classes of tumors.\nEach dataset was split into a training set and a test set with respectively 75% and 25% of the examples. The tuning parameter is performed by 10-fold crossvalidation and the test performances are then evaluated. The process is repeated 10 times, with random choices of training and test set split.\nWe present the test error rates and the number of selected variables in Table 2. The three methods have comparable prediction performances on the Nakayama and Sun data, but GLOSS performs better on the Ramaswamy data, where the SparseLDA package failed to return a solution, due to numerical problems in the LARS-EN implementation. Regarding the number of selected variables, GLOSS is again much sparser than its competitors.\nFinally, Figure 1 displays the projection of the observations for the Nakayama and Sun datasets in the first canonical planes estimated by GLOSS and SLDA. For the Nakayama dataset, groups 1 and 2 are wellseparated from the other ones in both representations, but GLOSS is more discriminant in the meta-cluster gathering groups 3 to 5. For the Sun dataset, SLDA suffers from a high colinearity of its first canonical variables that renders the second one almost noninformative. As a result, group 1 is better separated in the first canonical plane with GLOSS."
    }, {
      "heading" : "7. Conclusions and Further Works",
      "text" : "We described GLOSS, an efficient algorithm that performs sparse LDA based on the regression of class in-\ndicators. Our proposal is equivalent to a penalized LDA problem. This is up to our knowledge the first approach that enjoys this property in the multi-class setting. This relationship is also amenable to accommodate interesting constraints on the equivalent penalized LDA problem, such as imposing a diagonal structure of the within-class covariance matrix.\nComputationally, GLOSS is based on an efficient active set strategy that is amenable to the processing of problems with a large number of variables. The inner optimization problem decouples the p× (K−1)dimensional problem into (K − 1) independent pdimensional problems. The interaction between the (K − 1) problems is relegated to the computation of the common adaptive quadratic penalty. The algorithm presented here is highly efficient in medium to high dimensional setups, which makes it a good candidate for the analysis of gene expression data.\nOur experimental results confirm the relevance of the approach, which behaves well compared to its competitors, either regarding its prediction abilities or its interpretability (sparsity). Employing the same features in all discriminant directions enables to generate models that are globally extremely parsimonious, with good prediction abilities. The resulting sparse discriminant directions also allow for visual inspection of data from the low-dimensional representations that can be produced.\nThe approach has many potential extensions that have not yet been implemented. A first line of development is to consider a broader class of penalties. For example, plain quadratic penalties can also be added to the group-penalty to encode priors about the withinclass covariance structure, in the spirit of the Penalized Discriminant Analysis of Hastie et al. (1995). Also, besides the group-Lasso, our framework can be customized to any penalty that is uniformly spread within groups, and many composite or hierarchical penalties that have been proposed for structured data meet this condition."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by the PASCAL2 Network of Excellence, the European ICT FP7 (grant 247022 - MASH), and the French National Research Agency ANR (grant ClasSel 08-EMER-002)."
    } ],
    "references" : [ {
      "title" : "The use of multiple measurements",
      "author" : [ "A. R" ],
      "venue" : null,
      "citeRegEx" : "R.,? \\Q2011\\E",
      "shortCiteRegEx" : "R.",
      "year" : 2011
    }, {
      "title" : "Outcomes of the equivalence of adaptive ridge with least absolute shrinkage",
      "author" : [ "Y. Grandvalet", "S. Canu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Grandvalet and Canu,? \\Q1998\\E",
      "shortCiteRegEx" : "Grandvalet and Canu",
      "year" : 1998
    }, {
      "title" : "Interpretable classifiers for fMRI improve prediction of purchases",
      "author" : [ "L. Grosenick", "S. Greer", "B. Knutson" ],
      "venue" : "IEEE transactions on Neural Systems and Rehabilitation Engineering,",
      "citeRegEx" : "Grosenick et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Grosenick et al\\.",
      "year" : 2008
    }, {
      "title" : "Discriminant analysis by Gaussian mixtures",
      "author" : [ "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B (Methodological),",
      "citeRegEx" : "Hastie and Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Hastie and Tibshirani",
      "year" : 1996
    }, {
      "title" : "Flexible discriminant analysis by optimal scoring",
      "author" : [ "T. Hastie", "R. Tibshirani", "A. Buja" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hastie et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 1994
    }, {
      "title" : "Penalized discriminant analysis",
      "author" : [ "T. Hastie", "A. Buja", "R. Tibshirani" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Hastie et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 1995
    }, {
      "title" : "Sparse optimal scoring for multiclass cancer diagnosis and biomarker detection using microarray data",
      "author" : [ "C. Leng" ],
      "venue" : "Computational Biology and Chemistry,",
      "citeRegEx" : "Leng,? \\Q2008\\E",
      "shortCiteRegEx" : "Leng",
      "year" : 2008
    }, {
      "title" : "On the LASSO and its dual",
      "author" : [ "M.R. Osborne", "B. Presnell", "B.A. Turlach" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Osborne et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Osborne et al\\.",
      "year" : 2000
    }, {
      "title" : "The group-lasso for generalized linear models: Uniqueness of solutions and efficient algorithms",
      "author" : [ "V. Roth", "B. Fischer" ],
      "venue" : "In ICML ’08: Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Roth and Fischer,? \\Q2008\\E",
      "shortCiteRegEx" : "Roth and Fischer",
      "year" : 2008
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R.J. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B (Methodological),",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "Penalized classification using Fisher’s linear discriminant",
      "author" : [ "D. Witten", "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B (Statistical Methodology),",
      "citeRegEx" : "Witten and Tibshirani,? \\Q2011\\E",
      "shortCiteRegEx" : "Witten and Tibshirani",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Their approach later disseminated under the name optimal scoring (OS) by Hastie et al. (1994), who proposed",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "several extensions of LDA, either aiming at constructing more flexible discriminants (Hastie & Tibshirani, 1996) or more conservative ones (Hastie et al., 1995).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "Several sparse LDA have been derived using sparsityinducing penalties on the OS regression problem (Leng, 2008; Grosenick et al., 2008; Clemmensen et al., 2011).",
      "startOffset" : 99,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "Several sparse LDA have been derived using sparsityinducing penalties on the OS regression problem (Leng, 2008; Grosenick et al., 2008; Clemmensen et al., 2011).",
      "startOffset" : 99,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "Several sparse LDA have been derived using sparsityinducing penalties on the OS regression problem (Leng, 2008; Grosenick et al., 2008; Clemmensen et al., 2011). These proposals are motivated by the equivalence between penalized OS and penalized LDA, but since sparsity-inducing penalties are non-quadratic, they fall beyond the realm of the equivalence stated by Hastie et al. (1995). Until now, no rigorous link was derived between sparse OS and sparse LDA.",
      "startOffset" : 112,
      "endOffset" : 385
    }, {
      "referenceID" : 4,
      "context" : "Our derivation uses a variational formulation of the group-Lasso to generalize the equivalence drawn by Hastie et al. (1995) for quadratic penalties.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Quadratic variational forms of the Lasso and groupLasso have been proposed shortly after the original Lasso paper of Tibshirani (1996), as a means to address optimization issues, but also as an inspiration for generalizing the Lasso penalty (Grandvalet, 1998; Grandvalet & Canu, 1999).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "For more than two classes, this approach is impaired by the masking effect (Hastie et al., 1994), where the scores assigned to a class situated between two other ones may never dominate.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Though non-convex, the OS problem is readily solved by a decomposition in Θ and B: the optimal B does not intervene in the optimality conditions with respect to Θ and the optimization with respect to B is obtained in a closed form as a linear combination of the optimal scores Θ (Hastie et al., 1995).",
      "startOffset" : 279,
      "endOffset" : 300
    }, {
      "referenceID" : 4,
      "context" : "The proof simply consists in applying the result of Hastie et al. (1995), which holds for quadratic penalties, to the quadratic variational form of the grouplasso.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "The proposition applies in particular to the Lassobased OS approaches to sparse LDA (Grosenick et al., 2008; Clemmensen et al., 2011) for K = 2, that is, for binary classification or more generally for a single discriminant direction.",
      "startOffset" : 84,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "The efficient approaches developed for the Lasso take advantage of the sparsity of the solution by solving a series of small linear systems, whose sizes are incrementally increased/decreased (Osborne et al., 2000).",
      "startOffset" : 191,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "The equivalence proof between penalized OS and penalized LDA of Hastie et al. (1995) reveals that quadratic penalties in the OS problem are equivalent to penalties on the within-class covariance matrix in the LDA formulation.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "This section presents some experimental results comparing our proposed algorithm, GLOSS, to two other sparse linear classifiers recently proposed to perform sparse LDA, namely the Penalized LDA (PLDA) of Witten & Tibshirani (2011), which applies a Lasso penalty in Fisher’s LDA framework, and the Sparse Linear Discriminant Analysis (SLDA) of Clemmensen et al.",
      "startOffset" : 213,
      "endOffset" : 231
    }, {
      "referenceID" : 8,
      "context" : "This section presents some experimental results comparing our proposed algorithm, GLOSS, to two other sparse linear classifiers recently proposed to perform sparse LDA, namely the Penalized LDA (PLDA) of Witten & Tibshirani (2011), which applies a Lasso penalty in Fisher’s LDA framework, and the Sparse Linear Discriminant Analysis (SLDA) of Clemmensen et al. (2011), which applies an elastic net penalty to the OS problem.",
      "startOffset" : 213,
      "endOffset" : 368
    }, {
      "referenceID" : 0,
      "context" : "The two competitors where implemented with the code publicly available from the authors in R and MATLAB respectively (Witten, 2011; Clemmensen, 2008). All results have been computed using the same training, validation and test sets. Note that they differ significantly from the ones of Witten & Tibshirani (2011) in Simulation 4 for which there was a typo in their paper.",
      "startOffset" : 91,
      "endOffset" : 313
    }, {
      "referenceID" : 9,
      "context" : "We first compare the three techniques in the simulation study of Witten & Tibshirani (2011), which considers four setups with 1200 examples equally distributed between classes.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "We first compare the three techniques in the simulation study of Witten & Tibshirani (2011), which considers four setups with 1200 examples equally distributed between classes. They are split in a training set of size n = 100, a validation set of size 100, and a test set of size 1000. We are in the small sample regime, with p = 500 variables, out of which 100 differ between classes. Independent variables are generated for all simulations except for simulation 2 where they are slightly correlated. In simulation 2 and 3, classes are optimally separated by a single projection of the original variables, while the two other scenarios require three discriminant directions. The Bayes’ error was estimated to be respectively 1.7%, 6.7%, 7.3% and 30.0%. We follow all the other details of the simulation protocol of Witten & Tibshirani (2011).",
      "startOffset" : 74,
      "endOffset" : 843
    }, {
      "referenceID" : 4,
      "context" : "For example, plain quadratic penalties can also be added to the group-penalty to encode priors about the withinclass covariance structure, in the spirit of the Penalized Discriminant Analysis of Hastie et al. (1995). Also, besides the group-Lasso, our framework can be customized to any penalty that is uniformly spread within groups, and many composite or hierarchical penalties that have been proposed for structured data meet this condition.",
      "startOffset" : 195,
      "endOffset" : 216
    } ],
    "year" : 2012,
    "abstractText" : "We present a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis (LDA). Our proposal, is based on penalized Optimal Scoring. It has an exact equivalence with penalized LDA, contrary to the multi-class approaches based on the regression of class indicator that have been proposed so far. Sparsity is obtained thanks to a group-Lasso penalty that selects the same features in all discriminant directions. Our experiments demonstrate that this approach generates extremely parsimonious models without compromising prediction performances. Besides prediction, the resulting sparse discriminant directions are also amenable to low-dimensional representations of data. Our algorithm is highly efficient for medium to large number of variables, and is thus particularly well suited to the analysis of gene expression data.",
    "creator" : "LaTeX with hyperref package"
  }
}