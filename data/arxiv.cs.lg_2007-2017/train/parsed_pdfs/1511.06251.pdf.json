{
  "name" : "1511.06251.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dynamics of Stochastic Gradient Algorithms",
    "authors" : [ "Qianxiao Li" ],
    "emails" : [ "qianxiao@math.princeton.edu", "chengt@math.princeton.edu", "weinan@math.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n06 25\n1v 1\nKeywords: stochastic gradient algorithms, stochastic modified equations, dynamics, optimal control"
    }, {
      "heading" : "1. Introduction",
      "text" : "Stochastic Gradient algorithms (SGA) are often used to solve optimization problems of the form\nmin x∈Rd\nf(x) := 1\nn\nn ∑\ni=1\nfi(x), (1)\nwhere n is typically large. The basic form of SGA iteration is the following:\nxk+1 = xk − η∇fγk(xk), (2)\nwhere {γk} are i.i.d random variables uniformly distributed over {1, 2, · · · , n}. Compared with the deterministic gradient descent algorithms (GD), the SGA requires only one evaluation of the gradients at each time step. This makes it appealing when gradient evaluations are expensive. SGA algorithms have been increasingly popular in machine learning applications and have become “the algorithm” for extremely large scale problems. Due to the stochastic nature of the algorithm, its behavior is different from deterministic algorithms in several aspects. Figure 1 shows a typical application of GD and SGA. We see that the noisy dynamics of the SGA is significantly different from that of the GD algorithm. Note that the computational cost for the GD is n times that of the SGA. Hence, to compare the two, we hereafter define an iteration of the SGA to be n steps of (2).\nAlthough there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics. For example, the convergence rate gives an error bound for the asymptotic behavior of the algorithms, but for practitioners, the initial convergence speed is equally important, if not more. We need more precise characterizations of the dynamics of the algorithms other than asymptotic error bounds. In this paper, we make an attempt in this direction. In particular, we are interested in the following questions:\n1. How to characterize the dynamics of SGA algorithms?\n2. It is often observed that SGA is much faster than GD at the beginning, but starts oscillating at some point and eventually GD has higher precision. How to quantify the initial acceleration? When does the acceleration break down? How to characterize the oscillation?\n3. Momentum is often used in conjunction with the SGA algorithm, how does this affect the dynamics?\n4. In practice, choosing the learning rate schedule has been a delicate issue. How does learning rate affect the solution? And how do we choose the optimal learning rate schedule?\n5. Often, mini-batch is also used. How does mini-batch sizes affect the solution? And if we are allowed to change batch-size during the iteration, what is the optimal batch-size schedule?\nThese questions are difficult in the general case, but they are of both theoretical and practical importance. In the following, we develop a tool that can be used to provide answers to the above questions, at least in some simple cases. The key idea is to approximate the dynamics of SGA iterations by a suitable modified stochastic differential equation. This is inspired by the classical method of modified equations.\nThe method of modified equations is a means of analyzing finite difference schemes. Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974). The method of modified equations consists of creating a differential equation which approximates the given numerical scheme more accurately than the original equation. So the numerical scheme can be analyzed by studying the modified equations.\nWhen using the methodology of modified equations to study SGA, we are faced with an important problem: unlike classical numerical schemes for solving differential equations, the SGA involve random sampling and classical modified equations cannot deal with such stochasticity. To address this issue, we introduce the method stochastic modified equations. Instead of associating a differential equation with the numerical scheme, we associate with it a stochastic differential equation. More concretely, the SGA iteration is modeled as a numerical scheme for solving an SDE of the diffusion type. The SDE is then used to analyze the properties of the the numerical scheme, in a similar spirit as the method of modified equations. We call this technique the method of stochastic modified equations (SME).\nThe flavor of results given by the SME analysis is very different from those given by convergence analysis. Instead of giving error bounds, the SME gives the precise dynamics. For simple examples, the initial convergence rate can be computed, thus we know not only that the rate is linear but also exactly how many times faster the SGA is than GD. It also gives the transition point when this acceleration ceases to persist and the distribution of the eventual oscillation. Even though the SME is an approximation method, the results are often surprisingly accurate.\nMoreover, with the SME, we can go further to study various speed-up techniques for the SGA. For example, the problem of selecting the optimal learning rate schedule can be readily formulated as a control problem. The associated Hamilton-Jacobi-Bellman equation\ncan be solved exactly in some special interesting cases. Such exact calculations can provide important and new insights to designing optimized speed-up procedures in practice.\nAs an application of the SME, we consider solving linear equations using stochastic gradient algorithms. Using SGA to solve linear equations can be seen as a relaxation of the well known randomized Kaczmarz algorithm. For some recent reference, see e.g. Strohmer and Vershynin (2009); Needell et al. (2014). Introducing the relaxation, the Kaczmarz iteration rule is\nxk+1 = xk + αk bγk − 〈aγ k , xk〉\n‖aγk‖2 a γk . (3)\nWe study this problem from the SME perspective. Both the initial exponential convergence rate and the eventual oscillation can be computed. We compute the optimal learning rate schedule and the optimal batch-size schedule given a fixed computational budget. Numerical results are given to demonstrate the improvements.\nWe stress here that the goal of this paper is to introduce the SME framework to analyze the dynamics of the SGA. Where possible, we also perform exact calculations so as to elucidate various features of the SGA and provide guiding principles to optimizing the SGA in general. A rigorous treatment of some results, such as the sense of which the SME approximates the SGA, is not within the scope of the current work. However, we also emphasize that such statements can be made rigorous.\nThe rest of the paper is organized as follows. In section 2, we introduce the SME technique, followed by two concrete examples. In section 3, we use the SME approach to characterize various speed-up techniques for the SGA. An application of the framework to improving the relaxed randomized Karczmarz method is discussed in section 4. Lastly, a discussion and conclusion is given in section 5."
    }, {
      "heading" : "2. Stochastic modified equations",
      "text" : "In this section we introduce the concept of the SME, followed by two concrete examples to illustrate the methodology. As discussed before, the main purpose of this paper is to introduce this technique and how we can use it to gain some understandings of the stochastic gradient methods. Therefore, we sacrificed some mathematical rigor in the derivation. The exact statement such as in what sense the SME approximates the original SGA will not be dealt here. Instead, we use some numerical examples to justify the SME approximation.\nThe motivation behind the SME is the following. Recall that at each step of the SGA iterations, a function fγk is picked randomly and uniformly from the n functions. As Efγk = f , fγk is an unbiased estimator of f . Hence\nExk+1 = xk − η∇f(xk). (4)\nThis can be seen as the forward Euler scheme for solving the differential equation:\nx′(t) = −∇f(x) (5)\nwith step size ∆t = η. Under mild conditions on f , the scheme converges for any time interval [0, T ] as η → 0. Hence the behavior of the expectation of SGA iterations is well characterized by (5).\nBut this approximation does not capture the variance of the iterates, hence can only seen as a first order approximation. If the variance does not converge, the actual implementations would still fail even if the expectation converges. Therefore, it is important to approximate the variance as well.\nRewrite the SGA iteration:\nxk+1 − xk = −η∇f(xk) + η(∇f(xk)−∇fγk(xk)). (6)\n∇f(xk) is the deterministic drift term, and ∇f(xk)−∇fγk(xk) is the noise term with mean 0. Intuitively, in the small η limit, the accumulation of the noise term over several iterations over a fixed small time interval will converge in distribution to a random variable with mean 0 and finite covariance. As a second order approximation, we further approximate this noise term by a normal variate whose covariance matches the empirical covariance, which is:\nΣ(xk) := cov(∇fγk(xk))\n= E\n[\n1\nn\n∑\ni\n(∇fi(xk)−∇f(xk))(∇fi(xk)−∇f(xk))T ] . (7)\nThe expectation is taken over the distribution of {γk}. We could continue to approximate the higher order terms, but as it turns out that a second order approximation is sufficient to characterize the dynamics of the SGA iterations. For small η, the SGA iterations behave like a diffusion process under a potential f . It is naturally linked to the following N -dimensional SDE:\ndX(t) = −∇f(X(t))dt+ σ(X(t))dB(t). (8)\nThe usual Euler-Maruyama scheme for solving this SDE is\nxk+1 = xk −∆t∇f(xk) + √ ∆tσ(xk)zk, (9)\nwhere the zks are independent d-dimensional N (0, I) random variables and σ(x)σT (x) is the covariance matrix. The SGA iteration with learning rate η can be thought of as the Eular-Maruyama scheme for solving (8). Comparing the drift and volatility terms in (6) and (9), we see that\n{\n∆t = η σ(x)σ(x)T = ηΣ(x). (10)\nTherefore, for small η, the SGA iterations are approximated by the following SDE:\ndX(t) = −∇f(X(t))dt+√ησ(X(t))dB(t), σ(x)σ(x)T = Σ(x). (11)\nWe call this equation the stochastic modified equation (SME).\nNow we study two examples to concretely illustrate the technique. Simple as they are, they can provide us with many insights, which helps us understand more complex problems."
    }, {
      "heading" : "2.1 One dimensional example",
      "text" : "Let f1(x) = (x− a)2 and f2(x) = (x− b)2 where x, a, b ∈ R. We want to minimize\nf(x) = 1\n2 f1(x) +\n1 2 f2(x). (12)\nLet α = 12(a+ b) be the point where minimum is obtained. The GD step for solving this problem is\nxk+1 = xk − η∇f(xk), (13)\nand we have xk+1 − α = (1− 2η)(xk − α) = (1− 2η)k(x0 − α), (14)\ni.e., the convergence is linear. On the other hand, the SGA step is\nxk+1 = xk − η∇fγk(xk), (15)\nwhere {γk} are i.i.d random variable that takes value 1 and 2 with probability 1/2. Figure 2(a) shows the GD and SGA iterations for a = −1, b = 1, η = 0.005. There are some\ninteresting features in this figure. First, SGA path approaches to 0 twice as fast as GD. When it is close to 0, it begins to oscillate. The amplitude of oscillation is related to η. Let N be the total number of SGA iterations. We now make these observations precise. The SME for this problem is:\ndX = −2(X − α)dt+ 2√ηdB, X(0) = x0 (16)\nfrom [0, T ] where T = ηN is the final time. Figure 2(b) shows the SME approximation for one sample path. Both the initial convergence and eventual oscillation are well captured.\nThe SDE is the mean-reverting Ornstein-Uhlenbeck process and the exact solution is given by\nX(t) = α+ (x0 − α)e−2t + 2 √ η ∫ t\n0 e2(s−t)dB(s). (17)\nA lot of information about the dynamics about the SGA iterations can be read out from this solution.\nAcceleration. Using Itò’s isometry, we have\n{\nE[X(t)|X(0) = x0] = α+ (x0 − α)e−2t Var[X(t)|X(0) = x0] = η(1 − e−4t).\n(18)\nWe see that initially, the variance is small, thus the behavior of SGA resembles GD and has the same convergence rate as GD. However, as the SGA requires only one evaluation of the gradient at each iteration, it is exactly twice as fast as GD. The evolution of the distribution of X(t) is characterized by the Fokker-Planck equation:\n{\n∂p(x,t) ∂t = ∂(2(x−α)p(x,t)) ∂x + 2η ∂p(x,t) ∂x2 p(x, 0) = δ(x− x0). (19)\nThe solution is\np(x, t|x0, 0) = √\n1 2πη(1 − e−4t) exp ( −(x− α− (x− α)e −2tx0) 2 2η(1 − e−4t) ) . (20)\nThus, X(t) ∼ N (α + (x0 − α)e−2t, η(1 − e−4t)). Let t → ∞, the distribution converges to N (α, η).\nBreak down. The acceleration breaks down when\nx0e −2t ≈ √η. (21)\nThis happens around\nk ≈ 1 4η log\n(\nx20 η\n)\n(22)\nSGA iterations.\nLearning rate. We see that learning rate η enters in two ways. The initial convergence speed, and the stationary distribution. Doubling the learning rate implies twice speedup in the beginning (if converges) and twice the variance of stationary distribution.\nTo summarize what we learned from this example: the SGA iteration converges linearly and is initially twice as fast as GD, after k ≈ 14η log(x20/η) steps, the acceleration breaks down. Eventually, it converges to N (α, η) in distribution. This example conveys the basic idea of using SME to analyze SGA dynamics. And as we will see, the technique can be useful in more complicated problems as well."
    }, {
      "heading" : "2.2 Multi-dimensional example",
      "text" : "Our second example is a multi-dimensional problem. Consider minimizing\nf(x) = 1\n2n\nn ∑\ni=1\n(λi(xi + 1) 2 + λi(xi − 1)2), x ∈ Rd. (23)\nMinimizing this function is equivalent to solving a particular inconsistent linear system Ax = b where λi represent the eigenvalues of A\nTA. For this objective function, we have {\n(∇f(x))i = 2λixin , (σσT )ij = 4λ2i (x 2 i+1) n δij − 4λiλjxixjn2 .\n(24)\nUsing the technique introduced before, we get the following SME:\ndXi(t) = − 2λi\nn Xi(t)dt+\n√ ησ(X(t))dBi(t), i = 1, · · · , n. (25)\nObtaining the exact solution to (25) is complicated. But note that due to the linearity of the drift term, for small η the SME is close to a Gaussian process. Hence, we can obtain a lot of information about the dynamics from studying the moment equations. Taking expectations on the equations, we have\ndEXi(t) = − 2λi\nn Xi(t)dt, i = 1, · · · , n. (26)\nThe solution is\nEXi(t) = xi exp(− 2λit\nn ). (27)\nThis is the same as the deterministic case with gradient descent. We see the expectation converges linearly. Yet, more important is the second moment because the variance could still explode even if the expectation converges. Taking derivative of X2(t) and taking expectation, we have\ndEX2i (t) = (− 4λi n + 4ηλ2i n − 4ηλ 2 i n2 )EXi(t) 2dt+ 4ηλ2i n dt (28)\nThe solution is\nEX2i (t) = λiη\n1− n−1 n λiη + (x2i −\nλiη\n1− n−1 n λiη )e− 4tλi n e\n4tηλ2i (n−1)\nn2 . (29)\nWe immediately see that for small η, the convergence rate is linear in the beginning, but is slower than the deterministic case with GD. The variance of the gradients introduces a positive exponent that slows down the convergence. To ensure overall convergence, we should have η < n(n−1)λi for i = 1, · · · , n.\nThe variance of the eventual oscillation is λiη 1−n−1\nn λiη\n. If η is small, then\nE(X2i ) ∗ ≈ λiη. (30)\nThe overall second moment is E‖X(t)‖22 = ∑ i EXi(t) 2. The transition from exponential convergence to oscillation happens when\n∑\ni\nλiη\n1− n−1 n λiη ≈\n∑\ni\n(x2i − λiη\n1− n−1 n λiη )e− 4tλi n e\n4tηλ2i (n−1)\nn2 . (31)\nAs a sanity check, we run the SGA algorithm for sufficient time to demonstrate the above calculations. We take λ to be evenly distributed from 0 to 100 in log scale. Figure 3 shows one sample path. The eventual variance is computed for some different learning rates, as shown in Table 1. The transition point is marked as red dot in Figure 3(a).\nWe see that the initial convergence rate and the eventual variance are both characterized very well with the SME approximation, and gets better for smaller learning rates.\nFrom the expression of the moments, we see that the overall convergence speed is controlled by λmin, and the learning rate is limited by the largest λmax. Let s =\nλmin λmax\nbe the reciprocal of the condition number, then this corresponds to a convergence rate of O (\nexp(−4sk n ) ) in the discrete case."
    }, {
      "heading" : "3. Acceleration techniques.",
      "text" : "We saw in the previous section that the SME provides a precise characterization of the dynamics of the SGA when the learning rate is sufficiently small. In this section, we show how the SME can also be useful for analyzing and improving speed-up techniques of the classical SGA. We will discuss three of these in detail, namely adding momentum, adjusting the learning rates and implementing mini-batch."
    }, {
      "heading" : "3.1 SGA with momentum",
      "text" : "The classical momentum method is a technique for accelerating gradient descent. For quadratic functions, it can be shown that the momentum reduces the number of iterations from O(κ) to O(√κ) where κ is the condition number of the problem. More recently, the momentum has been used in conjunction with SGA in large scale machine learning problems such as training deep neural networks. It has been empirically observed that SGA with momentum accelerates convergence, see for example, Sutskever et al. (2013). We study the effects of momentum using the SME approach.\nThe usual SGA with momentum proceeds as follows. Set x0 = x0, v 0 = v0, then iterate\nthe following two steps:\n{\nvk+1 = µvk − η∇fγk(xk) xk+1 = xk + vk+1,\n(32)\nfor k = 0, 1, 2, · · · , where {γk} are i.i.d random variables uniform over {1, · · · , n}. Using the SME technique as introduced in the previous section, we arrive at the following SDE:\n{\ndX = 1 η V dt dV = (µ−1 η V −∇f(X)dt+√ησ(X,V )dB (33)\nUnlike the previous case, the SDE above involves two variables and can be recast as a second order differential equation for X.\nFigure 4(a) shows a sample path of the SGA with momentum and the SDE approximation for solving an inconsistent linear equation. We see a good agreement in the initial descent as well as the size of eventual fluctuations.\nFor general f , we may not be able to get a closed form solution. But we can always analyze the moment equations, as long as they are closed, to get useful information. Taking expectations on the equations we see that the first order moments corresponds to the deterministic gradient descent algorithm with momentum. The second moment equations are as follows:\n\n \n \ndEXiXj = ( 1 η EXiVj + 1 η EXjVi)dt dEXiVj = ( µ−1 η\nEXiVj − EXi∇f(X)j + 1ηEViVj)dt dEViVj = ( 2(µ−1) η EViVj − EVi∇f(X)j − EVj∇f(Xt)i + η(σσT )ij)dt (34)\nfor i, j ∈ {1, · · · , d}. To give a concrete example, let f be the one defined in (23). Among the moment equations, it is EX2i that we care most about as it is directly related to the convergence. The relevant equations are :\n\n  \n  \ndEX2i dt = 2 η EXiVi dEXiVi dt = −2λi n EX2i + µ−1 η EXiVi + 1 η EV 2i dEV 2i dt = 4ηλ2i (n−1) n2 EX2i − 4λin EXiVi + 2(µ−1) η EV 2i dt,\n(35)\nfor i = 1, 2, · · · , d. First, we analyze the asymptotic behavior of the solution. Setting the right hand side of (35) to zero, we get the stationary solution: \n  \n  \nE(X2i ) ∗ = nηλi\nn−(n−1)ηλi−nµ\nE(XiVi) ∗ = 0 E(V 2i ) ∗ = 2η2λ2i n−(n−1)ηλi−nµ\n(36)\nfor i = 1, · · · , d. This solution tells us that the eventual oscillation of X is amplified compared with the plain SGA. In particular, if η is small,\nE(X2i ) ∗ ≈ ηλi\n1− µ. (37)\nCompared with the plain SGA in (30), we see that the eventual variance is amplified by a factor of 1/(1 − µ).\nNext, we analyze the convergence speed. To do this, we need to compute the eigenvalues of the coefficient matrix of (35), which is\nCi =\n\n \n0 2 η\n0 −2λi n µ−1 η 1 η\n4ηλ2i (n−1) n2 −4λi n 2(µ−1) η\n\n  , i = 1, · · · , d. (38)\nThe characteristic polynomial for Ci is cubic in ξ, its roots are complicated. Note that 4ηλ2i (n−1)\nn2 is small compared with the other entries when η is small, hence we perform a\nperturbation analysis by considering the following matrix\nCi(δ) =\n\n \n0 2 η\n0 −2λi n µ−1 η 1 η\nδ −4λi n 2(µ−1) η\n\n  . (39)\nThe eigenvalues of Ci(δ) can be write as a series of powers of δ:\nξ(δ) = c0 + c1δ + c2δ 2 + · · · . (40)\nFor small η , Ci(0) is already a good approximation. The characteristic polynomial of Ci(0) is\np(ξ, λi) = − 1\nnη2 (1− µ+ ηξ)(8λi + 2nξ − 2nµξ + nηξ2). (41)\nLet ξi,0(λi), ξi,1(λi), ξi,2(λi) be the roots of p(ξ, λi). For distinct roots, the solution of (35) has the form\nci,0e tξi,0 + ci,1e tξi,1 + ci,2e tξi,2 . (42)\nThe convergence is controlled by the real part of ξ. For given η, we can choose µ to ensure fast overall convergence. The optimal µ∗ is given by\nmin θ,µ θ\nsubject to θ ≥ |eξi,0(λi)|, |eξi,1(λi)|, |eξi,2(λi)| ξi,0(λi), ξi,1(λi) and ξi,2(λi) are roots of p(ξ, λi).\n(43)\nLet δ be the smallest among all λi, then it can be verified that\nµ∗ = √ n− 2√2ηδ√\nn and θ∗ = exp(−2 √ 2δ√ nη ), (44)\nwhere δ = λmin. One can proceed to compute the next order approximation, and doing so introduces an o(η) modification to (44). Hence the above µ∗ is sufficient for analyzing Ci.\nTherefore, for small η, the above overall convergence rate translates into\nO ( (1 + kη + k2η2) exp(−2 √ 2δη√ n k) )\n(45)\nfor the discrete SGA iterations with momentum. We see that for fixed learning rate η, whereas the convergence rate of the SGA depends linearly on λmin, adding a momentum reduces the dependence to the square root of λmin. Figure 4(b) illustrates the usage of momentum in example with η = 1e − 5, µ = 0.9. The predicted variance is 0.047 and the actual variance is 0.047. Some other configurations are listed in Table 2.\nWe see an acceleration of momentum from Figure 4(b). On the other hand, if we use SGA with a learning rate η1−µ∗ , the convergence rate is also of order O(exp(− √ 2δη√ n k)) provided it converges. And the eventual variances are also the same. Hence using momentum with parameter µ∗ is effectively equivalent to using the plain SGA with learning rate multiplied by 11−µ∗ ."
    }, {
      "heading" : "3.2 Optimal learning rate",
      "text" : "Recall that when the SGA is applied to minimize a generic function, one often observes good performance at early times. However, after a number of iterations the objective function begins to fluctuate and fails to decrease further. We called this point the transition point of the SGA. In practice, one common way to avoid hitting this point is to gradually decrease the learning rate. The optimal learning rate problem can be phrased as follows: given a fixed running time, how does one choose a learning rate schedule so as to minimize the error at the end of the run?"
    }, {
      "heading" : "3.2.1 Optimal control setup",
      "text" : "We now apply the SME formalism to cast the above into an optimal control problem. The iteration step for the SGA with adjusted learning rates can be written as\nxk+1 = xk − ηuk∇fk ( xk ) , (46)\nwhere uk ∈ [0, 1] represents the factor by which the learning rate at the current step is reduced. The corresponding SME is\n{ dX (t) = −u (t)∇f (X (t)) dt+√ηu (t)σ (X (t)) dB (t) , X (0) = x0\n(47)\nfor 0 ≤ t ≤ T = ηN . As before, we have the restriction u (t) ∈ [0, 1] for all t ∈ [0, T ]. At the end of the run, the squared error is ‖X (T )− x∗‖2, which we would like to minimize. Hence, the optimal learning rate problem can be cast as a stochastic control problem:\nmin u∈U\nE ‖X (T )− x∗‖2 subject to (47), (48)\nwhere U contains processes u which are adapted to X and satisfies u (t) ∈ [0, 1] for t ∈ [0, T ]. In general, solving the above stochastic control problem requires the solution of the associated Hamilton-Jacobi-Bellman (HJB) equation, which is a second order partial differential equation. Even for simple examples in one dimension, this can be rather involved.\nWe shall adopt an alternative route that is analytically tractable and still reveals key features of the optimal learning rate problem. Let us denote\nm (t) := E ‖X (t)− x∗‖2 , (49)\nand assume that we can derive from (47) a closed moment equation1\n{\nṁ (t) = F (u (t) ,m (t)) , m (0) = x20 (50)\nNow, instead of (48), we can consider the optimal control problem\nmin u∈U m (T ) subject to (50), (51)\nwhere U contains processes u which satisfy u (t) ∈ [0, 1] but are no longer dependent on individual sample paths X. When η is small, however, the fluctuations around expected values are small and hence we expect the problem (51) to closely approximate the problem (48). Now, solving (51) for simple examples is tractable as they involve only first order PDEs.\nFor concreteness, let us again consider the example d = 1, n = 2 and\nf (x) = 1\n2 (f1 (x) + f2 (x)) =\n1\n2\n( (x− 1)2 + (x+ 1)2 ) . (52)\nThe modified equation with controlled learning rate is\ndX (t) = −2u (t)X (t) dt+ 2√ηu (t) dB (t) , (53)\nand the moment equation is\nṁ (t) = F (u (t) ,m (t)) := 4 ( ηu (t) 2 − u (t)m (t) ) . (54)\nRecall that m (t) = EX2, since x∗ = 0. We now solve the optimal control problem (51) exactly. For 0 ≤ t ≤ T and m ≥ 0, Define the value function\nV (m, t) := min u∈U\n{m (T ) |ṁ (t) = F (u (t) ,m (t)) ,m (t) = m} . (55)\n1. This can always be done if f is quadratic, e.g. f (x) = ‖Ax− b‖2.\nThen, the solution of (51) is simply V ( x20, 0 )\n. By the dynamic programming principle, V satisfies the following HJB equation\n\n \n \nVt + min u∈[0,1] {F (u,m)Vm} = 0, V (m,T ) = m, V (0, t) = 0.\n(56)\nThe optimal control is given by\nu∗ = argmin u {F (u,m)Vm} (57)"
    }, {
      "heading" : "3.2.2 Exact Solution of the HJB",
      "text" : "First, we perform the minimization over u:\nmin u∈[0,1]\n{F (u,m)Vm} =\n\n \n \n−m2 η Vm m ≤ 2η, Vm ≥ 0, 4(η −m)Vm m > 2η, Vm ≥ 0 or m ≤ η, Vm < 0, 0 m > η, Vm < 0.\n(58)\nThe minimizing control is\nu∗ =\n\n \n  m 2η m ≤ 2η, Vm ≥ 0, 1 m > 2η, Vm ≥ 0 or m ≤ η, Vm < 0, 0 m > η, Vm < 0.\n(59)\nSubstituting (58) back into (56) yields a Hamilton-Jacobi equation that can be easily solved by method of characteristics. The solution is\nV (m, t) =\n\n  \n  \nmη η+m(T−t) m ≤ 2η, 2η 1+2(T−t∗(m)−t) m > 2η, 0 ≤ t < T − t∗ (m) , η + (m− η) e−4(T−t) m > 2η, T − t∗ (m) ≤ t ≤ T,\n(60)\nwhere\nt∗ (m) = 1\n4 log\n(\nm η − 1\n)\n. (61)\nObserve that Vm ≥ 0 for all m ≥ 0 and 0 ≤ t ≤ T , thus the corresponding optimal control can be found by first solving for the optimally controlled process m∗ in (54) with u = u∗ given by (59), and then substituting the result back into (59). We have\nm∗ (t) =\n\n  \n  \nηx20 η+x20t\nx0 ≤ √ 2η,\nη + ( x20 − η ) e−4t x0 > √ 2η, 0 ≤ t < t∗ ( x20 )\n, 2η\n1+2(t−t∗(x20)) x0 >\n√ 2η, t∗ (\nx20 )\n≤ t ≤ T, (62)\nNotice that by identifying t with kη, the optimal control derived above can be directly translated into a learning rate schedule in the SGA iteration. Let x0 > √ 2η, which represents the case where the initial guess is far from the minimum. The optimal control tells us that in order to minimize the error at the end of the run, we use the original learning rate until time k∗ = t∗/η, after which we decrease the learning rate by a factor of uk = 1/ (1 + 2η (k − k∗)) (see Figure 5, where we plot the optimally controlled error process m∗ and the optimal control u∗ for η = 0.05, x0 = 1, N = 50). Notice that k\n∗ is consistent with the transition time found in (22). The fact that we do not decrease the learning rate before the transition time is precisely because for k ≤ k∗, the drift of the SDE dominates the volatility and the classical SGA is very efficient. It is only after k∗ when the error starts to fluctuate that we have to decrease the learning rate. The optimal control then shows that the best way to do so is to decrease it like 1/ (1 + 2η (k − k∗)).\nWe see that for this example, the SME approach allows us to derive a precise optimal strategy for decreasing the learning rate. Despite its simplicity, the calculation reveals some key insights. In the literature, a learning rate schedule of O (1/k) is usually proposed to ensure convergence (Shamir and Zhang, 2012). Here, we showed that this scaling is asymptotically correct, but for optimal performance, we should only apply it after the transition time. Before then, we should use a constant learning rate. These insights can be used to optimize more general minimization problems such as the case of f = ‖Ax− b‖2, which we will discuss in Section 4."
    }, {
      "heading" : "3.3 Optimal mini-batch size",
      "text" : "Another approach to optimize the SGA is by applying mini-batch, where at each iteration we use more than one sample of the gradient ∇fk. This is essentially a variance reduction technique, where a larger batch size results in a lower variance. However, increasing the batch size also incurs computational overheads. We will employ the stochastic modified equation to find a balance between the two."
    }, {
      "heading" : "3.3.1 Optimal control setup",
      "text" : "The SGA iteration with mini-batching can be written as\nxk+1 = xk − η 1 1 + uk\n1+uk ∑\nj=1\n∇fγj ( xk ) , (64)\nwhere uk is a non-negative integer and hence 1+uk is the batch size at the current iteration. The corresponding stochastic modified equation is\ndX (t) = −∇f (X (t)) dt+ √\nη\n1 + u (t) σ (X (t)) dB (t) , (65)\nwhere u (t) ≥ 0 for all t ∈ [0, T ]. To perform exact calculations, we again consider the one dimensional example (52), for which the modified equation is\ndX (t) = −2X (t) dt+ 2 √\nη\n1 + u (t) dB (t) , (66)\nand the moment equation is\n{ ṁ = F (u (t) ,m (t)) := 4 (\nη 1+u −m\n)\n,\nm (0) = x20. (67)\nConsider the following optimal control problem\nmin u∈U\n{\nm (T ) + γ\nη\n∫ T\n0 u (s) ds\n}\nsubject to (67), (68)\nwhere U contains processes u such that u (t) ≥ 0. The constant γ measures the unit cost of introducing an extra gradient sample. Define the value function\nV (m, t) := min u∈U\n{\nm (T ) + γ\nη\n∫ T\nt\nu (s) ds ∣ ∣ ∣ ṁ (t) = F (u (t) ,m (t)) ,m (t) = m\n}\n. (69)\nThe corresponding HJB equation is \n  \n  \nVt +min u≥0\n{\nF (u,m)Vm + γ\nη u\n}\n= 0,\nV (m,T ) = m, V (0, t) = 0.\n(70)\nThe optimal control is given by\nu∗ = argmin u\n{\nF (u,m)Vm + γ\nη u\n}\n. (71)"
    }, {
      "heading" : "3.3.2 Exact Solution of the HJB",
      "text" : "Performing the minimization over u, we have\nmin u≥0\n{\nF (u,m)Vm + γ\nη u\n}\n=\n{\n4 (η −m)Vm Vm ≤ γ4η2 , 4 (√ γVm −mVm ) − γ η Vm > γ 4η2 , (72)\nwith\nargmin u\n{\nF (u,m)Vm + γ\nη u\n}\n=\n{\n0 Vm ≤ γ4η2 , 2η √\nVm/γ − 1 Vm > γ4η2 . (73)\nThe resulting Hamilton-Jacobi equation is solved by the method of characteristics. We have\nV (m, t) =\n\n \n \nη + (m− η) e−4(T−t) γ > 4η2, me−4(T−t) + (T − t) γ\nη + 2\n√ γ ( 1− e−2(T−t) )\nγ ≤ 4η2, T − t̃ < t ≤ T e−4(T−t) (m− η) + 2√γ + γ\nη\n( t̃− 34 )\nγ ≤ 4η2, 0 ≤ t ≤ T − t̃, , (74)\nwhere\nt̃ = 1\n4 log\n(\n4η2\nγ\n)\n. (75)\nWith the expression for V , we can obtain the optimal control and optimally controlled process by solving (73) and (67). We obtain\nu∗ (t) =\n\n \n \n0 γ > 4η2, 0 γ ≤ 4η2, 0 ≤ t < T − t̃, 2ηe2(t−T )/ √ γ − 1 γ ≤ 4η2, T − t̃ ≤ t ≤ T,\n(76)\nand\nm∗ (t) =\n\n \n \nη + ( x20 − η ) e−4t γ > 4η2, η + (\nx20 − η ) e−4t γ ≤ 4η2, 0 ≤ t ≤ T − t̃, √ γe2(T−t) − γ4ηe4(T−t) + ( x20 − η ) e−4t γ ≤ 4η2, T − t̃ < t ≤ T. (77)\nAs before, by setting t = kη we can obtain a precise batch-size strategy for the SGA. Consider the case γ ≤ 4η2, corresponding to the situation where the computational cost for adding gradient samples is not too large and mini-batching can be applied. Expression (76) says that we should not apply mini-batch for early times (k ≤ N − k̃ = ( T − t̃ ) /η). After N − k̃ steps, we apply mini-batch with a batch size that is exponentially increasing in k. See figure 6 for an illustration for η = 0.05, x0 = 1, N = 50. The final batch size is\n1 + u∗ (ηN) = 2η√ γ − 1. (78)\nIn particular, the optimal control result states that instead of using a batch size that is constant in time (as is often applied in practice, see for example, Krizhevsky et al. (2012); LeCun et al. (1989); Bengio (2012)), the better strategy is to perform an aggressive minibatching at the end. On a qualitative level, the latter makes sense because increasing batch size decreases the variance. However, the variance of the uncontrolled error process is O (η) at large times, thus with mini-batching only a finite amount of steps is required to decrease this variance to a desired magnitude. Any mini-batching before this is wasted computation. The optimal control solution makes these statements precise: N − k̃ is the exact time that we should start mini-batching and (76) gives the optimal batch size schedule. Applying the control in practice requires the value of the constant γ, which is not available in practice. What is available, however, is the amount of additional computation that we are willing to introduce, which in turn allows us to determine the value of γ by integrating (76) in time. See Section 4.4."
    }, {
      "heading" : "4. Application to General Linear Equation",
      "text" : "In this section, we translate the insights obtained thus far to practical procedures, and apply them to solving general linear equations, one of the most basic and important testbeds for SGA algorithms."
    }, {
      "heading" : "4.1 Dynamics",
      "text" : "The essential dynamical feature are the same as the example in Section 2.2. For general A,\n1\nn\n∑\ni\n∇fi(x)∇fi(x)T = 2\nn\n∑\ni\nai(a T i x− bi)2aTi\n∇f(x)∇f(x)T = 4 n2\nAT (Ax− b)(Ax− b)TA (79)\nThe SME is then: {\ndX = − 2 m AT (AX − b)dt+√ησ(X)dB, σ(x)σ(x)T = 1 n ∑ i∇fi(x)∇fi(x)T −∇f(x)∇f(x)T . (80)\nThe drift matrix and the volatility matrix do not commute in general, hence we cannot expect to diagonalize them simultaneously. To carry on the analysis, we need to make some approximations. We replace the volatility term by a constant matrix σ(x∗). This makes sense because eventually, Xt will be oscillating around x\n∗, σ(x∗) is therefore a first order approximation. In the beginning of the iterations, the drift term dominates, the error caused by approximating the volatility term is negligible.\nLet ATA = UΛUT be the eigenvalue decomposition, let X = UY + x∗ with ATAx∗ = AT b. We have\ndY = − 2 n ΛY dt+ √ ηUTσ(x∗)dB. (81)\nLet β = diag(UTσ(x∗)σ(x∗)TU) and λ = diag(Λ). The second moment equations are\ndEY 2i = − 4\nn λiEY\n2 i dt+ ηβidt (82)\nfor i = 1, · · · , d. We have\nEY 2i (t) = ηnβi 4λi + (y2i − ηnβi 4λi )e− 4λit n . (83)\nSubstitute Y = UT (X − x∗), we have\nE‖X − x∗‖2 = ∑\ni\nEY 2i . (84)\nThis expression gives both the initial exponential convergence rate and the eventual oscillation. In our numerical simulations, we found the above first order approximation is very accurate. Figure 7(a) shows an example where A ∈ R200×100, b ∈ R20, both with i.i.d N (0, 1) entries, η = 1e−5. The matrices are generated using MATLAB r2015b, with random seed 721. In this example, the predicted eventual variance is 0.000488; the actual variance is 0.000495. This demonstrates the validity of the SME approximation.\nNote that if the system Ax = b is consistent, first order approximation of σ(Xt) at x ∗ gives zero matrix, second order approximation gives quadratic terms, corresponding to the geometric Brownian motion type behavior. Therefore, as long as it converges, it converges linearly all the way to optimum."
    }, {
      "heading" : "4.2 Momentum",
      "text" : "Now, let us study the SGA with momentum applied to solving the inconsistent equation Ax = b. In particular, we demonstrate the choice of optimum µ. Using the same same notation, we write X = UY + x∗, V = UZ. Then the second moment equations become\n\n \n \ndY 2i = 2 η YiZidt dYiZi = (− 2nλiY 2i + µ−1 η YiZi + 1 η Z2i )dt dZ2i = (− 4nλiYiZi + 2(µ−1) η Z2i + ηβi)dt\n(85)\nfor i = 1, · · · , d. Setting the right hand size to zero, we see that EY 2i = βiηn4λi(µ−1) . Hence\nE‖Xt − x∗‖2 = ∑\ni\nβiηn\n4λi(µ− 1) . (86)\nThis characterizes the variance. The optimal µ∗ is given by\nµ∗ = √ n− 2√2δη√\nn , δ = λmin. (87)\nFigure 8(a) shows a comparison of performance with optimal µ and other choices. Choosing a smaller µ gives slower initial convergence, but the final oscillation is also small. Choosing the optimal µ gives best initial convergence speed, but the eventual oscillation is also larger. Choosing larger µ is the worst, it gives slower initial convergence and larger eventual oscillation. Figure 8(b) shows the performance of different choices of µ around µ∗. Each dot in the figure represents the average of the mean square error at the end of 25 iterations, which is about the transition time for the µ∗. The average is taken over 200 trials. As can be seen, the performance is very sensitive to the choices of µ. Choosing smaller or larger values than µ∗ both lead to inferior performance. From a practical point of view, when the optimal µ cannot be reliably estimated, it is safer to use smaller momentum.\nOur previous analysis shows that for the optimal µ∗, if we run SGA without momentum but with a learning rate which is the base learning rate multiplied by 1/(1 − µ∗), both the initial convergence rate and the eventual oscillation will be similar. Figure 9 shows this phenomenon."
    }, {
      "heading" : "4.3 Learning rate schedule",
      "text" : "Here, we apply the insights gained from Section 3.2 to select the optimal learning rate. Let us define the total error\nm (t) := E ‖X (t)− x∗‖2 .\nSumming expression (82) over i, we have\ndm = − 4 n\nd ∑\ni=1\nλiEY 2 i dt+ ηβdt, (88)\nwhere β = Tr ( σ (x∗)σT (x∗) )\n. Let δ denote the smallest singular value of A, then we have the bound\nṁ (t) ≤ −4δ n m (t) + ηβ. (89)\nFor large times, the error is dominated by the eigenmode associated with δ. Hence the above bound is in fact a good approximation. Thus, we consider the following moment equation\n{\nṁ (t) = −4δ n m (t) + ηβ, m (0) = m0, (90)\nwhere m0 := ‖x0 − x∗‖2. The relevant moment equation for a time varying learning rate is (c.f. (54))\n{\nṁ (t) = −4δ n m (t)u (t) + ηβu (t)2 , m (0) = m0 (91)\nThus, an optimal learning rate schedule can be found by considering the optimal control problem\nmin u∈U m (T ) subject to (91). (92)\nFollowing the procedure outlined in Section 3.2, we can solve the associated HJB equation to obtain the optimal control\nu∗ (t) =\n{\n1 0 ≤ t < t∗ (m0) , n\nn+2δ(t−t∗(m0)) t ∗ (m0) ≤ t ≤ T,\n(93)\nwhere\nt∗ = n\n4δ log\n(\n4δm0 nβη − 1 ) . (94)\nFor simplicity, we have only considered the case where m0 is large (m0 > βηn 2δ ). This is usually the relevant case for relatively well conditioned A and generic initial guesses x0. To apply the control (93), we have to first estimate2 m0 and β since they depend on the unknown quantity x∗. Observe that f is strongly convex with parameter δ. Hence we have\nm0 = ‖x0 − x∗‖2\n≤ 1 δ (∇f (x0)−∇f (x∗))T (x0 − x∗) ≤ 1 2 ‖x0 − x∗‖2 + 1 2δ2 ‖∇f (x0)‖2 . (95)\n2. We stress here that we are only looking for a rough estimate, as we have checked that the performance is not very sensitive to these estimates.\nHence, m0 ≤ 1δ2 ‖∇f (x0)‖ 2 and we use the latter as an estimate of the former. Note that a slight over-estimate of t∗ is not detrimental to performance. To estimate β, recall that\nβ = Tr\n\n\n2\nn\nn ∑\nj=1\naja T j\n( aTj x ∗ − bj\n)2\n\n . (96)\nFor generic A and x∗, we assume that each aTj x ∗ − bj is of order 1, and hence, we have\nβ ≈ 2 n TrATA. (97)\nUsing these estimates, we can apply the optimal control to the minimization problem and the result is shown in Figure 10, where we fixed d = 100, n = 200, η = 0.001. We see that the performance of the controlled process is superior to both the uncontrolled process and the badly controlled process, where we immediately apply the 1/k schedule on the learning rate at the beginning of the SGA."
    }, {
      "heading" : "4.4 Mini-batch schedule",
      "text" : "The moment equation for mini-batch control is\nṁ (t) = −4δ n m (t) +\nηβ\n1 + u (t) , (98)\nand the corresponding optimal control problem is {\nminu∈U\n{\nm (T ) + γ η ∫ T 0 u (s) ds }\nsubject to (98) and m (0) = m0. (99)\nWe shall only consider the case where γ is sufficiently small (γ ≤ βη2) so that mini-batching is desirable. Solving the HJB, we obtain the optimal control\nu∗ (t) =\n{\n0 0 ≤ t < T − t̃, η √ β/γe 2δ n (t−T ) − 1 T − t̃ ≤ t ≤ T,\n(100)\nwhere\nt̃ = n\n4δ log\n(\nβη2\nγ\n)\n(101)\nWe can estimate β as before, but we also have to determine γ. To do this, we first fix M to be the number of gradient samples we are going to use in addition to the usual SGA. Thus M measures the computational overhead. Now, it is easy to see that\nM = 1\nη\n∫ T\n0 u∗ (s) ds =\nn\n4δη2\n(\nlog\n(\nγ\nβη2\n)\n+ 2η\n√\nβ γ − 2\n)\n. (102)\nPut γ = ǫη2 with ǫ ≪ 1. Then,\nM = n\n2δη2\n( √\nβ ǫ − 1\n)\n+ n\n4δη2 log\n(\nǫ\nβ\n)\n. (103)\nFor small ǫ, the first term dominates. Hence, we can express γ (via ǫ) as a function of M . We obtain\nγ = βη2n2\n(2δηM + n)2 . (104)\nHence,\nu∗ (t) =\n{\n0 0 ≤ t < T − t̃, (1 + 2δηM\nn )e\n2δ(t−T ) n − 1 T − t̃ ≤ t ≤ T,\n(105)\nand\nt̃ = n\n2δ log\n(\n1 + 2δηM\nn\n)\n. (106)\nApplying (105) to the problem of minimizing f = ‖Ax− b‖2, we see that this mini-batching schedule out-performs the schedule where batch size is constant in time, while having the same overhead M . See Figure 11, where d = 100, n = 200, η = 0.001,M = 106. Note that due to the setup of the control problem, we have fixed the total running time N , as well as the total overhead M . Thus, we see in Figure 11 that the optimally controlled error process has a flat region approximately between iterations 100 to 370. This is of course not optimal from the practical point of view, since the computations in this region are wasted. In practice, we should instead apply mini-batch right after k∗, the transition point of the SGA. In this case, it is approximately at iteration 100. To obtain this type of solutions, we have to consider a optimal control problem with variable end times, which is analytically more involved. However, we see that by combining the insights from section 4.3 it is not hard to devise such controls on a heuristic level."
    }, {
      "heading" : "5. Discussion",
      "text" : "In this paper, we have introduced the stochastic modified equations approach to analyzing stochastic gradient algorithms. For simple prototypical examples, this allows us to perform exact calculations that quantify the precise stochastic dynamics of the SGA. The salient features can be summarized as follows:\n• At small times, the SGA iteration is very efficient since in the corresponding SME, the drift dominates the volatility. In this regime, for generic problems one expects the SGA to be as fast as GD, but the latter involves n times the computational cost (due to gradient evaluations), where n is the number of functions that make up the objective function f .\n• When the error is O(η), we enter a regime where the SGA fails to further decrease the objective function due to variance. The SME reflects this as the volatility dominating the drift.\n• We call the division between the two regimes the transition time of the SGA. Past this time, various variance reduction techniques should be applied.\nWith regards to the last point, we saw that the SME is also a useful framework to study various speed-up techniques. Previously, the analysis of these techniques is mostly restricted\nto heuristic arguments. In Sections 3 and 4, we studied three such modifications to the plain SGA, namely adding momentum, adjusting the learning rate and adjusting the mini-batch size. The key findings are:\n• Momentum. Adding momentum improves the initial convergence of the SGA, but introduces a higher asymptotic variance. The parameter µ must be optimally chosen, and the performance of the momentum SGA depends sensitively on the relationship between µ and the learning rate η. SGA with momentum µ∗ has the same order of convergence rate as the plan SGA with learning rate η/(1− µ∗).\n• Learning rate. Using optimal control, we derived exactly the optimal strategy for setting the learning rate in simple examples: we wait until the transition time of the SGA, after which we decrease the learning rate with a schedule that is asymptotically O (1/k). This is shown to be superior to following a 1/k schedule from the beginning, as proposed in some literature.\n• Mini-batch size. Again, using optimal control we showed that the best mini-batch strategy is to only perform an aggressive batch-size increment towards the end of the SGA run. The batch-size schedule and the duration of mini-batching can be determined from η and the prescribed allowed overhead M . We showed that the above strategy is superior to the usual approach where a constant mini-batch size is applied throughout.\nThe phenomenon described above is expected to be generic for problems beyond the scope of linear equations, and our analysis provides useful guidelines for designing optimal speed-up strategies.\nWe stress here that the application of the SME methodology is not limited to the scope presented in this paper. It can be used to study and improve other modifications to the plain stochastic gradient algorithm as well. For example, the SVRG algorithm (Johnson and Zhang, 2013) can be formulated as a time-delayed SME. On a broader perspective, we expect the SME approach to be useful in analyzing other stochastic algorithms, much like how modified equations are ubiquitous in traditional numerical analysis.\nOn the theoretical side, it is also interesting to see whether the SME approximation provides an alternative approach to proving rigorous results for the SGA. For instance, the asymptotic convergence properties of the SGA can be translated to large time properties of the SME. The latter is well-studied in the stochastic differential equations literature, and this approach may yield stronger theorems compared to the current collection of rigorous results for the SGA. For example, little is known about the properties of the SGA when applied to non-convex objective functions, as the tools used to establish rigorously results in the literature mostly relied on convexity. However, the SME approach may surmount this difficulty, since the invariant properties of SDE with drift derived from a non-convex potential can be readily posed as escape problems (Freidlin et al., 2012). These issues are worthy of exploration in future works."
    } ],
    "references" : [ {
      "title" : "Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n)",
      "author" : [ "Francis Bach", "Eric Moulines" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bach and Moulines.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bach and Moulines.",
      "year" : 2013
    }, {
      "title" : "Practical recommendations for gradient-based training of deep architectures",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2012
    }, {
      "title" : "The stability properties of a coupled pair of non-linear partial difference equations",
      "author" : [ "Bart J Daly" ],
      "venue" : "Mathematics of Computation,",
      "citeRegEx" : "Daly.,? \\Q1963\\E",
      "shortCiteRegEx" : "Daly.",
      "year" : 1963
    }, {
      "title" : "Random perturbations of dynamical systems, volume 260",
      "author" : [ "Mark I Freidlin", "Joseph Szücs", "Alexander D Wentzell" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Freidlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Freidlin et al\\.",
      "year" : 2012
    }, {
      "title" : "Heuristic stability theory for finite-difference equations",
      "author" : [ "CW Hirt" ],
      "venue" : "Journal of Computational Physics,",
      "citeRegEx" : "Hirt.,? \\Q1968\\E",
      "shortCiteRegEx" : "Hirt.",
      "year" : 1968
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "Eric Moulines", "Francis R Bach" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Moulines and Bach.,? \\Q2011\\E",
      "shortCiteRegEx" : "Moulines and Bach.",
      "year" : 2011
    }, {
      "title" : "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm",
      "author" : [ "Deanna Needell", "Rachel Ward", "Nati Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Needell et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Needell et al\\.",
      "year" : 2014
    }, {
      "title" : "Difference methods and the equations of hydrodynamics",
      "author" : [ "WF Noh", "MH Protter" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Noh and Protter.,? \\Q1960\\E",
      "shortCiteRegEx" : "Noh and Protter.",
      "year" : 1960
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2014
    }, {
      "title" : "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
      "author" : [ "Ohad Shamir", "Tong Zhang" ],
      "venue" : "arXiv preprint arXiv:1212.1824,",
      "citeRegEx" : "Shamir and Zhang.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shamir and Zhang.",
      "year" : 2012
    }, {
      "title" : "A randomized kaczmarz algorithm with exponential convergence",
      "author" : [ "Thomas Strohmer", "Roman Vershynin" ],
      "venue" : "Journal of Fourier Analysis and Applications,",
      "citeRegEx" : "Strohmer and Vershynin.,? \\Q2009\\E",
      "shortCiteRegEx" : "Strohmer and Vershynin.",
      "year" : 2009
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 30th international conference on machine learning",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "The modified equation approach to the stability and accuracy analysis of finite-difference methods",
      "author" : [ "RF Warming", "BJ Hyett" ],
      "venue" : "Journal of computational physics,",
      "citeRegEx" : "Warming and Hyett.,? \\Q1974\\E",
      "shortCiteRegEx" : "Warming and Hyett.",
      "year" : 1974
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Lin Xiao", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Xiao and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiao and Zhang.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.",
      "startOffset" : 91,
      "endOffset" : 241
    }, {
      "referenceID" : 8,
      "context" : "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.",
      "startOffset" : 91,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.",
      "startOffset" : 91,
      "endOffset" : 241
    }, {
      "referenceID" : 16,
      "context" : "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.",
      "startOffset" : 91,
      "endOffset" : 241
    }, {
      "referenceID" : 11,
      "context" : "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.",
      "startOffset" : 91,
      "endOffset" : 241
    }, {
      "referenceID" : 0,
      "context" : "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.",
      "startOffset" : 91,
      "endOffset" : 241
    }, {
      "referenceID" : 3,
      "context" : "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974).",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974).",
      "startOffset" : 32,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974).",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974).",
      "startOffset" : 69,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974).",
      "startOffset" : 69,
      "endOffset" : 254
    }, {
      "referenceID" : 2,
      "context" : "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974). The method of modified equations consists of creating a differential equation which approximates the given numerical scheme more accurately than the original equation.",
      "startOffset" : 69,
      "endOffset" : 280
    }, {
      "referenceID" : 12,
      "context" : "Strohmer and Vershynin (2009); Needell et al.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "Strohmer and Vershynin (2009); Needell et al. (2014). Introducing the relaxation, the Kaczmarz iteration rule is x = x + α bγk − 〈aγ k , xk〉 ‖aγ‖2 a γk .",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "It has been empirically observed that SGA with momentum accelerates convergence, see for example, Sutskever et al. (2013). We study the effects of momentum using the SME approach.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : "In the literature, a learning rate schedule of O (1/k) is usually proposed to ensure convergence (Shamir and Zhang, 2012).",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "In particular, the optimal control result states that instead of using a batch size that is constant in time (as is often applied in practice, see for example, Krizhevsky et al. (2012); LeCun et al.",
      "startOffset" : 160,
      "endOffset" : 185
    }, {
      "referenceID" : 5,
      "context" : "In particular, the optimal control result states that instead of using a batch size that is constant in time (as is often applied in practice, see for example, Krizhevsky et al. (2012); LeCun et al. (1989); Bengio (2012)), the better strategy is to perform an aggressive minibatching at the end.",
      "startOffset" : 160,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : "(1989); Bengio (2012)), the better strategy is to perform an aggressive minibatching at the end.",
      "startOffset" : 8,
      "endOffset" : 22
    } ],
    "year" : 2017,
    "abstractText" : "Stochastic gradient algorithms (SGA) are increasingly popular in machine learning applications and have become “the algorithm” for extremely large scale problems. Although there are some convergence results, little is known about their dynamics. In this paper, We propose the method of stochastic modified equations (SME) to analyze the dynamics of the SGA. Using this technique, we can give precise characterizations for both the initial convergence speed and the eventual oscillations, at least in some special cases. Furthermore, the SME formalism allows us to characterize various speed-up techniques, such as introducing momentum, adjusting the learning rate and the mini-batch sizes. Previously, these techniques relied mostly on heuristics. Besides introducing simple examples to illustrate the SME formalism, we also apply the framework to improve the relaxed randomized Kaczmarz method for solving linear equations. The SME framework is a precise and unifying approach to understanding and improving the SGA, and has the potential to be applied to many more stochastic algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}