{
  "name" : "1510.08520.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n08 52\n0v 1\n[ cs\n.L G\n] 2\n8 O\nct 2\n01 5"
    }, {
      "heading" : "1. Introduction",
      "text" : "Clustering is a common unsupervised data analysis method which partitions data into a set of self-similar clusters. The obtained data clusters always play an important role in solving various machine learning and computer vision problems by the disclosed grouping patters in the original data. Most clustering algorithms can be categorized into two classes: similarity-based and model-based clustering methods. With Gaussian Mixture Model (GMM) as a representative, model-based clustering methods typically model the data by a mixture of parametric distributions, and the parameters of the distributions are estimated via fitting a statistical model to the data [10]. However, high dimensionality always impose difficulty on the parameter estimation, and the fact that the used parametric distribution may not match the underlying true distribution of the data further restricts the application of model-based methods to complex data.\nIn contrast, similarity-based clustering methods segment the data based on the similarity function, and they alleviate the difficult problem of parameter estimation in case of high dimensionality. For example, K-means [6] groups similar data together by a local minimum of sum of within-cluster dissimilarities. Affinity Propagation [11] uses the same\nprinciple and automatically determines the cluster number. Spectral Clustering [16] identifies clusters of complex shapes lying on some low dimensional manifolds by spectral embedding. Among various similarity-based clustering methods, graph-based methods [18] are important wherein the edge weight of the graph serving as the data similarity, and sparse graphs (where only a few edges have non-zero weights for each vertex) are demonstrated to be especially effective for clustering high dimensional data. Examples of sparse graph methods include ℓ1-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation. ℓ1-graph and sparse subspace clustering have been shown to be robust to noise and capable of producing superior results for high dimensional data, compared to K-means and spectral clustering. ℓ1-graph is further extended to incorporate local manifold structure of the data in [21, 22].\nTo avoid the non-convex optimization problem incurred by ℓ0-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces ℓ0-norm with ℓ1-norm so as to solve a convex optimization problem. In addition, ℓ1-norm has been widely used as a convex relaxation of ℓ0-norm for efficient sparse coding algorithms [12, 13, 14]. [9] points out that in case that the data are drawn from linear independent subspaces, sparse representation by ℓ1-norm can recover the underlying subspaces.\nOn the other hand, sparse representation methods [15] that directly optimize objective function involving ℓ0-norm demonstrate compelling performance compared to its ℓ1norm counterpart. In order to deal with general cases when the subspaces are not independent of each other and follow the original principle of sparse representation by ℓ0norm, we propose a new sparse graph called ℓ0-graph which employs ℓ0-norm to enforce the sparsity of the graph, and develop a proximal method to optimize the associated objective function with convergence guarantee. The proximal method is inspired by the proximal linearized method in [3].\nThe remaining parts of the paper are organized as follows. Sparse coding and ℓ1-graph are introduced in the next subsection, and then the detailed formulation of ℓ0-graph is illustrated. We then show the clustering performance of ℓ0graph, and conclude the paper. We use bold upper letters for matrices and vectors, and regular lower letter for scalars\n1\nthroughout this paper."
    }, {
      "heading" : "1.1. Sparse Coding and ℓ1-Graph for Clustering",
      "text" : "Sparse coding methods represent an input signal by a linear combination of only a few atoms of a dictionary which is usually over-complete, and the sparse coefficients are named sparse code. Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering. Denote the data by X = [x1,x2, . . . ,xn] which lie in the d-dimensional Euclidean space IRd, and let the dictionary matrix be D = [d1,d2, . . . ,dp] ∈ IRd×p with each dm (m = 1, . . . , p) being the atom or the basis vector of the dictionary. Sparse coding method searches for the linear sparse representation with respect to the dictionary D for each vector xi. Sparse coding is performed by solving the following convex optimization problem:\nα i = argmin\nα i ‖xi −Dα i‖22 + λ||α i||1 i = 1, . . . , n\n(1) where λ is a weighting parameter for the sparsity of αi.\nℓ1-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes. In sparse subspace clustering [9], the authors pointed out that such sparse representation for each datum recovers the underlying subspaces from which the data are generated. With the data X = [x1, . . . ,xn] ∈ IRd×n, it is mentioned in [7, 9] that the following sparse representation for each data point by ℓ0-norm\nmin α i\n‖αi‖0 s.t. xi = Xα i (2)\ncan effectively recovers the subspace Si that the point xi belongs to, and the non-zero elements of the sparse code αi correspond to the data points that also lie in the subspace Si. Since (2) is a non-convex problem and NP-hard, sparse subspace clustering method replaces the ℓ0-norm with ℓ1norm in (2) and solves the following convex optimization problem instead:\nmin α i\n‖αi‖1 s.t. xi = Xα i (3)\nBy solving almost the same optimization problem as (3), ℓ1graph obtains the robust sparse representation for each data point by solving the ℓ1-norm optimization problem below:\nmin α i\n‖αi‖1 s.t. xi = Aα i i = 1, . . . , n (4)\nwhich is equivalent to\nmin α i\n‖xi −Aα i‖22 + λℓ1‖α i‖1 i = 1, . . . , n (5)\nfor some weighting parameter λℓ1 > 0, and A = [X Id], Id is a d × d identity matrix, αi ∈ IR(n+d)×1, α = [α1, . . . ,αn] ∈ IR(n+d)×n is the coefficient matrix with the element αij = α j i . To avoid trivial solution that α = [In 0] T , the diagonal elements of α are enforced to be zero, i.e. αii = 0 for 1 ≤ i ≤ n. Let G = (X,W) be the ℓ1-graph where X is the set of vertices, W is the graph weight matrix and Wij indicates the similarity between xi and xj , ℓ1-graph builds the n× n similarity matrix W by the sparse codes:\nWij = (|αij |+ |αji|)/2 1 ≤ i, j ≤ n (6)\nℓ1-graph then performs spectral clustering on the sparse similarity matrix W to partition the data. In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9]. Therefore, in this case W exhibits block-diagonal structure with a proper perturbation of the data and spectral clustering on W can effectively segment the data in accordance with the underlying subspaces. Extensive empirical study shows that ℓ1-graph achieves much better performance than spectral clustering on widely used Gaussian kernel graph. Moreover, it is clear that the pairwise similarity matrix (6) constructed by the coefficient matrix α leads to the superior performance of ℓ1-graph based clustering.\nIn the following section, we propose ℓ0-graph, which follows the original principle of sparsely representing each data point by other data using ℓ0-norm as in (2)."
    }, {
      "heading" : "2. Formulation of ℓ0-Graph",
      "text" : "As mentioned in the previous section, ℓ1-norm is used in ℓ1-graph and sparse subspace clustering as a relaxation of the ℓ0-norm for the sparsity of the graph. To handle the general cases when the subspaces are not independent and comply to the original sparse representation using ℓ0-norm, ℓ0-graph is proposed. The objective function of ℓ0-graph is obtained by using ℓ0-norm instead of ℓ1-norm in (5):\nmin α\nL(α) = ‖X −Aα‖2F + λ‖α‖0 s.t. αii = 0, i = 1, . . . , n\n(7)\nwhere ‖ · ‖F is the Frobenius norm of a matrix and ‖ · ‖0 indicates the ℓ0-norm that counts the number of nonzero elements in a vector or matrix. Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to ℓ0-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative. In the following text, the superscript with bracket indicates the iteration number of the proposed proximal method.\nIn t-th iteration of our proximal method for t ≥ 1, gradient descent is performed on the square loss term of (7), i.e.\nQ(α) = ‖X −Aα‖2F , to obtain\nα̃ (t) = α(t−1) −\n2\nγs (ATAα(t−1) −ATX) (8)\nwhere γ > 1 is a constant and s is the Lipschitz constant for the gradient of function Q(·), namely\n‖∇Q(Y)−∇Q(Z)‖F ≤ s‖Y − Z‖F , ∀Y,Z ∈ IR (n+d)×n\n(9)\nThen α(t) is obtained as the solution to the following ℓ0\nregularized problem:\nα (t) = argmin\nv∈IR(n+d)×n\nγs\n2 ‖v − α̃(t)‖2F + λ‖v‖0 (10)\ns.t. vii = 0, i = 1, . . . , n\nIt can be verified that (10) has closed-form solution, i.e.\nα (t) ij =\n{\n0 : |α̃ (t) ij | <\n√\n2λ γs or i = j\nα̃ (t) ij : otherwise\n(11)\nThe iterations start from t = 1 and continue until the sequence {L(α(t))} converges or maximum iteration number is achieved. We initialize α as α(0) = αℓ1 and αℓ1 is the sparse codes generated by ℓ1-graph by solving (5) with some proper weighting parameter λℓ1 . In all the experimental results shown in the next section, we empirically set λℓ1 = 0.1 when initializing ℓ0-graph.\nThe data clustering algorithm by ℓ0-graph is described in Algorithm 1. Also, the following theorem shows that each iteration of the proposed proximal method decreases the value of the objective function L(·) in (7), therefore, our proximal method always converges.\nTheorem 1. Let s = 2σmax(ATA) where σmax(·) indicates the largest eigenvalue of a matrix, then the sequence {L(αt)} generated by the proximal method with (8) and (11) decreases, and the following inequality holds for t ≥ 1:\nL(αt) ≤ L(αt−1)− (γ − 1)s\n2 ‖α(t) −α(t−1)‖2F (12)\nAnd it follows that the sequence {L(α(t))} converges.\nThe proof of Theorem 1 is shown in the Appendix. Furthermore, we show that if the sequence {αt} generated by the proposed proximal method is bounded (which often holds in practice, and it always holds in our experiments), then it is a Cauchy sequence and it converges to a critical point of the objective function L in (7).\nTheorem 2. Suppose that the sequence {αt} generated by the proximal method with (8) and (11) is bounded, then 1)\n∞ ∑ t=1 ‖αt − αt−1‖F < ∞ 2) {αt} converges to a critical point 1 of the function L(·) in (7).\nSketch of the Proof. [3] shows that the ℓ0-norm function ‖ · ‖0 is a semi-algebraic function. The conclusions of this theorem directly follows from Theorem 1 in [3].\nAlgorithm 1 Data Clustering by ℓ0-Graph Input:\nThe data set X = {xi}ni=1, the number of clusters c, the parameter λ for ℓ0-graph, λℓ1 for the initialization of the the ℓ0-graph, maximum iteration numberM , stopping threshold ε 1: t = 1, initialize the coefficient matrix as α(0) = αℓ1 , s = 2σmax(A T A). 2: while t ≤ M do 3: Obtain α(t) from α(t−1) by (8) and (11) 4: if |L(α(t))− L(α(t−1))| < ε then 5: print 6: else 7: t = t+ 1. 8: end if 9: end while 10: Obtain the optimal coefficient matrix α∗ when the above iterations converge or maximum iteration number is achieved. 11: Build the pairwise similarity matrix by symmetrizing\nα ∗: W∗ = |α\n∗|+|α∗|T\n2 , compute the corresponding\nnormalized graph Laplacian L∗ = (D∗)− 1 2 (D∗ − W ∗)(D∗)− 1 2 , where D∗ is a diagonal matrix with\nD ∗ ii =\nn ∑\nj=1\nW ∗ ij\n12: Construct the matrix v = [v1, . . . ,vc] ∈ IRn×c, where {v1, . . . ,vc} are the c eigenvectors of L∗ corresponding to its c smallest eigenvalues. Treat each row of v as a data point in IRc, and run K-means clustering method to obtain the cluster labels for all the rows of v. Output: The cluster label of xi is set as the cluster label of the i-th row of v, 1 ≤ i ≤ n."
    }, {
      "heading" : "3. Experimental Results",
      "text" : "The superior clustering performance of ℓ0-graph is demonstrated in this section with extensive experimental results. We compare our ℓ0-graph to K-means (KM), Spectral Clustering (SC), ℓ1-graph, Sparse Manifold Clustering and Embedding (SMCE). Moreover, we derive the OMP-graph, which builds the sparse graph in the same way as ℓ0-graph\n1x is a critical point of function f if 0 ∈ ∂f(x), where ∂f(x) is the limiting-subdifferential of f at x. Please refer to more detailed definition in [3].\nexcept that it solves the following optimization problem by Orthogonal Matching Pursuit (OMP) to obtain the sparse code:\nmin α i\n‖xi −Aα i‖2F s.t. ‖α i‖0 ≤ T,α i i = 0, i = 1, . . . , n\n(13)\nℓ0-graph is also compared to OMP-graph to show the advantage of the proposed proximal method in the previous section. By adjusting the parameters and settings, ℓ1-graph and sparse subspace clustering generate equivalent results, so we omit the performance of sparse subspace clustering\nin the comparison."
    }, {
      "heading" : "3.1. Evaluation Metric",
      "text" : "Two measures are used to evaluate the performance of the clustering methods, i.e. the accuracy and the Normalized Mutual Information(NMI) [25]. Let the predicted label of the datum xi be ŷi which is produced by the clustering method, and yi is its ground truth label. The accuracy is\ndefined as\nAccuracy = 1IΩ(ŷi) 6=yi\nn (14)\nwhere 1I is the indicator function, and Ω is the best permutation mapping function by the Kuhn-Munkres algorithm\n[17]. The more predicted labels match the ground truth ones, the more accuracy value is obtained.\nLet X̂ be the index set obtained from the predicted labels {ŷi} n i=1 and X be the index set from the ground truth labels\n{yi} n i=1. The mutual information between X̂ and X is\nMI(X̂, X) = ∑\nx̂∈X̂,x∈X\np(x̂, x)log2( p(x̂, x)\np(x̂)p(x) ) (15)\nwhere p(x̂) and p(x) are the margined distribution of X̂ and X respectively, induced from the joint distribution p(x̂, x) over X̂ and X . Let H(X̂) and H(X) be the entropy of X̂ and X , then the normalized mutual information (NMI) is defined as below:\nNMI(X̂,X) = MI(X̂,X)\nmax{H(X̂),H(X)} (16)\nIt can be verified that the normalized mutual information takes values in [0, 1]. The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25]."
    }, {
      "heading" : "3.2. Clustering on UCI Data Sets and MNIST Handwritten Digits Database",
      "text" : "In this subsection, we conduct experiments on two real data sets from UCI machine learning repository [1], i.e. Heart and Ionosphere, as well as the MNIST database of handwritten digits. The three data sets are summarized in Table 5. MNIST handwritten digits database has a total number of 70000 samples for digits from 0 to 9. The digits are normalized and centered in a fixed-size image. For MNIST data set, we randomly select 500 samples for each digit to obtain a subset of MNIST data consisting of 5000 samples. The random sampling is performed for 10 times and the average clustering performance is recorded. The clustering results on these three data sets are shown in Table 1."
    }, {
      "heading" : "3.3. Clustering On COIL-20 and COIL-100 Database",
      "text" : "COIL-20 Database has 1440 images of 20 objects in which the background has been removed, and the size of each image is 32×32, so the dimension of this data is 1024. COIL-100 Database contains 100 objects with 72 images of size 32 × 32 for each object. The images of each object were taken 5 degrees apart when the object was rotated on a turntable. The clustering results on these two data sets are shown in Table 2 and Table 3 respectively. We observe that ℓ0-graph performs consistently better than all other competing methods. On COIL-100 Database, SMCE renders\nslightly better results than ℓ1-graph on the entire data due to its capability of modeling non-linear manifolds."
    }, {
      "heading" : "3.4. Clustering On Extended Yale Face Database B",
      "text" : "The Extended Yale Face Database B contains face images for 38 subjects with 64 frontal face images taken under different illuminations for each subject. The clustering results are shown in Table 4. We can see that ℓ0-graph achieves significantly better clustering result than ℓ1-graph, which is the second best method on this data."
    }, {
      "heading" : "3.5. Parameter Setting",
      "text" : "We use the sparse codes generated by ℓ1-graph with weighting parameter λℓ1 = 0.1 in (5) to initialize ℓ0-graph, and set λ = 0.5 for ℓ0-graph empirically throughout all the experiments in this section, and we observe that the average number of non-zero elements of the sparse code for each data point is around 3 for most data sets. The maximum iteration number M = 100 and the stopping threshold ε = 10−6. For OMP-graph, we tune the parameter T in (13) to control the sparsity of the generated sparse codes such that the aforementioned average number of non-zero elements of the sparse code matches that of ℓ0-graph. For ℓ1-graph, the weighting parameter for the ℓ1-norm is chosen from [0.1, 1] for the best performance.\nWe investigate how the clustering performance on the Extended Yale Face Database B and COIL-20 Database changes by varying the weighting parameter λ for ℓ0-graph, and illustrate the result in Figure 1 and Figure 2 respectively. We observe that the performance of ℓ0-graph is much better than other algorithms over a relatively large range of λ, revealing the robustness of our algorithm with respect to the weighting parameter λ."
    }, {
      "heading" : "4. Conclusion",
      "text" : "We propose a novel ℓ0-graph for data clustering in this paper. In contrast to the existing sparse graph methods such as ℓ1-Graph that uses ℓ1-norm as a relaxation of the ℓ0-norm, ℓ0-graph enforces the sparsity of the constructed graph by ℓ0-norm and optimizes the objective function using a proposed proximal method. Convergence of this proximal method is proved, and extensive experimental results on various real data sets demonstrate the effectiveness and superiority of ℓ0-graph over other competing methods."
    }, {
      "heading" : "5. Appendix",
      "text" : "Proof of Theorem 1. First of all, when s be 2 times the maximum eigenvalue of ATA, then s is the Lipschitz constant for the gradient of function Q. To see this, we have ∇Q(Y) = 2(ATAY −ATX), and\n‖Q(Y)−∇Q(Z)‖F = 2‖A T A(Y − Z)‖F (17)\n≤ 2σmax(A T A) · ‖(Y − Z)‖F\nSince α(t) = argmin v∈IR(n+d)×n,diag(v)=0 γs 2 ‖v − α̃ (t)‖2F + λ‖v‖0,\nγs\n2 ‖α(t) − α̃(t)‖2F + λ‖α (t)‖0 (18)\n≤ γs 2 ‖ ∇Q(α(t−1)) γs ‖2F + λ‖α (t−1)‖0\nwhich is equivalent to\n〈∇Q(α(t−1)),α(t) −α(t−1)〉+ γs\n2 ‖α(t) −α(t−1)‖2F + λ‖α (t)‖0\n(19)\n≤ λ‖α(t−1)‖0\nAlso, since s is the Lipschitz constant for ∇Q,\nQ(α(t)) ≤ Q(α(t−1)) + 〈∇Q(α(t−1)),α(t) −α(t−1)〉 (20)\n+ s\n2 ‖α(t) −α(t−1)‖2F\nCombining (19) and (21), we have\nQ(α(t)) + λ‖α(t)‖0 ≤ Q(α (t−1)) + λ‖α(t−1)‖0 (21)\n− (γ − 1)s\n2 ‖α(t) −α(t−1)‖2F\nAnd (12) is verified. Since the sequence {L(α(t))} is deceasing with lower bound 0, it must converge."
    } ],
    "references" : [ {
      "title" : "L0 norm based dictionary learning by proximal methods with global convergence",
      "author" : [ "C. Bao", "H. Ji", "Y. Quan", "Z. Shen" ],
      "venue" : "2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 3858–3865",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Proximal alternating linearized minimization for nonconvex and nonsmooth problems",
      "author" : [ "J. Bolte", "S. Sabach", "M. Teboulle" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Learning with l1-graph for image analysis",
      "author" : [ "B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang" ],
      "venue" : "IEEE Transactions on Image Processing, 19(4):858–866",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Sparse representation and learning in visual recognition: Theory and applications",
      "author" : [ "H. Cheng", "Z. Liu", "L. Yang", "X. Chen" ],
      "venue" : "Signal Process.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Pattern Classification (2Nd Edition)",
      "author" : [ "R.O. Duda", "P.E. Hart", "D.G. Stork" ],
      "venue" : "Wiley-Interscience",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Sparse subspace clustering",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "CVPR, pages 2790–2797",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparse manifold clustering and embedding",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "NIPS, pages 55–63",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparse subspace clustering: Algorithm",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "theory, and applications. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2765–2781",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Model-Based Clustering, Discriminant Analysis, and Density Estimation",
      "author" : [ "C. Fraley", "A.E. Raftery" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "Clustering by passing messages between data points",
      "author" : [ "B.J. Frey", "D. Dueck" ],
      "venue" : "Science, 315:2007",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Proximal methods for sparse hierarchical dictionary learning",
      "author" : [ "R. Jenatton", "J. Mairal", "F.R. Bach", "G.R. Obozinski" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 487–494",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Supervised dictionary learning",
      "author" : [ "J. Mairal", "F.R. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman" ],
      "venue" : "Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pages 1033–1040",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "L0-norm-based sparse representation through alternate projections",
      "author" : [ "L. Mancera", "J. Portilla" ],
      "venue" : "In Image Processing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "NIPS, pages 849–856",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Matching Theory",
      "author" : [ "D. Plummer", "L. Lovász" ],
      "venue" : "North- Holland Mathematics Studies. Elsevier Science",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Survey: Graph clustering",
      "author" : [ "S.E. Schaeffer" ],
      "venue" : "Comput. Sci. Rev.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Semi-supervised learning by sparse representation",
      "author" : [ "S. Yan", "H. Wang" ],
      "venue" : "SDM, pages 792–801",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Linear spatial pyramid matching using sparse coding for image classification",
      "author" : [ "J. Yang", "K. Yu", "Y. Gong", "T.S. Huang" ],
      "venue" : "CVPR, pages 1794–1801",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Regularized l1-graph for data clustering",
      "author" : [ "Y. Yang", "Z. Wang", "J. Yang", "J. Han", "T. Huang" ],
      "venue" : "Proceedings of the British Machine Vision Conference. BMVA Press",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Data clustering by laplacian regularized l1-graph",
      "author" : [ "Y. Yang", "Z. Wang", "J. Yang", "J. Wang", "S. Chang", "T.S. Huang" ],
      "venue" : "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec City, Québec, Canada., pages 3148–3149",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Lowrank sparse coding for image classification",
      "author" : [ "T. Zhang", "B. Ghanem", "S. Liu", "C. Xu", "N. Ahuja" ],
      "venue" : "IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 281–288",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Graph regularized sparse coding for image representation",
      "author" : [ "M. Zheng", "J. Bu", "C. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai" ],
      "venue" : "IEEE Transactions on Image Processing, 20(5):1327– 1336",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Locality preserving clustering for image database",
      "author" : [ "X. Zheng", "D. Cai", "X. He", "W.-Y. Ma", "X. Lin" ],
      "venue" : "Proceedings of the 12th Annual ACM International Conference on Multimedia, MULTIMEDIA ’04, pages 885–891, New York, NY, USA",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn.",
      "startOffset" : 8,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn.",
      "startOffset" : 8,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "With Gaussian Mixture Model (GMM) as a representative, model-based clustering methods typically model the data by a mixture of parametric distributions, and the parameters of the distributions are estimated via fitting a statistical model to the data [10].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 4,
      "context" : "For example, K-means [6] groups similar data together by a local minimum of sum of within-cluster dissimilarities.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "Affinity Propagation [11] uses the same principle and automatically determines the cluster number.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "Spectral Clustering [16] identifies clusters of complex shapes lying on some low dimensional manifolds by spectral embedding.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Among various similarity-based clustering methods, graph-based methods [18] are important wherein the edge weight of the graph serving as the data similarity, and sparse graphs (where only a few edges have non-zero weights for each vertex) are demonstrated to be especially effective for clustering high dimensional data.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "l-graph is further extended to incorporate local manifold structure of the data in [21, 22].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "l-graph is further extended to incorporate local manifold structure of the data in [21, 22].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "[9] points out that in case that the data are drawn from linear independent subspaces, sparse representation by l-norm can recover the underlying subspaces.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, sparse representation methods [15] that directly optimize objective function involving l-norm demonstrate compelling performance compared to its lnorm counterpart.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "The proximal method is inspired by the proximal linearized method in [3].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.",
      "startOffset" : 172,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.",
      "startOffset" : 172,
      "endOffset" : 187
    }, {
      "referenceID" : 21,
      "context" : "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.",
      "startOffset" : 172,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.",
      "startOffset" : 172,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "l-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes.",
      "startOffset" : 8,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "l-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes.",
      "startOffset" : 8,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "In sparse subspace clustering [9], the authors pointed out that such sparse representation for each datum recovers the underlying subspaces from which the data are generated.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : ",xn] ∈ IR, it is mentioned in [7, 9] that the following sparse representation for each data point by l-norm",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : ",xn] ∈ IR, it is mentioned in [7, 9] that the following sparse representation for each data point by l-norm",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9].",
      "startOffset" : 178,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9].",
      "startOffset" : 178,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to l-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to l-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative.",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "[3] shows that the l-norm function ‖ · ‖0 is a semi-algebraic function.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "The conclusions of this theorem directly follows from Theorem 1 in [3].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Please refer to more detailed definition in [3].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "the accuracy and the Normalized Mutual Information(NMI) [25].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "where 1I is the indicator function, and Ω is the best permutation mapping function by the Kuhn-Munkres algorithm [17].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].",
      "startOffset" : 130,
      "endOffset" : 141
    } ],
    "year" : 2017,
    "abstractText" : "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn. It is well known that l-norm used in l-graph is a convex relaxation of lnorm for enforcing the sparsity. In order to handle general cases when the subspaces are not independent and follow the original principle of sparse representation, we propose a novel l-graph that employs l-norm to encourage the sparsity of the constructed graph, and develop a proximal method to solve the associated optimization problem with the proved guarantee of convergence. Extensive experimental results on various data sets demonstrate the superiority of l-graph compared to other competing clustering methods including l-graph.",
    "creator" : "LaTeX with hyperref package"
  }
}