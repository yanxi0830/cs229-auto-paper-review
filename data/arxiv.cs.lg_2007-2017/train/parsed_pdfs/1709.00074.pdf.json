{
  "name" : "1709.00074.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unsupervised Learning of Semantic Mappings",
    "authors" : [ "Tomer Galanti", "Lior Wolf" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples. For example, given a set of unlabeled images of horses, and a set of unlabeled images of zebras, CycleGAN (Zhu et al., 2017) creates the analog zebra image for a new image of a horse and vice versa. These recent methods employ two types of constraints. First, when mapping from one domain to another, the output has to be indistinguishable from the samples of the new domain. This is enforced using GANs (Goodfellow et al., 2014) and is applied at the distribution level: the mapping of horse images to the zebra domain should create images that are indistinguishable from the training images of zebras and vice versa. The second type of constraints enforces that for every single sample, transforming it to the other domain and back (by a composition of the mappings in the two directions) results in the original sample. This is enforced for each training sample from either domain: every training image of a horse (zebra), which is mapped to a zebra (horse) image and then back to the source domain, should be as similar as possible to the original input image. In another example, taken from DiscoGAN (Kim et al., 2017), a function is learned to map a handbag to a shoe of a similar style. One may wonder why striped bags are not mapped, for example, to shoes with a checkerboard pattern. If every striped pattern in either domain is mapped to a checkerboard pattern in the other and vice-versa, then both the distribution constraints and the circularity constraints might hold. The former could hold since both striped and checkerboard patterned objects would be generated. Circularity could hold since, for example, a striped object would be mapped to a checkerboard object in the other domain and then back to the original striped object. One may claim that the distribution of striped bags is similar to those of striped shoes and that the distribution of checkerboard patterns is also the same in both domains. In this case, the alignment follows from fitting the shapes of the distributions. This explanation is unlikely, since no effort is being made to create handbags and shoes that have the same distributions of these properties, as well as many other properties.\nar X\niv :1\n70 9.\n00 07\n4v 1\n[ cs\n.L G\n] 3\n1 A\nug 2\n01 7"
    }, {
      "heading" : "2. The Unsupervised Alignment Problem",
      "text" : "The learning algorithm is provided with only two unlabeled datasets: one includes i.i.d samples from the first distribution and the second includes i.i.d samples from the other distribution.\nxi ∈ XA for i = 1 . . .m where xi i.i.d∼ DA and XA denotes the space of domain A = (XA, DA) xj ∈ XB for j = 1 . . . n where xj i.i.d∼ DB and XB denotes the space of domain B = (XB , DB) (1)\n(all notations are listed in the appendix, see Tab. 1). To semantically tie the two distributions together, our model is based on a generative approach, which is well aligned with the success of GAN-based image generation, e.g., (Radford et al., 2015), in mapping random input vectors into realistic-looking images. Let z ∈ X be a random vector that is distributed according to the distribution DZ and which we employ to denote the semantic essence of samples in XA and XB . We denote DA = yA ◦ DZ and DB = yB ◦ DZ , where the functions yA : X → XA and yB : X → XB , and f ◦D denotes the distribution of f(x), where x ∼ D. It makes sense to assume that both yA and yB are invertible, since given training samples, one may be expected to be able to recover the underlying properties of the generated samples, even with very weak supervision (Chen et al., 2016). We denote by yAB = yB ◦ y−1A , the function that maps the first domain to the second domain. It is semantic in the sense that it goes through the shared semantic space X . The goal of the learner is to fit a function h ∈ H, for some hypothesis classH that is closest to yAB ,\ninf h∈H RDA [h, yAB ], (2)\nwhere RD[f1, f2] = Ex∼D`(f1(x), f2(x)), for a loss function ` : R× R→ R and a distribution D. It is not clear that such fitting is possible without further information. Assume, for example, that there is a natural order on the samples in XB . A mapping that maps an input sample x ∈ XA to the sample that is next in order to yAB(x) could be just as feasible. More generally, one can permute the samples in XA by some function Π that replaces each sample with another sample that has a similar likelihood (the formal definition is given in Sec. 4) and learn h that satisfies h = yAB ◦Π. We call this difficulty “the alignment problem” and our work is dedicated to understanding the plausibility of learning despite this problem. In the cross domain transfer line of work (Taigman et al., 2017), the alignment problem is dealt with by incorporating a fixed pre-trained feature map f and requiring what is called f -constancy, namely that the following risk is small RDA [f, f ◦ h], or informally, f(x) = f(h(x)). In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed. Circularity requires the recovery of both yAB and yBA = yA ◦ y−1B simultaneously. Namely, functions h and h′ are learned jointly by minimizing the risk:\ninf h,h′∈H\ndiscC(h ◦DA, DB) + discC(h′ ◦DB , DA)\n+RDA [h ′ ◦ h, IdA] +RDB [h ◦ h′, IdB ]\n(3)\nwhere discC(D1, D2) = supc1,c2∈C |RD1 [c1, c2]−RD2 [c1, c2]| denotes the discrepancy between distributions D1 and D2 that is implemented with a GAN (Ganin et al., 2016). The first term in Eq. 3 ensures that the samples generated by mapping domain A to domain B follow the distribution of samples in domain B. The second term is the analog term for the mapping in the other direction. The last two terms ensure that mapping a sample from one domain to the second and back, results in the original sample. While the circularity constraints, expressed as the last two terms in Eq. 3, are elegant and do not require additional supervision, for every invertible permutation Π of the samples in domain B (not to be confused with a permutation of the vector elements of the representation of samples in B) we have\n(h′ ◦Π−1) ◦ (Π ◦ h) = h ◦ h′ ≈ IdA, and (Π ◦ h) ◦ (h′ ◦Π−1) = Π ◦ (h◦h′) ◦Π−1 ≈ Π ◦ IdB ◦Π−1 = IdB .\n(4)\nTherefore, every circularity preserving h and h′ gives rise to many possible solutions of the form h̃ = h ◦Π and h̃′ = Π−1 ◦ h′. If Π happens to satisfy DB(x) ≈ DB(Π(x)), then the discrepancy terms in Eq. 3 also remain largely unchanged. Circularity by itself cannot, therefore, explain the recent success of unsupervised mapping."
    }, {
      "heading" : "2.1 An Illustrative Example",
      "text" : "Despite the availability of a large number of alternative hypotheses h′ that satisfy the constraints of Eq. 3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig. 1. Consider the domain A of uniformly distributed points (x1, x2)> ∈ R2, where 0 ≤ x1 < 1 and x2 = 0.5. Let B be a similar domain, except x2 = 2. We are interested in learning the mapping y2DAB((x1, 0.5)\n>) = (x1, 2)>. We note that there are infinitely many mappings from domain A to B that satisfy the constraints of Eq. 3. However, when we learn the mapping using a neural network with one hidden layer of size 2, and Leaky ReLU activations1 (Maas et al., 2013), y2DAB is one of only two options. In this case h(x) = σa(Wx+ b), for W ∈ R2×2,b ∈ R2 and where σa is applied per coordinate. The only admissible solutions are of the form Wb =\n( 1 −2b1 0 4− 2b2 ) or W ′b = ( −1 1− 2b1 0 4− 2b2 ) and b = (b1, b2)>, which are identical, for every b, to\ny2DAB or to an alternative y 2D′ AB ((x1, 0.5) >) = (1− x1, 2)>. Exactly the same situation holds for any pair of line segments in Rd+. Therefore, by restricting the hypothesis space of h, we eliminate all alternative solutions, except two. These two are exactly the two mappings that would commonly be considered “more semantic” than any other mapping, and can be expressed as the simplest possible mapping through a shared one dimensional space. While this is an extreme example, we are able to show that limiting the complexity of the admissible solutions\n1. σa(x) = Ind[x < 0]ax+ Ind[x ≥ 0]x, for the indicator function Ind[q] which maps a true value to one, zero otherwise.\neliminates the solutions that are derived from yAB by permuting the samples in the space XA, since such mixing requires added complexity."
    }, {
      "heading" : "2.2 Informal Statement of the Main Results",
      "text" : "In this work, we show that what separates h ≈ yAB and h̃ ≈ Π ◦ yAB is the complexity of these functions. Namely, that the complexity of h is much lower and, therefore, it is learnable with much smaller networks than the alternatives. Therefore, in order to learn h and not h̃, all that one needs is to use a network that is not “too big”. To show this, we develop a new framework for measuring the complexity of composition of functions. Our function complexity framework measures the complexity of a function as the depth of a neural network which implements it, or the shallowest network, if there are multiple such networks. In other words, we use the number of layers of a network as a proxy for the Kolmogorov complexity of functions, using layers in lieu of the primitives of the universal Turing machines, which is natural for studying functions that can be computed by feedforward neural networks. To make it a useful tool, an elaborate theoretical system is developed for this complexity framework. The system is based on well-justified assumptions and is presented in the next section. In Sec. 4, we apply the new framework and study the properties of the low-complexity solutions when learning in an unsupervised manner. We provide a complexity-based definition to the illusive notion of semantics. A semantic mapping is a mapping with the lowest complexity among all permuted versions of it, i.e., it is the most straightforward one in this set. Using this notion, we are able to state our main results informally as: (1) There are only a handful of semantic mappings between two domains. (2) every semantic function between two domains A and B either passes through a shared space Z, or is very different than any composition of a semantic function form A to Z with a semantic function from Z to B. Therefore, if there is a semantic function through Z, one obtains it by learning a minimal network between A and B, or alternatively, but unlikely, learns a completely different function through another shared space Z ′. Based on our results, we are able to make concrete predictions. The first one, which is empirically validated in the appendix, states that in contrast to the current common wisdom, one can learn a semantic mapping between two spaces without any matching samples and even without circularity.\nPrediction 1 When learning with a small enough network in an unsupervised way a mapping between domains that share common characteristics, the GAN constraint in the target domain is sufficient to obtain a semantic mapping.\nThe strongest clue that helps identify the semantic mapping from the other mappings is the suitable complexity of the network that is learned. A network with a complexity that is too low cannot replicate the target distribution, when taking inputs in the source domain. A network that has a complexity that is too high, would not learn the semantic mapping. We believe that the success of the recent methods arises from selecting the architecture used in an appropriate way. For example, DiscoGAN (Kim et al., 2017) employs either eight or ten layers, depending on the dataset. We make the following prediction, which is also validated empirically in the appendix:\nPrediction 2 When learning in an unsupervised way a mapping between domains, the size of the network needs to be carefully adjusted.\nThis prediction is also surprising, since in supervised learning, extra depth is not as detrimental, if at all. As far as we know, this is the first time that this clear distinction between supervised and unsupervised learning is made. Although our first prediction states that circularity is not needed, there does seem to be an advantage to using it. We show that using circularity, ambiguity is reduced in a very specific way. Namely, the potential family of permutations Π that extend the semantic solution h to the alternatives Π ◦ h are constrained to be of lower complexity. Therefore, the argument made following Eq. 4 holds only for a very limited set of possible permutations Π."
    }, {
      "heading" : "3. A Complexity Measure for Functions",
      "text" : "In order to model the composition of neural networks, we define a complexity measurement that assigns a value based on the number of simple functions that make up a complex function.\nDefinition 1 (Stratified complexity model (SCM)) A stratified complexity model N := SCM[C] is a hypothesis class of functions p : RM → RM specified by a set of functions C. Every function p in N has an appropriate decomposition:\n• There are p1, ..., pn ∈ C such that p = pn ◦ pn−1 ◦ · · · ◦ p1 (if n = 0 then p = Id and if n = 1 then p = p1).\n• Every function in C is invertible. Informally, a SCM partitions a set of invertible functions into disjoint complexity classes,\nC0 := {Id} Cn := { p = pn ◦ ... ◦ p1 ∣∣∣pn ◦ ... ◦ p1 is an appropriate decomposition of p } \\ n−1⋃\ni=0\nCi (5)\nWhen considering simple functions pi that are layers in a neural network, each complexity class contains the functions that are implemented by networks of n layers. In addition, we denote the complexity of a function p:\nC(p) := argn{p ∈ Cn} (6) If the complexity of a function p equals n, then any appropriate decomposition p = pn ◦ ... ◦ p1 will be called a minimal decomposition of p. If there is no such n, we denote C(p) =∞. According to this measurement, the complexity of a function p is determined by the minimal number of primitive functions required in order to represent it. It is, therefore, necessary to understand the rules that dictate the complexity of a composition of functions, inverse functions, etc. We begin with the simplest relationship that occurs between two composed functions.\nDefinition 2 (Unfused functions) Let N = SCM[C] and let p, q ∈ N be any two functions. We say that p is unfused in q if C(p ◦ q) = C(p) + C(q) and denote p 7 q. Otherwise, we say that p is fused in q and denote p . q. In addition, the function p and q will be called left and right partial functions of p ◦ q (resp.). Informally, a function p is unfused in another function q, if the first operations of p do not invert the last processing steps of q. For instance, if we can represent p = g1 ◦ g2 and q = g−12 ◦ g3 such that C(g1) < C(p) and C(g3) < C(q), then (by Lem. 4 below) the two functions are fused. In addition to characterizing the result of composing two functions, we also define a measure for the complexity of transforming one function into the other. The conditional complexity between the functions p and q is the complexity of the function g that satisfies: p = g ◦ p. Definition 3 (Conditional complexity) Let N = SCM[C] and let p, q ∈ N be any two functions. The conditional complexity between p and q is denoted: C(p||q) := C(p ◦ q−1). In this work, we focus our attention on SCMs that represent the architectures of fully connected neural networks with layers of a fixed size, i.e.,\nDefinition 4 (NN-SCM) A NN-SCM is a SCM N = SCM[C] that satisfies the following conditions:\n• C = { σ ◦W ∣∣∣W ∈ RM×M and W is invertible }\n. Here, W denotes both a linear transformation and the associated matrix form.\n• σ is a non-linear element-wise activation function. For brevity, we denote N := SCM[σ] to refer to a NN-SCM with the activation function σ. The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh are also used)."
    }, {
      "heading" : "3.1 Semantic mappings",
      "text" : "The following definition of a semantic mapping is both intuitive and well defined in concrete complexity terms. We say that given two distributions DA and DB , a semantic mapping f : XA → XB between domains A and B is a mapping that has minimal complexity among the functions h : XA → XB that satisfy h ◦DA ≈ DB . Consider, again, the example of a line segment in RM (Sec. 2.1) and the semantic space of the [0, 1] ⊂ R interval. The two linear mappings, which map either segment ends to 0 and the other to 1 are semantic, when using f that are ReLU based neural networks. Other mappings to this segment are possible, simply by permuting points on the segment in RM . However, these are much more complicated. In order to measure the distance between h ◦DA and DB we will use the discrepancy distance, discD. In this work, we will focus on classes of discriminators D of the form Dm := {u|C(u) ≤ m} for some m ∈ N. In addition, for simplicity, we will write discm := discDm .\nDefinition 5 (Semantic mapping) Let N = SCM[C]. Let A = (XA, DA) and B = (XB , DB) be two domains. We define the (m, 0)-semantic complexity between A and B as:\nCm, 0A,B := min i∈N∪{0} {∃h s.t C(h) = i and discm(h ◦DA, DB) ≤ 0} (7)\nThe set of (m, 0)-semantic functions between A and B is: H 0(DA, DB ;m) := H 0(DA, DB ;m,C m, 0 A,B ), where H 0(DA, DB ;m, k) := { h ∣∣∣C(h) ≤ k and discm(h ◦DA, DB) ≤ 0 } . (8)\nWe note that for any fixed 0 > 0, the sequence {Cm, 0A,B }∞m=0 is monotonically increasing asm tends to infinity. In addition, we assume that for every two distributions of interest, DI and DJ , and an error rate 0 > 0, there is a function h of finite complexity such that disc∞(h ◦DI , DJ) ≤ 0. Therefore, the sequence {Cm, 0A,B }∞m=0 is upper bounded by C(h) for all m ∈ N ∪ {0}. In particular, there is a minimal value m0 > 0 such that Cm, 0A,B = C m0, 0 A,B for all m ≥ m0. We denote: E 0A,B := m0 and C 0A,B := Cm0, 0A,B ."
    }, {
      "heading" : "3.2 Identifiability",
      "text" : "Every neural network implementation gives rise to many alternative implementations by performing simple operations, such as permuting the units of any hidden layer, and then permuting back as part of the linear mapping in the next layer. Therefore, it is first required to identify and address the set of transformations that could be inconsequential to the function which the network computes.\nDefinition 6 (Invariant set) Let N = SCM[σ] be a NN-SCM. The invariant set Invariant(N ) is the set of all π : RM → RM that satisfy the following conditions:\n• π : RM → RM is an invertible linear transformation.\n• σ ◦ π = π ◦ σ.\nFunctions in Invariant(N ) are called invariants or invariant functions.\nFor example, for neural networks with the tanh activation function, the set of invariant functions contains the linear transformations that take vectors, permute them and multiply each coordinate by ±1. Formally, each π = [ 1 · et(1), ..., M · et(M)]> where ei is the i’th standard basis vector, t is a permutation over [M ] and i ∈ {±1} (Fefferman and Markel, 1993). Our analysis is made much simpler, if every function has one invariant representation up to a sequence of manipulations using invariant functions that do not change the essence of the processing at each layer.\nDefinition 7 (Identifiability of minimal representation) A NN-SCM N = SCM[σ] obeys identifiability of minimal representation with respect to Invariant(N ), if for all n ∈ N ∪ {0} and p ∈ Cn such that there\nare two appropriate decompositions p = pn ◦ ... ◦ p1 and p = qn ◦ ... ◦ q1, then there are invariants π1, ..., πn ∈ Invariant(N ) such that:\nq1 = σ ◦ (π1 ◦W1) and p1 = σ ◦W1 ∀1 < i < n : qi = σ ◦ (πi ◦Wi ◦ π−1i−1) and pi = σ ◦Wi qn = σ ◦ (Wn ◦ π−1n−1) and pn = σ ◦Wn\n(9)\nWe consider that since any π ∈ Invariant(N ) commutes with σ, then, an alternative writing could be:\nq1 = π1 ◦ p1, ∀i = 2, ..., n− 1 : qi = πi ◦ pi ◦ π−1i−1 and qn = pn ◦ π−1n−1 (10)\nA stronger identifiability condition requires that every non-minimal implementation of a function p goes through the same processing steps as dictated by the layers of the minimal representation, where each of these steps can be mapped to multiple layers in the longer implementation.\nDefinition 8 (Identifiability) Let N = SCM[σ] be a NN-SCM obeying identifiability of minimal representation. We say that N obeys identifiability if for every function p ∈ N , such that p = pn ◦ ... ◦ p1 is a minimal decomposition, if p = qm ◦ ... ◦ q1, then:\n∃j1 = 1 < ... < jn+1 = m+ 1, π1, ..., πn ∈ Invariant(N ) : qj2:j1 = π1 ◦ p1, ∀i = 2, ..., n− 1 : qji+1:ji = πi ◦ pi ◦ π−1i−1 and qjn+1:jn = pn ◦ π−1n−1\n(11)\nHere, if u = un ◦ ... ◦ u1 then ui+1:j = ui ◦ ... ◦ uj if j ≤ i and ui:i = Id.\nIn the context of neural networks, the general question of uniqueness up to invariants, also known as identifiability, is an open question. Nevertheless, several authors have made progress in this area for different neural network architectures. The most notable work has been done by Fefferman and Markel (1993) that proves identifiability for σ = tanh. Furthermore, the representation is unique up to the invariant functions. Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurková and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions. As far as we know, there are no recent results continuing this line of work for activation functions such as Leaky ReLU. Uniqueness, which is stronger than identifiability, since it means that even multiple representations with different number of layers do not exist, does not hold for these activation functions. To see this, note that for every M ×M mapping W , the following holds:\nσ ◦W = (σ ◦W ) ◦ (σ ◦ −Id) ◦ (σ ◦ −Id/a) (12)\nwhere σ is the Leaky ReLU activation function with parameter a. We conjecture that for networks with Leaky ReLU activations identifiability holds, or at least for networks with a fixed number of neurons per layer."
    }, {
      "heading" : "3.3 Properties of Inverses and Compositions",
      "text" : "It is necessary to study the effect of inversion on the complexity of functions, since, for example, we care about both h′ = Π ◦ h and h = Π−1 ◦ h′.\nDefinition 9 Let N = SCM[C] be an SCM. We say that N is d-inverse-complexity-preserving (d-ICP for short) if: ∀p ∈ C : C(p−1) ≤ d ·C(p). Sometimes, we will omit writing I(N ) and write I instead, when N is obvious from the context.\nAn immediate consequence of the following theorem is that neural networks with Leaky ReLU activations are 3-ICP, see also Lem. 11 in the appendix, which is part of the theorem’s proof (all proofs can be found in the appendix).\nTheorem 1 Let N = SCM[σ] be a NN-SCM with σ that is the Leaky ReLU with parameter a > 0. Then, for any u ∈ N , |C(u−1)− C(u)| is either 0 or 2. The case C(u−1) = C(u) holds, for example, for invertible linear mappings because every linear mapping W can be expressed as\n(σ ◦ −Id) ◦ (σ ◦ −W/a) = (σ ◦ −Id) ◦ (σ ◦ −Id/a)︸ ︷︷ ︸ =Id ◦W,\nwhere σ is the Leaky Relu activation function with a parameter a. Nevertheless, for a generic function u, C(u−1) = C(u) + 2, as the following theorem shows.\nTheorem 2 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) ∈ RM×M×n such that C(u−1n+1:1) = C(un+1:1) + 2 and ui = σ ◦Wi (for i ∈ [n]) is open and dense in RM×M×n. As we discussed earlier, two functions f and g can be either fused or unfused. In the following Theorem, it is shown that a generic decomposition un+1:1 is unfused, i.e, C(un+1:1) = n. In particular, for generic functions f and g are unfused.\nTheorem 3 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) ∈ RM×M×n such that C(un+1:1) = n where ui = σ ◦Wi (for i ∈ [n]) is open and dense in RM×M×n.\nIt is still important to cover both fused and unfused functions, since our main results are “for every” and not “for general” due to the fact that we study specific functions, and specifically we study relations of the form h′ = Π ◦ h, and look for the h′ with the minimal complexity, for which, C(h′) < C(h). If two functions are known to be unfused then, from definition, the complexity of their composition is exactly the sum of their complexities. It is natural to assume that the same holds for the composition of three unfused functions. However, this is not necessarily the situation when the middle function in this composition has a complexity that is lower than the ICP factor I (see Lem. 13 in the appendix). However, when the function in the middle of the composition has a complexity ≥ I , the complexity of the composition is the sum of the complexities.\nTheorem 4 LetN = SCM[σ] be a NN-SCM obeying identifiability. Then, for all f, g, h ∈ N such that f 7 g, g 7 h and I ≤ C(g), we have:\nC(f ◦ g ◦ h) = C(f) + C(g) + C(h) (13)"
    }, {
      "heading" : "4. Learning Semantic Mappings",
      "text" : "In this section, we present the theoretical foundations for unsupervised alignment algorithms. In the unsupervised alignment problem, the algorithms are provided with only two unmatched datasets of samples from the domains A and B and the task is to learn a semantic function between them. The goal of this section is to understand under which constraints one is able to learn the semantic mapping using unsupervised alignment."
    }, {
      "heading" : "4.1 Counting Semantic Mappings",
      "text" : "Recall that discm is the discrepancy distance for discriminators of complexity up to m. We have discussed the functions Π which replaces between members in the domain B that have similar probabilities. Formally, these are defined using the discrepancy distance.\nDefinition 10 (Density preserving mapping) Let N = SCM[C] and D a distribution. A (m, 0)-density preserving mapping over D (or an (m, 0)-DPM for short) is a function Π such that\ndiscm(Π ◦D,D) ≤ 0 (14) We denote (m, 0)-DPMs of complexity k by DPM 0(D;m, k) := { Π ∣∣discm(Π ◦D,D) ≤ 0 and C(Π) = k } .\nWe would like to bound the number of shared semantic distributions by the number of DPMs. We consider that there are infinitely many DPMs and semantic mappings. For example, if we slightly perturb the weights of a minimal representation of DPM, Π, we obtain a new DPM. Therefore, we define a relation between of functions that reflects whether the two are similar. In this way, we are able to bound the number of different (not-similar) semantic mappings by the number of different DPMs.\nDefinition 11 (Similarity between pairs of distributions or functions) Let N = SCM[C].\n• Distributions D1 and D2 are (m, 0)-similar and we denote\nD1 ∼ m, 0 D2 ⇐⇒ discm(D1, D2) ≤ 0 (15)\n• Functions f and g are (D,m, 0)-similar and we denote f D∼ m, 0 g, if C(f) = C(g) =: n and there are\nminimal decompositions: f = fn+1:1 and g = gn+1:1 such that\n∀i ∈ [n] : fi+1:1 ◦D ∼ m, 0 gi+1:1 ◦D (16)\nThe defined similarity is reflexive and symmetric, but not transitive. Therefore, there are many different ways to partition the space of functions into disjoint subsets such that in each subset, any two functions are similar. We count the number of functions up to the similarity as the minimal number of subsets required in order to cover the entire space. This idea is presented in the following Def. 12.\nDefinition 12 (Covering numbers) Let (U ,∼U ) be a set and a reflexive and symmetric relation. A covering of (U ,∼U ), is a tuple (U ,≡U ) such that: ≡U is an equivalence relation and u1 ≡U u2 =⇒ u1 ∼U u2. The covering number of (U ,∼U ) is:\nmin ∣∣U/ ≡U ∣∣ s.t: the minimum is taken over (U ,≡U ) that is a covering of (U ,∼U ) (17)\nWe denote the covering number of (U ,∼U ) by Covering(U ,∼U ). Here, U/ ≡U is the quotient set of U by ≡U .\nInformally, the following theorem states that the number of semantic mappings is upper bounded by the square root of the number of DPMs of size 2C 0A,B + 2. This result is useful since DPMs are expected to be rare in real-world domains . When imagining mapping a space to itself, in a way that preserves the distribution, one first considers symmetries. Near-perfect symmetries are rare in natural domains, and when these occur, e.g., (Kim et al., 2017), they form well-understood ambiguities. Another option that can be considered is that of replacing specific samples in domain B with other samples of the same probability. However, these very local discontinuous mappings are of very high complexity, since this complexity is required in order to reduce the modeling error for discontinuous functions. One can also consider replacing larger sub-domains with other sub-domains such that the distribution is preserved. This could be possible, for example, if the distribution within the sub-domains is almost uniform (unlikely), or if it is estimated inaccurately due to the limitations of the training set. Thm. 5 employs the following weak assumption. In Lem. 35 in the Appendix we prove that this assumption holds for the case of a continuous risk if the discriminators have bounded weights.\nAssumption 1 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. For every m > 0 and n > 0, the function discm(fWn,...,W1 ◦D1, D2) (18) is continuous as a function of the weights of Wn, ...,W1. Here, fWn,...,W1 = (σ ◦Wn) ◦ ... ◦ (σ ◦W1).\nTheorem 5 (Counting semantic mappings) Let N = SCM[σ] be a NN-SCM with σ that is a Leaky ReLU with parameter a > 0 and assume Assumption 1. Let 0, 1 and 2 < 1 − 2 0 are three positive constants and A = (XA, DA) and B = (XB , DB) are two domains. Assume that m ≥ k + 2C 0A,B + 5. Then,\nCovering ( H 0(DA, DB ;m),\nDA∼ k, 1 ) ≤ lim →0 √ Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) (19)\nAn interesting variant of this theorem follows from trying to count the number of mappings inH+ 0(DA, DB ;m) := H 0(DA, DB ;m) ∩ (H 0(DB , DA;m))−1, i.e., the set of semantic mappings between A and B such that their inverse is semantic between B and A. This is the case of the circularity constraint, which requires both sides to be semantic. In this case, we obtain the same bound with min{2C 0A,B + 2, 2C 0B,A + 2}. Therefore, the number of two-sided semantic mappings is smaller. To see an example where a function is semantic while its inverse is not, consider the network representation of a linear function W = (σ ◦ −Id) ◦ (σ ◦ −W/a). The inverse function W−1 is of the same complexity (2). However, a small perturbation of the network representation, which is still semantic, has an inverse complexity of 4, while the original inverse function W−1 is the semantic function and has a complexity of 2. Similar situations of higher complexities can be constructed, for example, by having linear functions as the top two layers of a neural network."
    }, {
      "heading" : "4.2 Shared Semantic Distributions",
      "text" : "The recovery of an analog in domain B for a sample in domain A, naturally takes place as a two step process: first recovering the properties of the source sample that can be transfered between the domains, and then generating a sample in B that has these properties. Therefore, when discussing analogies, there are elementary compositions that play a major role. For example, drawing semantic analogies (i.e., mapping semantically) between domain A and domain B through domain Z is naturally given by a composition of a mapping from A to Z and a mapping from Z to B.\nDefinition 13 (Z-mappings and shared semantic distributions) Let A = (XA, DA) and B = (XB , DB) be two domains and DZ a distribution.\n• The set of Z-mappings associated with DZ is denoted by: Z(DA, DZ , DB ;m, 0, 1) := H 1(DZ , DB ;m) ◦H 0(DA, DZ ;m) (20)\nIf 0 = 1 we write Z(DA, DZ , DB ;m, 0) for short. • DZ is a (m, 0, 1, 2)-shared semantic distribution between A and B if for all yB ∈ H 1(DZ , DB ;m)\nand y−1A ∈ H 0(DA, DZ ;m) we have: yB 7 y−1A and, Z(DA, DZ , DB ;m, 0, 1) ∩H 2(DA, DB ;m) 6= ∅ (21)\nIf 0 = 1 and 2 = 2 0 we write (m, 0) for short.\nIn general, there are many shared semantic distributions DZ between A and B. For example, DA itself is a shared semantic distribution. In addition, under mild assumptions (see Thm. 40), for every semantic function y ∈ H 0(DA, DB ;m) and a minimal decomposition y = yn ◦ ... ◦ y1, for every i ≤ n, yi+1:1 ◦DA is a shared semantic distribution between A and B. For simplicity, when considering a function h = g ◦ f ∈ Z(DA, DZ , DB ;m, 0, 1) such that g ∈ H 1(DZ , DB ;m) and f ∈ H 0(DA, DZ ;m) we will simply write h = g ◦ f ∈ Z(DA, DZ , DB ;m, 0, 1). A semantic mapping betweenA andB passes through a sequence of shared semantic distributionsDZ1 , ..., DZn . One may expect that there exists a semantic mapping in the other direction, from B to A, that passes through the same sequence. In the proof of the following theorem, it is shown that for the typical case of C 0+ 1B,A = C 0+ 1A,B + 2 (see Thm. 2), there is a semantic mapping that passes through (−σ−1) ◦DZn , ..., (−σ−1) ◦DZ1 . The case C 0+ 1B,A = C 0+ 1 A,B − 2 is similar and we do not cover it in this paper.\nTheorem 6 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. Let A = (XA, DA) and B = (XB , DB) be two domains and DZ 6= DA, DB is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Let k ≥ max { E 0A,B , E 4 0+4 1 B,A , E 4 0 B,A + C 0+ 1 A,Z + 1 } and m ≥ k+3C 0+ 1A,B +4. Assume thatC3 0+ 1B,A = C 0+3 1 B,A = C 0+ 1 B,A = C 0+ 1 A,B +2. Then, (−σ−1)◦DZ is a (k, 1, 0, 0+ 1)-shared semantic distribution between B and A.\nThe importance of shared semantic distributions comes from the fact that they define the semantic functions that pass through them. Under reasonable assumptions, there is only one semantic function that passes through a given shared semantic distribution. In the following theorem, we study the relationship between a semantic mapping h and a shared semantic distribution DZ and show that h passes through DZ or is very different from any mapping that passes through DZ . The theorem assumes that the classes of the discriminators used in order to differentiate between the domains DA, DB , and DZ have sufficient capacity.\nTheorem 7 (Alignment) Let N = SCM[σ] be a NN-SCM with σ that is Leaky ReLU with parameter a > 0 and A = (XA, DA) and B = (XB , DB) are two domains. Let k ≥ max { E2 0A,B , E 0 A,Z , E 0 Z,B } and m ≥ k + C 0Z,B . Assume that DZ is a (m, 0)-shared semantic distribution between A and B such that C3 0Z,B = C 0 Z,B . Let h ∈ H2 0(DA, DB ;m). Then, one of the following holds:\n• h = g ◦ f ∈ Z(DA, DZ , DB ; k, 0, 3 0) such that g 7 f .\n• For every y ∈ Z(DA, DZ , DB ;m, 0) we have: C(h||y) ≥ 2C 0Z,B − 9.\nThis theorem requires that DZ is a shared semantic distribution and that h, g, and f are semantic functions. However, this result can be generalized, given a function b, to the domain b ◦DZ and the functions g ◦ b−1, b ◦ f as is shown in Lem. 41 and Lem. 43 in the Appendix."
    }, {
      "heading" : "5. Discussion",
      "text" : "The semantic mapping stands out of all alternative mappings, which allows it to be learned in an unsupervised manner. The usual generalization considerations still hold. For example, if all horse riders are covered in the training set, a shirtless rider in the test set (as demonstrated in (Zhu et al., 2017)) would become striped when converting the horse image to a zebra image. The limitations of unsupervised based learning that are due to symmetry, are also a part of our model. For example, the mapping of cars in one pose to cars in the mirrored pose that sometimes happens in (Kim et al., 2017), is similar in nature to the mapping of x to 1− x in the simple example given in Sec. 2.1. Such\nsymmetries occur when we can divide yAB into two functions yAB = y2 ◦ y1 such that a function π in the invariant set is a DPM of y1 ◦DA and, therefore, DB ≈ y2 ◦ π ◦ y1. We base our work on the assumption of identifiability, which constitutes an open question for most activation functions. We hope that there would be a renewed interest in this question, which has been open for decades for networks with more than a single hidden layer and is unexplored for modern activation functions. The stratified complexity model (SCM) is related to structural risk minimization by Vapnik and Chervonenkis (1971), which employs a hierarchy of nested subsets of hypothesis classes in order of increasing complexity. SCMs are specific to a hypothesis class of a certain recursive form, and the complexity classes are not nested. While we focused on unsupervised learning, the emergence of semantics from learning with a restricted capacity is widely applicable, e.g., to autoencoders, transfer learning, semi-supervised learning and elsewhere. As an extreme example, Sutskever et al. (2015) present empirical evidence that a semantic mapper can be learned, even from very few examples, if the network trained is kept small. We point to a key difference between supervised learning and unsupervised learning. While in the former, deeper networks, which can learn even random labels, work well (Zhang et al., 2017), unsupervised learning requires a careful control of the network capacity."
    }, {
      "heading" : "6. Conclusion",
      "text" : "The recent success in mapping between two domains in an unsupervised way and without any existing knowledge, other than network hyperparameters is nothing less than extraordinary and has far reaching consequences. As far as we know, nothing in the existing machine learning or cognitive science literature suggests that this would be possible. We provide the necessary machinery for understanding such phenomena by presenting a framework for measuring the complexity of compositions of functions and by providing a concrete definition of semantics. Using the new machinery, we explain how, simply by training networks that are not too complex, the semantic mapping stands out from all other alternative mappings. There are a few results that require further exploration. As one curious example, a surprising result is that all invertible functions can be divided into three classes, depending on the complexity of the inverse, which can be lower by 2, larger by 2, or the same. This can be an artifact of measuring complexity with Leaky ReLU networks, or something that has profound implications."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant ERC CoG 725974). The authors would like to thank Sagie Benaim for helping with the experiments and Moustapha Cisse, Léon Bottou and Ofir Yakovian for insightful discussions."
    }, {
      "heading" : "Appendix A. Summary of Notation",
      "text" : "Tab. 1 lists the symbols used in our work."
    }, {
      "heading" : "Appendix B. Empirical Validation of Prediction 1",
      "text" : "In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA → XB , based only on the GAN constraint on B, is presented as a failing baseline. In (Yi et al., 2017), among many non-semantic mappings obtained by the GAN baseline, one can find images of GANs that are successful. However, this goes unnoticed. In order to validate the prediction that a purely GAN based solution is viable, we conducted a series of experiments using the DiscoGAN architecture. We consider image domains A and B, where XA = XB = R3×64×64. In DiscoGAN, the generator is build of: (i) an encoder consisting of convolutional layers with 4× 4 filters followed by Leaky ReLU activation units and (ii) a decoder consisting of deconvolutional layers with 4 × 4 filters followed by a ReLU activation units. Sigmoid is used for the output layer. Between 4 to 5 convolutional/deconvolutional layers are used, depending on the domains used in A and B (we match the published code architecture per dataset). The discriminator is similar to the encoder, but has an additional convolutional layer as the first layer and a sigmoid output unit. The first set of experiments considers the CelebA face dataset. Transformations are learned between the subset of images labeled as “man” and those labeled as “woman”, as well as from blond to black hair and glasses to no eyewear. The results are shown in Fig. 3, 4, and 5, (resp.). It is evident that the output image is highly related to the input images. In the case of mapping handbag to shoes, as seen in Fig. 6, the GAN does not provide a meaningful solution. However, in the case of edges to shoes and vice versa (Fig. 7), the GAN solution is successful."
    }, {
      "heading" : "Appendix C. Empirical Validation of Prediction 2",
      "text" : "We predict that the selection of the right number of layers is crucial in unsupervised learning. Using fewer layers than needed will not support the modeling of the semantic transformation. In contrast, adding superfluous layers would mean that more and more alternative mappings obscure the semantic transformation. In (Kim et al., 2017), 8 layers are sometimes employed while at other times 10 layers are used (counting both convolution and deconvolution). In our experiment we vary the number of layers and inspect the influence on the results. These experiments were done on the CelebA gender conversion task, where eight layers are employed in the experiments of (Kim et al., 2017). Using the public implementation and adding and removing layers, we obtain the results in Fig. 8,9,10,11. Note that since the encoder and the decoder parts of the learned network are symmetrical, the number of layers is always even. As can be seen, changing the number of layers has a dramatic effect on the results and the best results are obtained at eight layers. The results degrade quickly as one deviates from the optimal value. Using fewer layers, the GAN fails to produce images of the desired class. Adding layers, the semantic alignment is lost, just as expected."
    }, {
      "heading" : "Appendix D. Assumptions",
      "text" : "Assumption 1 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. For every m > 0 and n > 0, the function discm(fWn,...,W1 ◦D1, D2) (18) is continuous as a function of the weights of Wn, ...,W1. Here, fWn,...,W1 = (σ ◦Wn) ◦ ... ◦ (σ ◦W1).\nAssumption 2 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. For all m > 0, the function\nRD[fVm,...,V1 , fWm,...,W1 ] (22)\nis continuous as a function of Vm, ..., V1,Wm, ...,W1."
    }, {
      "heading" : "Appendix E. Lemmas",
      "text" : "Lemma 1 Let D1 and D2 be two classes of functions and D1, D2 two distributions. Assume that D1 ◦ {p} ⊂ D2 then, discD1(p ◦D1, p ◦D2) ≤ discD2(D1, D2) (23) In particular, if m ≥ k + C(p) then,\ndisck(p ◦D1, p ◦D2) ≤ discm(D1, D2) (24)\nProof By the definition of discrepancy:\ndiscD1(p ◦D1, p ◦D2) = sup c1,c2∈D1\n∣∣∣Rp◦D1 [c1, c2]−Rp◦D2 [c1, c2] ∣∣∣\n= sup c1,c2∈D1\n∣∣∣RD1 [c1 ◦ p, c2 ◦ p]−RD2 [c1 ◦ p, c2 ◦ p] ∣∣∣\n(25)\nSince D1 ◦ {p} ⊂ D2 we have:\ndiscD1(p ◦D1, p ◦D2) = sup c1,c2∈D1\n∣∣∣RD1 [c1 ◦ p, c2 ◦ p]−RD2 [c1 ◦ p, c2 ◦ p] ∣∣∣\n≤ sup u1,u2∈D2\n∣∣∣RD1 [u1, u2]−RD2 [u1, u2] ∣∣∣ = discD2(D1, D2)\n(26)\nThe second inequality is a special case for D1 = Dk and D2 = Dm.\nLemma 2 Let A = (X1, D1) and B = (X2, D2) be two domains and DZ a distribution.\n1. Assume that m ≥ k + C(p). Then,\ndisck(p ◦D1, D3) ≤ discm(D1, D2) + disck(p ◦D2, D3) (27)\n2. Let y1, y2 and y = y2 ◦ y−11 be three functions and m ≥ k + C(y2). Then,\ndisck(y ◦D1, D2) ≤ discm(DZ , y−11 ◦D1) + disck(y2 ◦DZ , D2) (28)\n3. Let h be any function and m ≥ k + C(h−1). Then,\ndisck(D1, h−1 ◦D2) ≤ discm(h ◦D1, D2) (29)"
    }, {
      "heading" : "Proof",
      "text" : "1. Follows from Lem. 1, since Dk ◦ {p} ⊂ Dm, we have:\ndisck(p ◦D1, p ◦D2) ≤ discm(D1, D2) (30)\nTherefore, by the triangle inequality,\ndisck(p ◦D1, D3) ≤ disck(p ◦D1, p ◦D2) + disck(p ◦D2, D3) ≤ discm(D1, D2) + disck(p ◦D2, D3)\n(31)\n2. We use Lem. 1 with p :← y2, D1 :← Dk, and D2 :← Dm and Dk ◦ {y2} ⊂ D2:\ndisck(y2 ◦DZ , y ◦D1) = disck(y2 ◦DZ , y2 ◦ y−11 ◦D1) ≤ discm(DZ , y−11 ◦D1) (32)\nTherefore, by the triangle inequality,\ndisck(y ◦D1, D2) ≤ disck(y2 ◦DZ , D2) + disck(y2 ◦DZ , y ◦D1) ≤ disck(y2 ◦DZ , D2) + discm(DZ , y−11 ◦D1)\n(33)\n3. Follows immediately from Lem. 1 for p :← h−1 and Dk ◦ {h−1} ⊂ Dm.\nLemma 3 Let N = SCM[C]. Assume that D1 ∼ m, 1 D2 and D2 ∼ m, 2 D3 then D1 ∼ m, 1+ 2 D3.\nProof We consider that D1 ∼\nm, 1 D2 =⇒ discm(D1, D2) ≤ 1 (34)\nand, D2 ∼\nm, 2 D3 =⇒ discm(D2, D3) ≤ 2 (35)\nTherefore, by the triangle inequality,\ndiscm(D1, D3) ≤ discm(D1, D2) + discm(D2, D3) ≤ 1 + 2 (36)\nLemma 4 Let N = SCM[C]. In addition, let u, v be any two functions. Then,\nmax{C(u)− C(v−1), C(v)− C(u−1)} ≤ C(u ◦ v) ≤ C(u) + C(v) (37)\nProof We begin by proving the upper bound. We denote C(u) = n and C(v) = m. Let u = un+1:1 and v = vm+1:1 be minimal decompositions of u and v (resp.). Therefore, we can represent, u◦v = un+1:1◦vm+1:1. In particular, C(u ◦ v) ≤ n+m = C(u) + C(v). The lower bound follows immediately from the upper bound:\nC(u) = C(u ◦ v ◦ v−1) ≤ C(u ◦ v) + C(v−1) =⇒ C(u)− C(v−1) ≤ C(u ◦ v) (38)\nBy similar considerations, C(v)− C(u−1) ≤ C(u ◦ v).\nLemma 5 LetN = SCM[C]. In addition, let u1, u2, u3 ∈ N be three functions such that: C(u1 ◦ u2 ◦ u3) = C(u1) + C(u2) + C(u3). Then, u1 7 u2 and u2 7 u3.\nProof By Lem. 4, C(u1 ◦ u2 ◦ u3) ≤ C(u1 ◦ u2) + C(u3). In addition, by C(u1 ◦ u2 ◦ u3) = C(u1) + C(u2) + C(u3), we have: C(u1) + C(u2) ≤ C(u1 ◦ u2). Again by Lem. 4, C(u1 ◦ u2) ≤ C(u1) + C(u2) and conclude that C(u1 ◦ u2) = C(u1) + C(u2). The second equation follows by similar arguments.\nLemma 6 Invariant(N ) is closed under inverse and composition, i.e,\nπ ∈ Invariant(N ) ⇐⇒ π−1 ∈ Invariant(N ) (39)\nAnd, π1, π2 ∈ Invariant(N ) =⇒ π1 · π2 ∈ Invariant(N ) (40)\nProof Inverse: Let π ∈ Invariant(N ). Then, by definition, π is an invertible linear mapping and π ◦σ = σ ◦π. In particular, π−1 is also an invertible linear mapping and π−1 ◦ σ = σ ◦ π−1. Thus, π−1 ∈ Invariant(N ). Composition: Let π1, π2 ∈ Invariant(N ). Then, πi is an invertible linear mapping and πi ◦ σ = σ ◦ πi for i = 1, 2. In particular, π1 ◦ π2 is also an invertible linear mapping and π1 ◦ π2 ◦ σ = π1 ◦ σ ◦ π2 = σ ◦ π1 ◦ π2. Thus, π1 ◦ π2 ∈ Invariant(N ).\nRecall the notation introduced in Eq. 11.\nLemma 7 Let N = SCM[σ] obeying identifiability. If p = pn+1:1 = qn+1:1 are two minimal decompositions of p then:\n∀i ∈ [n] : pi+1:1 ◦ q−1i+1:1 ∈ Invariant(N ) and pn+1:i ◦ q−1n+1:i ∈ Invariant(N ) (41)\nProof First, if i = n then pi+1:1 ◦ q−1i+1:1 = Id ∈ Invariant(N ). Otherwise, by minimal identifiability,\nq1 = π1 ◦ p1, ∀i = 2, ..., n− 1 : qi = πi ◦ pi ◦ π−1i−1 and qn = pn ◦ π−1n−1 (42)\nIn addition, pi+1:1 = pi ◦ pi−1 ◦ ... ◦ p1 qi+1:1 = (πi ◦ pi ◦ π−1i−1) ◦ (πi−1 ◦ pi ◦ π−1i−2) ◦ ... ◦ (π1 ◦ p1)\n(43)\nTherefore, qi+1:1 = πi ◦ pi+1:1 and pi+1:1 ◦ q−1i+1:1 = π−1i ∈ Invariant(N ). By similar considerations, pn+1:i ◦ q−1n+1:i ∈ Invariant(N ).\nLemma 8 Let N = SCM[σ] obeying identifiability and σ is a non-linear element-wise activation function. Let p1, p2 and p3 be three functions such that C(pi) = 1 for all i = 1, 2, 3. Then, p2 ◦ p1 6= p3.\nProof There are invertible linear mappings, Wi, such that pi = σ ◦Wi for all i = 1, 2, 3. Therefore, if:\np2 ◦ p1 = p3 (44)\nthen, (σ ◦W2) ◦ (σ ◦W1) = (σ ◦W3) (45)\nwhich is equivalent to: σ = W−12 ◦W3 ◦W−11 (46)\nin contradiction to the assumption that σ is a non-linear function.\nLemma 9 Let N = SCM[σ], f 6= Id is any function such that C(f) > 0 and π ∈ Invariant(N ). Then, C(π ◦ f) ≤ C(f).\nProof Let f = fn+1:1 be a minimal decomposition of f and fi = σ ◦Wi for i ∈ [n] and Wi are invertible linear mappings. Since π ◦ σ = σ ◦ π, we have:\nπ ◦ f = (σ ◦ (π ◦Wn)) ◦ (σ ◦Wn−1) ◦ ... ◦ (σ ◦W1) (47)\nThis is a decomposition of length n. Therefore, C(π ◦ f) ≤ n = C(f).\nLemma 10 Let N = SCM[σ], f 6= Id is any function and W is an invertible linear mapping. Then, C(f ◦W ) ≤ C(f).\nProof Let f = fn+1:1 be a minimal decomposition of f and fi = σ ◦Wi for i ∈ [n] and Wi are invertible linear mappings. We have:\nf ◦W = (σ ◦Wn) ◦ ... ◦ (σ ◦W2) ◦ (σ ◦W1 ·W ) (48)\nThis is a decomposition of length n. Therefore, C(f ◦W ) ≤ n = C(f)."
    }, {
      "heading" : "Appendix F. Proofs for the Thms. 1 and 4",
      "text" : ""
    }, {
      "heading" : "F.1 Properties of inverses",
      "text" : "Lemma 11 Let N = SCM[σ], where σ is the Leaky ReLU activation function, with parameter a > 0. Then, I(N ) ≤ 3.\nProof We show that the inverse of every function u of complexity 1 can be represented as a decomposition u1 ◦ u2 ◦ u3 for u1, u2, u3 ∈ C. It follows from the following simple identities,\nσ−1 = −Id ◦ σ ◦ −Id/a (49)\nTherefore, W−1 ◦ σ−1 = −W−1 ◦ σ ◦ −Id/a (50)\nAnd, Id = (σ ◦ −Id) ◦ (σ ◦ −Id/a) (51)\nWe obtain, W−1 ◦ σ−1 = (σ ◦ −Id) ◦ (σ ◦W−1/a) ◦ (σ ◦ −Id/a) (52)\nTherefore, C(u−1) ≤ 3. Finally, let p ∈ N be a function of complexity n ≥ 1 with minimal decomposition p = pn+1:1.\nTheorem 1 Let N = SCM[σ] be a NN-SCM with σ that is the Leaky ReLU with parameter a > 0. Then, for any u ∈ N , |C(u−1)− C(u)| is either 0 or 2."
    }, {
      "heading" : "Proof",
      "text" : "Part 1: In this part, we prove by induction on C(u) = n that:\nu−1 = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦ −Id/a) (53)\nWhere u = (σ ◦Wn) ◦ ... ◦ (σ ◦W1) is a minimal decomposition of u. Case C(u) = 0: Then u = Id, u−1 = (σ ◦ −Id) ◦ (σ ◦ −Id/a) = Id and C(u−1) = 0. Case C(u) = 1: Follows immediately from Lem. 11 and Eq. 52. Induction hypothesis: Assume that:\nu−1 = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦ −Id/a) (54)\nWhere u = (σ ◦Wn) ◦ ... ◦ (σ ◦W1) is a minimal decomposition of u. Case C(u) = n+ 1: Let u = un+2:1 = (σ ◦Wn+1) ◦ ... ◦ (σ ◦W1) be a minimal decomposition of u. We denote v = un+1:1. By the induction hypothesis,\nv−1 = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦ −Id/a) (55)\nBy Eq. 50 we can represent: u−1n+1 = −W−1n+1 ◦ σ ◦ −Id/a. We consider that:\nu−1 = v−1 ◦ u−1n+1 = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦ −Id/a) ◦ (W−1n+1 ◦ σ ◦ −Id/a) = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦W−1n+1/a) ◦ (σ ◦ −Id/a)\n(56) In particular, C(u−1) ≤ n+ 3 = C(u) + 2 and C(u) = C((u−1)−1) ≤ C(u−1) + 2. Therefore,\n|C(u−1)− C(u)| ≤ 2 (57)\nPart 2: In this part, we show that |C(u−1)− C(u)| 6= 1 (58)\nAssume by contradiction thatC(u−1) = C(u)+1. Therefore, there is a minimal decomposition u−1 = pn+2:1. By Part 1, there is a decomposition u−1 = qn+3:1. By identifiability,\n∃j1 < ... < jn+2 = n+ 3, π1, ..., πn ∈ Invariant(N ) : qj2:j1 = π1 ◦ p1, ∀i = 2, ..., n : qji+1:ji = πi ◦ pi ◦ π−1i−1 and qjn+2:jn+1 = pn+1 ◦ π−1n\n(59)\nSince jn+2 = n+3 and ji < ji+1, there is k ∈ [n+1] such that jk+1− jk = 2 and for every i ∈ [n+1]\\{k} we have ji+1 − ji = 1. In particular, C(qjk+2:jk) = 1 in contradiction to Lem. 8. We conclude that C(u−1) 6= C(u) + 1 and C(u) = C((u−1)−1) 6= C(u−1) + 1.\nLemma 12 Let N = SCM[σ] where σ is the Leaky ReLU activation function, with parameter a > 0. We define a function from one decomposition to another as follows:\nF+(un+1:1) = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦ −Id/a) (60)\nwhere ui = σ ◦Wi for i ∈ [n]. Then, for all j ∈ [n+ 1] we have:\n[F+(un+1:1)]n−j+3:1 ◦ u ◦D = (−σ−1) ◦ uj:1 ◦D (61)\nProof In the proof of Thm. 1, we built recursively the composition:\nF+(un+1:1) = (σ ◦ −Id) ◦ (σ ◦W−11 /a) ◦ ... ◦ (σ ◦W−1n /a) ◦ (σ ◦ −Id/a) (62)\nto invert un+1:1, i.e, F+(un+1:1) = u−1n+1:1. Similarly, for all j ∈ [n+ 1]:\n(σ◦−Id)◦ [F+(un+1:1)]n−j+3:1 = (σ◦−Id)◦(σ◦W−1j /a)◦ ...◦(σ◦W−1n /a)◦(σ◦−Id/a) = u−1n+1:j (63)\nIn particular, for all j ∈ [n+ 1]:\n[F+(un+1:1)]n−j+3:1 = (−σ−1) ◦ u−1n+1:j (64)\nTherefore, for all j ∈ [n+ 1]:\n[F+(un+1:1)]n−j+3:1 ◦ u ◦D = (−σ−1) ◦ u−1n+1:j ◦ u ◦D = (−σ−1) ◦ uj:1 ◦D (65)"
    }, {
      "heading" : "F.2 Properties of compositions",
      "text" : "Lemma 13 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. Then, there are functions f, g, h ∈ N such that C(f ◦ g ◦ h) < C(f) + C(g) + C(h), f 7 g and g 7 h.\nProof Let W 6= Id be any invertible M ×M real matrix. Consider the following functions:\nf = (σ ◦W ), g = (σ ◦ −Id) and h = (σ ◦ −W−1/a) (66)\nSince both f and g are functions of complexity 1, the complexity of f ◦ g is at most 2. The complexity of f ◦ g is not 0, since f ◦ g 6= Id. In addition, by Lem. 8, C(f ◦ g) 6= 1. Similarly, C(g ◦ h) = 2. On the other hand, f ◦ g ◦ h = σ and C(σ) = 1.\nLemma 14 Let N = SCM[σ] be a NN-SCM obeying identifiability. Let u ∈ N be a function such that C(u) = n > 0 and a ∈ N be a function of complexity 1. Then, there is 0 ≤ r ≤ n such that:\nC(ur+1:1 ◦ a) = 1 and C(u) = C(u ◦ a) + r − 1 (67)\nProof If u 7 a then we take r = 0. Otherwise, u . a. Let u = un+1:1 be a minimal decomposition of u and let u ◦ a = vd+1:1 be a minimal decomposition of u ◦ a. Therefore,\nu ◦ a = un+1:1 ◦ a = vd+1:1 (68)\nBy identifiability, there is 0 ≤ r ≤ n such that:\nur+1:1 ◦ a = π1 ◦ v1 (69)\nFor some π1 ∈ Invariant(N ). In addition,\n∃j2 = r + 1 < ... < jd+1 = n+ 1 and π2, ..., πd−1 ∈ Invariant(N ) : ∀i = 2, ..., d− 1 : uji+1:ji = πi ◦ vi ◦ π−1i−1 ujd+1:jd = vn ◦ π−1d−1\n(70)\nTherefore, ∀i = 2, ..., d : 1 = C(uji+1:ji) = ji+1 − ji (71)\nIn particular, ∀i = 2, ..., d : ji = r + i− 1 and ui = πi ◦ vi ◦ π−1i−1 (72)\nWe also have, jd+1 = n+ 1. Therefore,\nC(u) = n = jd+1 − 1 = d+ r − 1 = C(u ◦ a) + r − 1 (73)\nLemma 15 Let N = SCM[σ] be a NN-SCM obeying identifiability. Let u ∈ N be a function such that C(u) = n > 0 and a ∈ N be a function of complexity 1. If u . a, u = b ◦ d such that b 7 d and C(a−1) = C(d), then, we have d . a or d ◦ a ∈ Invariant(N ).\nProof Let b = bn+1:1 and d = dm+1:1 be minimal decompositions of b and d (resp.). We consider that u = b ◦ d = bn+1:1 ◦ dm+1:1 is a minimal decomposition of u. We denote u = un+m+1:1 where ui = bi−m for m+ 1 ≤ i ≤ n+m and ui = di for i ≤ m. By Lem. 14 we have:\n∃0 ≤ r ≤ n+m : C(ur+1:1 ◦ a) = 1 and C(u) = C(u ◦ a) + r − 1 (74)\nIf r ≤ m: then we have C(d ◦ a) = C(um+1:r+1 ◦ ur+1:1 ◦ a) ≤ m− r + C(ur+1:1 ◦ a) = m− r + 1. If r 6= 0, then d . a as desired. On the other hand, if r = 0 then C(u) = C(u ◦a) + 1 and u 7 a in contradiction. If r ≥ m: then there is a function v such that C(v) = 1 and:\nv = br−m+1:1 ◦ d ◦ a (75)\nAlternatively, v ◦ a−1 = br−m+1:1 ◦ d (76)\nSince b 7 d, by Lem. 5,\nC(br−m+1:1 ◦ d) = C(br−m+1:1) + C(d) = C(d) + r −m (77)\nIf r −m > 1: then C(d) + r −m = C(v ◦ a−1) ≤ C(a−1) + 1 = C(d) + 1 in contradiction to r > 1. If r − m = 0: then v = d ◦ a. Therefore, d . a and C(d ◦ a) = 1 (since C(a) = 1, 1 ≤ C(d) and C(d ◦ a) = 1 < C(a) + C(d)). If r −m = 1: then, we have,\nC(v ◦ a−1) = C(b1 ◦ d) = C(b1) + C(d) = 1 + C(d) = 1 + C(a−1) = C(v) + C(a−1) (78)\nAlternatively, v 7 a−1. By Lem. 7, for p = v ◦ a−1 = b1 ◦ d we have: d ◦ a ∈ Invariant(N ). We conclude that d . a or d ◦ a ∈ Invariant(N ).\nLemma 16 (Composition reduction) Let N = SCM[σ] be a NN-SCM obeying identifiability. Let u, v ∈ N . Then, we can represent u = a ◦ b and v = b−1 ◦ c such that: a 7 c, b−1 7 c and\nC(a) + C(b)− I − 1 ≤ C(a ◦ b) . (79)\nProof Let u = un+1:1 and v = vm+1:1 be minimal decompositions of u and v (resp.). Let u ◦ v = zd+1:1 be a minimal decomposition of u ◦ v. By identifiability we have:\n∃j1 = 1 < ... < jd+1 = n+m+ 1,∃π1, ..., πd−1 ∈ Invariant(N ) [un ◦ ... ◦ u1 ◦ vm ◦ ... ◦ v1]j2:j1 = π1 ◦ z1 ∀i = 2, ..., d− 1 : [un ◦ ... ◦ u1 ◦ vm ◦ ... ◦ v1]ji+1:ji = πi ◦ zi ◦ π−1i−1 [un ◦ ... ◦ u1 ◦ vm ◦ ... ◦ v1]jd+1:jd = zd ◦ π−1d−1\n(80)\nHere, [fk ◦ ... ◦ f1]i:j = fi−1 ◦ ... ◦ fj for 0 ≤ j < i and ui:i = Id. We consider two options. The first: there is no index k such that jk ≤ m and m + 1 < jk+1. The second: there is such k.\nCase 1: In this case, for every index i = 2, ..., d− 1:\nvji+1:ji = πi ◦ zi ◦ π−1i−1 or uji+1−m:ji−m = πi ◦ zi ◦ π−1i−1 (81)\nAnd for i = 1 and i = d: vj2:j1 = π1 ◦ z1 and vjd+1:jd = zd ◦ π−1d−1 (82)\nIf the first equation holds,\n1 = C(πi ◦ zi ◦ π−1i−1) = C(vji+1:ji) = ji+1 − ji (83)\nTherefore, for any i = 1, ..., d: ji+1 = ji + 1. We conclude that:\nC(u ◦ v) = d = n+m = C(u) + C(v) and u 7 v (84)\nFinally, we choose a = u, b = Id and c = v. This gives a 7 b, b−1 7 c and a 7 c.\nCase 2: Let 1 ≤ k ≤ d be the index for which jk ≤ m and m+ 1 < jk+1. With no loss of generality, we assume that k 6= 1, d. For any index i 6= k such that 1 < i < d, we have,\nvji+1:ji = πi ◦ zi ◦ π−1i−1 or uji+1−m:ji−m = πi ◦ zi ◦ π−1i−1 (85)\nAnd if i = 1 or i = d: vj2:j1 = π1 ◦ z1 or vjd+1:jd = zd ◦ π−1d−1 (86)\nAs in Case 1, we conclude that ji+1 = ji + 1. Therefore, we have jk = k and we denote jk+1 = m+ r + 1, i.e,\n[un ◦ ... ◦ u1 ◦ vm ◦ ... ◦ v1]jk+1:jk = ur+1:1 ◦ vm+1:k = πk ◦ zk ◦ π−1k−1 (87)\nFinally, we choose: b = v−1m+1:k, a = u ◦ b−1 and c = vk:1 (88) It follows immediately that v = b−1 ◦ c such that b−1 7 c. In addition, we have, u = a ◦ b and u ◦ v = a ◦ c. We consider that:\nC(a ◦ c) = C(zd+1:1) = C(zd+1:k ◦ zk:1) = C(zd+1:k) + C(zk:1) (89)\nBy Lem. 5 we have C(c) = C(vk:1) = k − 1 = C(zk;1). In addition,\na = u ◦ b−1 = un+1:r+1 ◦ ur+1:1 ◦ vm+1:k (90)\nBy identifiability, ur+1:1 ◦ vm+1:k = πk ◦ zk ◦ π−1k−1 and un+1:r+1 ◦ πk = zd+1:k+1. Therefore,\na = u ◦ b−1 = zd+1:k ◦ π−1k−1 (91)\nThus, by Lem. 10, C(a) ≤ C(zd+1:k) since zd+1:k 6= Id because k ≤ d. In particular,\nC(a ◦ c) = C(zd+1:k) + C(zk:1) ≥ C(a) + C(c) (92)\nFinally, by Lem. 4, we have: C(a ◦ c) = C(a) + C(c) or a 7 c. Since ur+1:1 = π−1k ◦ zk ◦ πk−1 ◦ b we have:\nC(u) = C(un+1:r+1 ◦ πk ◦ zk ◦ π−1k−1 ◦ b) = C(un+1:r+1) + C(πk ◦ zk ◦ π−1k−1 ◦ b) = C(a)− 1 + C(πk ◦ zk ◦ π−1k−1 ◦ b) ≥ C(a)− 1 + C(b)− C((πk ◦ zk ◦ π−1k−1)−1) ≥ C(a)− 1 + C(b)− C(πk−1 ◦ z−1k ◦ π−1k ) ≥ C(a)− 1 + C(b)− I\n(93)\nTheorem 4 LetN = SCM[σ] be a NN-SCM obeying identifiability. Then, for all f, g, h ∈ N such that f 7 g, g 7 h and I ≤ C(g), we have:\nC(f ◦ g ◦ h) = C(f) + C(g) + C(h) (13)\nProof Denote C(f) = i, C(g) = n, C(h) = m. We prove the theorem by induction on C(f) = i. Case i = 0: In this case f = Id and C(f ◦ g ◦ h) = C(g ◦ h) = C(g) + C(h) as desired. Case i = 1: Let\ng = gn+1:1 and h = hm+1:1 (94)\nbe minimal decompositions of g and h (resp.). In addition, we denote: b = gn+1:n−q+1 where q = C(f−1) and c = gn−q+1:1 ◦ hm+1:1. Assume by contradiction that f . g ◦ h. Then, we apply Lem. 15 and conclude that f . b or f ◦ b ∈ Invariant(N ). In addition, since C(b) = C(f−1) ≤ I ≤ C(g), we decompose c as follows: c = d1 ◦ d2, d1 = gn−q+1:1 and d2 = hm+1:1. By Lem. 5, b 7 d1 and d1 7 d2. In addition,\nC(f ◦ b ◦ d1) = C(f ◦ g) = C(f) + C(g) = C(f) + C(b ◦ d1) = C(f) + C(b) + C(d1) (95)\nTherefore, by Lem. 5, f 7 b and C(f ◦ b ◦ d1) > C(d1). This immediately eliminates the option f . b. On the other hand, if f ◦ b ∈ Invariant(N ) then by Lem. 9,\nC(f ◦ b ◦ d1) ≤ C(d1) (96)\nin contradiction. Therefore, f 7 g ◦ h and\nC(f ◦ g ◦ h) = C(f) + C(g ◦ h) = C(f) + C(g) + C(h) (97)\nInduction hypothesis: Assume that the statement holds for all f such that C(f) = i ≥ 1, g and h that satisfy, i.e., I ≤ C(g), f 7 g and g 7 h =⇒ C(f ◦ g ◦ h) = C(f) + C(g) + C(h) (98) Induction step: Let f be a function such that C(f) = i + 1 and f 7 g. We denote f = f ′ ◦ f1 such that f ′ 7 f1, C(f1) = 1 and C(f ′) = i. Then, by Lem. 5, f1 7 g. Therefore, by case i = 1 we have: C(f1 ◦ g ◦ h) = C(f1) + C(g) + C(h) = C(f1 ◦ g) + C(h). Alternatively, f1 ◦ g 7 h. In addition, since f 7 g then C(f ′ ◦ (f1 ◦ g)) = C(f) + C(g) = C(f ′) + 1 + C(g) = C(f ′) + C(f1 ◦ g). Alternatively, f ′ 7 f1 ◦ g. Therefore, because I ≤ C(g) < C(f1 ◦ g) by the induction hypothesis we have:\nI ≤ C(f1 ◦ g), f ′ 7 f1 ◦ g and f1 ◦ g 7 h =⇒ C(f ◦ g ◦ h) = C(f ′ ◦ (f1 ◦ g) ◦ h) = C(f ′) + C(f1 ◦ g) + C(h)\n= C(f ′) + 1 + C(g) + C(h) = C(f) + C(g) + C(h)\n(99)"
    }, {
      "heading" : "Appendix G. Proofs for Thms. 2 and 3",
      "text" : "Lemma 17 LetN = SCM[C]. LetA = (XA, DA) andB = (XB , DB) be two domains andDZ a distribution. Then, if m ≥ k + Cm, 1Z,B we have: Ck, 0+ 1A,B ≤ Cm, 0A,Z + Cm, 1Z,B (100) In particular,\nC 0+ 1A,B ≤ C 0A,Z + C 1Z,B (101)\nProof Let f ∈ H 0(DA, DZ ;m) and g ∈ H 1(DZ , DB ;m). Then, by the first item of Lem. 2, for D1 :← f ◦DA, D2 :← DZ , D3 :← DB , p :← g and m ≥ k + Cm, 1Z,B , we have:\ndisck(g ◦ f ◦DA, DB) ≤ discm(f ◦DA, DZ) + discm(g ◦DZ , DB) ≤ 0 + 1 (102)\nTherefore, Ck, 0+ 1A,B ≤ C(f) + C(g) = Cm, 0A,Z + Cm, 1Z,B (103) In order to prove the second inequality, we choose k ≥ max { E 0+ 1A,B , E 1 Z,B , E 0 A,Z } and obtain:\nC 0+ 1A,B = C k, 0+ 1 A,B ≤ Cm, 0A,Z + Cm, 1Z,B = C 0A,Z + C 1Z,B (104)\nLemma 18 Let N = SCM[C] and A = (XA, DA) and B = (XB , DB) are two domains. If m ≥ k ≥ E 0A,B then H 0(DA, DB ;m) ⊂ H 0(DA, DB ; k).\nProof Let y ∈ H 0(DA, DB ;m). Since m ≥ k ≥ E 0A,B , we have: C(y) ≤ Cm, 0A,B ≤ C 0A,B = Ck, 0A,B . In addition, sinceDk ⊂ Dm: disck(y◦DA, DB) ≤ discm(y◦DA, DB) ≤ 0. In particular, y ∈ H 0(DA, DB ; k).\nLemma 19 LetN = SCM[C]. LetA = (XA, DA) andB = (XB , DB) be two domains andDZ a distribution. If m ≥ max { E 0+ 1A,B , E 0 A,Z , E 1 Z,B } , then, DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B iff: C 0+ 1A,B = C 1 Z,B + C 0 A,Z (105)"
    }, {
      "heading" : "Proof",
      "text" : "Part 1: Assume that DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Then, there is a function yAB ∈ Z(DA, DZ , DB ;m, 0, 1)∩H 0+ 1(DA, DB ;m). In addition, there is a decomposition yAB = yB ◦ y−1A such that yB ∈ H 1(DZ , DB ;m) and y−1A ∈ H 0(DA, DZ ;m) and yB 7 y−1A . Therefore,\nC 0+ 1A,B = C m, 0+ 1 A,B = C(yAB) = C(yB) + C(y −1 A ) = C m, 0 A,Z + C m, 1 Z,B = C 0 A,Z + C 1 Z,B (106)\nPart 2: Assume that C 0+ 1A,B = C 1 Z,B + C 0 A,Z . Let t = m + C 1 Z,B , yB ∈ H 1(DZ , DB ; t) and y−1A ∈ H 0(DA, DZ ; t) and denote yAB = yB ◦ y−1A . By the second item of Lem. 2, for D1 :← DA, D2 :← DB , DZ :← DZ , p :← yB and t ≥ m+ C(yB) we have:\ndiscm(yAB ◦DA, DB) ≤ disct(y−1A ◦DA, DZ) + disct(yB ◦DZ , DB) ≤ 0 + 1 (107)\nTherefore, C 0+ 1A,B ≤ C(yAB) ≤ C(yB) + C(y−1A ) = C 1Z,B + C 0A,Z = C 0+ 1A,B (108)\nIn particular, yAB ∈ Z(DA, DZ , DB ;m, 0, 1) ∩H 0+ 1(DA, DB ;m) and yB 7 y−1A . Alternatively, DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution.\nNote: we assumed that H 1(DZ , DB ; t) 6= ∅ and H 0(DA, DZ ; t) 6= ∅. It follows from the assumption that for every two distributions of interest in the paper, DI and DJ , and an error rate > 0, there is a function h of finite complexity such that disc∞(h ◦DI , DJ) ≤ ."
    }, {
      "heading" : "G.1 Topological properties of decompositions",
      "text" : "Let un+1:1 be a decomposition such that ui = σ ◦Wi for i ∈ [n]. We denote:\nS (W ) = { W̄ ∣∣ ||W − W̄ ||2 < } (109)\nand, S (W1, ...,Wn) = S (W1)× ...× S (Wn) (110)\nDefinition 1 (Perturbation of a function) Let N = SCM[σ] be a NN-SCM and a decomposition un+1:1. The set of -perturbations of un+1:1 is:\nS (un+1:1) = {ūn+1:1 ∣∣ ūi = σ ◦ W̄i for i ∈ [n] and (W̄1, ..., W̄n) ∈ S (W1, ...,Wn)} (111)\nLemma 20 Let N = SCM[σ] be a NN-SCM with σ that is Leaky ReLU with parameter a > 0. Let un+1:1 be a decomposition such that ui = σ ◦Wi for i ∈ [n]. Therefore, we have:\n∀x : ||un+1:1(x)||2 ≤ max{a, 1}n ◦ n∏\ni=1\n||Wi||2 · ||x||2 (112)\nand,\n∀x : ||σ−1 ◦ un+1:1(x)||2 ≤ max{a, 1}n−1 ◦ n∏\ni=1\n||Wi||2 · ||x||2 (113)\nProof We prove the second inequality by induction on n ≥ 1. The first inequality follows immediately from the second and ||σ(x)||2 ≤ ||a · x||2 ≤ a · ||x||2. Case n = 1: We have:\n||σ−1 ◦ un+1:1(x)||2 = ||W1(x)||2 ≤ ||W1||2 · ||x||2 (114)\nInduction hypothesis: We assume that:\n∀x : ||σ−1 ◦ un+1:1(x)||2 ≤ max{a, 1}n−1 ◦ n∏\ni=1\n||Wi||2 · ||x||2 (115)\nAs mentioned above, an immediate consequence is the following inequality,\n∀x : ||un+1:1(x)||2 ≤ max{a, 1}n ◦ n∏\ni=1\n||Wi||2 · ||x||2 (116)\nCase n+ 1: We have:\n||σ−1 ◦ un+2:1(x)||2 = ||Wn ◦ un+1:1(x)||2 ≤ ||Wn+1 ◦ un+1:1(x)||2 ≤ ||Wn+1||2 · ||un+1:1(x)||2\n(117)\nAnd by the induction hypothesis we have:\n||σ−1 ◦ un+2:1(x)||2 ≤ ||Wn+1||2 ·max{a, 1}n ◦ n∏\ni=1\n||Wi||2 · ||x||2\n= max{a, 1}n ◦ n+1∏\ni=1\n||Wi||2 · ||x||2 (118)\nLemma 21 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = n. Then, there is > 0 such that if ūn+1:1 ∈ S (un+1:1) then: C(ūn+1:1) = n.\nProof We denote un+1:1 = (σ ◦Wn) ◦ ... ◦ (σ ◦W1). Assume by contradiction that for all > 0, there is ūn+1:1 = (σ ◦ W̄n) ◦ ... ◦ (σ ◦ W̄1) such that\nC(ūn+1:1) < n and ∀i ∈ [n] : ||Wi − W̄i||2 < (119)\nThen, there is a sequence {ukn+1:1}∞k=1 such that ∀i ∈ [n] : ||Wi −W ki ||2 < /k we have C(ukn+1:1) < n. Here, ukn+1:1 = u k n ◦ ... ◦ uk1 = (σ ◦W kn ) ◦ ... ◦ (σ ◦W k1 ). By identifiability, for every k, there are indexes pk, qk ≤ n such that pk + 2 ≤ qk and C(ukqk+1:pk) = 1 (120)\nSince the sequence {ukn+1:1}∞i=1 is infinitely long, by the pigeonhole principle, there is a tuple (p, q) such that p + 2 ≤ q and there are infinitely many indexes k that satisfy C(ukq+1:p) = 1. Therefore, with no loss of generality, we can assume that for all k we have C(ukq+1:p) = 1 for a fixed tuple of indexes p, q ≤ n that satisfy p+ 2 ≤ q (or replace the original sequence with such a sequence). We denote Uk an invertible linear mapping such that:\nukq+1:p = (σ ◦ Uk) (121)\nIn particular, σ−1 ◦ ukq+1:p = Uk (122)\nTherefore, ∀x : ||σ−1 ◦ ukq+1:p(x)||2 = ||Uk(x)||2 (123)\nBy Lem. 20,\n||σ−1 ◦ ukq+1:p(x)||2 ≤ max{a, 1}q−p−1 · q∏\ni=p\n||uki || · ||x||2 (124)\nTherefore,\n||Uk||2 ≤ max{a, 1}q−p−1 · q∏\ni=p\n(||Wi||+ /k) (125)\nAnd also, if the input dimension is M and || · ||F is the Frobenius norm, then,\n||Uk||F ≤M ·max{a, 1}q−p−1 · q∏\ni=p\n(||Wi||+ /k) (126)\nIn particular, the sequence Uk is bounded. Thus, by the Bolzano-Weierstrass theorem, there is a subsequence Ukt that converges to a matrix U (w.r.t the `2 norm). With no loss of generality, we can replace Uk with the sequence Ukt . Alternatively, we can assume that Uk converges to the matrix U (w.r.t the `2 norm). In addition, we have W ki →Wi (w.r.t the `2 norm). Therefore, since for any x, fEn,...,E1(x) (see Tab. 1) is continuous as a function of a the matrices En, ..., E1, we have:\n∀x : lim k→∞ ukq+1:p(x) = uq+1:p(x) (127)\nOn the other hand, since for any x, the function fE(x) is continuous as a function of the matrix E, we have:\nlim k→∞ ukq+1:p(x) = lim k→∞ (σ ◦ Uk)(x) = (σ ◦ U)(x) (128)\nFinally, ∀x : uq+1:p(x) = (σ ◦ U)(x) (129)\nAlternatively, uq+1:p = (σ ◦ U) (130)\nTherefore, C(uq+1:p) ≤ 1. On the other hand, since C(un+1:1) = n, by Lem. 5 we have C(uq+1:p) = q − p > 1 in contradiction.\nLemma 22 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = 1. Then, for every > 0 there is a ūn−1 ∈ S (un−1) such that C(un ◦ ūn−1 ◦ un−2:1) > 1.\nProof We assume that C(un+1:1) = 1 and show that for every > 0 there is ūn−1 ∈ S (un−1) such that C(un ◦ ūn−1 ◦ un−2:1) > 1. Assume by contradiction that there is > 0 such that for all ūn−1 ∈ S (un−1) we have C(un ◦ ūn−1 ◦ un−2:1) = 1. By Lem. 8, there is no possibility that n = 2. Therefore, n ≥ 3. We denote ui = σ ◦Wi for i ∈ [n]. Let\n∀k = 1, 2 : ūkn+1:1 = un ◦ ukn−1 ◦ un−2:1 (131)\nwhere ukn−1 = un−1 ◦ Vk such that:\n∀k = 1, 2 : ||Wn−1 · Vk −Wn−1||2 ≤ ||Wn−1||2 · ||Vk − Id||2 < (132)\nand σ−1 ◦ (Wn−1 ◦ V1 · V −12 ◦W−1n−1) ◦ σ is not linear (133)\nThus, ∀k = 1, 2 : ūkn−1 ∈ S (un−1). Therefore,\nun ◦ (un−1 ◦ Vk) ◦ un−2:1 = σ ◦ Uk (134)\nfor some linear mappings Uk for k = 1, 2. In particular,\nσ ◦Wn ◦ σ ◦Wn−1 · V1 · V −12 ·W−1n−1 ◦ σ−1 ◦W−1n ◦ σ−1\n=un ◦ u−1n−1 ◦ V1 · V −12 ◦ u−1n−1 ◦ u−1n =(un ◦ (un−1 ◦ V1) ◦ un−2:1) ◦ (un ◦ (un−1 ◦ V2) ◦ un−2:1)−1 =(σ ◦ U1) ◦ (σ ◦ U2)−1 = σ ◦ U1 · U−12 ◦ σ−1\n(135)\nAlternatively, σ ◦Wn−1 · V1 · V −12 ·W−1n−1 · σ−1 = W−1n · U1 · U−12 ·Wn (136)\nin contradiction to the assumption that σ ◦Wn−1 · V1 · V −12 ·W−1n−1 · σ−1 is not linear.\nLemma 23 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = 1. Then, for every > 0 there is an ūn:2 ∈ S (un:2) such that C(un ◦ ūn:2 ◦ u1) = n.\nProof We prove this claim by induction on n.\nCase n = 2: By Lem. 8, there is no possibility that C(u3:1) = 1. Therefore, the claim follows immediately.\nCase n = 3: By Lem. 22, there is ū4:1 ∈ S (u4:1) such that C(ū4:1) = k > 1. If k = 2, then, by identifiability, C(ū4:2) = 1 or C(ū3:1) = 1, in contradiction to Lem. 8. Therefore, k = 3 = n.\nCase n = 4: By Lem. 22, there is ū5:1 ∈ S /4(u5:1) such that C(ū5:1) = k > 1. If k = 3, then, by identifiability, there is an index i ≤ n− 1 such that: C(ūi+2:i) = 1 in contradiction to Lem. 8. If k = 2, then, by identifiability, there are three options:\n• C(ū3:1) = 1 and C(ū5:3) = 1.\n• C(ū5:2) = 1.\n• C(ū4:1) = 1.\nThe first option is not a possibility, since it contradicts Lem. 8. The second and third options are analogous - we prove them in parallel. By Case n = 3, there is ū′4:2 ∈ S /4(ū4:2) such that C(u4 ◦ ū′4:2) = 3. By Lem. 21, there is 0 > 0 such that for every ū′′4:2 ∈ S 0(ū′4:2) we have: C(u4 ◦ ū′4:2) = 3. We denote ′ = min{ /4, 0}. Again, by Case n = 3, there is ū′′4:2 ∈ S ′(ū′4:2) such that C(ū′′4:2 ◦ u1) = 3. Therefore, we have:\nC(ū′′4:2 ◦ u1) = 3 and C(u4 ◦ ū′′4:2) = 3 (137)\nBy the above, C(u4 ◦ ū′′4:2 ◦ u1) 6= 2, 3. In addition, by Lem. 21 and Lem. 22 there is ′′ ≤ /4 such that there is ū′′′4:2 ∈ S ′′(ū′4:2) that satisfies:\nC(u4 ◦ ū′′′4:2 ◦ u1) > 1, C(ū′′′4:2 ◦ u1) = 3 and C(u4 ◦ ū′′′4:2) = 3 (138)\nTherefore, C(u4 ◦ ū′′′4:2 ◦ u1) 6= 1, 2, 3. We conclude that C(u4 ◦ ū′′′4:2 ◦ u1) = 4. We consider that ū′′′4:2 ∈ S /4(ū′′4:2) ⊂ S /2(ū′4:2) ⊂ S3 /4(ū4:2) ⊂ S (u4:2). Alternatively, we found ū′′′4:2 ∈ S (u4:2) such that C(u4 ◦ ū′′′4:2 ◦ u1) = 4. Induction hypothesis: We assume that for every k ≤ n and decomposition uk+1:1 such that C(uk+1:1) = 1, for every > 0 there is an ūk:2 ∈ S (uk:2) such that C(uk ◦ ūk:2 ◦ u1) = k. Case n+ 1: By the induction hypothesis,\n∃ūn:2 ∈ S /2(un:2) such that C(un ◦ ūn:2 ◦ u1) = n (139)\nIn addition, by Lem. 21, there is 0 > 0 such that\n∀ū′n+1:1 ∈ S 0(un ◦ ūn:2 ◦ u1) =⇒ C(ū′n+1:1) = n (140)\nIn particular,\n∀ū′n+1:2 ∈ S 0(ūn+1:2) =⇒ ū′n+1:2 ◦ u1 ∈ S 0(un ◦ ūn:2 ◦ u1) =⇒ C(ū′n+1:2 ◦ u1) = n (141)\nWe denote ′ = min{ /2, 0} and obtain: ∀ū′n+1:2 ∈ S ′(ūn+1:2) =⇒ C(ū′n+1:2 ◦ u1) = n. In addition, by Case n = 4, there is ū′n+1:2 ∈ S ′(ūn+1:2) such that C(un+1 ◦ ū′n+1:n−2) = 4. Thus, we have:\nC(ū′n+1:2 ◦ u1) = n and C(un+1 ◦ ū′n+1:n−2) = 4 (142)\nTherefore, by Thm. 4, since un+1 7 ū′n+1:n−2 7 ū′n−2:1 and C(ū′n+1:n−2) = I = 3 we have:\nC(un+1 ◦ ū′n+1:2 ◦ u1) = n+ 1 (143)\nIn addition, ū′n+1:2 ∈ S (un+1:2) (144) since ū′n+1:2 ∈ S ′(ūn+1:2) and ūn+1:2 ∈ S /2(un+1:2) and ′ ≤ /2.\nLemma 24 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = n. Then, if for every > 0 there is ūn+1:1 ∈ S (un+1:1) such that C(ūq+1:p) = 1 then C(uq+1:p) ≤ 1.\nProof Assume that for every > 0, there is ūn+1:1 ∈ S (un+1:1) such that C(ūq+1:p) = 1. Let {ukn+1:1}∞k=1 be a sequence such that ukn+1:1 ∈ S1/k(un+1:1) and C(ukq+1:p) = 1. We denote, uki = (σ ◦W ki ) for i ∈ [n] and k ∈ N. We denote Uk a invertible linear mapping such that:\nukq+1:p = (σ ◦ Uk) (145)\nIn particular, σ−1 ◦ ukq+1:p = Uk (146)\nTherefore, ∀x : ||σ−1 ◦ ukq+1:p(x)||2 = ||Uk(x)||2 (147)\nBy Lem. 20,\n||σ−1 ◦ ukq+1:p(x)||2 ≤ max{a, 1}q−p−1 · q∏\ni=p\n||W ki ||2 · ||x||2\n≤ max{a, 1}q−p−1 · q∏\ni=p\n(||Wi||2 + /k) · ||x||2 (148)\nTherefore,\n||Uk||2 ≤ max{a, 1}q−p+1 · q∏\ni=p\n(||Wi||+ /k) (149)\nAnd also, if the input dimension is M and || · ||F is the Frobenious norm, then,\n||Uk||F ≤M ·max{a, 1}q−p−1 · q∏\ni=p\n(||Wi||+ /k) (150)\nIn particular, the sequence Uk is bounded. Thus, by the Bolzano-Weierstrass theorem, there is a subsequence Ukt that converges to a matrix U (w.r.t the `2 norm). With no loss of generality, we can replace Uk with the sequence Ukt . Alternatively, we can assume that Uk converges to the matrix U (w.r.t the `2 norm). In addition, we have W ki →Wi (w.r.t the `2 norm). Therefore, since for any x, fEn,...,E1(x) is continuous as a function of a the matrices En, ..., E1, we have:\nlim k→∞\nukq+1:p(x) = uq+1:p(x) (151)\nOn the other hand, since for any x, the function fE(x) is continuous as a function of a matrix E, we have:\nlim k→∞ ukq+1:p(x) = lim k→∞ (σ ◦ Uk)(x) = (σ ◦ U)(x) (152)\nFinally, ∀x : uq+1:p(x) = (σ ◦ U)(x) (153)\nAlternatively, uq+1:p = (σ ◦ U) (154)\nTherefore, C(uq+1:p) ≤ 1.\nLemma 25 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 and vk+1:1 be two decompositions such that C(un+1:1) = n and C(vk+1:1) = k. Then, for every > 0, there is ūn:1 ◦ v̄k+1:2 ∈ S (un:1 ◦ vk+1:2) such that: C(un ◦ ūn:1 ◦ v̄k+1:2 ◦ v1) = n+ k.\nProof Assume by contradiction that there is > 0 such that:\n∀ūn:1 ◦ v̄k+1:2 ∈ S (un:1 ◦ vk+1:2) =⇒ C(un ◦ ūn:1 ◦ v̄k+1:2 ◦ v1) < n+ k (155)\nIf un+1:1 7 vk+1:1 then we take ūn:1 = un:1 and v̄k+1:2 = vk+1:2. Otherwise, by identifiability, there are indexes p ≤ k and q ≤ n such that\nC(uq+1:1 ◦vk+1:p) = 1 and C(un+1:q+1 ◦uq+1:1 ◦vk+1:p ◦vp:1) = (n−q)+1+(p−1) = n−q+p (156)\nBy Lem. 21, there is 0 > 0 such that ūn:1 ∈ S (un:1) and v̄k+1:2 ∈ S (vk+1:2) then C(un ◦ ūn:1) = n and C(v̄k+1:2 ◦ v1) = k. We denote 1 = min{ 0, }. We assumed that if ūn:1 ◦ v̄k+1:2 ∈ S (un:1 ◦ vk+1:2) then C(un ◦ ūn:1 ◦ v̄k+1:2 ◦ v1) < n + k. Thus, by identifiability, for every ūn:1 ◦ v̄k+1:2 ∈ S 1(un:1 ◦ vk+1:2) there are r < s such that:\nC(ūr+1:1◦v̄k+1:s) = 1 and C(un◦ūn:r+1◦ūr+1:1◦v̄k+1:s◦v̄s:2◦v1) = (n−r)+1+(s−1) = n−r+s (157)\nThus, by Lem. 22, for every sequence {uin+1:1 ◦ vik+1:2}∞i=1 such that\nuin+1:1 ◦ vik+1:2 ∈ S 1/i(un:1 ◦ vk+1:2) C(uir+1:1 ◦ vik+1:s) = 1 where s ≤ k and r ≤ n C(un ◦ uin:1 ◦ vik+1:2 ◦ v1) = n− r + s\n=⇒ r = q and s = p\n(158)\nIn particular, there is an 0 < 2 ≤ 1 such that\nūn:1 ◦ v̄k+1:2 ∈ S 2(un:1 ◦ vk+1:2), C(ūr+1:1 ◦ v̄k+1:s) = 1 and C(un ◦ ūn:1 ◦ v̄k+1:2 ◦ v1) = n− r + s =⇒ r = q and s = p\n(159)\nTherefore, ūn:1 ◦ v̄k+1:2 ∈ S 2(un:1 ◦ vk+1:2) =⇒ C(ūq+1:1 ◦ v̄k+1:p) = 1 (160)\nIn particular, ūq+1:1 ◦ v̄k+1:p ∈ S 2(uq+1:1 ◦ vk+1:p) =⇒ C(ūq+1:1 ◦ v̄k+1:p) = 1 (161)\nin contradiction to Lem. 23.\nLemma 26 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition. Then, for every > 0 there is ūn:2 ∈ S (un:2) such that: C(un ◦ ūn:2 ◦ u1) = n.\nProof If C(un+1:1) = n then we take ūn+1:1 = un+1:1. Otherwise, we denote C(un+1:1) = k and by identifiability, ∃j1 = 1 < ... < jk+1 = n+ 1 : C(uji+1:ji) = 1 (162) By Lem. 8, for every i such that ji+1 6= ji + 1, we have: ji + 3 ≤ ji+1. By Lem. 23, for each i such that ji+1 6= ji + 1\n∃ūji+1−1:ji+1 ∈ S /k(uji+1−1:ji+1) such that C(uji+1 ◦ ūji+1−1:ji+1 ◦ uji) = ji+1 − ji (163)\nWe denote: ∀i ∈ [k] : v1ji = uji and ∀t ∈ [n] \\ {j1, ..., jk} : v1t = ūt (164)\nWe denote: ∀i ∈ [k + 1] : j1i = ji (165)\nWe have: ∀i ∈ [k] : C(v1j1i+1:j1i ) = j 1 i+1 − j1i\nv1n:2 ∈ S /k(un:2) v11 = u1\n(166)\nBy Lem. 25, there is a decomposition\nv̄1j13−1:j11+1 ∈ S /k(v 1 j13−1:j11+1) (167)\nsuch that C(v1 j13 ◦ v̄1 j13−1:j11+1 ◦ v1 j11 ) = j13 − j11 . Next, we replace v1n+1:1 with v2n+1:1 defined as follows:\n∀i ≥ j13 : v2i = v1i , ∀1 < i < j13 : v2i = v̄1i and v21 = v11 = u1 (168)\nand we denote: j21 = 1 and ∀i ∈ [k] \\ {1} : j2i = j1i+1 (169)\nWe have: ∀i ∈ [k − 1] : C(v2j2i+1:j2i ) = j 2 i+1 − j2i\nv2n:2 ∈ S2 /k(un:2) v21 = v 1 1 = u1\n(170)\nWe continue the process of replacing each vtn+1:1 with v t+1 n+1:1 for k − 1 times. For each iteration, we have a decomposition that satisfies:\n∀i ∈ [k − t+ 1] :C(vtjti+1:jti ) = j t i+1 − jti , jt1 = 1, jtk−t+2 = n+ 1, vt1 = u1,\nvtn = un and v t n:2 ∈ St /k(un:2)\n(171)\nand obtain that ūn+1:1 = vkn+1:1 satisfies:\nC(ūn+1:1) = n, ū1 = u1, ūn = un and ūn:2 ∈ S (un:2) (172)\nLemma 27 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 such that C(un+1:1) = n. Then, for all > 0 there is ūn+1:1 ∈ S (un+1:1) such that C(ū−1n+1:1) = n+ 2.\nProof We define a one-to-one function between compositions:\nF((σ ◦ Un) ◦ ... ◦ (σ ◦ U1)) = (σ ◦ U−11 /a) ◦ ... ◦ (σ ◦ U−1n /a) (173)\nWe consider that for any decomposition dn+1:1, we have:\nd−1n+1:1 = (σ ◦ −Id) ◦ F(dn+1:1) ◦ (σ ◦ −Id/a) (174)\nLet u = un+1:1 and ūn+1:1 be two decompositions such that ui = σ ◦Wi and ūi = σ ◦ Ui, where Wi and Ui are invertible linear mapping. Since W−1 is continuous as a function of W , there is ′ > 0 such that\n∀i ∈ [n] : ||W−1i /a− U−1i /a|| < ′ =⇒ ∀i ∈ [n] : ||Wi − Ui|| < (175)\nIn particular, S ′(F(un+1:1)) ⊂ { v̄n+1:1 ∣∣ F−1(v̄n+1:1) ∈ S (un+1:1) }\n(176)\nBy Lem. 26, there is v̄n+1:1 ∈ S ′(F(un+1:1)) such that C((σ ◦ −Id) ◦ v̄n+1:1 ◦ (σ ◦ −Id/a)) = n+ 2 (177)\nTherefore, there is a decomposition ūn+1:1 = F−1(v̄n+1:1) ∈ S (un+1:1) such that: ū−1n+1:1 = (σ ◦ −Id) ◦ v̄n+1:1 ◦ (σ ◦ −Id/a) (178)\nThus, C(ū−1n+1:1) = n+ 2.\nTheorem 2 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) ∈ RM×M×n such that C(u−1n+1:1) = C(un+1:1) + 2 and ui = σ ◦Wi (for i ∈ [n]) is open and dense in RM×M×n. Proof We denote by F the function from Eq. 173, Z = { (W1, ...,Wn) ∈ RM×M×n ∣∣ ∀i ∈ [n] : ui = σ ◦Wi, Wi is invertible and C(u−1n+1:1) = C(un+1:1) + 2 } (179) and,\nZ ′ = { (W1, ...,Wn) ∈ RM×M×n ∣∣ ∀i ∈ [n] : Wi is invertible } (180)\nOpenness: Let un+1:1 be a decomposition such that C(u−1n+1:1) = n+ 2 and ui = σ ◦Wi. We have: u−1n+1:1 = (σ ◦ −Id) ◦ F(un+1:1) ◦ (σ ◦ −Id/a) (181)\nis a minimal decomposition. By Lem. 21, there is an > 0 such that:\nv̄n+1:1 ∈ S (F(un+1:1)) =⇒ C((σ ◦ −Id) ◦ v̄n+1:1 ◦ (σ ◦ −Id/a)) = n+ 2 (182) Since W−1 is continuous as a function of W , there is ′ > 0 such that\n∀i ∈ [n] : ||Wi − Ui|| < ′ =⇒ i ∈ [n] : ||W−1i /a− U−1i /a|| < (183) In particular,\nS ′(un+1:1) ⊂ { ūn+1:1 ∣∣ F(ūn+1:1) ∈ S ′(F(un+1:1)) }\n(184)\nTherefore,\nūn+1:1 ∈ S ′(un+1:1) =⇒ C(ū−1n+1:1) = C((σ ◦ −Id) ◦ F(ūn+1:1) ◦ (σ ◦ −Id/a)) = n+ 2 (185) Density: By Lem. 27, the set Z is dense in Z ′. In addition, the set Z ′ is dense in RM×M×n. Since density is a transitive relation, Z is also dense in RM×M×n.\nTheorem 3 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) ∈ RM×M×n such that C(un+1:1) = n where ui = σ ◦Wi (for i ∈ [n]) is open and dense in RM×M×n.\nProof We denote:\nZ = { (W1, ...,Wn) ∈ RM×M×n ∣∣ ∀i ∈ [n] : ui = σ ◦Wi, Wi is invertible and C(un+1:1) = n } (186)\nand, Z ′ = { (W1, ...,Wn) ∈ RM×M×n ∣∣ ∀i ∈ [n] : Wi is invertible }\n(187)\nBy Lem. 21 and Lem. 27, the set Z is open and dense in Z ′. In addition, Z ′ is open and dense in RM×M×n. Openness and density are both transitive relations. Therefore, Z is also open and dense in RM×M×n."
    }, {
      "heading" : "Appendix H. Proof of Thm. 5",
      "text" : ""
    }, {
      "heading" : "H.1 Covering numbers",
      "text" : "Definition 14 (Set embedding) Let (U ,∼U ) and (V,∼V) be two tuples of sets and symmetric and reflexive relations on them (resp.). A function F : U → V is an embedding of (U ,∼U ) in (V,∼V) and we denote (U ,∼U ) (V,∼V) if:\n∀u1, u2 ∈ U : F (u1) ∼V F (u2) =⇒ u1 ∼U u2 (188)\nLemma 28 Let (U ,∼U ) and (V,∼V) be two tuples of sets and equivalence relations on them (resp.). Assume that |U/ U | and |V/ V | are finite. Then,\n(U ,∼U ) (V,∼V) ⇐⇒ |U/ ∼U | ≤ |V/ ∼V | (189)\nProof Assume that (U ,∼U ) (V,∼V). Then, there is a function F : U → V such if F (u1) ∼V F (u2) then u1 ∼U u2. We denote n := |U/ ∼U |. Let d1, ..., dn be n representatives of the n classes in U/ ∼U , i.e, ∀i 6= j ≤ n : di 6∼U dj . Then, ∀i 6= j ≤ n : F (di) 6∼V F (dj). Therefore, we found n representatives of n different equivalence classes of (V,∼V). In particular, |U/ ∼U | ≤ |V/ ∼V |. Assume that |U/ ∼U | ≤ |V/ ∼V |. We denote n := |U/ ∼U |. Let d1, ..., dn be n representatives of the n classes in U/ ∼U , i.e, ∀i 6= j ≤ n : di 6∼U dj . Similarly, e1, ..., em are m representatives of the m classes in V/ ∼V , i.e, ∀i 6= j ≤ n : ei 6∼V ej . We define a mapping F (u) = F (di) = ei iff u ∼U di. Thus, for any u1, u2 ∈ U , if u1 ∼U u2 then F (u1) = F (u2). In addition, if F (u1) 6∼V F (u2) then F (u1) = ei and F (u2) = ej for i 6= j ≤ n. Thus, u1 ∼U di and u2 ∼U dj such that di 6= dj . But, by the way we defined d1, ..., dn we have: di 6= dj implies di 6∼U dj . Therefore, u1 ∼U u2 iff F (u1) ∼V F (u2). Alternatively, F is an embedding from (U ,∼U ) to (V,∼V).\nLemma 29 Let (U ,∼U ) be a tuples of a set and an equivalence relation on it (resp.). Then,\nCovering(U ,∼U ) = |U/ ∼U | (190)\nProof First, we consider that (U,∼U ) is a covering of itself and therefore, Covering(U ,∼U ) ≤ |U/ ∼U |. Assume by contradiction that Covering(U ,∼U ) < |U/ ∼U |. Thus, there is a covering (U ,≡U ) of (U ,∼U ) such that |U/ ≡U | < |U/ ∼U |. But, by definition u1 ≡U u2 =⇒ u1 ∼U u2. Thus, if u1, ..., un ∈ U are n representations of n different equivalence classes in (U ,∼U ) then u1, ..., un ∈ U are also n representations of n different equivalence classes in (U ,≡U ). Therefore, |U/ ∼U | ≤ |U/ ≡U | in contradiction to |U/ ≡U | < |U/ ∼U |. Finally, we conclude that: Covering(U ,∼U ) = |U/ ∼U |.\nLemma 30 Let (U ,∼U ) and (V,∼V) be two tuples of sets and reflexive and symmetric relations on them (resp.). If (U ,∼U ) (V,∼V) then Covering(U ,∼U ) ≤ Covering(V,∼V).\nProof Assume that (U ,∼U ) (V,∼V). Then, by definition, there is an embedding function F : U → V such that: ∀u1, u2 ∈ U : F (u1) ∼V F (u2) =⇒ u1 ∼U u2 (191) Let (V,≡V) be a covering of (V,∼V). We define a covering (U ,≡U ) of (U ,∼U ) as follows:\nu1 ≡U u2 ⇐⇒ F (u1) ≡V F (u2) (192)\nPart 1: We would like to prove that (U ,≡U ) is a covering of (U ,∼U ). It is easy to see that ≡U is an equivalence relation since ≡V is an equivalence relation. Next, we would like to prove that u1 ≡U u2 =⇒ u1 ∼U u2. By the definition of ≡U :\nu1 ≡U u2 =⇒ F (u1) ≡V F (u2) (193)\nIn addition, since (V,≡V) is a covering of (V,∼V):\nF (u1) ≡V F (u2) =⇒ F (u1) ∼V F (u2) (194)\nFinally, since F is an embedding:\nF (u1) ∼V F (u2) =⇒ u1 ∼U u2 (195)\nWe conclude: u1 ≡U u2 =⇒ u1 ∼U u2 (196)\nTherefore, (U ,≡U ) is indeed a covering of (U ,∼U ). Part 2: We would like to prove that |U/ ≡U | ≤ |V/ ≡V |. Let u1, u2 ∈ U such that u1 6≡U u2. Then, by definition of ≡U we have: F (u1) 6≡V F (u2). Therefore, if we take u1, ..., un ∈ U representations of n different equivalence classes in (U ,≡U ) then, F (u1), ..., F (un) ∈ V are n representations of n different equivalence classes in (V,≡V). In particular, |U/ ≡U | ≤ |V/ ≡V |. Therefore, the covering number of (U ,∼U ) is at most the covering number of (V,∼V).\nLemma 31 Let (U ,≡1) and (U ,≡2) be coverings of (U ,∼U ). Then, (U2,≡1 × ≡2) is a covering of (U2,∼2U ). Where U2 = U × U and the relation ∼2U is defined as follows:\n(a, b) ∼2U (c, d) ⇐⇒ a ∼U c and b ∼U d (197)\nand ≡1 × ≡2 is defined as:\n(a, b) ≡1 × ≡2 (c, d) ⇐⇒ a ≡1 c and b ≡2 d (198)\nProof We have to prove that ≡1 × ≡2 is an equivalence relation and that (u1, u2) ≡1 × ≡2 (v1, v2) =⇒ (u1, u2) ∼2U (v1, v2). Reflexivity: (u1, u2) ≡1 × ≡2 (u1, u2) ⇐⇒ u1 ≡1 u1 and u2 ≡1 u2 (199) The RHS is true since ≡1 and ≡2 are reflexive relations. Symmetry: (u1, u2) ≡1 × ≡2 (v1, v2) ⇐⇒ u1 ≡1 v1 and u2 ≡2 v2 (200) Since ≡1 and ≡2 are symmetric, we have:\nu1 ≡1 v1 and u2 ≡2 v2 ⇐⇒ v1 ≡1 u1 and v2 ≡2 u2 (201)\nIn addition, (v1, v2) ≡1 × ≡2 (u1, u2) ⇐⇒ v1 ≡1 u1 and v2 ≡2 u2 (202)\nTherefore, (u1, u2) ≡1 × ≡2 (v1, v2) ⇐⇒ (v1, v2) ≡1 × ≡2 (u1, u2) (203)\nTransitivity: follows from similar arguments. Covering:\n(u1, u2) ≡1 × ≡2 (v1, v2) ⇐⇒ u1 ≡1 v1 and u2 ≡2 v2 (204)\nSince (U ,≡i) is a covering of (U ,∼U ), for i = 1, 2, we have: u1 ≡1 v1 and u2 ≡2 v2 =⇒ u1 ∼U v1 and u2 ∼U v2 (205)\nBy the definition of ∼2U we have: u1 ∼U v1 and u2 ∼U v2 ⇐⇒ (u1, u2) ∼U (v1, v2) (206)\nTherefore, (u1, u2) ≡1 × ≡2 (v1, v2) =⇒ (u1, u2) ∼2U (v1, v2) (207)\nLemma 32 Let (U ,∼U ) be a tuple of a set and a reflexive and symmetric relation on it (resp.). Then, Covering(U2,∼2U ) = Covering(U ,∼U )2 (208)"
    }, {
      "heading" : "Proof",
      "text" : "(≤) : Let ≡U be an equivalence relation such that (U ,≡U ) is a covering of (U ,∼U ). By Lem. 31, (U2,≡2U ) is a covering of (U2,∼2U ). In addition,\n|U2/ ≡2U | = |U/ ≡U |2 (209) Thus, for every covering (U ,≡U ) of (U ,∼U ) we can construct a covering of (U2,∼2U ) of size |U/ ≡U |2. In particular, Covering(U2,∼2U ) ≤ Covering(U ,∼U )2 (210) (≥) : Let (U2,≡) be a covering of (U2,∼2U ). Let ≡i be the following equivalence relation on U (proving that ≡1, ≡2 are equivalence relations is left for the reader),\nu1 ≡1 v1 ⇐⇒ ∃u2, v2 ∈ U s.t (u1, u2) ≡ (v1, v2) (211) we similarly define ≡2 (w.r.t the second coordinate). We also note that (U ,≡1) and (U ,≡2) are coverings of (U ,∼U ). That is because,\nu1 ≡1 v1 ⇐⇒ ∃v1, v2 s.t (u1, v1) ≡ (u2, v2) =⇒ u1 ∼U u2\n(212)\nLet u1, ..., un are representations of the n different equivalence classes of ≡1 and v1, ..., vk are representations of the k different equivalence classes of ≡2 then: (ui, vj) ≡ (us, vt) iff ui = us and vj = vt. Otherwise, there are ui, us and vj , vt such that (with no loss of generality), ui 6= us and (ui, vj) ≡ (us, vt). Since ui 6= us then by the way we defined u1, ..., un we have ui 6≡1 us. Thus, by definition, there are no v, v′ ∈ U such that (ui, v) ≡ (us, v′) in contradiction to (ui, vj) ≡ (us, vt). Thus, {(ui, vj)}i,j∈[n]×[k] are in different equivalence classes by ≡. In particular,\n|U2/ ≡1 × ≡2 | ≤ |U/ ≡1 | · |U/ ≡2 | ≤ |U2/ ≡2U | (213) And by Lem. 31, Covering(U ,∼U )2 ≤ |U2/ ≡1 × ≡2 | (214) By combining Eq. 213 and 214, Covering(U ,∼U )2 ≤ |U2/ ≡2U | (215) Therefore, Covering(U ,∼U )2 ≤ Covering(U2,∼2U ) (216) By taking minimum over the RHS of Eq. 215 over the different possibilities of ≡2U .\nLemma 33 Let (U ,∼U ) and (V,∼V) be two tuples of sets and reflexive and symmetric relations on them (resp.). Assume that U ⊂ V and ∼U= (∼V) ∣∣ U , i.e,\n∀u, v ∈ U : u ∼U v ⇐⇒ u ∼V v (217)\nThen, Covering(U ,∼U ) ≤ Covering(V,∼V) (218)\nProof Let (V,≡V) be a covering of (V,∼V). Then, it is easy to see that (U ,≡U ) is a covering of (U ,∼U ), where ≡U= (≡V) ∣∣ U . In addition, we have: |U/ ≡U | ≤ |V/ ≡V |. Thus, for every covering of (V,∼V), we can find a smaller covering for (U ,∼U ). In particular, Covering(U ,∼U ) ≤ Covering(V,∼V).\nLemma 34 Let (U1,∼U1) and (U2,∼U2) be two tuples of sets and reflexive and symmetric relations on them (resp.). Assume that U1 ∩ U2 = ∅. We define (U ,∼U ) as follows:\nU = U1 ∪ U2 u ∼U v ⇐⇒ ∃i = 1, 2 : u, v ∈ Ui and u ∼Ui v\n(219)\nThen, Covering(U ,∼U ) ≤ Covering(U1,∼U1) + Covering(U2,∼U2) (220)\nProof Let (U1,≡U1) and (U2,≡U2) be minimal coverings of (U1,∼U1) and (U2,∼U2) (resp.). We define a covering (U ,≡U ) of (U ,∼U ) as follows:\nu ≡U v ⇐⇒ ∃i = 1, 2 : u, v ∈ Ui and u ≡Ui v (221)\nSince both ≡U1 and ≡U2 are equivalence relations, ≡U is also an equivalence relation. In addition,\nu ≡U v =⇒ ∃i = 1, 2 : u, v ∈ Ui and u ≡Ui v =⇒ ∃i = 1, 2 : u, v ∈ Ui and u ∼Ui v =⇒ u ∼U v\n(222)\nWe also consider that the members U1 and the members of U2 do not share equivalence classes by ≡U . In addition, the members of U1 are partitioned into equivalence classes by ≡U1 . Similarly, U2 w.r.t ≡U2 . Therefore,\nCovering(U ,∼U ) ≤ |U/ ≡U | = |U1/ ≡U1 |+ |U2/ ≡U2 | = Covering(U1,∼U1) + Covering(U2,∼U2)\n(223)"
    }, {
      "heading" : "H.2 Perturbations and discrepancy",
      "text" : "Assumption 1 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. For every m > 0 and n > 0, the function discm(fWn,...,W1 ◦D1, D2) (18) is continuous as a function of the weights of Wn, ...,W1. Here, fWn,...,W1 = (σ ◦Wn) ◦ ... ◦ (σ ◦W1).\nAssumption 2 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. For all m > 0, the function\nRD[fVm,...,V1 , fWm,...,W1 ] (22)\nis continuous as a function of Vm, ..., V1,Wm, ...,W1.\nLemma 35 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0 and assume Assumption 2 with D :← D1. Let discm,E = discCm,E for Cm,E = { um+1:1 ∣∣∀i ∈ [m] : ui = σ ◦Wi s.t: ||Wi|| ≤ E }\n. For all m > 0, n > 0 and E > 0, the function\ndiscm,E(fWn,...,W1 ◦D1, D2) (224)\nis continuous as a function of Wn, ...,W1.\nProof Let Wn, ...,W1 and W kn , ...,W k1 be any invertible matrices in RM×M such that for all i ∈ [n], W ki → Wi. We denote GE = { W ∈ RM×M ∣∣ ||W || ≤ E } . By the triangle inequality,\ndiscm,E(D1, D2) ≤ discm,E(D1, D3) + discm,E(D3, D2) =⇒ discm,E(D1, D2)− discm,E(D3, D2) ≤ discm,E(D1, D3)\n(225)\nSimilarly, discm,E(D3, D2) ≤ discm,E(D1, D3) + discm,E(D1, D2)\n=⇒ discm,E(D3, D2)− discm,E(D1, D2) ≤ discm,E(D1, D3) (226)\ntherefore, |discm,E(D3, D2)− discm,E(D1, D2)| ≤ discm,E(D1, D3) (227)\nIn particular, ∣∣∣discm,E(fWn,...,W1 ◦D1, D2)− discm,E(fWkn ,...,Wk1 ◦D1, D2)\n∣∣∣ ≤discm,E(fWn,...,W1 ◦D1, fWkn ,...,Wk1 ◦D1)\n≤ sup c1,c2∈Cm,E\n∣∣∣RD1 [c1 ◦ fWn,...,W1 , c2 ◦ fWn,...,W1 ]−RD1 [c1 ◦ fWkn ,...,Wk1 , c2 ◦ fWkn ,...,Wk1 ] ∣∣∣\n≤ sup V1,..,Vm,U1,...,Um∈GE\n∣∣∣RD1 [fVm,...,V1 ◦ fWkn ,...,Wk1 , fUm,...,U1 ◦ fWn,...,W1 ]\n−RD1 [fVm,...,V1 ◦ fWkn ,...,Wk1 , fUm,...,U1 ◦ fWkn ,...,Wk1 ] ∣∣∣\n≤ sup V1,..,Vm,U1,...,Um∈GE\n∣∣∣RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ]\n−RD1 [fVm,...,V1,Wkn ,...,Wk1 , fUm,...,U1,Wkn ,...,Wk1 ] ∣∣∣\n(228)\nAssume by contradiction that the last expression does not converge to 0. Therefore, there is a sequence (V k1 , ..., V k m, U k 1 , ..., U k m) such that V k 1 , .., V k m, U k 1 , ..., U k m ∈ GE and\n∣∣∣RD1 [fV km,...,V k1 ,Wn,...,W1 , fUkm,...,Uk1 ,Wn,...,W1 ]\n−RD1 [fV km,...,V k1 ,Wkn ,...,Wk1 , fUkm,...,Uk1 ,Wkn ,...,Wk1 ] ∣∣∣ 6→ 0\n(229)\nSince (V k1 , ..., V k m, U k 1 , ..., U k m) ∈ G2mE and G2mE is compact in RM×M×2m, by the Bolzano-Weierstrass theorem, there is a converging subsequence. Alternatively, there is an increasing sequence {kj}∞j=1 ⊂ N such that\n(V kj 1 , ..., V kj m , U kj 1 , ..., U kj m )→ (V1, ..., Vm, U1, ..., Um) ∈ G2mE (230)\nIn particular, (V kjm , ..., V kj 1 ,W kj n , ...,W kj 1 )→ (Vm, ..., V1,Wn, ...,W1)\n(Ukjm , ..., U kj 1 ,W kj n , ...,W kj 1 )→ (Um, ..., U1,Wn, ...,W1)\n(231)\nBy Assumption 2, the function RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ] is a continuous. Therefore, ∣∣∣RD1 [fV km,...,V k1 ,Wn,...,W1 , fUkm,...,Uk1 ,Wn,...,W1 ]\n−RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ] ∣∣∣→ 0\n(232)\nand, ∣∣∣RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ]\n−RD1 [fV km,...,V k1 ,Wkn ,...,Wk1 , fUkm,...,Uk1 ,Wkn ,...,Wk1 ] ∣∣∣→ 0\n(233)\nand therefore, by the triangle inequality, ∣∣∣RD1 [fV km,...,V k1 ,Wn,...,W1 , fUkm,...,Uk1 ,Wn,...,W1 ] −RD1 [fV km,...,V k1 ,Wkn ,...,Wk1 , fUkm,...,Uk1 ,Wkn ,...,Wk1 ]→ 0\n(234)\nin contradiction. Thus, we conclude that:\nlim k→∞ ∣∣∣discm,E(fWn,...,W1 ◦D1, D2)− discm,E(fWkn ,...,Wk1 ◦D1, D2) ∣∣∣ = 0 (235)\nLemma 36 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. In addition, let f = fn+1:1 and g = gn+1:1 be two decompositions. Assume that discm(g ◦DA, DB) ≤ 0 and m ≥ t + n + 2. Then, there are f̄ = f̄n+1:1, ḡ = ḡn+1:1 and q̄ = q̄n+3:1 such that:\n• q̄n+2:1 = F(ḡn+1:1) ◦ (σ ◦ −Id/a).\n• C(f̄n+1:1 ◦ q̄n+3:1) = 2n+ 2.\n• ∀j ∈ [n+ 1] : q̄n−j+3:1 ◦ ḡ ◦DA = (−σ−1) ◦ ḡj:1 ◦DA.\n• disct(q̄ ◦DB , DA) ≤ 0 + .\n• ∀j ∈ [n] : discm(f̄j+1:1 ◦DA, fj+1:1 ◦DA) ≤ .\n• ∀j ∈ [n] : discm(ḡj+1:1 ◦DA, gj+1:1 ◦DA) ≤ .\nProof We denote F+ and F are the functions from Eq 60 and Eq. 173 (resp.). Let:\nqn+3:1 = F+(gn+1:1) (236)\nLet q̄ = q̄n+3:1 = q̄n+3:2 ◦ (σ ◦ −Id/a) and ḡ = F−1(q̄n+2:2). By the first item of Lem. 2, for D1 :← DB , D2 :← ḡ ◦DA, D3 :← DA, p :← q̄, m ≥ t+ n+ 2 ≥ t+ C(q̄), we have:\ndisct(q̄ ◦DB , DA) ≤ discm(q̄ ◦ ḡ ◦DA, DA) + discm(ḡ ◦DA, DB) = discm(q̄n+2 ◦ q−1n+2 ◦DA, DA) + discm(ḡ ◦DA, DB) ≤ discm(q̄n+2 ◦ q−1n+2 ◦DA, DA) + discm(g ◦DA, DB) + discm(ḡ ◦DA, g ◦DA) ≤ discm(q̄n+2 ◦ q−1n+2 ◦DA, DA) + 0 + discm(ḡ ◦DA, g ◦DA)\n(237) The weights of ḡn+1:1 are continuous functions of the weights of q̄n+2:2. In addition, for all j ∈ [n], the functions discm(f̄j+1:1◦DA, fj+1:1◦DA), discm(ḡj+1:1◦DA, gj+1:1◦DA) and discm(q̄n+2◦q−1n+2◦DA, DA) are continuous as a function of the weights of f̄n+1:1 and ḡn+1:1 (resp.). In particular,\n(discm(f̄j+1:1 ◦DA, fj+1:1 ◦DA))nj=1 ||(discm(ḡj+1:1 ◦DA, gj+1:1 ◦DA))nj=1 ||(discm(q̄n+2 ◦ q−1n+2 ◦DA, DA))\n(238)\nis a continuous function of the weights of f̄n+1:1 ◦ q̄n+3:1. Here, || is the concatenations operator between tuples, i.e, (x1, ..., xn)||(y1, ..., ym) = (x1, ..., xn, y1, ..., ym). Therefore, for every > 0 there is ′ > 0 such that for all f̄n:1 ◦ q̄n+3:2 ∈ S ′(fn:1 ◦ qn+3:2) we have:\n∀j ∈ [n] : discm(f̄j+1:1 ◦DA, fj+1:1 ◦DA) ≤ /2 discm(ḡj+1:1 ◦DA, gj+1:1 ◦DA) ≤ /2 discm(q̄n+2 ◦ q−1n+2 ◦DA, DA) ≤ /2\n(239)\nWhere q̄ = q̄n+3:1 = q̄n+3:2 ◦ (σ ◦ −Id/a) and ḡ = F−1(q̄n+2:2). By Lem. 26, there is f̄n+1:1 ◦ q̄n+3:1 such that:\n• f̄n:1 ◦ q̄n+3:2 ∈ S ′(fn:1 ◦ qn+3:2).\n• C(f̄n+1:1 ◦ q̄n+3:1) = 2n+ 2.\n• q̄1 = q1 = (σ ◦ −Id/a).\n• f̄n = fn.\nFinally, by Lem. 12,\n∀j ∈ [n+ 1] : q̄n−j+3:1 ◦ ḡ ◦DA = (−σ−1) ◦ ḡj:1 ◦DA (240)\nTherefore, we found f̄n+1:1 ◦ q̄n+3:1 with all the desired properties.\nLemma 37 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. Let f D∼ m+2, g. Then, for every minimal decomposition f = f ′n+1:1 there is a minimal decomposition g = g ′ n+1:1 such that:\n∀i ∈ [n] : f ′i+1:1 ◦D ∼ m, g′i:1 ◦D (241)\nProof Since f D∼ m, g there are minimal decompositions f = fn+1:1 and g = gn+1:1 such that:\n∀i ∈ [n] : fi+1:1 ◦D ∼ m, gi+1:1 ◦D (242)\nBy minimal identifiability, f ′1 = π1 ◦ f1, for all i = 2, ..., n − 1: f ′i = πi ◦ fi ◦ π−1i−1 and f ′n = fn ◦ π−1n−1. Therefore, we define a minimal decomposition for g as follows: g = g′n+1:1 such that g ′ 1 = π1 ◦ g1, for all i = 2, ..., n− 1: g′i = πi ◦ gi ◦ π−1i−1 and g′n = gn ◦ π−1n−1. This is a minimal decomposition of g, since each invariant function is an invertible linear mapping and commutes with σ. By Lem. 7 we have:\nf ′i+1:1 = πi ◦ fi+1:1 and g′i+1:1 = πi ◦ gi+1:1 (243)\nTherefore, discm(f ′n+1:1 ◦D, g′n+1:1 ◦D) = discm(fn+1:1 ◦D, gn+1:1 ◦D) ≤ (244)\nand,\n∀i ∈ [n− 1] : discm(f ′i+1:1 ◦D, g′i+1:1 ◦D) = discm(πi ◦ fi+1:1 ◦D,πi ◦ gi+1:1 ◦D) (245)\nBy Lem. 1, since C(πi) = 2 we have:\n∀i ∈ [n− 1] : discm(f ′i+1:1 ◦D, g′i+1:1 ◦D) = discm(πi ◦ fi+1:1 ◦D,πi ◦ gi+1:1 ◦D) ≤ discm+2(fi+1:1 ◦D, gi+1:1 ◦D) ≤\n(246)\nAlternatively, ∀i ∈ [n] : f ′i+1:1 ◦D ∼\nm, g′i+1:1 ◦D (247)\nLemma 38 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. Let h = hn+1:1 be any function such that C(h) = n and q = qn+3:1 such that qn+2:1 = F(hn+1:1) ◦ (σ ◦−Id/a) (F is the function in Eq. 173) andC(q) = n+2. Then, for any minimal decomposition q = q′n+3:1 there is a minimal decomposition h = h′n+1:1 such that:\n∀j ∈ [n] \\ {1} :q′n−j+3:1 ◦ h ◦DA = (−σ−1) ◦ h′j:1 ◦DA (248)\nProof Let q = q′n+3:1 and h = hn+1:1 be minimal decompositions of q and h (resp.). By Lem. 12,\n∀j ∈ [n+ 1] : qn−j+3:1 ◦ h ◦DA = (−σ−1) ◦ hj:1 ◦DA (249)\nBy minimal identifiability, for decompositions q = q′n+3:1 we have:\n∃π1, ..., πn+1 ∈ Invariant(N ) : q′1 = π1 ◦ q1, ∀j = 2, ..., n+ 1 : q′j = πj ◦ qj ◦ π−1j−1 and q′n+2 = qn+2 ◦ π−1n+1\n(250)\nBy Lem. 7, ∀j ∈ [n+ 1] : q′n−j+3:1 = πn−j+2 ◦ qn−j+3:1 (251)\nTherefore, for all j ∈ [n+ 1] we have:\n(σ ◦ −Id) ◦ q′n−j+3:1 ◦ h ◦DA = (σ ◦ −Id) ◦ πn−j+2 ◦ qn−j+3:1 ◦ h ◦DA (252)\nWe define a decomposition h = h′n+1:1 as follows:\nh′1 = πn ◦ h1, ∀j = 2, ..., n− 1 : h′j = πn−j+1 ◦ hj ◦ π−1n−j+2 and h′n = hn ◦ π−12 (253)\nIn particular, ∀j ∈ [n] \\ {1} : h′j:1 = πn−j+2 ◦ hj:1 (254)\nBut, πn−j+2 commutes with both σ and −Id and therefore, for all j ∈ [n] such that j 6= 1 we have:\n(σ ◦ −Id) ◦ q′n−j+3:1 ◦ h ◦DA = (σ ◦ −Id) ◦ πn−j+2 ◦ qn−j+3:1 ◦ h ◦DA = πn−j+2 ◦ (σ ◦ −Id) ◦ qn−j+3:1 ◦ h ◦DA = πn−j+2 ◦ hj:1 ◦DA = h′j:1 ◦DA\n(255)\nLemma 39 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. Assume that:\nf DA 6∼ k, 1 f ′, f̄ DA∼ k+2, f and f̄ ′ DA∼ k+2, f ′ (256)\nThen, f̄ DA 6∼\nk, 1−2 f̄ ′.\nProof Assume by contradiction that f̄ DA∼ k, 1−2 f̄ ′. Then, there are decompositions f̄ = f̄n+1:1 and f̄ ′ = f̄ ′n+1:1 such that:\n∀j ∈ [n] : disck(f̄j+1:1 ◦DA, f̄ ′j+1:1 ◦DA) ≤ 1 − 2 (257)\nBy Lem. 37, since f̄ DA∼ k+2, f and f̄ ′ DA∼ k+2, f ′, there are minimal decompositions f = fn+1:1 and f ′ = f ′n+1:1 such that:\n∀j ∈ [n] : disck(fj+1:1 ◦DA, f̄j+1:1 ◦DA), disck(f ′j+1:1 ◦DA, f̄ ′j+1:1 ◦DA) ≤ (258)\nSince f DA 6∼ k, 1 f ′, there is an index i ∈ [n] such that:\ndisck(fi+1:1 ◦DA, f ′i+1:1 ◦DA) > 1 (259)\nTherefore, by the triangle inequality,\ndisck(fi+1:1 ◦DA, f ′i+1:1 ◦DA) ≤disck(f̄i+1:1 ◦DA, f ′i+1:1 ◦DA) + disck(f̄i+1:1 ◦DA, fi+1:1 ◦DA) ≤disck(f̄i+1:1 ◦DA, f̄ ′i+1:1 ◦DA) + disck(f̄i+1:1 ◦DA, fi+1:1 ◦DA)\n+ disck(f̄ ′i+1:1 ◦DA, f ′i+1:1 ◦DA) ≤( 1 − 2 ) + + = 1\n(260)\nin contradiction."
    }, {
      "heading" : "H.3 Proof of Thm. 5",
      "text" : "Theorem 5 (Counting semantic mappings) Let N = SCM[σ] be a NN-SCM with σ that is a Leaky ReLU with parameter a > 0 and assume Assumption 1. Let 0, 1 and 2 < 1 − 2 0 are three positive constants and A = (XA, DA) and B = (XB , DB) are two domains. Assume that m ≥ k + 2C 0A,B + 5. Then,\nCovering ( H 0(DA, DB ;m),\nDA∼ k, 1 ) ≤ lim →0 √ Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) (19)\nProof We denote by any positive constant such that: < ( 1 − 2 0 − 2)/4 and t := k + C 0A,B + 3. We would like to find an embedding mapping:\nG : (H 0(DA, DB ;m)) 2 → DPM2( 0+ ) ( k, 2C 0A,B + 2 ) (261)\nPart 1: In this part, we show how to constructG. Let (f, g) ∈ (H 0(DA, DB ;m))2 and f = fn+1:1 and g = gn+1:1 are decompositions of f and g (resp.). Let F be the function from Eq. 173 and qn+3:1 = F+(gn+1:1). Then, by Lem. 36 there are f̄ = f̄n+1:1, ḡ = ḡn+1:1 and q̄ = q̄n+3:1 such that:\n• q̄n+2:1 = F(ḡn+1:1) ◦ (σ ◦ −Id/a).\n• C(f̄n+1:1 ◦ q̄n+3:1) = 2n+ 2.\n• ∀j ∈ [n+ 1] : q̄n−j+3:1 ◦ ḡ ◦DA = (−σ−1) ◦ ḡj:1 ◦DA.\n• disct(q̄ ◦DB , DA) ≤ 0 + .\n• ∀j ∈ [n] : discm(f̄j+1:1 ◦DA, fj+1:1 ◦DA) ≤ .\n• ∀j ∈ [n] : discm(ḡj+1:1 ◦DA, gj+1:1 ◦DA) ≤ .\nWe define: G(f, g) = f̄n+1:1 ◦ q̄n+3:1. Part 2: In this part, we show that:\n(f, g) ∈ (H 0(DA, DB ;m))2 =⇒ G(f, g) ∈ DPM2( 0+ ) ( k, 2C 0A,B + 2 ) (262)\nBy Part 1, C(f̄n+1:1 ◦ q̄n+3:1) = 2n+ 2 = 2C 0A,B + 2. In addition, by the first item of Lem. 2, for D1 :← q̄ ◦DB , D2 :← DA, D3 :← DB , p :← f̄ , t ≥ k + C 0A,B we have: disck(f̄ ◦ q̄ ◦DB , DB) ≤ disct(f̄ ◦DA, DB) + disct(q̄ ◦DB , DA) (263) Since f ∈ H 0(DA, DB ;m):\ndisct(f̄ ◦DA, DB) ≤ discm(f ◦DA, DB) + discm(f̄ ◦DA, f ◦DA) ≤ 0 + (264)\nFinally, disck(f̄ ◦ q̄ ◦DB , DB) ≤ 2( 0 + ) (265)\nWe conclude that G(f, g) ∈ DPM2( 0+ ) ( k, 2C 0A,B + 2 ) (266)\nPart 3: In this part, we show that G is an embedding. It requires showing that\nG(f, g) DB∼ m, 2 G(f ′, g′) =⇒ (f, g) ( DA∼ k, 1 )2 (f ′, g′) (267)\nAssume by contradiction that G(f, g) DB∼ m, 2 G(f ′, g′) and that (f, g) DA 6∼ k, 1 (f ′, g′). Then, we have\nf DA 6∼ k, 1 f ′ or g DA 6∼ k, 1 g′ (268)\nWe denote G(f, g) = f̄ ◦ q̄ and G(f ′, g′) = f̄ ′ ◦ q̄′ (see Part 1).\nAssume that f DA 6∼ k, 1 f ′: By Lem. 39, f̄ DA 6∼ k, 1−2 f̄ ′. In particular, for every f̄ = f̄n+1:1 and f̄ ′ = f̄ ′n+1:1, there is an index i ∈ [n] such that:\ndisck(f̄i+1:1 ◦DA, f̄ ′i+1:1 ◦DA) > 1 − 2 (269)\nAs we showed in Part 2,\ndisct(q̄ ◦DB , DA), disct(q̄′ ◦DB , DA) ≤ 0 + (270)\nBy the first item of Lem. 2, forD1 :← DA,D2 :← q̄◦DB ,D3 :← f̄ ′i+1:1◦DA, t ≥ k+C 0A,B ≥ k+C(f̄i+1:1), we have:\ndisck(f̄i+1:1 ◦DA, f̄ ′i+1:1 ◦DA) ≤ disct(f̄i+1:1 ◦ q̄ ◦DB , f̄ ′i+1:1 ◦DA) + disct(q̄ ◦DB , DA) ≤ disct(f̄i+1:1 ◦ q̄ ◦DB , f̄ ′i+1:1 ◦DA) + 0 +\n(271)\nAgain, by the first item of Lem. 2, for D1 :← DA, D2 :← q̄′ ◦DB , D3 :← fi+1:1 ◦ q̄ ◦DB , m ≥ t+C 0A,B ≥ t+ C(f̄ ′i+1:1), we have:\ndisct(f̄i+1:1 ◦ q̄ ◦DB , f̄ ′i+1:1 ◦DA) ≤discm(f̄i+1:1 ◦ q̄ ◦DB , f̄ ′i+1:1 ◦ q̄′ ◦DB) + discm(q̄′ ◦DB , DA) ≤discm(f̄i+1:1 ◦ q̄ ◦DB , f̄ ′i+1:1 ◦ q̄′ ◦DB) + 0 +\n(272)\nTherefore, we conclude that:\n1 − 2 0 − 4 < discm(f̄i+1:1 ◦ q̄ ◦DB , f̄ ′i+1:1 ◦ q̄′ ◦DB) (273)\nAlternatively, for any minimal decompositions f̄ ◦ q̄ = f̄n+1:1 ◦ q̄n+3:1 and f̄ ′ ◦ q̄′ = f̄ ′n+1:1 ◦ q̄′n+3:1 there are right partial functions f̄i+1:1 ◦ q̄n+3:1 and f̄ ′i+1:1 ◦ q̄′n+3:1 such that:\n2 ≤ 1 − 2 0 − 4 < discm(f̄i+1:1 ◦ q̄n+3:1 ◦DB , f̄ ′i+1:1 ◦ q̄′n+3:1 ◦DB) (274)\nin contradiction to F (f, g) DB∼ m, 2 F (f ′, g′).\nAssume that g DA 6∼ k, 1 g′: Let q̄ = q̄n+3:1 and q̄′ = q̄′n+3:1 be any two minimal decompositions of q̄ and q̄′ (resp.). Then, by Lem. 38, there are minimal decompositions ḡ = ḡn+1:1 and ḡ′ = ḡ′n+1:1 such that:\n∀j ∈ [n] \\ {1} :q̄n−j+3:1 ◦ ḡ ◦DA = (−σ−1) ◦ ḡj:1 ◦DA and:q̄′n−j+3:1 ◦ ḡ′ ◦DA = (−σ−1) ◦ ḡ′j:1 ◦DA\n(275)\nBy Lem. 39, since ḡ DA∼ m, g and ḡ′ DA∼ m,\ng′, we have: ḡ DA 6∼\nk, 1−2 ḡ′. In particular, there is an index i ∈ [n+ 1] such\nthat: disck(ḡi:1 ◦DA, ḡ′i:1 ◦DA) > 1 − 2 (276)\nand, disck(ḡ1:1 ◦DA, ḡ′1:1 ◦DA) = 0 ≤ 1 − 2 (277)\nand,\ndisck(ḡn+1:1 ◦DA, ḡ′n+1:1 ◦DA) = disck(ḡ ◦DA, ḡ′ ◦DA) ≤ disck(ḡ ◦DA, DB) + disck(DB , ḡ′ ◦DA) ≤ disck(g ◦DA, DB) + disck(DB , g′ ◦DA) + disck(ḡ ◦DA, g ◦DA) + disck(g′ ◦DA, ḡ′ ◦DA) ≤ 2( 0 + ) ≤ 1 − 2 (278)\nTherefore, i 6= n+ 1, 1. Thus, i ∈ [n] \\ {1} and: disck+1((−σ−1) ◦ ḡi:1 ◦DA, (−σ−1) ◦ ḡ′i:1 ◦DA)\n=disck+1(q̄n−i+3:1 ◦ ḡ ◦DA, q̄′n−i+3:1 ◦ ḡ′ ◦DA) (279)\nBy Lem. 1, for p :← (σ ◦ −Id) of complexity 1 we have: 1 − 2 < disck(ḡi:1 ◦DA, ḡ′i:1 ◦DA)\n≤ disck+1((−σ−1) ◦ ḡi:1 ◦DA, (−σ−1) ◦ ḡ′i:1 ◦DA) (280)\nIn addition, by Lem. 2, forD1 :← ḡ◦DA,D2 :← DB ,D3 :← q̄′n−i+3:1◦ ḡ′◦DA, t ≥ (k+1)+(C 0A,B+2) ≥ (k + 1) + C(q̄n−i+3:1), we have:\ndisck+1(q̄n−i+3:1 ◦ ḡ ◦DA, q̄′n−i+3:1 ◦ ḡ′ ◦DA) ≤ disct(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦ ḡ′ ◦DA) + disct(ḡ ◦DA, DB) ≤ disct(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦ ḡ′ ◦DA) + disct(g ◦DA, DB) + disct(ḡ ◦DA, g ◦DA) ≤ disct(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦ ḡ′ ◦DA) + 0 +\n(281)\nAgain, by Lem. 2, for D1 :← ḡ′ ◦ DA, D2 :← DB , D3 :← q̄n−i+3:1 ◦ DB , m ≥ t + (C 0A,B + 2) ≥ t+ C(q̄′n−i+3:1), we have:\ndisct(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦ ḡ′ ◦DA) ≤ discm(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦DB) + discm(ḡ′ ◦DA, DB) ≤ discm(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦DB) + discm(g′ ◦DA, DB) + discm(ḡ′ ◦DA, g′ ◦DA) ≤ discm(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦DB) + 0 +\n(282)\nFinally, 1 − 2 < disck(ḡi:1 ◦DA, ḡ′i:1 ◦DA)\n≤ disct(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦ ḡ′ ◦DA) + 0 + ≤ discm(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦DB) + 2( 0 + )\n(283)\nIn particular, 2 ≤ 1 − 2 0 − 4 < discm(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦DB) (284) Alternatively, for any minimal decompositions f̄ ◦ q̄ = f̄n+1:1 ◦ q̄n+3:1 and f̄ ′ ◦ q̄′ = f̄ ′n+1:1 ◦ q̄′n+3:1 there are right partial functions q̄n−i+3:1 and q̄′n−i+3:1 such that:\n2 ≤ 1 − 2 0 − 4 < discm(q̄n−i+3:1 ◦DB , q̄′n−i+3:1 ◦DB) (285)\nin contradiction to F (f, g) DB∼ m, 2 F (f ′, g′).\nPart 3: Finally, by Lem. 32 and Lem. 30,\nCovering ( H 0(DA, DB ;m),\nDA∼ k, 1\n)2 = Covering ( (H 0(DA, DB ;m)) 2, ( DA∼ k, 1 )2)\n≤ Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) (286)\nAlternatively, for all 0, 1, 2, such that < ( 1 − 2 0 − 2)/4,\nCovering ( H 0(DA, DB ;m),\nDA∼ k, 1\n)2 ≤ Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) (287)\nBy Lem. 33, the function Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) is monotonically decreasing as tends to 0 and is lower bounded by Covering ( DPM2 0 ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) . Thus, the limit\nlim →0\nCovering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 ) (288)\nexists and\nCovering ( H 0(DA, DB ;m),\nDA∼ k, 1\n) ≤ √\nlim →0\nCovering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 )\n= lim →0\n√ Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 )\n= lim →0\n√ Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB∼ m, 2 )\n(289)\nAppendix I. Proof of Thms. 6 and 7 Lemma 40 Let N = SCM[C], 0, 1, 2 > 0 are three constants, A = (XA, DA) and B = (XB , DB) are two domains and y ∈ H 0(DA, DB ;m) such that y = yn+1:1 is a minimal decomposition of y. Assume that k ≥ max { E 0A,B , E 2 0+2 1+2 2 A,B } , m ≥ k + 2C 0A,B and that C 0A,B = C 0+2 1+2 2A,B . Let DI and DJ be two distributions such that:\ndiscm(yi+1:1 ◦DA, DI) ≤ 1 and discm(yj+1:1 ◦DA, DJ) ≤ 2 (290)\nThen, yj+1:i+1 ∈ H 1+ 2(DI , DJ ; k).\nProof Assume by contradiction that yj+1:i+1 /∈ H 1+ 2(DI , DJ ; k). Then, exactly one of the following options hold:\n• There is a function u such that C(u) < C(yj+1:i+1) = j − i and disck(u ◦DI , DJ) ≤ 1 + 2.\n• disck(yj+1:i+1 ◦DI , DJ) > 1 + 2.\nIf the second option holds, then, by the triangle inequality,\n1 + 2 < disck(yj+1:i+1 ◦DI , DJ) ≤ disck(yj+1:i+1 ◦DI , yj+1:1 ◦DA) + disck(DJ , yj+1:1 ◦DA) ≤ disck(yj+1:i+1 ◦DI , yj+1:1 ◦DA) + 2\n(291) In addition, by the first part of Lem. 2, forD1 :← DI ,D2 :← yi+1:1◦DA,D3 :← yj+1:1◦DA, p :← yj+1:i+1 and m ≥ k + j − i\ndisck(yj+1:i+1 ◦DI , yj+1:1 ◦DA) ≤discm(yj+1:1 ◦DA, yj+1:1 ◦DA) + discm(yi+1:1 ◦DA, DI) ≤ 1\n(292)\nTherefore, 1 + 2 < disck(yj+1:i+1 ◦DI , DJ) ≤ 1 + 2 in contradiction. Thus, we conclude that the first option must hold. We denote t = k + C 0A,B . We note that since m ≥ t ≥ k ≥ E 0A,B , by Lem. 18, we have:\ny ∈ H 0(DA, DB ;m) ⊂ H 0(DA, DB ; t) (293)\nBy the triangle inequality,\ndisct(u ◦ yi+1:1 ◦DA, yj+1:1 ◦DA) ≤disct(yj+1:1 ◦DA, DJ) + disct(u ◦ yi+1:1 ◦DA, DJ) ≤disct(u ◦ yi+1:1 ◦DA, DJ) + 2\n(294)\nBy the first item of Lem. 2; for D1 :← yi+1:1 ◦DA, D2 :← DI , D3 :← DJ , p :← u, and m ≥ t+ C 0A,B ≥ t+ C(u), we have:\ndisct(u ◦ yi+1:1 ◦DA, DJ) ≤ discm(u ◦DI , DJ) + discm(yi+1:1 ◦DA, DI) ≤ 2 1 + 2 (295)\nTherefore, disct(u ◦ yi+1:1 ◦DA, yj+1:1 ◦DA) ≤ 2( 1 + 2) (296)\nBy the first item of Lem. 2; for D1 :← u ◦ yi+1:1 ◦DA, D2 :← yj+1:1 ◦DA, D3 :← DB , p :← yn+1:j+1 and t = k + C 0A,B ≥ k + C(yn+1:j+1), we have:\ndisck(yn+1:j+1 ◦ u ◦ yi+1:1 ◦DA, DB) ≤disct(yn+1:j+1 ◦ yj+1:1 ◦DA, DB) + disct(u ◦ yi+1:1 ◦DA, yj+1:1 ◦DA)\n≤disct(y ◦DA, DB) + 2( 1 + 2) ≤ 0 + 2( 1 + 2) (297)\nOn the other hand,\nC(yn+1:j+1 ◦ u ◦ yi+1:1) ≤ n− j + C(u) + i < n− j + (j − i) + i = n = C(y) (298)\nThus, we found a function g = yn+1:j+1 ◦ u ◦ yi+1:1 such that C(g) < C(y) and disck(g ◦ DA, DB) ≤ 0 + 2( 1 + 2). Since k ≥ E 0+2 1+2 2A,B we have:\nC 0+2 1+2 2A,B = C k, 0+2 1+2 2 A,B ≤ C(g) < C(y) = C 0A,B (299)\nin contradiction to C 0+2 1+2 2A,B = C 0 A,B ."
    }, {
      "heading" : "Proof of Thm. 6",
      "text" : "Theorem 6 Let N = SCM[σ] with σ that is Leaky ReLU with parameter a > 0. Let A = (XA, DA) and B = (XB , DB) be two domains and DZ 6= DA, DB is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Let k ≥ max { E 0A,B , E 4 0+4 1 B,A , E 4 0 B,A + C 0+ 1 A,Z + 1 } and m ≥ k+3C 0+ 1A,B +4. Assume thatC3 0+ 1B,A = C 0+3 1 B,A = C 0+ 1 B,A = C 0+ 1 A,B +2. Then, (−σ−1)◦DZ is a (k, 1, 0, 0+ 1)-shared semantic distribution between B and A.\nProof Let yAB ∈ Z(DA, DZ , DB ;m, 0, 1) ∩ H 0+ 1(DA, DB ;m). Assume that yAB = pn+1:1 is a minimal decomposition of yAB , such that discm(pi+1:1 ◦DA, DZ) ≤ 0 and discm(DB , pn+1:i+1 ◦DZ) ≤ 1 for some i ∈ {1, ..., n− 1}. We denote yBA = y−1AB , t := m− C 0+ 1A,B − 2. Part 1: In this part, we show that\nyBA ∈ H 0+ 1(DB , DA; t) ⊂ H 0+ 1(DB , DA; k) (300)\nBy the third item of Lem. 2, for: D1 :← DA,D2 :← DB , h :← yAB , h−1 :← yBA andm = t+C 0+ 1A,B +2 ≥ t+ C(yBA), we have:\ndisct(DA, yBA ◦DB) ≤ discm(yAB ◦DA, DB) ≤ 0 + 1 (301)\nSince m ≥ t ≥ k ≥ E 0+ 1B,A , we have:\nCt, 0+ 1B,A = C k, 0+ 1 B,A = C 0+ 1 B,A = C 0+ 1 A,B + 2 = C m, 0+ 1 A,B + 2 = n+ 2 (302)\nTherefore, C(yAB) + 2 = C m, 0+ 1 A,B + 2 ≤ C(yBA) and by Thm. 1, C(yBA) = C(yAB) + 2. In particular,\nyBA ∈ H 0+ 1(DB , DA; t) ⊂ H 0+ 1(DB , DA; k) (303)\nIn addition, since C(yBA) = C(yAB) + 2, the following is a minimal decomposition of yBA:\nyBA = F+(pn+1:1) (304)\nWhere, F+ is the function defined in Eq. 173 (see the proof of Thm. 1).\nPart 2: In this part, we show that:\ndisct((−σ−1) ◦DZ , [F+(pn+1:1)]n−i+2:1 ◦DB) ≤ 1 and disct(DA, [F+(pn+1:1)]n+3:n−i+2 ◦ (−σ−1) ◦DZ) ≤ 0\n(305)\nBy Eq. 64, for i ∈ [n], [F+(pn+1:1)]n−i+2:1 = (−σ−1) ◦ p−1n+1:i+1 (306)\nIn particular, for i ∈ [n], [F+(pn+1:1)]n+3:n−i+2 ◦ (−σ−1) = p−1i+1:1 (307)\nBy Lem. 12, for i ∈ [n],\n[F+(pn+1:1)]n−i+2:1 ◦ yAB ◦DA = (−σ−1) ◦ pi+1:1 ◦DA (308)\nBy Lem. 1, for p :← (−σ−1) of complexity 1,\ndisct ( (−σ−1) ◦DZ , [F+(pn+1:1)]n−i+2:1 ◦DB ) = disct ( (−σ−1) ◦DZ , (−σ−1) ◦ p−1n+1:i+1 ◦DB )\n≤ disct+1 ( DZ , p −1 n+1:i+1 ◦DB )\n(309) By the third item of Lem. 1, for h :← pn+1:i+1 and m = (t+ 1) + C 0+ 1A,B + 2 = (t+ 1) + C(yAB) + 2 = (t+ 1) + C(yBA) ≥ (t+ 1) + C(p−1n+1:i+1) and by Eq. 307 we have:\ndisct ( (−σ−1) ◦DZ , [F+(pn+1:1)]n−i+2:1 ◦DB ) ≤disct+1 ( DZ , p −1 n+1:i+1 ◦DB )\n≤discm (pn+1:i+1 ◦DZ , DB) ≤ 1 (310)\nAgain, by Eq. 307 and by the third item of Lem. 1, for h :← pi+1:1 andm ≥ t+C 0+ 1A,B +2 ≥ t+C(yAB)+2 ≥ t+ C(p−1i+1:1) we have:\ndisct ( [F+(pn+1:1)]n+3:n−i+2 ◦ (−σ−1) ◦DZ , DA ) =disct ( (pi+1:1) −1 ◦DZ , DA )\n≤discm (DZ , pi+1:1 ◦DB) ≤ 0 (311)\nPart 3: In this part, we show that:\n[F+(pn+1:1)]n−i+2:1 ∈ H 1(DB , (−σ−1) ◦DZ ; k) and [F+(pn+1:1)]n+3:n−i+2 ∈ H 0((−σ−1) ◦DZ , DA; k)\n(312)\nThe following conditions hold:\n• t ≥ k + 2C 0+ 1A,B + 4 = k + 2C 0+ 1B,A . • k ≥ max { E 0+ 1B,A , E 4 0+4 1 B,A } .\n• C 0+ 1B,A = C3 0+ 1B,A = C 0+3 1B,A .\n• disct([F+(pn+1:1)]n−i+2:1 ◦DB , (−σ−1) ◦DZ) ≤ 1.\n• disct([F+(pn+1:1)]n+3:n−i+2 ◦ (−σ−1) ◦DZ , DA) ≤ 0.\n• yBA = F+(pn+1:1) ∈ H 0+ 1(DB , DA; t).\nTherefore, by Lem. 40,\n[F+(pn+1:1)]n−i+2:1 ∈ H 1(DB , (−σ−1) ◦DZ ; k) (313)\nand [F+(pn+1:1)]n+3:n−i+2 ∈ H 0((−σ−1) ◦DZ , DA; k) (314)\nWe conclude that: Z(DB , (−σ−1) ◦DZ , DA; k, 1, 0) ∩H 0+ 1(DB , DA; k) (315)\nFinally, let f ∈ H 1(DB , (−σ−1) ◦ DZ ; k) and g ∈ H 0((−σ−1) ◦ DZ , DB ; k) and s = k − C 0+ 1Z,B − 1, then:\ndiscs(g ◦ f ◦DB , DA) ≤disck(f ◦DA, (−σ−1) ◦DZ) + disck(g ◦ (−σ−1) ◦DZ , DA) ≤ 0 + 1\n(316)\nSince s ≥ E 0+ 1B,A , we have: C(g ◦ f) ≥ C 0+ 1B,A . Therefore,\nC 0+ 1B,A ≤ C(g ◦ f) ≤ C(g) + C(f) ≤ C([F+(pn+1:1)]n+3:n−i+2) + C([F+(pn+1:1)]n−i+2:1) = C(yBA) ≤ Ck, 0+ 1B,A = C 0+ 1B,A (317)\nIn particular, g 7 f . Alternatively, (−σ−1) ◦DZ is a (k, 1, 0, 0 + 1)-shared semantic distribution between B and A."
    }, {
      "heading" : "Proof of Thm. 7",
      "text" : "Definition 15 Let A = (XA, DA) and B = (XB , DB) be two domains and DZ a distribution. We say that DZ is a (m, 0, 1)-shared irreducible distribution between A and B, if for all yB ∈ H 1(DZ , DB ;m) and y−1A ∈ H 0(DA, DZ ;m) we have: yB 7 y−1A . If 0 = 1 we write (m, 0) for short.\nLemma 41 (Alignment) Let N = SCM[σ] be a NN-SCM with σ that is Leaky ReLU with parameter a > 0 and A = (XA, DA) and B = (XB , DB) are two domains and DZ a distribution. Let k ≥ max { E 0A,Z , E 0 Z,B } and m ≥ k + C 0Z,B . Let DZ is a (m, 0)-shared irreducible distribution between A and B. Let h be a function that satisfies:\n• C(h) ≤ C 0A,Z + C 0Z,B .\n• discm(h ◦DA, DB) ≤ 1."
    }, {
      "heading" : "Then, one of the following holds:",
      "text" : "• h ∈ H 0+ 1(DZ , DB ; k,C 0Z,B) ◦H 0(DA, DZ ;m).\n• For every y ∈ Z(DA, DZ , DB ;m, 0) we have: C(h||y) ≥ 2C 0Z,B − 9.\nProof We denote i = C 0A,Z + C 0 Z,B , j = 2C 0 Z,B − 9 and:\nH(DA, DB ; i, j, 0, 1) := { h ∣∣∣∃y ∈ Z(DA, DZ , DB ;m, 0)\ndiscm(h ◦DA, DB) ≤ 1, C(h) ≤ i and C(h||y) < j } (318)\nWe would like to show that if C(h) ≤ C 0A,Z + C 0Z,B and discm(h ◦DA, DB) ≤ 1, then exactly one of the following holds:\n• h ∈ H 0+ 1(DZ , DB ; k,C 0Z,B) ◦H 0(DZ , DB ;m).\n• h /∈ H(DA, DB ; i, j, 0, 1).\nIf the second option holds; then, for every y ∈ Z(DA, DZ , DB ;m, 0) we have C(h||y) = C ( h ◦ y−1 ) ≥ 2C 0Z,B − 9. Therefore, we assume that h ∈ H(DA, DB ; i, j, 0, 1) and prove that the first option holds. We denote by yAB ∈ Z(DA, DZ , DB ;m, 0) the function, y, that corresponds to h (see Eq. 318). In addition, we denote y−1A ∈ H 0(DA, DZ ;m) and yB ∈ H 0(DZ , DB ;m) such that yAB = yB ◦ y−1A and Π := h ◦ y−1AB . Since DZ is a (m, 0)-shared irreducible distribution, yB 7 y−1A .\nPart 1: We would like to prove that C(Π ◦ yB) ≤ C(yB). By Thm. 16, there are decompositions Π = a ◦ b and yB = b−1 ◦ c such that: b−1 7 c, a 7 c and C(Π) ≥ C(a) + C(b)− I − 1. Case 1: Assume that C(c) ≥ I . By Thm. 4, for f :← a, g :← c, h :← y−1A , we have,\nC(Π ◦ yAB) = C(a) + C(c) + C(y−1A ) = C(a ◦ c) + C(y−1A ) = C(Π ◦ yB) + C(y−1A )\n(319)\nIn particular,\nC(Π ◦ yAB)− C(yAB) = C(Π ◦ yAB)− C(yAB) ≥ C(Π ◦ yB)− C(yB) (320)\nBut, C(h) ≤ C 0A,Z + C 0Z,B = C(yAB) and therefore, C(Π ◦ yB) ≤ C(yB). Case 2: Assume that C(c) < I . If C(yB) < C(Π ◦ yB) then we have: Π ◦ yB = a ◦ c such that C(a ◦ c) = C(a) + C(c) > C(yB) and since C(c) < I we have:\nC(a) ≥ C(yB) + 1− C(c) ≥ C(yB) + 1− (I − 1) = C(yB)− I + 2 (321)\nIn addition, C(Π) ≥ C(a) + C(b)− I − 1 ≥ C(yB) + C(b)− 2I + 1 (322) Since C(yB) = C(b−1) + C(c), by Thm. 1, we have:\nC(b) ≥ C(b−1)− 2 = C(yB)− C(c)− 2 ≥ C(yB)− I − 1 (323)\nWe conclude that: C(Π) ≥ 2C(yB)− 3I (324)\nSince σ is a Leaky ReLU with parameter a > 0, we have, I ≤ 3, and, therefore, we can replace every instance of I in the proof with 3. In contradiction to C(Π) < 2C(yB)− 9. In particular, C(Π ◦ yB) ≤ C(yB). Part 2: In this part, we show that:\ndisck(Π ◦ yB ◦DZ , DB) ≤ 0 + 1 (325)\nBy the first item of Lem. 2; forD1 :← DZ ,D2 :← y−1A ◦DA,D3 :← DB , p :← Π◦yB andm ≥ k+C 0Z,B ≥ k + C(yB) ≥ k + C(Π ◦ yB), we have:\ndisck(Π ◦ yB ◦DZ , DB) ≤ discm(y−1A ◦DZ , DB) + discm(Π ◦ yB ◦ y−1A ◦DA, DB) = discm(y−1A ◦DZ , DB) + discm(h ◦DA, DB) ≤ 0 + 1\n(326)\nPart 3: As we mentioned earlier, one can represent h = Π ◦ yAB = (Π ◦ yB) ◦ y−1A . In Parts 1 and 2, we showed that Π ◦ yB ∈ H 0+ 1(DZ , DB ; k,C 0Z,B) (327) since C(Π ◦ yB) ≤ C(yB) = C 0Z,B and disck(Π ◦ yB ◦ DZ , DB) ≤ 0 + 1. In addition, by definition, y−1A ∈ H 0(DA, DZ ;m). Therefore, we conclude that:\nh ∈ H 0+ 1(DZ , DB ; k,C 0Z,B) ◦H 0(DA, DZ ;m) (328)\nLemma 42 LetN = SCM[σ] be a NN-SCM with σ that is Leaky ReLU with parameter a > 0,A = (XA, DA) andB = (XB , DB) are two domains andDZ is a distribution. m ≥ max { E 0+ 1A,B , E 0 A,Z , E 1 Z,B } and assume that DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Let f ∈ H 0(DA, DZ ;m) and h = g ◦ f is a function such that:\n• discm(h ◦DA, DB) ≤ 0 + 1. • C(g) ≤ C 1Z,B ."
    }, {
      "heading" : "Then, g 7 f .",
      "text" : "Proof Since m ≥ max { E 0+ 1A,B , E 0 A,Z , E 1 Z,B } and DZ being a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B, by Lem. 19, we have:\nC 1Z,B + C 0 A,Z = C 0+ 1 A,B (329)\nOn the other hand, we assumed that C(g) ≤ C 1Z,B and that f ∈ H 0(DA, DZ ;m). In particular,\nC(h) ≤ C(g) + C(f) ≤ C 1Z,B + C 0A,Z (330)\nBy the definition of Cm, 0+ 1A,B as the minimal complexity of a function h ′ satisfying discm(h′ ◦DA, DB) ≤\n0 + 1, we have: C 0+ 1A,B = C m, 0+ 1 A,B ≤ C(h) (331)\nTherefore, C(h) = C(g) + C(f) and g 7 f .\nTheorem 7 (Alignment) Let N = SCM[σ] be a NN-SCM with σ that is Leaky ReLU with parameter a > 0 and A = (XA, DA) and B = (XB , DB) are two domains. Let k ≥ max { E2 0A,B , E 0 A,Z , E 0 Z,B } and m ≥ k + C 0Z,B . Assume that DZ is a (m, 0)-shared semantic distribution between A and B such that C3 0Z,B = C 0 Z,B . Let h ∈ H2 0(DA, DB ;m). Then, one of the following holds:\n• h = g ◦ f ∈ Z(DA, DZ , DB ; k, 0, 3 0) such that g 7 f . • For every y ∈ Z(DA, DZ , DB ;m, 0) we have: C(h||y) ≥ 2C 0Z,B − 9.\nProof By Lem. 19, we have: C 0A,Z + C 0 Z,B = C 2 0 A,B (332) Therefore, since h ∈ H2 0(DA, DB ;m) we obtain: • C(h) ≤ C 0A,Z + C 0Z,B . • discm(h ◦DA, DB) ≤ 2 0.\nIn addition, DZ is a shared (m, 0)-semantic distribution between A and B. Thus, DZ is a shared (m, 0)- irreducible distribution between A and B. By Lem. 41 for 1 :← 2 0, one of the following holds: • h ∈ H3 0(DZ , DB ; k,C 0Z,B) ◦H 0(DA, DZ ;m). • For all y ∈ Z(DA, DZ , DB ;m, 0) we have: C(h||y) ≥ 2C 0Z,B − 9.\nIf the first option holds, then h = g ◦ f such that f ∈ H 0(DA, DZ ;m) and C(g) ≤ C 0Z,B . Therefore, by Lem. 42 for 1 :← 0 we have: g 7 f . In addition, by Lem. 18 we have:\nH 0(DA, DZ ;m) ⊂ H 0(DA, DZ ; k) (333)\nand since C3 0Z,B = C 0 Z,B we can replace the first option by:\nh = g ◦ f ∈ H3 0(DZ , DB ; k,C3 0Z,B) ◦H 0(DA, DZ ; k) = Z(DA, DZ , DB ; k, 0, 3 0) s.t g 7 f (334)"
    }, {
      "heading" : "Appendix J. Irreducible distributions",
      "text" : "If yAB = yB ◦ y−1A is the minimal mapping that passes through a distribution DZ as DA y−1A−→ DZ yB−→ DB , then one can replace DZ with a shared irreducible distribution DT , such that yAB = gB ◦ g−1A , DA g−1A−→ DT gB−→ DB and there is a function b such that yA = gA ◦ b, yB = gB ◦ b. For simplicity we assume that the discriminators are D∞. We will omit writing that the discriminators are of complexity ≤ ∞, i.e, H (D1, D2) := H (D1, D2;∞), Z(D1, D2; ) := Z(D1, D2;∞, ), etc’. Lemma 43 Let N = SCM[σ] be a NN-SCM with σ that is Leaky ReLU with parameter a > 0. Let A = (XA, DA) and B = (XB , DB) be two domains and DZ a distribution. Assume that C 0/2A,Z = C2 0A,Z and C 0Z,B = C 2 0 Z,B . Let yAB = yB ◦ y−1A ∈ Z(DA, DZ , DB ; 0/2) such that:\nu ∈ H2 0(DZ , DB ;C 0Z,B + 4) ◦H2 0(DA, DZ) we have: C(yAB) ≤ C(u) (335) Then, there is a 0-shared irreducible distribution DT such that:\nyAB = gB ◦ g−1A ∈ H2 0(DT , DB ;C 0T,B + 4) ◦H 0(DA, DT ) such that: gB 7 g−1A and ∃b : yA = gA ◦ b, yB = gB ◦ b\n(336)\nProof We divide the proof into a few parts.\nPart 1: In this part, we find a distribution DT such that:\nyAB ∈ H2 0(DT , DB ;C 0T,B + 4) ◦H 0(DA, DT ) (337)\nBy Thm. 16, there are decompositions, yB = gB ◦ b and y−1A = b−1 ◦ g−1A such that gB 7 g−1A , b−1 7 g−1A and C(gB) + C(b)− 4 ≤ C(gB ◦ b). We define DT := g−1A ◦DA. In particular,\ndisc∞(g−1A ◦DA, DT ) ≤ 0 (338)\nSince b−1 7 g−1A and C 0/2 A,Z = C 2 0 A,Z , we have:\nC(b−1) + C(g−1A ) = C(b −1 ◦ g−1A ) = C(y−1A ) ≤ C 0/2 A,Z = C 2 0 A,Z (339)\nIn addition,\ndisc∞(b−1 ◦DT , DZ) = disc∞(b−1 ◦ g−1A ◦DA, DZ) = disc∞(y−1A ◦DA, DZ) ≤ 0 (340) Thus, C(b−1) ≥ C 0T,Z and by Lem. 17, we have:\nC(b−1) + C(g−1A ) = C(y −1 A ) ≤ C2 0A,Z ≤ C 0A,T + C 0T,Z ≤ C 0A,T + C(b−1) (341)\nIn particular, C(g−1A ) ≤ C 0A,T . Therefore, g−1A ∈ H 0(DA, DT ). Thus, C(g−1A ) = C 0A,T . Next, we would like to prove that gB ∈ H 0(DT , DB ;C 0T,B + 4). We consider that:\ndisc∞(gB ◦DT , DB) = disc∞(gB ◦ g−1A ◦DA, DB) = disc∞(yAB ◦DA, DB) ≤ 0 (342) In particular, C(g−1A ) ≥ C 0T,B . And by the third part of Lem. 2, for p :← b,\ndisc∞(b ◦DZ , DT ) ≤ disc∞(DZ , b−1 ◦DT ) ≤ 0 (343) In particular, C(b) ≥ C 0Z,T . By Lem. 17,\nC(gB) + C(b)− 4 ≤ C(gB ◦ b) = C(yB) ≤ C 0Z,B = C2 0Z,B ≤ C 0T,B + C 0Z,T ≤ C 0T,B + C(b)\n(344)\nTherefore, C(gB) ≤ C 0T,B + 4. In particular, yAB ∈ H2 0(DT , DB ;C 0T,B + 4) ◦H 0(DA, DT ) (345)\nPart 2: We would like to prove that:\nZ(DA, DT , DB ; 0) ⊂ H2 0(DZ , DB ;C 0Z,B + 4) ◦H2 0(DA, DZ) (346)\nLet uAB = uB ◦ u−1A ∈ Z(DA, DT , DB ; 0). Then, uAB = uB ◦ u−1A such that u−1A ∈ H 0(DA, DT ) and uB ∈ H 0(DT , DB). By the first item of Lem. 2, for D1 :← u−1A ◦DA, D2 :← DT , D3 :← DZ we have:\ndisc∞(b−1 ◦ u−1A ◦DA, DZ) ≤ disc∞(b−1 ◦DT , DZ) + disc∞(DT , u−1A ◦DA) = disc∞(y−1A ◦DA, DZ) + disc∞(DT , u−1A ◦DA) ≤ 2 0\n(347)\nIn addition,\nC(b−1 ◦ u−1A ) ≤ C(b−1) + C(u−1A ) = C(b−1) + C(g−1A ) = C(b−1 ◦ g−1A ) = C(y−1A ) (348) In particular, b−1 ◦ u−1A ∈ H2 0(DA, DZ) (349) On the other hand, by the first item of Lem. 2, for D1 :← b ◦DZ , D2 :← DT , D3 :← DB , p :← uB ,\ndisc∞(uB ◦ b ◦DZ , DB) ≤ disc∞(uB ◦DT , DB) + disc∞(b ◦DZ , DT ) = disc∞(uB ◦DT , DB) + disc∞(b ◦DZ , b ◦ y−1A ◦DA) ≤ 0 + disc∞(b ◦DZ , b ◦ y−1A ◦DA)\n(350)\nBy Lem. 1, for p :← b, we have: disc∞(b ◦DZ , b ◦ y−1A ◦DA) ≤ disc∞(DZ , y−1A ◦DA) ≤ 0 (351)\nFinally, disc∞(uB ◦ b ◦DZ , DB) ≤ 2 0 (352)\nIn addition, since disc∞(gB ◦DT , DB) ≤ 0 we have C(gB) ≥ C 0T,B = C(uB). Therefore, C(uB ◦ b) ≤ C(uB) + C(b) ≤ C(gB) + C(b) ≤ C(gB ◦ b) + 4 = C(yB) + 4 = C 0Z,B + 4 (353)\nWe conclude that:\nb−1 ◦ u−1A ∈ H2 0(DA, DT ) and uB ◦ b ∈ H2 0(DT , DB ;C 0Z,B + 4) (354) Therefore,\nuAB ∈ H2 0(DT , DB ;C 0Z,B + 4) ◦H2 0(DA, DT ) (355)\nPart 3: Now, let uAB = uB ◦ u−1A ∈ Z(DA, DT , DB ; 0) such that u−1A ∈ H 0(DA, DT ) and uB ∈ H 0(DT , DB). Therefore,\nC(uB) ≤ C 0T,B ≤ C(gB) and C(u−1A ) ≤ C 0A,T = C(g−1A ) (356) In addition, by part 2:\nuAB ∈ H2 0(DT , DB ;C 0Z,B + 4) ◦H2 0(DA, DT ) := F (357) Since yAB has a smaller complexity than all of the functions in F , we have C(yAB) ≤ C(uAB) and:\nC(yAB) ≤ C(uAB) ≤ C(uB) + C(u−1A ) (358) In addition, since gB 7 g−1A and the fact that C(uB) ≤ C(gB) and C(u−1A ) ≤ C(g−1A ), we have:\nC(uB) + C(u −1 A ) ≤ C(gB) + C(g−1A ) = C(yAB) ≤ C(uB ◦ u−1A ) ≤ C(uB) + C(u−1A ) (359)\nIn particular, uB 7 u−1A .\nmaps between DT and DB ."
    } ],
    "references" : [ {
      "title" : "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Uniqueness of weights for neural networks",
      "author" : [ "E.D. Sontag F. Albertini", "V. Maillot" ],
      "venue" : "Artificial Neural Networks for Speech and Vision,",
      "citeRegEx" : "Albertini and Maillot.,? \\Q1993\\E",
      "shortCiteRegEx" : "Albertini and Maillot.",
      "year" : 1993
    }, {
      "title" : "Recovering a feed-forward net from its output",
      "author" : [ "Charles Fefferman", "Scott Markel" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Fefferman and Markel.,? \\Q1993\\E",
      "shortCiteRegEx" : "Fefferman and Markel.",
      "year" : 1993
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Ganin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to discover cross-domain relations with generative adversarial networks",
      "author" : [ "Taeksoo Kim", "Moonsu Cha", "Hyunsoo Kim", "Jungkwon Lee", "Jiwon Kim" ],
      "venue" : "arXiv preprint arXiv:1703.05192,",
      "citeRegEx" : "Kim et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Comparing fixed and variable-width gaussian networks",
      "author" : [ "Vera Kurková", "Paul C. Kainen" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Kurková and Kainen.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kurková and Kainen.",
      "year" : 2014
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng" ],
      "venue" : "In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing,",
      "citeRegEx" : "Maas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniqueness of the weights for minimal feedforward nets with a given input-output map",
      "author" : [ "Héctor J. Sussmann" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Sussmann.,? \\Q1992\\E",
      "shortCiteRegEx" : "Sussmann.",
      "year" : 1992
    }, {
      "title" : "Towards principled unsupervised learning",
      "author" : [ "Ilya Sutskever", "Rafal Józefowicz", "Karol Gregor", "Danilo Jimenez Rezende", "Timothy P. Lillicrap", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1511.06440,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised cross-domain image generation",
      "author" : [ "Yaniv Taigman", "Adam Polyak", "Lior Wolf" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Taigman et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Taigman et al\\.",
      "year" : 2017
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V.N. Vapnik", "A.Y. Chervonenkis" ],
      "venue" : "Theory of Probab. and its Applications,",
      "citeRegEx" : "Vapnik and Chervonenkis.,? \\Q1971\\E",
      "shortCiteRegEx" : "Vapnik and Chervonenkis.",
      "year" : 1971
    }, {
      "title" : "Existence and uniqueness results for neural network approximations",
      "author" : [ "Robert C. Williamson", "Uwe Helmke" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "Williamson and Helmke.,? \\Q1995\\E",
      "shortCiteRegEx" : "Williamson and Helmke.",
      "year" : 1995
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "Yingce Xia", "Di He", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma" ],
      "venue" : "arXiv preprint arXiv:1611.00179,",
      "citeRegEx" : "Xia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2016
    }, {
      "title" : "Dualgan: Unsupervised dual learning for image-to-image translation",
      "author" : [ "Zili Yi", "Hao Zhang", "Ping Tan", "Minglun Gong" ],
      "venue" : "arXiv preprint arXiv:1704.02510,",
      "citeRegEx" : "Yi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Unpaired image-to-image translation using cycle-consistent adversarial networkss",
      "author" : [ "Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros" ],
      "venue" : "arXiv preprint arXiv:1703.10593,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2017
    }, {
      "title" : "Empirical Validation of Prediction 1 In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping",
      "author" : [ "B. Appendix" ],
      "venue" : null,
      "citeRegEx" : "Appendix,? \\Q2017\\E",
      "shortCiteRegEx" : "Appendix",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.",
      "startOffset" : 24,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.",
      "startOffset" : 24,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.",
      "startOffset" : 24,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.",
      "startOffset" : 24,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "For example, given a set of unlabeled images of horses, and a set of unlabeled images of zebras, CycleGAN (Zhu et al., 2017) creates the analog zebra image for a new image of a horse and vice versa.",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "This is enforced using GANs (Goodfellow et al., 2014) and is applied at the distribution level: the mapping of horse images to the zebra domain should create images that are indistinguishable from the training images of zebras and vice versa.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "In another example, taken from DiscoGAN (Kim et al., 2017), a function is learned to map a handbag to a shoe of a similar style.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : ", (Radford et al., 2015), in mapping random input vectors into realistic-looking images.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "It makes sense to assume that both yA and yB are invertible, since given training samples, one may be expected to be able to recover the underlying properties of the generated samples, even with very weak supervision (Chen et al., 2016).",
      "startOffset" : 217,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "In the cross domain transfer line of work (Taigman et al., 2017), the alignment problem is dealt with by incorporating a fixed pre-trained feature map f and requiring what is called f -constancy, namely that the following risk is small RDA [f, f ◦ h], or informally, f(x) = f(h(x)).",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "where discC(D1, D2) = supc1,c2∈C |RD1 [c1, c2]−RD2 [c1, c2]| denotes the discrepancy between distributions D1 and D2 that is implemented with a GAN (Ganin et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 14,
      "context" : "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.",
      "startOffset" : 18,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.",
      "startOffset" : 18,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.",
      "startOffset" : 18,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.",
      "startOffset" : 18,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "However, when we learn the mapping using a neural network with one hidden layer of size 2, and Leaky ReLU activations1 (Maas et al., 2013), y AB is one of only two options.",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "For example, DiscoGAN (Kim et al., 2017) employs either eight or ten layers, depending on the dataset.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh are also used).",
      "startOffset" : 86,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh are also used).",
      "startOffset" : 86,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : ", M · et(M)]> where ei is the i’th standard basis vector, t is a permutation over [M ] and i ∈ {±1} (Fefferman and Markel, 1993).",
      "startOffset" : 100,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurková and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions.",
      "startOffset" : 12,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurková and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions.",
      "startOffset" : 12,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurková and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions.",
      "startOffset" : 12,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "The most notable work has been done by Fefferman and Markel (1993) that proves identifiability for σ = tanh.",
      "startOffset" : 39,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : ", (Kim et al., 2017), they form well-understood ambiguities.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "For example, if all horse riders are covered in the training set, a shirtless rider in the test set (as demonstrated in (Zhu et al., 2017)) would become striped when converting the horse image to a zebra image.",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "For example, the mapping of cars in one pose to cars in the mirrored pose that sometimes happens in (Kim et al., 2017), is similar in nature to the mapping of x to 1− x in the simple example given in Sec.",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "While in the former, deeper networks, which can learn even random labels, work well (Zhang et al., 2017), unsupervised learning requires a careful control of the network capacity.",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "The stratified complexity model (SCM) is related to structural risk minimization by Vapnik and Chervonenkis (1971), which employs a hierarchy of nested subsets of hypothesis classes in order of increasing complexity.",
      "startOffset" : 84,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "As an extreme example, Sutskever et al. (2015) present empirical evidence that a semantic mapper can be learned, even from very few examples, if the network trained is kept small.",
      "startOffset" : 23,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA → XB , based only on the GAN constraint on B, is presented as a failing baseline.",
      "startOffset" : 18,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA → XB , based only on the GAN constraint on B, is presented as a failing baseline.",
      "startOffset" : 18,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "In (Yi et al., 2017), among many non-semantic mappings obtained by the GAN baseline, one can find images of GANs that are successful.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "In (Kim et al., 2017), 8 layers are sometimes employed while at other times 10 layers are used (counting both convolution and deconvolution).",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "These experiments were done on the CelebA gender conversion task, where eight layers are employed in the experiments of (Kim et al., 2017).",
      "startOffset" : 120,
      "endOffset" : 138
    } ],
    "year" : 2017,
    "abstractText" : "We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that the target mapping is of lower complexity than all other mappings. The measured complexity is directly related to the depth of the neural networks being learned and the semantic mapping could be captured simply by learning using architectures that are not much bigger than the minimal architecture.",
    "creator" : "LaTeX with hyperref package"
  }
}