{
  "name" : "1606.04038.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Trace Norm Regularised Deep Multi-Task Learning",
    "authors" : [ "Yongxin Yang", "Timothy M. Hospedales" ],
    "emails" : [ "t.hospedales}@qmul.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n04 03\n8v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n16"
    }, {
      "heading" : "1 Introduction and Related Work",
      "text" : "The paradigm of multi-task learning (MTL) [Caruana, 1997] is to learn multiple tasks jointly, and the knowledge obtained from one task could be reused by others. In this section, we will briefly review some studies in this area."
    }, {
      "heading" : "1.1 Matrix-based Multi-Task Learning",
      "text" : "Matrix-based MTL is usually built on linear model, i.e., each task is parameterised by a D-dimensional weighting vector w, and the model prediction is ŷ = x · w = xTw, where x is a D-dimensional feature vector representing an instance. The objective function for matrix-based MTL can be written as,\nT∑\ni=1\nN(i)∑\nj=1\nℓ(y (i) j , x (i) j · w (i)) + λΩ(W ) (1)\nHere ℓ(y, ŷ) is a loss function for the true label y and the predicted label ŷ. T is the number of tasks, and for the i-th task there are N (i) training instances. Assuming the dimensionality of every task’s feature is the same, the models – w(i)s – are of the same size. Then the collection of w(i)s forms a D×T matrix W of which the i-th column is the linear model for the i-t task. Ω(W ) is a kind of regularisation that encourages W to be a low-rank matrix. Some choices could be the ℓ2,1 norm [Argyriou et al., 2008], and the trace norm [Ji and Ye, 2009]. An alternative approach [Kumar and Daumé III, 2012] is to explicitly formulate W as a low-rank matrix, i.e., W = LS where L is a D ×K matrix and S is a K × T matrix with K < min(D,T ) as a hyper-parameter (matrix rank)."
    }, {
      "heading" : "1.2 Tensor-based Multi-Task Learning",
      "text" : "Though it is a classic setting that each task is indexed by a single factor, in many realworld problems, tasks are indexed by multiple factors. For example, to build a restaurant recommendation system, we want a regression model that predicts the scores for different aspects (food quality, environment) by different customers. Then the task is indexed by aspects × customers. For this case, the collection of all linear models for all tasks is then a 3-way tensor W of size D × T1 × T2, where T1 and T2 is the number of aspects and the\nnumber of customers respectively. Consequently Ω(W) has to be a kind of tensor regularisation [Tomioka et al., 2010]. For example, sum of the trace norms on all matriciations1 [Romera-paredes et al., 2013], and scaled latent trace norm [Wimalawarne et al., 2014]. An alternative solution is to concatenate the one-hot encodings of task factors and feed it as input into a two-branch neural network model [Yang and Hospedales, 2015], in which there are two input channels for feature vector and encoded task factor."
    }, {
      "heading" : "1.3 Multi-Task Learning for Neural Networks",
      "text" : "With the great success of deep learning, many researches are proposed for deep multi-task learning. Zhang et al. [2014] use a convolutional neural network to find facial landmarks as well as recognise face attributes (e.g., emotions). Liu et al. [2015] propose a deep neural network for query classification and information retrieval (ranking for web search). One of the key commonalties of these work is that they all use a kind of predefined parameter sharing strategy. A typical design is to use the same set of parameters for the bottom layers of the deep neural network and use the task-specific parameters for the top layers. This kind of architecture design can be traced back to 2000s [Bakker and Heskes, 2003]. However, modern neural network models usually contain a large number of layers, which makes the decision after which layer the neural network is split for different tasks extremely hard."
    }, {
      "heading" : "2 Methodology",
      "text" : "Instead of predefining a parameter sharing strategy, we propose the following framework: For T tasks, each is modelled by a neural network of the same architecture2. We collect the parameters in a layer-wise fashion, and put a tensor norm on every collection. We illustrate our framework by a simple example: assuming that we have T = 2 tasks, and each is modelled by a 4-layer convolution neural network (CNN). The CNN architecture is: (1) convolutional layer (‘conv1’) of size 5 × 5 × 3 × 32, (2) convolutional layer (‘conv2’) of size 3 × 3 × 32 × 64, (3) fully-connected layer (‘fc1’) of size 256 × 256, (4) fully-connected layer (‘fc2’(1)) of size 256 × 10 for the first task and fully-connected layer (‘fc2’(2)) of size 256× 20 for the second task. Note that we assume these two tasks have different number of outputs. In our framework, the shareable layers are ‘conv1’, ‘conv2’, and ‘fc1’. For single task learning mode, the parameters are ‘conv1’(1), ‘conv2’(1), ‘fc1’(1), and ‘fc2’(1) for the first task; ‘conv1’(2), ‘conv2’(2), ‘fc1’(2), and ‘fc2’(2) for the second task. We can see that there are not any parameter sharing between these two tasks. For one of the possible predefined deep MTL, the parameters could be ‘conv1’, ‘conv2’, ‘fc1’(1), and ‘fc2’(1) for the first task; ‘conv1’, ‘conv2’, ‘fc1’(2), and ‘fc2’(2) for the second task, i.e., the first and second layer are fully shared in this case. For our proposed method, the parameter setting is the same as single task learning mode, but we put three tensor norms on the stacked {‘conv1’(1), ‘conv1’(2)} (a tensor of size 5× 5× 3× 32× 2), the stacked {‘conv2’(1), ‘conv2’(2)} (a tensor of size 3× 3× 32× 64× 2), and the stacked {‘fc1’(1), ‘fc1’(2)} (a tensor of size 256× 256× 2) respectively."
    }, {
      "heading" : "2.1 Tensor Norm",
      "text" : "We choose to use the trace norm, which is defined as the sum of matrix’s singular values.\n||X ||∗ = ∑\ni=1\nσi (2)\n1Matriciation is also known as tensor unfolding or flattening. 2For the case that each task has a different number of outputs, the parameters of topmost layers from\nneural networks should be of different size, thus they are opted out for sharing.\nThe trace norm is named by the fact that when X is a positive semidefinite matrix, it is the trace of X . It is sometimes referred to as nuclear norm. Trace norm is the tightest convex relation of matrix rank [Recht et al., 2010]. The extension of trace norm from matrix to tensor is not unique, just like the rank of tensor has different definitions. How to define the rank of tensor largely depends on how the tensor is factorised, e.g., Tucker decomposition [Tucker, 1966] and Tensor-Train decomposition Oseledets [2011]. We propose three tensor trace norm designs here, which are corresponding to three variants of the proposed method. For an N -way tensor W of size D1 ×D2 × · · · ×DN . We define Tensor Trace Norm Tucker\n||W||∗ =\nN∑\ni=1\nγi||W(i)||∗ (3)\nHere W(i) is the mode-i tensor flattening/unfolding, which is obtained by,\nW(i) = reshape(transpose(W , [Di, D1, . . . , Di−1, Di+1 . . . , DN ]), [Di, ∏\nj¬i\nDj ]) (4)\nTensor Trace Norm TT\n||W||∗ =\nN−1∑\ni=1\nγi||W[i]||∗ (5)\nHere W[i] is yet another way to unfold the tensor, which is obtained by,\nW[i] = reshape(W , [D1 ×D2 . . .Di, Di+1 ×Di+2 . . . DN ]) (6)\nTensor Trace Norm Matrix\n||W||∗ = γ||W(N)||∗ (7)\nThis is the simplest definition. Given that in our framework, the last axis of tensor indexes the tasks, i.e., DN = T , it is the most straightforward way to adapt the technique in matrix-based MTL – reshape the D1 ×D2 × · · · × T tensor to D1D2 · · · × T matrix."
    }, {
      "heading" : "2.2 Optimisation",
      "text" : "Using gradient-based method for optimisation involved with trace norm is not a common choice, as there are better solutions based on semi-definite programming or proximal gradients since the trace norm is essentially non-differentiable. However, deep neural network is usually trained by gradient descent, and it is relatively harder to modify the training process of neural network. Therefore we use the (sub-)gradient descent for trace norm terms. The sub-gradient for trace norm can be derived as,\n∂||X ||∗ ∂X = X(XTX)− 1 2 (8)\nA more numerical stable method instead of computing (XTX)− 1 2 is to use SVD. For an N × P matrix X , SVD computes\nX = UΣV T (9)\nHere U is an N × K matrix, Σ is a K × K matrix and V is a P × K matrix, and K = min(N,P ).\nX(XTX)− 1 2 = UV T (10)"
    }, {
      "heading" : "3 Experiment",
      "text" : "We implement the proposed method by TensorFlow [Abadi et al., 2015], and the code is available on Github3."
    }, {
      "heading" : "3.1 Omniglot",
      "text" : "We demonstrate the proposed method using the Omniglot dataset [Lake et al., 2015]. Omniglot contains handwritten characters in 50 different alphabets (e.g., Cyrillic, Korean, Tengwar), each with its own number of unique characters (14 ∼ 55). In total, there are 1623 unique characters, and each has exactly 20 instances. Here each task corresponds to an alphabet, and the goal is to recognise its characters. The images are monochrome of size 105 × 105. We design a CNN with 3 convolutional and 2 FC layers. The first conv layer has 8 filters of size 5 × 5; the second conv layer has 12 filters of size 3 × 3, and the third convolutional layer has 16 filters of size 3 × 3. Each convolutional layer is followed by a 2×2 max-pooling. The first FC layer has 64 neurons, and the second FC layer has size corresponding to the number of unique classes in the alphabet. The activation function is tanh. We compare the three variants of the proposed framework – TNRDMTL-1 (based on Eq. 7), TNRDMTL-2 (based on Eq. 3), and TNRDMTL-3 (based on Eq. 5) with single task learning (STL). For every layer, there are one (TNRDMTL-1) or more (TNRDMTL-2 and TNRDMTL-3) γ that control the trade-off between the classification loss (cross-entropy) and the trace norm terms, for which we set all γ = 0.01. The experiments are repeated 10 times, and every time 10% training data and 90% testing data are randomly selected. The results are shown in Table 1.\nBesides, we plot the changes of cross-entropy loss in training set and the values of norm terms with the neural networks’ parameters updating. As we can see in Fig 1, STL has the lowest training loss, but its performance is the worst in Table 1, this indicates over-fitting. With the help of regularisation, MTL suffers less from this issue.\n3https://github.com/wOOL/TNRDMTL"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org",
      "author" : [ "Y. Yu", "X. Zheng" ],
      "venue" : null,
      "citeRegEx" : "Yu and Zheng.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu and Zheng.",
      "year" : 2015
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2008
    }, {
      "title" : "Task clustering and gating for Bayesian multitask learning",
      "author" : [ "B. Bakker", "T. Heskes" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Bakker and Heskes.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bakker and Heskes.",
      "year" : 2003
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Caruana.,? \\Q1997\\E",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "An accelerated gradient method for trace norm minimization",
      "author" : [ "S. Ji", "J. Ye" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ji and Ye.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ji and Ye.",
      "year" : 2009
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "A. Kumar", "H. Daumé III" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kumar and III.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kumar and III.",
      "year" : 2012
    }, {
      "title" : "Human-level concept learning through probabilistic program induction",
      "author" : [ "B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum" ],
      "venue" : null,
      "citeRegEx" : "Lake et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2015
    }, {
      "title" : "Representation learning using multi-task deep neural networks for semantic classification and information",
      "author" : [ "X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Tensor-train decomposition",
      "author" : [ "I.V. Oseledets" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Oseledets.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oseledets.",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P.A. Parrilo" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "Recht et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2010
    }, {
      "title" : "Multilinear multitask learning",
      "author" : [ "B. Romera-paredes", "H. Aung", "N. Bianchi-berthouze", "M. Pontil" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Romera.paredes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Romera.paredes et al\\.",
      "year" : 2013
    }, {
      "title" : "On the extension of trace norm to tensors",
      "author" : [ "R. Tomioka", "K. Hayashi", "H. Kashima" ],
      "venue" : "In NIPS Workshop on Tensors, Kernels, and Machine Learning,",
      "citeRegEx" : "Tomioka et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tomioka et al\\.",
      "year" : 2010
    }, {
      "title" : "Some mathematical notes on three-mode factor analysis",
      "author" : [ "L.R. Tucker" ],
      "venue" : null,
      "citeRegEx" : "Tucker.,? \\Q1966\\E",
      "shortCiteRegEx" : "Tucker.",
      "year" : 1966
    }, {
      "title" : "Multitask learning meets tensor factorization: task imputation via convex optimization",
      "author" : [ "K. Wimalawarne", "M. Sugiyama", "R. Tomioka" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Wimalawarne et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wimalawarne et al\\.",
      "year" : 2014
    }, {
      "title" : "A unified perspective on multi-domain and multi-task learning",
      "author" : [ "Y. Yang", "T.M. Hospedales" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Yang and Hospedales.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang and Hospedales.",
      "year" : 2015
    }, {
      "title" : "Facial landmark detection by deep multi-task learning",
      "author" : [ "Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang" ],
      "venue" : "In European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The paradigm of multi-task learning (MTL) [Caruana, 1997] is to learn multiple tasks jointly, and the knowledge obtained from one task could be reused by others.",
      "startOffset" : 42,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Some choices could be the l2,1 norm [Argyriou et al., 2008], and the trace norm [Ji and Ye, 2009].",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : ", 2008], and the trace norm [Ji and Ye, 2009].",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Consequently Ω(W) has to be a kind of tensor regularisation [Tomioka et al., 2010].",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "For example, sum of the trace norms on all matriciations [Romera-paredes et al., 2013], and scaled latent trace norm [Wimalawarne et al.",
      "startOffset" : 57,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : ", 2013], and scaled latent trace norm [Wimalawarne et al., 2014].",
      "startOffset" : 38,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "An alternative solution is to concatenate the one-hot encodings of task factors and feed it as input into a two-branch neural network model [Yang and Hospedales, 2015], in which there are two input channels for feature vector and encoded task factor.",
      "startOffset" : 140,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "This kind of architecture design can be traced back to 2000s [Bakker and Heskes, 2003].",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "Zhang et al. [2014] use a convolutional neural network to find facial landmarks as well as recognise face attributes (e.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Liu et al. [2015] propose a deep neural network for query classification and information retrieval (ranking for web search).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Trace norm is the tightest convex relation of matrix rank [Recht et al., 2010].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : ", Tucker decomposition [Tucker, 1966] and Tensor-Train decomposition Oseledets [2011].",
      "startOffset" : 23,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : ", Tucker decomposition [Tucker, 1966] and Tensor-Train decomposition Oseledets [2011]. We propose three tensor trace norm designs here, which are corresponding to three variants of the proposed method.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "1 Omniglot We demonstrate the proposed method using the Omniglot dataset [Lake et al., 2015].",
      "startOffset" : 73,
      "endOffset" : 92
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that one neural network is encouraged to reuse others’ parameters if possible – this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning work, we do not predefine a parameter sharing strategy by tying some (usually bottom) layers’ parameters, instead, our framework allows the sharing for all shareable layers thus the sharing strategy is learned from a pure data-driven way.",
    "creator" : "LaTeX with hyperref package"
  }
}