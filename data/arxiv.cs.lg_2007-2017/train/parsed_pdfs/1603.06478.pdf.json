{
  "name" : "1603.06478.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hard-Clustering with Gaussian Mixture Models",
    "authors" : [ "Johannes Blömer", "Sascha Brauer", "Kathrin Bujna" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n06 47\n8v 1\n[ cs\n.L G\n] 2\n1 M\nar 2\n01 6\nHard-Clustering with Gaussian Mixture Models\nJohannes Blömer, Sascha Brauer, and Kathrin Bujna\nDepartment of Computer Science\nPaderborn University\n33102 Paderborn, Germany\nMarch 22, 2016\nTraining the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions.\nA popular hard-clustering variant of the MLE problem is the so-called completedata maximum likelihood estimation (CMLE) method. The standard approach to solve the CMLE problem is the Classification-Expectation-Maximization (CEM) algorithm [CG92]. Unfortunately, it is only guaranteed that the algorithm converges to some (possibly arbitrarily poor) stationary point of the objective function.\nIn this paper, we present two algorithms for a restricted version of the CMLE problem. That is, our algorithms approximate reasonable solutions to the CMLE problem which satisfy certain natural properties. Moreover, they compute solutions whose cost (i.e. complete-data log-likelihood values) are at most a factor (1 + ε) worse than the cost of the solutions that we search for. Note the CMLE problem in its most general, i.e. unrestricted, form is not well defined and allows for trivial optimal solutions that can be thought of as degenerated solutions.\n1 Preliminaries\nGiven set of observations, the objective of the CMLE problem is to find a Gaussian mixture model and a hard clustering with maximum complete-data likelihood. In this section, we will first describe and define this objective function. Then, we will present an alternating optimization scheme for this problem. However, the problem is not well-defined. Hence, we will restrict the problem to reasonable instances and solutions.\n1.1 Complete-Data Log-Likelihood\nLet X ⊂ Rd be a finite set of observations. Given a spherical Gaussian distribution Nd(µ, σ), the likelihood that all x ∈ X have been drawn according to Nd(µ, σ) is given by\n∏\nx∈X\nNd(x|µ, σ) ,\nassuming that the observations have been drawn independently at random.\nDefinition 1. Given a finite set X ⊂ Rd and a spherical Gaussian distribution with mean µ ∈ Rd and variance σ2 ∈ R, let\nLX(µ, σ 2) := − ln\n(\n∏\nx∈X\np(x|µ, σ2)\n)\n= |X|d\n2 ln(2πσ2k) +\n1\n2σ2k\n∑\nx∈X\n‖x− µk‖ 2 .\nWe denote the minimal value by OPT (X, 1) = min(µ,σ2)LX(µ, σ 2).\nNow consider a Gaussian mixture model (GMM) given by parameters θ = {(wk, µk, σ 2 k)} K k=1.\nDrawing an observation xn according to a GMM corresponds to a two-step process:\n1. Draw a component zn ∈ [K] with probability p(zn = k|θ) = wk.\n2. Draw an observation xn ∈ X according to Nd(µzn , σzn).\nNote that the assignment zn ∈ [K] is a (latent) random variable in this two-step process. With the help of this random variable, we can compute the likelihood that observation x ∈ X has been generated by the k-th component of the GMM, i.e.\np(xn, zn = k|θ) = p(zn = k|θ) · p(xn|zn = k, θ) = wk · Nd(x|µk, σk) .\nSince xn and zn completely describe the two-step process, the likelihood p(xn, zn|θ) is also called complete-data likelihood, while p(xn|θ) = ∑K zn=1 p(xn, zn|θ) is refered to as (marginal) likelihood.\nAssume, we are given a set of observations X = {xn} N n=1 and assignments {zn} N n=1. Then, the likelihood that all observations have been drawn according to a GMM θ and that each xn has been generated by the zn-th component, is given by\nN ∏\nn=1\np(xn, zn|θ) = N ∏\nn=1\nwzn · Nd(xn|µzn , σzn) , (1)\nassuming that the observations have been drawn inpendently at random. Note that the assignments {zn} N n=1 define a partition C = ∪̇ K k=1Ck via xn ∈ Ck iff zn = k. Hence, we can also rewrite Equation (1) as\nK ∏\nk=1\n∏\nxn∈Ck\np(xn, zn = k|θ) = K ∏\nk=1\n∏\nxn∈Ck\nwk · Nd(xn|µk, σk) .\nBy taking (negative) logarithm of this expression, we obtain\n− log\n\n\nK ∏\nk=1\n∏\nxn∈Ck\np(xn, zn = k|θ)\n\n\n= K ∑\nk=1\n∑\nxn∈Ck\n(ln(wk) + ln (Nd(xn|µk,Σk))\n=\nK ∑\nk=1\nLCk(µk, σ 2 k)− ln(wk) · |Ck| .\nDefinition 2. Given a finite set X ⊂ Rd, a partition C = {C1, . . . , CK} of X, and a mixture of spherical Gaussians with parameters θ = {(wk, µk, σ 2 k)} K k=1, we call\nLX(θ, C) := K ∑\nk=1\nLCk(µk, σ 2 k)− ln(wk) · |Ck|\nthe complete-data negative log-likelihood.\nNote that a solution maximizing the complete-data likelihood also minimizes the completedata negative log-likelihood, and vice versa. Therefore, we define the complete-cata maximum likelihood estimation (CMLE) problem as follows.\nProblem 3 (CMLE). Given a finite set X ⊂ Rd and an integer K ∈ N, find a partition C = {C1, . . . , CK} of X and a mixture of spherical Gaussians with parameters θ = {(wk, µk, σ 2 k)} K k=1 minimizing LX(θ, C). We denote the minimal value by OPT (X,K). For a fixed model θ, we let LX(θ) = minC LX(θ, C). Analogously, for a fixed clustering C, we let LX(C) = minθ LX(θ, C).\nDefinition 4. Given parameters (wk, µk, σ 2 k) and a cluster Ck ⊆ X, we let\nLx(wk, µk, σ 2 k) :=\nd 2 ln(2πσ2k) + 1\n2σ2k ‖x− µk‖\n2 − ln(wk),\nand LCk(wk, µk, σ 2 k) := ∑\nx∈Ck\nLx(wk, µk, σ 2 k) .\nRemark 5. For all partitions C = {C1, . . . , CK}, we have\nLX(C) = K ∑\nk=1\nOPT (Ck, 1)− ln\n(\n|Ck| |X|\n)\n· |Ck| .\nFor all θ = {(w1, µ1, σ 2 1), . . . , (wK , µK , σ 2 K)}, we have\nLX(θ) = N ∑\nn=1\nargmink∈[K]{Lx(wk, µk, σ 2 k)} .\n1.2 Alternating Optimization Scheme (CEM algorithm)\nAn alternating optimization algorithm for this problem is given by the following first order optimality conditions. Fixing the partition C = {Ck} K k=1, the optimal mixture of spherical Gaussians is given by θ = {(wk, µk, σ 2 k)} K k=1 with\nwk = |Ck|\n|X| , µk =\n1\n|Ck|\n∑\nxn∈Ck\nxn , σ 2 k =\n1\nd|Ck|\n∑\nxn∈Ck\n‖xn − µk‖ 2 .\nFixing the Gaussian mixture model θ = {(wk, µk, σ 2 k)} K k=1, the optimal partition C = {Ck} K k=1 is given by assigning each point to its most likely component, i.e.\nxn ∈ Ck ⇔ k = argmaxl∈[K] p(zn = l|xn, θ) ,\nwhere\np(zn = k|xn, θ) = wkN (xn|µk, σ\n2 k)\n∑K l=1wlN (xn|µl, σ 2 l )\n,\nwhich is the posterior probability that xn has been generated by the k-th component of the given mixture.\nIf we repeatedly compute these update formulas, the solution converges to a local extremum or a saddlepoint of the likelihood function.\nA proof of the correctenss of these update formulas (which we omit here) uses the following lemma.\nLemma 6. Let X ⊂ Rd be a finite set. Define\nµ(X) = 1\n|X|\n∑\nx∈X\nx .\nThen, for all y ∈ Rd\n∑\nx∈X\n‖x− y‖2 = ∑\nx∈X\n‖x− µ(X)‖2 + |X| · ‖y − µ(X)‖2 .\nIn particular, µ(X) = argminy∈Rd ∑ x∈X‖x− y‖ 2.\nNote that an optimal CMLE solution is not changed by this algorithm. Hence, an optimal CMLE solution is completely defined by a partition or a Gaussian mixture model. Similarly, if we refer to a partition or a Gaussian mixture as a CMLE solution we assume that the missing parameters are as defined by the update formulas given above, respectively.\n1.3 Well-Defined Instances\nUnfortunately, the CMLE problem is not well defined in this form. For example, you could choose C1 = {x} and µ1 = x for some x ∈ X. Then, as σ1 → 0 we get that LK(X) → −∞. Consequently, we impose the following restrictions on instances.\nDefinition 7. We call X = ˙ ⋃K\nk=1Ck a well-defined partition if\n1. for all k ∈ [K] : |Ck| ≥ 2.\nWe call X itself a well-defined instance if\n2. ∀x, y ∈ X,x 6= y : ‖x− y‖2 ≥ 4dπ .\nWe denote X = ˙ ⋃K\nk=1Ck as a well-defined solution if X is a well-defined instance and {Ck} K k=1\nis a well-defined partition.\nIn the following, we prove that, with these restrictions, the CMLE problem is well defined. That is, the minimum in Problem 3 is well defined (LK(X) > −∞). Moreover, we will see (Lemma 9) that for the optimal solution we have σ2k ≥ 1 2π or\n2πσ2k ≥ 1 for k ∈ [K]. (2)\nFirst of all, note that the sum of squared distances between the points in X and the mean µ(X) can be rewritten using pairwise distances (which are lower bounded in Restriction 2).\nLemma 8. Let X ⊂ Rd be a finite set and µ(X) := 1|X| ∑ x∈X its mean, then\n∑\nx∈X\n‖x− µ(X)‖2 = 1\n2|X|\n∑\nx∈X\n∑\ny∈X\n‖x− y‖2.\nProof.\n∑\nx∈X\n∑\ny∈X\n‖x− y‖2 = ∑\nx∈X\n∑\ny∈X\n〈x− y, x− y〉\n= ∑\nx∈X\n∑\ny∈X\n(〈x, x〉+ 〈y, y〉 − 2 〈x, y〉\n= 2|X| ∑\nx∈X\n〈x, x〉 − 2 ∑\nx∈X\n∑\ny∈X\n〈x, y〉\n= 2|X| ∑\nx∈X\n〈x, x〉 − 2|X| ∑\nx∈X\n〈x, µ(X)〉\n= 2|X| ∑\nx∈X\n〈x, x− µ(X)〉\n= 2|X| ∑\nx∈X\n〈x− µ(X), x − µ(X)〉 (using |X| ∑\nx∈X 〈µ(X), x− µ(X)〉 = 0)\n= 2|X| ∑\nx∈X\n‖x− µ(X)‖2.\nNow using the restriction on the minimum pairwise difference between points (Restriction 2) and on the minimum number of points (Restriction 1) in a cluster, we can lower bound the variance of each cluster. This directly yields Equation (2) and our claim that the problem is well-defined under the restrictions given in Definition 7.\nLemma 9. Let Y be a subset of a set X that satisfies Restriction 2 from Definition 7 and that contains at least two different elements. Then,\nσ(Y )2 = 1\n|Y |d\n∑\ny∈Y\n‖y − µ(Y )‖2 ≥ 1\n2π .\nProof.\nσ(Y )2 = 1\n|Y |d\n∑\ny∈Y\n‖y − µ(Y )‖2\n= 1\n2|Y |2d\n∑\nx∈Y\n∑\ny∈Y\n‖x− y‖2 (using Lemma 8)\n≥ 1\n2|Y |2d\n(\n|Y |\n2\n)\nmin x,y∈Y,x 6=y\n‖x− y‖2\n≥ 1\n8d min x,y∈Y,x 6=y ‖x− y‖2\n≥ 1\n2π (using Restriction 2)\nThroughout the rest of this paper, we will restrict the search space of CMLE to well-defined solutions. In particular, we only consider the optimal solution among all well-defined solutions.\n1.4 Well-Balanced Instances\nA central idea behind the algorithms that we present in this paper is that we do not allow somewhat degenerate instances. This means that we can find a function f in the number of clusters that can be used to lower bound the number of points in a cluster and a function g that can be used to lower bound the costs OPT (Ck, 1) of optimal clusters Ck. Definition 10 (well-balanced). Let f, g : N → R. We denote a partition X = ˙ ⋃K\nk=1Ck as f -balanced if for all k ∈ [K]\n|Ck| ≥ |X|\nf(K) .\nFurthermore, we denote the partition as an (f, g)-balanced CMLE solution if it is f -balanced and additionally for all k ∈ [K]\nOPT (Ck, 1) ≥ 1\ng(K) ·\nK ∑\nk=1\nOPT (Ck, 1) .\nDefinition 11. Given a finite set X ⊂ Rd and K ∈ N, we let\nOPTdiam(X,K) = min {C1,...,CK},\n∪̇Kk=1Ck=X\nmax k∈[K] max x,y∈Ck ‖x− y‖ .\nLemma 12 (From f -balanced to (f, g)-balanced). An f -balanced solution X = ˙ ⋃K\nk=1Ck is also an (f,Γ · f)-balanced CMLE solution, where Γ ≤ 2 · ln (32π · OPTdiam(X,K)) + ln(K) + 1.\nProof.\nOPT (Ck, 1) ≥ |Ck|d\n2 ≥\n1\nf(K)\n|X|d\n2 (due to Lemma 20 and f balanced)\n≥ 1\nf(K) · Γ LK(X) (due to Lem. 21)\n≥ 1\nf(K) · Γ\nK ∑\nk=1\nOPT (Ck, 1) .\n2 Main Results (Theorems 13 and 15)\nTheorem 13. Let X ⊂ Rd, K ∈ N and δ, ε ∈ [0, 1]. If X has an (f, g)-balanced optimal CMLE solution, then there exists an algorithm which computes a mixture of K spherical Gaussians θ = {(wk, µk, σ 2 k)} K k=1, such that\nPr [LX(θ) ≤ (1 + ε)OPT (X,K)] ≥ 1− δ .\nThe runtime of the algorithm is bounded by\n|X| ·K · log(Γ) · log(g(K)) · 2 Õ ( f(K) εδ )\nwhere Γ ≤ 2 · ln (32π · OPTdiam(X,K)) + ln(K) + 1.\nCorollary 14. Let X ⊂ Rd, K ∈ N and δ, ε ∈ [0, 1]. If X has an f -balanced optimal CMLE solution, then there exists an algorithm which computes a mixture of K spherical Gaussians θ, such that\nPr [LX(θ) ≤ (1 + ε)OPT (X,K)] ≥ 1− δ .\nThe runtime of the algorithm is bounded by\n|X| ·K · log(Γ)2 · 2 Õ ( f(K) εδ )\nwhere Γ ≤ 2 · ln (32π · OPTdiam(X,K)) + ln(K) + 1.\nTheorem 15. Let X ⊂ Rd, K ∈ N, and δ, ε > 0. Let C = ˙ ⋃K\nk=1Ck be a well-defined solution for the CMLE problem. There is an algorithm that computes a mixture of K spherical Gaussians θ, such that\nPr [LX(θ) ≤ (1 + ε)LX(C)] ≥ 1− δ .\nThe running time of the algorithm is bounded by\n|X| d log\n(\n1 δ\n)\n2 O ( K ε ·log ( K ε2 )) ( log(log(∆2)) + 1 )K (log(f(K)))K ,\nwhere ∆2 = maxx,y∈X{‖x− y‖ 2}.\n3 Proof of Theorem 13\nIn the following we prove Theorem 13.\n• In Section 3.1 we show that, if the parameters of a CMLE solution are sufficently close to those of an optimal CMLE solution, then its complete-data log-likelihood is close to that of the optimal CMLE solution. In Sections 3.2 and 3.3 we then show how to obtain such parameter estimates.\n• In Section 3.2 we deal with the problem of estimating the means. We use the superset sampling technique introduced by [IKI94] to compute a set of candidate means which contains a good candidate, i.e. a good estimation to the mean parameters of an optimal solution.\n• In Section 3.3 we use a grid search to obtain estimates of the weights and variances. The core idea is to simply test all solutions lying on a specific grid in the search space. By choosing a grid that is dense enough, we ensure that there are solutions on the grid which are sufficiently close to the parameters that we search for.\n3.1 Estimate the Costs of Parameter Estimates\nFor an optimal (f, g)-balanced CMLE solutions, we can estimate the parameters of the the respective optimal Gaussian mixture model and the likelihood of the optimal clusters. We can show that the CMLE solution determined by these parameter estimates yields an approximation with respect to the complete data log-likelihood.\nTheorem 16. Let X ⊂ Rd, K ∈ N and ε > 0. Assume X has an f -balanced optimal CMLE solution X = ˙ ⋃K\nk=1Ck and let (µ̃1, . . . , µ̃K) such that for all k ∈ [K]\n‖µ̃k − µ(Ck)‖ 2 ≤\nε\n|Ck|\n∑\nx∈Ck\n‖x− µ(Ck)‖ 2 .\nLet (n1, . . . , nK), such that for all k ∈ [K]\n|Ck| ≤ nk ≤ (1 + ε)|Ck| . (3)\nand ~̃σ = (σ̃21 , . . . , σ̃ 2 K) ∈ R K , such that for all k ∈ [K] it holds\nσ̃2k ≥ σ 2 k (4)\nand\nln(σ̃2k)− ln(σ 2 k) ≤\n( (1 + ε)2 − 1 ) 2\n|Ck|d OPT (Ck, 1) . (5)\nDefine θ̃ = {(w̃k, µ̃k, σ̃ 2 k)}k=1,...,K , where w̃k = nk ∑K\nl=1 nl . Then,\nLX(θ̃) ≤ (1 + ε) 4OPT (X,K).\nProof. Using that |Cl| ≤ nl ≤ (1+ε)|Cl| for all l = 1, . . . ,K, we obtain w̃k ≥ 1 (1+ε) · |Ck | |X| . Hence,\n− ln(w̃k) · |Ck| ≤ − ln\n(\n1 (1 + ε) · |Ck| |X|\n)\n|Ck| (by Equation (3))\n≤ ln(1 + ε)|Ck| − ln\n(\n|Ck| |X|\n)\n· |Ck|\n≤ ε|Ck| − ln\n(\n|Ck| |X|\n)\n· |Ck| (since ln(1 + ε) ≤ ε)\n≤ 2ε\nd OPT (Ck, 1)− ln\n(\n|Ck| |X|\n)\n· |Ck| (since OPT (Ck, 1) ≥ |Ck |·d\n2 )\nFurthermore, observe that\nLCk(µ̃k, σ̃k) = |Ck|d\n2 ln(2πσ̃2k) +\n1\n2σ̃2k\n∑\nx∈Ck\n‖x− µ̃k‖ 2\n(4) ≤ |Ck|d\n2 ln(2πσ̃2k) +\n1\n2σ2k\n∑\nx∈Ck\n‖x− µ̃k‖ 2\n≤ |Ck|d\n2 ln(2πσ̃2k) +\n1\n2σ2k (1 + ε)\n∑\nx∈Ck\n‖x− µk‖ 2 (By Lemma 6 and property of µ̃k)\n= |Ck|d\n2 ln(2πσ̃2k) + (1 + ε)\n|Ck|d\n2 (By def. of µk)\n= |Ck|d\n2 (ln(2π) + ln(σ̃2k)) + (1 + ε)\n|Ck|d\n2 (5) = |Ck|d\n2\n(\nln(2π) + ( (1 + ε)2 − 1 ) 2\n|Ck|d OPT (Ck, 1) + ln(σ\n2 k)\n)\n+ (1 + ε) |Ck|d\n2\n= |Ck|d\n2 ln(2πσ2k) + (1 + ε)\n|Ck|d\n2 +\n( (1 + ε)2 − 1 )\nOPT (Ck, 1)\n≤ (1 + ε)OPT (Ck, 1) + ( (1 + ε)2 − 1 ) OPT (Ck, 1) ≤ ( (1 + ε)2 + ε )\nOPT (Ck, 1)\n≤ (1 + ε)3OPT (Ck, 1)\nOverall, we have\nLX(θ̃) = K ∑\nk=1\nLCk(µk, σ 2 k)− ln(wk) · |Ck|\n≤ K ∑\nk=1\n(1 + ε)3OPT (Ck, 1) + 2ε\nd OPT (Ck, 1) − ln\n(\n|Ck| |X|\n)\n· |Ck|\n= K ∑\nk=1\n(\n(1 + ε)3 + 2ε\nd\n)\nOPT (Ck, 1)− ln\n(\n|Ck| |X|\n)\n· |Ck|\n≤\n(\n(1 + ε)3 + 2ε\nd\n) K ∑\nk=1\nOPT (Ck, 1)− ln\n(\n|Ck| |X|\n)\n· |Ck|\n=\n(\n(1 + ε)3 + 2ε\nd\n)\nOPT (X,K)\n≤ (1 + ε)4OPT (X,K)\n3.2 Generate Candidate Means by Sampling\nWe reuse the following well-known lemma on superset sampling.\nLemma 17 (superset-sampling). Let X ⊂ Rd be a finite set, α < 1 and X ′ ⊂ X with |X ′| ≥ α|X|. Let S ⊆ X be a uniform sample multiset of size at least 2αεδ . Then with probability at\nleast 1−δ5 there is a subset S ′ ⊆ S with |S′| = 1εδ such that\n‖µ(S′)− µ(X ′)‖2 ≤ ε\n|X ′|\n∑\nx∈X′\n‖x− µ(X ′)‖2.\nIf we plug our notion of f -balanced solutions into this lemma, then we receive an algorithm that samples good approximative means.\nTheorem 18 (sampling means). For a finite set X ⊂ Rd, K ∈ N and ε, δ > 0, if X = ˙ ⋃K\nk=1Ck\nis an f -balanced partition, then there is an algorithm that computes a set of log(1/δ)·2 K εδ\n·log (\nf(K) εδ\n)\nK-tuples of points from Rd, such that with probability 1− δ for one of these tuples it holds that for all k ∈ [K]\n‖µk − µ(Ck)‖ 2 ≤\nε\n|Ck|\n∑\nx∈Ck\n‖x− µ(Ck)‖ 2 .\nThe runtime of the algorithm is bounded by log(1/δ) ·K ·\n(\n|X|+ 2 K εδ\n·log (\nf(K) εδ\n))\n.\nProof. Consider the following algorithm, which computes a candidate set of tuples of means.\nAlgorithm 1: Approx-Means(X,K)\nInput: X ⊂ Rd : input points\nK ∈ N : number of clusters\nOutput: set of candidate tuples of means\nP ← ∅;\nfor k = 1, . . . ,K do sample a multiset S of size 1αεδ from X;\nT ← { µ(S′)|S′ ⊂ S, |S′| = ⌈ 1εδ⌉ } ; P ← P × T ;\nend\nreturn P ;\nUsing Lemma 17 with α = 1f(K) , we know that the output of a single run of Approx-Means\ncontains a tuple with the desired property with probability ( 1−δ 5 )K .\nWe know that\n|T | ≤\n(\n1\nαεδ\n) 1 εδ\n,\nthus\n|P | = |T |K ≤ 2 K εδ\n·log (\nf(K) εδ\n)\n.\nThe runtime is bounded by\nK · |X|+ K ∑\nk=1\n|T |k ≤ K\n(\n|X|+ 2 K εδ\n·log (\nf(K) εδ\n))\n.\nBy executing Approx-Means log(1/δ) times we receive the desired success probability.\n3.3 Generate Candidate Cluster Sizes and Variances by Using Grids\nSo far, we have formulated an algorithm that gives us good means. In the following, we will use the gridding technique to determine a set of candidates for the the cluster sizes and variances. First of all, we generate a set of cluster sizes that contains good approximations of the cluster sizes of any f -balanced solutions. Then, we approximate the negative log-likelihood of optimal CMLE clusters, i.e.\n∑K k=1OPT (Ck, 1) where the Ck are the optimal CMLE clusters. Then,\nwe present how to construct a candidate set of variances that contains good estimates of the variances of any (f, g)-balanced optimal CMLE solution.\n3.3.1 Grid Search for Cluster Sizes Theorem 19. Let X ⊂ Rd, K ∈ N and let X = ˙ ⋃K\nk=1Ck be an f -balanced partition. Then\nthere exists an algorithm that outputs a set S ⊆ NK , |S| = (\nlog(f(K)) log(1+ε)\n)K , that contains a tuple\n(n1, . . . , nK) ∈ S such that\n|Ck| ≤ nk ≤ (1 + ε)|Ck|. (6)\nfor all k ∈ [K].\nProof. Since we assume a f -balanced solution, we know that for all k ∈ [K]\n|X|\nf(K) ≤ |Ck| ≤ |X|.\nThus, there exist a value i∗ ∈ {1, . . . , ⌈log1+ε(f(K))⌉} such that\n(1 + ε)i ∗−1 |X|\nf(K) ≤ |Ck| ≤ (1 + ε)\ni∗ |X|\nf(K) .\nThus, we receive ⌈log1+ε(f(K))⌉ many values for each cluster size nk. The algorithm outputs all possible combinations of these values.\n3.3.2 Bounds on the Log-Likelihood of optimal CMLE clusters\nLemma 9 provides us with a lower bound on the negative log-likelihood of a cluster.\nCorollary 20 (Lower Bound on the Optimal Log-Likelihood). Let X = ˙ ⋃K\nk=1Ck be an optimal\nCMLE solution. Then, OPT (Ck, 1) ≥ |Ck |d 2 .\nThe next step is to find an upper bound on the optimal complete-data likelihood value. We use Gonzales algorithm to compute a value that gives us a tighter bound than just the maximum spread (over the dimensions of the vectors in the data set).\nLemma 21 (Upper Bound on the Optimal Complete-Data Log-Likelihood). Let X ⊂ Rd and K ∈ N. A Value Γ can be computed in time O(K ·d · |X|) such that the complete-data likelihood of an optimal CMLE solution can be bounded by\nOPT (X,K) ≤ |X|d\n2 · Γ\nand Γ = ln(2πs2) + 1 + ln(K) for some s ≤ 4 ·OPTdiam(X).\nProof. Run Gonzales algorithm. The output is a set of K points p1, . . . , pK ∈ X. Compute the point z with maximum distance to its closest point in {p1, . . . , pK} and set s := mink=1,...,K‖z− pk‖. Consider the solution where the pk are the centers. Partition the points into point sets C = {C1, . . . , CK}, with ‖x−pk‖ = mini=1,...,K‖x−pi‖ for all x ∈ Ck. Notice that the distances between any point and its center is at most s. Thus, when computing the optimal variance in each cluster, it is at most s2. Then, for θ = {(\n1 K , pk, σ(Xk, pk)\n)}K\nk=1 we have\nOPT (X,K) ≤ LX(θ, C) = K ∑\nk=1\n|Ck|d\n2 ln(2πσ(Ck, pk)\n2) + |Ck|d\n2 − ln(wk) · |Ck|\n≤\n(\nK ∑\nk=1\n|Ck|d\n2 ln(2πs2) +\n|Ck|d\n2\n)\n− ln\n(\n1\nK\n)\n· |X|\n= |X|d\n2 ln(2πs2) +\n|X|d\n2 + ln(K) · |X|\n≤ |X|d\n2\n( ln(2πs2) + 1 + ln(K) )\nGiven two bounds, we can find a constant factor approximation of the the sum of the negative log-likelihoods of optimal CMLE clusters, i.e.\n∑K k=1OPT (Ck, 1), using a grid search.\nLemma 22 (Estimating the Optimal Log-Likelihood). Let X ⊂ Rd, K ∈ N, and ε > 0. Let X = ∪̇Kk=1Ck be an optimal CMLE solution. Then, there exists a set of log(3Γ/d)/ log(1 + ε) many values which contains a value Nest with\n1\n1 + ε Nest ≤\nK ∑\nk=1\nOPT (Ck, 1) ≤ Nest .\nProof. Combining Corollary 20 and Lemma 21, we know that\n|X|d\n2 ≤\nK ∑\nk=1\nOPT (Ck, 1) ≤ OPT (X,K) ≤ |X|d\n2 Γ.\nThus, there exist a value i∗ ∈ {1, . . . , ⌈log1+ε(Γ)⌉} such that\n(1 + ε)i ∗−1 |X|d\n2 ≤\nK ∑\nk=1\nOPT (Ck, 1) ≤ (1 + ε) i∗ |X|d\n2 .\nThe algorithm outputs all ⌈log1+ε(Γ)⌉ values.\nGiven this approximation of the sum of the negative log-likelihoods, we will be able to find an approximation of the negative log-likelihoods of a single cluster as we will see in the next section.\n3.3.3 Grid Search for Variances\nGiven the approximations of the size of the clusters and their negative log-likelihod, we are now able to find estimates of the variances.\nTheorem 23. Let X ⊂ Rd, K ∈ N and ε > 0. Assume X has an (f, g)-balanced CMLE solution X = ˙ ⋃K\nk=1Ck. Let additionally Nest ∈ R, with\n1\n1 + ε Nest ≤\nK ∑\nk=1\nOPT (Ck, 1) ≤ Nest, (7)\nand (n1, . . . , nK), such that for all k ∈ [K]\n|Ck| ≤ nk ≤ (1 + ε)|Ck|. (8)\nThen there exists an algorithm that computes a set of size K · log(g(K))log(1+ε) , that contains a tuple (σ̃21 , . . . , σ̃ 2 K), such that for all k ∈ [K] it holds\nσ̃2k ≥ σ 2 k (9)\nand\nln(σ̃2k)− ln(σ 2 k) ≤\n( (1 + ε)2 − 1 ) 2\n|Ck|d OPT (Ck, 1) . (10)\nProof. Observe that\n1\ng(K)(1 + ε) Nest ≤\n1\ng(K)\nK ∑\nk=1\nOPT (Ck, 1) Def. 10 ≤ OPT (Ck, 1) ≤\nK ∑\nk=1\nOPT (Ck, 1) ≤ Nest.\nThus, there exists a value j∗ ∈ { ⌈− log1+ε(g(K))⌉, . . . , 0 } which satisfies\n(1 + ε)j ∗−1Nest ≤ OPT (Ck, 1) ≤ (1 + ε) j∗Nest .\nDenote the upper bound by N̂ := (1 + ε)j ∗ Nest and set σ̃ 2 k := exp\n(\n2(1+ε) nkd\nN̂ − ln(2π)− 1 ) .\nNotice that\nOPT (Ck, 1) = LCk(µk, σ 2 k) =\n|Ck|d\n2\n( ln(2πσ2k + 1) )\n⇔ ln(σ2k) = 2\n|Ck|d OPT (Ck, 1) − ln(2π) − 1\nThus,\nln(σ̃2k) = 2(1 + ε)\nnkd N̂ − ln(2π) − 1 ≥\n2\n|Ck|d OPT (Ck, 1) − ln(2π) − 1 = ln(σ\n2 k)\nand\nln(σ̃2k)− ln(σ 2 k) =\n2(1 + ε)\nnkd N̂ −\n2\n|Ck|d OPT (Ck, 1)\n≤ 2(1 + ε)2\n|Ck|d OPT (Ck, 1) −\n2\n|Ck|d OPT (Ck, 1)\n= ( (1 + ε)2 − 1 ) 2\n|Ck|d OPT (Ck, 1)\n4 Proof of Theorem 15\nIn the following we present the proof of Theorem 15.\n• In Section 4.1 we show how to estimate the variances and the cluster sizes of a well-defined CMLE solution via gridding. The idea behind a grid search is simply to test all solutions lying on a grid in the search space. By choosing a grid that is dense enough, we ensure that there are solutions on the grid which are sufficiently close to the parameters that we search for.\n• In Section 4.2, we show how one can find good estimates of the means when given good estimates of the weights and covariances. To this end, we adapt the sample-and-prune technique presented in [ABS10].\n4.1 Generate Candidates for Variances and Weights\nLemma 24. Let X ⊂ Rd, and {Ck} K k=1 be a well-defined CMLE solution for X, with corresponding variances {σ2k} K k=1. Then, there exists an algorithm which outputs a set of at most (\nlog(log(∆2))+1 log(1+ε) )K tuples of variances, which contains a tuple (σ̃2k) K k=1, such that\n∀k ∈ [K] : σ2k ≤ σ̃ 2 k ≤ (σ 2 k) (1+ε) ,\nwhere ∆2 = maxx,y∈X{‖x− y‖ 2}.\nProof. We know that optimal variances σ2k of a well-defined solution are bounded from below by\n∀k ∈ [K] : 1\n2π ≤ σ2k.\nFurthermore, we know that these are also bounded from above by\n∀k ∈ [K] : σ2k = 1\n|Ck|d\n∑\nx∈Ck\n‖x− µ(Ck)‖ 2 ≤\n1\n|Ck|d\n∑\nx∈Ck\n∆2 ≤ ∆2 .\nBecause 1/(2π) ≤ σ2k ≤ ∆ 2, there exists a value\nk∗ ∈ {1, . . . , log1+ε(− log1/(2π)(∆ 2))}\nsuch that (1/(2π))(1+ε) k∗−1\n≤ σ2i ≤ (1/(2π)) (1+ε)k\n∗\n.\nThus, we receive ⌈ log(log(∆2))−log(log(2π)) log(1+ε) ⌉ many values for each variance. The algorithm outputs all possible combinations of these values.\nThe following result is the same as in Section 3.3.\nTheorem 25. Let X ⊂ Rd, K ∈ N and let C = ˙ ⋃K\nk=1Ck be an f -balanced partition. Then there\nexists an algorithm that outputs a set S ⊆ NK , |S| = (\nlog(f(K)) log(1+ε) )K , that contains {n1, . . . , nK} ⊂\nS such that\n|Ck| ≤ nk ≤ (1 + ε)|Ck|. (11)\nfor all k ∈ [K].\nAlgorithm 2: Approx-Means(R, l,Mk−l,Σ)\nInput: R ⊂ X ⊂ Rd : set of remaining input points\nl ∈ N : number of means yet to be found\n~µ = (µ1, . . . , µj) : tuple of j ≤ k − l candidate means (σ̃21 , . . . , σ̃ 2 k) : vector of k variances (w̃21, . . . , w̃ 2 k) : vector of k weights Notation: ~S : vector containing the elements of set S in arbitrary order\n~x ◦ ~y : concatenation of vectors, i.e. for ~x = (x1, . . . , xn) and ~y = (y1, . . . , ym), ~x ◦ ~y = (x1, . . . , xn, y1, . . . , ym) Output: θ = {(wi, µi, σi)} containing at most k tuples of mean and variance if l = 0 then\nreturn ~P ;\nelse\nif l ≥ |R| then\nreturn θ = {(µi, σi)}i where ~µ ◦ ~R = (µi)i;\nelse\n/* sampling phase */ ; sample a multiset S of size 1αεδ from R; T ← {\nµ(S′)|S′ ⊂ S, |S′| = 1εδ } ;\nMk ← ∅; for t ∈ T do Mk ← Mk ∪Approx-Means(R, l − 1, {~µ ◦ (t)|~µ ∈ Mk−l},Σ); end\n/* pruning phase */ ; N ← set of |R|2 points x from R with smallest minimum negative complete-data log-likelihood cost wrt. the weighted component given by (w̃i, µi, σ̃ 2 i ) for i ∈ [j], i.e.\nmin i∈[j]\n{\nd 2 ln(2πσ̃2i ) + 1\n2σ̃2i ‖x− µi‖\n2 − ln(w̃i)\n}\nMk ← Mk ∪Approx-Means(R \\N, l,Mk−l,Σ); return the candidate θ = {(wi, µi, σi)}i, (µi) ∈ Mk, which has minimal cost LX(θ) ;\nend\nend\n4.2 Applying the ABS Algorithm\nIn the following we analyze Algorithm 2. We show that the algorithm can be used to construct means such that, together with appropriate approximations of the weights and variances, we obtain a CMLE solution with costs close to the costs of the given CMLE solution.\nTheorem 26. Let σ̃i ∈ [σ 2 i , (σ 2 i ) (1+ε)] and w̃k ≥ 1 (1+ε)wk for i ∈ [k]. Algorithm 2 started with (X, k, ∅, (σ̃21 , . . . , σ̃ 2 k)) computes a tuple (µ̃1, . . . , µ̃k) such that with probability at least ( 1−δ 5 )k\nLX((w̃i, µ̃i, σ̃ 2 i )i∈[k]) ≤ (1 + ε)L(X) .\nThe running time of the algorithm is bounded by |X| d 2O(k/ε·log(k/ε 2)).\nLet ˙ ⋃k\ni=1Ci be a partition of X into optimal CMLE clusters. We introduce\nC[i,j] = ˙⋃j\nt=i Ct\nas a short notation for the disjoint union of clusters i through j. We assume that the Ci are numbered by the order their approximate means µ̃i are found by the superset-sampling technique.\nNow, let X = R0 ⊇ Ri ⊇ · · · ⊇ Rk−1 be a sequence of input sets computed by the algorithm, such that\n|Ci ∩Ri−1| ≥ α|Ri−1|.\nWithout loss of generality assume that each Ri is the largest of these sets with this property. By using Lemma 17, we obtain the following Lemma.\nLemma 27 (By Superset-Sampling). With probability at least ((1 − δ)/5)k we have\n‖µ̃i − µ(Ci ∩Ri−1)‖ 2 ≤\nε\n|Ci ∩Ri−1|\n∑\nx∈Ci∩Ri−1\n‖x− µ(Ci ∩Ri−1)‖ 2\nfor all i ∈ [K].\nBy Ni := Ri−1 \\Ri we denote the set of points remove between two sampling phases. Using these definitions we can see that\n˙⋃k\ni=1 (Ci ∩Ri−1) ∪̇\n˙⋃k\ni=1\n( C[i+1,k] ∩Ni )\nis a disjoint partition of X. Each set Ci∩Ri−1 on the left side contains the points that the mean µ̃i has been sampled from. The sets C[i+1,k] ∩ Ni on the right side contain points incorrectly assigned to {µ̃1, . . . , µ̃i} during the pruning phases between the sampling of µ̃i and µ̃i+1.\nDenote by θi the parameters of the first i weighted Gaussians obtained by the algorithm, i.e.\nθ̃i = ((w̃1, µ̃1, σ̃1), . . . , (w̃i, µ̃i, σ̃i)) .\nLemma 28 (cf. Claim 4.8 in [Ack09]).\nLC[i+1,k]∩Ni(θ̃i) ≤ 8αkLC[1,i]∩Ri−1(θ̃i)\nProof. As in [Ack09, p. 70ff], with “cost“ replaced by ”L“.\nDenote by cost(P,C) the k-means cost of a point set P wrt. a set of means C.\nLemma 29 (cf. Claim 4.9 in [Ack09]). For every i ∈ [k] we have\ncost(Ci ∩Ri−1, µ̃i) ≤ (1 + ε) cost(Ci, µi) .\nProof. As in [Ack09, p. 70ff], using that optimal means in CMLE are means of the optimal CMLE clusters.\nGiven appropriate approximate variances, we can conclude that a similar bound holds wrt. the complete-data log-likelihood.\nLemma 30. Given σ̃i ∈ [σ 2 i , (σ 2 i ) (1+ε)] and w̃i = ni |X| with ni ∈ [|Ci|, (1 + ε)|Ci|], we have\nLCi∩Ri−1(w̃i, µ̃i, σ̃ 2 i ) ≤ (1 + ε)LCi(wi, µi, σ 2 i ) .\nProof.\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) =\n|Ci ∩Ri−1|d\n2 ln(2πσ̃2i ) +\n1\n2σ̃2i cost(Ci ∩Ri−1, µ̃i)− |Ci ∩Ri−1| ln(w̃i) .\nWe have\nln(2πσ̃2i ) ≤ ln(2π(σ 2 i ) (1+ε)) = (1 + ε) ln(2πσ2i ) .\nFurthermore, Using that |Cl| ≤ nl ≤ (1+ε)|Cl| for all l = 1, . . . ,K, we obtain w̃k ≥ |Ck| |X| . Hence,\n− ln(w̃i) · |Ci ∩Ri−1| ≤ − ln(w̃i) · |Ci|\n≤ − ln\n(\n|Ci| |X|\n)\n|Ci| (by Equation (3))\n= − ln (wi) · |Ci|\nBy Lemma 29 and σ̃2i ≥ σ 2 i ,\n1\n2σ̃2i cost(Ci ∩Ri−1, µ̃i) ≤ (1 + ε)\n1\n2σ2i cost(Ci, µi) .\nFrom this and by using that σ2i = 1 |Ci|d cost(Ci, µi), we conclude\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) ≤ (1 + ε)\n|Ci|d\n2 ln(2πσ2i ) + (1 + ε)\n1\n2σ2i cost(Ci, µi)− ln(wi)|Ci|\n≤ (1 + 2ε)N1(Ci)− ln(wi)|Ci| ≤ (1 + 2ε)LCi(µi, σ 2 i ) .\nRunning Algorithm 2 with ε/3 instead of ε yields the claim.\nAnalogously to [Ack09], we can prove Theorem 26 as follows.\nProof of Theorem 26. Let θ̃k = (µ̃i, σ̃ 2 i )i∈[k]. Then,\nLX(θ̃k) ≤ k ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) +\nk−1 ∑\ni=1\nLC[i+1,k]∩Ni(θ̃k)\n≤ k ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) + 8αk\nk−1 ∑\ni=1\nLC[1,i]∩Ri−1(θ̃k) (due to Lemma 28)\n≤ k ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) + 8αk\nk−1 ∑\ni=1\ni ∑\nt=1\nLCt∩Ri−1(µ̃t, σ̃ 2 t ) .\nSince Ri ⊆ Ri−1, we have Ct ∩Ri−1 ⊆ Ct ∩Rt−1. Hence,\nk−1 ∑\ni=1\ni ∑\nt=1\nLCt∩Ri−1(µ̃t, σ̃ 2 t ) ≤\nk−1 ∑\ni=1\ni ∑\nt=1\nLCt∩Rt−1(µ̃t, σ̃ 2 t )\n≤ k k−1 ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) .\nThus,\nLX(θ̃k) ≤ k ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i ) + 8αk\n2 k−1 ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i )\n≤ (1 + 8αk2) k ∑\ni=1\nLCi∩Ri−1(µ̃i, σ̃ 2 i )\n≤ (1 + 8αk2)(1 + ε)L(X) . (by Lemma 30)\nFinally, running the algorithm for ε := ε/2 and α = θ(ε/k2) yields the theorem.\n5 Special Cases\n5.1 Weighted K-Means (Identical Covariances)\nIn this section we consider a restricted version of the CMLE problem where we are only interested in Gaussian mixture models where all components share the same fixed spherical covariance matrix, i.e. parameters θ = {(wk, µk,Σk)}k∈[K] where Σk = 1 2β Id for all k ∈ [K]. We call this problem the Weighted K-Means (WKM) problem.\nProblem 31 (WKM). Given a finite set X ⊂ Rd and an integer K ∈ N, find a partition C = {C1, . . . , CK} of X into K disjoint subsets and K weighted means θ = {(wk, µk)} K k=1, where µk ∈ R D, wk ∈ R, and ∑K k=1wk = 1, minimizing\nLwmX (θ, C) = K ∑\nk=1\nβ\n\n\n∑\nx∈Ck\n‖x− µk‖ 2\n\n− ln(wk) · |Ck| .\nWe denote the minimal value by OPTwm(X,K).\nCorollary 32. Let X ⊂ Rd, K ∈ N, and δ, ε > 0. Let X = ˙ ⋃K\nk=1Ck be a well-defined solution for the WKM problem. There is an algorithm that computes K weighted means θ = {(w̃k, µ̃k)} K k=1 such that with probability at least 1− δ\nLwmX ((w̃i, µ̃i)i∈[K]) ≤ (1 + ε)OPTwm(X,K) .\nThe running time of the algorithm is bounded by\n|X| d 2O(K/ε·log(K/ε 2)) · (log(f(K)))K .\nProof. Use a grid search to obtain candidates for the weights, then apply the ABS algorithm.\n5.2 Uniform Weights\nIn this section we consider a restricted version of the CMLE problem where we are only interested in Gaussian mixture models with fixed uniform weights, i.e. parameters θ = {(wk, µk,Σk)}k∈[K] where wk = 1/K for all k ∈ [K]. We denote this problem by Uniform Complete-Data Maximum Likelihood Estimation (UCMLE).\nProblem 33 (UCMLE). Given a finite set X ⊂ Rd and an integer K ∈ N, find a partition C = {C1, . . . , CK} of X into K disjoint subsets and K spherical Gaussians with parameters θ = {(µk, σ 2 k)} K k=1 minimizing\nLunifX (θ, C) = K ∑\nk=1\nLCk(µk, σ 2 k)\n= K ∑\nk=1\n|Ck|d\n2 ln(2πσ2k) +\n1\n2σ2k\n\n\n∑\nx∈Ck\n‖x− µk‖ 2\n\n .\nWe denote the minimal value by OPTunif (X,K).\nCorollary 34. Let X ⊂ Rd, K ∈ N, and δ, ε > 0. Let X = ˙ ⋃K\nk=1Ck be a well-defined solution for the UCMLE problem. There is an algorithm that computes K spherical Gaussians θ = {(µ̃k, σ̃ 2 k)} K k=1 such that with probability at least 1− δ\nLunifX ((µ̃i, σ̃ 2 i )i∈[K]) ≤ (1 + ε)OPTunif (X,K) .\nThe running time of the algorithm is bounded by\n|X| d log(1/δ) 2O(K/ε·log(K/ε 2)) ( log(log(∆2)) + 1 )K ,\nwhere ∆2 = maxx,y∈X{‖x− y‖ 2}.\nProof. Use a grid search to obtain candidates for the variances, then apply the ABS algorithm.\nReferences\n[ABS10] Marcel R. Ackermann, Johannes Blömer, and Christian Sohler. Clustering for metric and nonmetric distance measures. ACM Trans. Algorithms, 6(4):59:1–59:26, September 2010.\n[Ack09] Marcel R. Ackermann. Algorithms for the Bregman k-Median Problem. PhD thesis, University of Paderborn, 2009.\n[CG92] Celeux and Govaert. A Classification EM Algorithm for Clustering and Two Stochastic Versions. Comput. Stat. Data Anal., 14(3), 1992.\n[IKI94] M. Inaba, N. Katoh, and H. Imai. Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering. In Proceedings of the Tenth Annual Symposium on Computational Geometry, SoCG ’94, pages 332–339, New York, NY, USA, 1994. ACM."
    } ],
    "references" : [ {
      "title" : "Clustering for metric and nonmetric distance measures",
      "author" : [ "Marcel R. Ackermann", "Johannes Blömer", "Christian Sohler" ],
      "venue" : "ACM Trans. Algorithms,",
      "citeRegEx" : "Ackermann et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ackermann et al\\.",
      "year" : 2010
    }, {
      "title" : "Algorithms for the Bregman k-Median Problem",
      "author" : [ "Marcel R. Ackermann" ],
      "venue" : "PhD thesis, University of Paderborn,",
      "citeRegEx" : "Ackermann.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ackermann.",
      "year" : 2009
    }, {
      "title" : "A Classification EM Algorithm for Clustering and Two Stochastic Versions",
      "author" : [ "Celeux", "Govaert" ],
      "venue" : "Comput. Stat. Data Anal.,",
      "citeRegEx" : "Celeux and Govaert.,? \\Q1992\\E",
      "shortCiteRegEx" : "Celeux and Govaert.",
      "year" : 1992
    }, {
      "title" : "Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering",
      "author" : [ "M. Inaba", "N. Katoh", "H. Imai" ],
      "venue" : "In Proceedings of the Tenth Annual Symposium on Computational Geometry,",
      "citeRegEx" : "Inaba et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Inaba et al\\.",
      "year" : 1994
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Training the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions. A popular hard-clustering variant of the MLE problem is the so-called completedata maximum likelihood estimation (CMLE) method. The standard approach to solve the CMLE problem is the Classification-Expectation-Maximization (CEM) algorithm [CG92]. Unfortunately, it is only guaranteed that the algorithm converges to some (possibly arbitrarily poor) stationary point of the objective function. In this paper, we present two algorithms for a restricted version of the CMLE problem. That is, our algorithms approximate reasonable solutions to the CMLE problem which satisfy certain natural properties. Moreover, they compute solutions whose cost (i.e. complete-data log-likelihood values) are at most a factor (1 + ε) worse than the cost of the solutions that we search for. Note the CMLE problem in its most general, i.e. unrestricted, form is not well defined and allows for trivial optimal solutions that can be thought of as degenerated solutions.",
    "creator" : "LaTeX with hyperref package"
  }
}