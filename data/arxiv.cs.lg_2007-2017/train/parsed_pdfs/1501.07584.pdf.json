{
  "name" : "1501.07584.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Qi Guo", "Bo-Wei Chen", "Feng Jiang", "Xiangyang Ji", "Sun-Yuan Kung" ],
    "emails" : [ "dennisbwc@gmail.com)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n07 58\n4v 1\n[ cs\n.L G\n] 2\n9 Ja\nn 20\nIndex Terms— Feature space decomposition, feature space division, fusion, divide-and-conquer, classification"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models. Furthermore, RBFs can map the input features into the intrinsic space [3] that is spanned by infinite-dimensional vectors. This correspondingly increases the opportunity of creating a discriminant hyperplane in the empirical space [3], subsequently enhancing discriminability. However, when input dimensions are sufficiently large, calculation of a kernel matrix becomes a burden. Moreover, RBFs may lead to overfitting due to infinite dimensions. To deal with such problems, rather than using conventional RBFs, Wu et al. [4] proposed using Truncated Radical Basis Functions (TRBFs) to avoid generating\ninfinite dimensions in the intrinsic space. Furthermore, they also devised an intrinsic data matrix, which was derived from a finite-decomposable kernel, to replace calculation of kernel matrices in the empirical space. Therefore, the time complexity was saved from original O(N3) to min(N3, J2N + J3) for KRR, where N is the number of instances, and J is the number of feature dimension expanded by TRBFs. Moreover, avoiding direct calculation of kernel matrices effectively resolved the need for matrix expansion.\nThe success of TRBF-based method relies on dimensional reduction in the intrinsic space and the conversion from empirical space to intrinsic space. Although computational load is relieved without losing too much accuracy, however, that method [4] did not improve discriminability and separability between features. Furthermore, the algorithmic architecture of that method did not support distributed processing, especially when mainstream toolboxes like Apach Hadoop (hadoop.apache.org) and Spark (spark.apache.org) adopt divide-and-conquer strategy in their implementation. Proposing a new architecture that supports divide-and-conquer computation correspondingly becomes necessary.\nIn response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far. Zhang et al. used divide-and-conquer KRR [5] to support computation of large-scale data. Firstly, their method randomly partitioned a dataset into subsets of equal size. Local solutions were subsequently computed by using KRR based on each subset. By averaging the local solutions, a global predictor was therefore obtained. Instead of using randomized data selection as Zhang et al. did, Hsieh et al. [6] focused on systematic data division before applying divideand-conquer classifiers to the data. In their approach, kernel K-means clustering was performed to select the representatives of the entire input data. Next, the members of a subset were selected based on one representative. Their experimental result showed a favorable accuracy when systematic data division was used.\nAlthough the above-mentioned approaches realized divideand-conquer concept in their algorithms, overfitting of kernel space was not fully addressed and resolved. To deal with the\naforementioned problems, this study proposes 1) A novel approach for feature-space decomposition, where the original feature space is converted to subspaces. Besides, the bases of each subspace are reranked according to their importance.\n2) A divide-and-conquer structure that allows independent local classifiers to create discriminant hyperplanes based subspaces rather than the entire empirical space. This lowers computational complexity while avoiding overfitting problems.\nThe rest of this paper is organized as follows. Section 2 introduces the overview of the proposed method. Section 3 then describes details of the proposed feature-space decomposition and fusion method. Next, Section 4 summarizes the performance of the proposed method and the analysis results. Conclusions are finally drawn in Section 5."
    }, {
      "heading" : "2. SYSTEM OVERVIEW",
      "text" : "Given an M×N data matrix X with N instances and M features and a 1×N label vector y, denote the feature space as Ω, and X are the projection of the N instances on Ω. We first define the feature-space decomposition method D = {T, I}, where T is a feature-space transform function, and I is a set of feature index groups.\nThe decomposition method D contains five sub-methods which are discussed in Section 3.1, namely, Random Decomposition (RD), Principle Component Analysis (PCA), Discriminant Component Analysis (DCA), Block Cholesky Decomposition (BCD) and Approximate Block Decomposition (ABD). Furthermore, each have an M×M sub-transform matrix, denoted as WRD, WPCA, WDCA, WBCD and WABD. Also, each contains a subset of feature index groups, e.g., IRD = {IRDi |I RD i ⊂ {1, 2, · · · ,M}, i = 1, 2, · · · , h\nRD}, where hRD is the number of feature subspaces decomposed by RD sub-method, respectively. As for T , we have\nΩ∗ = T (Ω), X∗ = T (X) = WX =\n\n  \nWRD\nWPCA WDCA WBCD WABD\n\n   X (1)\nwhere W and Ω∗ are respectively the transform matrix and the new feature space. As for I , we have I = {IRD, IPCA, · · · , IABD} , and the total number of subspaces is h = hRD+ hPCA+ · · ·+hABD. Not all the sub-methods need to be used in real practice. If some are not applied, the corresponding WMethod and IMethod can just be empty.\nThe original feature space Ω is first transformed to Ω∗ by T and then decomposed into subspaces Ω∗1,Ω ∗ 2, ...,Ω ∗\nh by I; all the instances are first projected X∗ and subsequently decomposed into X∗1 , X ∗ 2 , ..., X ∗\nh.Then, a local classifier fi(i = 1, 2, · · · , h), e.g., SVMs, KRRs, etc. is trained using data matrix X∗i . Let row vectors Ri = fi(X ∗ i ) denote the results of\nfi on X∗i and R =\n\n \nR1 R2\n... Rh\n\n  . The elements of Ri can be\ndiscrete labels or continuous prediction values. The system generates the output based on R using fusion methods which are discussed in Section 3.2."
    }, {
      "heading" : "3. PROPOSED DIVISION AND FUSION METHODS",
      "text" : ""
    }, {
      "heading" : "3.1. Feature-Space Decomposition",
      "text" : "Section 2 shows that the merit of the proposed method is, it can perform classification within the subspaces and ignores the dependance among subspaces. Theoretically, decomposition method should be able to reduce the dependance as much as possible between any two feature subspaces while remaining dependance within the subspaces. This is the reason for conducting transformation on the feature space before division.\nAmong all the sub-methods in this study, the simplest idea is RD which directly decompose the feature space based on I . Its WRD is an M ×M identity matrix.\nAs for PCA, we conduct PCA on the data matrix X and split up the features according to I . Since PCA diagonizes the feature covariance matrix S, this method eliminates the relevance of different features among and within subspaces. If the data obey Gaussian distribution, the PCA also eliminate the dependance of features among and within feature subspaces.\nDCA also conducts orthogonal transformation like PCA, while its discriminant matrix is [Sw + ρI]−1S , where Sw is the within-class scatter matrix, and ρ is the ridge parameter [3]. We have\nSw = Σ L l=1Σ Nl j=1[x (l) j − µ (l)][x (l) j − µ (l)]T (2)\nwhere l is the number of classes, Nl represents the number of samples in class l, and µ(l) specifies the average point of class. We conduct generalized eigenvector decomposition [7] to obtain the eigenvectors ν1, ν2, ..., νMand eigenvalue matrix λ1, λ2, ..., λM , such that\nSνk = λk[Sw + ρI]νk, k = 1, 2, ...,M (3)\nand the transform matrix is defined as WDCA = [ν1, ν2, · · · , νM ]. Computing S and Sw both enjoys O(M2N) complexity. As [Sw + ρI] can hardly be singular, the complexity of generalized eigenvalue decomposition equals that of λk[Sw + ρI]\n−1Sνk = λkνk, k = 1, 2, ...,M , which is of O(M3) time complexity. Therefore, the total complexity of DCA is O(2M2N +M3).\nBCD exploits a blocked Doolittle Algorithm, which is a form of Gaussian transformation rather than the orthogonal transformation, to eliminate the relevance among subspaces while remaining relevance within subspaces. For a symmetrical block matrix A , we eliminate the first row and column of\nblocks, as shown in Equation 4.\n\n  \nA′ 11 O12 · · · OTk1 O21 A′22 · · · A ′ k2 T\n... ...\n. . . ...\nOk1 A ′ k2 · · · A′ kk\n\n   = B1\n\n A11 · · · ATk1 ... . . . ...\nAk1 · · · Akk\n\n (B1)T\n(4)\nwhere B1 =\n\n  \nI11 O12 · · · O1k −A21A −1\n11 I22 · · · O2k\n... ...\n. . . ...\n−Ak1A −1 11 Ok2 · · · Ikk\n\n   , and the divi-\nsion of blocks remains the same. The subscript of B indicates the row and column it eliminates. Iteratively, we subsequently generate B2,· · · ,Bk to sequentially eliminate the rest rows and columns of blocks. The main goal of BCD is sequentially block-diagonizing the discriminant matrix based on the blocked Doolittle Algorithm. As shown in Algorithm 1, X is firstly rearranged to generate X̃ according to IBCD, in which MatrixSplit(X, IBCD) splits X into X̃1, X̃2, · · · , X̃hBCD according to IBCD. The discriminant matrix of BCD is S. Function BlockedDoolittle(X̃, I) generates Bi(i = 1, 2, · · · , h\nBCD) based on the idea of Equation 4 to eliminate the ith row and column of S. The BCD transform matrix is WBCD = BhBCDBhBCD−1 · · ·B1. Comparing to BCD, PCA needs to do an M × M matrix inversion, whose complexity is O(M3) on non-sparse matrix, whereas BCD only uses an M\nm × M m matrix for m(m+1)2 times if divided\nequally, which only costs 1 m of the time of PCA.\nAlgorithm 1 [X∗,WBCD] = BCD(X, I) {X̃1, X̃2, ..., X̃n} =MatrixSplit(X, I) X̃ = [X̃1, X̃2, ..., X̃m]\nS = X̃X̃T =\n\n   S11 · · · S1m ... . . . ...\nSm1 · · · Smm\n\n   , where Sij = X̃iX̃j T\nfor i = 1 to hBCD do Bi =BlockedDoolittle(S, i) S = BiS(Bi)T end for WBCD = BhBCDBhBCD−1 · · ·B1 X∗ = WBCDX̃\nBesides the aforementioned orthogonal transformation of PCA and DCA, as well as the Gaussian transformation of BCD, we also propose an approximate orthogonal transformation on which the ABD method is based. First we define a new operator ⊗ as Definition 1.\nDefinition 1 For two blocked matrix A = {Aij} and B = {Bij} with the same size and division of blocks, define operator ⊗, s.t.\nA⊗B =\n\n x11 · · · x1m ... . . . ...\nxm1 · · · xmm\n\n (5)\nTable 1. Time complexity of different transformation methods.\nComplexity Detail RD O(N) Unsupervised, identity transform PCA O(M2N +M3) Unsupervised, orthogonal transform (OT) DCA O(2M2N +M3) Supervised, OT BCD O(M2N +M3/m) Unsupervised, Gaussian transform ABD O(MmN +m3) Unsupervised, approximate OT\nwhere xij equals the sum of all the elements of Aij elementwise multiply Bij .\nWe rewrite X as\n\n X11 · · · X1N ... . . . ...\nXhABD1 · · · XhABDN\n\n where\nwe divide each instance into m vectors according to I . The discriminant matrix is X⊗X using this division. By conducting eigenvector decomposition on the discriminant matrix, we have\nX ⊗X = V TΛV (6)\nwhere V = {vij}, and each column of V is an eigenvector. The transform matrix is\nWABD =\n\n v11I11 · · · v1N I1N ... . . . ...\nvm1Im1 · · · vmN ImN\n\n . (7)\nIf there are approximately equal number of features in each subset, computing X ⊗X yields O(MmN) complexity and the eigenvalue decomposition costs O(m3). Therefore, the total complexity of ABD is O(MmN +m3).\nTable 1 shows the time complexity and details of the aforementioned sub-methods. By combining the five methods together, D includes both supervision (i.e., DCA) and unsupervision (i.e., RD,PCA,BCD and ABD) in transformation as well as four transformation methods."
    }, {
      "heading" : "3.2. Feature Subspace Fusion",
      "text" : "After obtaining the classification result matrix R from local classifier, we weight the outcome of each subspace by training a global classifier fn+1 by using R as a data matrix and y as labels. The output of fn+1 is the final prediction result. Observations show that m < 50 << N and TRBFKRR[4] generates favorable results for fn+1. As the training complexity of TRBFKRR is min(N3, J2N + J3),\nwhere J =\n(\nm+ p p\n)\n, and p is TRBF order. It is efficient\nto train on data matrix with a large number of instances and a small number of features like R."
    }, {
      "heading" : "4. EXPERIMENTAL RESULT",
      "text" : "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,· · · ,fh in our system respectively and\ntested the results on large scale datasets (i.e., either M or N is larger than 104). Our methods are notated as “DCclassifier1-classifier2”, where “classifier1” indicates the classifier used for f1,f1,· · · ,fm, and “classifier2” is for fn+1. All the experiments are conducted on an Intel Core i7 2.1GHz CPU and 8G RAM machine. The datasets tested in this paper are shown in Table 3 and can be downloaded from http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/ or UCI Machine Learning Repository.\nFeature-Space Decomposition Setting: Table 2 shows the decomposition setting in our experiments. For data matrices with high feature dimensions, e.g., news20, RCV1, we just use RD and ABD with relatively low computational complexity. For data matrices with low feature dimensions, all the transformation methods can be combined together to achieve a lower error rate. We use LibLinear as the global classifier when dealing with census dataset, as its proportion of positive and negative instances are 0.06/0.94, which can cause bias when TRBFKRR is applied.\nLinear Classification: Linear classification is conducted on news20 and RCV1 datasets. LibLinear is exploited as local classifiers f1,f2,· · · ,fm, and TRBFKRR is used as global classifier fn+1 in our system. We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11]. As shown in Table 4, our methods either have advantages on training times or error rates. .\nNon-Linear Classification: DCSVM is set as local classifiers for nonlinear classification. Interestingly, in DCDCSVM-TRBFKRR, divide-and-conquer process is conducted on both instance dimension and feature dimension in the method. We evaluate it on covtype dataset and compare with the results of the other SVM methods by Hsieh et al. [6], as is shown in Table 5. The proposed method achieve the lowest error rate with relatively low time complexity in both covtype and census datasets.\nMoreover, comparing to directly training a TRBFKRR classifier using the whole data matrix, DC-TRBFKRRTRBFKRR greatly reduces the training complexity from min(N3, J2N+J3) to min(N3, J\n2N m + J 3 m2 ), which enables\nTRBFKRR training on data matrix with large N and M ."
    }, {
      "heading" : "5. CONCLUSION",
      "text" : "This paper presents a feature-space decomposition classification method including five sub-methods. The experimental results show that our divide-and-conquer classification scheme can reduce error rates (e.g., reduce error rates by 10.53% and 7.53% in covtype and RCV1 datasets), comparing to training directly using the whole datasets, and outperform stateof-the-art fast SVM solver by reducing overfitting problem. The future work will focus on providing theoretical analysis for feature-space decomposition and its effects on divide-andconquer classification.\n1Results are cited from Keerthi et al. [12] 2Results are cited from Hsieh et al. [6]"
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] V. N. Vapnik and V. Vapnik, Statistical Learning Theory, vol. 2, New York, NY: Wiley, 1998.\n[2] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-based Learning Methods, Cambridge: Cambridge University Press, 2000.\n[3] S.-Y. Kung, Kernel Methods and Machine Learning, Cambridge: Cambridge University Press, 2014.\n[4] S.-Y. Kung and P.-Y. Wu, “On efficient learning and classification kernel methods,” in Proc. 2012 IEEE Int. Conf. Acoustics, Speech and Signal Processing, Kyoto, Japan, Mar. 25-30, 2012, pp. 2065–2068.\n[5] Y. Zhang, J. Duchi, and M. Wainwright, “Divide and conquer kernel ridge regression,” in Proc. 2013 Conf. on Learning Theory, Princeton, NJ, USA, Jun. 12-14, 2013, pp. 592–617.\n[6] C.-J. Hsieh, S. Si, and I. S. Dhillon, “A divide-andconquer solver for kernel support vector machines,” in Proc. 31st Int. Conf. Machine Learning, Beijing, China, 2014, pp. 566–574.\n[7] C. B. Moler and G. W. Stewart, “An algorithm for generalized matrix eigenvalue problems,” SIAM Journal on Numerical Analysis, vol. 10, no. 2, pp. 241–256, 1973.\n[8] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “Liblinear: A library for large linear classification,” J. Mach. Learn. Res., vol. 9, pp. 1871–1874, June 2008.\n[9] T. Joachims, “Making large-scale SVM learning practical,” in Advances in Kernel Methods – Support Vector Learning, B. Schölkopf, C. Burges, and A. Smola, Eds. Cambridge, MA: MIT Press, 1999.\n[10] W. Kao, K. Chung, C. Sun, and C. Lin, “Decomposition methods for linear support vector machines,” Neural Computation, vol. 16, no. 8, pp. 1689–1704, Aug 2004.\n[11] V. Sindhwani and S. S. Keerthi, “Large scale semisupervised linear svms,” in Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, New York, NY, USA, 2006, pp. 477–484.\n[12] S. S. Keerthi and D. DeCoste, “A modified finite newton method for fast solution of large scale linear svms,” Journal of Machine Learning Research, vol. 6, no. 3, pp. 341–361, 2005."
    } ],
    "references" : [ {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik", "V. Vapnik" ],
      "venue" : "vol. 2, New York, NY: Wiley",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : "Cambridge: Cambridge University Press",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Kernel Methods and Machine Learning",
      "author" : [ "S.-Y. Kung" ],
      "venue" : "Cambridge: Cambridge University Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On efficient learning and classification kernel methods,",
      "author" : [ "S.-Y. Kung", "P.-Y. Wu" ],
      "venue" : "IEEE Int. Conf. Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "and M",
      "author" : [ "Y. Zhang", "J. Duchi" ],
      "venue" : "Wainwright, “Divide and conquer kernel ridge regression,” in Proc. 2013 Conf. on Learning Theory, Princeton, NJ, USA, Jun. 12-14",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and I",
      "author" : [ "C.-J. Hsieh", "S. Si" ],
      "venue" : "S. Dhillon, “A divide-andconquer solver for kernel support vector machines,” in Proc. 31st Int. Conf. Machine Learning, Beijing, China",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An algorithm for generalized matrix eigenvalue problems,",
      "author" : [ "C.B. Moler", "G.W. Stewart" ],
      "venue" : "SIAM Journal on Numerical Analysis,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1973
    }, {
      "title" : "Liblinear: A library for large linear classification,",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Making large-scale SVM learning practical,",
      "author" : [ "T. Joachims" ],
      "venue" : "Advances in Kernel Methods – Support Vector Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Decomposition methods for linear support vector machines,",
      "author" : [ "W. Kao", "K. Chung", "C. Sun", "C. Lin" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Large scale semisupervised linear svms,",
      "author" : [ "V. Sindhwani", "S.S. Keerthi" ],
      "venue" : "Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "A modified finite newton method for fast solution of large scale linear svms,",
      "author" : [ "S.S. Keerthi", "D. DeCoste" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, RBFs can map the input features into the intrinsic space [3] that is spanned by infinite-dimensional vectors.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "This correspondingly increases the opportunity of creating a discriminant hyperplane in the empirical space [3], subsequently enhancing discriminability.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "[4] proposed using Truncated Radical Basis Functions (TRBFs) to avoid generating infinite dimensions in the intrinsic space.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Although computational load is relieved without losing too much accuracy, however, that method [4] did not improve discriminability and separability between features.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "In response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "In response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "used divide-and-conquer KRR [5] to support computation of large-scale data.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "[6] focused on systematic data division before applying divideand-conquer classifiers to the data.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "DCA also conducts orthogonal transformation like PCA, while its discriminant matrix is [Sw + ρI]S , where Sw is the within-class scatter matrix, and ρ is the ridge parameter [3].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "We conduct generalized eigenvector decomposition [7] to obtain the eigenvectors ν1, ν2, .",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "Observations show that m < 50 << N and TRBFKRR[4] generates favorable results for fn+1.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,· · · ,fh in our system respectively and",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,· · · ,fh in our system respectively and",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "[6], as is shown in Table 5.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[12] 2Results are cited from Hsieh et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6]",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2015,
    "abstractText" : "This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}