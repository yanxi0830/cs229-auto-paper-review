{
  "name" : "1608.04320.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "namrata@iastate.edu", "hanguo@iastate.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n04 32\n0v 2\n[ cs\n.L G\n] 2\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "Principal Components Analysis (PCA) is among the most frequently used tools for dimension reduction. Given a matrix of data, it computes a small number of orthogonal directions that contain all (or most) of the variability of the data. The subspace spanned by these directions is the “principal subspace”. To use PCA for dimension reduction, one projects the observed data onto this subspace. The standard solution to PCA is to compute the reduced singular value decomposition (SVD) of the data matrix, or, equivalently, to compute the reduced eigenvalue decomposition (EVD) of the empirical covariance matrix of the data. If all eigenvalues are nonzero, a threshold is used and all eigenvectors with eigenvalues above the threshold are retained. This solution, which we henceforth refer to as simple EVD, or just EVD, has been used for many decades and is well-studied in literature, e.g., see [1] and references therein. However, to the best of our knowledge, all existing results for it assume that the true data and the corrupting noise in the observed data are independent, or, at least, uncorrelated. This is valid in practice often, but not always. Here, we study the PCA problem in the setting where the data and noise vectors may be correlated (correlated-PCA). Such noise is sometimes called “data-dependent” noise.\nContributions. (1) Under a boundedness assumption on the true data vectors, and some other assumptions, for a fixed desired subspace error level, we show that the sample complexity of simpleEVD for correlated-PCA scales as f2r2 logn where n is the data vector length, f is the condition number of the true data covariance matrix and r is its rank. Here “sample complexity” refers to the number of samples needed to get a small enough subspace recovery error with high probability (whp). The dependence on f2 is problematic for datasets with large condition numbers, and, especially in the high dimensional setting where n is large. (2) To address this, we also develop and analyze a generalization of simple-EVD, called cluster-EVD. Under an eigenvalues’ “clustering” assumption, cluster-EVD weakens the dependence on f .\nTo our best knowledge, the correlated-PCA problem has not been explicitly studied. We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5]. The version of correlated-PCA studied here is motivated\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nby these works. Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.\nNotation. We use the interval notation [a, b] to mean all of the integers between a and b, inclusive, and similarly for [a, b) etc. We use ‖ · ‖ to denote the l2 norm of a vector or the induced l2 norm of a matrix. For other lp norms, we use ‖ · ‖p. For a set T , IT refers to an n× |T | matrix of columns of the identity matrix indexed by entries in T . For a matrix A, AT := AIT . A tall matrix with orthonormal columns is referred to as a basis matrix. For basis matrices P̂ and P , we quantify the subspace error (SE) between their range spaces using\nSE(P̂ ,P ) := ‖(I − P̂ P̂ ′)P ‖. (1)"
    }, {
      "heading" : "1.1 Correlated-PCA: Problem Definition",
      "text" : "We are given a time sequence of data vectors, yt, that satisfy\nyt = ℓt +wt, with wt = Mtℓt and ℓt = Pat (2)\nwhere P is an n × r basis matrix with r ≪ n. Here ℓt is the true data vector that lies in a low dimensional subspace of Rn, range(P ); at is its projection into this r-dimensional subspace; and wt is the data-dependent noise. We refer to Mt as the correlation / data-dependency matrix. The goal is to estimate range(P ). We make the following assumptions on ℓt and Mt.\nAssumption 1.1. The subspace projection coefficients, at, are zero mean, mutually independent and bounded random vectors (r.v.), with a diagonal covariance matrix Λ. Define λ− := λmin(Λ), λ+ := λmax(Λ) and f := λ +\nλ− . Since the at’s are bounded, we can also define a finite constant\nη := maxj=1,2,...r maxt (at)\n2 j\nλj . Thus, (at)2j ≤ ηλj .\nFor most bounded distributions, η will be a small constant more than one, e.g., if the distribution of all entries of at is iid zero mean uniform, then η = 3. From Assumption 1.1, clearly, the ℓt’s are also zero mean, bounded, and mutually independent r.v.’s with a rank r covariance matrix Σ EVD = PΛP ′. In the model, for simplicity, we assume Λ to be fixed. However, even if we replace Λ by Λt and define λ− = mint λmin(Λt) and λ+ = λmax(Λt), all our results will still hold.\nAssumption 1.2. Decompose Mt as Mt = M2,tM1,t. Assume that\n‖M1,tP ‖ ≤ q < 1, ‖M2,t‖ ≤ 1, (3) and, for any sequence of positive semi-definite Hermitian matrices, At, the following holds\nfor a β < α,\n∥ ∥ ∥ ∥ ∥ 1 α α ∑\nt=1\nM2,tAtM2,t ′\n∥ ∥ ∥ ∥ ∥ ≤ β α max t∈[1,α] ‖At‖. (4)\nWe will need the above to hold for all α ≥ α0 and for all β ≤ c0α with a c0 ≪ 1. We set α0 and c0 in Theorems 2.1 and 3.3; both will depend on q. Observe that, using (3), ‖wt‖‖ℓt‖ ≤ q, and so q is an upper bound on the signal-to-noise ratio (SNR).\nTo understand the assumption on M2,t, notice that, if we allow β = α, then (4) always holds and is not an assumption. Let B denote the matrix on the LHS of (4). One example situation when (4) holds with a β ≪ α is if B is block-diagonal with blocks At. In this case, it holds with β = 1. In fact, it also holds with β = 1 if B is permutation-similar to a block diagonal matrix. The matrix B will be of this form if M2,t = ITt with all the sets Tt being mutually disjoint. More generally, if B is permutation-similar to a block-diagonal matrix with blocks given by the summation of At’s over at most β0 < α time instants, then (4) holds with β = β0. This will happen if M2,t = ITt with Tt = T [k] for at most β0 time instants and if sets T [k] are mutually disjoint for different k. Finally, the T [k]’s need not even be mutually disjoint. As long as they are such that B is a matrix with nonzero blocks on only the main diagonal and on a few diagonals near it, e.g., if it is block tri-diagonal, it can be shown that the above assumption holds. This example is generalized in Assumption 1.3 given below."
    }, {
      "heading" : "1.2 Examples of correlated-PCA problems",
      "text" : "One key example of correlated-PCA is the PCA with missing data (PCA-missing) problem. Let Tt denote the set of missing entries at time t. Suppose, we set the missing entries of yt to zero. Then,\nyt = ℓt − ITtITt ′ℓt. (5)\nIn this case M2,t = ITt and M1,t = −ITt ′. Thus, q is an upper bound on ‖ITt ′P ‖. Clearly, it will be small if the columns of P are dense vectors. For the reader familiar with low-rank matrix completion (MC), e.g., [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix. This would, of course, be much more expensive than directly solving PCA-missing and would need more assumptions.\nAnother example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component’s magnitude is correlated with ℓt. Let Tt denote the support set of wt and let xt be the |Tt|-length vector of its nonzero entries. If we assume linear dependency of xt on ℓt, we can write out yt as\nyt = ℓt + ITtxt = ℓt + ITtMs,tℓt. (6) Thus M2,t = ITt and M1,t = Ms,t and so q is an upper bound on ‖Ms,tP ‖. In the rest of the paper, we refer to this problem is “PCA with sparse data-dependent corruptions (PCA-SDDC)”. One key application where it occurs is in foreground-background separation for videos consisting of a slow changing background sequence (modeled as lying close to a low-dimensional subspace) and a sparse foreground image sequence consisting typically of one or more moving objects [14]. The PCA-SDDC problem is to estimate the background sequence’s subspace. In this case, ℓt is the background image at time t, Tt is the support set of the foreground image at t, and xt is the difference between foreground and background intensities on Tt. An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components’ pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L. However, as shown in Sec. 5, Table 1, this approach will be much slower; and it will work only if its required incoherence assumptions hold. For example, if the columns of P are sparse, it fails.\nFor both problems above, a solution for PCA will work only when the corrupting noise wt is small compared to ℓt. A sufficient condition for this is that q is small.\nA third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5]. We refer the reader to [18] to understand this application.\nIn all three of the above applications, the assumptions on the data-noise correlation matrix given in Assumption 1.2 hold if there are enough changes of a certain type in the set of missing or corrupted entries, Tt. One example where this is true is in case of a 1D object of length s or less that remains static for at most β frames at a time. When it moves, it moves by at least a certain fraction of s pixels. The following assumption is inspired by the object’s support. Assumption 1.3. Let l denote the number of times the set Tt changes in the interval [1, α] (or in any given interval of length α in case of dynamic robust PCA). So 0 ≤ l ≤ α − 1. Let t0 := 1; let tk, with tk < tk+1, denote the time instants in this interval at which Tt changes; and let T [k] denote the distinct sets. In other words, Tt = T [k] for t ∈ [tk, tk+1), for each k = 1, 2, . . . , l. Assume that the following hold with a β < α:\n1. (tk+1 − tk) ≤ β̃ and |T [k]| ≤ s;\n2. ρ2β̃ ≤ β where ρ is the smallest positive integer so that, for any 0 ≤ k ≤ l, T [k] and T [k+ρ] are disjoint;\n3. for any k1, k2 satisfying 0 ≤ k1 < k2 ≤ l, the sets (T [k1] \\ T [k1+1]) and (T [k2] \\ T [k2+1]) are disjoint.\nAn implicit assumption for condition 3 to hold is that ∑l k=0 |T [k] \\ T [k+1]| ≤ n. Observe that conditions 2 and 3 enforce an upper bound on the maximum support size s.\nTo connect Assumption 1.3 with the moving object example given above, condition 1 holds if the object’s size is at most s and if it moves at least once every β̃ frames. Condition 2 holds, if, every time it moves, it moves in the same direction and by at least s\nρ pixels. Condition 3 holds if, every\ntime it moves, it moves in the same direction and by at most d0 ≥ sρ pixels, with d0α ≤ n (or, more generally, the motion is such that, if the object were to move at each frame, and if it started at the top of the frame, it does not reach the bottom of the frame in a time interval of length α).\nThe following lemma [4] shows that, with Assumption 1.3 on Tt, M2,t = ITt satisfies the assumption on M2,t given in Assumption 1.2. Its proof generalizes the discussion below Assumption 1.2.\nLemma 1.4. [[4], Lemmas 5.2 and 5.3] Assume that Assumption 1.3 holds. For any sequence of |Tt| × |Tt| symmetric positive-semi-definite matrices At,\n‖ α ∑\nt=1\nITtAtITt ′‖ ≤ (ρ2β̃) max t∈[1,α] ‖At‖ ≤ β max t∈[1,α] ‖At‖\nThus, if ‖ITt ′P ‖ ≤ q < 1, then the PCA-missing problem satisfies Assumption 1.2. If ‖Ms,tP ‖ ≤ q < 1, then the PCA-SDDC problem satisfies Assumption 1.2.\nAssumption 1.3 is one model on Tt that ensures that, if M2,t = ITt , the assumption on M2,t given in Assumption 1.2 holds. For its many generalizations, see Supplementary Material, Sec. 7, or [4]."
    }, {
      "heading" : "2 Simple EVD",
      "text" : "Simple EVD computes the top eigenvectors of the empirical covariance matrix, 1\nα\n∑α t=1 ytyt ′, of\nthe observed data. The following can be shown.\nTheorem 2.1 (simple-EVD result). Let P̂ denote the matrix containing all the eigenvectors of 1 α ∑α t=1 ytyt\n′ with eigenvalues above a threshold, λthresh, as its columns. Pick a ζ so that rζ ≤ 0.01. Suppose that yt’s satisfy (2) and the following hold.\n1. Assumption 1.1 on ℓt holds. Define\nα0 := Cη 2 r\n211 logn\n(rζ)2 max(f, qf, q2f)2, C :=\n32\n0.012 .\n2. Assumption 1.2 on Mt holds for any α ≥ α0 and for any β satisfying β\nα ≤\n( 1− rζ 2\n)2\nmin\n(\n(rζ)2\n4.1(qf)2 , (rζ) q2f\n)\n3. Set algorithm parameters λthresh = 0.95λ− and α ≥ α0.\nThen, with probability at least 1− 6n−10, SE(P̂ ,P ) ≤ rζ. Proof: The proof involves a careful application of the sin θ theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin θ bound. It is given in the Supplementary Material, Section 8.\nConsider the lower bound on α. We refer to this as the “sample complexity”. Since q < 1, and η is a small constant (e.g., for the uniform distribution, η = 3), for a fixed error level, rζ, α0 simplifies to cf2r2 logn. Notice that the dependence on n is logarithmic. It is possible to show that the sample complexity scales as logn because we assume that the ℓt’s are bounded r.v.s. As a result we can apply the matrix Hoeffding inequality [20] to bound the perturbation between the observed data’s empirical covariance matrix and that of the true data. The bounded r.v. assumption is actually a more practical one than the usual Gaussian assumption since most sources of data have finite power.\nBy replacing matrix Hoeffding by Theorem 5.39 of [21] in places where one can apply a concentration of measure result to ∑\nt atat ′/α (which is at r × r matrix), and by matrix Bernstein [20] else-\nwhere, it should be possible to further reduce the sample complexity to cmax((qf)2r logn, f2(r+ logn)). It should also be possible remove the boundedness assumption and replace it by a Gaussian (or a sub-Gaussian) assumption, but, that would increase the sample complexity to c(qf)2n.\nConsider the upper bound on β/α. Clearly, the smaller term is the first one. This depends on 1/(qf)2. Thus, when f is large and q is not small enough, the bound required may be impractically small. As will be evident from the proof (see Remark 8.3 in Supplementary Material), we get this bound because wt is correlated with ℓt and this results in E[ℓtwt′] 6= 0. If wt and ℓt were uncorrelated, qf would get replaced by λmax(Cov(wt)) λ−\nin the upper bound on β/α as well as in the sample complexity.\nApplication to PCA-missing and PCA-SDDC. By Lemma 1.4, the following is immediate. Corollary 2.2. Consider the PCA-missing model, (5), and assume that maxt ‖ITt ′P ‖ ≤ q < 1; or consider the PCA-SDDC model, (6), and assume that maxt ‖Ms,tP ‖ ≤ q < 1. Assume that everything in Theorem 2.1 holds except that we replace Assumption 1.2 by Assumption 1.3. Then, with probability at least 1− 6n−10, SE(P̂ ,P ) ≤ rζ."
    }, {
      "heading" : "3 Cluster-EVD",
      "text" : "To try to relax the strong dependence on f2 of the result above, we develop a generalization of simple-EVD that we call cluster-EVD. This requires the clustering assumption."
    }, {
      "heading" : "3.1 Clustering assumption",
      "text" : "To state the assumption, define the following partition of the index set {1, 2, . . . r} based on the eigenvalues of Σ. Let λi denote its i-th largest eigenvalue. Definition 3.1 (g-condition-number partition of {1, 2, . . . r}). Define G1 = {1, 2, . . . r1} where r1 is the index for which λ1\nλr1 ≤ g and λ1 λr1+1 > g. In words, to define G1, start with the index of the first\n(largest) eigenvalue and keep adding indices of the smaller eigenvalues to the set until the ratio of the maximum to the minimum eigenvalue first exceeds g.\nFor each k > 1, define Gk = {r∗+1, r∗+2, . . . , r∗+rk} where r∗ = ( ∑k−1 i=1 ri), rk is the index for which λr∗+1 λr∗+rk ≤ g and λr∗+1 λr∗+rk+1\n> g. In words, to define Gk , start with the index of the (r∗ + 1)-th eigenvalue, and repeat the above procedure.\nStop when λr∗+rk+1 = 0, i.e., when there are no more nonzero eigenvalues. Define ϑ = k as the number of sets in the partition. Thus {G1,G2, . . . ,Gϑ} is the desired partition.\nDefine G0 = [.], Gk := (P )Gk , λ + k := maxi∈Gk λi (Λ), λ − k := mini∈Gk λi (Λ) and\nχ := max k=1,2,...,ϑ λ+k+1 λ−k .\nχ quantifies the “distance” between consecutive sets of the above partition. Moreover, by definition, λ +\nk λ −\nk\n≤ g. Clearly, g ≥ 1 and χ ≤ 1 always. We assume the following.\nAssumption 3.2. For a 1 ≤ g+ < f and a 0 ≤ χ+ < 1, assume that there exists a g satisfying 1 ≤ g ≤ g+ and a χ satisfying 0 ≤ χ ≤ χ+, for which we can define a g-condition-number partition of {1, 2, . . . r} that satisfies χ ≤ χ+. The number of sets in the partition is ϑ. When g+ and χ+ are small, we say that the eigenvalues are “well-clustered” with “clusters”, Gk.\nThis assumption can be understood as a generalization of the eigen-gap condition needed by the block power method, which is a fast algorithm for obtaining the k top eigenvectors of a matrix [22]. We expect it to hold for data that has variability across different scales. The large scale variations would result in the first (largest eigenvalues’) cluster and the smaller scale variations would form the later clusters. This would be true, for example, for video “textures” such as moving waters or waving trees in a forest. We tested this assumption on some such videos. We describe our conclusions here for three videos - “lake” (video of moving lake waters), “waving-tree” (video consisting of waving trees), and “curtain” (video of window curtains moving due to the wind). For each video, we first made it low-rank by keeping the eigenvectors corresponding to the smallest number of eigenvalues that contain at least 90% of the total energy and projecting the video onto this subspace. For the low-rankified lake video, f = 74 and Assumption 3.2 holds with ϑ = 6 clusters, g+ = 2.6 and χ+ = 0.7. For the waving-tree video, f = 180 and Assumption 3.2 holds with ϑ = 6, g+ = 9.4 and χ+ = 0.72. For the curtain video, f = 107 and the assumption holds ϑ = 3, g+ = 16.1 and χ+ = 0.5. We show the clusters of eigenvalues in Fig. 1."
    }, {
      "heading" : "3.2 Cluster-EVD algorithm",
      "text" : "The cluster-EVD approach is summarized in Algorithm 1. I Its main idea is as follows. We start by computing the empirical covariance matrix of the first set of α observed data points, D̂1 := 1 α ∑α t=1 ytyt ′. Let λ̂i denote its i-th largest eigenvalue. To estimate the first cluster, Ĝ1, we start with the index of the first (largest) eigenvalue and keep adding indices of the smaller eigenvalues\nAlgorithm 1 Cluster-EVD Parameters: α, ĝ, λthresh. Set Ĝ0 ← [.]. Set the flag Stop ← 0. Set k ← 1. Repeat\n1. Let Ĝdet,k := [Ĝ0, Ĝ1, . . . Ĝk−1] and let Ψk := (I − Ĝdet,kĜdet,k′). Notice that Ψ1 = I. Compute\nD̂k = Ψk\n\n\n1\nα\nkα ∑\nt=(k−1)α+1 ytyt\n′\n\nΨk\n2. Find the k-th cluster, Ĝk: let λ̂i = λi(D̂k); (a) find the index r̂k for which λ̂1\nλ̂r̂k ≤ ĝ and either λ̂1 λ̂r̂k+1 > ĝ or λ̂r̂k+1 < λthresh;\n(b) set Ĝk = {r̂∗ + 1, r̂∗ + 2, . . . , r̂∗ + r̂k} where r̂∗ := ∑k−1 j=1 r̂j ;\n(c) if λ̂r̂k+1 < λthresh, update the flag Stop ← 1 3. Compute Ĝk ← eigenvectors(D̂k, r̂k); increment k\nUntil Stop == 1. Set ϑ̂ ← k. Output P̂ ← [Ĝ1 · · · Ĝϑ̂]. eigenvectors(M, r) returns a basis matrix for the span of the top r eigenvectors of M.\nto it until the ratio of the maximum to the minimum eigenvalue exceeds ĝ or until the minimum eigenvalue goes below a “zero threshold”, λthresh. Then, we estimate the first cluster’s subspace, range(G1) by computing the top r̂1 eigenvectors of D̂1. To get the second cluster and its subspace, we project the next set of α yt’s orthogonal to Ĝ1 followed by repeating the above procedure. This is repeated for each k > 1. The algorithm stops when λ̂r̂k+1 < λthresh.\nAlgorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS. The one introduced in [3] assumed that the clusters were known to the algorithm (which is unrealistic). The one studied in [5] has an automatic cluster estimation approach, but, one that needs a larger lower bound on α compared to what Algorithm 1 needs."
    }, {
      "heading" : "3.3 Main result",
      "text" : "We give the performance guarantee for Algorithm 1 here. Its parameters are set as follows. We set ĝ to a value that is a little larger than g. This is needed to allow for the fact that λ̂i is not equal to the i-th eigenvalue of Λ but is within a small margin of it. For the same reason, we need to also use a nonzero “zeroing” threshold, λthresh, that is larger than zero but smaller than λ−. We set α large enough to ensure that SE(P̂ ,P ) ≤ rζ holds with a high enough probability. Theorem 3.3 (cluster-EVD result). Consider Algorithm 1. Pick a ζ so that r2ζ ≤ 0.0001, and r2ζf ≤ 0.01. Suppose that yt’s satisfy (2) and the following hold.\n1. Assumption 1.1 and Assumption 3.2 on ℓt hold with χ+ satisfying χ+ ≤ min(1 − rζ − 0.08 0.25 , g+−0.0001 1.01g++0.0001 − 0.0001). Define\nα0 := Cη 2 r\n2(11 logn+ logϑ)\n(rζ)2 max(g+, qg+,\nq2f, q(rζ)f, (rζ)2f, q √ fg+, (rζ) √ fg+)2, C := 32 · 16 0.012 .\n2. Assumption 1.2 on Mt holds with α ≥ α0 and with β satisfying β\nα ≤\n( (1− rζ − χ+) 2\n)2\nmin\n(\n(rkζ) 2\n4.1(qg+)2 , (rkζ) q2f\n)\n.\n3. Set algorithm parameters ĝ = 1.01g+ + 0.0001, λthresh = 0.95λ− and α ≥ α0.\nThen, with probability at least 1− 12n−10, SE(P̂ ,P ) ≤ rζ.\nProof: The proof is given in Section 9 in Supplementary Material.\nWe can also get corollaries for PCA-missing and PCA-SDDC for cluster-EVD. We have given one specific value for ĝ and λthresh in Theorem 3.3 for simplicity. One can, in fact, set ĝ to be anything that satisfies (12) given in Supplementary Material and one can set λthresh to be anything satisfying 5rζλ− ≤ λthresh ≤ 0.95λ−. Also, it should be possible to reduce the sample complexity of clusterEVD to cmax(q2(g+)2r logn, (g+)2(r + logn)) using the approach explained in Sec. 2."
    }, {
      "heading" : "4 Discussion",
      "text" : "Comparing simple-EVD and cluster-EVD. Consider the lower bounds on α. In the cluster-EVD (c-EVD) result, Theorem 3.3, if q is small enough (e.g., if q ≤ 1/√f ), and if (r2ζ)f ≤ 0.01, it is clear that the maximum in the max(., ., ., .) expression is achieved by (g+)2. Thus, in this regime, c-EVD needs α ≥ C r 2(11 logn+log ϑ)\n(rζ)2 g 2 and its sample complexity is ϑα. In the EVD result\n(Theorem 2.1), g+ gets replaced by f and ϑ by 1, and so, its sample complexity, α ≥ C r211 logn(rζ)2 f2. In situations where the condition number f is very large but g+ is much smaller and ϑ is small (the clustering assumption holds well), the sample complexity of c-EVD will be much smaller than that of simple-EVD. However, notice that, the lower bound on α for simple-EVD holds for any q < 1 and for any ζ with rζ < 0.01 while the c-EVD lower bound given above holds only when q is small enough, e.g., q = O(1/ √ f), and ζ is small enough, e.g., rζ = O(1/f). This tighter bound on ζ is needed because the error of the k-th step of c-EVD depends on the errors of the previous steps times f . Secondly, the c-EVD result also needs χ+ and ϑ to be small (clustering assumption holds well), whereas, for simple-EVD, by definition, χ+ = 0 and ϑ = 1. Another thing to note is that the constants in both lower bounds are very large with the c-EVD one being even larger.\nTo compare the upper bounds on β, assume that the same α is used by both, i.e., α = max(α0(EVD), α0(c-EVD)). As long as rk is large enough, χ+ is small enough, and g is small enough, the upper bound on β needed by the c-EVD result is significantly looser. For example, if χ+ = 0.2, ϑ = 2, rk = r/2, then c-EVD needs β ≤ (0.5 · 0.79 · 0.5)2 (rζ) 2\n4.1q2g2α while simple-EVD\nneeds β ≤ (0.5 · 0.99)2 (rζ) 2\n4.1q2f2α. If g = 3 but f = 100, clearly the c-EVD bound is looser.\nComparison with other results for PCA-SDDC and PCA-missing. To our knowledge, there is no other result for correlated-PCA. Hence, we provide comparisons of the corollaries given above for the PCA-missing and PCA-SDDC special cases with works that also study these or related problems. An alternative solution for either PCA-missing or PCA-SDDC is to first recover the entire matrix L and then compute its subspace via SVD on the estimated L. For the PCA-missing problem, this can be done by using any of the low-rank matrix completion techniques, e.g., nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23]. Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components’ pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].\nHowever, as explained earlier doing the above has two main disadvantages. The first is that it is much slower (see Sec. 5). The difference in speed is most dramatic when solving the matrix-sized convex programs such as NNM or PCP, but even the Alt-Min methods are slower. If we use the time complexity from [17], then finding the span of the top k singular vectors of an n ×m matrix takes O(nmk) time. Thus, if ϑ is a constant, both simple-EVD and c-EVD need O(nαr) time, whereas, Alt-Min-RPCA needs O(nαr2) time per iteration [17]. The second disadvantage is that the above methods for MC or RPCA need more assumptions to provably correctly recover L. All the above methods need an incoherence assumption on both the left singular vectors, P , and the right singular vectors, V , of L. Of course, it is possible that, if one studies these methods with the goal of only recovering the column space of L correctly, the incoherence assumption on the right singular vectors is not needed. From simulation experiments (see Sec. 5), the incoherence of the left singular vectors is definitely needed. On the other hand, for the PCA-SDDC problem, simple-EVD or c-EVD do not even need the incoherence assumption on P .\nThe disadvantage of both EVD and c-EVD, or in fact of any solution for the PCA problem, is that they work only when q is small enough (the corrupting noise is small compared to ℓt)."
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "We use the PCA-SDDC problem as our case study example. We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code\nfrom the authors’ webpage). For both PCP and Alt-Min-RPCA, P̂ is recovered as the top r eigenvectors of of the estimated L. To show the advantage of EVD or c-EVD, we let ℓt = Pat with columns of P being sparse. These were chosen as the first r = 5 columns of the identity matrix. We generate at’s iid uniformly with zero mean and covariance matrix Λ = diag(100, 100, 100, 0.1, 0.1). Thus the condition number f = 1000. The clustering assumption holds with ϑ = 2, g+ = 1 and χ+ = 0.001. The noise wt is generated as wt = ITtMs,tℓt with Tt generated to satisfy Assumption 1.3 with s = 5, ρ = 2, and β̃ = 1; and the entries of Ms,t being iid N (0, q2) with q = 0.01. We used n = 500. EVD and c-EVD (Algorithm 1) were implemented with α = 300, λthresh = 0.095, ĝ = 3. 10000-time Monte Carlo averaged values of SE(P̂ ,P ) and execution time are shown in the first row of Table 1. Since the columns of P are sparse, both PCP and Alt-Min-RPCA fail. Both have average SE close to one whereas the average SE of c-EVD and EVD is 0.0908 and 0.0911 respectively. Also, both EVD and c-EVD are much faster than the other two. We also did an experiment with the settings of this experiment, but with P dense. In this case, EVD and c-EVD errors were similar, but PCP and Alt-Min-RPCA errors were less than 10−5.\nFor our second experiment, we used images of a low-rankified real video sequence as ℓt’s. We chose the escalator sequence from http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html since the video changes are only in the region where the escalator moves (and hence can be modeled as being sparse). We made it exactly low-rank by retaining its top 5 eigenvectors and projecting onto their subspace. This resulted in a data matrix L of size n × r with n = 20800 and r = 5. We overlaid a simulated moving foreground block on it. The intensity of the moving block was controlled to ensure that q is small. We estimated P̂ using EVD, c-EVD, PCP and Alt-Min-RPCA. We let P be the eigenvectors of the low-rankified video with nonzero eigenvalues and computed SE(P̂ ,P ). The errors and execution time are displayed in the second row of Table 1. Since n is very large, the difference in speed is most apparent in this case.\nThus c-EVD outperforms PCP and AltMinRPCA when columns of P are sparse. It also outperforms EVD but the advantage in mean error is not as much as our theorems predict. One reason is that the constant in the required lower bounds on α is very large. It is hard to pick an α that is this large and still only O(log n) unless n is very large. Secondly, both guarantees are only sufficient conditions."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We studied the problem of PCA in noise that is correlated with the data (data-dependent noise). We obtained sample complexity bounds for the most commonly used PCA solution, simple EVD. We also developed and analyzed a generalization of EVD, called cluster-EVD, that has lower sample complexity under extra assumptions. We provided a detailed comparison of our results with those for other approaches to solving its example applications - PCA with missing data and PCA with sparse data-dependent corruptions.\nWe used the matrix Hoeffding inequality [20] to obtain our results. As explained in Sec. 2, it should be possible to improve the sample complexity bounds if this is replaced by [21, Theorem 5.39] or matrix Bernstein. Moreover, as done in [5] (for ReProCS), the mutual independence of ℓt’s can be easily replaced by a more practical assumption of ℓt’s following autoregressive model with almost no change to our assumptions. Thirdly, by generalizing the proof techniques developed here, we can also study the problem of correlated-PCA with partial subspace knowledge. This is done in [25]. The solution to the latter problem helps to greatly simplify the proof of correctness of ReProCS for online dynamic RPCA [18]. Fourthly, the boundedness assumption on ℓt’s can be replaced by a Gaussian or a well-behaved sub-Gaussian assumption but this will increase the sample complexity to O(n). Finally, an open-ended question is how we relax Assumption 1.2 on Mt and still get results similar to Theorem 2.1 or Theorem 3.3."
    }, {
      "heading" : "7 More examples of Assumption 1.2",
      "text" : "Assumption 1.3 is one simple example of a support change model that ensures that, if M2,t = ITt , the assumption on M2,t given in Assumption 1.2 holds. If instead of one object, there are k objects, and each of their supports satisfies Assumption 1.3, then again, with some modifications, it is possible to show that both the PCA-missing and PCA-SDDC problems satisfy Assumption 1.2. Moreover, notice that Assumption 1.3 does not require the entries in Tt to be contiguous at all (they need not correspond to the support of one or a few objects). Similarly, we can replace the condition that Tt be constant for at most β̃ time instants in Assumption 1.3 by |{t : Tt = T [k]}| ≤ β̃. Thirdly, the requirement of the object(s) always moving in one direction may seem too stringent. As explained in [4, Lemma 9.4], a Bernoulli-Gaussian “constant velocity with random acceleration” motion model will also work whp. It allows the object to move at each frame with probability p and not move with probability 1 − p independent of past or future frames; when the object moves, it moves with an iid Gaussian velocity that has mean 1.1s/ρ and variance σ2; σ2 needs to be upper bounded and p needs to be lower bounded.\nLastly, if s < c1α for c1 ≪ 1, another model that works is that of an object of length s or less moving by at least one pixel and at most b pixels at each time [4, Lemma 9.5]."
    }, {
      "heading" : "8 Proof of Theorem 2.1",
      "text" : "This result also follows as a corollary of Theorem 3.3. We prove it separately first since its proof is short and and less notation-ally intensive. It will help understand the proof of Theorem 3.3 much more easily. Both results rely on the sin θ theorem reviewed next. 8.1 sin θ theorem Davis and Kahan’s sin θ theorem [19] studies the rotation of eigenvectors by perturbation.\nTheorem 8.1 (sin θ theorem [19]). Consider two Hermitian matrices D and D̂. Suppose that D can be decomposed as\nD = [ E E⊥ ]\n[\nA 0 0 A⊥\n] [ E′\nE⊥ ′\n]\nwhere [E E⊥] is an orthonormal matrix. Suppose that D̂ can be decomposed as\nD̂ = [ F F⊥ ]\n[\nΛ 0 0 Λ⊥\n] [ F ′\nF⊥ ′\n]\nwhere [F F⊥] is another orthonormal matrix and is such that rank(F ) = rank(E). Let H := D̂ −D denote the perturbation. If λmin(A) > λmax(Λ⊥), then\n‖(I − FF ′)E‖ ≤ ‖H‖ λmin(A)− λmax(Λ⊥) .\nLet r = rank(E). Suppose that F is the matrix of top r eigenvectors of D̂. Then Λ and Λ⊥ are diagonal and λmax(Λ⊥) = λr+1(D̂) ≤ λr+1(D) + ‖H‖. The inequality follows using Weyl’s inequality. Suppose also that λmin(A) > λmax(A⊥). Then, (i) λr(D) = λmin(A) and λr+1(D) = λmax(A⊥) and (ii) range(E) is equal to the span of the top r eigenvectors ofD. Thus, λmax(Λ⊥) ≤ λmax(A⊥) + ‖H‖. With this we have the following corollary. Corollary 8.2. Consider a Hermitian matrix D and its perturbed version D̂. Suppose that D can be decomposed as\nD = [ E E⊥ ]\n[\nA 0 0 A⊥\n] [ E′\nE⊥ ′\n]\nwhere E is a basis matrix. Let F denote the matrix containing the top rank(E) eigenvectors of D̂. Let H := D̂ −D denote the perturbation. If λmin(A)− λmax(A⊥)− ‖H‖ > 0, then\n‖(I − FF ′)E‖ ≤ ‖H‖ λmin(A)− λmax(A⊥)− ‖H‖ .\nand range(E) is equal to the span of the top rank(E) eigenvectors of D."
    }, {
      "heading" : "8.2 Proof of Theorem 2.1",
      "text" : "We use the sin θ theorem [19] from Corollary 8.2. Apply it with D̂ = 1\nα\n∑ t ytyt ′ and\nD = 1 α\n∑ t ℓtℓt ′. Thus, F = P̂ . Recall that at = P ′ℓt. Then, D can be decomposed as\nP ( 1 α\n∑ t atat ′)P ′ + P⊥0P ′⊥, and so we have E = P , A = 1 α ∑ t atat ′ and A⊥ = 0. Moreover,\nit is easy to see that the perturbation H := 1 α\n∑ t ytyt ′ − 1 α ∑ t ℓtℓt ′ satisfies\nH = 1\nα\n∑\nt\nℓtw ′ t +\n1\nα\n∑\nt\nwtℓ ′ t +\n1\nα\n∑\nt\nwtw ′ t. (7)\nThus,\nSE(P̂ ,P )\n≤ 2‖ 1 α\n∑ t ℓtw ′ t‖+ ‖ 1α ∑ t wtw ′ t‖\nλr( 1 α\n∑ t ℓtℓ ′ t)− (2‖ 1α ∑ t ℓtw ′ t‖+ ‖ 1α ∑ t wtw ′ t‖)\nif the denominator is positive. Remark 8.3. Because wt is correlated with ℓt, the ℓtw′t terms are the dominant ones in the perturbation expression given in (7). If they were uncorrelated, these two terms would be close to zero whp due to law of large numbers and the wtw′t term would be the dominant one.\nIn the next lemma, we bound the terms in the bound on SE(P̂ ,P ) using the matrix Hoeffding inequality [20]. Lemma 8.4. Let ǫ = 0.01rζλ−.\n1. With probability at least 1− 2n exp ( −α ǫ232(ηrqλ+)2 ) ,\n‖ 1 α ∑\nt\nℓtwt ′‖ ≤ qλ+\n√\nβ α + ǫ = [qf\n√\nβ α + 0.01rζ]λ−\n2. With probability at least 1− 2n exp(− αǫ232(ηrq2λ+)2 ),\n‖ 1 α ∑\nt\nwtwt ′‖ ≤ β\nα q2λ+ + ǫ = [q2f\nβ α + 0.01rζ]λ−\n3. With probability at least 1− 2n exp(− αǫ232(ηrλ+)2 ),\nλr( 1\nα\n∑\nt\nℓtℓ ′ t) ≥ (1− (rζ)2)λ− − ǫ\nProof. This follows by using Lemma 9.6 given later with Gcur ≡ P , Gdet ≡ [.], Gundet ≡ [.], ζdet ≡ 0, rζ ≡ 0, rcur = r, g ≡ f , χ ≡ 0, ϑ ≡ 1. ⊠\nUsing this lemma to bound the subspace error terms, followed by using the bounds on β/α and ζ, we conclude the following: w.p. at least 1 − 2n exp (\n−α ǫ232(ηrqλ+)2 ) − 2n exp(− αǫ232(ηrq2λ+)2 ) − 2n exp(− αǫ232(ηrλ+)2 ),\nSE(P̂ ,P )\n≤ 2qf\n√\nβ α + q2f β α + 0.03rζ\n1− (rζ)2 − 0.01rζ − (2qf √\nβ α + q2f β α + 0.03rζ)\n≤ 0.75(1− rζ)rζ + 0.03rζ 1− rζ < rζ\nUsing the bound α ≥ α0 from the theorem, the probability of the above event is at least 1− 6n−10. We get this by bounding each of the three negative terms in the probability expression by −2n−10. We work this out for the first term: α ǫ 2\n32(ηrqλ+)2 ≥ 32·11(0.01)2 η2r2(logn) (rζ)2 (qf) 2 (0.01rζλ −)2 32η2r2q2λ+2 = 11 logn.\nThus, 2n exp ( −α ǫ232(ηrqλ+)2 ) ≤ 2n exp(−11 logn) ≤ 2n−10."
    }, {
      "heading" : "9 Proof of Theorem 3.3",
      "text" : "We explain the overall idea of the proof next. In Sec. 9.2, we give a sequence of lemmas in generalized form (so that they can apply to various other problems). The proof of Theorem 3.3 is given in Sec. 9.3 and follows easily by applying these. One of the lemmas of Sec. 9.2 is proved in Sec. 10 while the others are proved there itself."
    }, {
      "heading" : "9.1 Overall idea",
      "text" : "We need to bound SE(P̂ ,P ). From Algorithm 1, P̂ = [Ĝ1, Ĝ2, . . . , Ĝϑ] where Ĝk is the matrix of top r̂k eigenvectors of D̂k defined in Algorithm 1. Also, P = [G1,G2, . . . ,Gϑ] where Gk is a basis matrix with rk columns. Definition 9.1. Define ζk := SE([Ĝ1, Ĝ2, . . . , Ĝk],Gk) and ζ0 = 0. Define ζ+k := rkζ. Let r0 = 0.\nIt is easy to see that\nSE(P̂ ,P ) ≤ ϑ ∑\nk=1\nSE(P̂ ,Gk)\n≤ ϑ ∑\nk=1\nSE([Ĝ1, Ĝ2, . . . , Ĝk],Gk) =\nϑ ∑\nk=1\nζk (8)\nThe first inequality is triangle inequality, the second follows because [Ĝ1, Ĝ2, . . . , Ĝk] is orthogonal to [Ĝk+1, . . .Gϑ]. Since r = ∑\nk rk, if we can show that ζk ≤ ζ+k = rkζ for all k we will be done. We bound ζk using induction. The base case is easy and follows just from the definition, ζ0 = SE([.], [.]) = 0 = r0ζ. For bounding ζk, assume that for all i = 1, 2, . . . , k − 1, ζi ≤ riζ. This implies that\nSE([Ĝ1, Ĝ2, . . . , Ĝk−1], [G1,G2, . . . ,Gk−1])\n≤ k−1 ∑\ni=1\nSE([Ĝ1, Ĝ2, . . . , Ĝk−1],Gi)\n≤ k−1 ∑\ni=1\nζi ≤ k−1 ∑\ni=1\nriζ ≤ rζ (9)\nUsing this, we will first show that r̂k = rk, and then we will use this and the sin θ result to bound ζk.\nBefore proceeding further, we simplify notation.\nDefinition 9.2.\n1. Let\nGdet := [G1,G2, . . . ,Gk−1], Gcur := Gk,\nGundet := [Ĝk+1, . . .Gϑ]\n2. Similarly, let Ĝdet := [Ĝ1, Ĝ2, . . . , Ĝk−1], Ĝcur := Ĝk.\n3. Let Gdet := G1 ∪ G2 · · · ∪ Gk−1 and Gcur = Gk. 4. Let rcur := rk = rank(Gk) and r̂cur := r̂k.\n5. Let λ+cur := λ + k , λ − cur := λ − k , λ + undet := λ + k+1\n6. Let t∗ = kα."
    }, {
      "heading" : "9.2 Main lemmas - generalized form",
      "text" : "In this section, we give a sequence of lemmas that apply to a generic problem where yt = ℓt + wt = ℓt + Mtℓt with ℓt satisfying Assumption 1.1; Mt satisfying Assumption 1.2; and with P split into three parts as P = [Gdet,Gcur,Gundet]. We can correspondingly split Λ as Λ = diag(Λdet,Λcur,Λundet).\nWe are given Ĝdet that was computed using (some or all) yt’s for t ≤ t∗ and that satisfies ζdet ≤ rζ. The goal is to estimate range(Gcur) and bound the estimation error. This is done by first estimating r̂cur and then computing Ĝcur as the top r̂cur eigenvectors of\nD̂ := 1\nα\nt∗+α ∑\nt=t∗+1\nΨytyt ′ Ψ. (10)\nTo bound the estimation error, we first show that, whp, r̂cur = rcur and so Ĝcur = Gcur; and then we use this to show that ζcur ≤ rcurζ. Definition 9.3.\n1. Define Ψ := I − ĜdetĜdet′.\n2. Define ζdet := SE(Ĝdet,Gdet) = ‖ΨGdet‖ and ζ+det = rζ\n3. Define ζcur := SE([Ĝdet, Ĝcur],Gcur).\n4. Let (ΨGcur) QR = EcurRcur denote its reduced QR decomposition. Thus Ecur is a basis\nmatrix whose span equals that of (ΨGcur) and Rcur is a square upper triangular matrix with ‖Rcur‖ = ‖ΨGcur‖ ≤ 1.\n5. Let λ+cur = λmax(Λcur), λ − cur = λmin(Λcur), λ + undet = λmax(Λundet).\n6. Let rcur = rank(Gcur). Clearly, rcur ≤ r. Remark 9.4. In special cases, Gdet (and hence Ĝdet) could be empty; and/or Gundet could be empty.\n• Since Λ contains eigenvalues in decreasing order, when Gundet is not empty, λ− ≤ λ+undet ≤ λ−cur ≤ λ+cur ≤ λ+. • When Gundet is empty, λ+undet = 0 and λ− ≤ λ−cur ≤ λ+cur ≤ λ+.\nUsing ‖Rcur‖ = ‖ΨGcur‖ ≤ 1, ζcur = ‖(I − ĜcurĜcur′)ΨGcur‖\n= ‖(I − ĜcurĜcur′)EcurRcur‖ ≤ ‖(I − ĜcurĜcur′)Ecur‖ = SE(Ĝcur,Ecur).\nThus, to bound ζcur we need to bound SE(Ĝcur,Ecur). Ĝcur is the matrix of top r̂cur eigenvectors of D̂. From its definition, Ecur is a basis matrix with rcur columns. Suppose for a moment that r̂cur = rcur. Then, in order to bound SE(Ĝcur,Ecur), we can use the sin θ result, Corollary 8.2. To do this, we need to define a matrix D so that, under appropriate assumptions, the span of its top rcur eigenvectors equals range(Ecur). For the simple EVD proof, we used 1α ∑t∗+α t=t∗+1 Ψℓtℓ ′ tΨ as the matrix D. However, this will not work now since Ecur is not orthonormal to ΨGdet or to ΨGundet. But, instead we can use\nD = EcurAEcur ′ +Ecur,⊥A⊥Ecur,⊥ ′, where\nA := Ecur ′( 1\nα\nt∗+α ∑\nt=t∗+1\nΨℓtℓ ′ tΨ)Ecur and\nA⊥ := Ecur,⊥ ′( 1\nα\nt∗+α ∑\nt=t∗+1\nΨℓtℓ ′ tΨ)Ecur,⊥ (11)\nNow, by construction, D is in the desired form.\nWith the above choice of D, H := D̂ − D satisfies1 H = term1 + term1′ + term2 + term3 + term3′ where term1 := 1\nα\n∑ t Ψℓtw ′ tΨ, term2 := 1 α ∑ t Ψwtw ′ tΨ and term3 =\nEcurEcur ′( 1\nα\n∑ t Ψℓtℓ ′ tΨ)Ecur,⊥Ecur,⊥ ′.\n1This follows easily by writing H = (D̂ − 1 α ∑ t Ψℓtℓ ′ tΨ) + ( 1 α ∑ t Ψℓtℓ ′\ntΨ − D) and using the fact that M = (EE′ +E⊥E⊥′)M(EE′ +E⊥E⊥′) for 1\nα ∑ t Ψℓtℓ ′ tΨ.\nThus, using the above along with Corollary 8.2, we can conclude the following. Fact 9.5.\n1. If r̂cur = rcur, and λmin(A)− λmax(A⊥)− ‖H‖ > 0,\nζcur ≤ SE(Ĝcur,Ecur) ≤ ‖H‖\nλmin(A) − λmax(A⊥)− ‖H‖ .\n2. Let Q := EcurEcur′( 1α ∑ t Ψℓtℓ ′ tΨ)Ecur,⊥Ecur,⊥ ′. We have\n‖H‖ ≤ 2‖ 1 α ∑\nt\nΨℓtw ′ t‖+ ‖\n1\nα\n∑\nt\nwtw ′ t‖+ 2‖Q‖.\nThe next lemma bounds the RHS terms in the above lemma and a few other quantities needed for showing r̂cur = rcur.\nLemma 9.6. (1) Assume that yt = ℓt+wt = ℓt+Mtℓt with ℓt satisfying Assumption 1.1 and Mt satisfying Assumption 1.2.\n(2) Assume that we are given Ĝdet that was computed using (some or all) yt’s for t ≤ t∗ and that satisfies ζdet ≤ rζ. Define g := λ+cur/λ − cur, χ := λ + undet/λ − cur. Set ǫ := 0.01rcurζλ − cur.\nThen, the following hold:\n1. Let p1 := 2n exp(− αǫ 2\n32b2 prob\nwhere bprob := ηrq((rζ)λ+ + λ+cur + (rζ) √ λ+λ+cur + √\nλ+λ+cur). Conditioned on {ζdet ≤ rζ}, with probability at least 1− p1\n‖ 1 α ∑\nt\nΨℓtwt ′‖ ≤ q((rζ)λ+ + λ+cur)\n√\nβ α + ǫ\n≤ [q(rζ)f √ β\nα + qg\n√\nβ α + 0.01rcurζ]λ − cur.\n2. Let p2 := 2n exp(− αǫ 2 32(q2ηrλ+)2 ). Conditioned on {ζdet ≤ rζ}, with probability (w.p.) at least 1− p2,\n‖ 1 α ∑\nt\nwtwt ′‖ ≤ β α q2λ+ + ǫ ≤ [β α q2f + 0.01rcurζ]λ − cur.\n3. Let p3 := 2n exp(− αǫ 2\n32b2 prob\n) with bprob := ηr((rζ)2λ+ + λ+cur + 2(rζ) √ λ+λ+cur). Condi-\ntioned on {ζdet ≤ rζ}, with probability at least 1− p3,\n‖EcurEcur′( 1\nα Ψℓtℓ\n′ tΨ)Ecur,⊥Ecur,⊥ ′‖\n≤ (rζ)2λ+ + (rζ) 2 √\n1− (rζ)2 λ+undet + ǫ\n≤ [(rζ)2f + (rζ) 2 √\n1− (rζ)2 χ+ 0.01rcurζ]λ\n− cur.\n4. Conditioned on {ζdet ≤ rζ}, w.p. at least 1− p3, λmin(A) ≥ (1− (rζ)2)λ−cur − ǫ = [1− (rζ)2 − 0.01rcurζ]λ−cur 5. Conditioned on {ζdet ≤ rζ}, w.p. at least 1− p3,\nλmax(A⊥) ≤ ((rζ)2λ+ + λ+undet) + ǫ ≤ [(rζ)2f + χ+ 0.01rcurζ]λ−cur.\n6. Conditioned on {ζdet ≤ rζ}, with probability at least 1− p3,\nλmax(A⊥) ≥ (1− (rζ)2 − (rζ)2 √\n1− (rζ)2 )λ+undet − ǫ.\n7. Conditioned on {ζdet ≤ rζ}, w.p. at least 1− p3, λmax(A) ≥ (1− (rζ)2)λ+cur − ǫ\n= [(1− (rζ)2)g − 0.01rcurζ]λ−cur.\n8. Conditioned on {ζdet ≤ rζ}, w.p. at least 1− p3,\nλmax(A) ≤ λ+cur + (rζ)2λ+ + 1\n1− r2ζ2 (rζ) 2λ+undet + ǫ\n≤ [g + (rζ)2f + (rζ) 2\n1− (rζ)2 χ+ 0.01rcurζ]λ − cur.\nProof. The proof is in Section 10. ⊠\nCorollary 9.7. Consider the setting of Lemma 9.6. Assume\n1. r(rζ) ≤ 0.0001, and r(rζ)f ≤ 0.01. Since rcur ≤ r, this implies that rcurζ ≤ 0.0001, and\n2. β ≤ ( (1−rcurζ−χ) 2\n)2\nmin ( (rcurζ) 2\n4.1q2g2 , (rcurζ) q2f\n)\nα.\nUsing these and using g ≥ 1, g ≤ f , χ ≤ 1 (these hold by definition), with probability at least 1− p1 − p2 − 4p3,\n‖H‖ ≤ [2.02qg √ β\nα +\nβ α q2f + 0.08rcurζ]λ − cur\n≤ [0.75(1− rζ − χ)rcurζ + 0.08rcurζ]λ−cur ≤ 0.83rcurζλ−cur,\nλmax(A⊥) ≤ [χ+ 0.02rcurζ]λ−cur, λmax(A⊥) ≥ [χ− 0.02rcurζ]λ−cur, λmin(A) ≥ [1− 0.0101rcurζ]λ−cur, λmax(A) ≤ [g + 0.0202rcurζ]λ−cur, λmax(A) ≥ [g − 0.02rcurζ]λ−cur.\nLemma 9.8. Consider the setting of Corollary 9.7. In addition, also assume that\n1. ĝ = 1.01g + 0.0001 and\n2. χ ≤ min ( g−0.0001 1.01g+0.0001 − 0.0001, 1− rcurζ − 0.080.25 ) .\nLet λ̂i := λi(D̂). Then, with probability at least 1− p1 − p2 − 4p3, the following hold.\n1. When Gundet is not empty: λ̂1 λ̂rcur ≤ ĝ, λ̂1 λ̂rcur+1 > ĝ, and λ̂rcur+1 ≥ λthresh.\n2. When Gundet is empty: λ̂1 λ̂rcur ≤ ĝ and λ̂rcur+1 < λthresh < λ̂rcur .\n3. If r̂cur = rcur, then ζcur ≤ ‖H‖λmin(A)−λmax(A⊥)−‖H‖ ≤ 0.75rcurζ + 0.08rcurζ (1−rcurζ−χ) ≤ rcurζ.\nProof.\nFact 9.9. From the bound on χ, χ ≤ 1−0.0001 ≤ 1−rcurζ. Thus, using Corollary 9.7, λmin(A) > λmax(A⊥) and so λrcur(D) = λmin(A), λrcur+1(D) = λmax(A⊥), and λ1(D) = λmax(A). Recall: λ1(.) is the same as λmax(.).\nProof of item 1. Recall that D̂ and D are defined in (10) and (11). Using Weyl’s inequality, Fact 9.9, and Corollary 9.7, with the probability given there,\nλ̂1 λ̂rcur ≤ λmax(A) + ‖H‖ λmin(A)− ‖H‖ ≤ g + 0.86rcurζ 1− 0.85rcurζ\nand λ̂1\nλ̂rcur+1 > λmax(A) − ‖H‖ λmax(A⊥) + ‖H‖ > g − 0.85rcurζ χ+ 0.85rcurζ\nThus, if\ng + 0.85rcurζ 1− 0.85rcurζ ≤ ĝ ≤ g − 0.85rcurζ χ+ 0.85rcurζ (12)\nholds, we will be done. The above requires χ to be small enough so that the lower bound is not larger than the upper bound and it requires ĝ to be appropriately set. Both are ensured by the assumptions in the lemma.\nSince Gundet is not empty, λ + undet = χλ − cur > λ − Thus, using Weyl’s inequality followed by Corollary 9.7, with the probability given there,\nλ̂rcur+1 ≥ λrcur+1(D)− ‖H‖ = λmax(A⊥)− ‖H‖ ≥ [χ− 0.02rcurζ]λ−cur − 0.83rcurζλ−cur ≥ (1− 0.85rcurζ)λ− > λthresh\nProof of item 2. Since Gundet is empty, λ + undet = 0 and so χ = 0. Thus, using Corollary 9.7, with probability given there,\nλ̂rcur+1 ≤ λrcur+1(D) + ‖H‖ = λmax(A⊥) + ‖H‖ ≤ 0 + 0.02rcurζλ− + ‖H‖ ≤ 0.85rcurζλ− < λthresh,\nλ̂rcur ≥ λrcur(D)− ‖H‖ = λmin(A)− ‖H‖ ≥ λ−cur − 0.085rcurζλ−cur ≥ (1− 0.85rcurζ)λ− > λthresh,\nand λ̂1\nλ̂rcur ≤ λmax(A) + ‖H‖ λmin(A)− ‖H‖ ≤ g + 0.85rcurζ 1− 0.85rcurζ ≤ ĝ\nProof of item 3. Using Fact 9.5 and Corollary 9.7, since r̂cur = rcur is assumed, we get\nζcur ≤ [0.75(1− rcurζ − χ)rcurζ + 0.08rcurζ]λ−cur\nλ−cur[1− 0.0101rcurζ − χ− 0.02rcurζ − 0.83rζ]\n≤ 0.75(1− rζ − χ)rcurζ + 0.08rcurζ (1− rcurζ − χ) ≤ rcurζ (13)\nThe last inequality used the bound on χ. ⊠"
    }, {
      "heading" : "9.3 Proof of Theorem 3.3",
      "text" : "The theorem is a direct consequence of using (9) and applying Lemma 9.8 for each of the k steps with the substitutions given in Definition 9.2; along with picking α appropriately. A detailed proof is in Sec. 11."
    }, {
      "heading" : "10 Proof of Hoeffding lemma, Lemma 9.6",
      "text" : "The following lemma, which is a modification of [3, Lemma 8.15], will be used in our proof. It is proved in Sec. 11. The proof uses [3, Lemma 2.10].\nLemma 10.1. Given ζdet ≤ rζ.\n1. ‖ΨGdet‖ ≤ rζ and ‖ΨGcur‖ ≤ 1.\n2. √ 1− (rζ)2 ≤ σi(Rcur) = σi(ΨGcur) ≤ 1 and √ 1− (rζ)2 ≤ σi(ΨGundet) ≤ 1\n3. ‖Ecur′ΨGundet‖ ≤ (rζ)2 √\n1− (rζ)2\n4.\nΨΣΨ = [ΨGdet ΨGcur ΨGundet] [\nΛdet 0 0\n0 Λcur 0 0 Λundet\n][\nΨGdet ΨGcur ΨGundet\n]′\nwith λmax(Λdet) ≤ λ+, λ−cur ≤ λmin(Λcur) ≤ λmax(Λcur) ≤ λ+cur, λmax(Λundet) ≤ λ+undet.\n5. Using the first four claims, it is easy to see that\n(a) ‖Ecur,⊥′ΨΣΨEcur,⊥‖ ≤ (rζ)2λ+ + λ+undet (b) ‖Ecur,⊥′ΨΣΨEcur‖ ≤ (rζ)2λ+ + (rζ)\n2√ 1−(rζ)2 λ+undet\n(c) ‖ΨΣ‖ ≤ (rζ)λ+ + λ+cur and ‖ΨΣM1,t′‖ ≤ q((rζ)λ+ + λ+cur) (d) ‖M1,tΣ‖ ≤ qλ+ and ‖M1,tΣM1,t′‖ ≤ q2λ+\nIf Ĝdet = Gdet = [.], then all the terms containing (rζ) disappear.\n6. λmin(A+B) ≥ λmin(A) + λmin(B) 7. Let at := P ′ℓt, at,det := Gdet′ℓt, at,cur := Gcur′ℓt and at,undet := Gundet′ℓt. Also let\nat,rest := [at,cur ′,at,undet′]′. Then ‖at,rest‖2 ≤ rηλ+cur and ‖at,det‖2 ≤ ‖at‖2 ≤ rηλ+.\n8. σmin(Ecur,⊥′ΨGundet)2 ≥ 1− (rζ)2 − (rζ) 2√\n1−(rζ)2 .\nThe following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.\nCorollary 10.2. Given an α-length sequence {Zt} of random Hermitian matrices of size n × n, a r.v. X , and a set C of values that X can take. For all X ∈ C, (i) Zt’s are conditionally independent given X; (ii) P(b1I Zt b2I|X) = 1 and (iii) b3I E[ 1α ∑\nt Zt|X ] b4I. For any ǫ > 0, for all X ∈ C,\nP\n(\nλmax\n(\n1\nα\n∑\nt\nZt\n)\n≤ b4 + ǫ ∣ ∣ ∣X\n)\n≥ 1− n exp ( −αǫ2 8(b2 − b1)2 ) ,\nP\n(\nλmin\n(\n1\nα\n∑\nt\nZt\n)\n≥ b3 − ǫ ∣ ∣ ∣ X\n)\n≥ 1− n exp ( −αǫ2 8(b2 − b1)2 ) .\nCorollary 10.3. Given an α-length sequence {Zt} of random matrices of size n1 × n2. For all X ∈ C, (i) Zt’s are conditionally independent given X; (ii) P(‖Zt‖ ≤ b1|X) = 1 and (iii) ‖E[ 1\nα\n∑\nt Zt|X ]‖ ≤ b2. For any ǫ > 0, for all X ∈ C,\nP\n(\n∥ ∥ ∥ ∥ 1\nα\n∑\nt\nZt\n∥ ∥ ∥ ∥ ≤ b2 + ǫ ∣ ∣ ∣X\n)\n≥ 1− (n1 + n2) exp (−αǫ2 32b1 2 ) .\nProof of Lemma 9.6. Recall that we are given Ĝdet that was computed using (some or all) yt’s for t ≤ t∗ and that satisfies ζdet ≤ rζ. From (2), yt is a linear function of ℓt. Thus, we can let X := {ℓ1, ℓ2, . . . ℓt∗} denote all the random variables on which the event {ζdet ≤ rζ} depends. In each item of this proof, we need to lower bound the probability of the desired event conditioned on ζdet ≤ rζ. To do this, we first lower bound the probability of the event conditioned on X that is such that X ∈ {ζdet ≤ rζ}. We get a lower bound that does not depend on X as long as X ∈ {ζdet ≤ rζ}. Thus, the same probability lower bound holds conditioned on {ζdet ≤ rζ}. Fact 10.4. For an event E and random variable X , P(E|X) ≥ p for all X ∈ C implies that P(E|X ∈ C) ≥ p.\nProof of Lemma 9.6, item 1. Let\nterm := 1\nα\n∑\nt\nΨℓtwt ′ =\n1\nα\n∑\nt\nΨℓtℓ ′ tM1,t ′M2,t ′\nSince Ψ is a function of X , since ℓt’s used in the summation above are independent of X and E[ℓtℓt ′] = Σ,\nE[term|X ] = 1 α ∑\nt\nΨΣM1,t ′M2,t ′\nNext, we use Cauchy-Schwartz for matrices: ∥\n∥ ∥ ∥ ∥\nα ∑\nt=1\nXtYt ′\n∥ ∥ ∥ ∥ ∥ 2 ≤ λmax ( α ∑\nt=1\nXtXt ′ ) λmax ( α ∑\nt=1\nYtYt ′ )\n(14)\nUsing (14), with Xt = ΨΣM1,t ′ and Yt = M2,t, followed by using\n√\n‖ 1 α\n∑ t XtX ′ t‖ ≤\nmaxt ‖Xt‖, Assumption 1.2 with At ≡ I, and Lemma 10.1,\n‖E[term|X ]‖ ≤ max t\n‖ΨΣM1,t′‖ √ β\nα\n≤ q((rζ)λ+ + λ+cur) √ β\nα\nfor all X ∈ {ζdet ≤ rζ}. To bound ‖Ψℓtw′t‖, rewrite it as Ψℓtw′t = [ΨGdetat,det + ΨGrestat,rest][a ′ t,detG ′ det + a ′ t,restG ′ rest]M ′ 1,tM ′ 2,t. Thus, using ‖M2,t‖ ≤ 1, ‖M1,tP ‖ ≤ q < 1, and Lemma 10.1,\n‖Ψℓtw′t‖ ≤ qrη((rζ)λ+ + λ+cur + (rζ) √ λ+λ+cur + √ λ+λ+cur)\nholds w.p. one when {ζdet ≤ rζ}. Finally, conditioned on X , the individual summands in term are conditionally independent. Using matrix Hoeffding, Corollary 10.3, followed by Fact 10.4, the result follows.\nProof of Lemma 9.6, item 2.\nE[ 1\nα\n∑\nt\nwtw ′ t|X ] =\n1\nα\n∑\nt\nM2,tM1,tΣM1,t ′M2,t ′\nBy Lemma 10.1, ‖M1,tΣM1,t′‖ ≤ q2λ+. Thus, using Assumption 1.2 with At ≡ M1,tΣM1,t′,\n‖E[ 1 α ∑\nt\nwtw ′ t|X ]‖ ≤\nβ α q2λ+.\nUsing Assumption 1.2 and Lemma 10.1,\n‖wtw′t‖ = ‖M2,tM1,tPat‖2 ≤ q2ηrλ+. Conditional independence of the summands holds as before. Thus, using Corollary 10.3 and Fact 10.4, the result follows.\nProof of Lemma 9.6, item 3.\nE[ 1\nα\n∑\nt\nEcurEcur ′ Ψℓtℓt ′ ΨEcur,⊥Ecur,⊥ ′‖|X ]\n= EcurEcur ′ ΨΣΨEcur,⊥Ecur,⊥ ′\nUsing Lemma 10.1, ‖EcurEcur′ΨΣΨEcur,⊥Ecur,⊥′‖ ≤ (rζ)2λ++ (rζ) 2√ 1−(rζ)2 λ+undet when {ζdet ≤ rζ}. Also, ‖Ecur′Ψℓtℓt′ΨEcur,⊥‖ ≤ ‖Ψℓtℓt′Ψ‖ ≤ ηr((rζ)2λ+ + λ+cur + 2(rζ) √\nλ+λ+cur) := bprob holds w.p. one when {ζdet ≤ rζ}. In the above bound, the first inequality is used to get a loose bound, but one that will also apply for the proofs of the later items given below. The rest is the same as in the proofs of the earlier parts.\nProof of Lemma 9.6, item 4. Using Ostrowski’s theorem,\nλmin(E[A|X ]) = λmin(Ecur′Ψ(Σ)ΨEcur) ≥ λmin(Ecur′ΨGcurΛcurGcur′ΨEcur) = λmin(RcurΛcurRcur ′)\n≥ λmin(RcurRcur′)λmin(Λcur) ≥ (1− (rζ)2)λ−cur for all X ∈ {ζdet ≤ rζ}. Ostrowski’s theorem is used to get the second-last inequality, while Lemma 10.1 helps get the last one.\nAs in the proof of item 3, ‖Ecur′Ψℓtℓt′ΨEcur‖ ≤ ‖Ψℓtℓt′Ψ‖ ≤ bprob holds w.p. one when {ζdet ≤ rζ}. Conditional independence of the summands holds as before. Thus, by matrix Hoeffding, Corollary 10.2, the result follows.\nProof of Lemma 9.6, item 5. By Lemma 10.1,\nλmax(E[A⊥|X ]) = λmax(Ecur,⊥′ΨΣΨEcur,⊥) ≤ ((rζ)2λ+ + λ+undet)\nwhen {ζdet ≤ rζ}. The rest of the proof is the same as that of the previous part. Proof of Lemma 9.6, item 6. Using Ostrowski’s theorem, λmax(E[A⊥|X ]) ≥ λmax(Ecur,⊥′ΨGundetΛundetGundet′ΨEcur,⊥) ≥ λmin(Ecur,⊥ ′ ΨGundetGundet ′ ΨEcur,⊥)λmax(Λundet). By definition, λmax(Λundet) = λ + undet. By Lemma 10.1, λmin(Ecur,⊥ ′ ΨGundetGundet ′ ΨEcur,⊥) = σmin(Ecur,⊥ ′ ΨGundet)\n2 ≥ (1− (rζ)2 − (rζ)\n2√ 1−(rζ)2 ) when {ζdet ≤ rζ}. The rest of the proof is the same as above.\nProof of Lemma 9.6, item 7. Using Ostrowski’s theorem and Lemma 10.1, λmax(E[A|X ]) ≥ λmax(Ecur ′ ΨGcurΛcurGcur ′ ΨEcur) ≥ λmin(RcurRcur′)λmax(Λcur) ≥ (1 − (rζ)2)λ+cur when {ζdet ≤ rζ}. The rest of the proof is the same as above. ⊠"
    }, {
      "heading" : "11 Detailed Proof of Theorem 3.3 and Proof of Lemma 10.1",
      "text" : "Proof of Theorem 3.3. Recall that we need to show that ζk ≤ rkζ. Assume the substitutions given in Definition 9.2. We will use induction.\nConsider a k < ϑ. For the k-th step, assume that ζi ≤ riζ for i = 1, 2, . . . , k − 1. Thus, using (9), ζdet ≤ rζ and so Lemma 9.8 is applicable. We first show that r̂k = rk and that Algorithm 1 does not stop (proceeds to (k+1)-th step). From Algorithm 1, r̂k = rk if λ̂1\nλ̂rk ≤ ĝ, and λ̂1 λ̂rk+1 > ĝ. Also\nit will not stop if λ̂rk+1 ≥ λthresh. Since k < ϑ, Gundet is not empty. Thus, item 1 of Lemma 9.8 shows that all these hold. Hence r̂k = rk and algorithm does not stop w.p. at least 1−p1−p2−4p3. Thus, by item 3 of the same lemma, with the same probability, ζk ≤ rkζ. Now consider k = ϑ. We first show r̂k = rk and that Algorithm 1 does stop, i.e., ϑ̂ = ϑ. This will be true if λ̂1 λ̂rk\n≤ ĝ and λ̂rk+1 < λthresh. For k = ϑ, Gundet is empty. Thus, item 2 of Lemma 9.8 shows that this holds w.p. at least 1− p1 − p2 − 4p3. Thus, by item 3 of the same lemma, with the same probability, ζk ≤ rkζ.\nThus, using the union bound, w.p. at least 1 − ϑ(p1 + p2 + 4p3), r̂k = rk and ζk ≤ rkζ for all k. Using (8), this implies that SE ≤ rζ with the same probability. Finally, the choice α ≥ α0, implies that p1 ≤ 1ϑ2n−10, p2 ≤ 1ϑ2n−10, p3 ≤ 1ϑ2n−10. Hence SE ≤ rζ w.p. at least 1− 12n−10. We work this out for p1 below. The others follow similarly. Recall that p1 = 2n exp(−α ǫ 2\n32b2 prob\n), ǫ = 0.01(rζ)λ− and bprob = ηrq((rζ)λ+ + λ+cur +\n(rζ) √ λ+λ+cur + √ λ+λ+cur). Thus, b2prob (λ−)2 ≤ (4ηrmax(q(rζ)f, qg, q\n√ fg, q(rζ) √ fg))2 ≤\n16η2r2 max(q(rζ)f, qg, q √ fg)2\nThus, α ǫ 2\n32b2 prob\n≥ 32·16(0.01)2 η2r2(11 logn+logϑ) (rζ)2 max(q(rζ)f, qg, q √ fg) (0.01(rζ)) 2 32·16η2r2 max(q(rζ)f,qg,q √ fg)2\n≥ 11 logn+ logϑ. Thus, p1 ≤ 1ϑ2n−10. ⊠\nProof of Lemma 10.1. The first claim is obvious. The next two claims follow using the following lemma:\nLemma 11.1 ([3], Lemma 2.10). Suppose that P , P̂ and Q are three basis matrices. Also, P and P̂ are of the same size, Q′P = 0 and ‖(I − P̂ P̂ ′)P ‖ = ζ∗. Then,\n1. ‖(I − P̂ P̂ ′)PP ′‖ = ‖(I − PP ′)P̂ P̂ ′‖ = ‖(I − PP ′)P̂ ‖ = ‖(I − P̂ P̂ ′)P ‖ = ζ∗ 2. ‖PP ′ − P̂ P̂ ′‖ ≤ 2‖(I − P̂ P̂ ′)P ‖ = 2ζ∗ 3. ‖P̂ ′Q‖ ≤ ζ∗\n4. √ 1− ζ2∗ ≤ σi ( (I − P̂ P̂ ′)Q ) ≤ 1\nUse item 4 of Lemma 11.1 and the fact that Gdet′Gcur = 0 and Gdet′Gundet = 0 to get the second claim.\nFor the third claim, notice that Ecur′ΨGundet = R−1curGcur ′ ΨGundet = R−1curGcur ′ĜdetĜdet′Gundet. since Ψ2 = Ψ and Gcur′Gundet = 0. Using the second claim, ‖R−1cur‖ ≤ 1σmin(Rcur) ≤ 1 1−(rζ)2 . Use item 3 of Lemma 11.1 and the facts that Gcur ′Gdet = 0 and Gundet ′Gdet = 0 to bound ‖Gcur′Ĝdet‖ and ‖Ĝdet′Gundet‖ respectively.\nThe fourth claim just uses the definitions. The fifth claim uses the previous claims and the assumptions on Mt from Assumption 1.2. The sixth claim follows using Weyl’s inequality.\nThe second last claim: We show how to bound at,rest: ‖at,rest‖2 = ‖at,cur‖2 + ‖at,undet‖2 ≤ ∑\nj∈Gcur ηλj + ∑ j∈Gundet ηλj ≤ rηλ+cur (since λj ≤ λ+cur for all the j’s being summed over). The other bounds follow similarly.\nLast claim:\nσmin(Ecur,⊥ ′ ΨGundet) 2\n= λmin(Gundet ′ ΨEcur,⊥Ecur,⊥ ′ ΨGundet) = λmin(Gundet ′ Ψ(I −EcurEcur′)ΨGundet)\n≥ λmin(Gundet′ΨΨGundet)− λmax(Gundet ′ ΨEcurEcur ′ ΨGundet)\n= σmin(ΨGundet) 2 − ‖Ecur′ΨGundet‖\n≥ 1− (rζ)2 − (rζ) 2 √\n1− (rζ)2 .\nThe last inequality follows using the second and the third claim. ⊠"
    } ],
    "references" : [ {
      "title" : "Finite sample approximation results for principal component analysis: A matrix perturbation approach",
      "author" : [ "B. Nadler" ],
      "venue" : "The Annals of Statistics, vol. 36, no. 6, 2008.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Real-time robust principal components’ pursuit",
      "author" : [ "C. Qiu", "N. Vaswani" ],
      "venue" : "Allerton Conf. on Communication, Control, and Computing, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Recursive robust pca or recursive sparse recovery in large but structured noise",
      "author" : [ "C. Qiu", "N. Vaswani", "B. Lois", "L. Hogben" ],
      "venue" : "IEEE Trans. Info. Th., pp. 5007–5039, August 2014.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online matrix completion and online robust pca",
      "author" : [ "B. Lois", "N. Vaswani" ],
      "venue" : "IEEE Intl. Symp. Info. Th. (ISIT), 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online (and Offline) Robust PCA: Novel Algorithms and Performance Guarantees",
      "author" : [ "J. Zhan", "B. Lois", "H. Guo", "N. Vaswani" ],
      "venue" : "Intnl. Conf. Artif. Intell. and Stat. (AISTATS), 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Stochastic optimization of pca with capped msg",
      "author" : [ "R. Arora", "A. Cotter", "N. Srebro" ],
      "venue" : "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 1815–1823.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A stochastic pca and svd algorithm with an exponential convergence rate",
      "author" : [ "O. Shamir" ],
      "venue" : "arXiv:1409.2848, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online principal components analysis",
      "author" : [ "C. Boutsidis", "D. Garber", "Z. Karnin", "E. Liberty" ],
      "venue" : "Proc. ACM-SIAM Symposium on Discrete Algorithms (SODA), 2015, pp. 887–901.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The fast convergence of incremental pca",
      "author" : [ "A. Balsubramani", "S. Dasgupta", "Y. Freund" ],
      "venue" : "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 3174–3182.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online pca with spectral bounds",
      "author" : [ "Z. Karnin", "E. Liberty" ],
      "venue" : "Proce. Conference on Computational Learning Theory (COLT), 2015, pp. 505–509.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Memory limited, streaming pca",
      "author" : [ "I. Mitliagkas", "C. Caramanis", "P. Jain" ],
      "venue" : "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 2886–2894.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Matrix rank minimization with applications",
      "author" : [ "M. Fazel" ],
      "venue" : "PhD thesis, Stanford Univ, 2002.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candes", "B. Recht" ],
      "venue" : "Found. of Comput. Math, , no. 9, pp. 717–772, 2008.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E.J. Candès", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "Journal of ACM, vol. 58, no. 3, 2011.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rank-sparsity incoherence for matrix decomposition",
      "author" : [ "V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "SIAM Journal on Optimization, vol. 21, 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Robust matrix decomposition with sparse corruptions",
      "author" : [ "D. Hsu", "S.M. Kakade", "T. Zhang" ],
      "venue" : "IEEE Trans. Info. Th., Nov. 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Non-convex robust pca",
      "author" : [ "P. Netrapalli", "U N Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain" ],
      "venue" : "Neural Info. Proc. Sys. (NIPS), 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Provably correct recursive projected compressive sensing (reprocs) for dynamic robust pca: A correlated-pca reformulation",
      "author" : [ "N. Vaswani", "B. Lois", "P. Narayanamurthy" ],
      "venue" : "http://www.ece.iastate.edu/long_RobSubTrack_3.pdf, submitted to ICASSP 2017.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "The rotation of eigenvectors by a perturbation. iii",
      "author" : [ "C. Davis", "W.M. Kahan" ],
      "venue" : "SIAM J. Numer. Anal., vol. 7, pp. 1–46, Mar. 1970.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Found. Comput. Math., vol. 12, no. 4, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "Compressed sensing, pp. 210–268, 2012.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Eigenvalue computation in the 20th century",
      "author" : [ "G.H. Golub", "H.A. Van der Vorst" ],
      "venue" : "Journal of Computational and Applied Mathematics, vol. 123, no. 1, pp. 35–65, 2000.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "P. Netrapalli", "P. Jain", "S. Sanghavi" ],
      "venue" : "Symposium on Theory of Computing (STOC), 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Alternating direction algorithms for l1 problems in compressive sensing",
      "author" : [ "Z. Lin", "M. Chen", "Y. Ma" ],
      "venue" : "Tech. Rep., University of Illinois at Urbana-Champaign, November 2009. 9",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", see [1] and references therein.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.",
      "startOffset" : 121,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.",
      "startOffset" : 121,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.",
      "startOffset" : 121,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.",
      "startOffset" : 121,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : ", [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 12,
      "context" : ", [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 13,
      "context" : "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component’s magnitude is correlated with lt.",
      "startOffset" : 98,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component’s magnitude is correlated with lt.",
      "startOffset" : 98,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component’s magnitude is correlated with lt.",
      "startOffset" : 98,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "One key application where it occurs is in foreground-background separation for videos consisting of a slow changing background sequence (modeled as lying close to a low-dimensional subspace) and a sparse foreground image sequence consisting typically of one or more moving objects [14].",
      "startOffset" : 281,
      "endOffset" : 285
    }, {
      "referenceID" : 13,
      "context" : "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components’ pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components’ pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components’ pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "A third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5].",
      "startOffset" : 223,
      "endOffset" : 229
    }, {
      "referenceID" : 4,
      "context" : "A third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5].",
      "startOffset" : 223,
      "endOffset" : 229
    }, {
      "referenceID" : 17,
      "context" : "We refer the reader to [18] to understand this application.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "The following lemma [4] shows that, with Assumption 1.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "[[4], Lemmas 5.",
      "startOffset" : 1,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "7, or [4].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 18,
      "context" : "Proof: The proof involves a careful application of the sin θ theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin θ bound.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "Proof: The proof involves a careful application of the sin θ theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin θ bound.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "As a result we can apply the matrix Hoeffding inequality [20] to bound the perturbation between the observed data’s empirical covariance matrix and that of the true data.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "39 of [21] in places where one can apply a concentration of measure result to ∑ t atat ′/α (which is at r × r matrix), and by matrix Bernstein [20] elsewhere, it should be possible to further reduce the sample complexity to cmax((qf)r logn, f(r+ logn)).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : "39 of [21] in places where one can apply a concentration of measure result to ∑ t atat ′/α (which is at r × r matrix), and by matrix Bernstein [20] elsewhere, it should be possible to further reduce the sample complexity to cmax((qf)r logn, f(r+ logn)).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "This assumption can be understood as a generalization of the eigen-gap condition needed by the block power method, which is a fast algorithm for obtaining the k top eigenvectors of a matrix [22].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "Algorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS.",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Algorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS.",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "The one introduced in [3] assumed that the clusters were known to the algorithm (which is unrealistic).",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "The one studied in [5] has an automatic cluster estimation approach, but, one that needs a larger lower bound on α compared to what Algorithm 1 needs.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : ", nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : ", nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components’ pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 14,
      "context" : "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components’ pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components’ pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components’ pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 16,
      "context" : "If we use the time complexity from [17], then finding the span of the top k singular vectors of an n ×m matrix takes O(nmk) time.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "Thus, if θ is a constant, both simple-EVD and c-EVD need O(nαr) time, whereas, Alt-Min-RPCA needs O(nαr) time per iteration [17].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "We used the matrix Hoeffding inequality [20] to obtain our results.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Moreover, as done in [5] (for ReProCS), the mutual independence of lt’s can be easily replaced by a more practical assumption of lt’s following autoregressive model with almost no change to our assumptions.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "The solution to the latter problem helps to greatly simplify the proof of correctness of ReProCS for online dynamic RPCA [18].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "References [1] B.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] C.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] C.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] B.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] R.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] O.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] C.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Z.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] I.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] V.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] C.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] G.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Z.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "1 sin θ theorem Davis and Kahan’s sin θ theorem [19] studies the rotation of eigenvectors by perturbation.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "1 (sin θ theorem [19]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "1 We use the sin θ theorem [19] from Corollary 8.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "In the next lemma, we bound the terms in the bound on SE(P̂ ,P ) using the matrix Hoeffding inequality [20].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "The following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "The following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "1 ([3], Lemma 2.",
      "startOffset" : 3,
      "endOffset" : 6
    } ],
    "year" : 2016,
    "abstractText" : "Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as “data-dependent noise”. We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes.",
    "creator" : "LaTeX with hyperref package"
  }
}