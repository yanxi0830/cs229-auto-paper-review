{
  "name" : "1610.04900.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convergence rate of stochastic k-means",
    "authors" : [ "Cheng Tang", "Claire Monteleoni" ],
    "emails" : [ "tangch@gwu.edu", "cmontel@gwu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Stochastic k-means, including online [6] and mini-batch k-means [20], has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [20] and scikit-learn [18]. Figure 1 demonstrates the efficiency of stochastic k-means against batch k-means on the RCV1 dataset [15]. The advantage is clear, and the results raise some natural questions: Can we characterize the convergence rate of stochastic k-means? Why do the algorithms appear to converge to different ∗Department of Computer Science, George Washington University\nar X\niv :1\n61 0.\n04 90\n0v 1\n[ cs\n.L G\n] 1\n“local optima”? Why and how does mini-batch size affect the quality of the final solution? Our goal is to address these questions rigorously.\nGiven a discrete dataset of n input data points, denoted by X := {x, x ∈ Rd}, a common way to cluster X is to first select a set of k centroids, denoted by C = {cr ∈ Rd, r ∈ [k]}, and assign each x to a centroid. Data points assigned to the same centroid cr, form a cluster Ar ⊂ X, and ∪r∈[k]Ar = X; we let A := {Ar, r ∈ [k]} denote the resulting clustering. This is center-based clustering, where each clustering is specified by a tuple (C,A). The k-means cost of (C,A) is defined as: φX(C,A) := ∑ r∈[k] ∑ x∈Ar ‖x−cr‖\n2. The k-means clustering problem is cast as the optimization problem: minC,A φX(C,A). This is an NP-hard problem [17].\nHowever, if either C or A is fixed, the problem can be easily solved. Fixing the set of k centroids C, the smallest k-means cost is achieved by choosing the clustering that assigns each point x to it closest center, which we denote by C(x) := mincr∈C ‖x− cr‖. That is,\nφX(C) := min A φX(C,A) = ∑ r∈[k] ∑ C(x)=cr ‖x− cr‖2 (1)\nThis clustering can also be induced by the Voronoi diagram of C, denoted by V (C) := {V (cr), r ∈ [k]}, where\nV (cr) := {x ∈ Rd, ‖x− cr‖ ≤ ‖x− cs‖, ∀s 6= r}\nClustering A induced by V (C) is such that ∀Ar ∈ A,Ar = V (cr) ∩ X. Subsequently, we will use V (C)∩X to denote this induced clustering. Likewise, fixing a k-clustering A of X, the smallest k-means cost is achieved by setting the new centers as the mean of each cluster, denoted by m(Ar),\nφX(A) := min C φX(C,A) = ∑ r∈[k] ∑ x∈Ar ‖x−m(Ar)‖2 (2)\nBatch k-means or Lloyd’s algorithm [16], one of the most popular clustering heuristics [11], essentially proceeds by finding the solution to (1) and (2) alternatively: At t = 0, it initializes the position of k centroids, C0, via a seeding algorithm. ∀t ≥ 1, it alternates between two steps, Step 1 Fix Ct−1, find At such that\nAt = arg min A φX(C t−1, A) = V (Ct−1) ∩X\nStep 2 Fix At, find Ct such that\nCt = arg min C φX(C,A t) = m(At)\nHowever, Step 1 requires computation of the closest centroid to every point in the dataset. Even with fast implementations such as [8], which reduces the computation for finding the closest centroid of each point, the per-iteration running time is still O(n), making it a computational bottleneck for large datasets.\nTo scale up batch k-means, “stochastic approximation” was proposed [6, 20], which we present as Algorithm 1. 1 The main idea is, at each iteration, the centroids are updated using one (online [6]) or a few (mini-batch [20]) randomly sampled points, denoted by St, instead of the entire dataset X. At every iteration, Algorithm 1 first approximates Step 1 and 2 using a random sample St of constant size, then it computes Ct by interpolating between Ct−1 and Ĉt. Thus, stochastic k-means never directly clusters X but keeps updating its set of centroids using constant sized random samples St, so the per-iteration computation cost is reduced from O(n) in the batch case to O(1).\nNotation: In the paper, superscripts index a particular clustering, e.g., At denotes the clustering at the t-th iteration in Algorithm 1 (or batch kmeans); subscripts index individual clusters or centroids: cr denotes the r-th centroid in C corresponding to the r-th cluster Ar. We use letter n to denote cardinality, n = |X|, nr = |Ar|, etc. conv(X) denotes the convex hull of set X. We let φ(C,A) denote the k-means cost of (C,A); we let φ(C), φ(A) denote the induced (optimal) k-means cost of a fixed C or A; as a shorthand, we often move the superscript (subscript) on the input of φ(·) to φ, e.g., we use φt to denote φ(Ct), and φtr to denote the cost of the r-th cluster at t. We denote the largest k-means cost on X as φmax and the smallest k-means cost as φopt. Finally, we let π(·) to denote a permutation on [k]."
    }, {
      "heading" : "2 Related work and our contributions",
      "text" : "Our work builds on analysis of batch k-means, which is notoriously difficult [7]. A great breakthrough was made recently by Kumar and Kannan [14], where\n1In Claim 1 of the Appendix, we formally show Algorithm 1 subsumes both online and mini-batch k-means.\nthey showed the k-SVD + constant k-means approximation + k-means update scheme efficiently and correctly clusters most data points on well-clusterable instances, and that the algorithm converges to an approximately optimal solution at geometric rate until reaching a plateau. Subsequent progress has been made on relaxing the assumptions [2] and simplifying the seeding [21]. However, these works only study local convergence of batch k-means in the sense that they require that the initial centroids are already close to the optimal solution.\nAlternating Minimization and EM Temporarily abusing notation, we let X ∈ Rd×n be a matrix whose columns represent data points; similarly, let columns of C ∈ Rd×k represent centroids, and let columns of A ∈ Rk×n indicate cluster membership, i.e., Aij = 1 if and only if xi belongs to cluster j. The k-means problem can be equivalently formulated as\nmin C,A ‖X − CA‖2F subject to Aij ∈ {0, 1}, ‖Ai‖2 = 1\nThis is a non-convex low-rank matrix factorization problem. In particular, with the discrete constraint on A, the k-means problem is close to the sparse coding problem (also known as dictionary learning or basis pursuit), where C corresponds to a set of dictionary items and A the coding matrix, and batch\nk-means can be viewed as a variant of Alternating Minimization (AM) for sparsity-constrained matrix factorization. In statistics, the algorithm is also well known to relate to EM for gaussians mixture models. Both AM and EM are popular heuristics for non-convex unsupervised learning problems, ranging from matrix completion to latent variable models, where growing interest and exciting progress towards understanding their convergence behavior are emerging [12, 10, 1, 3].\nHowever, existing analyses of AM or EM do not apply to k-means. For one thing, the prevalent assumptions for AM-style algorithms is incoherence+sparsity, while EM applies to generative models. Both are different from our deterministic assumption on the geometry of the dataset. For another, the [12, 10, 1, 3] all rely on closed form update expression; usually partial gradients are easily derivable in these problems. In contrast, we cannot obtain a closed form update rule for Step 1 of batch k-means, due to the discrete constraint on A. Instead, we work around this problem using geometric insights about k-means update developed from the clustering literature [14, 21].\nNon-convex stochastic optimization For convex problems, stochastic optimization methods are well studied [19]. Much less can be said about non-convex problems. Our analysis of stochastic k-means is influenced by two recent works in non-convex stochastic optimization [9, 4]. The first [9] studies the convergence of stochastic gradient descent (SGD) for tensor decomposition problems, which amounts to finding a local optimum of a nonconvex objective function with only saddle points and local optima. Inspired by their analysis framework, we divide our analysis of Algorithm 1 into two phases, those of global and local convergence, according to the distance from the current solution to the set of all “local optima”. At the local convergence phase, since multiple local optima are present, stochastic noise may drive the algorithm’s iterate off the neighborhood of attraction, and the algorithm may fail to converge locally. To deal with this, we adapted techniques that bound martingale large deviation from [4] to our local convergence analysis; the work in [4] studies the convergence of stochastic PCA algorithms, where the objective function is the non-convex Rayleigh quotient.\nOur contributions Our contributions are two-fold. For users of stochastic k-means, Theorem 1 guarantees that it converges to a local optimum with any reasonable seeding (it only requires the seeds be in the convex hull of the\ndataset) and a properly chosen learning rate, with O(1 t ) expected convergence rate, where the convergence is with respect to k-means objective. In contrast to recent batch k-means analysis [14, 2, 21], it establishes a global convergence result for stochastic k-means, since it applies to almost any initialization C0; it also applies to a wide range of datasets, without requiring a strong clusterability assumption.\nTheoretically, we have three major contributions. First, our analysis provides a novel analysis framework for k-means algorithms, by connecting the discrete optimization approach to that of gradient-based continuous optimization. Under this framework, we identify a “Lipschitz” condition of a local optimum, under which stochastic k-means converges locally. This approach to establish local convergence is similar in-spirit to the local convergence analysis of [1, 3]. Second, we show this “Lipschitz” condition relates to clusterability assumptions of the dataset. Consequently, Theorem 2 shows, just as batch k-means, stochastic k-means can also converge to an optimal k-means solution, under a clusterability assumption similar to [14], with a scalable seeding algorithm developed in [21]. This result extends the batch k-means results on well-clusterable instances [14, 2, 21] to stochastic k-means, and shows the two are equally powerful at finding an optimal k-means solution with strong enough clusterability assumption. Finally, the martingale technique, which we modified from [4], can be applied to future analysis of non-convex stochastic optimization problems.\n3 Revisiting batch k-means, local convergence, and clusterability\nThis section introduces the framework we construct to study k-means updates, under which we characterize the “local optima” of batch k-means and identify a sufficient condition for a local optimum to be a locally stable attractor2 to the algorithm. Then we show how the clusterability of the dataset in fact determines the strength of a local optimum as an attractor.\n2By “locally stable”, we mean within a radius, batch k-means always converges to this local optimum.\nAlgorithm 1 Stochastic k-means Input: dataset X, number of clusters k, mini-batch size m, learning rate ηtr, r ∈ [k], convergence_criterion Seeding: Apply seeding algorithm T on X and obtain seeds C0 = {c01, . . . , c0k}; repeat At iteration t (t ≥ 1), obtain sample St ⊂ X of size m uniformly at random with replacement; set count n̂tr ← 0 and set Str ← ∅, ∀r ∈ [k] for s ∈ St do Find I(s) s.t. cI(s) = C(s) StI(s) ← StI(s) ∪ s; n̂tI(s) ← n̂tI(s) + 1\nend for for ct−1r ∈ Ct−1 do if n̂tr 6= 0 then ctr ← (1− ηtr)ct−1r + ηtrĉtr with ĉtr := ∑ s∈Str s\nn̂tr end if\nend for until convergence_criterion is satisfied\n3.1 Batch k-means as an alternating mapping\nCorresponding to the two steps in an iteration of batch k-means, it alternates between two solution spaces: the continuous space of sets of k centroids, which we denote by {C}, and the finite set of all k-clusterings, which we denote by {A}. Our key observation is that {C} can be partitioned into equivalence classes by the clustering they induce on X, and the algorithm stops if and only if two consecutive iterations stay within the same equivalence class in {C}: for any C, let v(C) denote the clustering induced by its Voronoi diagram, i.e., v is the mapping v(C) := V (C) ∩X, where V (C) ∩X is as defined in Section 1. It can be shown that v is a well-defined function if and only if C is not a boundary point.\nDefinition 1 (Boundary points). C is a boundary point if ∃A ∈ V (C) ∩X s.t. for some r ∈ [k], s 6= r and x ∈ Ar ∪ As, ‖x− cr‖ = ‖x− cs‖.\nWe used “A ∈ V (C)∩X” instead of A = V (C)∩X, since V (C)∩X may induce more than one clustering if C is a boundary point (see Lemma 2). So we abuse notation V (C)∩X to let it be the set of all possible clusterings of C.\nFor now, we ignore boundary points. Then we say C1, C2 are equivalent if they induce the same clustering, i.e., C1 ∼ C2 if v(C1) = v(C2). This construction reveals that {C} can be partitioned into a finite number of equivalence classes; each corresponds to a unique clustering A ∈ {A}.\nAn iteration of batch k-means can be viewed as applying the composite mapping m◦v : {C} → {C}, where Step 1 goes from {C} to {A} via mapping v, and Step 2 goes from {A} to {C} via the mean operation m.\n“Local optima” It is well known that batch k-means stops at t when At+1 = At. Since At+1 = v(Ct) = v ◦m(At) and At = v(Ct−1), this implies it stops at t if and only if v(Ct) = v(Ct−1), or equivalently, v ◦m(At) = At. Similarly, we can derive the algorithm stops if and only if m(At+1) = Ct, or m ◦ v(Ct) = Ct.\nWe can thus visualize batch k-means as an iterative mapping m ◦ v on {C} that jumps from one equivalence class to another until it stays in the same equivalence class in two consecutive iterations, i.e., v(Ct+1) = v(Ct), and stops at some C∗ ∈ {C∗} (see Figure 3 in the Appendix). This stopping condition also provides a natural way of defining the “local optima” of k-means cost, i.e., they are the fixed points of mapping m ◦ v and v ◦m, which we call stationary solutions.\nDefinition 2 (Stationary solutions). We define C∗ ∈ {C} such that m ◦ v(C∗) = C∗, and call it a stationary point of batch k-means. We let {C∗} denote the set of all stationary points. Similarly, we call A∗ ∈ {A} a stationary clustering if v ◦m(A∗) = A∗, and we let {A∗} denote the set of all stationary clusterings.\nFinally, to characterize local convergence we need to define a suitable measure of distance on each space.\nDefinition 3 (Centroidal distance). For C ′ and C, we define centroidal distance ∆(C ′, C) := minπ:[k]→[k] ∑ r nr‖c′π(r) − cr‖2, where nr = |Ar|.\nDefinition 4 (Clustering distance). For v(C ′) and v(C), we define the clustering distance ClustDist(v(C ′), v(C)) := maxr |A′ π(r) 4Ar|\nnr , where A′ := v(C ′),\nA = v(C), 4 denotes set difference, and π is the permutation attaining ∆(C ′, C).\nBoth distances are asymmetric, non-negative, and evaluates to zero if and only if two sets of centroids (clusterings) coincide. If C∗ is a stationary point,\nthen for any solution C, ∆(C,C∗) upper bounds the difference of k-means objective, φ(C)− φ(C∗) (Lemma 18).\nRemark For clarity of presentation, we have ignored the fact that k-means may produce degenerate solutions, where one or more clusters may be empty; similarly, the definitions of stationary solutions and centroidal distance here ignore the possible existence of boundary points. In our analysis, we used more general definitions to handle these issues, whose details are provided in the Appendix.\n3.2 A sufficient condition for the local convergence of batch k-means\nWhen is a stationary point also a locally stable attractor on {C}? We propose to characterize stability as a local Lipschitz condition on mapping v(·): we require ClustDist(v(C), v(C∗)) to be upper bounded by ∆(C,C∗) locally.\nDefinition 5. We call C∗ a (b0, α)-stable stationary point if for any C ∈ {C} such that ∆(C,C∗) ≤ b′φ∗, b′ ≤ b0, we have ClustDist(v(C), v(C∗)) ≤ b 5b+4(1+φ(C)/φ∗) , with b ≤ αb′ for some α ∈ [0, 1).\nGeneralizing combinatorial arguments about batch k-means update in [14, 21], we show that the definition above is indeed a sufficient condition for local convergence of batch k-means.\nLemma 1. Let C∗ be a (b0, α)-stable stationary point. For any C such that ∆(C,C∗) ≤ b′φ∗, b′ ≤ b0, apply one step of batch k-means update on C results in a new solution C1 such that ∆(C1, C∗) ≤ αb′φ∗.\nNeighborhood of attraction By Lemma 1, we can view b0 as the radius of the neighborhood of attraction and α the strength of the attractor, since it determines the convergence rate. A special case of (b0, α)-stability is when α = 0, which implies v(C) = v(C∗) if C is within radius b0 to C∗. In this case, batch k-means converges in one iteration. Per our construction in Section 3.1, b0 in this case is the radius of the equivalence class that maps to clustering A∗ = v(C∗). In general, when α > 0, we expect the radius b0 to be much larger.\nOur characterization of local convergence of batch k-means does not depend on a specific clusterability assumption, unlike previous work [14, 2, 21].\nInstead, we will see that clusterability implies local Lipschitzness of mapping v."
    }, {
      "heading" : "3.3 Local Lipschitzness and clusterability",
      "text" : "As discussed in Section 3.1, boundary points are problematic to the definition of mapping v, but how likely do they arise in practice? We answer this question by revealing the geometric implication of a boundary point, which will lead us to discover the connection between local Lipschitzness of v and clusterability.\nConsider any C ∈ {C}, and let A′ ∈ V (C) ∩X: for a point x ∈ A′r ∪ A′s, s 6= r, let x̄ denote the projection of x onto the line joining cr, cs, we define\n∆rs(C) := min x∈A′r∪A′s\n|‖x̄− cr‖ − ‖x̄− cs‖|\nDefinition 6 (δ-margin). For any C, we say V (C) has a δ-margin with respect to X if ∃A ∈ V (C) ∩X such that minr,s 6=r ∆rs(C) = δ.\nLemma 2. The following are equivalent 1. C is a boundary point 2. V (C) has a zero margin with respect to X 3. |V (C)∩X| > 1, i.e., the clustering determined by V (C) is not unique. Thus, a set of k centroids C is a boundary point in space {C} if and only if there is a data point x ∈ X that sits exactly on the bisector of two centroids in C. We believe a symmetric configuration like this has a low probability to arise in practice, due to random perturbations in the real world, e.g., computational error. With this insight, we define a general dataset to be one that is free of boundary stationary points.\nAssumption A [General dataset] X is a general dataset if ∀C∗ ∈ {C∗}, C∗ has δ-margin with δ > 0.\nNote {C∗} is a finite set, since {A∗} is finite and we have the relation C∗ = m(A∗) (Lemma 5). Thus, Assumption A is a mild condition, as it only requires that a finite subset of the continuous space {C} to be free of boundary points, hence the name “general”.\nWe show that for a general dataset, every stationary point is a locally stable attractor (in fact its neighborhood of attraction is exactly its equivalence\nclass induced by v). In other words, mapping v is always locally Lipschitz on a sufficiently small neighborhood of any stationary point. Moreover, on a general dataset, we can lower bound the centroidal distance between two consecutive k-means iteration, provided the algorithm has not converged. Both results, summarized in Lemma 3, are important building blocks for our proof of Theorem 1.\nLemma 3. If X is a general dataset, then ∃rmin > 0 s.t.\n1. ∀C∗ ∈ {C∗}, C∗ is a (rmin, 0)-stable stationary point.\n2. Let m(A′) /∈ {C∗} for some A′ ∈ {A} and let A′ ∈ V (C ′) ∩ X, then ∆(C ′,m(A′)) ≥ rminφ(m(A′)).\nIn the lemma, rmin is a lower bound on the radius of attraction for points in {C∗}. As discussed below Lemma 1, this radius, although positive, can be very small. The radius of an attractor, as we show next, can be related to the strength of margin δ.\nAssumption B [f(α)-clusterability] We say a dataset-solution pair (X,C∗) is f(α)-clusterable, if C∗ ∈ {C∗} and C∗ has δ-margin s.t. ∀r ∈ [k], s 6= r,\nδ ≥ f(α) √ φ∗(\n1√ n∗r + 1√ n∗s ) for α ∈ (0, 1)\nwith f(α) > max{642, 5α+5 256α ,maxr∈[k],s 6=r n∗r n∗s }.\nProposition 1. Suppose (X,C∗) satisfies Assumption (B). Then, for any C such that ∆(C,C∗) ≤ bφ∗ for some b ≤ f(α) 2\n162 , we have maxr∈[k] |Ar4A∗r | n∗r ≤ b f(α)3 .\nThat is, C∗ is (f(α) 2\n162 , α) -stable.\nf(α)-clusterability assumption is a simplified version of the proximity assumption in [14]. It essentially requires that δ = Ω( √ kσmax) for a stationary point C∗, where σmax is the maximal standard deviation of an individual cluster. It serves as an example showing how clusterability implies local Lipschitzness of v. Furthermore, Proposition 1 reveals that the larger f(α) is, the larger the radius of attraction.\n4 Convergence analysis of stochastic k-means This section has three components. We first describe the technique we developed that helps us establish the local convergence of stochastic k-means, which will be used in the proof of both our main theorems. Then, we provide proof outlines of Theorem 1 and Theorem 2, respectively. Throughout our analysis, we consider learning rate of the form:\nηtr = η t =\nc′\nto + t , ∀r ∈ [k] (3)"
    }, {
      "heading" : "4.1 Local convergence in the presence of stochastic noise",
      "text" : "Unlike a convex problem, the difficulty of establishing local convergence in our case is, if the algorithm’s solution is driven off the current neighborhood of attraction by stochastic noise at any iteration, it may be drawn to a different attractor. Fixing a (b0, α)-stable stationary point C∗, suppose the algorithm is within the neighborhood of attraction of C∗ at time τ . The event “the algorithm’s iterate is within radius b0 to C∗ up to t− 1” can be formalized as:\nΩt := {∆(Ci, C∗) ≤ b0φ∗,∀τ ≤ i < t} (4)\nLetting t→∞ leads to the following definition:\nΩ∞ := {∆(Ci, C∗) ≤ b0φ∗, ∀i ≥ τ} (5)\nClearly, local convergence to C∗ implies Ω∞; Lemma 1 also requires Ω∞ as a prerequisite. So we need to show Pr(Ω∞) ≈ 1, even with stochastic noise."
    }, {
      "heading" : "4.1.1 Inequality for a martingale-like process",
      "text" : "We use ∆t := ∆(Ct, C∗) as a shorthand and we let EΩt [·] denote the expectation conditioning on Ωt. Let Ω represent the sample space starting from τ , then Ωt+1 ⊂ Ωt ⊂ Ω,∀t > τ . Conditioning on Ωt, we can apply Lemma 1 to get\n∆t ≤ ∆t−1(1− β to + t\n) + [ c′\nto + t ]2 t1 +\n2c′\nto + t t2 |Ωt\nwhere with probability 1, β ≥ 2, and the stochastic noise terms t1, t2 are of order O(φt−1). Therefore, (∆t) is a supermartingale-like process with bounded stochastic noise, conditioning on Ωt.\nTo exploit this conditional structure, we partition the failure event Ω\\Ω∞, i.e., the event that the algorithm eventually escapes this neighborhood, as a disjoint union of events Ωt \\Ωt+1, and then our task becomes upper bounding Pr(Ωt \\Ωt+1) for all t. To achieve this, we first derive an upper bound on the conditional moment generating function EΩt [expλ∆t] as a function of b0φ∗ and the noise terms, using ideas in [4]. Then applying conditional Markov’s inequality, we get\nPr(Ωt \\ Ωt+1) = Pr{∆t > b0φ∗|Ωt} ≤ EΩt [expλ∆\nt]\nexpλb0φ∗\nSince the inequality holds for all λ > 0, we can choose λ as a function of ln t, which enables us to bound Pr(Ωt \\ Ωt+1) by δ(t+1)2 , for all t ≥ 1, δ > 0, with sufficiently large c′ and to in (3). This implies\nP (Ω∞) = 1− ∑ t≥τ Pr(Ωt \\ Ωt+1) ≥ 1− δ\nEssentially, this is our variant of martingale large deviation bound. Comparing to related work [4, 9], our technique yields a tighter bound on the failure probability than [9], which uses Azuma’s inequality, and is much simpler than [4]; the latter constructs a complex nested sample space and applies Doob’s inequality, whereas ours simply uses Markov’s inequality. In addition, our technique allows us to explore the noise dependence on Ωt, which leads to a weaker dependence of parameter to on the initial condition boφ∗.\nWe believe this technique can be useful for other non-convex analysis of stochastic methods. We provide one example here. Our current analysis considers the flat learning rate in (3). However, in practice the following adaptive learning rate is commonly used:\nηtr := n̂tr∑ i≤t n̂ i r\n(6)\nWe conjecture that stochastic k-means with the above learning rate also has O(1\nt ) convergence, as supported by our experiments (see Section 5).\nHowever, it is difficult to incorporate (6) into our analysis: n̂ir is a random quantity whose probability depends on the clustering configuration v(Ci−1). To establish O(1\nt ) convergence, we need to show Eηtr ≈ Θ(1t ). Without\nadditional information, this is hopeless, as ηtr depends on information of the entire history of the process. But conditioning on Ωt, we can show that nir ≈ n∗r, for all r ∈ [k], i ≥ τ . Using this relation, we may approximate Eηtr.\nSince our technique allows this conditional dependence, we may extend our local convergence analysis to incorporate the case where ηtr is adaptive.\nFinally, conditioning on Ωt, we can combine Lemma 1 with the standard arguments in stochastic gradient descent [19, 1, 3] to obtain the O(1\nt ) local\nconvergence rate (Theorem 3): EΩt [∆(Ct, C∗)] = O(1t )."
    }, {
      "heading" : "4.1.2 Proof sketch of main theorems",
      "text" : "Equipped with the necessary ingredients, we are ready to explain the analysis that leads to our main theorems.\nTheorem 1 is a global convergence result. To prove it, we divide our analysis of Algorithm 1 into two phases, that of global convergence and local convergence, indicated by the distance from the current solution to stationary points, ∆(Ct, C∗).\nWe define global convergence phase as a time interval of random length τ such that ∀t < τ , ∀C∗ ∈ {C∗}, ∆(Ct, C∗) > 1\n2 rminφ ∗ (rmin as defined in Lemma 3). During this phase, we obtain a lower bound on the expected decrease in k-means objective (Lemma 14):\nE[φt+1 − φt|Ft] ≤ −2ηt+1pt+1min(φt − φ̃t) + (ηt+1)26φt\nHere, φ̃t := ∑\nr ∑ x∈v(ctr) ‖x−m(v(ctr))‖2 and ptmin := minr,ptr(m)>0 p t r(m), with\nptr(m) = Pr{ct−1r is updated at t with sample size m} = 1− (1− ntr n\n)m. Thus, the term pt+1min(φt − φ̃t) lower bounds the drop in k-means objective. For pt+1r (m) > 0, by the discrete nature of cluster assignment, ntr ≥ 1. So pt+1min ≥ 1− (1− 1n)\nm ≥ 1− e−mn . On the other hand, φt − φ̃t = ∆(Ct,m(v(Ct))) by Lemma 21. Thus, to\nlower bound the decrease by zero, we only need to lower bound ∆(Ct,m(v(Ct))). The idea is, in case m(v(Ct))) is a non-stationary point, by part 2 of Lemma 3, ∆(Ct,m(v(Ct))) > 1\n2 rminφ(m(v(C t))). Otherwise, m(v(Ct))) is a stationary point, and by definition of the global convergence phase, the same lower bound applies, which implies ptmin(φt− φ̃t) is lower bounded by a positive constant in the global convergence phase. Since we choose ηt := Θ(1\nt ), the expected per\niteration drop of cost is of order Ω(1 t ), which forms a divergent series; after a sufficient number of iterations the expected drop can be arbitrarily large. We conclude that ∆(Ct, C∗) cannot be bounded away from zero asymptotically, since the k-means cost of any clustering is positive (Lemma 15). Hence, starting from any initial point C0, the algorithm will always be drawn to a\nstationary point, ending its global convergence phase after a finite number of iterations, i.e., Pr(τ <∞) = 1.\nAt the beginning of the local convergence phase, ∆(Cτ , C∗) ≤ 1 2 rminφ ∗ for some C∗ ∈ {C∗}. Again by Lemma 3, the algorithm is within the neighborhood of attraction of C∗, and thus we can apply the local convergence result in Theorem 3. Combining both phases leads us to Theorem 1.\nTheorem 1. Suppose X satisfies Assumption (A). Fix any 0 < δ < 1 e , if we run Algorithm 1 with arbitrary C0 such that C0 ⊂ conv(X), and any mini-batch size m ≥ 1, and choose learning rate ηt = c′\nt+to such that\nc′ > max{ φmax (1− e−mn )rminφopt , 1 (1− e 4m5n ) }\nto ≥ 768(c′)2(1 + 1\nrmin )2n2 ln2\n1\nδ\nThen there exists events G(A∗), parametrized by A∗, such that\nPr{∪A∗∈{A∗}[k]G(A ∗)} ≥ 1− δ\nFor any stationary clustering A∗, we have ∀t ≥ 0,\nE{φt − φ∗|G(A∗)} = O(1 t )\nRemark: ∪A∗∈{A∗}G(A∗) is contained in the event that Algorithm 1 converges to a stationary point. Thus, Theorem 1 implies that, with any reasonable initialization and sufficiently large c′, to, stochastic k-means converges globally almost surely; conditioning on global convergence to a stationary point A∗, the convergence rate is O(1\nt ) in expectation. Also note φmax is\nupper bounded, since C0 ⊂ conv(X) implies Ct ⊂ conv(X), ∀t ≥ 1 (see Claim 2). Finally, note that Theorem 1 establishes global convergence to a local optimum, but it does not guarantee that stochastic k-means converges to the same local optimum as its batch counterpart, even with the same initialization.\nOur next theorem complements Theorem 1 in the sense that it provides local convergence to a global optimum of k-means problem: it shows that if we use Algorithm 2 as our seeding algorithm and the optimal clustering satisfies f(α)-clusterability, then stochastic k-means converges to a global\nAlgorithm 2 Buckshot seeding [21] {νi, i ∈ [mo]} ← sample mo points from X uniformly at random with replacement {S1, . . . , Sk} ←run Single-Linkage on {νi, i ∈ [mo]} until there are only k connected components left C0 = {ν∗r , r ∈ [k]} ← take the mean of the points in each connected component Sr, r ∈ [k]\noptimum of k-means objective at rate O(1 t ). Its proof has three parts: First, we show f(α)-clusterability implies (b0, α)-stability, as stated in Proposition 1. Second, we show C0 found by Algorithm 2 is within the neighborhood of attraction of the optimal solution with high probability, by adapting Theorem of [21] with the additional assumption that\nf(α) ≥ 5\n√ 1\n2wmin ln(\n2\nξp∗min ln\n2k\nξ ) (7)\nwhere wmin and p∗min are geometric properties of clustering v(C∗) (see definitions in Appendix). Finally, combining these with Theorem 3 completes the proof.\nTheorem 2. Suppose (X,C∗) satisfies Assumption (B) and f(α) in addition satisfies (7) for any 0 < α < 1, ξ > 0. Fix β ≥ 2, and 0 < δ < 1\ne . If we\ninitialize Co in Algorithm 1 by Algorithm 2, with mo satisfying log 2k ξ\np∗min < mo <\nξ 2 exp{2(f(α) 4 − 1)2w2min}, and running Algorithm 1 with learning rate of the form ηt = c ′\nt+to and mini-batch size m so that\nm > ln(1−\n√ α)\nln(1− 4 5 p∗min)\nc′ > β\n2[1− √ α− e− 45mp∗min ]\nand to ≥ 867(c′)2n2 ln2 1\nδ\nThen ∀t ≥ 1, there exists event Gt ⊂ Ω s.t.\nPr{Gt} ≥ (1− δ)(1− ξ) and\nE[φt|Gt]− φ∗ ≤ E[∆(Ct, C∗)|Gt] = O( 1\nt )\nRemark: Theorem 2 in fact applies to any stationary point satisfying f(α)clusterability, which includes the optimal k-means solution. Interestingly, we cannot provide guarantee for online k-means (m = 1) here. Our intuition is, instead of allowing stochastic k-means to converge to any stationary point as in Theorem 1, it studies convergence to a target stationary point; a larger m provides more stability to the algorithm and prevents it from straying away from the target.\nAlgorithmic parameters: The constant on the bound of convergence rate for both theorems, which we hide in Big-O notation, in fact scales with c′. This suggests one should choose c′ to be neither too large nor too small. The lower bound on c′, in turn, depends on other parameters like m and k (implicitly). For larger m and smaller k, the lower bound on c′ is smaller."
    }, {
      "heading" : "5 Experiments",
      "text" : "To verify the O(1\nt ) global convergence rate of Theorem 1, we run stochastic\nk-means with varying learning rate, mini-batch size, and k on RCV1 [15]. The\ndataset has manually categorized 804414 newswire stories with 103 topics, where each story is a 47236-dimensional sparse vector; it was used in [20] for empirical evaluation of mini-batch k-means. We experimented with both flat learning rate in (3) and the adaptive learning rate (6), which we refer to as BBS-rate, since it was proposed by authors of [6, 20].\nFigure 2 shows the convergence in k-means cost of stochastic k-means algorithm over 100 iterations for varying m and k; fix each pair (m, k), we initialize Algorithm 1 with a same set of k random centroids and run stochastic k-means with varying learning rate parameters (c′, to), and we average the performance of each learning rate setup over 5 runs to obtain the original convergence plot. Figure 2b is an example of a convergence plot before transformation. The dashed black line in each log-log figure is φ\n0−φmin t\n(φmin is the lowest empirical k-means cost), a function of order Θ(1t ). To compare the performance of stochastic k-means with this baseline, we first transform the original φt vs t plot to that of φt − φmin vs t. By Theorem 1, E[φt−φ∗|G(A∗)] = O(1\nt ), so we expect the slope of the log-log plot of φt−φ∗\nvs t to be at least as large as that of Θ(1 t ). Although we do not know the exact cost of the stationary point, since the algorithm has reached a stable phase over 100 iterations, as illustrated by Figure 2b, we simply use φmin, as an estimate of φ∗. Most log-log convergence graphs fluctuate around a line with a slope at least as steep as that of Θ(1\nt ), and do not seem to be sensitive\nto the choice of learning rate in our experiment. Note in some plots, such as Figure 2a, the initial phase is flat. This is because we force the plot to start at φ0 − φmin instead of their true intercept on the y-axis. BBS-rate exhibits similar behavior to our flat learning rates."
    }, {
      "heading" : "6 Discussion",
      "text" : "This work provides the first analysis of the convergence rate of stochastic kmeans, but several questions remain unanswered. First, our analysis applies to the flat learning rate in (3) while adaptive learning rate in (6) is more common in practice. From our experiments, we conjecture that O(1\nt ) convergence can\nalso be attained in the latter case. As discussed in Section 4.1.1, the key question is whether we can show the adaptive rate, a random quantity that depends on all information prior t, is of order Θ(1\nt ). Second, we provide two\nexamples of assumptions that imply Lipschitzness of v. Can we find other assumptions? In particular, is there a spectrum of assumptions in-between our Assumption (A) and (B) that imply different strength of Lipschitzness? We\nalso believe further study of batch k-means can be made using our framework in Section 3.1. For example, we observed that the radius of the neighborhood of an attractor (stationary point) is related to clusterability. Can we use this to relate the number of local attractors to clusterability of the dataset? In addition, if a stationary point has a large radius of attraction in {C}, then intuitively, two different random initializations will likely fall into this same neighborhood. Does this provide another angle to the clustering stability analysis [22, 5]?"
    }, {
      "heading" : "7 Appendix A: supplementary materials to Sec-",
      "text" : "tion 3.1\nIn this part of the Appendix, we provide details on the construction of our framework that are not included in Section 3.1 due to space constraints.\nHandling non-degenerate and boundary points One problem with k-means is it may produce degenerate solutions: if the solution Ct has k centroids, it is possible that data points are mapped to only k′ < k centroids. To handle degenerate cases, starting with |C0| = k, we consider an enlarged clustering space {A}[k], which is the union of all k′-clusterings with 1 ≤ k′ ≤ k. We use the pre-image v−1(A) ∈ {C} to denote the non-boundary points C such that v(C) = A, i.e., these are the set of non-boundary points in the equivalence class induced by clustering A. To include boundary points as well, we devise the operator Cl(·) as the “closure” of an equivalence class v−1(A), which includes all boundary points C ′ such that A ∈ V (C ′) ∩X.\nUsing the above two extensions, we give the robust definition of stationary clusterings and stationary points, which we use in our analysis.\nDefinition 7 (Stationary clusterings). We call A∗ a stationary clustering of X, if m(A∗) ∈ Cl(v−1(A∗)). We let {A∗}[k] ⊂ {A}[k] denote the set of all stationary clusterings of X with number of clusters k′ ∈ [k].\nFor each A∗, we define a matching centroidal solution C∗.\nDefinition 8 (Stationary points). For a stationary clustering A∗ with k′ clusters, we define C∗ = {c∗r , r ∈ [k′]} to be a stationary point corresponding to A∗, so that ∀A∗r ∈ A∗, c∗r := m(A∗r). We let {C∗}[k] denote the corresponding set of all stationary points of X with k′ ∈ [k].\nWith the robust definitions, Figure 3 provides a visualization of batch k-means walking on {C} (and {A}[k]) as an iterative mapping m ◦ v (v ◦m, resp.). In {C}, it jumps from one equivalence class to another until it stays in the same equivalence class in two consecutive iterations.\nNow we extend ∆(·, ·) to include the degenerate cases. Fix a clustering A with its induced k centroids C := m(A), and another set of k′-centroids C ′ (k′ ≥ k) with its induced clustering A′, if |A′| = |A| = k (this means if k′ > k, then C ′ has at least one degenerate centroid), then we can pair the subset of non-degenerate k centroids in C ′ with those in C, and ignore the degenerate centroids. Under this condition, we can extend Definition 3 to include degenerate solutions as well, provided C = m(A) for some clustering A, which is always satisfied in our subsequent analysis.\nA sufficient condition for the local convergence of batch k-means We show batch k-means algorithm has geometric convergence in the local neighborhood of a stable stationary point in the solution space.\nProof of Lemma 1. Without loss of generality, we let π(r) = r, ∀r ∈ [k]. Let ρrout := |∪s 6=r(As∩A∗r)| n∗r , and ρrin := |∪s6=r(Ar∩A∗s)| n∗r ; let ρmax := maxr |Ar4A∗r | n∗r . Clearly, (ρrout + ρ r in) = |Ar4A∗r | n∗r ≤ ρmax, by our definition. Now, similar to [21], we can get ‖m(Ar) − c∗r‖ = ‖ (1−ρrout)n∗rm(Ar∩A∗r)+ ∑ s 6=r ∑ x∈Ar∩A∗s x\n(1−ρrout+ρrin)n∗r − c∗r‖ ≤ 1−ρout 1−ρrout+ρrin ‖m(Ar ∩ A∗r)− c∗r‖+ ‖ ∑ s 6=r ∑ x∈Ar∩A∗s x−c∗r‖ (1−ρrout+ρrin)n∗r And as in [21], we get (1− ρout)‖m(Ar ∩A∗r)−\nc∗r‖ ≤ √ ρroutφ ∗ r√\nn∗r . Now we bound the second term: by Cauchy-Schwarz inequality, ‖ ∑ s 6=r ∑ x∈Ar∩A∗s x − c ∗ r‖2 ≤ ( ∑ s 6=r ∑ x∈Ar∩A∗s 1 2)( ∑ s 6=r ∑ x∈Ar∩A∗s ‖x − c ∗ r‖2) = ρrinn ∗ r ∑ s 6=r ∑ x∈Ar∩A∗s ‖x − c ∗ r‖2. Thus, ∀r ∈ [k], ‖m(Ar) − c∗r‖2 ≤ 4 ρroutφ ∗ r n∗r + 4 ρrin ∑ s 6=r ∑ x∈Ar∩A∗s ‖x−c∗r‖2\nn∗r , where we use the assumption that ρmax < 14 < 1− 1√ 2 . Summing over all r, ∑\nr n ∗ r‖m(Ar)− c∗r‖2 ≤ 4ρmax ∑ r(φ ∗ r + ∑ s 6=r ∑\nx∈Ar∩A∗s ‖x− c∗r‖2). By Lemma 4, ∑ r ∑ s 6=r ∑ x∈Ar∩A∗s ‖x − c ∗ r‖2 can be upper bounded by\nφ(C ′) + ∑ r nr‖m(Ar) − c∗r‖2 = φ(C ′) + ∑\nr(1 − ρrout + ρrin)n∗r‖m(Ar) − c∗r‖2 ≤ φ(C ′) + (1 + ρmax) ∑ r n ∗ r‖m(Ar) − c∗r‖2. Substituting this into the previous\ninequality, we have (1 − 4ρmax(1 + ρmax)) ∑ r n ∗ r‖m(Ar) − c∗r‖2 ≤ 4ρmax(φ∗ +\nφ(C ′)). Thus, ∑\nr n ∗ r‖m(Ar) − c∗r‖2 ≤ ρmax 1−4ρmax(1+ρmax) [φ ∗ + φ(C ′)]. By our assumption, ρmax ≤ b\n5b+4(1+ φ(C) φ∗ )\n< 14 , so ρmax 1−4ρmax(1+ρmax) ≤ ρmax 1−5ρmax ≤ b\n1+ φ(C) φ∗ , and ρmax\n1−4ρmax(1+ρmax) [φ ∗ + φ(C ′)] ≤ bφ∗, since φ(C ′) ≤ φ(C) (equality holds if C is a stationary point).\nLemma 4. Fix any target clustering C∗, and another clustering C with a matching π : [k]→ [k]. Let C ′ := {m(Ar), r ∈ [k]}. Then∑\nr ∑ s6=r ∑ x∈Aπ(r)∩A∗s ‖x− c∗r‖2\n≤ φ(C ′)− ∑ r φ(c∗r ;Aπ(r) ∩A∗r) + ∑ r nr‖m(Ar)− c∗r‖2\nProof. Without loss of generality, we let π(r) = r.\nφ(C ′)− φ(C∗) = ∑ r ∑ x∈Ar ‖x− c∗r‖2 − ∑ r ∑ x∈A∗r ‖x− c∗r‖2\n+ ∑ r ∑ x∈Ar ‖x−m(Ar)‖2 − ∑ r ∑ x∈Ar ‖x− c∗r‖2\nSo ∑\nr ∑ x∈Ar ‖x− c ∗ r‖2− ∑ r ∑ x∈A∗r ‖x− c ∗ r‖2 = φ(C ′)− φ(C∗)− ∑ r ∑ x∈Ar ‖x−\nm(Ar)‖2 + ∑\nr ∑ x∈Ar ‖x− c ∗ r‖2 ≤ φ(C)− φ(C∗) + ∑ r nr‖m(Ar)− c∗r‖2. Now, we\nclaim ∑\nr ∑ x∈Ar ‖x−c ∗ r‖2− ∑ r ∑ x∈A∗r ‖x−c ∗ r‖2 = ∑ r ∑ s 6=r ∑ x∈Ar∩A∗s{‖x−c ∗ r‖2−\n‖x − c∗s‖2}. This is because we can enumerate x using clustering ∪rAr: for each x ∈ Ar, either x ∈ Ar ∩A∗r , then ‖x− c∗r‖2−‖x− c∗r‖2 = 0, or x ∈ Ar ∩A∗s for some s 6= r, which means the difference is ‖x−c∗r‖2−‖x−c∗s‖2 (and this term is positive by optimality of clustering ∪rA∗r fixing {c∗r}). Thus, ∑ r ∑ s 6=r ∑ x∈Ar∩A∗s ‖x− c ∗ r‖2 =∑\nr ∑ x∈Ar ‖x − c ∗ r‖2 − ∑ r ∑ x∈A∗r ‖x − c ∗ r‖2 + ∑ r ∑ s 6=r ∑ x∈Ar∩A∗s ‖x − c ∗ s‖2 ≤\nφ(C ′)− φ(C∗) + ∑ r nr‖m(Ar)− c∗r‖2 + ∑ r ∑ s 6=r ∑ x∈Ar∩A∗s ‖x− c ∗ s‖2 = φ(C ′)−∑\nr φ(c ∗ r;Ar ∩A∗r) + ∑ r nr‖m(Ar)− c∗r‖2, where the last equality is by observing\nthat φ(C∗) = ∑\nr ∑ Ar∩A∗r ‖x− c ∗ r‖2 + ∑ r ∑ s 6=r ∑ x∈Ar∩A∗s ‖x− c ∗ s‖2."
    }, {
      "heading" : "7.1 Local Lipschitzness and clusterability",
      "text" : "Proof of Lemma 2. “1 =⇒ 2” obviously holds since ‖x− cr‖ = ‖x− cs‖ if and only if ‖x̄−cr‖−‖x̄−cs‖. “2 =⇒ 3”: let A ∈ V (C)∩X be the clustering achieving the zero margin, and consider x ∈ Ar ∪ As s.t. ‖x̄− cr‖ − ‖x̄− cs‖; without loss of generality, assume x ∈ Ar according to clustering A, and define A′ to be the same clustering as A for all points in X but x, where it assigns x to As. Then A′ ∈ V (C) ∩X and |V (C) ∩X| ≥ 2 > 1. “3 =⇒ 1”: Suppose otherwise. Then every point x has a unique center that minimizes its distance to it, which means the clustering determined by V (C) ∩A is unique. A contradiction.\nLemma 5. If C∗ ∈ {C∗}, then C∗ = m(A∗), where A∗ ∈ {A∗} and A∗ = v(C∗).\nProof. By definition of stationary points, C∗ = m ◦ v(C∗). Let A = v(C∗), then m(A) = C∗ and v◦m(A) = v(C∗) = A. Thus A ∈ {A∗} by definition of a stationary clustering.\nLemma 6. Fix a clustering A = {A1, . . . , Ak}, and let C ∈ v−1(A). Then ∃δ > 0 such that the following statement holds:\nFor C ′s.t. ∆(·, ·) is defined ,∆(C ′, C) < δ =⇒ C ′ ∈ v−1(A) (8)\nProof. Since C is not a boundary point, ∀x ∈ Ar, r ∈ [k],\n‖x− cr‖ < ‖x− cs‖,∀s 6= r\nSo we can choose δ > 0 s.t. ∀x ∈ Ar, ∀r ∈ [k], s 6= r,\n‖x− cr‖ < ‖x− cs‖ − 2 √ δ\nLet π∗ be a permutation such that ∆(C ′, C) is defined. We have ∀x ∈ Ar, r ∈ [k], s 6= r,\n‖x− c′π∗(s)‖ − ‖x− c ′ π∗(r)‖ ≥ ‖x− cs‖ − ‖c ′ π∗(s) − cs‖\n−(‖x− cr‖+ ‖cr − c′π∗(r)‖) > ‖x− cs‖ − ‖x− cr‖ − 2 √ δ ≥ 0\nwhere the second inequality is by the fact that\nmax r ‖c′π∗(r) − cr‖ 2 ≤ ∆(C ′, C) < δ\nTherefore, V (C ′) ∩X = A, i.e., C ′ ∈ v−1(A).\nLemma 7. Suppose ∀C∗ ∈ {C∗}[k], C∗ is not a boundary point (i.e., suppose Assumption (A) holds). Let C = m(A′) /∈ {C∗}[k] for some A′ ∈ {A} and let C ′ ∈ Cl(v−1(A′)), then ∃δ > 0 s.t. ∆(C ′, C) ≥ δ.\nProof. We prove the lemma by contradiction: suppose ∀δ > 0, ∃C ′ s.t. C ′ ∈ Cl(v−1(A′)) and ∆(C ′, C) < δ. First, we claim that for δ sufficiently small, C must be a boundary point: suppose otherwise, then by Lemma 6, v(C ′) = v(C) = A′, contradicting the fact that C /∈ {C∗}[k]. Let A ∈ V (C)∩X. Since C is a boundary point, ∃r, s and x ∈ Ar ∪As s.t.\n‖x− cr‖ = ‖x− cs‖\nNow, we choose δ > 0 to be sufficiently small so that for any A′ ∈ V (C ′) ∩ X, clustering A′ only differs from A on the assignment of these points sitting on the bisector. This implies C ∈ Cl(v−1(A′)), which implies C is a boundary stationary point, a contradiction.\nLemma 8. If ∀C∗ ∈ {C∗}[k], C∗ is a non-boundary stationary point, that is, C∗ := m(A∗) ∈ v−1(A∗). Then ∃rmin > 0 such that ∀C∗ ∈ {C∗}[k], C∗ is a (rmin, 0)-stable stationary point.\nProof. Fix any k in the range of [k] (we abuse the notation with the same k here). For any C such that ∆(C,C∗) exists (i.e., |C| = k′ ≥ k = |C∗|), we first show ∃r∗ > 0, such that the following statement holds:\n∆(C,C∗) < r∗φ∗ =⇒ C ∈ v−1(A∗)\nSince C∗ is a non-boundary point, there is a permutation πo of [k] such that ∀x ∈ Ar,∀r ∈ [k] and ∀s 6= r,\n‖x− c∗πo(r)‖ < ‖x− c ∗ πo(s) ‖\nWe choose r∗ > 0 so that ∀x ∈ Ar,∀r ∈ [k], ∀s 6= r,\n‖x− c∗πo(r)‖ ≤ ‖x− c ∗ πo(s) ‖ − 2\n√ r∗φ∗, ∀r ∈ [k], s 6= r\nwith equality holds for at least one triple of (x, r, s). Let π∗ be a permutation satisfying\nπ∗ = arg min π ∑ r∈[k] n∗r‖cπ(r) − c∗r‖2\nLet π′ := π∗ ◦ πo. We have ∀(x, r, s) triples,\n‖x− cπ′(s)‖ − ‖x− cπ′(r)‖ ≥ ‖x− c∗πo(s)‖ − ‖c ∗ πo(s)\n− cπ′(s)‖ −(‖x− c∗πo(r)‖+ ‖c ∗ πo(r) − cπ′(r)‖)\n> ‖x− c∗πo(s)‖ − ‖x− c ∗ πo(r) ‖ − 2\n√ r∗φ∗ ≥ 0\nwhere the second inequality is by the fact that\nmax r ‖cπ∗(r) − c∗r‖2 ≤ ∆(C,C∗) < r∗φ∗\n=⇒ max r ‖cπ∗(r) − c∗r‖ <\n√ r∗φ∗\nSince π′ is the composition of two permutations of [k], it is also a permutation of [k], and ∀r, s 6= r, ‖x− cπ′(r)‖ < ‖x− cπ′(s)‖, so C ∈ v−1(A∗). Since by our definition, r∗ is unique for each C∗. Since {C∗}[k] is finite, taking the minimum over all such r∗, i.e., rmin := minC∗∈{C∗}[k] r ∗ completes the proof.\nThe following is a restatement of Lemma 3, which is robust to degeneracy and boundary points.\nLemma 9 (Restatement of Lemma 3). If X is a general dataset, then ∃rmin > 0 s.t.\n1. ∀C∗ ∈ {C∗}[k], C∗ is a (rmin, 0)-stable stationary point.\n2. Let m(A′) /∈ {C∗}[k] for some A′ ∈ {A} and let C ′ ∈ Cl(v−1(A′)), then ∆(C ′,m(A)) ≥ rminφ(m(A)).\nProof. By Lemma 8, ∃r∗min > 0 s.t. ∀C∗, C∗ is r∗min-stable. Furthermore, by Lemma 7, ∃r′min > 0 s.t. ∀C∗, ∆(C ′,m(A)) ≥ r′minφ(m(A)). Let rmin := min{r∗min, r′min} completes the proof.\nProof of Proposition 1. For all r ∈ [k],\nn∗r‖cr − c∗r‖2 ≤ ∆(C,C∗) ≤ bφ∗\nso ‖cr − c∗r‖ ≤ √ bφ∗\nn∗r . Then for all r 6= s,\n‖cr − c∗r‖+ ‖cs − c∗s‖ ≤ √ b √ φ∗(\n1√ n∗r + 1√ n∗s )\n=\n√ b f f √ φ∗( 1√ n∗r + 1√ n∗s ) ≤ √ b f ∆rs ≤ 1 16 ∆rs\nwhere the second inequality is by (B), and the last inequality by our assumption on b. Thus, we may apply Lemma 17 to get |Ar4A ∗ r |\nn∗r ≤ b f3 for all r, proving the first\nstatement. Now by Lemma 18, φ(C) ≤ (b+ 1)φ∗, so\nαb 5αb+ 4(1 + φ(C)φ∗ ) ≥ αb 5αb+ 4(2 + b)\n≥ αb 5αf2/162 + 4(2 + f2/162)\n≥ b f3(α)\n≥ |Ar4A ∗ r |\nn∗r\nwhere the third inequality holds since f ≥ max{642, 5α+5 162α } by (B). This proves the second statement since C∗ is then ( f 2\n162 , α)-stable by Definition 5."
    }, {
      "heading" : "8 Appendix B: Proof of Theorem 3",
      "text" : "Theorem 3. Fix any 0 < δ ≤ 1e . Suppose C\n∗ is (bo, α)-stable. If we run Algorithm 1 with parameters satisfying\nm > ln(1−\n√ α)\nln(1− 45p ∗ min)\nc′ > β\n2[1− √ α− (1− 45p ∗ min) m] with β ≥ 2\nto ≥ 768(c′)2(1 + 1\nbo )2n2 ln2\n1\nδ\nThen if at some iteration i, ∆i ≤ 12boφ ∗, we have ∀t > i,\nPr(Ωt) ≥ 1− δ and\nEt[∆ t] ≤ ( to + i+ 1\nto + t+ 1 )β∆i\n+ (c′)2B\nβ − 1 ( to + i+ 2 to + i+ 1 )β+1\n1\nto + t+ 1\nwhere B := 4(bo + 1)nφ∗."
    }, {
      "heading" : "8.1 Proofs leading to Theorem 3",
      "text" : "In the subsequent analysis, we let\nβt := 2c′min r ptr(m)(1−\nmaxr p t r(m) mins pts(m) √ α)\nwhere\nptr(m) := Pr{ct−1r is updated at t with sample size m} = 1− (1− n t−1 r\nn )m\nSo, βt = 2c′(min\nr ptr(m)−\n√ αmax\ns pts(m))\nThe noise terms appearing in our analysis are: E[ ∑ r ∑ x∈At+1r\n‖x− ĉt+1r ‖2 + φt|Ft] (9)∑ r\nn∗r〈ct−1r − c∗r , ĉtr − E[ĉtr|Ft−1]〉 (10)∑ r n∗r‖ĉtr − c∗r‖2 (11)\nIn the analysis of this section, we use Et[·] as a shorthand notation for E[·|Ωt], where Ωt is as defined in the main paper. Let Ft denote the natural filtration of the stochastic process C0, C1, . . . , up to t.\nThe main idea of the proof is to show that with proper choice with the algorithm’s parameters m, c′, and to, the following holds at every step t:\n• βt ≥ 2 |Ωt\n• Noise terms (10) and (11) are upper bounded by a function of φ∗|Ωt\n• Pr(Ωt \\ Ωt+1) is negligible |Ωt, βt ≥ 2, bounded noise\n• Et[∆t|Ft−1] ≤ (1− β t to+t )∆t−1 + t |Ωt\nwhere t, the noise term, decreases of order O( 1 t2 ).\nLemma 10. Suppose C∗ is (bo, α)-stable. If\nm > ln(1−\n√ α)\nln(1− 45p ∗ min)\nand c′ >\nβ\n2[1− √ α− (1− 45p ∗ min) m]\nThen conditioning on Ωt, we have βt ≥ β.\nProof. Let’s first consider ptr(1) = nt−1r n . Conditioning on Ωt, using the fact that C ∗ is (bo, α)-stable, we have\nnt−1r n ≥ p∗min(1−maxr |Atr4A∗r | n∗r )\n≥ p∗min(1− αbo\n5αbo + 4(1 + φt φ∗ ) ) ≥ 4 5 p∗min\nAnd hence,\nmin r ptr(m) ≥ 1− (1−\n4 5 p∗min) m\nNow,\nβt ≥ 2c′(min r ptr(m)−\n√ α)\n≥ 2c′(1− (1− 4 5 p∗min) m − √ α) ≥ β\nwhere the last inequality is by our requirement on c′ and the fact that 1 − (1 − 4 5p ∗ min) m − √ α > 0 by our requirement on m.\nLemma 11. Suppose C∗ is (bo, α)-stable. Then if we apply one step of Algorithm 1, with m, c′ satisfying conditions in Lemma 10, then conditioning on Ωi,\n∆i ≤ ∆i−1(1− β to + i\n) + [ c′ to + i ]2 ∑ r n∗r‖ĉir − c∗r‖2\n+ 2c′\nto + i ∑ r n∗r〈ci−1r − c∗r , ξir〉\nwhere ξir := ĉir − E[ĉir|Fi−1]. Proof. Let ∆ir := n∗r‖cir − c∗r‖2, so ∆i = ∑ r ∆ i r, and we use ptr as a shorthand for ptr(m). By the update rule of Algorithm 1,\n∆ir = n ∗ r‖(1− ηi)(ci−1r − c∗r) + ηi(ĉir − c∗r)‖2\n≤ n∗r{(1− 2ηi)‖ci−1r − c∗r‖2 + 2ηi〈ci−1r − c∗r , ĉir − c∗r〉 +(ηi)2[‖ci−1r − c∗r‖2 + ‖ĉir − c∗r‖2]}\nLet ξir = ĉir − E[ĉir|Fi−1], where E[ĉir|Fi−1] = (1− pir)ci−1r + pirm(Air). Since\n〈ci−1r − c∗r , ĉir − c∗r〉 = 〈ci−1r − c∗r , E[ĉir|Fi−1] + ξir − c∗r〉 ≤ (1− pir)‖ci−1r − c∗r‖2\n+pir‖m(Air)− c∗r‖‖ci−1r − c∗r‖+ 〈ci−1r − c∗r , ξir〉\nWe have\n∆ir ≤ n∗r{−2ηi[‖ci−1r − c∗r‖2 − (1− pir)‖ci−1r − c∗r‖2\n−pir‖ci−1r − c∗r‖‖m(Air)− c∗r‖] + ‖ci−1r − c∗r‖2\n+2ηi〈ξir, ci−1r − c∗r〉+ (ηi)2[‖ci−1r − c∗r‖2 + ‖ĉir − c∗r‖2]}\n≤ n∗r{− 2c′\nto + i min r ptr‖ci−1r − c∗r‖2\n+ 2c′\nto + i max s pts‖ci−1r − c∗r‖‖m(Air)− c∗r‖\n+‖ci−1r − c∗r‖2 + 2ηi〈ξir, ci−1r − c∗r〉 +(ηi)2[‖ci−1r − c∗r‖2 + ‖ĉir − c∗r‖2]}\nNote ∑ r n∗r‖cir − c∗r‖‖m(Air)− c∗r‖\n≤ √ ( ∑ r n∗r‖ci−1r − c∗r‖2)( ∑ r n∗r‖m(Air)− c∗r‖2)\n= √ ∆i−1∆(m(Ai), C∗) ≤ √ α∆i−1\nwhere the first inequality is by Cauchy-Schwartz and the last inequality is by applying Lemma 1. Finally, summing over ∆ir, we get\n∆i = ∑ r ∆ir ≤ ∆i−1[1− 2c′ to + i min r ptr(1− maxs p t s minr ptr √ α)]\n+[ c′ (to + i) ]2 ∑ r n∗r‖ĉir − c∗r‖2\n+ 2c′\n(to + i)pir ∑ r n∗r〈ci−1r − c∗r , ξir〉\n≤ ∆i−1(1− β to + i\n) + [ c′ to + i ]2 ∑ r n∗r‖ĉir − c∗r‖2\n+ 2c′\nto + i ∑ r n∗r〈ci−1r − c∗r , ξir〉\nThe second inequality is by βt ≥ β, as proven in Lemma 10. Lemma 12. Suppose X satisfies (A1), Co ∈ conv(X), and C∗ is (bo, α)-stable. If we run one step of Algorithm 1, with m, c′ satisfying conditions in Lemma 10, then conditioning on Ωi, we have, for any λ > 0,\nEi{exp{λ∆i}|Fi−1} ≤ exp { λ{(1− β\nt0 + i )∆i−1 +\n(c′)2B (t0 + i)2 + λ(c′)2B2 2(t0 + i)2 } }\nProof. By Lemma 24, we have (10) and (11) are both upper bounded by B. By Lemma 11, we have\nEi{exp(λ∆i)|Fi−1} ≤ expλ[∆i−1(1− β\nto + i ) +\n(c′)2B\n(to + i)2 ]\nEi{expλ 2c′\nto + i ∑ r n∗r〈ci−1r − c∗r , ξir〉|Fi−1}\nSince 2λc′\ni+ t0 ∑ r n∗r〈ξir, ci−1r − c∗r〉 ≤ 2λc′ i+ t0 B\nand Ei{ 2λc ′\ni+t0\n∑ r n ∗ r〈ξir, ci−1r − c∗r〉|Fi−1} = 0, by Hoeffding’s lemma\nEi\n{ exp{ 2λc ′\ni+ t0 ∑ r n∗r〈ξir, ci−1r − c∗r〉|Fi−1}\n}\n≤ exp{λ 2(c′)2B2\n2(i+ t0)2 }\nCombining this with the previous bound completes the proof.\nLemma 13 (adapted from [4]). For any λ > 0, Ei{eλ∆ i−1} ≤ Ei−1{eλ∆ i−1}\nProof. By our partitioning of the sample space, Ωi−1 = Ωi ∪ (Ωi−1 \\ Ωi), and for any ω ∈ Ωi and ω′ ∈ Ωi−1 \\ Ωi, ∆i−1(ω) ≤ boφ∗ < ∆i−1(ω′). Taking expectation over Ωi and Ωi−1, we get Ei{eλ∆ i−1} ≤ Ei−1{eλ∆ i−1}.\nProposition 2. Fix any 0 < δ ≤ 1e . Suppose C ∗ is (bo, α)-stable. If ∆o ≤ 12boφ ∗, and if\nm > ln(1−\n√ α)\nln(1− 45p ∗ min)\nc′ > β\n2[1− √ α− (1− 45p ∗ min) m] with β ≥ 2\nto ≥ 768(c′)2(1 + 1\nbo )2n2 ln2\n1\nδ\nThen P (Ω∞) ≤ δ\n(here we used ∆0 instead of ∆i and treat the starting time, the i-th iteration in Theorem 3 as the zeroth iteration for cleaner presentation).\nProof. By Lemma 12, for any λ > 0,\nEi{eλ∆ i} ≤ Ei{eλ{(1− β to+i )∆i−1} exp{ λ(c ′)2B (to + i)2 + λ2(c′)2B2 2(to + i)2 }\n≤ Ei−1{eλ (1)∆i−1} exp{ λ(c\n′)2B (to + i)2 + λ2(c′)2B2 2(to + i)2 }\nwhere λ(1) = λ(1− βto+i), and the second inequality is by Lemma 13. Similarly, the following recurrence relation holds for k = 0, . . . , i:\nEi−k{eλ (k)∆i−k} ≤ Ei−(k+1){eλ (k+1)∆i−k−1}\nexp{ λ (k)(c′)2B\n(to + i− k)2 +\n(λ(k))2(c′)2B2 2(to + i− k)2 }\nwhere λ(0) := λ, and for k ≥ 1, λ(k) := Πkt=1(1− β to+(i−t+1))λ (0).\nNote (see, e.g., [4]) ∀β > 0, k ≥ 1,\nλ(k) = Πkt=1(1− β to + (i− t+ 1) ) ≤ ( to + i− k + 1 to + i )β\nSince the bound is shrinking as β increases and β ≥ 2,\nλ(k) (t0 + i− k)2 ≤ ( to + i− k + 1 to + i )2\nλ (to + i− k)2 ≤ 4λ (to + i)2\nRepeatedly applying the relation, we get\nEi{eλ∆ i} ≤ eλ(i)∆0 exp{ i−1∑ k=0 ( 4λ(c′)2B (to + i)2 + 4λ2(c′)2B2 2(to + i)2 )}\n≤ exp{λ( to + 1 to + i\n)β∆0 + [λ(c′)2B + λ2(c′)2B2\n2 ]\n4i\n(to + i)2 }\n≤ exp{λ( to + 1 to + i )β 1 2 boφ ∗ + [λ(c′)2B +\nλ2(c′)2B2\n2 ]\n4i\n(to + i)2 }\nThen we can apply the conditional Markov’s inequality, for any λi > 0,\nPr(ω ∈ Ωi \\ Ωi+1) = Pr(∆i > boφ∗|Ωi)\n= Pr(eλi∆ i > eλiboφ ∗ |Ωi) ≤ E[eλi∆ i r |Ωi]\neλiboφ∗\nCombining this with the upper bound on Eieλi∆ i , we get\nPr(ω ∈ Ωi \\ Ωi+1)\n≤ exp{−λi{ 1\n2 bo[2− (\nto + 1 to + i )β]\n−(B + λiB 2\n2 )\n4(c′)2i\n(to + i)2 }} ≤ exp { −λi{ boφ ∗\n2 − (B + λiB\n2\n2 )\n4(c′)2i (to + i)2 } }\nsince i ≥ 1. We choose λi = 1∆ ln (i+1)2 δ with ∆ = boφ∗ 4 , and show that boφ∗\n2 − (B + λiB 2\n2 ) 4(c′)2i (to+i)2 is lower bounded by ∆.\nCase 1: B > λiB22 . We get\n1 2 boφ ∗ − (B + λiB\n2\n2 )\n4(c′)2i\n(to + i)2 ≥ ∆\nsince to ≥ 128(c ′)2(bo+1)n bo = 64(c ′)2(bo+1)nφ∗\n1 2 boφ∗\n= 16(c ′)2B\n1 2 boφ∗\n.\nCase 2: B ≤ λiB22 . We get\n1 2 boφ ∗ − (B + λiB\n2\n2 )\n4(c′)2i\n(to + i)2\n≥ 2∆− λiB2 4(c′)2i\n(to + i)2\n= 2∆− 1 ∆ ln (1 + i)2 δ 4(c′)2B2i (to + i)2\n≥ 2∆− 1 ∆ ln (to + i)\n2\nδ\n4(c′)2B2(to + i)\n(to + i)2\nNow we show 1\n∆ ln\n(to + i) 2\nδ\n4(c′)2B2\nto + i ≤ ∆\nSince\nto + i ≥ to ≥ 768(c′)2(1 + 1\nbo )2n2 ln2\n1\nδ\n= 48(c′)2B2\n(12boφ ∗)2\nln2 1\nδ\nln 1δ ≥ 1, and 16(c′)2B2\n( 1 2 boφ∗)2\n≥ 13 , we can apply Lemma 25 with b = 2, C := 16(c′)2B2\n( 1 2 boφ∗)2\n,\nt := to + i ≥ ( 3Cb−1 ln 1 δ )\n2 b−1 , and get\n4(c′)2B2\n∆2 ln\n(to + i) 2\nδ := 2C ln t+ C ln\n1 δ < tb−1 = to + i\nThat is, 1∆ ln (to+i)2 δ 4(c′)2B2 to+i ≤ ∆. Thus, for both cases,\n2∆− (B + λiB 2\n2 )\n4(c′)2i\n(to + i)2 ≥= ∆\nand hence,\nPr(ω ∈ Ωi \\ Ωi+1) ≤ e− 1 ∆\n(ln (1+i)2\nδ )∆ =\nδ\n(i+ 1)2\nFinally, we have\nPr(∪i≥1Ωi \\ Ωi+1) ≤ ∞∑ i=1 Pr(ω ∈ Ωi \\ Ωi+1) ≤ δ\nProof of Theorem 3. Since the conditions in Proposition 2 holds for any t > i, we apply it and get\nPr(Ωt) ≥ 1− Pr(∪t>iΩt \\ Ωt+1) ≥ 1− δ\nThis proves the first statement. Taking expectation over Ωt conditioning on filtration Ft−1 with respect to the inequality derived in Lemma 11, we get\nEt[∆ t|Ft−1] ≤ ∆t−1(1−\nβ\nto + t ) + [\nc′\nto + t ]2B\nsince (11) is bounded by B by Lemma 24, and since Et{ξtr|Ft−1} = 0, ∀r ∈ [k]. Taking total expectation over Ωt, we get\nEt[∆ t] ≤ Et[∆t−1](1−\nβ\nto + t ) +\n(c′)2B\n(t+ to)2\n≤ Et−1[∆t−1](1− β\nto + t ) +\n(c′)2B\n(t+ to)2\nWe can apply Lemma 26 by letting ut ← Et+to [∆t+to ] (we temporarily change the notation Et[∆t] to Et+to [∆t+to ] to match the notation in Lemma 26), to ← to + i, a← β, and b← (c′)2B\nEt[∆ t] ≤ ( to + i+ 1\nto + t+ 1 )β∆i +\n(c′)2B\nβ − 1 ( to + i+ 2 to + i+ 1 )β+1\n1\nto + t+ 1"
    }, {
      "heading" : "9 Proofs of Theorem 1 and Theorem 2",
      "text" : "One subtlety we need to point out before the proofs is that, in Algorithm 1, the learning rate ηtr as well as the update rule:\nctr ← (1− ηtr)ct−1r + ηtr ĉtr\nis only defined for a cluster r that is “sampled” at the t-th iteration. However, even if the cluster is not “sampled”, i.e., ctr = ct−1r , the same update rule with ĉtr = ct−1r and and the same learning rate still holds for this case. So in our analysis, we equivalently treat each cluster r as updated with learning rate ηtr, and differentiates between a sampled and not-sampled cluster only through the definition of ĉtr.\nProof leading to Theorem 1 Lemma 14. Suppose ∀r ∈ [k], ηtr ≤ ηtmax w.p. 1. Then, E[φt+1 − φt|Ft] ≤ −2 minr,t;pt+1r >0 η t+1 r p t+1 r (φ t − φ̃t) + (ηt+1max)26φt, where φ̃t := ∑ r ∑ x∈At+1r ‖x − m(At+1r )‖2.\nProof of Lemma 14. For simplicity, we denote E[·|Ft] by Et[·] (the same notation is also used as a shorthand to E[·|Ωt] in the proof of Theorem 3; we abuse the notation here).\nEt[φ t+1] = Et[ k∑ r=1 ∑ x∈At+2r ‖x− ct+1r ‖2]\n≤ Et[ ∑ r ∑ x∈At+1r ‖x− ct+1r ‖2]\n= Et[ ∑ r ∑ x∈At+1r ‖x− (1− ηt+1r )ctr − ηt+1r ĉt+1r ‖2]\n= Et[ ∑ r ∑ x∈At+1r (1− ηt+1r )2‖x− ctr‖2\n+(ηt+1r ) 2‖x− ĉt+1r ‖2 + 2ηt+1r (1− ηt+1r )〈x− ctr, x− ĉt+1r 〉]\nwhere the inequality is due to the optimality of clustering At+2 for centroids Ct+1. Since\nEt[ĉ t+1 r ] = (1− pt+1r )ctr + pt+1r m(At+1r )\nwe have\n〈x− ctr, x− ĉt+1r 〉 = (1− pt+1r )‖x− ctr‖2 + pt+1r 〈x− ctr, x−m(At+1r )〉\nPlug this into the previous inequality, we get\nEt[φ t+1] ≤ ∑ r (1− 2ηt+1r )φtr + (ηt+1r )2φtr\n+(ηt+1r ) 2 ∑\nx∈At+1r\n‖x− ĉt+1r ‖2\n+2ηt+1r {(1− pt+1r ) ∑\nx∈At+1r\n‖x− ctr‖2\n+pt+1r ∑\nx∈At+1r\n〈x− ctr, x−m(At+1r )〉}\n= φt − 2 ∑ r ηt+1r p t+1 r φ t r\n+2 ∑ r ηt+1r p t+1 r ∑ x∈At+1r 〈x− ctr, x−m(At+1r )〉}\n+(ηt+1r ) 2φtr + (η t+1 r )\n2 ∑\nx∈At+1r\n‖x− ĉt+1r ‖2\nNow, ∑ x∈At+1r 〈x− ctr, x−m(At+1r )〉\n= ∑\nx∈At+1r\n〈x−m(At+1r ) +m(At+1r )− ctr, x−m(At+1r )〉\n= ∑\nx∈At+1r\n‖x−m(At+1r )‖2\n+ ∑\nx∈At+1r\n〈m(At+1r )− ctr, x−m(At+1r )〉 = φtr\nsince ∑\nx∈At+1r 〈m(A t+1 r )−ctr, x−m(At+1r )〉 = 0, by property of the mean of a cluster.\nThen\nEt[φ t+1] ≤ φt + ∑ r 2ηt+1r p t+1 r (−φtr + φ̃tr)\n+(ηt+1r ) 2[φtr + Et[ ∑ x∈At+1r ‖x− ĉt+1r ‖2]\nNow a key observation is that pt+1r = 0 if and only if cluster At+1r is empty, i.e., degenerate. Since the degenerate clusters do not contribute to the k-means cost,\nwe have ∑\nr;pt+1r >0 φtr = φ\nt, and similarly, ∑\nr;pt+1r >0 φ̃tr = φ̃ t. Therefore,\nEt[φ t+1] ≤ φt − 2 min r,t;pt+1r >0 ηt+1r p t+1 r (φ t − φ̃t)\n+(ηt+1max) 2(Et ∑ r ∑ x∈At+1r ‖x− ĉt+1r ‖2 + φt)\n= φt − 2 min r,t;pt+1r >0 ηt+1r p t+1 r (φ t − φ̃t) + (ηt+1max)26φt\nwhere the last inequality is by Lemma 23.\nLemma 15. Suppose Assumption (A) holds. If we run Algorithm 1 on X with ηt = c ′\nto+t , and to > 1, with any initial set of k centroids C0 ∈ conv(X). Then for\nany δ > 0, ∃t s.t. ∆(Ct, C∗) ≤ δ with C∗ := m(A∗) for some A∗ ∈ {A∗}[k].\nProof of Lemma 15. First note that since {C∗}[k] includes all stationary points with 1 ≤ k′ ≤ k non-degenerate centroids, and at any time t, Ct must have kt ∈ [k] non-degenerate centroids, so there exists C∗ ∈ {C∗}kt ∈ {C∗}[k] such that ∆(Ct, C∗) is well defined. For a contradiction, suppose ∀t ≥ 1, ∆(Ct, C∗) > δ, for all C∗ ∈ {C∗}kt . Then\nCase 1: m(At+1) ∈ {C∗}kt Then\n∆(Ct,m(At+1)) > δ\nby our assumption.\nCase 2: m(At+1) /∈ {C∗}kt Since Ct ∈ Cl(v−1(At+1)) by our definition, applying Lemma 3,\n∆(Ct,m(At+1)) ≥ rminφ(m(At+1))\nSo for both cases, ∆(Ct,m(At+1)) ≥ min{δ, rminφopt}\nLet denote δo := min{δ, rminφ(m(At+1))}, then by Lemma 14,\nE[φt+1 − φt|Ft]\n≤ − 2c′minr∈[k];pt+1r (m)>0 p t+1 r (m)\nt+ 1 + to φt(1− φ̃\nt\nφt )\n+( c′\nt+ 1 + to )26φmax\nNote for pt+1r (m) > 0, by the discrete nature of the dataset, nt+1r n ≥ 1 n , therefore,\nmin r∈[k];ptr(m)>0\nptr(m) ≥ 1− (1− 1 n )m ≥ 1− e− m n\nAlso note\nφt − φ̃t = ∑ r∈[k′] ∑ x∈At+1r ‖x− Ct‖2 − ‖x−m(At+1r )‖2\n= ∑ r ‖ctr −m(At+1r )‖2nt+1r = ∆(Ct,m(At+1)) ≥ δo\nThen ∀t ≥ 1,\nE[φt+1]− E[φt]\n≤ −2c ′(1− e− m n )\nt+ 1 + to δo +\n6φmax(c ′)2\n(t+ 1 + to)2\nSumming up all inequalities,\nE[φt+1]− E[φ0]\n≤ −2c′(1− e− m n )δo ln\nt+ to + 1\nto +\n6φmax(c ′)2\nto − 1\nSince t is unbounded and ln t+to+1to increases with t while 6φmax(c′)2\nto−1 is a constant, ∃T such that for all t ≥ T , Eφt − φ0 ≤ −φ0, which means E[φt] ≤ 0, for all t large enough. This implies the k-means cost of some clusterings is negative, which is impossible. So we have a contradiction.\nProof setup of Theorem 1 The goal of the proof is to show that first, with high probability, the algorithm converges to some stationary clustering, A∗ ∈ {A∗}[k]. We call this event G; formally,"
    }, {
      "heading" : "G := {∃T ≥ 1,∃A∗ ∈ {A∗}[k], s.t. At = A∗,∀t ≥ T}",
      "text" : "Second, we want to establish the O(1t ) expected convergence rate of the algorithm to this stationary clustering A∗.\nTo prove that the event G has high probability, we first consider random variable τ :\nτ := min{t ≥ 0 | min A∗∈{A∗}[k] ∆(Ct,m(A∗)) ≤ 1 2 rminφ ∗}\nThat is, τ is the first time the algorithm “hits” a stationary clustering; τ is a stopping time since ∀t ≥ 0, {τ ≤ t} is Ft-measurable. By Lemma 15\nPr({τ <∞}) = Pr({τ ∈ N}) = Pr(∪T≥0{τ = T}) = 1 (12)\nFixing τ , we denote the stationary clustering that the algorithm “hits” by\nA∗(τ) := arg min A∗∈{A∗}[k] ∆(Cτ ,m(A∗))\nA∗(τ) is well defined; the reason is that when ∆(Cτ ,m(A∗)) ≤ 12rminφ ∗, Aτ = A∗, so there can be only one minimizer. We will prove a subset Go ⊂ G holds with high probability. To do this, we construct Go as a union of disjoint events determined by the realization of τ and A∗(τ): we define events\nGT (A ∗) := {τ = T} ∩ {A∗(τ) = A∗} ∩ {∀t ≥ T,∆t ≤ rminφ∗}\nThen we can represent the event where the algorithm’s iterate converges to a particular stationary clustering A∗ as\nG(A∗) := ∪T≥0GT (A∗)\nFinally, we define Go := ∪A∗∈{A∗}[k]G(A ∗)\nGo ⊂ G since the event ∆t ≤ rminφ∗ implies At = A∗.\nProof of Theorem 1. Fix any (T,A∗), conditioning on {τ = T} ∩ {A∗(τ) = A∗}, since we have\nc′ > φmax\n(1− e− m n )rminφopt\nWe can envoke Lemma 16 to get ∀t < T ,\nE{φt − φ(A∗)|GT (A∗)} = O( 1\nt ) (13)\nNow let’s consider the case t ≥ T . Since by Lemma 3, A∗ is (rmin, 0)-stable, we can apply Theorem 3: in this context, the parameters in the statement of Theorem 3 are bo = rmin, α = 0, p∗min ≥ 1n . Thus, for any\nm ≥ 1\nc′ > β\n2(1− e 4m 5n )\nwith β ≥ 2\nand to ≥ 768(c′)2(1 + 1\nrmin )2n2 ln2\n1\nδ\nthe conditions required by Theorem 3 are satisfied. Then by the first statement of Theorem 3,\nPr({∀t ≥ T,∆t ≤ rminφ∗}|{τ = T} ∩ {A∗(τ) = A∗}) = P (Ω∞|{τ = T} ∩ {A∗(τ) = A∗}) ≥ 1− δ (14)\nand by the second statement of Theorem 3, ∀t > T ,\nE{φt − φ(A∗)|Ωt, {τ = T} ∩ {A∗(τ) = A∗}}\n≤ E{∆(Ct, C∗)|Ωt, {τ = T} ∩ {A∗(τ) = A∗}} = O( 1\nt )\nwhere the first inequality is by Lemma 18. Since Ω∞ ⊂ Ωt, ∀t ≥ 0, this implies\nE{∆(Ct, C∗)|Ω∞, {τ = T} ∩ {A∗(τ) = A∗}}\n= E{∆(Ct, C∗)|GT (A∗)} = O( 1\nt ) (15)\nFinally, we turn to prove Pr(G) is large. Recall\nPr{G} ≥ Pr{Go} = Pr{∪T≥0 ∪A∗∈{A∗}[k] GT (A ∗)} = ∑\nT≥0,A∗∈{A∗}[k]\nPr{GT (A∗)}\nwhere the second equality holds because the events GT (A∗) are disjoint for different pairs of (T,A∗), since the stopping time τ and the minimizer A∗(τ) are unique for each experiment. Since ∑\nT≥0,A∗∈{A∗}[k]\nPr{GT (A∗)}\n= ∑ T,A∗ Pr{Ω∞|{τ = T} ∩ {A∗(τ) = A∗}}\nPr({τ = T} ∩ {A∗(τ) = A∗}) ≥ (1− δ) ∑ T,A∗ Pr({τ = T} ∩ {A∗(τ) = A∗}) = (1− δ)Pr{∪T ∪A∗ {τ = T} ∩ {A∗(τ) = A∗}} = (1− δ)Pr{∪T≥0{τ = T}} = 1− δ\nwhere the inequality is by (14), and the last two equalities are due to the finiteness of {A∗}[k] and by (12), respectively. Therefore, Pr{G} ≥ 1− δ, which completes the proof of the first statement. In addition,\nPr{∪A∗∈{A∗}[k]G(A ∗)}\n= Pr{∪T≥0,A∗∈{A∗}[k]Ω∞ ∩ {τ = T} ∩ {A ∗(τ) = A∗}}\n≥ 1− δ\nwhich proves the second statement. Finally, combining inequalities (13) and (15), we have ∀ ≥ 1 and ∀t ≥ 1,\nE{φt − φ(A∗)|GT (A∗)} = O( 1\nt )\nSince the quantity φt − φ(A∗∗) is independent of T , we reach the conclusion\nE{φt − φ(A∗)|G(A∗)} = O(1 t )\nLemma 16. Suppose the assumptions and settings in Theorem 1 hold, conditioning on any GT (A∗), we have ∀1 ≤ t < T ,\nE{φt − φ(A∗)|GT (A∗)} = O( 1\nt )\nProof. First observe that conditioning on the event GT (A∗), ∆(Ct, C∗) > 12rminφ ∗, ∀t < T . Now we are in a setup similar to that in the proof Lemma 15, and the argument therein will lead us to the conclusion that\nφt − φ̃t > min{1 2 rmin, rmin}φ̃t = 1 2 rminφ̃ t\nProceeding as in Lemma 15, we have conditioning on GT (A∗),\nE[φt|GT (A∗)]\n≤ E[φt−1|GT (A∗)]{1− 2c′minr∈[k];ptr(m)>0 p t r(m)\nt+ to\nrminφopt 2φmax\n}+ ( c ′\nt+ to )26φmax\nsince ∀t ≥ 1,\n1− φ̃ t φt ≥ rmin 2 φ̃t φt ≥ rmin 2 φopt φmax\nNow, since we set\nc′ > φmax\n(1− e− m n )rminφopt\nwe have\n2c′ min r∈[k];ptr(m)>0 ptr(m) rminφopt 2φmax\n≥ 2c′(1− (1− 1 n )m) rminφopt 2φmax\n≥ 2c′(1− e− m n ) rminφopt 2φmax\n> 2 φmax\n(1− e− m n )rminφopt (1− e− m n ) rminφopt 2φmax > 1\nApplying Lemma 26 with\na := 2c′ min r∈[k];ptr(m)>0 ptr(m) rminφopt 2φmax > 1\nb := 6(c′)2φmax (to + t)2\nWe conclude that ∀1 ≤ t < T ,\nE[φt|GT (A∗)] ≤ to + 1\nto + t+ 1 φo +\nb a− 1 ( to + 2 to + 1 )a+1\n1\nto + t+ 1\nSubtracting φ(A∗) from both sides of the equation, we get\nE[φt − φ(A∗)|GT (A∗)] ≤ to + 1 to + t+ 1 (φo − φ(A∗))\n+ b a− 1 ( to + 2 to + 1 )a+1\n1\nto + t+ 1 = O(\n1 t )\nProofs leading to Theorem 2 Here, we additionally define two quantities that characterizes C∗: Let A∗ = v(C∗), we use p∗min := minr∈[k] n∗r n to characterize the fraction of the smallest cluster in A∗ to the entire dataset. We use wr := φr∗ n∗r\nmaxx∈A∗r ‖x−c ∗ r‖2 to characterize the ratio between average and maximal “spread” of cluster A∗r , and we let wmin := minr∈[k]wr."
    }, {
      "heading" : "9.1 Existence of stable stationary point under geometric assumptions on the dataset",
      "text" : "First, we observe that our Assumption (B) implies two lower bounds on ‖c∗r − c∗s‖, ∀r, s 6= r. Let x ∈ A∗r ∩Ats. Split x into its projection on the line joining c∗r and c∗s, and its orthogonal component:\nx = 1\n2 (c∗r + c ∗ s) + λ(c ∗ r − c∗s) + u (16)\nwith u ⊥ c∗r − c∗s. Note λ measures the ratio between departure of the projected point from the mid-point of c∗r and c∗s and the norm ‖c∗r − c∗s‖. By minimality of our definition of margin ∆rs,\n‖x̄− 1 2 (c∗r + c ∗ s)‖ = λ‖c∗r − c∗s‖ ≥ 1 2 ∆rs (17)\nIn addition, since c∗r is the mean of A∗r, we know there exists x ∈ A∗r such that x̄ falls outside of the line segment c∗r − c∗s (or exactly on c∗r in the special case where all points projects on c∗r). Similar holds for c∗s. Thus,\n‖c∗r − c∗s‖ ≥ ∆rs ≥ f(α) √ φ∗(\n1√ n∗r + 1√ n∗s ) (18)\nLemma 17 (Theorem 5.4 of [14]). Suppose (X,C∗) satisfies (B). If ∀r ∈ [k], s 6= r, ∆tr + ∆ t s ≤ ∆rs16 . Then for any s 6= r, |A ∗ r ∩Ats| ≤ b 2 f(α) , where b ≥ maxr,s ∆tr+∆ t s ∆rs .\nThe proof is almost verbatim of Theorem 5.4 of [14]; we include it here for completeness.\nProof. Since the projection of x on the line joining ctr, cts is closer to s, we have\nx(cts − ctr) ≥ 1\n2 (cts − ctr)(cts + ctr)\nSubstituting (16) into the inequality above,\n1 2 (c∗r + c ∗ s)(c t s − ctr) + λ(c∗r − c∗s)(cts − ctr)\n+u(cts − ctr) ≥ 1\n2 (cts − ctr)(cts + ctr) (19)\nSince u ⊥ c∗r − c∗s, let ∆ = ∆ts + ∆tr. We have\nu(cts − ctr) = u(cts − c∗s − (ctr − c∗r)) ≤ ‖u‖∆\nRearranging (19), we have\n1 2 (c∗r + c ∗ s − cts − ctr)(cts − ctr)\n+λ(c∗r − c∗s)(cts − ctr) + u(cts − ctr) ≥ 0\n≡ ∆ 2\n2 +\n∆ 2 ‖c∗r − c∗s‖ − λ‖c∗r − c∗s‖2 +λ∆‖c∗r − c∗s‖+ ‖u‖∆ ≥ 0\nTherefore,\n‖x− c∗r‖ = ‖( 1 2 − λ)(c∗s − c∗r) + u‖ ≥ ‖u‖\n≥ λ ∆ ‖c∗r − c∗s‖2 − ∆ 2\n−1 2 ‖c∗r − c∗s‖ − λ‖c∗r − c∗s‖ ≥ ∆rs‖c∗r − c∗s‖ 64∆\nwhere the last inequality is by our assumption that ∆ ≤ ∆rs16 , and λ ≥ ∆rs 2‖c∗r−c∗s‖ by (17). By previous inequality and our assumption on f , 3 for all s 6= r\n|A∗r ∩Ats| ∆2rs‖c∗r − c∗s‖2\nf∆2 ≤ ∑ x∈A∗r∩Ats ‖x− c∗r‖2\nSo |A∗r ∩ Ats| ≤ ∑ x∈A∗r∩Ats ‖x − c ∗ r‖2 f(∆tr+∆ t s) 2 ∆2rs‖c∗r−c∗s‖2 ≤ fb 2\nf2φ∗( 1 n∗r\n) ( ∑ A∗r∩Ats ‖x − c ∗ r‖2),\nwhere the second inequality is by (18). That is, |A ∗ r∩Ats| n∗r ≤ b2fφ∗ ∑ A∗r∩Ats ‖x− c ∗ r‖2. Similarly, for all s 6= r, |A ∗ s∩Atr| n∗r ≤ b2fφ∗ ∑ A∗s∩Atr ‖x − c ∗ s‖2 Summing over all s 6= r, |Ar4A∗r | n∗r = ρout + ρin ≤ b 2 fφ∗φ ∗ = b 2 f .\nLemma 18. Fix a stationary point C∗ with k centroids, and any other set of k′-centroids, C, with k′ ≥ k so that C has exactly k non-degenerate centroids. We have\nφ(C)− φ∗ ≤ min π ∑ r n∗r‖cπ(r) − c∗r‖2 = ∆(C,C∗)\nProof. Since degenerate centroids do not contribute to k-means cost, in the following we only consider the sets of non-degenerate centroids {cs, s ∈ [k]} ⊂ C and {c∗r , r ∈\n3We use f as a shorthand for f(α) in the subsequent proof.\n[k]} ⊂ C∗. We have for any permutation π,\nφ(C)− φ∗ = ∑ s ∑ x∈As ‖x− cs‖2 − ∑ r ∑ x∈A∗r ‖x− c∗r‖2\n≤ ∑ r ∑ x∈A∗r ‖x− cπ(r)‖2 − ∑ r ∑ x∈A∗r ‖x− c∗r‖2\n= ∑ r n∗r‖cπ(r) − c∗r‖2\nwhere the last inequality is by optimality of clustering assignment based on Voronoi diagram, and the second inequality is by applying the centroidal property in Lemma 21 to each centroid in C∗. Since the inequality holds for any π, it must holds for minπ ∑ r n ∗ r‖cπ(r) − c∗r‖2, which completes the proof.\nProofs regarding seeding guarantee\nLemma 19 (Theorem 4 of [21]). Suppose (X,C∗) satisfies (B). If we obtain seeds from Algorithm 2, then\n∆(C0, C∗) ≤ 1 2\nf(α)2\n162 φ∗\nwith probability at least 1−mo exp(−2(f(α)4 − 1) 2w2min)− k exp(−mop∗min).\nProof. First recall that, as in (18), assumption (B) implies center-separability assumption in Definition 1 of [21], i.e.\n∀r ∈ [k], s 6= r, ‖c∗r − c∗s‖ ≥ f(α) √ φ∗(\n1√ n∗r + 1√ n∗s )\nwith f(α) ≥ maxr∈[k],s 6=r n∗r n∗s . 4 Applying Theorem 4 of [21] with µr = c∗r and νr = c 0 r, we get ∀r ∈ [k], ‖c0r − c∗r‖ ≤ √ f(α)\n2 √ φ∗r n∗r with probability at least 1 −\nmo exp(−2(f(α)4 − 1) 2w2min) − k exp(−mop∗min). Summing over all r, the previous event implies ∑\nr n ∗ r‖c0r − c∗r‖2 ≤ f(α) 4 φ ∗ ≤ 12 f(α)2 162 φ∗, where the last inequality is by\nthe assumption that f ≥ 642 in (B).\nLemma 20. Assume the conditions Lemma 19 hold. For any ξ > 0, if in addition,\nf(α) ≥ 5\n√ 1\n2wmin ln(\n2\nξp∗min ln\n2k\nξ )\n4note: “α” in [21] is defined as minr∈[k],s6=r n∗r n∗s ,which is not to be confused with our “α”.\nIf we obtain seeds from Algorithm 2 choosing\nln 2kξ p∗min < mo < ξ 2 exp{2(f(α) 4 − 1)2w2min}\nThen ∆(C0, C∗) ≤ 12 f(α)2 162 φ∗ with probability at least 1− ξ.\nProof. By Lemma 19, a sufficient condition for the success probability to be at least 1− ξ is:\nmo exp(−2( f(α)\n4 − 1)2w2min) ≤\nξ\n2 and\nk exp(−mop∗min) ≤ ξ\n2 This translates to requiring\n1\np∗min ln\n2k\nξ ≤ mo ≤\nξ 2 exp(2( f(α) 4 − 1)2w2min)\nNote for this inequality to be possible, we also need 1p∗min ln 2k ξ ≤ ξ 2 exp(2( f(α) 4 − 1)2w2min), imposing a constraint on f(α). Taking logarithm on both sides and rearrange, we get\n( f(α) 4 − 1)2 ≥ 1 2wmin ln( 2 ξp∗min ln 2k ξ )\nThis satisfied since f(α) ≥ 5 √\n1 2wmin ln( 2ξp∗min ln 2kξ ).\nProof of Theorem 2. By Proposition 1, (X,C∗) satisfying (B) implies C∗ is (f(α) 2\n162 , α)-stable. Let b0 :=\nf(α)2\n162 , and we denote event F := {∆(C0, Copt) ≤ 12b0φ ∗}. Since f(α) ≥ 5 √\n1 2wmin ln( 2ξp∗min ln 2kξ ), and\nlog 2k ξ\np∗min < mo <\nξ 2 exp{2( f(α) 4 − 1) 2w2min}, we can apply Lemma 20 to get\nPr{F} ≥ 1− ξ\nConditioning on F , we can invoke Theorem 3, since (A1) is satisfied implicitly by (B), Co ⊂ conv(X) by the sampling method used in Algorithm 2, and we can guarantee that the setting of our parameters, m, c′, and to, satisfies the condition required in Theorem 3. Let Ωt be as defined in the main paper, by Theorem 3, ∀t ≥ 1,\nE{∆t|Ωt, F} = O( 1\nt ) and Pr{Ωt|F} ≥ 1− δ\nSo Pr{Ωt ∩ F} = Pr{Ωt|F}Pr{F} ≥ (1− δ)(1− ξ)\nFinally, using Lemma 18, and letting Gt := Ωt ∩ F , we get the desired result."
    }, {
      "heading" : "10 Appendix D: auxiliary lemmas",
      "text" : "Equivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates ηtr is equivalent to online k-means [6] and mini-batch k-means [20].\nClaim 1. In Algorithm 1, if we set a counter for N̂ tr := ∑t i=1 n̂ i r and if we set the learning rate ηtr := n̂tr N̂tr , then provided the same random sampling scheme is used,\n1. When mini-batch size m = 1, the update of Algorithm 1 is equivalent to that described in [Section 3.3, [6]].\n2. When m > 1, the update of Algorithm 1 is equivalent to that described from line 3 to line 14 in [Algorithm 1, [20]] with mini-batch size m.\nProof. For the first claim, we first re-define the variables used in [Section 3.3, [6]]. We substitute index k in [6] with r used in Algorithm 1. For any iteration t, we define the equivalence of definitions: s ← xi, ctr ← wk, n̂tr ← ∆nk, N̂ tr ← nk. According to the update rule in [6], ∆nk = 1 if the sampled point xi is assigned to cluster with center wk. Therefore, the update of the k-th centroid according to online k-means in [6] is:\nwk ← wk + 1\nnk (xi − wk)1{∆nk=1}\nUsing the re-defined variables, at iteration t, this is equivalent to\nctr = c t−1 r +\n1\nN̂ tr (s− ct−1r )1{n̂tr=1}\nNow the update defined by Algorithm 1 with m = 1 and ηtr = n̂tr N̂tr is:\nctr = c t−1 r + η t r(ĉ t r − ct−1r )1{n̂tr 6=0}\n= ct−1r + n̂tr\nN̂ tr (s− ct−1r )1{n̂tr=1}\n= ct−1r + 1 N̂ tr (s− ct−1r )1{n̂tr=1}\nsince n̂tr can only take value from {0, 1}. This completes the first claim. For the second claim, consider line 4 to line 14 in [Algorithm 1, [20]]. We substitute their index of time i with t in Algorithm 1. We define the equivalence of definitions: m← b, St ←M , s← x, ct−1I(s) ← d[x], c t−1 r ← c.\nAt iteration t, we let v[ct−1r ]t denote the value of counter v[c] upon completion of the loop from line 9 to line 14 for each center c, then N̂ tr ← v[ct−1r ]t. Since according to Lemma 22, from line 9 to line 14, the updated centroid ctr after iteration t is\nctr = 1\nv[ct−1r ]t ∑ s∈∪ti=1Sir s = 1 N̂ tr ∑ s∈∪ti=1Sir s\nThis implies\nctr − ct−1r = 1\nN̂ tr ∑ s∈∪ti=1Sir s− ct−1r\n= 1 N̂ tr [ ∑ s∈Str s+ ∑\ns′∈∪t−1i=1Sir\ns′]− ct−1r\n= 1 N̂ tr [ ∑ s∈Str s+ N̂ t−1r c t−1 r ]− ct−1r\n= − n̂ t r\nN̂ tr ct−1r +\nn̂tr N̂ tr\n∑ s∈Str s\nn̂tr = −ηtrct−1r + ηtr ĉtr\nHence, the updates in Algorithm 1 and line 4 to line 14 in [Algorithm 1, [20]] are equivalent.\nLemma 21 (Centroidal property, Lemma 2.1 of [13]). For any point set Y and any point c in Rd,∑\nx∈Y ‖x− c‖2 = ∑ x∈Y ‖x−m(Y )‖2 + |Y |‖m(Y )− c‖2\nLemma 22. Let wt, gt denote vectors of dimension Rd at time t. If we choose w0 arbitrarily, and for t = 1 . . . T , we repeatdly apply the following update\nwt = (1− 1\nt )wt−1 +\n1 t gt\nThen\nwT = 1\nT T∑ t=1 gt\nProof. We prove by induction on T . For T = 1, w1 = (1− 1)w0 + g1 = 11 ∑1\nt=1 gt. So the claim holds for T = 1.\nSuppose the claim holds for T , then for T + 1, by the update rule\nwT+1 = (1− 1\nT + 1 )wT +\n1\nT + 1 gT+1\n= (1− 1 T + 1 ) 1 T T∑ t=1 gt + 1 T + 1 gT+1\n= T\nT + 1\n1\nT T∑ t=1 gt + 1 T + 1 gT+1\n= 1\nT + 1 T+1∑ t=1 gt\nSo the claim holds for any T ≥ 1.\nLemma 23. ∀t ≥ 1, conditioning on Ft, the noise term (9) is upper bounded by B1 := 5φ t.\nProof. Since ‖x− ĉt+1r ‖2 ≤ 2‖x− ctr‖2 + 2‖ctr − ĉt+1r ‖2\nWe have\nE[ ∑ r ∑ x∈At+1r ‖x− ĉt+1r ‖2 + φt|Ft]\n≤ 2 ∑ r ∑ x∈At+1r ‖x− ctr‖2\n+2 ∑ r ∑ x∈At+1r E[‖ctr − ĉt+1r ‖2|Ft] + φt\nNow,\nE[‖ctr − ĉt+1r ‖2|Ft] ≤ E ∑ s∈Str ‖c t r − s‖2\n|Str| = φtr ntr\nwhere Str is the sampled from Atr in Algorithm 1, and the inequality is by convexity of l2-norm. Substituting this into the previous inequality completes the proof.\nLemma 24. Suppose C∗ is (bo, α)-stable. Conditioning on Ωi, we have, The terms (10), and (11), for t = i, are upper bounded by B := 4(bo + 1)nφ∗.\nProof. Conditioning on Ωi, ∆i−1 ≤ boφ∗\nBy Lemma 18, we also have\nφi−1 − φ∗ ≤ ∆i−1 ≤ boφ∗\nBy Cauchy-Schwarz, ∑ r n∗r〈ci−1r − c∗r , ĉir − ci−1r 〉\n≤ √∑\nr\nn∗r‖ci−1r − c∗r‖2 √∑\nr\nn∗r‖ĉir − ci−1r ‖2\nNow, since ĉir is the mean of a subset of Air, ‖ĉir − ci−1r ‖2 ≤ φi−1r Hence ∑\nr\nn∗r‖ĉir − ci−1r ‖2 ≤ nφi−1\nOn the other hand,∑ r n∗r‖ci−1r − E[ĉir|Fi−1]‖2 = ∑ r n∗r‖ci−1r −m(Air)‖2\n≤ n ∑ r φ(ci−1r )− φ(m(Air))\n= n[φi−1 − φ(m(Ai))] ≤ n(φi−1 − φ∗)\nNow we first bound (10): ∑ r n∗r〈ci−1r − c∗r , ĉir − E[ĉir|Fi−1]〉\n= ∑ r n∗r〈ci−1r − c∗r , ĉir − ci−1r 〉\n+ ∑ r n∗r〈ci−1r − c∗r , ci−1r − E[ĉir|Fi−1]〉\n≤ √ ∆i−1 √ nφi−1 + √ ∆i−1 √ n(φi−1 − φ∗)\n≤ √ boφ∗ √ n(bo + 1)φ∗ + √ nboφ ∗ ≤ 2(bo + 1) √ nφ∗\nTo bound (11), ∑ r n∗r‖ĉir − c∗r‖2\n≤ 2 ∑ r n∗r‖ĉir − ci−1r ‖2 + 2 ∑ r n∗r‖ci−1r − c∗r‖2\n≤ 2nφi−1 + 2∆i−1 ≤ 2n(bo + 1)φ∗ + 2boφ∗ ≤ 4n(bo + 1)φ∗\nClaim 2. In the context of Algorithm 1, if ∀ctr ∈ Ct, ctr ∈ conv(X), then ∀ct+1r ∈ Ct+1, ct+1r ∈ conv(X).\nProof of Claim. By the update rule in Algorithm 1, ct+1r is a convex combination of ctr and ĉt+1r , where ĉt+1r is the mean of a subset of X, and hence ĉt+1r ∈ conv(X). Since both ctr and ĉt+1r are in conv(X), ct+1r ∈ conv(X).\nLemma 25 (technical lemma). For any fixed b ∈ (1, 2]. If C ≥ b−13 , δ ≤ 1 e , and t ≥ ( 3Cb−1 ln 1 δ ) 2 b−1 , then tb−1 − 2C ln t− C ln 1δ > 0.\nProof. Let f(t) := tb−1 − 2C ln t − C ln 1δ . Taking derivative, we get f ′(t) = (b− 1)tb−2 − 2Ct ≥ 0 when t ≥ ( 2C b−1) 1 b−1 . Since ln 1δ 3C b−1 ≥ 3C b−1 ≥ 1, (ln 1 δ 3C b−1) 2 b−1 ≥ ( 2Cb−1) 1 b−1 , it suffices to show f((ln 1δ 3C b−1) 2 b−1 ) > 0 for our statement to hold. f((ln 1δ 3C b−1) 2 b−1 ) = (ln 1δ 3C b−1) 2−2C ln{(ln 1δ 3C b−1) 2 b−1 }−C ln 1δ = (ln 1 δ ) 2 9C2 (b−1)2− 4C b−1 ln(ln 1 δ 3C b−1)−C ln 1 δ =\n4C b−1 [\n3 2 C\nb−1 ln 1 δ − ln( 3C b−1 ln 1 δ )] + C ln 1 δ [ 3C (b−1)2 − 1] > 0, where the first term is greater\nthan zero because x− ln(2x) > 0 for x > 0, and the second term is greater than zero by our assumption on C.\nLemma 26 (Lemma D1 of [4]). Consider a nonnegative sequence (ut : t ≥ to), such that for some constants a, b > 0 and for all t > to ≥ 0, ut ≤ (1− at )ut−1 + b t2 . Then, if a > 1,\nut ≤ ( to + 1\nt+ 1 )auto +\nb\na− 1 (1 +\n1\nto + 1 )a+1\n1\nt+ 1"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>We analyze online [6] and mini-batch [20] k-means variants. Both<lb>scale up the widely used Lloyd’s algorithm via stochastic approximation,<lb>and have become popular for large-scale clustering and unsupervised<lb>feature learning. We show, for the first time, that they have global<lb>convergence towards “local optima” at rate<lb>O(1t ) under general condi-<lb>tions. In addition, we show if the dataset is clusterable, with suitable<lb>initialization, mini-batch k-means converges to an optimal k-means<lb>solution at rate<lb>O(1t ) with high probability. The k-means objective is<lb>non-convex and non-differentiable: we exploit ideas from non-convex<lb>gradient-based optimization by providing a novel characterization of the<lb>trajectory of k-means algorithm on its solution space, and circumvent<lb>its non-differentiability via geometric insights about k-means update.",
    "creator" : "LaTeX with hyperref package"
  }
}