{
  "name" : "1611.04920.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unsupervised Learning with Truncated Gaussian Graphical Models",
    "authors" : [ "Qinliang Su", "Xuejun Liao", "Chunyuan Li", "Zhe Gan", "Lawrence Carin" ],
    "emails" : [ "qinliang.su@duke.edu,", "xjliao@duke.edu,", "chunyuan.li@duke.edu,", "zhe.gan@duke.edu,", "lcarin@duke.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Gaussian graphical models (GGMs) have been widely used in practical applications (Honorio et al. 2009; Liu and Willsky 2013; Oh and Deasy 2014; Meng, Eriksson, and Hero 2014) to discover statistical relations of random variables from empirical data. The popularity of GGMs is largely attributed to the ubiquitousness of normal approximation adopted in practice, as well as the ease of inference due to the appealing properties of multivariate normal distributions. On the downside, however, the Gaussian assumption prevents GGMs from being applied to more complex tasks, for which the underlying statistical relations are inherently non-Gaussian and nonlinear. It is true for many models that, by adding hidden variables and integrating them out, a more expressive distribution can be obtained about the visible variables; such models include Boltzmann machines (BMs) (Ackley, Hinton, and Sejnowski 1985), restricted BMs (RBMs) (Hinton 2002; Hinton, Osindero, and Teh 2006; Salakhutdinov and Hinton 2009), and sigmoid belief networks (SBNs) (Neal 1992). Unfortunately, this approach does not work for GGMs since\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthe marginal distribution of visible variables always remains Gaussian no matter how many hidden variables are added (the marginals of a multivariate Gaussian distribution are still Gaussian).\nMany efforts have been devoted to improving the GGM’s representational versatility. In (Frey 1997; Frey and Hinton 1999), nonlinear Gaussian belief networks were proposed, with explicit nonlinear transformations applied on random variables to obtain nonlinearity. More recently, (Su et al. 2016) proposed to employ truncated Gaussian hidden variables to implicitly introduce nonlinearity. A great advantage of truncation over transformation is that many nice properties of GGMs are preserved, which can be exploited to facilitate inference of the model. However, the models in (Su et al. 2016; Frey 1997; Frey and Hinton 1999) all have a directed graphical structure, for which it is difficult to estimate the posteriors of hidden variables due to the ‘explaining away’ effect inherent in directed graphical models. As a result, mean-field variational Bayesian (VB) analysis was used. It is well known that, apart from the scalability issue, the independence assumption in mean-field VB is usually too restrictive to capture the actual statistical relations. Moreover, (Su et al. 2016) is primarily targeted at supervised learning and only considered regression and classification tasks.\nIn this paper, we consider an undirected GGM with truncated hidden variables, which serves as a counterpart of the directed model in (Su et al. 2016) and is particularly useful for unsupervised learning. Undirected graphical models have the advantages that conditional independencies are encoded in the graph. Thus, we can design the conditional dependencies by controlling the graph connections. In this paper, we impose a bipartite structure on the graph, such that it contains two layers (one hidden and one visible) and only has inter-layer connections, leading to a model termed a restricted truncated GGM (RTGGM). In RTGGM, visible variables are conditionally independent given the hidden variables, and vice versa. By exploiting the conditional independencies as well as the appealing properties of truncated normals, we show that the model can be trained efficiently using the contrastive divergence (CD) algorithm (Hinton 2002). This makes a striking contrast to the directed model in (Su et al. 2016), where the conditional independent properties do not exist and inference is done based on mean-field VB approximation.\nIt should be emphasized that although the variables in\nar X\niv :1\n61 1.\n04 92\n0v 1\n[ st\nat .M\nL ]\n1 5\nN ov\n2 01\nan RTGGM are conditional independent, their marginal distributions are flexible enough to be able to model many interesting data. In order to handle other types of data, such as real-valued, binary or count data, we also developed three variants of the basic RTGGM. It is shown that all variants can be trained efficiently by the CD algorithm as well. Furthermore, we extend the two-layer RTGGMs to deep models, by stacking multiple RTGGMs together, and show that the deep models can be trained in a layer-wise manner. To evaluate the performance of the proposed models, we have also developed methods to estimate their partition functions, based on annealed importance sampling (AIS) (Salakhutdinov and Murray 2008; Neal 2001). Extensive experimental results are provided to validate the advantages of the RTGGM models."
    }, {
      "heading" : "Related Work",
      "text" : "The proposed RTGGM is not only a new member of the GGM family, but is also closely related to RBM (Hinton 2002). One of the main differences between the two models is their inherent nonlinearities. In an RTGGM, the visible and hidden variables are related through smoothed ReLU functions, while they are related by sigmoid functions in an RBM. ReLU is used extensively in neural networks and has achieved tremendous success due to its easier training properties (Jarrett et al. 2009). In light of this, there are many efforts devoted to bringing ReLU into RBM. For example, (Nair and Hinton 2010) proposed to replace binary hidden units with rectified Gaussian approximation. Although an ReLU-like nonlinearity is induced, the proposed model is only specified by two conditional distributions, lacking a joint-distribution description. On the other hand, (Ravanbakhsh et al. 2016) proposed to use Exponential Family Harmoniums (EFH) (Welling, Rosen-Zvi, and Hinton 2004) and Bregman divergence to incorporate different monotonic nonlinearities into RBM. The model preserves a joint-distribution description, but their conditional distributions are complicated and do not admit exact and efficient sampling. To overcome this, they need to approximate the conditional pdfs as Gaussian and then sample from the approximate distributions. In contrast to the above models, the proposed RTGGM not only maintains an explicit joint distribution, but also preserves simple conditional distributions (truncated normals), which allows exact and efficient sampling. Moreover, because of the explicit joint distribution and the easy-sampled conditional distributions, we are able to estimate the partition function of the RTGGM for performance evaluation purposes. However, it is not clear how to estimate them for models from (Nair and Hinton 2010; Ravanbakhsh et al. 2016). Interestingly, we also notice that the smoothed ReLU in RTGGM shares some common ground with leaky ReLU (He et al. 2015), with both having small nonzero slopes at the negative axis for easier optimization."
    }, {
      "heading" : "Formulation of Restricted-Truncated Gaussian Graphical Models",
      "text" : ""
    }, {
      "heading" : "Basic Model",
      "text" : "Let x ∈ Rn and h ∈ Rm respectively denote the visible and hidden variables. The joint probability distribution of an\nRTGGM is defined by\np(x,h; Θ) = 1\nZ e−E(x,h)I(x ≥ 0)I(h ≥ 0), (1)\nwhere I(·) is the indicator function, E(x,h) is an energy function defined as\nE(x,h) , 1\n2\n( xT diag(a) x + hT diag(d) h\n−2xTWh− 2bTx− 2cTh ) , (2)\nZ is the partition function, the superscripted T denotes matrix transpose, and Θ , {W,a,d,b, c} collects all model parameters. The joint distribution in (1) can be equivalently written as\np(x,h; Θ) = NT ( [xT ,hT ]T ∣∣µ,P−1 ) , (3) where NT (·) represents the truncated normal distribution whose nonzero probability density concentrates in the pos-\nitive orthant, P , [\ndiag(a) −W −WT diag(d)\n] 0 and µ ,\nP−1 [\nb c\n] . Because of the diagonal matrices diag(a) and\ndiag(d) in (2), we have the conditional distributions as\np(x|h; Θ) = n∏ i=1 NT ( xi ∣∣∣∣ 1ai [Wh + b]i, 1ai ) , (4)\np(h|x; Θ) = m∏ j=1 NT ( hj ∣∣∣∣ 1dj [WTx + c]j , 1dj ) , (5)\nwhere the notations [z]i and zi both represent the i-th element of vector z. Equations (4) and (5) show that the visible variables are conditionally independent given the hidden variables, and vice versa.\nBy the properties of univariate truncated normal distributions (Johnson, Kotz, and Balakrishnan 1994), the conditional expectation is given by E[hj |x] = µT ( 1dj [W Tx + c]j , 1 dj\n), where\nµT (ξ, λ 2) , ξ + λφ(ξ/λ) /Φ(ξ/λ) (6)\nis the mean of NT (x|ξ, λ2) and it serves as the nonlinearity used in the RTGGM; φ(·) and Φ(·) are respectively the\nprobability density function (pdf) and cumulative distribution function (cdf) of the standard normal distribution. Shown in Figure 1 is µT (ξ, λ2) as a function of ξ for λ2 = 0.1, along with the sigmoidal and ReLU activation function, for comparison. It is observed that µT (·, 0.1) behaves very similarly to the ReLU nonlinearity, and deviates significantly from the sigmoidal nonlinearity used in RBMs."
    }, {
      "heading" : "Variants",
      "text" : "Truncating the hidden variables in RTGGM is essential to maintain the model’s expressiveness. In the basic RTGGM above, the visible variables are also truncated to obtain symmetry, but this is not necessary. The visible domain can be changed to match the type of data in an application. Below we present three variants of the basic RTGGM, which deal with real-valued, binary, and count data. In all cases, the expression of p(x|h; Θ) in (4) is modified, but the expression of p(h|x,Θ) remains the same as in (5), and thus the ReLU nonlinearity is preserved.\nReal-Valued Data The joint distribution in this case is p(x,h; Θ) = 1Z e\n−E(x,h)I(h ≥ 0). The expression of p(x|h; Θ) changes to\np(x|h; Θ) = n∏ i=1 N ( xi ∣∣∣∣ 1ai [Wh + b]i, 1ai ) . (7)\nBinary Data When each component of x is in {0, 1}, the quadratic term xT diag(a)x is dropped from the energy function E(x,h) and the domain restriction is changed from I(x ≥ 0) to I(x ∈ {0, 1}n). The conditional in (4) becomes p(x|h; Θ) = ∏ni=1 p(xi|h; Θ), with\np(xi = 1|h; Θ) = exp{[Wh + b]i}\n1 + exp{[Wh + b]i} . (8)\nCount Data Without loss of generality, we describe the count data model in the context of topic modeling. Following (Hinton and Salakhutdinov 2009), we employ N × 1 one-hot vectors (an one-hot vector is a binary vector of all 0’s except for a single 1) to represent the words in a vocabulary of size N . A document of size K is then represented by a matrix X = [x1, · · · ,xK ], where each column is a N × 1 one-hot vector. We define an energy function E(X,h) , 12 (h\nT diag(d)h− 2x̂TWh− 2bT x̂− 2KcTh) with x̂ , ∑K k=1 xk understood as a count vector. The energy function above reduces to that of replicated softmax (Hinton and Salakhutdinov 2009) if the quadratic term is dropped and h is restricted to h ∈ {0, 1}m. The conditional in (4) is accordingly modified to p(X|h; Θ) =∏N i=1 ∏K k=1 p([xk]i|h; Θ), with\np([xk]i = 1|h; Θ) = exp{[Wh + b]i}∑N j=1 exp{[Wh + b]j} . (9)\nModel Training When training an RTGGM one is concerned with finding the Θ that maximizes the log-likelihood L̃(Θ;X ) =∑\nx∈X L(Θ; x) given the training data set X , where L(Θ; x) = log ∫ +∞ 0\np(x,h; Θ)dh is the contribution from a single data sample, and ∫ +∞ 0\ndh is a shorthand for the multiple integral with respect to (w.r.t.) the components in h. It is known that ∂L(Θ;x)∂Θ = E[ ∂E(x,h) ∂Θ ] − E[ ∂E(x,h) ∂Θ |x]. The first term involves expectation of the energy function w.r.t. p(x,h), which is intractable because of the difficulty of computing the normalizing constant of p(x,h). Fortunately, p(x|h) and p(h|x) are both simple distributions, allowing a Gibbs sampler to efficiently sample from p(x,h). Starting with x(0) = x, the Gibbs sampler generates a chain of samples, (x(0),h(1),x(1), . . . ,h(k),x(k)), where h(t) ∼ p(h|x(t−1); Θ) and x(t) ∼ p(x|h(t); Θ). The contrastive divergence uses the first and last sample of x in the chain, i.e., x(0) (which is a datum) and x(k), to from an estimate of the expected gradient,\n∂L(Θ; x) ∂Θ\n≈ E [ ∂E(x,h)\n∂Θ ∣∣∣x(k) ]− E [ ∂E(x,h) ∂Θ ∣∣∣∣x(0)] . (10)\nNote that p(h|x; Θ) is always a truncated normal distribution as shown in (5), while p(x|h; Θ) is constituted according to (4), (7), (8), or (9), depending on the type of data x. For the basic RTGGM, we have ∂E(x,h)∂wij = xihj , ∂E(x,h) ∂ai = 12x 2 i , ∂E(x,h) ∂bi = xi, ∂E(x,h) ∂cj = hj , and ∂E(x,h) ∂dj = 12h 2 j . It can be seen that, to estimate ∂L(Θ;x)∂Θ , one only needs to know the conditional expectations E[hi|x = x(s)] and E[h2i |x(s)] for s = 0, k. It follows from (6) that E [ hj |x(s) ] = µT ( [WT x(s)+c]j\ndj , 1dj ). To compute E [ h2j |x(s) ] , we use the\nformula E [ h2j |x(s) ] = E [ hj |x(s) ]2 + Var[hj |x(s)], where\nVar [ hj |x(s) ] = 1\ndj\n( 1− βj φ (βj)\nΦ (βj) − φ\n2 (βj)\nΦ2 (βj)\n) (11)\naccording to (Johnson, Kotz, and Balakrishnan 1994), where βi ,\n[WT x(s)+c]j√ dj . The gradients for the variant RTGGM\nmodels can be estimated similarly. With these estimated gradients, the model parameters Θ can be updated using stochastic optimization algorithms.\nOne challenge in training RTGGMs is how to efficiently sample from truncated normal distributions. Fortunately, because the variables in an RTGGM are conditional independent, we only need to sample from univariate truncated normals, and such sampling has been investigated extensively. So far, many efficient algorithms have been proposed (Chopin 2011; Robert 1995). Another challenge is how to efficiently calculate the ratio φ(µ)Φ(µ) in (6) and (11). Direct calculation is expensive due to the integration involved in Φ(µ). To compute it cheaply, we adopt the approach in (Su et al. 2016), taking into consideration the approximations based on asymptotic expansions of the Gaussian hazard function."
    }, {
      "heading" : "Partition Function Estimation",
      "text" : "To evaluate the model’s performance, we need to calculate the partition function Z. By exploiting the bipartite structure in an RTGGM as well as the appealing properties of truncated normals, we show that we can use annealed importance sampling (AIS) (Salakhutdinov and Murray 2008; Neal 2001) to estimate it. Here, we only focus on the estimation for the RTGGM with binary data; details for the other types data are provided in the Supplementary Material. The joint distribution of the RTGGM for binary data can be represented as\np(x,h; Θ) = 1Z e − 12\n( ‖D 1 2 h‖2−2xT Wh−2bT x−2cT h ) I(x ∈\n{0, 1}n)I(h ≥ 0). After integrating out the hidden variable h, we obtain p(x; Θ) = 1Z p ∗(x; Θ), where\np∗(x; Θ) , eb T x × m∏ j=1 1√ dj Φ\n( [WT x+c]j√\ndj ) φ ( [WT x+c]j√\ndj ) . (12) Since p∗(x; Θ) is in closed-form, we only need calculate the partition function Z to obtain p(x; Θ).\nFollowing the AIS procedure (Salakhutdinov and Murray 2008; Neal 2001), we define two distributions pA(x,hA) = 1ZA e\n−EA(x,hA) and pB(x,hB) = 1 ZB e−EB(x,h B), where EA(x,hA) , 12 (‖diag 1 2 (d)hA‖2 − 2bTx) andEB(x,hB) , 12 (‖diag 1 2 (d)hB‖2−2xTWhB− 2bTx − 2cThB). By construction, p0(x,hA,hB) = pA(x,h\nA) and pK(x,hA,hB) = pB(x,hB). The partition function of pA(x,hA) is given by ZA = ∏n i=1(1 +\neb A i ) ∏m j=1\n1√ dj Φ(0) φ(0) , and the partition function of pB(x,h B)\ncan be approximated as (Neal 2001) ZB ≈ ∑M i=1 w (i)\nM ZA, (13)\nwhere w(i) is constructed from a Markov chain that gradually transits from pA(x,hA) to pB(x,hB), with the transition realized via a sequence of intermediate distributions,\npk(x,h A,hB)=\n1\nZk e−(1−βk)EA(x,h A)−βkEB(x,hB), (14)\nwhere 0 = β0 < β1 < . . . < βK = 1. In particular, the Markov chain (x(0)i ,x (1) i , . . . ,x (K) i ) is simulated as x (0) i ∼ p0(xi,h A,hB), (hA,hB) ∼ p1(hA,hB |xi(0)), x(1)i ∼ p1(xi|hA,hB), · · · , (hA,hB) ∼ pK(hA,hB |xi(K−1)) and x\n(K) i ∼ pK(xi|hA,hB). From the chain, a coefficient is\nconstructed as w(i) = p ∗ 1(x̃i (0)\np∗0(x̃i (0))\np∗2(x̃i (1))\np∗1(x̃i (1)) · · · p\n∗ K(x̃i (K−1))\np∗K−1(x̃i (K−1))\n,\nwhere\np∗k(x) = e (1−βk)bAT x m∏ j=1 1√ (1− βk)dj Φ (0) φ (0)\n× eβkbT x m∏ j=1 1√ βkdj Φ\n(√ βk[W\nT x+c]j√ dj ) φ (√ βk[WT x+c]j√\ndj\n) . (15)\nAssuming M independent Markov chains simulated in this way, one obtains {w(i)}Mi=1. Note that the Markov chains can be efficiently simulated, as all involved variables are conditionally independent.\nExtension to Deep Models The RTGGMs discussed so far consist of a visible layer and a hidden layer. These two-layer models, like RBMs, can be used to construct deep models. A deep RTGGM with L hidden layers, constructed by stacking L two-layer RTGGMs, can be defined by the following joint distribution,\np(hL,· · · ,h1,x)=p(hL,hL−1) · · · p(h1|h2)p(x|h1), (16) where p(hL,hL−1) is the joint distribution of a two-layer RTGGM and p(h`−1|h`) =∏M`−1 i=1 NT ([h`−1]i| 1a(`)i [W(`)h` + b (`)]i, 1 a (`) i ) is the associated conditional distribution. The bottom layer p(x|h1) could be defined by truncated normal, normal, or binary distributions, depending on the data type. A deep RTGGM with three hidden layers is illustrated in the left panel of Figure 2.\nSimilar to a DBN constructed from RBMs (Hinton, Osindero, and Teh 2006), a deep RTGGM can be trained in a layer-wise fashion. Specifically, we first train the bottom layer by simply treating it as an RTGGM, using the CDbased ML algorithm described above. We then compute the conditional expectation E[h1|x] from the already-trained bottom RTGGM and use E[h1|x] as data to train the second layer from the bottom, again treating it as a RTGGM. The layer-wise training procedure proceeds until the top layer is reached, as illustrated in the right panel of Figure 2. Similar to the proofs in (Hinton, Osindero, and Teh 2006), we can prove that the variational lower bound is guaranteed to increase as more layers are added under the layer-wise training.\nBesides serving as a generative model, the deep RTGGM can also be used to pretrain a feedforward neural network so as to improve its performance. It is known that, due to the sigmoidal nonlinearity inherent in RBMs, when we use the unsupervised learning result of a DBN to initialize sigmoidal feed-forward neural networks, remarkable improvements are observed, especially in the case of scarce label information (Hinton and Salakhutdinov 2006). Due to the similarity between the nonlinearity in RTGGMs and ReLU(·), we can also use the deep RTGGM learned unsupervisedly to initialize ReLU feedforward neural networks."
    }, {
      "heading" : "Experiments",
      "text" : "We report experimental results of the RTGGM models on various publicly available data sets, including binary, count and real-valued data, and compare them to competing models. For all RTGGM models considered below, we use x(0) and x(25) to get a CD-based gradient estimate and then use RMSprop (Tieleman and Hinton 2012) to update the model parameters, with the RMSprop delay set to 0.95."
    }, {
      "heading" : "Binary Data",
      "text" : "The binarized versions of MNIST and Caltech 101 Silhouettes data sets are considered. MNIST contains 28× 28 images of ten handwritten digits, with 60,000 and 10,000 images in the training and testing sets, respectively. Caltech 101 Silhouettes is a set of 28×28 images for the polygon outlines of objects, of which 6364 are used for training and 2307 for testing (Marlin et al. 2010). Two RTGGMs, with 100 and 500 hidden nodes, are trained and tested. The learning rate is set to 10−4. Log-probabilities are estimated based on 100,000 inverse temperatures βk, uniformly spaced in [0, 1]. The final estimate is an average over 100 independent AIS runs.\nTables 1 and 2 summarize the averaged test logprobabilities on MNIST and Caltech 101 Silhouettes. For comparison, the corresponding results of competing models are also presented. It is seen from Table 1 that the RTGGM with 500 hidden nodes achieves the best performance, significantly outperforming the RBM with the same number of hidden nodes as well as the deep models SBN and DBN. Similar results can be observed in Table 2 for the Caltech 101 Silhouettes. The performance gain may be largely attributed to the smooth ReLU nonlinearity brought by truncation as well as the less approximation made in training. The importance of truncation is also revealed in the results of restricted GGMs (RGGM), in which we do not truncate the hidden variables but only impose bipartite structure on GGMs. It can be seen from both Tables 1 and 2 that, without the truncation, RGGMs perform poorly compared to all models. Note that the models in (Nair and Hinton 2010; Ravanbakhsh et al. 2016) are not included here, because their log-probability cannot be computed, as explained earlier.\nTo demonstrate that the RTGGM is able to capture important statistical relations, we show in Figure 3 the samples drawn from the RTGGM trained on MNIST, using a Gibbs sampler with 50000 burn-in samples. We see that the generated digits exhibit a large variabilities and look very similar to real handwritten digits. Moreover, we also use the RTGGM to recover the missing values of an image. It is demonstrated in Figure 3 that, with only the upper-half part of an image presented, the RTGGM can recover the lower-half part reasonably well. The recovery is mostly correct except that “2” is mistaken for “0” in the upper subfigure and “7” is mistaken for “0” in the lower subfigure. However, we notice that the two cases are extremely difficult, in which it would be difficult even for a human to recognize the images based on only the upper-half parts. Finally, we demonstrate in Figure 4 the images drawn from the RTGGM trained on Caltech 101 Silhouettes, using Gibbs sampling with 50,000 burn-in samples. Again, it can be seen that the generated images re-\nsemble the training data, showing that the generative model has faithfully captured the features of the training data."
    }, {
      "heading" : "Count Data",
      "text" : "Two publicly available corpora are considered: 20NewsGroups and Reuters Corpus Volume. The two corpora are preprocessed in the same way as in (Hinton and Salakhutdinov 2009). An RTGGM with 50 hidden nodes is trained, using the same setting as in the previous experiment for the learning rate. The perplexity is evaluated over 50 heldout documents, based on the setting used in (Hinton and Salakhutdinov 2009). For each document, we obtain the test log-probability as an average over 100 AIS runs, each using 100,000 inverse temperatures βk.\nTable 3 shows the averaged test perplexity per word for the RTGGM. For comparison, we also report the perplexities of LDA with 50 and 200 topics, as well as those of the replicated softmax model (RSM) (Hinton and Salakhutdinov 2009) with 50 topics. The RSM is a variant of the RBM that handles count data and it is constructed similarly as the RTGGM for count data. As seen from Table 3, for both corpora, RTGGM50 performs better than RSM-50 and LDA models. This further demonstrates the performance gains brought about by the smooth ReLU nonlinearity in RTGGMs."
    }, {
      "heading" : "Unsupervised Pre-training of ReLU Neural Networks",
      "text" : "As described previously, deep RTGGMs can be used to pretrain multi-layer ReLU neural networks by exploiting the unlabeled information. In this task, MNIST and NORB datasets are considered, where MNIST is the same as in previous experiments. NORB is a dataset of images from 6 classes\nTable 1: Log-probability of test data on MNIST data set. Model Dim Test log-prob. RBM 500 −86.3 SBN 200 −113.1 SBN 300-400 −85.48 DBN 500-2000 −86.2 RTGGM 100 −89.3 RTGGM 500 −79.2\nTable 2: Log-probability of test data on Caltech 101Silhouettes data set. Model Dim Test log-prob. RBM 500 −114.7 RBM 4000 −107.7 SBN 100-300 −113.3 RTGGM 100 − RTGGM 500 −105.1\nFigure 3: (a) Illustration of deep RTGGM with three hidden layers. (b) Illustration of how to decompose a deep RTGGM into three shallow RTGGMs and train them layer by layer.\nFigure 4: (a) Illustration of deep RTGGM with three hidden layers. (b) Illustration of how to decompose a deep RTGGM into three shallow RTGGMs and train them layer by layer.\nTable 3: Perplexities of 20 newsgroup and Reuters data sets.\nData set Number of docs Vocabulary size Ave. test perplexity per word (in nats)\nTrain Test LDA-50 LDA-200 RSM-50 RTGGM-50\n20news 11, 314 7, 531 2000 1091 1058 953 915 Reuters 794, 414 10, 000 10, 000 1437 1142 988 934\n7\n0 200 400 600 800 Epochs\n-300\n-250\n-200\n-150\n-100\nA ve\n. T es\nt l og\n-p\nro b.\nFigure 4: (Left) Samples drawn from Caltech 101 Silhouettes data set. (Middle) Samples drawn from the RTGGM with 500 hidden nodes. (Right) The average test log-probability of the RTGGM with 500 hidden nodes, as a function of learning epoch.\nData set LDA-50? LDA-200? RSM-50? RTGGM-50\n20news 1091 1058 953 915 Reuters 1437 1142 988 934\nTable 3: Average test perplexity per word, on 20newsgroup and Reuters. (?) cited from (Hinton and Salakhutdinov 2009).\non cluttered background, partitioned into 291,600 training and 58,230 testing images. We pre-process these images the same way as in (Nair and Hinton 2010). We first train a 1000- 1000-1000 deep RTGGM on the unlabeled data, and then use the learned model parameters to initialize a deep ReLU neural network of the same size, which is further trained with the provided labels using RMSprop, with a learning rate of 10−5. The test accuracy is reported in Table 4. For comparison, we also report the results with no pre-training or pre-trained using the rectified-RBM (Nair and Hinton 2010). It is seen from Table 4 that the results of pre-training with the RTGGM performs the best. Note that this is an extension of (Hinton and Salakhutdinov 2006), where the RBM was used to pretrain a sigmoid-based neural network; here we use an RTGGM to pretrain a ReLU-based neural network."
    }, {
      "heading" : "Conclusions",
      "text" : "We have introduced a novel variant of the GGM, called restricted truncated GGM (RTGGM), to enhance its represen-\nData set Without pre-train With pre-train\nrectified-RBM RTGGM\nMNIST 1.43% 1.33% 1.17%\nNORB 16.88% 16.43% 16.12%\nTable 4: Average classification errors achieved by the multi-layer ReLU neural network without pre-training, pre-trained by the method in (Nair and Hinton 2010) (referred to as “rectified-RBM” in the table), and pre-trained by the deep RTGGM.\ntational abilities while preserving its nice (simple inference) properties. The new model is obtained by truncating the variables of an undirected GGM and imposing a bipartite structure on the truncated GGM. It is shown that the truncation brings strong nonlinear representational abilities to the model, while the bipartite structure enables the model to be trained efficiently using contrastive divergence. Three variants of the RTGGM have been developed to handle real-valued, binary and count data. The two-layer RTGGM has further been extended to produce deep models with multiple hidden layers. Methods have also been developed to estimate the partition function used in evaluating the unsupervised learning performance. Extensive experimental results have demonstrated the superior performance of RTGGMs in unsupervised learning of many types of data, as well as in unsupervised pre-training of feedforward ReLU neural networks."
    } ],
    "references" : [ {
      "title" : "T",
      "author" : [ "D.H. Ackley", "G.E. Hinton", "Sejnowski" ],
      "venue" : "J.",
      "citeRegEx" : "Ackley. Hinton. and Sejnowski 1985",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "and Bengio",
      "author" : [ "J. Bornschein" ],
      "venue" : "Y.",
      "citeRegEx" : "Bornschein and Bengio 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Enhanced gradient for training restricted boltzmann machines. Neural computation 25(3):805–831",
      "author" : [ "Raiko Cho", "K. Ilin 2013] Cho", "T. Raiko", "A. Ilin" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2013
    }, {
      "title" : "G",
      "author" : [ "B.J. Frey", "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Frey and Hinton 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "B",
      "author" : [ "Frey" ],
      "venue" : "J.",
      "citeRegEx" : "Frey 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "He" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "He,? \\Q2015\\E",
      "shortCiteRegEx" : "He",
      "year" : 2015
    }, {
      "title" : "R",
      "author" : [ "G.E. Hinton", "Salakhutdinov" ],
      "venue" : "R.",
      "citeRegEx" : "Hinton and Salakhutdinov 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "R",
      "author" : [ "G.E. Hinton", "Salakhutdinov" ],
      "venue" : "R.",
      "citeRegEx" : "Hinton and Salakhutdinov 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "G",
      "author" : [ "Hinton" ],
      "venue" : "E.; Osindero, S.; and Teh, Y.-W.",
      "citeRegEx" : "Hinton. Osindero. and Teh 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "G",
      "author" : [ "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Hinton 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "L",
      "author" : [ "J. Honorio", "D. Samaras", "N. Paragios", "R. Goldstein", "Ortiz" ],
      "venue" : "E.",
      "citeRegEx" : "Honorio et al. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "What is the best multi-stage architecture for object recognition",
      "author" : [ "Jarrett" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "Jarrett,? \\Q2009\\E",
      "shortCiteRegEx" : "Jarrett",
      "year" : 2009
    }, {
      "title" : "N",
      "author" : [ "Johnson" ],
      "venue" : "L.; Kotz, S.; and Balakrishnan, N.",
      "citeRegEx" : "Johnson. Kotz. and Balakrishnan 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A",
      "author" : [ "Y. Liu", "Willsky" ],
      "venue" : "2013. Learning gaussian graphical models with observed or latent fvss. In Advances in Neural Information Processing Systems, 1833–",
      "citeRegEx" : "Liu and Willsky 2013",
      "shortCiteRegEx" : null,
      "year" : 1841
    }, {
      "title" : "N",
      "author" : [ "B.M. Marlin", "K. Swersky", "B. Chen", "Freitas" ],
      "venue" : "D.",
      "citeRegEx" : "Marlin et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning latent variable gaussian graphical models",
      "author" : [ "Eriksson Meng", "Z. Hero 2014] Meng", "B. Eriksson", "A. Hero" ],
      "venue" : null,
      "citeRegEx" : "Meng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2014
    }, {
      "title" : "G",
      "author" : [ "V. Nair", "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Nair and Hinton 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "R",
      "author" : [ "Neal" ],
      "venue" : "M.",
      "citeRegEx" : "Neal 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "R",
      "author" : [ "Neal" ],
      "venue" : "M.",
      "citeRegEx" : "Neal 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "J",
      "author" : [ "J.H. Oh", "Deasy" ],
      "venue" : "O.",
      "citeRegEx" : "Oh and Deasy 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic neural networks with monotonic activation functions",
      "author" : [ "Ravanbakhsh" ],
      "venue" : null,
      "citeRegEx" : "Ravanbakhsh,? \\Q2016\\E",
      "shortCiteRegEx" : "Ravanbakhsh",
      "year" : 2016
    }, {
      "title" : "C",
      "author" : [ "Robert" ],
      "venue" : "P.",
      "citeRegEx" : "Robert 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "G",
      "author" : [ "R. Salakhutdinov", "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Salakhutdinov and Hinton 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Murray",
      "author" : [ "R. Salakhutdinov" ],
      "venue" : "I.",
      "citeRegEx" : "Salakhutdinov and Murray 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nonlinear statistical learning with truncated gaussian graphical models",
      "author" : [ "Su" ],
      "venue" : "In Proceedings of the 33st International Conference on Machine Learning (ICML-16)",
      "citeRegEx" : "Su,? \\Q2016\\E",
      "shortCiteRegEx" : "Su",
      "year" : 2016
    }, {
      "title" : "and Hinton",
      "author" : [ "T. Tieleman" ],
      "venue" : "G.",
      "citeRegEx" : "Tieleman and Hinton 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "G",
      "author" : [ "M. Welling", "M. RosenZvi", "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Welling. Rosen.Zvi. and Hinton 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Gaussian graphical models (GGMs) are widely used for statistical modeling, because of ease of inference and the ubiquitous use of the normal distribution in practical approximations. However, they are also known for their limited modeling abilities, due to the Gaussian assumption. In this paper, we introduce a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits efficient inference. Specifically, we impose a bipartite structure on the GGM and govern the hidden variables by truncated normal distributions. The nonlinearity of the model is revealed by its connection to rectified linear unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and appealing properties of truncated normals, we are able to train the models efficiently using contrastive divergence. We consider three output constructs, accounting for real-valued, binary and count data. We further extend the model to deep structures and show that deep models can be used for unsupervised pre-training of rectifier neural networks. Extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models.",
    "creator" : "LaTeX with hyperref package"
  }
}