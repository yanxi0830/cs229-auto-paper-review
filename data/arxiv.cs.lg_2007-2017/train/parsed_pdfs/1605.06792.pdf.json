{
  "name" : "1605.06792.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Active Nearest-Neighbor Learning in Metric Spaces",
    "authors" : [ "Aryeh Kontorovich", "Sivan Sabato", "Ruth Urner" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n06 79\n2v 2\n[ cs\n.L G\n] 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Active learning is a framework for reducing the amount of label supervision for prediction tasks. While labeling large amounts of data can be expensive and time-consuming, unlabeled data is often much easier to come by. In this paper we propose a non-parametric pool-based active learning algorithm for general metric spaces, which outputs a nearest-neighbor classifier.\nIn pool-based active learning [McCallum and Nigam, 1998], a collection of random examples is provided, and the algorithm can interactively query an oracle to label some of the examples. The goal is good prediction accuracy, while keeping the label complexity — that is, the number of labels queried — low. Our algorithm, MArgin Regularized Metric Active Nearest Neighbor (MARMANN), receives a pool of unlabeled examples in a general metric space, and outputs a variant of the 1-nearest-neighbor classifier, implemented by a 1-nearest-neighbor rule. The algorithm obtains a prediction error guarantee that depends on a noisy-margin property of the input sample, and has a provably smaller label complexity than any passive learner with a similar guarantee.\nActive learning has been mostly studied in a parametric setting, in which learning takes place with respect to a fixed hypothesis class with a bounded capacity. In contrast, the question of whether active querying strategies can yield label savings for non-parametric methods in a general setting, without distributional assumptions, had not been analyzed prior to this work. Here, we provide a first demonstration that this is indeed possible. We discuss related work in detail in Section 1.1 below.\nOur contributions. MARMANN is a new non-parametric pool-based active learning algorithm, which obtains an error guarantee competitive with that of a noisy-margin-based passive learner. Additionally, it provably uses significantly fewer labels in nontrivial regimes. This is the first non-parametric active learner for general metric spaces, which achieves competitive prediction error guarantees to the passive learner, while provably improving label complexity. The guarantees of MARMANN are given in Theorem 3.1 in Section 3. We further provide a passive learning lower bound (Theorem 3.2), which together with Theorem 3.1 shows that MARMANN can have a significantly reduced label complexity compared to any passive learner. The passive lower bound is more general than previous lower bounds, relies on a novel technique, and may be of independent interest. Additionally, we give an active label complexity lower bound (Theorem 3.3), which holds for any active learner with similar error guarantees as MARMANN. The proof of this active lower bound relies on a new No-Free-Lunch type result, which holds for active learning algorithms.\nOur approach. Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale.\nA central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information.\nWe derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al. [2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015]. The Bayes-consistency of the passive version of our approach is the subject of ongoing work.\nPaper structure. Related work is discussed in Section 1.1. We lay down the preliminaries in Section 2. In Section 3 we provide our main result: Theorem 3.1, which gives error and label complexity guarantees for MARMANN. Additionally we state the passive and active lower bounds, Theorem 3.2 and Theorem 3.3. The rest of the paper is devoted to the description and analysis of MARMANN, and proof of the main results. Section 4 shows how MARMANN defines the nearest neighbor rule for a given scale, and Section 5 describes the model selection procedure of MARMANN. Theorem 3.1 is proved in Section 6, based on a framework for compression with side information. The passive lower bound in Theorem 3.2 is proved in Section 7. The active lower bound Theorem 3.3 is proved in Section 8. We conclude with a discussion in Section 9."
    }, {
      "heading" : "1.1 Related work",
      "text" : "The theory of active learning has received considerable attention in the past decade [e.g., Dasgupta, 2004, Balcan et al., 2007, 2009, Hanneke, 2011, Hanneke and Yang, 2015]. Active learning theory has been mostly studied in a parametric setting (that is, learning with respect to a fixed hypothesis class with a bounded capacity). Benefits and limitations of various active querying strategies have been proven in the realizable setting [Dasgupta, 2004, Balcan et al., 2007, Gonen et al., 2013b,a] as well as in the agnostic case [Balcan et al., 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010].\nThe potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees.\nCastro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals. The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al., 2013, Kpotufe et al., 2015]. Dasgupta and Hsu [2008] showed that a suitable cluster-tree can yield label savings in this framework, and papers following up quantified the label savings under distributional clusterability assumptions. However, no active non-parametric strategy has been proposed so far that has label complexity guarantees for general distributions and for general metric spaces. Here, we provide the first such algorithm and guarantees.\nThe passive nearest-neighbor classifier, introduced by Fix and Hodges [1951, 1989], is popular among theorists and practitioners alike [Fix and Hodges, 1989, Cover and Hart, 1967, Stone, 1977, Kulkarni and Posner, 1995, Boiman et al., 2008]. This paradigm is applicable in general metric spaces, and its simplicity is an attractive feature for both implementation and analysis. When appropriately regularized — either by taking a majority vote among the k nearest neighbors [Stone, 1977, Devroye and Györfi, 1985, Zhao, 1987], or by enforcing a margin separating the classes [von Luxburg and Bousquet, 2004, Gottlieb et al., 2014a, Kontorovich and Weiss, 2015] — this type of learner can be made Bayes-consistent. Another desirable property of nearest-neighbor-based methods is their ability to generalize at a rate that scales with the intrinsic data dimension, which can be much lower than that of the ambient space [Kpotufe, 2011, Gottlieb et al., 2014a, 2016a, Chaudhuri and Dasgupta, 2014]. Furthermore, margin-based regularization makes nearest neighbor classifiers ideally suited for sample compression, which yields a compact representation, faster classification runtime, and improved generalization performance [Gottlieb et al., 2014b]. The resulting error guarantees can be stated in terms of the sample’s noisy-margin, which depends on the distances between differently-labeled examples in the input sample.\nActive learning strategies specific to nearest neighbor classification have recently received attention. It has been shown that certain active querying rules maintain Bayes consistency for nearest neighbor classification, while other, seemingly natural, rules do not lead\nto a consistent algorithm [Dasgupta, 2012]. A selective querying strategy has been shown to be beneficial for nearest neighbors under covariate shift [Berlind and Urner, 2015], where one needs to adapt to a change in the data generating process. However, the querying rule in that work is based solely on information in the unlabeled data, to account for a shift in the distribution over the covariates. It does not imply any label savings in the standard learning setting, where training and test distribution are identical. In contrast, our current work demonstrates how an active learner can take label information into account, to reduce the label complexity of a general nearest neighbor method in the standard setting."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section we lay down the necessary preliminaries. We formally define the setting and necessary notation in Section 2.1. We discuss nets in metric spaces in Section 2.2, and present the guarantees of the compression-based passive learner of Gottlieb et al. [2016b] in Section 2.3."
    }, {
      "heading" : "2.1 Setting and notation",
      "text" : "We consider learning in a general metric space (X , ρ), where X is a set and ρ is the metric on X . Our problem setting is that of classification of the instance space X into some finite label set Y . Assume that there is some distribution D over X × Y , and let S ∼ Dm be a labeled sample of size m, where m is an integer. Denote the sequence of unlabeled points in S by U(S). We sometimes treat S and U(S) as multisets, since the order is unimportant. For a labeled multiset S ⊆ X × Y and y ∈ Y , denote Sy := {x | (x, y) ∈ S}; in particular, U(S) = ∪y∈YSy .\nThe error of a classifier h : X → Y on D is denoted\nerr(h,D) := P[h(X) 6= Y ],\nwhere (X,Y ) ∼ D. The empirical error on a labeled sample S instantiates to\nerr(h, S) = 1 |S| ∑ I[h(X) 6= Y ].\nA passive learner receives a labeled sample Sin as input. An active learner receives the unlabeled part of the sample Uin := U(Sin) as input, and is allowed to adaptively select examples from Uin and request their label from Sin. When either learner terminates, it outputs a classifier ĥ : X → Y , with the goal of achieving a low err(ĥ,D). An additional goal of the active learner is to achieve a performance competitive with that of the passive learner, while querying considerably fewer labels.\nThe diameter of a set A ⊆ X is defined by\ndiam(A) := sup a,a′∈A\nρ(a, a′).\nDenote the index of the closest point in U to x ∈ X by\nκ(x, U) := argmin i:xi∈U ρ(x, xi).\nWe assume here and throughout this work that when there is more than one minimizer for ρ(x, xi), ties are broken arbitrarily (but in a consistent fashion). Any labeled sample S = ((xi, yi))i∈[k] naturally induces the nearest-neighbor classifier hnnS : X → Y , via hnnS (x) := yκ(x,U(S)). For a set Z ⊆ X , denote\nκ(Z,U) := {κ(z, U) | z ∈ Z}.\nFor x ∈ X , and t > 0, denote by ball(x, t) the (closed) ball of radius t around x:\nball(x, t) := {x′ ∈ X | ρ(x, x′) ≤ t} ."
    }, {
      "heading" : "2.2 Nets",
      "text" : "A set A ⊆ X is t-separated if infa,a′∈A:a 6=a′ ρ(a, a′) ≥ t. For A ⊆ B ⊆ X , the set A is a t-net of B if A is t-separated and B ⊆ ⋃a∈A ball(a, t).\nThe size of a t-net of a metric space is strongly related to its doubling dimension. The doubling dimension is the effective dimension of the metric space, which controls generalization and runtime performance of nearest-neighbors [Kpotufe, 2011, Gottlieb et al., 2014a]. It is defined as follows. Let λ = λ(X ) be the smallest number such that every ball in X can be covered by λ balls of half its radius, where all balls are centered at points of X . Formally,\nλ(X ) := min{λ ∈ N : ∀x ∈ X , r > 0, ∃x1, . . . , xλ ∈ X : ball(x, r) ⊆ ∪λi=1ball(xi, r/2)}.\nThen the doubling dimension of X is defined by ddim(X ) := log2 λ. In line with modern literature, we work in the low-dimension, large-sample regime, where the doubling dimension is assumed to be constant, and hence sample complexity and algorithmic runtime may depend on it exponentially. This exponential dependence is unavoidable, even under margin assumptions, as previous analyses [Kpotufe, 2011, Gottlieb et al., 2014a] indicate. Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al. [2014a].\nConstructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010]. However, a simple greedy algorithm constructs a (not necessarily minimal) t-net in time O(m2) [Gottlieb et al., 2014b, Algorithm 1]. There is also an algorithm for constructing a t-net in time 2O(ddim(X ))m log(1/t) [Krauthgamer and Lee, 2004, Gottlieb et al., 2014b]. The size of any t-net of a metric space A ⊆ X is at most ⌈diam(A)/t⌉ddim(X )+1 [Krauthgamer and Lee, 2004]. In addition, the size of any t-net is at most 2ddim(A) times the size of the minimal t-net, as the following easy lemma shows.\nLemma 2.1 (comparison of two nets). Let t > 0 and suppose that M1,M2 are t-nets of A ⊆ X . Then |M1| ≤ 2ddim(A)|M2|. Proof. Suppose that |M1| ≥ k|M2| for some positive integer k. SinceM1 ⊆ ⋃\nx∈M2 ball(x, t), it follows from the pigeonhole principle that at least one of the points in M2 must cover at least k points in M1. Thus, suppose that x ∈ M2 covers the set Z = {z1, . . . , zl} ⊆ M1, meaning that Z ⊆ ball(x, t), where l = |Z| ≥ k. By virtue of belonging to the t-net M1, the set Z is t-separated. Therefore, from the definition of the doubling dimension, we have |Z| ≤ 2ddim(A), hence k ≤ |Z| ≤ 2ddim(A).\nThroughout the paper, we fix a deterministic procedure for constructing a t-net, and denote its output for a multiset U ⊆ X by Net(U, t). Let Par(U, t) be a partition of X into regions induced by Net(U, t), that is: for Net(U, t) = {x1, . . . , xN}, define Par(U, t) := {P1, . . . , PN}, where Pi = {x ∈ X | κ(x,Net(U, t)) = i}. For t > 0, let N (t) := |Net(Uin, t)| be the size of the t-net for the input sample.\nAs shown in Gottlieb and Krauthgamer [2013], the doubling dimension is “almost hereditary” in the sense that forA ⊂ X , we have ddim(A) ≤ cddim(X ) for some universal constant c ≤ 2 [Feldmann et al., 2015, Lemma 6.6]. For simplicity, the bounds above are presented in terms of ddim(X ), the doubling dimension of the ambient space. It should be noted that one can obtain tighter bounds in terms of ddim(U(S)) when the latter is substantially lower than that of the ambient space, and it is also possible to perform metric dimensionality reduction, as in Gottlieb et al. [2013]."
    }, {
      "heading" : "2.3 Passive compression-based nearest-neighbors",
      "text" : "Non-parameteric binary classification admits performance guarantees that scale with the sample’s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension. Later, it was noticed in Gottlieb et al. [2014b] that the presence of a margin allows for compression — in fact, nearly optimally so.\nWe say that a labeled multiset S is (ν, t)-separated, for ν ∈ [0, 1] and t > 0 (representing a margin t with noise ν), if one can remove a ν-fraction of the points in S, and in the resulting multiset, points with different labels are at least t-far from each other. Formally, we have the following definition.\nDefinition 2.2. S is (ν, t)-separated if there exists a subsample S̃ ⊆ S such that\n1. |S \\ S̃| ≤ ν|S| and\n2. ∀y1 6= y2 ∈ Y, a ∈ S̃y1 , b ∈ S̃y2 , we have ρ(a, b) ≥ t.\nFor a given labeled sample S, denote by ν(t) the smallest value ν such that S is (ν, t)separated. Gottlieb et al. [2016b] propose a passive learner with the following guarantees1 as a function of the separation of S. Setting α := m/(m−N), define the following form of a generalization bound:\nGB(ǫ,N, δ,m, k) := αǫ+ 2\n3\n(N + 1) log(mk) + log(1δ )\nm−N + 3√ 2\n√\nαǫ((N + 1) log(mk) + log(1δ ))\nm−N .\nFurther, for an integer m and δ ∈ (0, 1), denote\nGmin(m, δ) := min t>0\nGB(ν(t),N (t), δ,m, 1).\n1The guarantees hold for the more general case of semimetrics.\nTheorem 2.3 (Gottlieb et al. [2016b]). Let m be an integer, δ ∈ (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier hnnSpas , where Spas ⊆ Sin, such that, with probability 1− δ,\nerr(hnnSpas ,D) ≤ Gmin(m, δ).\nThe passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|ν(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points). For the binary classification case (|Y| = 2) an efficient algorithm is shown in Gottlieb et al. [2016b]. However, in the general multiclass case, it is not known how to find a minimal t-separation efficiently — a naive approach requires solving the NP-hard problem of vertex cover. Our approach, which we detail below, circumvents this issue, and provides an efficient algorithm also for the multiclass case."
    }, {
      "heading" : "3 Main results",
      "text" : "We propose a novel approach for generating a subset for a nearest-neighbor rule. This approach, detailed in the following sections, does not require finding and removing all the obstructing points in Sin, and can be implemented in an active setting using a small number of labels. The resulting active learning algorithm, MARMANN, has an error guarantee competitive with that of the passive learner, and a label complexity that can be significantly lower. We term the subset used by the nearest-neighbor rule a compression set.\nAlgorithm 1 MARMANN: MArgin Regularized Metric Active Nearest Neighbor input Unlabeled sample Uin of size m, δ ∈ (0, 1). t̂ ← SelectScale(δ). # SelectScale is given in Section 5, Alg. 4. Ŝ ← GenerateNNSet(t̂, [N (t̂)], δ). # GenerateNNSet is given in Section 4, Alg. 2. Output hnn\nŜ .\nMARMANN, listed in Alg. 1, operates as follows. First, a scale t̂ > 0 is selected, by calling t̂ ← SelectScale(δ), where SelectScale is our model selection procedure. SelectScale has access to Uin, and queries labels from Sin as necessary. It estimates the generalization error bound GB for several different scales, and executes a procedure similar to binary search to identify a good scale. The binary search keeps the number of estimations (and thus requested labels) small. Crucially, our estimation procedure is designed to prevent the search from spending a number of labels that depends on the net size of the smallest possible scale t, so that the total label complexity of MARMANN depends only on the error of the selected t̂. Second, the selected scale t̂ is used to generate the compression set by calling Ŝ ← GenerateNNSet(t̂, [N (t̂)], δ), where GenerateNNSet is our procedure for generating the compression set. Our main result is the following guarantee for MARMANN.\nTheorem 3.1 (Main result; Guarantee for MARMANN). Let Sin ∼ Dm, where m ≥ max(6, |Y|), δ ∈ (0, 14 ). Let hnnŜ be the output of MARMANN(Uin, δ), where Ŝ ⊆ X × Y ,\nand let N̂ := |Ŝ|. Let ĥ := hnn Ŝ and ǫ̂ := err(ĥ, Sin), and denote Ĝ := GB(ǫ̂, N̂ , δ,m, 1). With a probability of 1− δ over Sin and randomness of MARMANN,\nerr(ĥ,D) ≤ 2Ĝ ≤ O (Gmin(m, δ)) ,\nand the number of labels from Sin requested by MARMANN is at most\nO\n(\nlog3( m\nδ )\n(\n1 Ĝ log( 1 Ĝ ) +mĜ\n))\n.\nHere the O(·) notation hides only universal multiplicative constants.\nOur error guarantee is thus a constant factor over the error guarantee of the passive learner of [Gottlieb et al., 2016b], given in Theorem 2.3.\nTo observe the advantages of MARMANN over a passive learner, consider a scenario in which the upper bound Gmin of Theorem 2.3, as well as the Bayes error of D, are of order Θ̃(1/ √ m). Then Ĝ = Θ(1/ √ m) as well. Therefore, MARMANN obtains a prediction error guarantee of Θ̃(1/ √ m), similarly to the passive learner, but it uses only Θ̃( √ m) labels instead of m. In contrast, the following result shows that no learner that selects labels uniformly at random from Sin can compete with MARMANN: Theorem 3.2 below shows that for any passive learner that uses Θ̃( √ m) random labels from Sin, there exists a distribution D with the above properties, for which the prediction error of the passive learner in this case is Ω̃(m−1/4), a decay rate which is almost quadratically slower than the Õ(1/ √ m) rate achieved by MARMANN. Thus, the guarantees of MARMANN cannot be matched by any passive learner.\nTheorem 3.2 (Passive lower bound). Let m > 0 be an integer, and suppose that (X , ρ) is a metric space such that for some t̄ > 0, there is a t̄-net T of X of size Θ(√m). Consider any passive learning algorithm that maps i.i.d. samples Sℓ ∼ Dℓ from some distribution D over X × {−1, 1}, to functions ĥℓ : X → {−1, 1}. For any such algorithm and any ℓ = Θ̃( √ m), there exists a distribution D such that:\ni. The Bayes error of D is Θ(1/√m);\nii. With at least a constant probability, both of the following events occur:\n(a) The passive learner achieves error err(ĥℓ,D) = Ω̃(m−1/4), (b) Gmin(m, δ) = Θ̃(1/ √ m).\nWe note that while this lower bound assumes that the passive learner observes only the random labeled sample of size ℓ, in fact the proof of this theorem holds also if the algorithm has access to the full unlabeled sample of size m, from which Sℓ is sampled. This proves that MARMANN even improves over a semi-supervised learner that has access to the same amount of unlabeled data. That is, the label savings of MARMANN stem from actively selecting labels, and are not achievable by merely exploiting information from unlabeled data or by randomly selecting examples to label.\nWe deduce Theorem 3.2 from a more general result, which might be of independent interest. Theorem 7.1, given in Section 7, improves existing passive learning sample complexity\nlower bounds. In particular, our result removes the restrictions of previous lower bounds on the relationship between the sample size, the VC-dimension, and the noise level, which render existing bounds inapplicable to our parameter regime. The proof of Theorem 3.2 is given thereafter in Section 7, as a consequence of Theorem 7.1.\nWe further provide a label complexity lower bound, in Theorem 3.3 below, which holds for any active learner that obtains similar guarantees to those of MARMANN. The lower bound shows that any such active learning algorithm has a label complexity which is Ω̃(mGmin(m, δ)), for a wide range of values of Gmin(m, δ). This implies that the term mĜ in the upper bound of the label complexity of MARMANN in Theorem 3.1 cannot be significantly improved.\nTheorem 3.3 (Active lower bound). Let X = R, δ ∈ (0, 1/14). Let C ≥ 1, and let A be an active learning algorithm that outputs ĥ. Suppose that for any distribution D over X × Y , if the input unlabeled sample is of size m, then err(ĥ,D) ≤ CGmin(m, δ) with probability at least 1− δ. Then for any α ∈ ( log(m)+log(28)\n8 √ 2m , 1240C ) there exists an a distribution D such with probability at least 128 over S ∼ Dm and the randomness of A, both of the following events hold:\n1. α ≤ Gmin(m, δ) ≤ 30α\n2. A queries at least 12 ⌊ mGmin(m,δ)−log(mδ ) 30 log(m) ⌋ labels.\nIn the rest of the paper, the components of MARMANN are described in detail, and the main results are proved."
    }, {
      "heading" : "4 Active nearest-neighbor at a given scale",
      "text" : "A main challenge for active learning in our non-parametric setting is performing model selection, that is, selecting a good scale t similarly to the passive learner of Gottlieb et al. [2016b]. In the passive supervised setting, the approach developed in several previous works [Gottlieb et al., 2010, 2014b, Kontorovich and Weiss, 2014, Gottlieb et al., 2014a, Kontorovich and Weiss, 2015] performs model selection by solving a minimum vertex cover problem for each considered scale t, so as to eliminate all of the t-blocking pairs — i.e., pairs of differently labeled points within a distance t. The passive algorithm generates a compression set by first finding and removing from Sin all points that obstruct (ν, t)-separation at a given scale t > 0. This incurs a computational cost but no significant sample complexity increase, aside from the standard logarithmic factor that comes from stratifying over data-dependent hierarchies [Shawe-Taylor et al., 1998].\nWhile this approach works for passive learning, in the active setting we face a crucial challenge: estimating the error of a nearest-neighbor rule at scale t using a small number of samples. A key insight that we exploit in this work is that instead of eliminating the blocking pairs, one may simply relabel some of the points in the compression set, and this would also generate a low-error nearest neighbor rule. This new approach enables estimation of the sample accuracy of a (possibly relabeled) t-net by label-efficient active sampling. In addition, this approach is significantly simpler than estimating the size of the minimum vertex cover of the t-blocking graph. Moreover, we gain improved algorithmic efficiency, by avoiding the relatively expensive vertex cover procedure.\nA small technical difference, which will be evident below, is that in this new approach, examples in the compression set might have a different label than their original label in Sin. Standard sample compression analysis [e.g. Graepel et al., 2005] assumes that the classifier is determined by a small number of labeled examples from Sin. This does not allow the examples in the compression set to have a different label than their original label in Sin. Therefore, we require a slight generalization of previous compression analysis (following previous works on compression, see details in Section 6.1), which allows adding side information to the compression set. This side information will be used to set the label of each of the examples in the compression set. The generalization incurs a small statistical penalty, which we quantify in Section 6, as a preliminary to proving Theorem 3.1.\nWe now describe our approach to generating a compression set for a given scale t > 0. Recall that ν(t) is the smallest value for which Sin is (ν, t)-separated. We define two compression sets. The first one, denoted Sa(t), represents an ideal compression set, which induces an empirical error of at most ν(t), but calculating it might require many labels. The second compression set, denoted Ŝa(t), represents an approximation to Sa(t), which can be constructed using a small number of labels, and induces a sample error of at most 4ν(t) with high probability. MARMANN constructs only Ŝa(t), while Sa(t) is defined solely for the sake of analysis.\nWe first define the ideal set Sa(t) := {(x1, y1), . . . , (xN , yN)}. The examples in Sa(t) are the points in Net(Uin, t/2), and the label of each example is the majority label, out of the labels of the examples in Sin to which xi is closest. Formally, {x1, . . . , xN} := Net(Uin, t/2), and for i ∈ [N ], yi := argmaxy∈Y |Sy ∩ Pi|, where Pi = {x ∈ X | κ(x,Net(U, t/2)) = i} ∈ Par(Uin, t/2). Lemma 4.1. Let S be a labeled sample of size m, and let {P1, . . . , PN} be a partition of U(S), with maxi diam(Pi) ≤ t for some t ≥ 0. For i ∈ [N ], let Λi := Syi ∩ Pi. Then\nν(t) ≥ 1− 1 m ∑\ni∈[N ] |Λi|.\nProof. Let S̃ ⊆ S be a subsample that witnesses the (ν(t), t)-separation of S, so that |S̃| ≥ m(1− ν(t)), and for any two points (x, y), (x′, y′) ∈ S̃, if ρ(x, x′) ≤ t then y = y′. Denote Ũ := U(S̃). Since maxi diam(Pi) ≤ t, for any i ∈ [N ] all the points in Ũ ∩Pi must have the same label in S̃. Therefore,\n∃y ∈ Y s.t. Ũ ∩ Pi ⊆ S̃y ∩ Pi.\nHence |Ũ ∩ Pi| ≤ |Λi|. It follows\n|S| − ∑\ni∈[N ] |Λi| ≤ |S| −\n∑\ni∈[N ] |Ũ ∩ Pi| = |S| − |S̃| = m · ν(t).\nDividing by m we get the statement of the lemma.\nFrom Lemma 4.1, we get the following corollary, which upper bounds the empirical error of hnnSa(t) by ν(t).\nCorollary 4.2. For every t > 0, err(hnnSa(t), Sin) ≤ ν(t).\nThis corollary is immediate from Lemma 4.1, since for anyPi ∈ Par(Uin, t/2), diam(Pi) ≤ t, and\nerr(hnnSa(t), Sin) = 1− 1\nm\n∑\ni∈[N ] |Λi|.\nNow, calculating Sa(t) requires knowing most of the labels in Sin. MARMANN constructs instead an approximation Ŝa(t), in which the examples are the points in Net(Uin, t/2) (so that U(Ŝa(t)) = U(Sa(t)) ), but the labels are determined using a bounded number of labels requested from Sin. The labels in Ŝa(t) are calculated by the simple procedure GenerateNNSet given in Alg. 2. The empirical error of the output of GenerateNNSet is bounded in Theorem 4.3 below.2\nA technicality in Alg. 2 requires explanation: In MARMANN, the generation of Ŝa(t) will be split into several calls to GenerateNNSet, so that different calls determine the labels of different points in Ŝa(t). Therefore GenerateNNSet has an additional argument I , which specifies the indices of the points in Net(Uin, t/2) for which the labels should be returned this time. Crucially, if during the run of MARMANN, GenerateNNSet is called again for the same scale t and the same point in Net(Uin, t/2), then GenerateNNSet returns the same label that it returned before, rather than recalculating it using fresh labels from Sin. This guarantees that despite the randomness in GenerateNNSet, the full Ŝa(t) is well-defined within any single run of MARMANN, and is distributed like the output of GenerateNNSet(t, [N (t/2)], δ), which is convenient for the analysis. Define\nQ := ⌈ 18 log(4m3/δ) ⌉ . (1)\nAlgorithm 2 GenerateNNSet(t, I, δ) input Scale t > 0, a target set I ⊆ [N (t/2)], confidence δ ∈ (0, 1). output A labeled set S ⊆ X × Y of size |I| {x1, . . . , xN} ← Net(Uin, t/2), {P1, . . . , PN} ← Par(Uin, t/2), S ← () for i ∈ I do\nif ŷi has not already been calculated for Uin with this value of t then Draw Q points uniformly at random from Pi and query their labels. Let ŷi be the majority label observed in these Q queries. end if S ← S ∪ {(xi, ŷi)}.\nend for Output S\nTheorem 4.3. Let Ŝa(t) be the output of GenerateNNSet(t, [N (t/2)], δ). With a probability at least 1− δ2m2 , the following event, which we denote by E(t), holds:\nerr(hnn Ŝa(t) , Sin) ≤ 4ν(t). 2In the case of binary labels (|Y| = 2), the problem of estimating Sa(t) can be formulated as a special case of the benign noise setting for parametric active learning, for which tight lower and upper bounds are provided in Hanneke and Yang [2015]. However, our case is both more general (as we allow multiclass labels) and more specific (as we are dealing with a specific “hypothesis class”). Thus we provide our own procedure and analysis.\nProof. By Cor. 4.2, err(hnnSa(t), Sin) ≤ ν(t). In Sa(t), the labels assigned to each point in Net(Uin, t/2) are the majority labels (based onSin) of the points in the regions inPar(Uin, t/2). As above, we denote the majority label for region Pi by yi := argmaxy∈Y |Sy ∩ Pi|. We now compare these labels to the labels ŷi assigned by Alg. 2. Let p(i) = |Λi|/|Pi| be the fraction of points in Pi which are labeled by the majority label yi, where Λi is as defined in Lemma 4.1. Let p̂(i) be the fraction of labels equal to yi out of those queried by Alg. 2 in round i. Let β := 1/6. By Hoeffding’s inequality and union bounds, we have that with a probability of at least\n1− 2N (t/2) exp(−Q 18 ) ≥ 1− δ 2m2 ,\nwe have maxi∈[N (t/2)] |p̂(i)− p(i)| ≤ β. Denote this “good” event by E′. We now prove that E′ ⇒ E(t). Let J ⊆ [N (t/2)] = {i | p̂(i) > 12}. It can be easily seen that ŷi = yi for all i ∈ J . Therefore, for all x such that κ(x,U(Sa(t))) ∈ J , hnnŜa(t)(x) = h nn Sa(t) (x), and hence err(hnnS , Sin) ≤ PX∼Sin [κ(X,U(Sa(t))) /∈ J ] + err(hnnSa(t), Uin).\nThe second term is at most ν(t) by Cor. 4.2, and it remains to bound the first term, on the condition that E′ holds. We have PX∼U [κ(X,U(Sa(t))) /∈ J ] = 1m ∑\ni/∈J |Pi|. If E′ holds, then for any i /∈ J , p(i) ≤ 12 + β, therefore\n|Pi| − |Λi| = (1− p(i))|Pi| ≥ ( 1\n2 − β)|Pi|.\nRecall that, by Lemma 4.1, ν(t) ≥ 1− 1m ∑ i∈[N (t/2)] |Λi|. Therefore,\nν(t) ≥ 1− 1 m ∑\ni∈[N (t/2)] |Λi|\n= 1\nm\n∑\ni∈[N (t/2)] (|Pi| − |Λi|)\n≥ 1 m ∑\ni/∈J (|Pi| − |Λi|)\n≥ 1 m ∑ i/∈J ( 1 2 − β)|Pi|.\nThus, under E′,\nPX∼U [κ(X,U(Sa(t))) /∈ J ] ≤ ν(t) 1 2 − β = 3ν(t).\nIt follows that under E′, err(hnnS , Uin) ≤ 4ν(t)."
    }, {
      "heading" : "5 Model Selection",
      "text" : "We now show how to select the scale t̂ that will be used to generate the output nearestneighbor rule. The main challenge is to do this with a low label complexity: Generating the\nfull classification rule for scale t requires a number of labels that depends on N (t), which might be very large. We would like the label complexity of MARMANN to depend only on N (t̂) (where t̂ is the selected scale), which is of the order mĜ. Therefore, during model selection we can only invest a bounded number of labels in each tested scale. In addition, to keep the label complexity low, we would like to avoid testing all scales. In Section 5.1 we describe how we estimate the error on a given scale. In Section 5.2 we provide a search procedure, resembling binary search, which uses the estimation procedure to select a single scale t̂."
    }, {
      "heading" : "5.1 Estimating the error at a given scale",
      "text" : "For t > 0, let Ŝa(t) be the compressed sample that MARMANN would generate if the selected scale were set to t. Our model selection procedure performs a search, similar to binary search, over the possible scales. For each tested scale t, the procedure estimates the empirical error ǫ(t) := err(hnn\nŜa(t) , S) within a certain accuracy, using an estimation\nprocedure given below, called EstimateErr. EstimateErr outputs an estimate ǫ̂(t) of ǫ(t), up to a given threshold θ > 0, using labels requested from Sin.\nTo estimate the error, we sample random labeled examples from Sin, and check the prediction error of hnn\nŜa(t) on these examples. The prediction error of any fixed hypothesis h\non a random labeled example from Sin is an independent Bernoulli variable with expectation err(h, Sin). EstimateErr is implemented using the following procedure, EstBer, which adaptively estimates the expectation of a Bernoulli random variable to an accuracy specified by the parameter θ, using a small number of random independent Bernoulli experiments. Let B1, B2, . . . ∈ {0, 1} be i.i.d. Bernoulli random variables. For an integer n, denote p̂n = 1 n ∑n i=1 Bi. The estimation procedure EstBer is given in Alg. 3. We prove a guarantee for this procedure in Theorem 5.1.\nAlgorithm 3 EstBer(θ, β, δ) input A threshold parameter θ > 0, a budget parameter β ≥ 7, confidence δ ∈ (0, 1) S ← {B1, . . . , B4} K ← 4βθ log( 8β δθ )\nfor i = 3 : ⌈log2(β log(2K/δ)/θ)⌉ do n ← 2i S ← S ∪ {Bn/2+1, . . . , Bn}. if p̂n > β log(2n/δ)/n then\nbreak end if end for Output p̂n.\nTheorem 5.1. Let δ ∈ (0, 1), θ > 0, β ≥ 7. Let B1, B2, . . . ∈ {0, 1} be i.i.d Bernoulli random variables with expectation p. Let po be the output of EstBer(θ, β, δ). The following holds with a probability of 1− δ, where f(β) := 1 + 83β + √ 2 β .\n1. If po ≤ θ, then p ≤ f(β)θ. Otherwise, pf(β) ≤ po ≤ p 2−f(β) .\n2. Letψ := max(θ, p/f(β)). The number of random draws in EstBer is at most 8β log( 8β δψ )\nψ .\nProof. First, consider any single round i with n = 2i. By the empirical Bernstein bound [Maurer and Pontil, 2009, Theorem 4],with a probability of 1− δ/n, for n ≥ 8, we have3\n|p̂n − p| ≤ 8 log(2n/δ)\n3n +\n√\n2p̂n log(2n/δ)\nn . (2)\nDefine g := (β + 8/3 + √ 2β), so that f(β) = g/β. Conditioned on Eq. (2), there are two cases:\n(a) p̂n ≤ β log(2n/δ)/n. In this case, p ≤ g log(2n/δ)/n.\n(b) p̂n > β log(2n/δ)/n. In this case, n ≥ β log(2n/δ)/p̂n. Thus, by Eq. (2),\n|p̂n − p| ≤ p̂n( 8 3β + √ 2/β) = p̂n(g/β − 1).\nTherefore βp\ng ≤ p̂n ≤\np\n2− g/β .\nTaking a union bound on all the rounds, we have that the guarantee holds for all rounds with a probability of at least 1− δ.\nCondition now on the event that these guarantees all hold. First, we prove the label complexity bound. Note that since β ≥ 7, K ≥ 28, thus we have 2 log(2K) > 8, therefore there is always at least one round. Let no be the value of n in the last round the algorithm runs, and let po = p̂no . Suppose that the algorithm reaches round i. To reach round i + 1, it must have p̂n ≤ β log(2n/δ)/n for n = 2i, therefore βpg ≤ p̂n ≤ p 2−g/β , which means\np ≤ gp̂n/β ≤ g log(2n/δ)/n.\nTherefore, if the algorithm reaches round i+ 1, n ≤ g log(2n/δ)/p. It follows that\nno ≤ 2g log(4g/(δp))/p.\nIn addition, the number of random draws in the algorithm is at most\n2no ≤ 2⌈log2(β log(2K/δ)/θ)⌉ ≤ 2 · 2log2(β log(2K/δ)/θ) ≤ 4β log(2K/δ)/θ.\nTherefore we have the following bound on the number of random draws:\n2no ≤ min( 4β log(2K/δ) θ , 4g log(4g/(δp)) p ).\n3This follows from Theorem 4 of Maurer and Pontil [2009] since 7 3(n−1) ≤ 8 3n for n ≥ 8.\nPlugging in the definition of K yields\nβmin( 4 log(8βδθ log( 8β δθ )) θ , 4f(β) log(4βf(β)δp ) p ) ≤\nβmin( 8 log(8βδθ )) θ , 4f(β) log(4βf(β)δp ) p ) ≤\n8βmin( 1\nθ (log(\n8β\nδ ) + log(\n1 θ )), f(β) p (log( 4β δ ) + log( f(β) p ))).\nUsing the definition of ψ, we get that the number of draws is at most 8β log( 8β δψ )\nψ . Next, we prove the accuracy of po (item 1 in the theorem statement) by considering two\ncases.\n(I) If po > β log(2no/δ)/no, then case (b) above holds for no, thus\nβp\ng ≤ po ≤\np\n2− g/β .\nIn addition, if po ≤ θ, the LHS implies p ≤ f(β)θ. Thus item 1 in the theorem statement holds in this case.\n(II) If po ≤ β log(2no/δ)/no, then EstBer could not have ended by breaking out of the loop, thus it ran until the last round. Therefore no ≥ β log(2K/δ)/θ. In addition, case (a) holds, therefore\np ≤ g log(2no/δ) no ≤ gθ log(2n0/δ) β log(2K/δ) . (3)\nNow, for any possible value of no,\nno ≤ 2β log(2K/δ)/θ ≤ K.\nThe first inequality follows from the bound on i in EstBer, and the second inequality holds since, as defined in EstBer, K ≥ 4βθ log( 8β θδ ). Since n0 ≤ K , Eq. (3) implies that\np ≤ gθ β = f(β)θ.\nIn addition, we have\npo ≤ β log(2no/δ)/no ≤ θ log(2n0/δ)\nlog(2K/δ) ≤ θ.\nTherefore in this case, necessarily po ≤ θ and p ≤ f(β)θ, which satisfies item 1 in the theorem statement.\nIn both cases item 1 holds, thus the theorem is proved.\nThe procedure EstimateErr(t, θ, δ) is then implemented as follows:\n• Call EstBer(θ, 52, δ/(2m2)), where the random variables Bi are independent copies of the Bernoulli variable\nB := I[hnn Ŝa(t) (X) 6= Y ]\nand (X,Y ) ∼ Sin.\n• To draw a single Bi, sample a random pair (x′, y′) from Sin, set\ni := κ(x′,Net(Uin, t/2)),\nand get S ← GenerateNNSet(t, {i}, δ). This returns S = ((xi, ŷi)) where ŷi is the label of xi in Ŝa(t). Then Bi := I[ŷi 6= y′]. Note that Bi is indeed distributed like B, and E[B] = ǫ(t). Note further that this call to GenerateNNSet(t, {i}, δ) uses Q(m) label queries. Therefore the overall label complexity of a single draw of a Bi is Q(m) + 1.\nCor. 5.2 gives a guarantee for the accuracy and label complexity of EstimateErr. The proof is immediate from Theorem 5.1, by setting β = 52, which implies f(β) ≤ 5/4.\nCorollary 5.2. Let t, θ > 0 and δ ∈ (0, 1), and let ǫ̂(t) ← EstimateErr(t, θ, δ). Let Q as defined in Eq. (1) The following properties hold with a probability of 1 − δ2m2 over the randomness of EstimateErr (and conditioned on Ŝa(t)).\n1. If ǫ̂(t) ≤ θ, then ǫ(t) ≤ 5θ/4. Otherwise,\n4ǫ(t) 5 ≤ ǫ̂(t) ≤ 4ǫ(t) 3 .\n2. Let ψ′ := max(θ, ǫ(t)). The number of labels that EstimateErr requests is at most\n520(Q+ 1) log(1040m 2\nδψ′ )\nψ′ .\nTo derive item 2. above from Theorem 5.1, note that for β = 52,\nψ′ = max(θ, ǫ(t)) ≤ f(β)max(θ, ǫ(t)/f(β)) = f(β)ψ ≤ 5 4 ψ,\nwhere ψ is as defined in Theorem 5.1. Below we denote the event that the two properties in Cor. 5.2 hold for t by V (t)."
    }, {
      "heading" : "5.2 Selecting a scale",
      "text" : "The model selection procedure SelectScale, given in Alg. 4, implements its search based on the guarantees in Cor. 5.2. First, we introduce some notation. We would like MARMANN to obtain a generalization guarantee that is competitive with Gmin(m, δ). Denote\nφ(t) := (N (t) + 1) log(m) + log(1δ )\nm , (4)\nand let\nG(ǫ, t) := ǫ+ 2\n3 φ(t) + 3√ 2 √ ǫφ(t).\nNote that for all ǫ, t,\nGB(ǫ,N (t), δ,m, 1) = m m−N (t)G(ǫ, t).\nWhen referring to G(ν(t), t), G(ǫ(t), t), or G(ǫ̂(t), t) we omit the second t for brevity. Instead of directly optimizing G(ν(t)), we will select a scale based on our estimate G(ǫ̂(t)) of G(ǫ(t)). Let Dist denote the set of pairwise distances in the unlabeled dataset Uin (note that |Dist| < ( m 2 )\n). We remove from Dist some distances, so that the remaining distances have a net size N (t) that is monotone non-increasing in t. We also remove values with a very large net size. Concretely, define\nDistmon := Dist \\ {t | N (t) + 1 > m/2} \\ {t | ∃t′ ∈ Dist, t′ < t and N (t′) < N (t)}.\nThen for all t, t′ ∈ Distmon such that t′ < t, we have N (t′) ≥ N (t). The output of SelectScale is always a value in Distmon. The following lemma shows that it suffices to consider these scales.\nLemma 5.3. Assume m ≥ 6 and let t∗m ∈ argmint∈Dist G(ν(t)). If Gmin(m, δ) ≤ 1/3 then t∗m ∈ Distmon. Proof. Assume by way of contradiction that t∗m ∈ Dist \\Distmon. First, since G(ν(t∗m)) ≤ Gmin(m, δ) ≤ 1/3 we have\nN (t∗m) + 1 m−N (t∗m) log(m) ≤ 1 2 .\nTherefore, since m ≥ 6, it is easy to verify N (t∗m) + 1 ≤ m/2. Therefore, by definition of Distmon there exists a t ≤ t∗m with φ(t) < φ(t∗m). Since ν(t) is monotone over all of t ∈ Dist, we also have ν(t) ≤ ν(t∗m). Now, φ(t) < φ(t∗m) and ν(t) ≤ ν(t∗m) together imply that G(ν(t)) < G(ν(t∗m)), a contradiction. Hence, t ∗ m ∈ Distmon.\nSelectScale follows a search procedure similar to binary search, however the conditions for going right and for going left are not exhaustive, thus it is possible that neither condition holds. The search ends either when neither conditions hold, or when no additional scale should be tested. The final output of the algorithm is based on minimizing G(ǫ̂(t)) over some of the values tested during search.\nFor c > 0, define\nγ(c) := 1 + 2\n3c + 3√ 2c and γ̃(c) := 1 c + 2 3 + 3√ 2c .\nFor all t, ǫ > 0 we have the implications\nǫ ≥ cφ(t) ⇒ γ(c)ǫ ≥ G(ǫ, t) and φ(t) ≥ cǫ ⇒ γ̃(c)φ(t) ≥ G(ǫ, t). (5)\nThe following lemma uses Eq. (5) to show that the estimate G(ǫ̂(t)) is close to the true G(ǫ(t)).\nAlgorithm 4 SelectScale(δ) input δ ∈ (0, 1) output Scale t̂\n1: T ← Distmon, # T maintains the current set of possible scales 2: while T 6= ∅ do 3: t ← the median value in T # break ties arbitrarily 4: ǫ̂(t) ← EstimateErr(t, φ(t), δ). 5: if ǫ̂(t) < φ(t) then 6: T ← T \\ [0, t] # go right in the binary search 7: else if ǫ̂(t) > 1110φ(t) then 8: T ← T \\ [t,∞) # go left in the binary search 9: else\n10: t0 ← t, T0 ← {t0}. 11: break from loop 12: end if 13: end while 14: if T0 was not set yet then 15: If the algorithm ever went to the right, let t0 be the last value for which this happened, and let T0 := {t0}. Otherwise, T0 := ∅. 16: end if 17: Let TL be the set of all t that were tested and made the search go left 18: Output t̂ := argmint∈TL∪T0 G(ǫ̂(t))\nLemma 5.4. Let t > 0, δ ∈ (0, 1), and suppose that SelectScale calls ǫ̂(t) ← EstimateErr(t, φ(t), δ). Suppose that V (t) as defined in Cor. 5.2 holds. Then\n1 6 G(ǫ̂(t)) ≤ G(ǫ(t)) ≤ 6.5G(ǫ̂(t)).\nProof. Under V (t), we have that if ǫ̂(t) < φ(t) then ǫ(t) ≤ 54φ(t). In this case,\nG(ǫ(t)) ≤ γ̃(4/5)φ(t) ≤ 4.3φ(t),\nby Eq. (5). Therefore\nG(ǫ(t)) ≤ 3 · 4.3 2 G(ǫ̂(t)).\nIn addition, G(ǫ(t)) ≥ 23φ(t) (from the definition of G), and by Eq. (5) and γ̃(1) ≤ 4,\nφ(t) ≥ 1 4 G(ǫ̂(t)).\nTherefore G(ǫ(t)) ≥ 16G(ǫ̂(t)). On the other hand, if ǫ̂(t) ≥ φ(t), then by Cor. 5.2 4\n5 ǫ(t) ≤ ǫ̂(t) ≤ 4 3 ǫ(t).\nTherefore G(ǫ̂(t)) ≤ 43G(ǫ(t)) and G(ǫ(t)) ≤ 54G(ǫ̂(t)). Taking the worst-case of both possibilities, we get the bounds in the lemma.\nThe next theorem bounds the label complexity of SelectScale. Let Ttest ⊆ Distmon be the set of scales that are tested during SelectScale (that is, their ǫ̂(t) was estimated).\nTheorem 5.5. Suppose that the event V (t) defined in Cor. 5.2 holds for all t ∈ Ttest for the calls ǫ̂(t) ← EstimateErr(t, φ(t), δ). If the output of SelectScale is t̂, then the number of labels requested by SelectScale is at most\n19240|Ttest|(Q+ 1) 1\nG(ǫ(t̂)) log(\n38480m2 δG(ǫ(t̂)) ).\nProof. The only labels used by the procedure are those used by calls to EstimateErr. Let ψt := max(φ(t), ǫ(t)), and ψmin := mint∈Ttest ψt. Denote also ψ̂t := max(φ(t), ǫ̂(t)). From Cor. 5.2 we have that the total number of labels in all the calls to EstimateErr in SelectScale is at most\n∑\nt∈Ttest\n520(Q+ 1) log(1040m 2\nδψt )\nψt ≤ |Ttest|\n520(Q+ 1) log(1040m 2\nδψmin )\nψmin . (6)\nWe now lower bound ψmin using G(ǫ(t̂)). By Lemma 5.4 and the choice of t̂,\nG(ǫ(t̂)) ≤ 6.5G(ǫ̂(t̂)) = 6.5 min t∈TL∪T0 G(ǫ̂(t)).\nFrom the definition of G, for any t > 0,\nG(ǫ̂(t)) ≤ γ(1)max(φ(t), ǫ̂(t)) ≤ 25ψ̂t.\nTherefore G(ǫ(t̂)) ≤ 25 min\nt∈TL∪T0 ψ̂t. (7)\nWe will show a similar upper bound when minimizing over all of Ttest and not just over TL ∪ T0. This is trivial if Ttest = TL ∪ T0, therefore consider now the case TL ∪ T0 ( Ttest. For any t ∈ Ttest, one of the following options hold:\n• The search went left on t (step 8), hence t ∈ TL.\n• The search went nowhere on t and the loop broke (step 11), hence t = t0 ∈ T0.\n• The search went right on t (step 6) and this was the last value for which this happened, hence t = t0 ∈ T0.\n• The search went right on t (step 6) and this was not the last value for which this happened. Hence t ∈ Ttest \\ (TL ∪ T0).\nSet some t1 ∈ Ttest \\ (TL ∪ T0). Since the search went right on t1, then t0 also exists, since the algorithm did go to the right for some t (see step 15). Since the binary search went right on t1, we have ǫ̂(t1) ≤ φ(t1). Since the binary search did not go left on t0 (it either broke from the loop or went right), ǫ̂(t0) ≤ 1110φ(t0).\nIn addition, t0 ≥ t1 (since the search went right at t1, and t0 was tested later than t1), thus φ(t0) ≤ φ(t1) (since t0, t1 ∈ Distmon). Therefore,\nψ̂t0 = max(φ(t0), ǫ̂(t0)) ≤ 11\n10 φ(t0) ≤\n11 10 φ(t1) = 11 10 max(φ(t1), ǫ̂(t1)) = ψ̂t1 .\nIt follows that for any such t1,\nmin t∈TL∪T0\nψ̂t ≤ 11\n10 ψ̂t1 .\nTherefore\nmin t∈TL∪T0\nψ̂t ≤ 11\n10 min t∈Ttest ψ̂t.\nTherefore, by Eq. (7) G(ǫ(t̂)) ≤ 27.5 min\nt∈Ttest ψ̂t."
    }, {
      "heading" : "By Cor. 5.2, ǫ̂(t) ≤ max(φ(t), 4ǫ(t)/3), therefore ψ̂t ≤ 43ψt. Therefore G(ǫ(t̂)) ≤ 37ψmin.",
      "text" : "Therefore, from Eq. (6), the total number of labels is at most\n19240|Ttest|(Q+ 1) 1\nG(ǫ(t̂)) log(\n38480m2 δG(ǫ(t̂)) ).\nThe following theorem provides a competitive error guarantee for the selected scale t̂.\nTheorem 5.6. Suppose that V (t) and E(t), defined in Cor. 5.2 and Theorem 4.3, hold for all values t ∈ Ttest, and that Gmin(m, δ) ≤ 1/3. Then SelectScale outputs t̂ ∈ Distmon such that GB(ǫ(t̂),N (t̂), δ,m, 1) ≤ O(Gmin(m, δ)), Where the O(·) notation hides only universal multiplicative constants.\nThe full proof of this theorem is given below. The idea of the proof is as follows: First, we show (using Lemma 5.4) that it suffices to prove that G(ν(t∗m)) ≥ O(G(ǫ̂(t̂))) to derive the bound in the theorem. Now, SelectScale ends in one of two cases: either T0 is set within the loop, or T = ∅ and T0 is set outside the loop. In the first case, neither of the conditions for turning left and turning right holds for t0, so we have ǫ̂(t0) = Θ(φ(t0)) (where Θ hides numerical constants). We show that in this case, whether t∗m ≥ t0 or t∗m ≤ t0, G(ν(t∗m)) ≥ O(G(ǫ̂(t0))). In the second case, there exist (except for edge cases, which are also handled) two values t0 ∈ T0 and t1 ∈ TL such that t0 caused the binary search to go right, and t1 caused it to go left, and also t0 ≤ t1, and (t0, t1) ∩Distmon = ∅. We use these facts to show that for t∗m ≥ t1, G(ν(t∗m)) ≥ O(G(ǫ̂(t1))), and for t∗m ≤ t0, G(ν(t∗m)) ≥ O(G(ǫ̂(t0))). Since t̂ minimizes over a set that includes t0 and t1, this gives G(ν(t∗m)) ≥ O(G(ǫ̂(t̂))) in all cases.\nof Theorem 5.6. First, note that it suffices to show that there is a constant C, such that for the output t̂ of SelectScale, we have G(ǫ(t̂)) ≤ CG(ν(t∗m)). This is because of the following argument: From Lemma 5.3 we have that if Gmin(m, δ) ≤ 1/3, then t∗m ∈ Distmon. Now\nGmin(m, δ) = m m−N (t∗m) G(ν(t∗m)) ≥ G(ν(t∗m)).\nAnd, if we have the guarantee on G(ǫ(t̂)) and Gmin(m, δ) ≤ 1/3 we will have\nGB(ǫ(t̂),N (t̂), δ,m, 1) = m m−N (t̂)G(ǫ(t̂)) ≤ 2G(ǫ(t̂)) ≤ 2CG(ν(t ∗ m)) ≤ 2CGmin(m, δ).\n(8) We now prove the existence of such a guarantee and set C. Denote the two conditions checked in SelectScale during the binary search by Condition 1: ǫ̂(t) < φ(t) and Condition 2: ǫ̂(t) > 1110φ(t). The procedure ends in one of two ways: either T0 is set within the loop (Case 1), or T = ∅ and T0 is set outside the loop (Case 2). We analyze each case separately.\nIn Case 1, none of the conditions 1 and 2 hold for t0. Therefore\nφ(t0) ≤ ǫ̂(t0) ≤ 11\n10 φ(t0).\nTherefore, by Eq. (5),\nφ(t0) ≥ G(ǫ̂(t0))/γ̃( 10\n11 ).\nBy Cor. 5.2, since ǫ̂(t0) > φ(t0),\n3 4 φ(t0) ≤ 3 4 ǫ̂(t0) ≤ ǫ(t0) ≤ 5 4 ǫ̂(t0) ≤ 55 40 φ(t0).\nSuppose t∗m ≥ t0, then\nG(ν(t∗m)) ≥ ν(t∗m) ≥ ν(t0) ≥ 1\n4 ǫ(t0) ≥\n3\n16 φ(t0).\nhere we used ǫ(t0) ≤ 4ν(t0) by Theorem 4.3. Therefore, from Eq. (5) and Lemma 5.4,\nG(ν(t∗m)) ≥ 3\n16 φ(t0) ≥\n3 16γ̃ (\n40 55\n)G(ǫ(t0)) ≥ 1 2\n16γ̃(4055 ) G(ǫ̂(t0)).\nNow, suppose t∗m < t0, then\nG(ν(t∗m)) ≥ 2 3 φ(t∗m) ≥ 2 3 φ(t0) ≥\n2\n3γ̃(1011 ) G(ǫ̂(t0)).\nIn this inequality we used the fact that t∗m, t0 ∈ Distmon, hence φ(t∗m) ≥ φ(t0). Combining the two possibilities for t∗m, we have in Case 1,\nG(ǫ̂(t0)) ≤ max(32γ̃( 40 55 ), 3γ̃(1011 ) 2 )G(ν(t∗m)).\nSince t̂ minimizes G(ǫ̂(t)) on a set that includes t0, we have, using Lemma 5.4\nG(ǫ(t̂)) ≤ 6.5G(ǫ̂(t̂)) ≤ 6.5G(ǫ̂(t0)).\nTherefore, in Case 1,\nG(ǫ(t̂)) ≤ 6.5max(32γ̃(40 55\n), 3γ̃(1011 )\n2 )G(ν(t∗m)). (9)\nIn Case 2, the binary search halted without satisfying Condition 1 nor Condition 2 and with T = ∅. Let t0 be as defined in this case in SelectScale (if it exists), and let t1 be the smallest value in TL (if it exists). At least one of these values must exist. If both values exist, we have t0 ≤ t1 and (t0, t1) ∩Distmon = ∅.\nIf t0 exists, it is the last value for which the search went right. We thus have ǫ̂(t0) < φ(t0). If t∗m ≤ t0, from condition 1 on t0 and Eq. (5) with γ̃(1) ≤ 4,\nG(ν(t∗m)) ≥ 2 3 φ(t∗m) ≥ 2 3 φ(t0) ≥ 1 6 G(ǫ̂(t0)).\nHere we used the monotonicity of φ on t∗m, t0 ∈ Distmon, and Eq. (5) applied to condition 1 for t0.\nIf t1 exists, the search went left on t1, thus ǫ̂(t1) > 1110φ(t1). By Cor. 5.2, it follows that ǫ̂(t1) ≤ 43ǫ(t1). Therefore, if t∗m ≥ t1,\nG(ν(t∗m)) ≥ ν(t∗m) ≥ ν(t1) ≥ 1\n4 ǫ(t1) ≥\n3\n16 ǫ̂(t1) ≥\n3\n16γ(11/10) G(ǫ̂(t1)).\nHere we used ǫ(t1) ≤ 4ν(t1) by Theorem 4.3 and Eq. (5). Combining the two cases for t∗m, we get that if t0 exists and t∗m ≤ t0, or t1 exists and t∗m ≥ t1,\nG(ν(t∗m)) ≥ min( 1\n6 ,\n3\n16γ(11/10) ) min t∈TE G(ǫ̂(t)).\nwhere we define TE = {t ∈ {t0, t1} | t exists}. We now show that this covers all possible values for t∗m: If both t0, t1 exist, then since (t0, t1) ∩ Distmon = ∅, it is impossible to have t∗m ∈ (t0, t1). If only t0 exists, then the search never went left, which means t0 = max(Distmon), thus t∗m ≤ t0. If only t1 exists, then the search never went right, which means t1 = min(Distmon), thus t∗m ≥ t1.\nSince t̂ minimizes G(ǫ̂(t)) on a set that has TE as a subset, we have, using Lemma 5.4 G(ǫ(t̂)) ≤ 6.5G(ǫ̂(t̂)) ≤ 6.5mint∈TE G(ǫ̂(t)). Therefore in Case 2,\nG(ν(t∗m)) ≥ 1\n6.5 min(\n1 6 ,\n3\n16γ(11/10) )G(ǫ(t̂)). (10)\nFrom Eq. (9) and Eq. (10) we get that in both cases\nG(ν(t∗m)) ≥ 1\n6.5 min(\n1 6 ,\n3\n16γ(11/10) ,\n2\n3γ̃(10/11) ,\n1\n32γ̃(4055 ) )G(ǫ(t̂)) ≥ G(ǫ(t̂))/865.\nCombining this with Eq. (8) we get the statement of the theorem.\n6 Bounding the label complexity of MARMANN\nWe are now almost ready to prove Theorem 3.1. Our last missing piece is quantifying the effect of side information on the generalization of sample compression schemes in Section 6.1. We then prove Theorem 3.1 in Section 6.2."
    }, {
      "heading" : "6.1 Sample compression with side information",
      "text" : "It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information. This side information is used to represent the labels of the sample points in the compression set. A similar idea appears in Floyd and Warmuth [1995] for hypotheses with short description length. Here we provide a generalization that is useful for the analysis of MARMANN.\nLet Σ be a finite alphabet, and define a mapping RecN : (X ×Y)N ×ΣN → YX .4 This is a reconstruction function mapping a labeled sequence of size N with side information T ∈ ΣN to a classifier. For I ⊆ [|S|], denote by S[I] the subsequence of S indexed by I . For a labeled sample S, define the set of possible hypotheses reconstructed from a compression of S of size N with side information inΣ: HN (S) := { h : X → Y | h = RecN (S[I], T ), I ∈ [m]N , T ∈ ΣN }\n. The following result closely follows the sample compression arguments in Graepel et al. [2005, Theorem 2], and Gottlieb et al. [2016b, Theorem 6], but incorporates side information.\nTheorem 6.1. Let m be an integer and δ ∈ (0, 1). Let S ∼ Dm. With probability at least 1 − δ, if there exist N < m and h ∈ HN (S) with ǫ := err(h, S) ≤ 12 , then err(h,D) ≤ GB(ǫ,N, δ,m, |Σ|). Proof. We recall a result of Dasgupta and Hsu [2008, Lemma 1]: if p̂ ∼ Bin(n, p)/n and δ > 0, then the following holds with probability at least 1− δ:\np ≤ p̂+ 2 3n log 1 δ +\n√\n9p̂(1− p̂) 2n log 1 δ . (11)\nNow fix N < m, and suppose that h ∈ HN (S) has ǫ̂ ≤ 12 . Let I ∈ [m]N , T ∈ ΣN such that h = RecN (S[I], T ). We have err(h, S[[m] \\ I]) ≤ ǫ̂mm−N = θǫ̂. Substituting into (11) p := err(h,D), n := m −N and p̂ := err(h, S[[m] \\ I]) ≤ θǫ̂, yields that for a fixed S[I] and a random S[[m] \\ I] ∼ Dm−N , with probability at least 1− δ,\nerr(h,D) ≤ θǫ̂+ 2 3(m−N) log 1 δ +\n√\n9θǫ̂ 2(m−N) log 1 δ . (12)\nTo make (12) hold simultaneously for all (I, T ) ∈ [m]N × ΣN , divide δ by (m|Σ|)N . To make the claim hold for all N ∈ [m], stratify (as in Graepel et al. [2005, Lemma 1]) over the (fewer than) m possible choices of N , which amounts to dividing δ by an additional factor of m.\n4If X is infinite, replace YX with the set of measurable functions from X to Y .\nFor MARMANN, we use the following sample compression scheme with Σ = Y . Given a subsequence S′ := S[I] := (x′1, . . . , x ′ N ) and T = (t1, . . . , tN ) ∈ YN , the reconstruction function RecN (S[I], T ) generates the nearest-neighbor rule induced by the labeled sample ψ(S′, T ) := ((x′i, ti))i∈[N ]. Formally, RecN (S\n′, T ) = hnnψ(S′,T ). Note the slight abuse of notation: formally, the yi in Sa(t) should be encoded as side information T , but for clarity, we have opted to “relabel” the examples {x1, . . . , xN} as dictated by the majority in each region. The following corollary is immediate from Theorem 6.1 and the construction above.\nTheorem 6.2. Let m ≥ |Y| be an integer, δ ∈ (0, 14 ). Let Sin ∼ Dm. With probability at least 1 − δ, if there exist N < m and S ⊆ (X × Y)N such that U(S) ⊆ Uin and ǫ := err(hnnS , Sin) ≤ 12 , then err(hnnS ,D) ≤ GB(ǫ,N, δ,m, |Y|) ≤ 2GB(ǫ,N, 2δ,m, 1).\nIf the compression set includes only the original labels, the compression analysis of Gottlieb et al. [2016b] gives the bound GB(ǫ,N, δ,m, 1). Thus the effect of allowing the labels to change is only logarithmic in |Y|, and does not appreciably degrade the prediction error."
    }, {
      "heading" : "6.2 Proof of Theorem 3.1",
      "text" : "The proof of the main theorem, Theorem 3.1, which gives the guarantee for MARMANN, is almost immediate from Theorem 6.2, Theorem 4.3, Theorem 5.6 and Theorem 5.5.\nof Theorem 3.1. We have |Distmon| ≤ ( m 2 )\n. By a union bound, the events E(t) and V (t) of Theorem 4.3 and Cor. 5.2 hold for all t ∈ Ttest ⊆ Distmon with a probability of at least 1− δ/2. Under these events, we have by Theorem 5.6 that if Gmin(m, δ) ≤ 1/3,\nGB(ǫ(t̂),N (t̂), δ,m, 1) ≤ O (\nmin t\nGB(ν(t),N (t), δ,m, 1) ) .\nBy Theorem 6.2, with a probability at least 1− δ/2, if ǫ(t̂) ≤ 12 then\nerr(ĥ,D) ≤ 2GB(ǫ(t̂),N (t̂), δ,m, 1). The statement of the theorem follows. Note that the statement trivially holds forGmin(m, δ) ≥\n1/3 and for ǫ(t̂) ≥ 12 , thus these conditions can be removed. To bound the label complexity, note that the total number of labels used by MARMANN is at most the number of labels used by SelectScale plus the number of labels used byGenerateNNSet when the final compression set is generated.\nBy Theorem 5.5, since Q = O(log(m/δ)), the number of labels used by SelectScale is at most\nO\n( |Ttest| log2(m/δ)\nG(ǫ(t̂)) log\n(\n1\nG(ǫ(t̂)\n))\n.\nIn addition, G(ǫ(t̂)) ≥ GB(ǫ(t̂),N (t̂), δ,m, 1) = Ĝ.\nThe number of tested scales in SelectScale is bounded by\n|Ttest| ≤ ⌊log2(|Distmon|) + 1⌋ ≤ 2 log2(m)\nTherefore the number of labels used by SelectScale is\nO\n( log3(m/δ)\nĜ log\n(\n1\nĜ\n))\n.\nThe number of labels used by GenerateNNSet is at most QN (t̂), where Q ≤ O(log(m/δ), and from the definition of Ĝ, N (t̂) ≤ O(mĜ/ log(m)). Summing up the number of labels used by SelectScale and the number used by GenerateNNSet, this gives the bound in the statement of the theorem."
    }, {
      "heading" : "7 Passive learning lower bounds",
      "text" : "Theorem 3.2 lower bounds the performance of a passive learner who observes a limited number ℓ of random labels from Sin. The number ℓ is chosen so that it is of the same order as the number of labels MARMANN observes for the case analyzed in Section 3. We deduce Theorem 3.2 from a more general result pertaining to the sample complexity of passive learning. The general result is given as Theorem 7.1 in Section 7.1. The proof of Theorem 3.2 is provided in Section 7.2.\nWe note that while the lower bounds below assume that the passive learner observes only the random labeled sample of size ℓ, in fact their proofs hold also if the algorithm has access to the full unlabeled sample of size m of which Sℓ is sampled. This is because the lower bound is based on requiring the learner to distinguish between distributions that all have the same marginal. Under this scenario, access to unlabeled examples does not provide any additional information to the algorithm."
    }, {
      "heading" : "7.1 A general lower bound",
      "text" : "In this section we show a general sample complexity lower bound for passive learning, which may be of independent interest. We are aware of two existing lower bounds for agnostic PAC with bounded noise: Devroye et al. [1996, Theorem 14.5] and Audibert [2009, Theorem 8.8]. Both place restrictions on the relationship between the sample size, VC-dimension, and noise level, which render them inapplicable as stated to some parameter regimes, including the one needed for proving Theorem 3.2.\nLet H be a hypothesis class with VC-dimension d and suppose that L is a passive learner5 mapping labeled samples Sℓ = (Xi, Yi)i∈[ℓ] to hypotheses ĥℓ ∈ H. For any distribution D over X × {−1, 1}, define the excess risk of ĥℓ by\n∆(ĥℓ,D) := err(ĥℓ,D)− inf h∈H err(h,D).\nLet D(η) be the collection of all η-noise bounded distributions D over X × {−1, 1}, which satisfy infh∈H err(h,D) ≤ η. We say that Z ∈ {−1, 1} has Rademacher distribution with parameter b ∈ [−1, 1], denoted Z ∼ Rb, if\nP[Z = 1] = 1− P[Z = −1] = 1 2 + b 2 .\n5 We allow L access to an independent internal source of randomness.\nAll distributions on {−1, 1} are of this form. For k ∈ N and b ∈ [0, 1], define the function\nbayes(k, b) = 12\n( 1− 12 ∥ ∥ ∥ Rkb −Rk−b ∥ ∥ ∥\n1\n)\n,\nwhere Rk±b is the corresponding product distribution on {−1, 1}k and 12 ‖·‖1 is the total variation norm. This expression previously appeared in Berend and Kontorovich [2015, Equation (25)] in the context of information-theoretic lower bounds; the current terminology was motivated in Kontorovich and Pinelis [2016], where various precise estimates on bayes(·) were provided. In particular, the function bay̌es(κ, b) was defined as follows: for each fixed b ∈ [0, 1], bay̌es(·, b) is the largest convex minorant on [0,∞) of the function bayes(·, b) on {0, 1, . . .}. It was shown in Kontorovich and Pinelis [2016, Proposition 2.8] that bay̌es(·, b) is the linear interpolation of bayes(·, b) at the points 0, 1, 3, 5, . . . .\nTheorem 7.1. Let 0 < η < 12 , ℓ ≥ 1, and H be a hypothesis class with VC-dimension d > 1. Then, for all 0 < b, p < 1 satisfying\np\n(\n1 2 − b 2\n)\n≤ η, (13)\nwe have\ninf ĥℓ sup D∈D(η) E Dℓ\n[ ∆(ĥℓ,D) ] ≥ bp bay̌es(ℓp/(d− 1), b). (14)\nFurthermore, for 0 ≤ u < 1,\ninf ĥℓ sup D∈D(η) P\n[ ∆(ĥℓ,Dσ,b,p) > bpu ] > bay̌es(ℓp/(d− 1), b)− u. (15)\nProof. This proof uses ideas from Devroye et al. [1996, Theorem 14.5], Anthony and Bartlett [1999, Theorem 5.2] and Kontorovich and Pinelis [2016, Theorem 2.1].\nWe will construct adversarial distributions supported on a shattered subset of size d, and hence there is no loss of generality in taking X = [d] and H = {−1, 1}X . A random distribution Dσ,b,p over X ×{−1, 1}, parametrized by a random σ ∼ Unif({−1, 1}d−1) and scalars b, p ∈ (0, 1) to be specified later, is defined as follows. The point x = d ∈ X gets a marginal weight of 1 − p where p is a parameter to be set; the remaining d − 1 points each get a marginal weight of p/(d− 1):\nP X∼Dσ,b,p [X = d] = 1− p, P X∼Dσ,b,p\n[X < d] = p\nd− 1 . (16)\nThe distribution of Y conditional on X is given by P(X,Y )∼Dσ,b,p [Y = 1 |X = d] = 1 and\nP (X,Y )∼Dσ,b,p [Y = ±1 |X = j < d] = 1 2 ± bσj 2 . (17)\nSuppose that (Xi, Yi)i∈[ℓ] is a sample drawn fromDℓσ,b,p. The assumption that Dσ,b,p ∈ D(η) implies that b and p must satisfy the constraint (13).\nA standard argument (e.g., Anthony and Bartlett [1999] p. 63 display (5.5)) shows that, for any hypothesis ĥℓ,\n∆(ĥℓ,Dσ,b,p) = err(ĥℓ,Dσ,b,p)− inf h∈H err(h,Dσ,b,p)\n= P X∼Dσ,b,p [X = d, ĥℓ(X) 6= 1] + b P X∼Dσ,b,p [X < d, ĥℓ(X) 6= σ(X)]\n≥ b P X∼Dσ,b,p [X < d, ĥℓ(X) 6= σ(X)]\n= bp P X∼Dσ,b,p\n[ĥℓ(X) 6= σ(X)|X < d]. (18)\nIt follows from Kontorovich and Pinelis [2016, Theorems 2.1, 2.5] that\nE σ P X∼Dσ,b,p [ĥℓ(X) 6= σ(X)|X < d] ≥ E N∼Bin(ℓ,p/(d−1)) [bayes(N, b)]\n≥ E N∼Bin(ℓ,p/(d−1)) [bay̌es(N, b)] ≥ bay̌es(E[N ], b) = bay̌es(ℓp/(d− 1), b),\nwhere the second inequality holds because bay̌es is, by definition, a convex minorant of bayes, and the third follows from Jensen’s inequality. Combined with (18), this proves (14).\nTo show (15), define the random variable\nZ = Z(σ,L) = P X∼Dσ,b,p [ĥℓ(X) 6= σ(X)|X < d].\nSince Z ∈ [0, 1], Markov’s inequality implies\nP[Z > u] ≥ E[Z]− u 1− u > E[Z]− u, 0 ≤ u < 1.\nNow (18) implies that ∆(ĥℓ,Dσ,b,p) ≥ bpZ and hence, for 0 ≤ u < 1,\ninf ĥℓ sup D∈D(η) P\n[ ∆(ĥℓ,Dσ,b,p) > bpu ]\n= inf ĥℓ sup D∈D(η) P[Z > u]\n> inf ĥℓ sup D∈D(η)\nE[Z]− u\n≥ 1 bp inf ĥℓ sup D∈D(η) E[∆(ĥℓ,Dσ,b,p)]− u ≥ bay̌es(ℓp/(d− 1), b)− u."
    }, {
      "heading" : "7.2 Proof of Theorem 3.2",
      "text" : "We break down the proof into several steps.\n(i) Defining a family of adversarial distributions. Let T be a t̄-net of X of size Θ(√m) and η = Θ(1/ √ m). For any passive learning algorithm mapping i.i.d. samples of size ℓ = Θ̃( √ m) to hypotheses ĥℓ : X → {−1, 1}, we construct a random adversarial distribution Dσ,b,p with Bayes error η. We accomplish this via the construction described in the proof of Theorem 7.1, with |T | = d = Θ(√m). The marginal distribution over T = {x1, . . . , xd} puts a mass of 1 − p on xd ∈ T and spreads the remaining mass uniformly over the other points, as in (16). The “heavy” point has a deterministic label and the remaining “light” points have noisy labels drawn from a random distribution with symmetric noise level b, as in (17). We proceed to choose b and p; namely,\np = d− 1 2ℓ √ η = Θ̃(m−1/4), b = 1− 2η p = 1− Θ̃(m−1/4),\nwhich makes the constraint in (13) hold with equality; this means that the Bayes error is exactly η and in particular, establishes (i).\n(ii.a) Lower-bounding the passive learner’s error. Our choice of p implies that ℓp/(d− 1) = √ η/2 =: κ < 1, for this range of κ, Kontorovich and Pinelis [2016, Proposition 2.8] implies that bay̌es(κ, b) = 12 (1 − κb) = Θ(1). Choosing u = 14 (1 − κb) = Θ(1) in (15), Theorem 7.1 implies that\ninf ĥℓ sup D∈D(η)\nP[∆(ĥℓ,D) > Ω̃(m−1/4)] > Ω(1).\nIn more formal terms, there exist constants c0, c1 > 0 such that\ninf ĥℓ sup D∈D(η)\nP[∆(ĥℓ,D) > c0p] > c1. (19)\n(ii.b) Upper-bounding ν(t̄). To establish (ii.b), it suffices to show that for (Xi, Yi)i∈[ℓ] ∼ Dℓσ,b,p, we will have ν(t̄) = O(m−1/2) with sufficiently high probability. Indeed, the latter condition implies the requisite upper bound on mint>0:N (t)<m GB(ν(t),N (t), δ,m, 1), while (i) implies the lower bound, since the latter quantity cannot be asymptotically smaller than the Bayes error.\nRecall that the t̄-net points {x1, . . . , xd−1} are the “light” ones (i.e., each has weight p/(d− 1)) and define the random sets Jj ⊂ [ℓ] by\nJj = {i ∈ [ℓ] : Xi = xj} , j ∈ [d− 1].\nIn words, Jj consists of the indices i of the sample points for which Xi falls on the net point xj . For y ∈ {−1, 1}, put τyj = ∑\ni∈Jj I[Yi = y] and define the minority count ξj at the net point xj by\nξj = min y∈{−1,1}\nτyj = 1 2 (|τ+j + τ−j | − |τ+j − τ−j |).\nObserve that by virture of being a t̄-net, T is t̄-separated and hence the only contribution to ν(t̄) is from the minority counts (to which the “heavy” point xd does not contribute due to\nits deterministic label):\nν(t̄) = 1\nℓ\nd−1 ∑\nj=1\nξj .\nNow\nE |τ+j + τ−j | = E |Jj | = ℓp d− 1 = Θ(m −1/4)\nand\nE |τ+j − τ−j | = E σj E[|τ+j − τ−j | ∣ ∣σj ]\n≥ E σj\n∣ ∣E[τ + j − τ−j ∣ ∣σj ] ∣ ∣ .\nComputing\nE[τ + j |σj = +1] = (\n1 2 + b 2 ) ℓp d− 1 , E[τ − j |σj = +1] = ( 1 2 − b 2 ) ℓp d− 1 ,\nwith an analogous calculation when conditioning on σj = −1, we get\nE |τ+j − τ−j | ≥ bℓp\nd− 1 and hence\nE[ξj ] ≤ 1\n2\n(\nℓp d− 1 − b ℓp d− 1\n)\n= (1− b) ℓp 2(d− 1) = 2η p · ℓp 2(d− 1) = ηℓ d− 1 .\nIt follows that\nE[ν(t̄)] = 1\nℓ\nd−1 ∑\nj=1\nE[ξj ]\n≤ d− 1 ℓ · ηℓ d− 1 = η = Θ(m −1/2).\nTo give tail bounds on ν(t̄), we use Markov’s inequality: for all c2 > 0,\nP[ν(t̄) > c2 E[ν(t̄)] ≤ 1\nc2 .\nChoosing c2 sufficiently large that 1− 1/c2 > c1 (the latter from (19)) implies the existence of constants c0, c2, c3 > 0 such that\ninf ĥℓ sup D∈D(η) P\n[( ∆(ĥℓ,D) > c0p ) ∧ (ν(t̄) ≤ c2η) ] > c3.\nSince p = Θ̃(m−1/4) and η = Θ(m−1/2), this establishes (ii) and concludes the proof of Theorem 3.2."
    }, {
      "heading" : "8 Active learning lower bound",
      "text" : "We now prove the active learning lower bound stated in Theorem 3.3. To prove the theorem, we first prove a result which is similar to the classical No-Free-Lunch theorem, except it holds for active learning algorithms. The proof follows closely the proof of the classical No-Free-Lunch theorem given in Shalev-Shwartz and Ben-David [2014, Theorem 5.1], with suitable modifications.\nTheorem 8.1. Let β ∈ [0, 12 ), and m be an integer. Let A any active learning algorithm over a finite domain X which gets as input a random labeled sample S ∼ Dm (with hidden labels) and outputs ĥ. If A queries fewer than X/2 labels from S, then there exists a distribution D over X × {0, 1} such that\n• Its marginal on X is uniform, and for each x ∈ X , P[Y = 1 | X = x] ∈ {β, 1− β}.\n• E[err(ĥ,D)] ≥ 14 .\nProof. Let F = {f1, . . . , fT } be the set of possible functions fi : X → {0, 1}. Let Di to be a distribution with a uniform marginal over X , and P(X,Y )∼Di [Y = 1 | X = x] = fi(x)(1 − β) + (1 − fi(x))β. Consider the following random process: First, draw an unlabeled sample X = (x1, . . . , xm) i.i.d. from DmX . Then, draw B = (b1, . . . , bm) independently from a Bernoulli distribution with P[bi = 1] = β. For i ∈ [T ], let Si(X,B) = ((x1, y1), . . . , (xm, ym)) such that xi are set by X , and yi = fi(x) if bi = 0 and 1 − fi(x) otherwise. Clearly, Si(X,B) is distributed according to Dmi . Let hi(S) be the output of A when the labeled sample is S. Denote by ĥi the (random) output of A when the sample is drawn from Di. Clearly\nE[err(ĥi,Di)] = E X,B [err(hi(S(X,B)),Di)].\nTherefore (as in (5.4) in Shalev-Shwartz and Ben-David [2014]), for some j,X,B it holds that\nE[err(ĥj ,Dj)] ≥ 1\nT\nT ∑\ni=1\nE[err(ĥi,Di)] ≥ 1\nT\nT ∑\ni=1\nerr(hi(S(X,B)),Di). (20)\nFix X,B, j as above, and denote for brevity hi := hi(S(X,B)). Let Vi be the set of examples x ∈ X for which that A does not observe their label if the labeled sample is Si(X,B) (this includes both examples that are not in the sample at all as well as examples that are in the sample but their label is not requested by A). We have |Vi| > |X |/2 by assumption. Then (as in Eq. (5.6) therein)\n1\nT\nT ∑\ni=1\nerr(hi,Di) ≥ 1\nT\nT ∑\ni=1\n1\n2|Vi| ∑ x∈Vi I[hi(x) 6= fi(x)]. (21)\nSince A is active, it selects which examples to request, which can depend on the labels observed by A so far. Therefore, Vi can be different for different i. However, an argument similar to that of the No-Free-Lunch theorem for the passive case still goes through, as follows.\nLet i, i′ such that fi(x) = fi′(x) for all x /∈ Vi, and fi(x) = 1 − fi′(x) for all x ∈ Vi. Since X,B are fixed, A observes the same labels for all x /∈ Vi for both Si ′\n(X,B) and Si(X,B), thus all its decisions and requests are identical for both samples, and so Vi = Vi′ , and hi = hi′ . Therefore, it is possible to partition T into T/2 pairs of indices i, i′ such that for each such pair,\n1\n2|Vi| ∑ x∈Vi I[hi(x) 6= fi(x)] +\n1\n2|Vi′ | ∑ x∈Vi′ I[hi′(x) 6= fi′(x)]\n= 1 2|Vi| ∑ x∈Vi I[hi(x) 6= fi(x)] + I[hi(x) 6= 1− fi(x)] = 1\n2 .\nTherefore, 1T ∑T i=1 err(hi,Di) ≥ 14 . Therefore, from Eq. (21), 1T ∑T i=1 err(hi,Di) ≥ 14 . Combining this with Eq. (20), it follows that E[err(ĥj ,Dj)] ≥ 14 .\nWe will also make use of the following simple lemma.\nLemma 8.2. Let β ∈ [0, 1]. Let D be a distribution over X×{0, 1} such that for (X,Y ) ∼ D, for any x in the support of D, P[Y = 1 | X = x] ∈ {β, 1− β}. Let N be the size of the support of D. Let S ∼ Dm. Denote by nx the number of sample pairs (x′, y′) in S where x′ = x, and let n+x be the number of sample pairs (x\n′, y′) where x′ = x and y′ = 1. Let p̂+x = n + x /nx (or zero if nx = 0). Then\n2β(1− β)(m−N) ≤ ∑\nx∈X E[2nxp̂\n+ x (1− p̂+x )] ≤ 2β(1− β)m.\nProof. We have\nE[2nxp̂ + x (1 − p̂+x )] =\n∞ ∑\ni=1\nP[nx = i] · i · E[2p̂+x (1 − p̂+x ) | nx = i].\nNote that E[2p̂+x (1 − p̂+x ) | nx = 1] = 0. For i > 1, let y1, . . . , yi be the labels of the examples that are equal to x in S, then\n∑\nj,k∈[i] I[yk 6= yj] = 2n+x (i − n+x ) = i2 · 2p̂+x (1− p̂+x ).\nTherefore, letting (X1, Y1), (X2, Y2) ∼ D2,\nE S∼Dm\n[2p̂+x (1− p̂+x ) | nx = i] = 1\ni2 E S∼Dm [\n∑\nj,k∈[nx] I[yk 6= yj ] | nx = i]\n= i2 − i i2 P[Y1 6= Y2 | X1 = X2 = x] = 2(1− 1 i )β(1 − β),\nThus\nE[2nxp̂ + x (1 − p̂+x )] = 2β(1− β)\n∞ ∑\ni=2\n(i− 1)P[nx = i]\n= 2β(1− β)(E[nx] + P[nx = 0]− 1).\nTo complete the proof, sum over all x in the support of D, and note that ∑x E[nx] = m, and ∑\nx(P[nx = 0]− 1) ∈ [−N, 0].\nWe now prove our lower bound, stated in Theorem 3.3, on the number of queries required by any active learning with competitive guarantees similar to ours.\nof Theorem 3.3. Let N = ⌊ mα−log(m δ )\nlog(m)\n⌋\n. Let β = 8α ≤ 12 . Consider a marginal distribution DX over X which is uniform over N points 1, . . . , N ∈ R. Consider the following family of distributions: D such that its marginal over X is DX , and for each x ∈ X , P[Y = 1 | X = x] ∈ {β, 1 − β}. Thus the Bayes optimal error for each of these distributions is β.\nLet S ∼ Dm. If one example in S is changed, ν(12 ) changes by at most 1/m. Hence, by McDiarmid’s inequality [McDiarmid, 1989], with probability at least 1 − 128 , |ν(12 ) − E[ν(12 )]| ≤ √ log(28) 2m . Denote the event that this holds EM . Since β = 8α ≥ log(m)+log(28)√ 2m\n, it follows that under EM ,\n|ν(1 2 )− E[ν( 1 2 )]| ≤ β/8. (22)\nWe now bound E[ν(12 )]. Using the notation p̂ + x , nx, n + x as in Lemma 8.2, we have\nν( 1\n2 ) =\n1\nm\n∑ x∈X min(n+x , nx − n+x ) =\n1\nm\n∑ x∈X nx min(p + x , 1− p+x )\nAlso, for all p ∈ [0, 1], min(p, 1− p) ≤ 2p(1− p) ≤ 2min(p, 1− p). Therefore\n1\n2m\n∑ x∈X 2nxp + x (1− p+x ) ≤ ν(\n1 2 ) ≤ 1 m ∑\nx∈X 2nxp\n+ x (1− p+x ).\nBy Lemma 8.2, it follows that\nm−N m β(1 − β) ≤ E[ν(1 2 )] ≤ 2β(1− β).\nSince N ≤ m/2 and β ∈ [0, 12 ], E[ν(12 )] ∈ (β/4, 2β). Combining this with Eq. (22), we get that under EM , α = β/8 ≤ ν(12 ) ≤ 178 β = 17α.\nNow, we bound Gmin from above and below assuming EM holds. Denote\nG(t) := GB(ν(t),N (t), δ,m, 1).\nTo establish a lower bound onGmin(m, δ), note thatGmin(m, δ) = mint>0 G(t) ≥ mint>0 ν(t). For t ∈ (0, 12 ), ν(t) = ν(12 ) (since the distances between any two distinct points in S is at\nleast 1). In addition, since ν is monotonically increasing, we have ν(t) ≥ ν(12 ) for t ≥ 12 . Hence mint>0 ν(t) ≥ ν(12 ) ≥ β/8 = α.\nTo show an upper bound on Gmin(m, δ), we upper bound G(12 ). Note that N (12 ) ≤ N . Recall the definition of φ(t) in Eq. (4). We have\nφ( 1\n2 ) =\n(N + 1) log(m) + log(1δ )\nm ≤ α.\nThen, since ν(12 ) ≤ 17α,\nG( 1 2 ) ≤ m m−N (ν( 1 2 ) + 2 3 α+ 3√ 2\n√\nν( 1\n2 )α) ≤ 30α.\nIn the last inequality we used the fact that mm−N ≤ 10/9. So if EM holds, Gmin(m, δ) ≤ G(12 ) ≤ 30α.\nFrom the assumption onA, with probability at least 1−δ, we have err(ĥ,D) ≤ CGmin(m, δ) ≤ 30Cα ≤ 1/8 (since α ≤ 1240C ). Let EL(D) denote the event that A queries fewer than N/2 labels, where the probability is over the randomness of S and A. Let h′ be the output of an algorithm that behaves like A in cases where EL(D) holds, and queries at most N/2 otherwise. By Theorem 8.1, there exists some D in the family of distributions such that E[err(h′,D)] ≥ 14 . By Markov’s inequality, P[err(h′,D) ≥ 18 ] ≥ 1/7. Also, P[h′ = ĥ] ≥ P[EL(D)]. Therefore\nP[err(ĥ,D) ≥ 1 8 ] ≥ P[err(h′,D) ≥ 1 8 ] + P[EL(D)]− 1 = P[EL(D)] − 6/7.\nTherefore P[EL(D)] − 6/7 ≤ P[err(ĥ,D) ≥ 18 ] ≤ δ. Since by assumption δ ≤ 1/14, it follows that P[EL(D)] ≤ 6/7 + δ ≤ 13/14. It follows that with a probability of at least 1/14, the negation of EL(D) holds. Since also EM holds with probability at least 1− 128 , it follows that with a probability of at least 128 , both EM and the negation of EL(D) hold. Now, as shown above, EM implies the bounds on Gmin(m, δ) (item 1 in the theorem statement). In addition, the negation of EL(D) implies that A queries at least N/2 = 12 ⌊ mα−log(m δ ) log(m) ⌋ labels (item 2 in the theorem statement). This completes the proof."
    }, {
      "heading" : "9 Discussion",
      "text" : "We have presented an efficient fully empirical proximity-based non parametric active learner. Our approach provides competitive error guarantees for general distributions, in a general metric space, while keeping label complexity significantly lower than any passive learner with the same guarantees. MARMANN yields fully empirical error estimates, easily computable from finite samples. This is in contrast with classic techniques, that present bounds and rates that depend on unknown distribution-dependent quantities.\nAn interesting question is whether the guarantees can be related to the Bayes error of the distribution. Our error guarantees give a constant factor over the error guarantees of Gottlieb et al. [2016b]. A variant of this approach [Gottlieb et al., 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for\nthe algorithm of Gottlieb et al. [2016b]. Since in our analysis MARMANN obtains a constant factor over the error of the passive learner, Bayes-consistency cannot be inferred from our present techniques; we leave this problem open for future research."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Sivan Sabato was partially supported by the Israel Science Foundation (grant No. 555/15). Aryeh Kontorovich was partially supported by the Israel Science Foundation (grants No. 1141/12 and 755/15) and a Yahoo Faculty award. We thank Lee-Ad Gottlieb and Dana Ron for the helpful discussions."
    } ],
    "references" : [ {
      "title" : "Neural Network Learning: Theoretical Foundations",
      "author" : [ "M. Anthony", "P.L. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "Anthony and Bartlett.,? \\Q1999\\E",
      "shortCiteRegEx" : "Anthony and Bartlett.",
      "year" : 1999
    }, {
      "title" : "Fast learning rates in statistical inference through aggregation",
      "author" : [ "J.-Y. Audibert" ],
      "venue" : "Ann. Statist., 37(4):1591–1646,",
      "citeRegEx" : "Audibert.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert.",
      "year" : 2009
    }, {
      "title" : "The power of localization for efficiently learning linear separators with noise",
      "author" : [ "P. Awasthi", "M. Balcan", "P.M. Long" ],
      "venue" : "In Symposium on Theory of Computing,",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2014
    }, {
      "title" : "Margin based active learning",
      "author" : [ "M. Balcan", "A.Z. Broder", "T. Zhang" ],
      "venue" : "In Proceedings of the 20th Annual Conference on Learning Theory, COLT",
      "citeRegEx" : "Balcan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2007
    }, {
      "title" : "The true sample complexity of active learning",
      "author" : [ "M. Balcan", "S. Hanneke", "J.W. Vaughan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2010
    }, {
      "title" : "Agnostic active learning",
      "author" : [ "M.-F. Balcan", "A. Beygelzimer", "J. Langford" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2009
    }, {
      "title" : "A finite sample analysis of the naive bayes classifier",
      "author" : [ "D. Berend", "A. Kontorovich" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Berend and Kontorovich.,? \\Q2015\\E",
      "shortCiteRegEx" : "Berend and Kontorovich.",
      "year" : 2015
    }, {
      "title" : "Active nearest neighbors in changing environments",
      "author" : [ "C. Berlind", "R. Urner" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Berlind and Urner.,? \\Q2015\\E",
      "shortCiteRegEx" : "Berlind and Urner.",
      "year" : 2015
    }, {
      "title" : "In defense of nearest-neighbor based image classification",
      "author" : [ "O. Boiman", "E. Shechtman", "M. Irani" ],
      "venue" : "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Boiman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Boiman et al\\.",
      "year" : 2008
    }, {
      "title" : "Using the doubling dimension to analyze the generalization of learning algorithms",
      "author" : [ "N.H. Bshouty", "Y. Li", "P.M. Long" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Bshouty et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bshouty et al\\.",
      "year" : 2009
    }, {
      "title" : "Minimax bounds for active learning",
      "author" : [ "R.M. Castro", "R.D. Nowak" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Castro and Nowak.,? \\Q2008\\E",
      "shortCiteRegEx" : "Castro and Nowak.",
      "year" : 2008
    }, {
      "title" : "Faster rates in regression via active learning",
      "author" : [ "R.M. Castro", "R. Willett", "R.D. Nowak" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Castro et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Castro et al\\.",
      "year" : 2005
    }, {
      "title" : "Active learning on trees and graphs",
      "author" : [ "N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella" ],
      "venue" : "In Proceedings of rhe 23rd Conference on Learning Theory, COLT",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Rates of convergence for nearest neighbor classification",
      "author" : [ "K. Chaudhuri", "S. Dasgupta" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chaudhuri and Dasgupta.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chaudhuri and Dasgupta.",
      "year" : 2014
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "T.M. Cover", "P.E. Hart" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Cover and Hart.,? \\Q1967\\E",
      "shortCiteRegEx" : "Cover and Hart.",
      "year" : 1967
    }, {
      "title" : "S2: an efficient graph based active learning algorithm with application to nonparametric classification",
      "author" : [ "G. Dasarathy", "R.D. Nowak", "X. Zhu" ],
      "venue" : "In Proceedings of the 28th Annual Conference on Learning Theory,",
      "citeRegEx" : "Dasarathy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dasarathy et al\\.",
      "year" : 2015
    }, {
      "title" : "Analysis of a greedy active learning strategy",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dasgupta.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 2004
    }, {
      "title" : "Consistency of nearest neighbor classification under selective sampling",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Learning Theory, COLT",
      "citeRegEx" : "Dasgupta.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 2012
    }, {
      "title" : "Hierarchical sampling for active learning",
      "author" : [ "S. Dasgupta", "D. Hsu" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Dasgupta and Hsu.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dasgupta and Hsu.",
      "year" : 2008
    }, {
      "title" : "Nonparametric density estimation: the L1 view. Wiley Series in Probability and Mathematical Statistics: Tracts on Probability and Statistics",
      "author" : [ "L. Devroye", "L. Györfi" ],
      "venue" : null,
      "citeRegEx" : "Devroye and Györfi.,? \\Q1985\\E",
      "shortCiteRegEx" : "Devroye and Györfi.",
      "year" : 1985
    }, {
      "title" : "A probabilistic theory of pattern recognition, volume 31 of Applications of Mathematics (New York)",
      "author" : [ "L. Devroye", "L. Györfi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Devroye et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Devroye et al\\.",
      "year" : 1996
    }, {
      "title" : "A (1 + ǫ)-embedding of low highway dimension graphs into bounded treewidth graphs",
      "author" : [ "A.E. Feldmann", "W.S. Fung", "J. Könemann", "I. Post" ],
      "venue" : "CoRR, abs/1502.04588,",
      "citeRegEx" : "Feldmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Report Number 4, Project Number 21-49-004",
      "author" : [ "E. Fix", "J.L.J. Hodges" ],
      "venue" : "USAF School of Aviation",
      "citeRegEx" : "Fix and Hodges,? \\Q1951\\E",
      "shortCiteRegEx" : "Fix and Hodges",
      "year" : 1951
    }, {
      "title" : "Discriminatory analysis. nonparametric discrimination: Consistency properties",
      "author" : [ "E. Fix", "J.L.J. Hodges" ],
      "venue" : "International Statistical Review / Revue Internationale de Statistique,",
      "citeRegEx" : "Fix and Hodges,? \\Q1989\\E",
      "shortCiteRegEx" : "Fix and Hodges",
      "year" : 1989
    }, {
      "title" : "Sample compression, learnability, and the vapnik-chervonenkis dimension",
      "author" : [ "S. Floyd", "M.K. Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Floyd and Warmuth.,? \\Q1995\\E",
      "shortCiteRegEx" : "Floyd and Warmuth.",
      "year" : 1995
    }, {
      "title" : "Efficient active learning of halfspaces: an aggressive approach",
      "author" : [ "A. Gonen", "S. Sabato", "S. Shalev-Shwartz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gonen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gonen et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient active learning of halfspaces: an aggressive approach",
      "author" : [ "A. Gonen", "S. Sabato", "S. Shalev-Shwartz" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Gonen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gonen et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient classification for metric data",
      "author" : [ "L. Gottlieb", "A. Kontorovich", "R. Krauthgamer" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2014
    }, {
      "title" : "Near-optimal sample compression for nearest neighbors",
      "author" : [ "L. Gottlieb", "A. Kontorovich", "P. Nisnevitch" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2014
    }, {
      "title" : "Proximity algorithms for nearly-doubling spaces",
      "author" : [ "L.-A. Gottlieb", "R. Krauthgamer" ],
      "venue" : "In APPROX-RANDOM, pages 192–204,",
      "citeRegEx" : "Gottlieb and Krauthgamer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gottlieb and Krauthgamer.",
      "year" : 2010
    }, {
      "title" : "Proximity algorithms for nearly doubling spaces",
      "author" : [ "L.-A. Gottlieb", "R. Krauthgamer" ],
      "venue" : "SIAM J. Discrete Math.,",
      "citeRegEx" : "Gottlieb and Krauthgamer.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gottlieb and Krauthgamer.",
      "year" : 2013
    }, {
      "title" : "Efficient classification for metric data",
      "author" : [ "L.-A. Gottlieb", "L. Kontorovich", "R. Krauthgamer" ],
      "venue" : "In Proceedings of the 23rd Annual Conference on Learning Theory, COLT",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2010
    }, {
      "title" : "Adaptive metric dimensionality reduction",
      "author" : [ "L.-A. Gottlieb", "A. Kontorovich", "R. Krauthgamer" ],
      "venue" : "In Proceedings of the 24th International Conference on Algorithmic Learning Theory, ALT",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive metric dimensionality reduction",
      "author" : [ "L.-A. Gottlieb", "A. Kontorovich", "R. Krauthgamer" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2016
    }, {
      "title" : "Nearly optimal classification for semimetrics",
      "author" : [ "L.-A. Gottlieb", "A. Kontorovich", "P. Nisnevitch" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2016
    }, {
      "title" : "Pac-bayesian compression bounds on the prediction error of learning algorithms for classification",
      "author" : [ "T. Graepel", "R. Herbrich", "J. Shawe-Taylor" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Graepel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graepel et al\\.",
      "year" : 2005
    }, {
      "title" : "Rates of convergence in active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Hanneke.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hanneke.",
      "year" : 2011
    }, {
      "title" : "Minimax analysis of active learning",
      "author" : [ "S. Hanneke", "L. Yang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hanneke and Yang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hanneke and Yang.",
      "year" : 2015
    }, {
      "title" : "Exact lower bounds for the agnostic probably-approximatelycorrect (PAC) machine learning model",
      "author" : [ "A. Kontorovich", "I. Pinelis" ],
      "venue" : null,
      "citeRegEx" : "Kontorovich and Pinelis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kontorovich and Pinelis.",
      "year" : 2016
    }, {
      "title" : "Maximum margin multiclass nearest neighbors",
      "author" : [ "A. Kontorovich", "R. Weiss" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "Kontorovich and Weiss.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kontorovich and Weiss.",
      "year" : 2014
    }, {
      "title" : "A bayes consistent 1-nn classifier",
      "author" : [ "A. Kontorovich", "R. Weiss" ],
      "venue" : "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Kontorovich and Weiss.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kontorovich and Weiss.",
      "year" : 2015
    }, {
      "title" : "k-nn regression adapts to local intrinsic dimension",
      "author" : [ "S. Kpotufe" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kpotufe.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kpotufe.",
      "year" : 2011
    }, {
      "title" : "Hierarchical label queries with data-dependent partitions",
      "author" : [ "S. Kpotufe", "R. Urner", "S. Ben-David" ],
      "venue" : "In Proceedings of the 28th Annual Conference on Learning Theory, COLT",
      "citeRegEx" : "Kpotufe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kpotufe et al\\.",
      "year" : 2015
    }, {
      "title" : "Navigating nets: Simple algorithms for proximity search",
      "author" : [ "R. Krauthgamer", "J.R. Lee" ],
      "venue" : "In 15th Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Krauthgamer and Lee.,? \\Q2004\\E",
      "shortCiteRegEx" : "Krauthgamer and Lee.",
      "year" : 2004
    }, {
      "title" : "Rates of convergence of nearest neighbor estimation under arbitrary sampling",
      "author" : [ "S.R. Kulkarni", "S.E. Posner" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Kulkarni and Posner.,? \\Q1995\\E",
      "shortCiteRegEx" : "Kulkarni and Posner.",
      "year" : 1995
    }, {
      "title" : "Relating data compression and learnability, unpublished",
      "author" : [ "N. Littlestone", "M.K. Warmuth" ],
      "venue" : null,
      "citeRegEx" : "Littlestone and Warmuth.,? \\Q1986\\E",
      "shortCiteRegEx" : "Littlestone and Warmuth.",
      "year" : 1986
    }, {
      "title" : "Empirical Bernstein bounds and sample-variance penalization",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "In Proceedings of the 22nd Annual Conference on Learning",
      "citeRegEx" : "Maurer and Pontil.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maurer and Pontil.",
      "year" : 2009
    }, {
      "title" : "Employing EM and pool-based active learning for text classification",
      "author" : [ "A. McCallum", "K. Nigam" ],
      "venue" : "In Proceedings of the 15th International Conference on Machine Learning,",
      "citeRegEx" : "McCallum and Nigam.,? \\Q1998\\E",
      "shortCiteRegEx" : "McCallum and Nigam.",
      "year" : 1998
    }, {
      "title" : "On the method of bounded differences",
      "author" : [ "C. McDiarmid" ],
      "venue" : null,
      "citeRegEx" : "McDiarmid.,? \\Q1989\\E",
      "shortCiteRegEx" : "McDiarmid.",
      "year" : 1989
    }, {
      "title" : "Active regression by stratification",
      "author" : [ "S. Sabato", "R. Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sabato and Munos.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sabato and Munos.",
      "year" : 2014
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "S. Shalev-Shwartz", "S. Ben-David" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz and Ben.David.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Ben.David.",
      "year" : 2014
    }, {
      "title" : "Structural risk minimization over data-dependent hierarchies",
      "author" : [ "J. Shawe-Taylor", "P.L. Bartlett", "R.C. Williamson", "M. Anthony" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Shawe.Taylor et al\\.,? \\Q1926\\E",
      "shortCiteRegEx" : "Shawe.Taylor et al\\.",
      "year" : 1926
    }, {
      "title" : "Consistent nonparametric regression",
      "author" : [ "C.J. Stone" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Stone.,? \\Q1977\\E",
      "shortCiteRegEx" : "Stone.",
      "year" : 1977
    }, {
      "title" : "PLAL: cluster-based active learning",
      "author" : [ "R. Urner", "S. Wulff", "S. Ben-David" ],
      "venue" : "In Proceedings of the 26th Annual Conference on Learning Theory, COLT",
      "citeRegEx" : "Urner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Urner et al\\.",
      "year" : 2013
    }, {
      "title" : "Distance-based classification with Lipschitz functions",
      "author" : [ "U. von Luxburg", "O. Bousquet" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Luxburg and Bousquet.,? \\Q2004\\E",
      "shortCiteRegEx" : "Luxburg and Bousquet.",
      "year" : 2004
    }, {
      "title" : "Submodularity in data subset selection and active learning",
      "author" : [ "K. Wei", "R.K. Iyer", "J.A. Bilmes" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Wei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2015
    }, {
      "title" : "Exponential bounds of mean error for the nearest neighbor estimates of regression functions",
      "author" : [ "L.C. Zhao" ],
      "venue" : "J. Multivariate Anal.,",
      "citeRegEx" : "Zhao.,? \\Q1987\\E",
      "shortCiteRegEx" : "Zhao.",
      "year" : 1987
    }, {
      "title" : "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "X. Zhu", "J. Lafferty", "Z. Ghahramani" ],
      "venue" : "In ICML 2003 workshop,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 47,
      "context" : "In pool-based active learning [McCallum and Nigam, 1998], a collection of random examples is provided, and the algorithm can interactively query an oracle to label some of the examples.",
      "startOffset" : 30,
      "endOffset" : 56
    }, {
      "referenceID" : 35,
      "context" : ", 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005].",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 40,
      "context" : "[2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015].",
      "startOffset" : 47,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al.",
      "startOffset" : 115,
      "endOffset" : 1430
    }, {
      "referenceID" : 27,
      "context" : "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al.",
      "startOffset" : 115,
      "endOffset" : 1478
    }, {
      "referenceID" : 27,
      "context" : "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al. [2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015].",
      "startOffset" : 115,
      "endOffset" : 1515
    }, {
      "referenceID" : 49,
      "context" : "Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014].",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010].",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals.",
      "startOffset" : 23,
      "endOffset" : 855
    }, {
      "referenceID" : 2,
      "context" : ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals.",
      "startOffset" : 23,
      "endOffset" : 880
    }, {
      "referenceID" : 2,
      "context" : ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals. The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al., 2013, Kpotufe et al., 2015]. Dasgupta and Hsu [2008] showed that a suitable cluster-tree can yield label savings in this framework, and papers following up quantified the label savings under distributional clusterability assumptions.",
      "startOffset" : 23,
      "endOffset" : 1337
    }, {
      "referenceID" : 17,
      "context" : "to a consistent algorithm [Dasgupta, 2012].",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "A selective querying strategy has been shown to be beneficial for nearest neighbors under covariate shift [Berlind and Urner, 2015], where one needs to adapt to a change in the data generating process.",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 27,
      "context" : "2, and present the guarantees of the compression-based passive learner of Gottlieb et al. [2016b] in Section 2.",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "Constructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010].",
      "startOffset" : 65,
      "endOffset" : 97
    }, {
      "referenceID" : 43,
      "context" : "The size of any t-net of a metric space A ⊆ X is at most ⌈diam(A)/t⌉ddim(X )+1 [Krauthgamer and Lee, 2004].",
      "startOffset" : 79,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al. [2014a]. Constructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010].",
      "startOffset" : 101,
      "endOffset" : 223
    }, {
      "referenceID" : 26,
      "context" : "As shown in Gottlieb and Krauthgamer [2013], the doubling dimension is “almost hereditary” in the sense that forA ⊂ X , we have ddim(A) ≤ cddim(X ) for some universal constant c ≤ 2 [Feldmann et al.",
      "startOffset" : 12,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "As shown in Gottlieb and Krauthgamer [2013], the doubling dimension is “almost hereditary” in the sense that forA ⊂ X , we have ddim(A) ≤ cddim(X ) for some universal constant c ≤ 2 [Feldmann et al., 2015, Lemma 6.6]. For simplicity, the bounds above are presented in terms of ddim(X ), the doubling dimension of the ambient space. It should be noted that one can obtain tighter bounds in terms of ddim(U(S)) when the latter is substantially lower than that of the ambient space, and it is also possible to perform metric dimensionality reduction, as in Gottlieb et al. [2013].",
      "startOffset" : 183,
      "endOffset" : 577
    }, {
      "referenceID" : 27,
      "context" : "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample’s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al.",
      "startOffset" : 189,
      "endOffset" : 289
    }, {
      "referenceID" : 27,
      "context" : "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample’s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension.",
      "startOffset" : 189,
      "endOffset" : 316
    }, {
      "referenceID" : 27,
      "context" : "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample’s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension. Later, it was noticed in Gottlieb et al. [2014b] that the presence of a margin allows for compression — in fact, nearly optimally so.",
      "startOffset" : 189,
      "endOffset" : 447
    }, {
      "referenceID" : 27,
      "context" : "Gottlieb et al. [2016b] propose a passive learner with the following guarantees1 as a function of the separation of S.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : "3 (Gottlieb et al. [2016b]).",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "3 (Gottlieb et al. [2016b]). Let m be an integer, δ ∈ (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier h Spas , where Spas ⊆ Sin, such that, with probability 1− δ, err(h Spas ,D) ≤ Gmin(m, δ). The passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|ν(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points).",
      "startOffset" : 3,
      "endOffset" : 286
    }, {
      "referenceID" : 27,
      "context" : "3 (Gottlieb et al. [2016b]). Let m be an integer, δ ∈ (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier h Spas , where Spas ⊆ Sin, such that, with probability 1− δ, err(h Spas ,D) ≤ Gmin(m, δ). The passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|ν(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points). For the binary classification case (|Y| = 2) an efficient algorithm is shown in Gottlieb et al. [2016b]. However, in the general multiclass case, it is not known how to find a minimal t-separation efficiently — a naive approach requires solving the NP-hard problem of vertex cover.",
      "startOffset" : 3,
      "endOffset" : 753
    }, {
      "referenceID" : 27,
      "context" : "A main challenge for active learning in our non-parametric setting is performing model selection, that is, selecting a good scale t similarly to the passive learner of Gottlieb et al. [2016b]. In the passive supervised setting, the approach developed in several previous works [Gottlieb et al.",
      "startOffset" : 168,
      "endOffset" : 192
    }, {
      "referenceID" : 36,
      "context" : "2In the case of binary labels (|Y| = 2), the problem of estimating Sa(t) can be formulated as a special case of the benign noise setting for parametric active learning, for which tight lower and upper bounds are provided in Hanneke and Yang [2015]. However, our case is both more general (as we allow multiclass labels) and more specific (as we are dealing with a specific “hypothesis class”).",
      "startOffset" : 224,
      "endOffset" : 248
    }, {
      "referenceID" : 46,
      "context" : "3This follows from Theorem 4 of Maurer and Pontil [2009] since 7 3(n−1) ≤ 8 3n for n ≥ 8.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 43,
      "context" : "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al.",
      "startOffset" : 132,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995].",
      "startOffset" : 167,
      "endOffset" : 189
    }, {
      "referenceID" : 20,
      "context" : "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information.",
      "startOffset" : 167,
      "endOffset" : 243
    }, {
      "referenceID" : 20,
      "context" : "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information. This side information is used to represent the labels of the sample points in the compression set. A similar idea appears in Floyd and Warmuth [1995] for hypotheses with short description length.",
      "startOffset" : 167,
      "endOffset" : 515
    }, {
      "referenceID" : 27,
      "context" : "If the compression set includes only the original labels, the compression analysis of Gottlieb et al. [2016b] gives the bound GB(ǫ,N, δ,m, 1).",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "This expression previously appeared in Berend and Kontorovich [2015, Equation (25)] in the context of information-theoretic lower bounds; the current terminology was motivated in Kontorovich and Pinelis [2016], where various precise estimates on bayes(·) were provided.",
      "startOffset" : 39,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : ", Anthony and Bartlett [1999] p.",
      "startOffset" : 2,
      "endOffset" : 30
    }, {
      "referenceID" : 50,
      "context" : "4) in Shalev-Shwartz and Ben-David [2014]), for some j,X,B it holds that E[err(ĥj ,Dj)] ≥ 1 T T",
      "startOffset" : 6,
      "endOffset" : 42
    }, {
      "referenceID" : 48,
      "context" : "Hence, by McDiarmid’s inequality [McDiarmid, 1989], with probability at least 1 − 1 28 , |ν(2 ) − E[ν(12 )]| ≤ √",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 31,
      "context" : "A variant of this approach [Gottlieb et al., 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 40,
      "context" : ", 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for",
      "startOffset" : 41,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "Our error guarantees give a constant factor over the error guarantees of Gottlieb et al. [2016b]. A variant of this approach [Gottlieb et al.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "the algorithm of Gottlieb et al. [2016b]. Since in our analysis MARMANN obtains a constant factor over the error of the passive learner, Bayes-consistency cannot be inferred from our present techniques; we leave this problem open for future research.",
      "startOffset" : 17,
      "endOffset" : 41
    } ],
    "year" : 2016,
    "abstractText" : "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.",
    "creator" : "LaTeX with hyperref package"
  }
}