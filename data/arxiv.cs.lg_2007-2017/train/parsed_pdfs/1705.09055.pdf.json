{
  "name" : "1705.09055.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The cost of fairness in classification",
    "authors" : [ "Aditya Krishna Menon", "Robert C. Williamson" ],
    "emails" : [ "bob.williamson}@data61.csiro.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Suppose we wish to learn a classifier to determine suitable candidates for a job. This classifier may accept as inputs various characteristics about a candidate, such as their interview performance, qualifications, and years of experience. Suppose one of these characteristics is deemed sensitive, e.g. their race. Then, we might be required to constrain the classifier to not be overly discriminative with respect to this sensitive feature. Subject to this constraint, we would of course like our classifier to be as accurate as possible. This is known as the fairness-aware learning problem, and has received considerable attention in the machine learning community of late [Pedreshi et al., 2008, Kamiran and Calders, 2009, Calders and Verwer, 2010, Dwork et al., 2012, Kamishima et al., 2012, Fukuchi et al., 2013, Zafar et al., 2016, Hardt et al., 2016, Zafar et al., 2017]. The primary focus has been on formalising what constitutes a perfectly fair classifier, and how one learns a classifier to approximately achieve such fairness. There have been several distinct proposals in both regards (see §2.2).\nIn this paper, we are interested in the tradeoffs inherent in the problem of learning with a fairness requirement. We specifically focus on the impact fairness has on two aspects of our original problem: the structure of the optimal solution, and the degradation in performance. Our three main contributions C1—C3 comprise analyses of both issues:\nC1: we provide a reduction of two popular existing fairness measures to cost-sensitive risks (Lemmas 1, 2). C2: we show that for such cost-sensitive classification and fairness measures, the optimal fairness-aware classifier is an instance-dependent thresholding of the class-probability function (Propositions 3, 4). C3: we provide a measure of the alignment between the class-probabilities for the target and sensitive features,\nwhich quantifies the degradation in performance owing to the fairness requirement (Propositions 8, 9).\nA consequence of C1 is a simple procedure for learning with a fairness requirement, involving training separate class-probability estimators for the target and sensitive features, and combining them suitably (§5.3). Underpinning our analysis is a general framework casting the fairness-aware learning problem as one of minimising the difference of two statistical risks (§2.2), which allows for an abstract, generic treatment of the problem.\nar X\niv :1\n70 5.\n09 05\n5v 1\n[ cs\n.L G\n] 2\n5 M\nay 2"
    }, {
      "heading" : "2 Background and notation",
      "text" : "We fix notation and review relevant background. Table 1 summarises some core concepts that we refer to frequently."
    }, {
      "heading" : "2.1 Standard learning from binary labels",
      "text" : "Let X ⊆ Rd be a measurable instance space, e.g. characteristics of a candidate for a job. In standard learning from binary labels, we have samples from a distribution D over X × {0, 1}, with (X,Y) ∼ D. Here, Y is some target feature we would like to predict, e.g. whether to hire a candidate. Our goal is to output a measurable randomised classifier parametrised by f : X→ [0, 1] that distinguishes between positive (Y = 1) and negative (Y = 0) instances. A randomised classifier predicts any x ∈ X to be positive with probability f (x); the quality of any such classifier is assessed by a statistical risk R(·; D) : [0, 1]X → R+ which, for some Φ : [0, 1]3 → R+, is [Narasimhan et al., 2014]\nR( f ; D) .= Φ(FNR( f ; D), FPR( f ; D), P(Y = 1)),\nfor the false-negative and false-positive rates (FNR( f ; D), FPR( f ; D)) .= ( E\nX |Y=1 [1 − f (X)] , E X |Y=0 [ f (X)]\n) , (1)\nwhich are average class-conditional probabilities of error when classifying x ∈ X as positive with probability f (x).\nExample 1. The cost-sensitive error with cost parameter c ∈ (0, 1) is parametrised by Φ : (u, v, p) 7→ p · (1 − c) · u + (1 − p) · c · v. When c = P(Y = 1), this is a scaled version of the balanced error,\nRbal( f ; D) = (FNR( f ; D) + FPR( f ; D))/2. (2)\nA Bayes-optimal randomised classifier for a risk is any f ∗ ∈ Argmin R( f ; D). For a broad class of Φ, the optimal classifier is a (possibly distribution dependent) thresholding of the class-probability function, f ∗(x) = nη(x) > t∗(D)o [Narasimhan et al., 2014], where η(x) .= P(Y = 1 | X = x) and n·o denotes the indicator function. For the cost-sensitive error with parameter c, the Bayes-optimal classifier is f ∗(x) = nη(x) > co [Elkan, 2001]. These Bayes-optimal classifiers motivate a plugin estimator, where one thresholds an empirical estimate of η [Narasimhan et al., 2014]."
    }, {
      "heading" : "2.2 Fairness-aware learning",
      "text" : "In fairness-aware learning, one modifies the standard problem of learning from binary labels in two ways. The statistical setup is modified by assuming that in addition to the target feature Y, there is some sensitive feature Ȳ we would like to treat in some special way, e.g. the race of a candidate. The classifier evaluation is modified by assuming that we reward classifiers that are “fair” in the treatment of Ȳ. To make this goal concrete, the literature has studied notions of perfect and approximate fairness. (We construct a general formalism for the problem using these in Problem 1.)\nPerfect fairness. We will focus on two simple notions of perfect fairness, stated in terms of the random variables Y, Ȳ, and classifier prediction Ŷ | X ∼ Bernoulli( f (X)). (Note that we assume Ȳ to be binary.) The first is demographic parity [Calders and Verwer, 2010], which requires the predictions to be independent of the sensitive feature: P(Ŷ = 1 | Ȳ = 0) = P(Ŷ = 1 | Ȳ = 1). (3) The second is equality of opportunity [Hardt et al., 2016], which requires the predictions to be independent of the sensitive feature, but only for the positive instances:\nP(Ŷ = 1 | Y = 1, Ȳ = 0) = P(Ŷ = 1 | Y = 1, Ȳ = 1).\nOther notions of perfect fairness include equalised odds [Hardt et al., 2016], and lack of disparate mistreatment [Zafar et al., 2017]. Demographic parity has received the most study; however, it is known to have deficiencies [Dwork et al., 2012, Hardt et al., 2016, Zafar et al., 2017].\nApproximate fairness. We will focus on two fairness measures that quantify the degree of fairness a given classifier possesses. The first is the disparate impact (DI) factor [Feldman et al., 2015], which is the ratio of the probabilities appearing in the definition of demographic parity:\nDI( f ) .= P(Ŷ = 1 | Ȳ = 0) P(Ŷ = 1 | Ȳ = 1) . (4)\nThe second is the mean difference (MD) score [Calders and Verwer, 2010], which replaces the ratio with a difference:\nMD( f ) .= P(Ŷ = 1 | Ȳ = 1) − P(Ŷ = 1 | Ȳ = 0). (5)\nWe refer the reader to Žliobaitė [2015] for a survey of other fairness measures, including variants of the above.\nA final remark is that the sensitive feature may or may not be available when one trains the classifier (see §5.2). Avoiding the use of the sensitive feature by itself does not guard against discrimination [Pedreshi et al., 2008]."
    }, {
      "heading" : "2.3 Existing work on fairness",
      "text" : "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview.\nIn the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances. Zemel et al. [2013], Louizos et al. [2016] proposed approaches to learn feature representations that guarantee fairness. Both methods depend directly on the specific instances x ∈ X. In contrast, our approach never touches the instances, but only risks; this has the substantial advantage of avoiding change under reparametrisation, and avoids the infinite regress of determining what is “similar”."
    }, {
      "heading" : "3 Fairness measures as statistical risks",
      "text" : "We present our general view of fairness measures as statistical risks where the sensitive feature is the target. This lets us analyse fairness measures using tools for studying risks."
    }, {
      "heading" : "3.1 General fairness measures",
      "text" : "To formalise the notion of a fairness measure, we first specify our statistical setup for fairness-aware learning. Let Djnt be a joint distribution over X × {0, 1} × {0, 1}, with random variables (X, Ȳ,Y) ∼ Djnt. Here, X\nrepresents the instance, Y the target feature, and Ȳ the sensitive feature. We will be interested in three induced distributions: we refer to P(X,Y) as D, P(X, Ȳ) as D̄DP, and P(X, Ȳ | Y = 1) as D̄EO. We use D̄ to refer generically to either D̄DP or D̄EO.\nIn fairness-aware learning, our goal is to output a randomised classifier1 f : X→ [0, 1] with small statistical risk on D, so that Y is well predicted; we will denote this risk by Rperf(·; D), and refer to it as a performance measure. In addition to this goal, we also want f to have large fairness measure Rfair(·; Djnt) : [0, 1]X → R+. Formally:\nProblem 1. Given a distribution Djnt, performance and fairness measures Rperf , Rfair, and tradeoff parameter λ > 0, minimise the combined risk\nRfull( f ; Djnt, λ) . = Rperf( f ; D) − λ · Rfair( f ; D̄). (6)\nWe will primarily focus on the following tractable special case of the above problem. (See §6.1 for a slight variant.)"
    }, {
      "heading" : "3.2 Classification-type fairness measures",
      "text" : "In Problem 1, Rfair depends on Djnt, which is defined over the triplet (X, Ȳ,Y). An interesting sub-class of Rfair are those that depend only on D̄DP, which is defined over the tuple (X, Ȳ). As with Rperf , such Rfair can be written as a statistical risk on D̄DP: in particular, given some Φfair : [0, 1]3 → R+, we may define a classification-type fairness measure via\nRfair( f ; D̄DP) . = Φfair((FPR( f ; D̄DP), FNR( f ; D̄DP), P(Ȳ = 1))).\nIntuitively, we are testing whether we can predict the sensitive feature Ȳ from X. When it is possible to do so well according to Rfair, we do not have fairness.\nReturning to the two fairness measures of §2.2, we observe that FPR( f ; D̄DP) = P(Ŷ = 1 | Ȳ = 0) and FNR( f ; D̄DP) = P(Ŷ = 0 | Ȳ = 1); thus, they are expressible as risks.\nExample 2. The disparate impact factor may be written\nDI( f ; D̄DP) . = FPR( f ; D̄DP) 1 − FNR( f ; D̄DP) , (7)\ni.e. it uses Φfair : (u, v, p) 7→ v1−u .\nExample 3. The mean difference score may be written\nMD( f ; D̄DP) . = 1 − FNR( f ; D̄DP) − FPR( f ; D̄DP), (8)\ni.e. it uses Φfair : (u, v, p) 7→ 1 − (u + v).\nWe make a few remarks on this class of fairness measures. First, by casting fairness measures as statistical risks, Equation 6 becomes the problem of minimising the difference of two statistical risks. This is a departure from standard tradeoffs between two risks, where one considers the sum rather than the difference; fairness measures are unusual as we seek to maximise the underlying risk Rfair.\nSecond, we can in principle plug-in any standard Φfair and get a sensible measure of fairness. However, certain Φfair may be more convenient to work with, e.g. from the point of view of interpretability; this is the case for disparate impact, which has roots in the 80% rule of the U.S. Equal Employment Opportunity Commission [EEOC, 1979].\n1Here and elsewhere, this is understood to mean a randomised classifier parametrised by f .\nThird, there is no requirement to restrict attention to D̄DP. In particular, we could equally use a risk on D̄EO, yielding\nRfair( f ; D̄EO) . = Φfair((FPR( f ; D̄EO), FNR( f ; D̄EO), P(Ȳ = 1))),\nwhich aligns with the equality of opportunity objective.\nFourth, in general one needs to impose additional structure on Rfair to guarantee fairness, as we now discuss."
    }, {
      "heading" : "3.3 Anti-classifiers and symmetrised fairness",
      "text" : "Employing a statistical risk for Rfair in Equation 6 constrains the false-positive and negative rates. However, these constraints may assume our classifier is non-trivial on D̄DP; as an example, if a classifier f has MD( f ; D̄DP) = τ, then MD(1 − f ; D̄DP) = 1 − τ. Thus, one might be able to deceive such measures via an anti-classifier; i.e. one which has high fairness, but whose negation has low fairness.\nIntuitively, one wishes to disallow such a trivial transformation from adversely affecting fairness. A simple way to do this is to consider the symmetrised fairness measure\nR fair( f ; Djnt) . = Rfair( f ; Djnt) ∧ Rfair(1 − f ; Djnt), (9)\nwhere ∧ denotes the min operation. Maximising Equation 9 requires that both the classifier and the anti-classifier perform well. Such symmetrised measures simply modify the underlying Φfair: note that FPR(1 − f ) = 1 − FPR( f ), and similarly for FNR(1 − f ). Thus, R fair is parametrised by\nΦ fair(u, v, p) . = Φfair(u, v, p) ∧ Φfair(1 − u, 1 − v, p).\nIn §4.4, we show that a broad class of Φfair have Φ fair maximised when f ≡ 1/2; i.e. a completely random classifier is maximally fair. (We can equally enforce that f ≡ π̄ is maximally fair via a simple correction; see Appendix C.)"
    }, {
      "heading" : "3.4 Relation to existing work",
      "text" : "The notion that statistical risks on D̄DP are suitable as fairness measures is implicit in prior surveys of such measures. Formalising this notion lets us subsequently use tools for studying risks to analyse a range of fairness measures.\nThe need for symmetrised fairness has not received much attention, with works employing the MD and DI scores e.g. Calders and Verwer [2010], Feldman et al. [2015] implicitly assuming that learned classifiers will perform better than random guessing on D̄DP."
    }, {
      "heading" : "4 A cost-sensitive view of fairness measures",
      "text" : "The previous section cast the DI and MD fairness measures as statistical risks on D̄. We now show how they may be further related to cost-sensitive risks. This implies that analysis of cost-sensitive fairness measures suffices to analyse both these measures. To begin, we first introduce a useful reparameterisation of the standard cost-sensitive risk."
    }, {
      "heading" : "4.1 Balanced cost-sensitive risk",
      "text" : "Recall that the standard cost-sensitive risk (Example 1) is parametrised byΦ : (u, v, p) 7→ p·(1−c)·u+(1−p)·c·v. Now define the balanced cost-sensitive risk to be parametrised by Φbal : (u, v, p) 7→ 2 · Φ(u, v, 1/2), so that\nCSbal( f ; D, c) . = (1 − c) · FNR( f ; D) + c · FPR( f ; D). (10)\nWhen c = 1/2, we get the balanced error (Equation 2). In general, this is simply a scaled and reparameterised version of the standard cost-sensitive risk: we have CS( f ; D, c) = (α+ β) ·CSbal( f ; D, c′), where α = π · (1− c), β = (1 − π) · c, and c′ = β/(α + β). This reparameterisation will however prove convenient in analysing existing fairness measures."
    }, {
      "heading" : "4.2 Disparate impact and cost-sensitive risk",
      "text" : "Our first result is that the disparate impact factor (Equation 7) can be related to the balanced cost-sensitive risk. This suggests that study of the latter helps understand the former.\nLemma 1. Pick any distribution D̄ and randomised classifier f . Then, for any τ ∈ [0, 1], if c .= 11+τ ∈ [ 1 2, 1 ] ,\nDI( f ; D̄) ≥ τ ⇐⇒ CSbal( f ; D̄, c) ≥ 1 − c, (11) DI ( f ; D̄) ≥ τ ⇐⇒ CSbal( f ; D, c) ∈ [1 − c, c]. (12)\nWe make two remarks. First, Lemma 1 does not imply that disparate impact equals a cost-sensitive risk, but rather, that their superlevel sets are related. This nonetheless means that a disparate impact constraint is equivalent to a cost-sensitive constraint, with the latter being easier to analyse.\nSecond, as Lemma 1 holds for any distribution D̄, we can plug in D̄EO, yielding an equivalent result for disparate impact in an “equality of opportunity” regime, i.e. DI( f ; D̄EO)."
    }, {
      "heading" : "4.3 Mean difference score and balanced error",
      "text" : "Our next result is that the mean difference score (Equation 8) has a strong connection to a balanced cost-sensitive risk.\nLemma 2. Pick any distribution D̄ and randomised classifier f . Then, for any τ ∈ [0, 1], if c = 1+τ2 ∈ [ 1 2, 1 ] ,\nMD( f ; D̄) = 1 − 2 · CSbal ( f ; D̄, 1/2 ) (13)\nMD( f ; D̄) ≥ τ ⇐⇒ CSbal ( f ; D̄, 1/2 ) ≥ 1 − c\nMD ( f ; D̄) ≥ τ ⇐⇒ CSbal ( f ; D̄, 1/2 ) ∈ [1 − c, c] .\nThus, the MD score is a transformation of the balanced error, as the latter corresponds to c = 1/2. Note that Equation 13 implies an equivalence of risks, and not just super-level sets.\nNote also that for the MD score, the corresponding balanced cost-sensitive risk has a cost-parameter that does not depend on the chosen τ. This proves beneficial for the purposes of learning with this measure, as we shall see in §5.3."
    }, {
      "heading" : "4.4 The cost-sensitive fairness problem",
      "text" : "The above results establish the versatility of cost-sensitive fairness measures. In the sequel, we will thus focus on such measures for general cost parameters, relying on Lemmas 1 and 2 to relate statements about them to statements about the DI and MD scores. For symmetry, we will also focus on cost-sensitive risks for the base problem, albeit with a possibly different cost parameter.\nThe above requires one tweak: as per §3.3, it is desirable to work with symmetrised versions of any fairness measure. For general balanced cost-sensitive risks, these symmetrised versions have a simple form: it is an easy calculation that for any c ∈ [0, 1] and classifier f , CSbal(1 − f ; D̄, c̄) = 1 − CSbal( f ; D̄, c̄). Thus, the symmetrised version is\nCS bal( f ; D̄, c̄) = CSbal( f ; D̄, c̄) ∧ (1 − CSbal( f ; D̄, c̄)). (14)\nThis risk is maximised when CSbal( f ; D̄, c) = 1/2. A sufficient condition for this is f ≡ 1/2, so that, in line with our intuition, a completely random classifier is maximally fair.\nEquipped with this, we can formalise the special case of the general Problem 1 that is the focus of the sequel.\nProblem 2. Given a distribution Djnt, costs c, c̄, and tradeoff parameter λ ∈ R, minimise (for D̃ ∈ {D̄DP, D̄EO}),\nRfull( f ; D, D̃, c, c̄, λ) . = CS( f ; D, c) − λ · CS( f ; D̃, c̄). (15)\nWe make three comments on Problem 2. First, we use the standard rather than balanced cost-sensitive risk as it simplifies the analysis in subsequent sections; recall from §4.1 that the two are related by a scaling and reparameterisation.\nSecond, Equation 15 employs the standard (non-symmetrised) cost-sensitive measure, but without a positivity constraint on λ. This is because, by Equation 14, a constraint on the symmetrised risk imposes upper and lower bounds on the cost-sensitive risk. Then, λ is the difference in the Lagrange multipliers for these two constraints, which need not be positive; see Appendix D for more discussion.\nThird, there is a subtlety in using Problem 2 as a proxy for the DI. As noted above, it is only the superlevel sets of the DI that are related to that of a cost-sensitive risk, and not the DI itself. This manifests in the cost parameter c̄ itself being a user-specified parameter, unlike for the MD score where it is fixed at 1/2. We will discuss this issue more in §5.3."
    }, {
      "heading" : "4.5 Relation to existing work",
      "text" : "Lemma 1 is a special case of a broader relationship between fractional performance measures and “level-finder” functions [Parambath et al., 2014, Theorem 1], [Narasimhan et al., 2015, Lemma 7]. Feldman et al. [2015] related the disparate impact to the balanced error, but their bound depends on the distribution and classifier, while ours uses a cost-sensitive risk with constant τ; see §7 and Appendix F."
    }, {
      "heading" : "5 Bayes-optimal fairness-aware classifiers",
      "text" : "Having formalised the fairness-aware learning problem, and having further related existing fairness measures to cost-sensitive risks, we are in a position to study the tradeoffs imposed by the problem. We begin by asking: what impact does the fairness requirement have on the Bayes-optimal solutions? The structure of these solutions provides insight into the problem, and also suggests a simple practical algorithm. In the following, we utilise the following quantities:\nη(x) .= P(Y = 1 | X = x) π .= P(Y = 1) ηDP(x) . = P(Ȳ = 1 | X = x) π̄ .= P(Ȳ = 1) ηEO(x) . = P(Ȳ = 1 | X = x,Y = 1).\n(16)"
    }, {
      "heading" : "5.1 Bayes-optimal cost-sensitive classifiers",
      "text" : "We will study the Bayes-optimal classifiers of Problem 2, so that both our fairness and performance measures are cost-sensitive risks. When working with D̄DP (i.e. the demographic parity setting), Equation 15 admits an interesting minimiser.\nProposition 3. Pick any distribution Djnt, costs c, c̄ ∈ [0, 1], and λ ∈ R. Then,\nArgmin f ∈[0,1]X\nRfull( f ; D, D̄DP, c, c̄, λ) = { f ∗ | (∀x) s∗(x) , 0 =⇒ f ∗(x) = ns∗(x) > 0o}, (17) (∀x ∈ X) s∗(x) .= η(x) − c − λ · (ηDP(x) − c̄). (18)\nTwo comments are in order. First, we observe that the optimal classifier above is in fact deterministic, except for those x for which s∗(x) is exactly 0. In general, for a given λ, we expect this to only hold for few or no x ∈ X. When s∗(x) = 0, however, then any value of f ∗(x) will be optimal. Second, assuming s∗(x) , 0, when λ = 0, the optimal f ∗ is the familiar Bayes-optimal classifier for a cost-sensitive risk, nη(x) > co. For λ , 0, however, we have an instance dependent threshold correction, which depends on ηDP(x). The correction increases the standard threshold of c whenever ηDP(x) > c̄; intuitively, when we are confident in the sensitive feature being active for an instance, we are more conservative in classifying the instance as positive.\nWe now consider a fairness measure that reflects the equality of opportunity notion, and thus works with D̄EO rather than D̄DP. This results in a slightly different Bayes-optimal classifier.\nProposition 4. Pick any Djnt, costs c, c̄ ∈ [0, 1], and λ ∈ R. Then,\nArgmin f ∈[0,1]X\nRfull( f ; D, D̄EO, c, c̄, λ) = { f ∗ | (∀x) s∗(x) , 0 =⇒ f ∗(x) = ns∗(x) > 0o}, (19) (∀x ∈ X) s∗(x) .= ( 1 − λ · π−1 · (ηEO(X) − c̄) ) · η(x) − c.\nThis result is of the same flavour as Proposition 3, with two important differences. First, we only need to know the probability of the sensitive feature being active for the positive instances. Second, the form of the threshold correction is no longer additive, but multiplicative."
    }, {
      "heading" : "5.2 Special case: using the sensitive feature as input",
      "text" : "The previous section studied a general X, where the sensitive feature was not necessarily provided as input to the classifier. The form of the optimal classifier simplifies when we allow the sensitive feature as an input. We have the following analogue of Proposition 3 when working with D̄DP.\nCorollary 5. Pick any distribution Djnt where D includes the sensitive feature, costs c, c̄ ∈ [0, 1], and λ ∈ R. For η(x, ȳ) = P(Y = 1 | X = x, Ȳ = ȳ),\nArgmin f ∈[0,1]X×{0,1}\nRfull( f ; D, D̄DP, c, c̄, λ) = { f ∗ | (∀x, ȳ) s∗(x, ȳ) , 0 =⇒ f ∗(x, ȳ) = ns∗(x, ȳ) > 0o}, (∀x ∈ X) s∗(x, 0) .= η(x, 0) − c + λ · c̄ (∀x ∈ X) s∗(x, 1) .= η(x, 1) − c − λ · (1 − c̄).\nHere, instead of an instance-dependent threshold, we simply apply different (constant) thresholds to the class-probabilities for each value of the sensitive feature. This is a simple consequence of Proposition 3, as we can simply consider one of the features of X to be perfectly predictive of the sensitive feature, which makes ηDP(x, ȳ) ∈ {0, 1}. An analogous special case holds for Proposition 4, and is deferred to Corollary 14 of the Appendix."
    }, {
      "heading" : "5.3 A plugin approach to fairness-aware learning",
      "text" : "The Bayes-optimal classifiers derived above rely on thresholding the class-probabilities η and η̄. Thus, analogously to the Bayes-optimal classifiers for standard statistical risks, this motivates a simple plugin estimation approach to fairness-aware learning problem: estimate η, η̄ separately, e.g. by logistic regression, and then combine them as per Equations 17, 18 to construct a classifier. When the sensitive feature is available, then all that is needed is a single model for η(x, ȳ), which is thresholded separately for each of the sensitive feature values.\nWe make three comments on the proposed approach. First, one must of course tune λ to achieve a desirable tradeoff between accuracy and fairness. This fortunately does not require retraining any model, as we can simply employ the learned η, η̄ and appropriately change how they are thresholded to form a classifier. One can tune λ, so as to reach some desired operating point on the accuracy-fairness curve.\nSecond, if we find s∗(x) = 0 for some x ∈ X, any prediction for that x optimises the objective of Equation 15; however, we may seek to tune this prediction to favour e.g. maximal performance on the original problem.\nThird, we reiterate that for the disparate impact, the cost parameter c̄ must be tuned as well, but that as per λ, this does not require retraining any model."
    }, {
      "heading" : "5.4 Relation to existing work",
      "text" : "Computing the Bayes-optimal classifiers as above is not without precedent: Hardt et al. [2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (§6), and also suggests a simple algorithm.\nHardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem. They considered a slightly different constrained version of the problem, where one forces the solution to have perfect rather than approximate fairness. Our Propositions 3 and 4 provide an explicit form for the correction when approximate fairness is desired, as well as when the sensitive feature is available or not during training. Recently, Woodworth et al. Woodworth et al. [2017] established limits on the post-processing approach of Hardt et al. [2016]; studying this in our context of approximate fairness measures would be of interest.\nCalders and Verwer [2010] proposed to modify the output of naïve Bayes so as to minimise the MD score. However, their approaches do have any theoretical guarantees.\nOur plugin learning procedure merely requires estimating class-probabilities, which for logistic regression is a convex problem. This avoids optimisation challenges facing existing approaches. For example, one way to approximately solve Equation 15 is to pick convex surrogate losses `, ¯̀: {0, 1} × R → R+, and find [Zafar et al., 2016, 2017]\ns∗ ∈ Argmin s : X→R CS(s; D, c, `) − λ · CS(s; D̄, c̄, ¯̀)\nfor the surrogate cost-sensitive risk [Scott, 2012],\nCS(s; D, c, `) .= E (X,Y)∼D [CY · `(Y, s(X))] (20)\nfor C1 = 1 − c,C0 = c. Note however that for nonlinear ¯̀, this objective will be non-convex in s. Even if one manages to overcome this challenge, guaranteeing large surrogate fairness does not imply large fairness of the underlying classifier, as the former is an upper bound to the latter. Similar problems plague related approaches based on regularisation [Kamishima et al., 2012, Fukuchi et al., 2013]."
    }, {
      "heading" : "6 Quantifying the accuracy-fairness tradeoff",
      "text" : "We now study the tradeoff between performance on our base problem and fairness, and show it is quantifiable by a measure of alignment of the target and sensitive variables."
    }, {
      "heading" : "6.1 The fairness frontier",
      "text" : "Our definition of the fairness-aware learning problem (Problem 1) was in terms of a linear tradeoff between the performance and fairness measures. To quantify the tradeoff2 imposed by a fairness constraint, we will study the following explicitly constrained problem: for τ ∈ [0, 1], let\nf ∗τ ∈ Argmin f : X→[0,1] Rperf( f ; D) : R fair( f ; D̄) ≥ τ (21) F(τ) = Rperf( f ∗τ ; D) − Rperf( f ∗0 ; D). (22)\nThe function F : [0, 1] → R+ represents the fairness frontier: for a given lower bound on (symmetrised) fairness, it measures the best excess risk over the solution without a fairness constraint. Evidently, F(·) is non-decreasing since the constraints on Rfair are nested as τ increases; i.e., demanding more fairness can never improve performance.\nAs per the previous section, the case of cost-sensitive performance and fairness measures is of interest. Here, the objectives in Equations 6 and 21 are related by the Lagrangian principle; see Appendix D for details. Further, when X is finite, Equation 21 reduces to a linear program. (For infinite X we obtain a semi-infinite linear program [Goberna and Lopéz, 1998], whose duality is subtler to analyse.)\nLemma 6. For finite X, pick any distribution Djnt, costs c, c̄ ∈ [0, 1], and τ ∈ R+. Then, the problem\nmin f : X→[0,1] CS( f ; D, c) : CS ( f ; D̄, c̄) ≥ τ\nis expressible as a linear program.\nExploiting this linearity of the objective and constraints, we can further establish that the frontier is a convex curve. We caution that this result requires equivalence of Rfair and a cost-sensitive risk, and so is not applicable for the DI factor.\nLemma 7. Pick any distribution Djnt, and cost-sensitive performance and fairness measures. Then, the function F : R+ → R+ of Equation 22 is convex.\nWhile Lemmas 6 and 7 are useful for computing the frontier, they do not specify how the curve’s behaviour as τ is varied relates to properties of Djnt. We now study this issue."
    }, {
      "heading" : "6.2 The frontier and class-probability alignment",
      "text" : "To obtain our first distribution-dependent statement about the curve, observe that F(τ) = 0 for any τ ∈ [0, τ∗], where τ∗ = R fair( f ∗ 0 ; D̄). This simply means that there is no penalty from a fairness constraint when we consider the fairness attained by the Bayes-optimal classifier for the original problem. Intuitively, when D and D̄ are disaligned, we expect this τ∗ to be large, so that there is no effect from virtually any fairness constraint.\nWe can formalise this notion of disalignment. Recall from Equation 16 that η, η̄ are the class-probabilities of the target and sensitive features respectively. With a cost-sensitive risk for Rperf , the Bayes-optimal classifier is f ∗(x) = nη(x) > co. The incurred errors for predicting Ȳ with f ∗ are then\nFPR( f ∗; D̄) = 1 1 − π̄ · EX [(1 − η̄(X)) · nη(X) > co] FNR( f ∗; D̄) = 1 π̄ · E X [η̄(X) · nη(X) < co] .\n(23)\nBoth these terms measure a form of disalignment of η and η̄, specifically looking at the concentration of the latter in regions where the former is above or below the threshold c. If Rfair is parametrised by Φfair, we then find\nτ∗ = Φ fair(FPR( f ; D̄), FNR( f ; D̄), π̄).\n2We stress that the tradeoff measured here is one inherent to the problem, rather than one owing to the technique one uses.\nFor generic Φfair, we can plug in Equation 23 to get an explicit expression for τ∗. In the case of cost-sensitive Φfair, this expression involves a concrete measure of disalignment.\nProposition 8. Pick any distribution Djnt, and cost-sensitive performance and fairness measures with cost parameters c, c̄. Then, F(τ) = 0 for any τ ∈ [0, τ∗], where\nτ∗ = min(∆c, c̄(η̄, η),∆−c, c̄(η̄,−η)) ∆c, c̄(η̄, η) = E\nX\n[ Bc, c̄(η̄(X), η(X)) ] − Iϕ(P̄1, P̄0),\nwhere P̄y = P(X | Ȳ = y), Iϕ(·, ·) denotes an f -divergence,\nϕ(t) .= −((1 − c̄) · π̄ · t) ∧ (c̄ · (1 − π̄)) Bc, c̄(η̄, η) . = |η̄ − c̄ | · n(η̄ − c̄) · (η − c) < 0o. (24)\nUnpacking the above, Equation 24 gives a concrete notion of disalignment between η and η̄ – which measures how much they disagree around the respective thresholds c and c̄ – and shows that when this disalignment is high, the fairness constraint has less of an effect. The additional f -divergence term is an intrinsic statement about D̄: when the class-conditionals of D̄ strongly overlap, i.e. there is limited predictability of the sensitive label from the features, then also the fairness constraint has less of an effect. Finally, the min(·, ·) term arises from using symmetrised fairness.\nProposition 8 specifies how much fairness we can ask for without paying any performance penalty. When there is a penalty, however, how does this depend on Djnt? To understand this, we appeal to Bayes-optimal classifiers to Problem 2, whose closed form reveals that the frontier is determined by a similar notion of disalignment.\nProposition 9. Pick any Djnt, and cost-sensitive performance and fairness measures with parameters c, c̄. Given τ ∈ [0, 1], there is some λ ∈ R and Bayes-optimal randomised classifier f ∗ ∈ Argmin\nf ∈[0,1]X Rfull( f ; D, D̄, c, c̄, λ) with\nF(τ) = E X [(c − η(X)) · ( f ∗(X) − nη(X) > co)] .\nIf further this f ∗ is deterministic i.e. Im( f ∗) ⊆ {0, 1},\nF(τ) = E X\n[ Bλ,c, c̄(η(X), η̄(X)) ] (25)\nwhere Bλ,c, c̄(·, ·) is defined by\nBλ,c, c̄(η, η̄) . = |η − c | · n(η − c) · (η − c − λ · (η̄ − c̄)) < 0o.\nThe function Bλ,c, c̄(·, ·) above has a similar flavour as Equation 24, and also measures the disalignment of η and η̄ in terms of disagreement around respective thresholds c, c̄. However, there is additionally a dependence on λ, which depends in some non-trivial manner on τ. We note that the requirement that f ∗ be deterministic may be dropped, at the expense of an additional term in Equation 25 that depends on the alignment of the non-deterministic component and η.\nWe make two final comments. First, the above result holds for both the MD and DI factor, as it only requires the superlevel sets to coincide with those of a cost-sensitive risk. Second, our notions of (dis)alignment are,\nroughly, analogous to the notion of compatibility functions in semi-supervised learning [Balcan and Blum, 2010], wherein one can guarantee that unlabelled data is useful when there is an alignment of the marginal data distribution with one’s function class.\nTo get an intuitive feel for the disalignment function of Equation 24, we illustrate the frontier for some simple distributions. Consider first the case where X = [−1, 1], and D is such that η(x) = nx > 0o and the marginal over instances is uniform. Suppose also that η̄(x) = nx > to, for t some parameter to be specified. Consider a cost-sensitive performance and fairness measure with c = c̄ = 1/2. We can explicitly compute the frontier here, shown in Figure 1(a) for a range of t. As t increases, η and η̄ grow increasingly dissimilar, and so the fairness constraint does not affect performance as dramatically: this is manifest in the fact that τ∗ increases with t, as predicted by Equation 24. Further, for every t, when there is a tradeoff, it is linear.\nSuppose we instead have η(x) = (1 + exp(−x))−1, and retain the same η̄. Here again, Figure 1(b) shows that as t increases, η and η̄ grow increasingly dissimilar, and we find that τ∗ again increases with t. The impact of changing the shape of η is the effect on the frontier when it is nonzero: as per Equation 25, this depends on the deviation of η from c, and hence the frontier here is nonlinear."
    }, {
      "heading" : "7 Experiments",
      "text" : "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D̄) ≤ τ for fixed τ) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + τ)−1. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with Ȳ denoting whether or not the age of a person is above 25, and fix τ = 0.8. For a number of train-test splits to be specified, we train models to minimise the cost-sensitive logistic loss with parameter c (Equation 20), and evaluate on the test set the disparate impact, as well as the gap ∆( f ) .= CSbal( f ; D̄, c) − (1 − c). Our Lemma 1 indicates that we should find the latter to be positive only when the former is larger than τ = 0.8.\nTo construct our training sets, we make an initial 2:1 train-test split of the full data, treating Ȳ as the label to predict. To obtain models with varying levels of accuracy in predicting Ȳ, we inject symmetric label noise of varying rates into the training (but not the test) set. Figure 6.2 shows that for the resulting models, as per Lemma 1, there is perfect agreement of disparate impact at τ = 0.8 and sign(∆( f )). We next present an experiment analogous to Zafar et al. [2016], where on the same german dataset we learn a classifier that respects a symmetrised MD score constraint, while being accurate for predicting the target variable in the sense of balanced error (BER). We employ the plugin estimator proposed in §5.3, training logistic regression models to predict the target and sensitive variable and then combining them via Equation 17 for some λ ∈ R. On the test set, we compute the BER for the target variable, and the symmetrised MD score for the sensitive variable. We then employ the COV method of Zafar et al. [2016], which uses a surrogate to the MD constraint as discussed in §5.4, with tuning parameter τ ∈ R+ to control the MD score. Varying λ and τ yields tradeoff curves for both methods. Figure 6.2 shows these curves at high fairness value, where we see that our plugin approach is generally competitive with COV, resulting in lower BER at higher\nfairness levels. (See Appendix G for further experiments.) This illustrates that our Bayes-optimal analysis of Problem 2 may be useful in designing fairness-aware classifiers."
    }, {
      "heading" : "8 Conclusion and future work",
      "text" : "We studied the tradeoffs inherent in the problem of learning with a fairness constraint, showing that for costsensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function, and quantifying the degradation in performance by a measure of alignment of the target and sensitive variable.\nThere are several interesting directions for future work. To name a few, we believe it valuable to study Bayes-optimal scorers for ranking measures such as AUC; establish consistency of the plugin estimators of §5; quantify the impact of working with a finite sample; and extend our analysis to the case of multi-category sensitive features."
    }, {
      "heading" : "A Proofs of results in main body",
      "text" : "Proof of Lemma 1. By definition,\nFPR( f ; D̄) 1 − FNR( f ; D̄) ≥ τ ⇐⇒ FPR( f ; D̄) ≥ τ − τ · FNR( f ; D̄) , since FNR( f ; D̄) ≤ 1\n⇐⇒ τ · FNR( f ; D̄) + FPR( f ; D̄) ≥ τ\n⇐⇒ τ 1 + τ · FNR( f ; D̄) + 1 1 + τ · FPR( f ; D̄) ≥ τ 1 + τ ⇐⇒ (1 − c) · FNR( f ; D̄) + c · FPR( f ; D̄) ≥ 1 − c ⇐⇒ CSbal( f ; D̄, c) ≥ 1 − c.\nThe first result follows by definition of disparate impact.\nThe above may be trivially extended to a symmetrised version of the disparate impact (Equation 9). This is since one may equally apply the above to the anti-classifier 1 − f ; further, we have\nCSbal(1 − f ; D, c) = 1 − CSbal( f ; D, c),\nand so DI(1 − f ; D̄) ≥ τ ⇐⇒ CSbal( f ; D, c) ≤ c, implying the second result.\nProof of Lemma 2. By definition,\nMD( f ; D̄) = 1 − FPR( f ; D̄) − FNR( f ; D̄) = 1 − 2 · CSbal ( f ; D̄, 1/2 ) .\nThe subsequent implications follow trivially.\nProof of Proposition 3. By Lemma 10, the performance measure is\nRperf( f ; D, c) = (1 − c) · π + E X [(c − η(X)) · f (X)] .\nSimilarly, the fairness measure is\nRfair( f ; D̄DP, c̄) = (1 − c̄) · π̄ + E X [(c̄ − ηDP(X)) · f (X)] .\nIgnoring constants independent of f , the overall objective is thus\nmin f Rperf( f ; D, c) − λ · Rfair( f ; D̄DP, c̄)\n= min f E X [((c − η(X)) − λ · (c̄ − ηDP(X))) · f (X)] = min f E X [−s∗(x) · f (X)] .\nThus, at optimality, when s∗(x) , 0, f ∗(x) = ns∗(x) > 0o.\nProof of Proposition 4. By Lemma 10, the fairness measure is\nRfair( f ; D̄EO, c̄) = (1 − c̄) · P(Ȳ = 1 | Y = 1) + E X |Y=1 [(c̄ − ηEO(X, 1)) · f (X)]\n= (1 − c̄) · P(Ȳ = 1 | Y = 1) + E X [ η(X) π · (c̄ − ηEO(X, 1)) · f (X) ] ,\nwhere the second line is from applying the importance weighting identity, and the fact that\nP(X | Y = 1) P(X) = P(Y = 1 | X = x) P(Y = 1) = η(x) π .\nEquivalently, for suitable λ, we seek\nmin f E X\n[( c − η(X) − λ · η(X)\nπ · (c̄ − ηEO(X, 1))\n) · f (X) ] = min\nf E X [−s∗(x) · f (X)]\nThus, at optimality, when s∗(x) , 0, f ∗(x) = ns∗(x) > 0o.\nProof of Corollary 5. We simply apply Proposition 3 to x̄ = (x, ȳ). Note that\nηDP(x, ȳ) = P(Ȳ = 1 | X = x, Ȳ = ȳ) = nȳ = 1o.\nThen,\nf ∗(x, ȳ) = 1 ⇐⇒ η(x, ȳ) > c + λ · (ηDP(x, ȳ) − c̄) ⇐⇒ η(x, ȳ) > c + λ · (nȳ = 1o − c̄).\nProof of Lemma 6. By Lemma 10, cost-sensitive risks are linear in the randomised classifier. In particular, for discrete X,\nRperf( f ; D) = (1 − c) · π + E X [(c − η(X)) · f (X)] = (1 − c) · π + ∑ x∈X m(x) · (c − η(x)) · f (x),\nwhere m(x) = P(X = x). Similarly,\nRfair( f ; D̄) = (1 − c̄) · π + E X [(c̄ − η̄(X)) · f (X)] = (1 − c̄) · π + ∑ x∈X m(x) · (c̄ − η̄(x)) · f (x).\nNow let\n(∀x ∈ X) a(x) .= m(x) · (c − η(x)) (∀x ∈ X) b(x) .= m(x) · (c̄ − η̄(x)) .\nThen, the optimisation is\nmin f aT f : − bT f ≤ −τ\nbT f ≤ 1 − τ 0 ≤ f ≤ 1.\nThis is a linear objective with linear constraints. We thus may find the optimal random classifier by the solution to a linear program.\nProof of Lemma 7. We wish to determine whether, for any τ, τ′ ∈ R+ and λ ∈ [0, 1],\nF(λτ + (1 − λ)τ′) ? ≤ λF(τ) + (1 − λ)F(τ′)\n⇐⇒ Rperf( f ∗λτ+(1−λ)τ′) ? ≤ λRperf( f ∗τ ) + (1 − λ)Rperf( f ∗τ′)\n⇐= Rperf( f ∗λτ+(1−λ)τ′) ? ≤ Rperf(λ f ∗τ + (1 − λ) f ∗τ′) for convex Rperf ⇐= λ f ∗τ + (1 − λ) f ∗τ′ feasible for λτ + (1 − λ)τ′\n⇐= Rfair(λ f ∗τ + (1 − λ) f ∗τ′) ? ≥ λτ + (1 − λ)τ′.\nBy definition, f ∗τ , f ∗τ must be feasible for their corresponding problems, and so\nRfair( f ∗τ ) ≥ τ Rfair( f ∗τ′) ≥ τ′.\nWe thus want to determine whether\nRfair(λ f ∗τ + (1 − λ) f ∗τ′) ≥ λτ + (1 − λ)τ′\n⇐= Rfair(λ f ∗τ + (1 − λ) f ∗τ′) ≥ λRfair( f ∗τ ) + (1 − λ)Rfair( f ∗τ′) ⇐= Rfair concave.\nSince Rfair is cost-sensitive, it is linear by Lemma 10, and hence concave. The result thus follows.\nProof of Proposition 8. As argued in the body, τ∗ = R fair( f ∗ 0 ; D̄). Thus, it remains to compute this quantity. First, note that we may pick f ∗0 = nη(x) > co by Lemma 11.3 Next, observe that since Rfair is a cost-sensitive risk,\nRfair( f ; D̄) = Rfair( f ; D̄) −min g Rfair(g; D̄) +min g Rfair( f ; D̄)\n= CS( f ; D̄, c̄) −min g CS(g; D̄, c̄) +min g CS( f ; D̄, c̄) = E X [(c − η̄(X)) · ( f (X) − nη̄(X) > co)] +min g CS( f ; D̄, c̄) by Lemma 12 .\nPlugging in f ∗0 , and applying the second statement of Lemma 12,\nRfair( f ; D̄) = E X [|η̄(X) − c | · n(η̄(X) − c̄) · (η(X) − c) < 0o] +min g CS( f ; D̄, c̄).\nNow we just apply Lemma 13 to the second term.\nProof of Proposition 9. Observe that f ∗0 ∈ Argmin Rperf( f ; D). Consequently, the frontier may be re-written\nF(τ) = reg( f ∗τ ; D)\nwhere the regret or excess risk of a classifier is\nreg( f ; D) = Rperf( f ; D) − min g : X→[0,1] Rperf(g; D).\nThis lets us specify the form of F(·) analytically when Rperf is a cost-sensitive risk. By Lemma 12,\nF(τ) = E X∼M\n[ (c − η(X)) · ( f ∗τ (X) − nη(X) > co) ] .\n3The behaviour of optimal solutions at η(x) = c is unconstrained; however, different choices can in fact lead to different fairness values. Nonetheless, by picking a specific optimal solution, we are nonetheless guaranteed that picking τ∗ as defined will yield zero risk.\nNow, since f ∗τ is the solution to a linear program by Lemma 6, we can appeal to strong duality (see Appendix D) to conclude that there exists some λ for which the corresponding soft-constrained version of the problem (Equation 15) has the same optimal value. This means there is some Bayes-optimal classifier f ∗λ to Equation 15 for which\nF(τ) = E X∼M\n[ (c − η(X)) · ( f ∗λ (X) − nη(X) > co) ] .\nNow, if additionally this f ∗λ is deterministic, then also by Lemma 12,\nF(τ) = E X∼M\n[ |η(X) − c | · n(η(X) − c) · (2 f ∗λ (X) − 1) < 0o ] .\nNow just plug in the definition of f ∗λ from Equations 18, 17."
    }, {
      "heading" : "B Helper results",
      "text" : "Lemma 10. Pick any distribution D and randomised classifier f . Then, for any cost parameter c ∈ [0, 1],\nCS( f ; D, c) = (1 − c) · π + E X [(c − η(X)) · f (X)]\nwhere π = P(Y = 1), η(x) = P(Y = 1 | X = x).\nProof of Lemma 10. By definition,\nCS( f ; D, c) = (1 − c) · π · E X |Y=1 [1 − f (X)] + c · (1 − π) · E X |Y=0 [ f (X)]\n= E X [(1 − c) · η(X) · (1 − f (X)) + c · (1 − η(X)) · f (X)]\n= E X [(1 − c) · η(X)] + E X [(c · (1 − η(X)) − (1 − c) · η(X)) · f (X)]\n= (1 − c) · π + E X [(c · (1 − η(X)) − (1 − c) · η(X)) · f (X)]\n= (1 − c) · π + E X [(c − η(X)) · f (X)] .\nThe second line is since P(X | Y = 1) · P(Y = 1) = P(X) · P(Y = 1 | X).\nLemma 11. Pick any distribution D and cost parameter c ∈ [0, 1]. Let\n(∀x ∈ X) s∗(x) = η(x) − c where η(x) = P(Y = 1 | X = x). Then, any randomised classifier f ∗ satisfying\n(∀x ∈ X) s∗(x) , 0 =⇒ f ∗(x) = ns∗(x) > 0o minimises CS( f ; D, c).\nProof of Lemma 11. By Lemma 10, we need to find, for each x ∈ X\nmin f (x)∈[0,1] (c − η(x)) · f (x) = min f (x)∈[0,1]\n−s∗(x) · f (x),\nobserving that the minimisation may be done pointwise. Clearly, it is optimal to predict f ∗(x) = 1 when s∗(x) > 0, and f ∗(x) = 0 when η(x) < c. When η(x) = c, any prediction is optimal.\nLemma 12. Pick any distribution Djnt and cost parameter c ∈ [0, 1]. Then, for any randomised classifier f ,\nCS( f ; D, c) − min g : X→[0,1] CS(g; D, c) = E X [(c − η(X)) · ( f (X) − nη(X) > co)] .\nIf further f ∈ {0, 1}X,\nCS( f ; D, c) − min g : X→[0,1] CS(g; D, c) = E X [|η(X) − c | · n(η(X) − c) · (2 f (x) − 1) < 0o] .\nProof of Lemma 12. ByLemma 11, an optimal classifier for Rperf(g; D) is the deterministic f ∗(x) = nη(x) > co. Thus, plugging this into Lemma 10,\nCS( f ; D, c) − min g : X→[0,1] CS(g; D, c) = E X [(c − η(X)) · ( f (X) − nη(X) > co)] .\nThe second statement follows from a simple case analysis. The difference f (x) − nη(x) > co takes on the value +1 when f (x) = 1 and η(x) < c, and −1 when f (x) = 0 and η(x) > c, i.e. the value sign(c − η(x)) when 2 f − 1 and η(x) − c disagree in sign. Since |z | = z · sign(z), the result follows.\nLemma 13. Pick any distribution Djnt and cost parameter c ∈ [0, 1]. Then,\nmin f : X→[0,1] CS( f ; D, c) = −Iϕ(P(X | Y = 1), P(X | Y = 0))\nwhere I f (·, ·) denotes the f -divergence between distributions, and\nϕ(t) = −min ((1 − c) · π · t, c · (1 − π)) .\nProof of Lemma 13. This follows from Reid and Williamson [2011, Theorem 9], applied as follows. Let f ∗(x) = nη(x) > co + 12 · nη(x) = co, which is an optimal classifier for CS( f ; D, c) by Lemma 11. Then,\nCS( f ∗; D, c) = E (X,Y)∼D [`(Y, η(X))]\nwhere ` is the cost-sensitive loss given by `(1, v) = (1 − c) · ( nv < co + 1\n2 nv = co ) `(0, v) = c · ( nv > co + 1\n2 nv = co\n) .\nNow, ` is proper in the sense of Reid and Williamson [2010]. Consequently,\nE (X,Y)∼D [`(Y, η(X))] = min η̂ : X→[0,1] E (X,Y)∼D [`(Y, η̂(X))] .\nThe right hand side above is the Bayes-risk for the proper loss ` in the sense of Reid and Williamson [2011]. Consequently, by Reid and Williamson [2011, Theorem 9],\nE (X,Y)∼D\n[`(Y, η(X))] = −Iϕ(P(X | Y = 1), P(X | Y = 0)),\nwhere\nϕ(t) = −min ((1 − c) · π · t, c · (1 − π)) .\nThis may be verified easily, since\nCS( f ∗; D, c) = E X\n[ (1 − c) · η(X) · ( nη(x) < co + 1\n2 · nη(x) = co\n) + c · (1 − η(X)) · ( nη(x) > co + 1\n2 · nη(x) = co )] = E\nX [min((1 − c) · η(X), c · (1 − η(X)))] ,\nwhile, if P = P(X | Y = 1),Q = P(X | Y = 0) with densities p, q,\n−Iϕ(P,Q) = − E X∼Q\n[ ϕ ( p(X) q(X) )] = E\nX∼Q\n[ min ( (1 − c) · π · p(X) q(X), c · (1 − π) )]\n= E X∼M\n[ min ( (1 − c) · π · p(X)\nm(X), c · (1 − π) · q(X) m(X) )] = E\nX∼M [min ((1 − c) · η(X), c · (1 − η(X)))]\n= CS( f ∗; D, c).\nCorollary 14. Pick any distribution Djnt, costs c, c̄ ∈ [0, 1], and λ ∈ R. Let s∗(x, 0) = ( 1 + λ · π−1 · c̄ ) · η(x, 0) − c\ns∗(x, 1) = ( 1 − λ · π−1 · (1 − c̄) ) · η(x, 1) − c\nwhere η(x, ȳ) = P(Y = 1 | X = x, Ȳ = ȳ) Then,\nArgmin f ∈[0,1]X\nR( f ; D, D̄EO, c, c̄, λ) = { f ∗ | (∀x ∈ X) s∗(x) , 0 =⇒ f ∗(x) = ns∗(x) > 0o}. Proof of Corollary 14. Plug in η̄(x, ȳ) = nȳ = 1o into Proposition 4.\nC Symmetrised fairness when π̄ , 1/2\nThe presentation of symmetrised in the body noted that one needs to guard against anti-classifiers. However, the proposal\nR fair( f ; D̄) = Rfair( f ; D̄) ∧ Rfair(1 − f ; D̄)\nis maximised when using the constant classifier f = 1/2. It is preferable for the fairness measure to instead be maximised when f = π̄, which would be the optimal prior prediction before we get a chance to look at the data. In fact, this is easy to achieve by modifying\nR fair( f ; D̄) = ((1 − α) · Rfair( f ; D̄)) ∧ (α · Rfair(1 − f ; D̄))\nwhere α = Rfair(π̄; D̄), so that we asymmetrically penalise raw fairness scores below and above those of Rfair(π̄; D̄). Note that this would simply introduce additional asymmetric scalings into our results, e.g. for bounds relating the symmetrised disparate impact to a cost-sensitive risk (Lemma 1)."
    }, {
      "heading" : "D Relating the constrained and unconstrained objectives",
      "text" : "Consider the constrained version of the fairness problem,\nf ∗ ∈ Argmin f ∈[0,1]X Rperf( f ; D) : R fair( f ; D̄) ≥ τ.\nBy Lemma 6, for finite X, this is expressible as the solution to a linear program\nmin f ∈F aT f\nwhere F = { f | bT f ∈ [τ, 1 − τ], 0 ≤ f (x) ≤ 1}\nand\n(∀x ∈ X) a(x) .= m(x) · (c − η(x)) (∀x ∈ X) b(x) .= m(x) · (c̄ − η̄(x)) .\nNow, by strong duality for linear programs4, we have\nmin f ∈F aT f = max λ1,λ2≥0\n( min\nf ∈[0,1]X (a − λ1b + λ2b)T f\n) + λ1τ − λ2(1 − τ). (26)\nObserve now that the inner optimisation is\nmin f ∈[0,1]X (a − λ1b + λ2b)T f\n= min f ∈[0,1]X (a − (λ1 − λ2)b)T f\n= min f ∈[0,1]X ∑ x∈X m(x) · [c − η(x) − (λ1 − λ2) (c̄ − η̄(x))] · f (x)\n= min f ∈[0,1]X E X [(c − η(X) − (λ1 − λ2) (c̄ − η̄(X))) · f (X)]\n= min f ∈[0,1]X CS( f ; D, c) − (λ1 − λ2) · CS( f ; D̄, c̄).\nThat is, we solve Equation 15 for λ = λ1 − λ2. By sweeping over λ, we can thus in principle find the one which achieves the highest value of the objective in Equation 26, and consequently find the solution to the constrained problem for a fixed τ.\nNote that strong duality guarantees agreement of the objective functions. In general, it does not mean that every optimal solution to the inner problem (for optimal λ1, λ2) will also be optimal for the original constrained problem. As an extreme case, suppose that η̄ = η, and c = c̄. Then, the constrained problem has optimal solution any f for which CS( f ; D̄, c̄) = τ, so that the frontier is linear. On the other hand, we will find that for the optimal λ1, λ2, the inner optimisation is simply of the constant 0, in which case every f is deemed optimal.\n4This implicitly assumes feasibility of the primal problem, i.e. that we pick τ such that it is possible to find a randomised classifier with symmetrised fairness at least τ."
    }, {
      "heading" : "E A survey of fairness in philosophy and welfare economics",
      "text" : "Fairness lies at the heart of justice, which is “the first virtue of social institutions” [Rawls, 1971, p. 586]. But what is fairness? Rawls develops a theory of fairness utilizing the “veil of ignorance” whereby a person’s position in society is held to be unknown while designing the rules of a just society. The analogue in machine learning is that membership of a given category should not be used, one way or another, in determining outcomes for an individual. Analogous theories such as that of [Harsanyi, 1955] and [Sen, 2009] differ in what ignorance means (uniform prior over what role one has, or the perspective of a separate impartial observer). In all cases, the general idea is that a just outcome should not depend (either way) on membership of a particular category, but should focus upon the individual; Rawls [1971] motivates his approach by saying “utilitarianism does not take seriously the distinction between persons” (page 26) because it is only concerned with average welfare. While these ideas have had profound impact on political philosophy, their translation into mathematical theories is lacking. With few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness.\nRawls argues the advantages of “pure procedural justice” where the attainment of justice (hence fairness) is a consequence of the process followed, not the outcome obtained. This is, of course, taken for granted in machine learning and statistics, where one analyses a procedure, not the results of the procedure on a given set of data. Taking this principle seriously means that protected attributes have to be identified ahead of time, and not after the fact because you did not like the outcome; confer the analogous problem in designing electoral districts: getting the process right makes the problem straightforward [Vickrey, 1961]; attempting to judge fairness a posteriori is a mess, with infinite arguments possible about whether a boundary is “bizarre” [Chambers and Miller, 2010].\nThe theory we present here follows the precept of Sen [2009, Chapter 18] that mere identification of “fully just social arrangements is neither necessary nor sufficient.” We embrace Sen’s pragmatism by focussing on the quantifiable tradeoffs one might make to approach (certain notions of) fairness and hence justice. Rawls acknowledges that there will be tradeoffs between overall social utility and fairness (pp37ff) but tries to argue what the “right” tradeoff is. We do not try to solve that question, believing instead there is unlikely to be a universal right tradoff, and instead focus upon merely quantifying what the tradeoffs might be. Focussing upon quantifying the traedoff between utility and fairness has only very recently drawn attention in the machine learning literature [Johnson et al., 2016].\nThe approaches to fairness in the machine learning literature, which we follow in this paper, focusses on the notion of a protected attribute, and assumes that both its choice is manifest, and indeed it is a sensible categorisation (e.g. notions of race). One should take care with this though, because any such categories to which people are assigned to are not intrinsic to the world, but another choice that we make [Lakoff, 1987], which can be highly ambiguous and contested [Bowker and Star, 1999].\nSingling out particular attributes to be protected (as opposed to the rather more sweeping requirements of Rawls’ full theory that requires no specific attributes be taken into account) opens the door to the problems inherent in the “ecological fallacy” [Kramer, 1983] — making inferences about individuals based on membership in a category, which is precisely what some argue one should not do [Lippert-Rasmussen, 2011].\nIndeed the very notions of fairness studied in this paper glaringly fails the test against “discrimination” if one adopts a standard definition (e.g. from Wikipedia):\nIn human social affairs, discrimination is treatment or consideration of, or making a distinction in favour or against a person or thing based on the group, class, or category to which that person or thing is perceived to belong rather than on individual merit.\nBy that definition, the only non-discriminatory approach is to ignore the protected attributes entirely and take the outcome as it comes. Reconciling this tension remains a challenge!"
    }, {
      "heading" : "F Relating disparate impact and balanced error",
      "text" : "Following Feldman et al. [2015], we explore the relationship between the balanced error and disparate impact. Intuitively, we expect that when the balanced error of a classifier is low – meaning that the classifier accurately predicts the sensitive variable – we will have disparate impact. Conversely, we might hope that possessing disparate impact implies a low balanced error. Can we formalise a relationship akin to Lemma 2?\nWe have the following relations between the two quantities. In what follows, let\nBER( f ; D̄) = FPR( f ; D̄) + FNR( f ; D̄) 2 .\nLemma 15. Pick any distribution D̄ and randomised classifier f : X→ [0, 1] with FNR( f ; D̄) , 1. Then,\nDI( f ; D̄) = FPR( f ; D̄) 1 − 2 · BER( f ; D̄) + FPR( f ; D̄)\n= 2 · BER( f ; D̄) − FNR( f ; D̄)\n1 − FNR( f ; D̄) ,\n(27)\nand similarly\nBER( f ; D̄) = 1 2 · FNR( f ; D̄) + 1 2 · (1 − FNR( f ; D̄)) · DI( f ; D̄)\n= 1 2 · FPR( f ; D̄) + 1 2 · ( 1 − FPR( f ; D̄) DI( f ; D̄) ) .\n(28)\nProof of Lemma 15. These are trivial consequences of the fact that, by definition of DI( f ; D̄) (Equation 7),\nFPR( f ; D̄) = (1 − FNR( f ; D̄)) · DI( f ; D̄).\nWe now turn to relating a bound on the balanced error to a bound on the disparate impact factor. The following is a minor generalisation of Feldman et al. [2015, Theorem 4.1] to account for disparate impact at any level.\nLemma 16. Pick any distribution D̄ and randomised classifier f : X→ [0, 1] with FNR( f ; D̄) , 1. Then, for any ∈ [0, 12 ],\nBER( f ; D̄) ≤ ⇐⇒ DI( f ; D̄) ≤ FPR( f ; D̄) 1 − 2 · + FPR( f ; D̄) ∧ 2 · − FNR( f ; D̄) 1 − FNR( f ; D̄) ,\nand for any τ ∈ [0, 1], DI( f ; D̄) ≤ τ ⇐⇒ BER( f ; D̄) ≤ ( τ\n2 + 1 − τ 2 · FNR( f ; D̄)\n) ∧ ( 1 2 − 1 − τ 2 · τ · FPR( f ; D̄) ) .\nProof of Lemma 16. The first equivalence follows from the two expressions in Equation 27, and the fact that the dependence on BER( f ; D̄) is monotone increasing. This second equivalence follows from the two expressions in Equation 28, and the fact that the dependence on DI( f ; D̄) is monotone increasing.\nWhen τ = 0.8, this means that DI( f ; D̄) ≤ 0.8 ⇐⇒ BER( f ; D̄) ≤ (\n2 5 + 1 10 · FNR( f ; D̄)\n) ∧ ( 1 2 − 1 8 · FPR( f ; D̄) ) .\nF.1 Low balanced error implies disparate impact\nIt is of interest to remove the dependence of the above bounds on the false positive and negative rates of f . For one direction, this is possible.\nCorollary 17. Pick any distribution D̄ and randomised classifier f : X→ [0, 1] with FNR( f ; D̄) , 1. Then, for any ∈ [0, 12 ],\nBER( f ; D̄) ≤ =⇒ DI( f ; D̄) ≤ 2 · ,\nor for any τ ∈ [0, 1], DI( f ; D̄) ≥ τ =⇒ BER( f ; D̄) ≥ τ\n2 .\nProof. The first bound follows from Lemma 16 and the fact that if BER( f ; D̄) ≤ , it must be true that FPR( f ; D̄) ∨ FNR( f ; D̄) ≤ 2 · . The second bound is the contrapositive of the first.\nCorollary 17 says that with a balanced error of τ2 or less, we are guaranteed a disparate impact of level at least τ, though possibly worse. So, if we want to guarantee a lack of disparate impact at level τ, it is necessary that the balanced error be at least τ2 . But is this condition also sufficient? Unfortunately, it is not.\nF.2 Disparate impact does not imply low balanced error\nIt is evident from Lemma 16 that regardless of the precise level of impact τ, we could have a classifier with balanced error arbitrarily close to 12 . The basic issue is that by driving the false positive rate to 0, we trivially have disparate impact. By further driving the false negative rate to 0 (i.e. by predicting everything negative), we trivially have a balanced error rate of 12 .\nCorollary 18. Pick any distribution D̄. Then, for any τ ∈ [0, 1] there exists a classifier f : X→ {0, 1} with\nBER( f ; D̄) = 1 2\nDI( f ; D̄) ≤ τ.\nProof. Consider the trivial classifier with FPR( f ; D̄) = 0\nFNR( f ; D̄) = 1.\nClearly, this has balanced error 12 . Evidently, this classifier also has disparate impact at level τ.\nCorollary 18 says that even if we have a classifier with high balanced error, there is no guarantee it will not have disparate impact. This is a worst case analysis over all possible classifiers we might have obtained. However, if we happen to know the false positive and negative rates we actually have obtained, we might be able to conclude there is no disparate impact. This is used in Feldman et al. [2015, Section 4.2] to certify the lack of disparate impact for a particular classifier.\nCorollary 19. Pick any distribution D̄ and randomised classifier f : X→ [0, 1]. For any τ ∈ [0, 1], BER( f ; D̄) ≥ ( τ\n2 + 1 − τ 2 · FNR( f ; D̄)\n) ∧ ( 1 2 − 1 − τ 2 · τ · FPR( f ; D̄) ) ⇐⇒ DI( f ; D̄) ≥ τ.\nProof of Corollary 19. This is the contrapositive of Lemma 16."
    }, {
      "heading" : "G Additional experiments",
      "text" : "We present a further experiment on the synthetic dataset considered in Zafar et al. [2016], where P(Y = 1) = 0.5, each X | Y = y ∼ N(µy, Σy) where\nµ1 = [ 2 2 ] Σ1 = [ 5 1 1 5\n] µ0 = [ 10 1 1 3\n] Σ0 = [ −2 −2 ] ,\nand P(Ȳ = 1 | X = x) = P(X = Rx | Y = 1)\nP(X = Rx | Y = 1) + P(X = Rx | Y = −1) for rotation matrix R = [ cos φ − sin φ sin φ cos φ ] . We pick φ = 0.5.\nWe generated N = 104 samples from this distribution, and followed the same setup as the body: we construct a 2:1 train-test split, and compare COV and our plugin (2LR) approach in terms of the balanced error of predicting Y, versus the MD score in predicting Ȳ.\nFigure 4(a) shows the tradeoff curves of the methods closely track each other. However, the plugin approach performs slightly worse at higher fairness levels. We conjecture this is due to the fact that logistic regression is not suitable for η, η̄, as the class-conditionals have non-isotropic covariance and thus possess quadratic boundaries. When we explicitly include quadratic features as input to both methods, Figure 4(b) shows that the plugin approach performs slightly better than COV."
    } ],
    "references" : [ {
      "title" : "A discriminative model for semi-supervised learning",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Balcan and Blum.,? \\Q2010\\E",
      "shortCiteRegEx" : "Balcan and Blum.",
      "year" : 2010
    }, {
      "title" : "Game Theory and the Social Contract Volume 1: Playing Fair",
      "author" : [ "Ken Bimore" ],
      "venue" : null,
      "citeRegEx" : "Bimore.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bimore.",
      "year" : 1994
    }, {
      "title" : "Natural Justice",
      "author" : [ "Ken Binmore" ],
      "venue" : null,
      "citeRegEx" : "Binmore.,? \\Q2005\\E",
      "shortCiteRegEx" : "Binmore.",
      "year" : 2005
    }, {
      "title" : "Sorting Things Out: Classification and its Consequences",
      "author" : [ "Geoffrey C. Bowker", "Susan Leigh Star" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Bowker and Star.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bowker and Star.",
      "year" : 1999
    }, {
      "title" : "Three Naive Bayes approaches for discrimination-free classification",
      "author" : [ "Toon Calders", "Sicco Verwer" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Calders and Verwer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Calders and Verwer.",
      "year" : 2010
    }, {
      "title" : "A measure of bizarreness",
      "author" : [ "Christopher P. Chambers", "Alan D. Miller" ],
      "venue" : "Quarterly Journal of Political Science,",
      "citeRegEx" : "Chambers and Miller.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chambers and Miller.",
      "year" : 2010
    }, {
      "title" : "Algorithmic decision making and the cost of fairness",
      "author" : [ "Sam Corbett-Davies", "Emma Pierson", "Avi Feller", "Sharad Goel", "Aziz Huq" ],
      "venue" : "CoRR, abs/1701.08230,",
      "citeRegEx" : "Corbett.Davies et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Corbett.Davies et al\\.",
      "year" : 2017
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel" ],
      "venue" : "In Innovations in Theoretical Computer Science Conference (ITCS),",
      "citeRegEx" : "Dwork et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2012
    }, {
      "title" : "The foundations of cost-sensitive learning",
      "author" : [ "Charles Elkan" ],
      "venue" : "In International joint conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Elkan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Elkan.",
      "year" : 2001
    }, {
      "title" : "Certifying and removing disparate impact",
      "author" : [ "Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Feldman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2015
    }, {
      "title" : "Prediction with model-based neutrality",
      "author" : [ "Kazuto Fukuchi", "Jun Sakuma", "Toshihiro Kamishima" ],
      "venue" : "In European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),",
      "citeRegEx" : "Fukuchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fukuchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Linear semi-infinite optimization, volume 2 of Wiley Series in Mathematical Methods in Practice",
      "author" : [ "Miguel A. Goberna", "Marco A. Lopéz" ],
      "venue" : null,
      "citeRegEx" : "Goberna and Lopéz.,? \\Q1998\\E",
      "shortCiteRegEx" : "Goberna and Lopéz.",
      "year" : 1998
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nathan Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Hardt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2016
    }, {
      "title" : "Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility",
      "author" : [ "John C. Harsanyi" ],
      "venue" : "The Journal of Political Economy,",
      "citeRegEx" : "Harsanyi.,? \\Q1955\\E",
      "shortCiteRegEx" : "Harsanyi.",
      "year" : 1955
    }, {
      "title" : "Impartial predictive modeling: Ensuring fairness in arbitrary models",
      "author" : [ "Kory D. Johnson", "Dean P. Foster", "Robert A. Stine" ],
      "venue" : null,
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Classification without discrimination",
      "author" : [ "Faisal Kamiran", "Toon Calders" ],
      "venue" : "In IEEE International Conference on Computer, Control and Communication",
      "citeRegEx" : "Kamiran and Calders.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kamiran and Calders.",
      "year" : 2009
    }, {
      "title" : "Fairness-aware classifier with prejudice remover regularizer",
      "author" : [ "Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma" ],
      "venue" : "In European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),",
      "citeRegEx" : "Kamishima et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kamishima et al\\.",
      "year" : 2012
    }, {
      "title" : "The ecological fallacy revisited: Aggregate- versus individual-level findings on economics and elections, and sociotropic voting",
      "author" : [ "Gerald H. Kramer" ],
      "venue" : "The American Political Science Review,",
      "citeRegEx" : "Kramer.,? \\Q1983\\E",
      "shortCiteRegEx" : "Kramer.",
      "year" : 1983
    }, {
      "title" : "Women, Fire, and Dangerous Things: What Categories Reveal about the Mind",
      "author" : [ "George Lakoff" ],
      "venue" : null,
      "citeRegEx" : "Lakoff.,? \\Q1987\\E",
      "shortCiteRegEx" : "Lakoff.",
      "year" : 1987
    }, {
      "title" : "We are all different”: Statistical discrimination and the right to be treated as an individual",
      "author" : [ "Kasper Lippert-Rasmussen" ],
      "venue" : "The Journal of ethics,",
      "citeRegEx" : "Lippert.Rasmussen.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lippert.Rasmussen.",
      "year" : 2011
    }, {
      "title" : "The variational fair autoencoder",
      "author" : [ "Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard S. Zemel" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Louizos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Louizos et al\\.",
      "year" : 2016
    }, {
      "title" : "On the statistical consistency of plug-in classifiers for non-decomposable performance measures",
      "author" : [ "Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimizing non-decomposable performance measures: A tale of two classes",
      "author" : [ "Harikrishna Narasimhan", "Purushottam Kar", "Prateek Jain" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimizing F-measures by cost-sensitive classification",
      "author" : [ "Shameem Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Parambath et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Parambath et al\\.",
      "year" : 2014
    }, {
      "title" : "Discrimination-aware data mining",
      "author" : [ "Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Pedreshi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pedreshi et al\\.",
      "year" : 2008
    }, {
      "title" : "A Theory of Justice",
      "author" : [ "John Rawls" ],
      "venue" : null,
      "citeRegEx" : "Rawls.,? \\Q1971\\E",
      "shortCiteRegEx" : "Rawls.",
      "year" : 1971
    }, {
      "title" : "Composite binary losses",
      "author" : [ "Mark D. Reid", "Robert C. Williamson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Reid and Williamson.,? \\Q2010\\E",
      "shortCiteRegEx" : "Reid and Williamson.",
      "year" : 2010
    }, {
      "title" : "Information, divergence and risk for binary experiments",
      "author" : [ "Mark D Reid", "Robert C Williamson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Reid and Williamson.,? \\Q2011\\E",
      "shortCiteRegEx" : "Reid and Williamson.",
      "year" : 2011
    }, {
      "title" : "Calibrated asymmetric surrogate losses",
      "author" : [ "Clayton Scott" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Scott.,? \\Q2012\\E",
      "shortCiteRegEx" : "Scott.",
      "year" : 2012
    }, {
      "title" : "The Idea of Justice",
      "author" : [ "Amartya K. Sen" ],
      "venue" : null,
      "citeRegEx" : "Sen.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sen.",
      "year" : 2009
    }, {
      "title" : "On the prevention of gerrymandering",
      "author" : [ "William Vickrey" ],
      "venue" : "Political Science Quarterly,",
      "citeRegEx" : "Vickrey.,? \\Q1961\\E",
      "shortCiteRegEx" : "Vickrey.",
      "year" : 1961
    }, {
      "title" : "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
      "author" : [ "Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna Gummadi" ],
      "venue" : "In International World Wide Web Conference (WWW),",
      "citeRegEx" : "Zafar et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zafar et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning fair classifiers",
      "author" : [ "Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna Gummadi" ],
      "venue" : "arXiv preprint arXiv:1507.05259,",
      "citeRegEx" : "Zafar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zafar et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning fair representations",
      "author" : [ "Richard Zemel", "Yu Wu", "Kevin Swersky", "Toniann Pitassi", "Cynthia Dwork" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zemel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zemel et al\\.",
      "year" : 2013
    }, {
      "title" : "The theory we present here follows the precept of Sen [2009, Chapter 18] that mere identification of “fully just social arrangements is neither necessary nor sufficient.” We embrace Sen’s pragmatism by focussing on the quantifiable tradeoffs one might make to approach (certain notions of) fairness and hence justice",
      "author" : [ "Miller" ],
      "venue" : null,
      "citeRegEx" : "Miller and 2010..,? \\Q2009\\E",
      "shortCiteRegEx" : "Miller and 2010..",
      "year" : 2009
    }, {
      "title" : "Relating disparate impact and balanced error",
      "author" : [ "Feldman" ],
      "venue" : null,
      "citeRegEx" : "Feldman,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman",
      "year" : 2015
    }, {
      "title" : "FPR( f ; D̄) = (1 − FNR( f ; D̄)",
      "author" : [ "Feldman" ],
      "venue" : null,
      "citeRegEx" : "Feldman,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman",
      "year" : 2015
    }, {
      "title" : "says that even if we have a classifier with high balanced error, there is no guarantee it will not have disparate impact. This is a worst case analysis over all possible classifiers we might have obtained. However, if we happen to know the false positive and negative rates",
      "author" : [ "Feldman" ],
      "venue" : null,
      "citeRegEx" : "Feldman,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "A randomised classifier predicts any x ∈ X to be positive with probability f (x); the quality of any such classifier is assessed by a statistical risk R(·; D) : [0, 1] → R+ which, for some Φ : [0, 1]3 → R+, is [Narasimhan et al., 2014] R( f ; D) .",
      "startOffset" : 210,
      "endOffset" : 235
    }, {
      "referenceID" : 21,
      "context" : "For a broad class of Φ, the optimal classifier is a (possibly distribution dependent) thresholding of the class-probability function, f ∗(x) = nη(x) > t∗(D)o [Narasimhan et al., 2014], where η(x) .",
      "startOffset" : 158,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "For the cost-sensitive error with parameter c, the Bayes-optimal classifier is f ∗(x) = nη(x) > co [Elkan, 2001].",
      "startOffset" : 99,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "These Bayes-optimal classifiers motivate a plugin estimator, where one thresholds an empirical estimate of η [Narasimhan et al., 2014].",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : ") The first is demographic parity [Calders and Verwer, 2010], which requires the predictions to be independent of the sensitive feature: P(Ŷ = 1 | Ȳ = 0) = P(Ŷ = 1 | Ȳ = 1).",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "(3) The second is equality of opportunity [Hardt et al., 2016], which requires the predictions to be independent of the sensitive feature, but only for the positive instances: P(Ŷ = 1 | Y = 1, Ȳ = 0) = P(Ŷ = 1 | Y = 1, Ȳ = 1).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "Other notions of perfect fairness include equalised odds [Hardt et al., 2016], and lack of disparate mistreatment [Zafar et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 31,
      "context" : ", 2016], and lack of disparate mistreatment [Zafar et al., 2017].",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "The first is the disparate impact (DI) factor [Feldman et al., 2015], which is the ratio of the probabilities appearing in the definition of demographic parity:",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "The second is the mean difference (MD) score [Calders and Verwer, 2010], which replaces the ratio with a difference: MD( f ) .",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "Avoiding the use of the sensitive feature by itself does not guard against discrimination [Pedreshi et al., 2008].",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "The second is the mean difference (MD) score [Calders and Verwer, 2010], which replaces the ratio with a difference: MD( f ) . = P(Ŷ = 1 | Ȳ = 1) − P(Ŷ = 1 | Ȳ = 0). (5) We refer the reader to Žliobaitė [2015] for a survey of other fairness measures, including variants of the above.",
      "startOffset" : 46,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances.",
      "startOffset" : 133,
      "endOffset" : 338
    }, {
      "referenceID" : 1,
      "context" : "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances. Zemel et al. [2013], Louizos et al.",
      "startOffset" : 133,
      "endOffset" : 437
    }, {
      "referenceID" : 1,
      "context" : "Fairness has received considerable study in philosophy and welfare economics [Rawls, 1971, Sen, 2009]; however, with few exceptions [Bimore, 1994, Binmore, 2005], there is little formal utilitarian literature that grapples with fairness. See Appendix E for a more detailed overview. In the machine learning community, Dwork et al. [2012] proposed an approach to guarantee fairness relying on a metric over instances. Zemel et al. [2013], Louizos et al. [2016] proposed approaches to learn feature representations that guarantee fairness.",
      "startOffset" : 133,
      "endOffset" : 460
    }, {
      "referenceID" : 4,
      "context" : "Calders and Verwer [2010], Feldman et al.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Calders and Verwer [2010], Feldman et al. [2015] implicitly assuming that learned classifiers will perform better than random guessing on D̄DP.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "Feldman et al. [2015] related the disparate impact to the balanced error, but their bound depends on the distribution and classifier, while ours uses a cost-sensitive risk with constant τ; see §7 and Appendix F.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "Computing the Bayes-optimal classifiers as above is not without precedent: Hardt et al. [2016], Corbett-Davies et al.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (§6), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem.",
      "startOffset" : 8,
      "endOffset" : 439
    }, {
      "referenceID" : 5,
      "context" : "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (§6), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem. They considered a slightly different constrained version of the problem, where one forces the solution to have perfect rather than approximate fairness. Our Propositions 3 and 4 provide an explicit form for the correction when approximate fairness is desired, as well as when the sensitive feature is available or not during training. Recently, Woodworth et al. Woodworth et al. [2017] established limits on the post-processing approach of Hardt et al.",
      "startOffset" : 8,
      "endOffset" : 994
    }, {
      "referenceID" : 5,
      "context" : "[2016], Corbett-Davies et al. [2017] considered the same question, but in the case of exact fairness measures. We are not aware of prior work on computing the optimal classifiers for approximate fairness measures. While the results have a similar flavour to the exact fairness case, explicating them is important to understand the full tradeoff between accuracy and fairness (§6), and also suggests a simple algorithm. Hardt et al. [2016] proposed to construct a fairness-aware classifier in the equality of opportunity setting by post-processing the results of a classifier trained on the original problem. They considered a slightly different constrained version of the problem, where one forces the solution to have perfect rather than approximate fairness. Our Propositions 3 and 4 provide an explicit form for the correction when approximate fairness is desired, as well as when the sensitive feature is available or not during training. Recently, Woodworth et al. Woodworth et al. [2017] established limits on the post-processing approach of Hardt et al. [2016]; studying this in our context of approximate fairness measures would be of interest.",
      "startOffset" : 8,
      "endOffset" : 1068
    }, {
      "referenceID" : 4,
      "context" : "Calders and Verwer [2010] proposed to modify the output of naïve Bayes so as to minimise the MD score.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 28,
      "context" : "for the surrogate cost-sensitive risk [Scott, 2012], CS(s; D, c, `) .",
      "startOffset" : 38,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "(For infinite X we obtain a semi-infinite linear program [Goberna and Lopéz, 1998], whose duality is subtler to analyse.",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "roughly, analogous to the notion of compatibility functions in semi-supervised learning [Balcan and Blum, 2010], wherein one can guarantee that unlabelled data is useful when there is an alignment of the marginal data distribution with one’s function class.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D̄) ≤ τ for fixed τ) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + τ)−1. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with Ȳ denoting whether or not the age of a person is above 25, and fix τ = 0.",
      "startOffset" : 37,
      "endOffset" : 453
    }, {
      "referenceID" : 9,
      "context" : "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D̄) ≤ τ for fixed τ) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + τ)−1. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with Ȳ denoting whether or not the age of a person is above 25, and fix τ = 0.8. For a number of train-test splits to be specified, we train models to minimise the cost-sensitive logistic loss with parameter c (Equation 20), and evaluate on the test set the disparate impact, as well as the gap ∆( f ) . = CSbal( f ; D̄, c) − (1 − c). Our Lemma 1 indicates that we should find the latter to be positive only when the former is larger than τ = 0.8. To construct our training sets, we make an initial 2:1 train-test split of the full data, treating Ȳ as the label to predict. To obtain models with varying levels of accuracy in predicting Ȳ, we inject symmetric label noise of varying rates into the training (but not the test) set. Figure 6.2 shows that for the resulting models, as per Lemma 1, there is perfect agreement of disparate impact at τ = 0.8 and sign(∆( f )). We next present an experiment analogous to Zafar et al. [2016], where on the same german dataset we learn a classifier that respects a symmetrised MD score constraint, while being accurate for predicting the target variable in the sense of balanced error (BER).",
      "startOffset" : 37,
      "endOffset" : 1423
    }, {
      "referenceID" : 9,
      "context" : "We present an experiment inspired by Feldman et al. [2015], who aimed to certify whether a dataset admits disparate impact (i.e. one can achieve DI( f ; D̄) ≤ τ for fixed τ) by testing if the minimal achievable balanced error is below some threshold (see Appendix F). Rather than employ the balanced error, we follow Lemma 1 and assess the minimal achievable balanced cost-sensitive risk for c = (1 + τ)−1. Specifically, following Feldman et al. [2015], we consider the UCI german dataset with Ȳ denoting whether or not the age of a person is above 25, and fix τ = 0.8. For a number of train-test splits to be specified, we train models to minimise the cost-sensitive logistic loss with parameter c (Equation 20), and evaluate on the test set the disparate impact, as well as the gap ∆( f ) . = CSbal( f ; D̄, c) − (1 − c). Our Lemma 1 indicates that we should find the latter to be positive only when the former is larger than τ = 0.8. To construct our training sets, we make an initial 2:1 train-test split of the full data, treating Ȳ as the label to predict. To obtain models with varying levels of accuracy in predicting Ȳ, we inject symmetric label noise of varying rates into the training (but not the test) set. Figure 6.2 shows that for the resulting models, as per Lemma 1, there is perfect agreement of disparate impact at τ = 0.8 and sign(∆( f )). We next present an experiment analogous to Zafar et al. [2016], where on the same german dataset we learn a classifier that respects a symmetrised MD score constraint, while being accurate for predicting the target variable in the sense of balanced error (BER). We employ the plugin estimator proposed in §5.3, training logistic regression models to predict the target and sensitive variable and then combining them via Equation 17 for some λ ∈ R. On the test set, we compute the BER for the target variable, and the symmetrised MD score for the sensitive variable. We then employ the COV method of Zafar et al. [2016], which uses a surrogate to the MD constraint as discussed in §5.",
      "startOffset" : 37,
      "endOffset" : 1979
    } ],
    "year" : 2017,
    "abstractText" : "We study the problem of learning classifiers with a fairness constraint, with three main contributions towards the goal of quantifying the problem’s inherent tradeoffs. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for cost-sensitive classification and fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we show how the tradeoff between accuracy and fairness is determined by the alignment between the class-probabilities for the target and sensitive features. Underpinning our analysis is a general framework that casts the problem of learning with a fairness requirement as one of minimising the difference of two statistical risks.",
    "creator" : "LaTeX with hyperref package"
  }
}