{
  "name" : "1412.2485.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accurate Streaming Support Vector Machines",
    "authors" : [ "Vikram Nathan", "Sharath Raghvendra" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Accurate Streaming Support Vector Machines\nVikram Nathan Sharath Raghvendra"
    }, {
      "heading" : "1 Abstract",
      "text" : "A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the “maximum margin” linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation."
    }, {
      "heading" : "2 Introduction",
      "text" : "Learning and classification with a large amount of data raises the need for algorithms that scale well in time and space usage with the number of data points being trained on. Streaming algorithms have properties that do just that: they run in a single pass over the data and use space polylogarithmic in the total number of points. The technique of making a single pass over the data has three key advantages: 1) points may be seen once and then discarded so they do not take up additional storage space; 2) the running time scales linearly in the size of the input, a practical necessity for data sets with sizes in the millions, and 3) it enables these algorithms to function in a streaming model, where instead of data is not immediately available, individual data points may arrive slowly over time. This third feature enables data to be learned and models to be updated ”online” in real time, instead of periodically running a batch update over all existing data.\nSupport Vector Machines (SVMs) are one such learning model that would benefit from an efficient and accurate streaming representation. Standard 2-class SVMs models attempt to find the maximum-margin linear separator (i.e. hyperplane) between positive and negative instances and as such, they have a very small hypothesis complexity but provable generalization bounds [8]. There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3]. The connection between these two problems has made it possible to harness the work done on streaming MEBs and apply them to SVMs, as was done in [6]. In this paper, we utilize the Blurred Ball cover approximation to the MEB problem proposed in [5] to obtain a streaming SVM that is both fast and space efficient. We also show that our implementation not only outperforms existing streaming SVM implementations (including those not based off of MEB reductions) but also that our error rates are competitive with LibSVM, a state-of-the-art batch SVM open-source project available [here]. ar X\niv :1\n41 2.\n24 85\nv1 [\ncs .L\nG ]\n8 D\nec 2\n01 4"
    }, {
      "heading" : "3 Background",
      "text" : "The Core Vector Machine (CVM) was introduced by [3] as an new take on the standard Support Vector Machine (SVM). Instead of attempting to solve a quadratic system, the CVM makes use of the observation that many common kernels for the standard SVM can be viewed as Minimum Enclosing Ball (MEB) problems. Consider the following SVM optimization problem on m inputs (xi,yi):\nmin w,b,ρ,ξi\n‖w‖2 +b2−2ρ +C n\n∑ i=1 ξ 2i (3.1)\ns.t. yi(w ·ϕ(xi)+b)≥ ρ−ξi i = 1, . . . ,n\nwhere xi and yi are the data points and labels, respectively, and φ is a feature map induced by the kernel of choice. Here, the ξi are error cushions that specify the cost of misclassifying xi. Let w∗ be the optimal separating hyperplane. [3] showed that if the kernel K(x,y) = ϕ(x) ·ϕ(y) satisfies K(x,x) = κ , a constant, then w∗ can be found by finding the minimum enclosing ball of the points {ϕ(xi,yi)}, where\nϕ i(xi,yi) =  yiϕ(xi)yi 1√ C ei  where ei is the ith standard basis element (all zeroes except for the ith position, which is 1). If c∗ is the optimal MEB center, then w∗ = c∗ · (e1 + . . .+ em).\nA couple of things to note about this equivalence: first, it transforms the supervised SVM training problem into an unsupervised one - the MEB is blind to the true label of the point. Second, the notion of a margin in the original SVM problem is transformed into a core set, a subset of inputs such that finding the MEB of the corset is a good approximation to finding the MEB over the entire input. As such, core sets can be thought of as the minimal amount of information that defines the MEB. The implementation of the CVM in [3] follows the MEB approximation algorithm described in [4]: given ε > 0, add to the core set C the point z that is the farthest from the current MEB center c. Recompute the MEB from the core set and continue until all points are within (1+ ε)R from c, where R is the radius of the MEB.\nThe core vector machine achieves a 1 + ε approximation factor but makes |C| passes over the data and requires storage space linear in the size of the input, an approach that doesn’t scale for streaming applications. To this end, [?] presented the StreamSVM, a streaming version of the CVM, which computes the MEB over the input {ϕ i} in a streaming fashion, keeping a running approximate MEB of all the data seen so far and adjusting it upon receiving a new input point. The StreamSVM used only constant space and using a small lookahead resulted in a favorable performance compared to libSVM (batch) as well as the streaming Perceptron, Pegasos, and LASVM implementations. However, the streaming MEB algorithm that powers StreamSVM is only approximate and offers a worst-case approximation ratio of between 1+ √ 2\n2 and 3 2 of the true MEB, leaving open the possibility of a better streaming algorithm to improve the performance of StreamSVM.\nIn this paper, we present the Blurred Ball SVM, a streaming algorithm based on the blurred ball cover proposed in [5]. It takes a parameter ε > 0 and keeps track of multiple core sets (and their corresponding MEBs), performing updates only when an incoming data point lies outside the union of the 1+ε expansion of all the maintained MEBs. The Blurred Ball SVM also makes a single pass over the input, with space complexity independent of n."
    }, {
      "heading" : "4 Algorithm",
      "text" : "The algorithm consists of two parts: a training procedure to update the blurred ball cover and a classification method, which harnesses the blurred ball to label the point with one of the two possible classes. For simplicity, we choose the classes to be ±1.\nAs described above, a ball with radius r and center c is a linear classifier consisting of a hyperplane passing through the origin with normal c. For the rest of this paper, we will require the following assumptions, established by Tsang et al:\n• The data points φ(xi,yi), denoted by D, are linearly separable (this is always the case if C < ∞).\n• |φ(xi,yi)|= κ , a constant.\nWith these assumptions, the training procedure is described in Algorithm 1 and is identical to the blurred ball cover update described in Algorithm 1 of [?].\nAlgorithm 1 Outline of training procedure for the Blurred Ball SVM 1: function TRAIN(xi,yi,L) 2: cores← [] 3: Compute x′ = ϕ(xi,yi) (normalized to norm κ) as defined above 4: Add x′ to the lookahead buffer buf 5: if buf < L then return 6: end if 7: if ∃x′ ∈ B s.t. x′ is not in the 1+ ε expansion of any MEB in cores then 8: c,B← new core set, MEB of {B} ∪ cores 9: Discard any core set with MEB radius smaller than r(B) · ε/4 10: cores← cores ∪ (c,B) 11: end if 12: buf ←{} 13: end function\nFor the purposes of analysis, we show the following properties of the linear classifier that results from the blurred balls:\nLemma 1. A ball B with center c and radius r corresponds to a linear classifier with hyperplane h having the following properties:\n1. |c|> 0 and r < κ . 2. Its margin has size √ κ2− r2.\n3. A point p lies inside B iff (p− c) · c≥ 0, with equality for support vectors, which lie on ∂B.\nProof. First, note that |c|> 0. Suppose instead that |c|= 0. Then we use the following property of a MEB: any half-space H such that c ∈ ∂H contains at least one data point used to construct the MEB. Suppose that r = κ , i.e. |c| = 0. Then this property shows that there is no hyperplane passing through the origin that contains all points entirely on one side. Now, assume that the data points are separable and let h be the normal of the hyperplane that separates the raw data points xi such that xi ·h > 0 for positively labeled\npoints and < 0 for negatively classified points. Then, φ i · h = yixi · h > 0 for all i, a contradiction to the above property if |c|= 0. We can thus assume that |c|> 0 and r < κ for a linearly separable dataset.\nThe reduction described in Section 3 shows that the linear separator defined by a MEB with center c and radius r is a hyperplane with normal parallel to c. Let d be the point farthest from the origin such that (p− d) · d ≥ 0 for all data points p. In other words, the maximum margin is d and c‖d from the reduction. We can further conclude that c = d as follows: let S = {p|(p− c) · c = 0} denote the support vectors, those that lie on the margin and are a distance ‖d‖ from the maximum-margin hyperplane. The ball B(d,‖s−d‖) for s ∈ S includes all data points (it intersects B(0,κ) along the margin). If c 6= d, then argmaxp∈D‖p− c‖2 = ‖p− d‖2 + ‖d− c‖2 > ‖s− d‖ and c is thus not the center of the MEB (since d is strictly better). So c = d and the MEB has radius ‖s− c‖ = |c|2 +κ2 for s ∈ S. Therefore, the margin is ‖c‖= √ κ2− r2.\nSince B∗ = B(c,‖s− c‖) intersects B(0,κ) only along the hyperplane that defines the margin, S ⊂ ∂B∗, and D\\S⊂ B∗ \\∂B∗.\nSince we have multiple linear separators, we have the ability to combine them in a non-linear fashion to classify a new point.\nDefinition 1. Define the support of a point p to be Sup(p) = {B ∈ cores|p ∈ B}, the cores in the blurred ball cover that contain p.\nDefinition 2. Define the score of a point p to be:\nS(p) = ∑ B∈Sup(p) p · cB ‖cB‖ ,\nthe sum of the distances of p to the separator of all the classifiers containing p.\nNote that Sup(p)∩Sup(−p) = /0, since each ball has r < κ .\nAlgorithm 2 Example classification procedure. 1: function CLASSIFY WITH MAJORITY(xi) 2: return H(p) = sgn [S(p)−S(−p)] 3: end function"
    }, {
      "heading" : "5 Results",
      "text" : "We ran the Blurred Ball SVM on several canonical datasets and compared the accuracy of each run with the batch LibSVM implementation, the Stream SVM proposed by Subramanian, and the streaming setting of the Perceptron (which runs through the data only once but is otherwise identical to the perceptron training algorithm). Table 5 shows the experimental results. All trials were averaged over 20 runs with respect to random orderings of the input data stream. The Perceptron, LASVM and Stream SVM data were taken from the experiments documented in [6]. The Blurred Ball SVM on the MNIST dataset was run with ε = 0.001 and C = ∞, and on the IJCNN dataset was run with ε = 10−6 and C = 105. The choice of ε and C was determined coarsely through experimentation. We offer two versions of the Blurred Ball SVM - using lookahead buffer sizes of L = 0 and L = 10. Figures 1 and 2 compare performance of different lookaheads\nas ε is varied. All experiments were run on a Macintosh laptop with a 1.7 GHz processor with 4 GB 1600 MHz standard flash memory.\nIt’s clear that the Blurred Ball SVM outperforms other streaming SVMs, but even more surprising is that it also manages to outperform the batch implementation on the MNIST dataset. We suspect that this is due to the fact that our classifier allows for non-convex separators."
    }, {
      "heading" : "6 Further Work",
      "text" : "Being able to learn an SVM model in an online setting opens up myriad possibilities in the analysis of large amounts of data. There are several open questions whose answers may shed light on a streaming approach with higher accuracy than the Blurred Ball SVM presented here:\n1. Is there a streaming algorithm for maintaining an MEB with better guarantees than the Blurred Ball cover proposed by [5]? The paper originally provided a bound of 1+ √ 3\n2 ≈ 1.3661, which was improved by [7] to less than 1.22. Although [5] showed that it is impossible to achieve an arbitrarily small approximation factor, with 1+ ε for any ε > 0, it’s possible that a better streaming MEB algorithm exists with provable bounds better than the 1.22 factor demonstrated by [7].\n2. The structure of the points in this SVM setup is unique: all data points lie on a sphere of radius κ centered at the origin. Although there is no streaming MEB algorithm for unrestricted points, does this specific structure lend itself to a 1+ ε MEB approximation? If so, we would be able to construct an SVM with separator arbitrarily close to the optimal."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have presented a streaming, or “online” algorithm for SVM learning by making use of a reduction from the Minimum Enclosing Ball problem. Our training algorithm is tunable using the ε parameter to adjust the desired approximation ratio. We also came up with multiple types of classifiers, some of them\nnon-convex, and showed that our implementation surpassed the accuracy of other streaming implementations. One surprising finding is that our implementation surpasses the standard libSVM dataset on canonical MNIST binary digit classification datasets. Tests on other digit recognition datasets show similar results, suggesting that this better performance could be due to structural idiosyncrasies of the data."
    } ],
    "references" : [ {
      "title" : "Fast kernel classifiers with online and active learning",
      "author" : [ "A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou" ],
      "venue" : "Journal of Machine Learning Research Vol",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "The perceptron: a probabilistic model for information storage and organization in the brain. Neurocomputing: Foundations of Research",
      "author" : [ "F. Rosenblatt" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1988
    }, {
      "title" : "Core vector machines: Fast SVM training on very large data sets",
      "author" : [ "I.W. Tsang", "J.T. Kwok", "P-M Cheung" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Optimal core-sets for balls",
      "author" : [ "M. Bǎdoiu", "K.L. Clarkson" ],
      "venue" : "In Proceedings of DIMACS Workshop on Computational Geometry",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Streaming Algorithms for Extent Problems in High Dimensions",
      "author" : [ "P.K. Agarwal", "R. Sharathkumar" ],
      "venue" : "Proceedings of the 2014 Symposium of Discrete Algorithms",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Streamed Learning: One-Pass SVMs",
      "author" : [ "P. Rai", "H. Daumé III", "S. Venkatasubramanian" ],
      "venue" : "International Joint Conferences on Artificial Intelligence",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Streaming and Dynamic Algorithms for Minimum Enclosing Balls in High Dimensions",
      "author" : [ "T.M. Chan", "V. Pathak" ],
      "venue" : "Computational Geometry Vol 47,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Wiley",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "hyperplane) between positive and negative instances and as such, they have a very small hypothesis complexity but provable generalization bounds [8].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 5,
      "context" : "The connection between these two problems has made it possible to harness the work done on streaming MEBs and apply them to SVMs, as was done in [6].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we utilize the Blurred Ball cover approximation to the MEB problem proposed in [5] to obtain a streaming SVM that is both fast and space efficient.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "The Core Vector Machine (CVM) was introduced by [3] as an new take on the standard Support Vector Machine (SVM).",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "[3] showed that if the kernel K(x,y) = φ(x) ·φ(y) satisfies K(x,x) = κ , a constant, then w∗ can be found by finding the minimum enclosing ball of the points {φ(xi,yi)}, where φ i(xi,yi) =  yiφ(xi) yi 1 √ C ei ",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "The implementation of the CVM in [3] follows the MEB approximation algorithm described in [4]: given ε > 0, add to the core set C the point z that is the farthest from the current MEB center c.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "The implementation of the CVM in [3] follows the MEB approximation algorithm described in [4]: given ε > 0, add to the core set C the point z that is the farthest from the current MEB center c.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we present the Blurred Ball SVM, a streaming algorithm based on the blurred ball cover proposed in [5].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "The Perceptron, LASVM and Stream SVM data were taken from the experiments documented in [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Is there a streaming algorithm for maintaining an MEB with better guarantees than the Blurred Ball cover proposed by [5]? The paper originally provided a bound of 1+ √ 3 2 ≈ 1.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "3661, which was improved by [7] to less than 1.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Although [5] showed that it is impossible to achieve an arbitrarily small approximation factor, with 1+ ε for any ε > 0, it’s possible that a better streaming MEB algorithm exists with provable bounds better than the 1.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "22 factor demonstrated by [7].",
      "startOffset" : 26,
      "endOffset" : 29
    } ],
    "year" : 2014,
    "abstractText" : "A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the “maximum margin” linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation.",
    "creator" : "LaTeX with hyperref package"
  }
}