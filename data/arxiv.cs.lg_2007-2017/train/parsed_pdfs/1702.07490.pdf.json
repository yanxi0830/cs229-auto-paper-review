{
  "name" : "1702.07490.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Meta-learning by Parallel Algorithm Competition",
    "authors" : [ "Stefan Elfwing", "Eiji Uchibe", "Kenji Doya" ],
    "emails" : [ "elfwing@atr.jp", "uchibe@atr.jp", "doya@oist.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n07 49\n0v 1\n[ cs\n.L G\n] 2\n4 Fe"
    }, {
      "heading" : "1 Introduction",
      "text" : "The efficiency of reinforcement learning (Sutton and Barto, 1998) algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration for new knowledge and exploitation of existing knowledge. Ideally, these meta-parameters should not be fixed during learning. Instead, they should be adapted according to the current learning progress in the task at hand. The adaptation of the meta-parameters is an open question in reinforcement learning, which arguably has become more of an issue recently with the success of deep reinforcement learning in tasks with high-dimensional state spaces, such as ability of the DQN algorithm to achieve human-level\nperformance in many Atari 2600 video games (Mnih et al., 2015). The complexity of and the long learning times in such tasks, where the episode length often increases with improvements in performance, makes it not feasible to perform comprehensive grid-like searches of appropriate meta-parameter values.\nWe propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. The idea behind OMPAC is simple. Run several instances of a reinforcement algorithm in parallel, with small differences in the initial values of the meta-parameters. After a fixed number of episodes, the instances are selected based on their performance in the task at hand. Before continuing the learning, Gaussian noise is added to the meta-parameters with a predefined probability. The OMPAC method is similar to a Lamarckian (Lamarck, 1809) evolutionary process without the crossover operator, but with two main differences compared with standard applications of artificial evolution. First, the goal is not to find the parameters that represent the optimal solutions directly, instead the goal is to find the meta-parameters that enable reinforcement learning agents to learn more efficiently. Second, the goal is not to find the best set of fixed parameters, instead the goal is to adapt the values of the meta-parameters according to the current learning progress. Importantly, the OMPAC method differentiates between the objective of the learning (i.e., maximize the expected accumulated discounted rewards, in the cases of value-based reinforcement learning algorithms used in this study) and the overall goal of the task which is used as the selection criteria for continued learning.\nThe studies most related to our research have used a Darwinian evolutionary approach (i.e., the learning started from scratch in each generation) to find appropriate fixed values of the meta-parameters (Unemi et al., 1994; Eriksson et al., 2003; Elfwing et al., 2008, 2011). Schweighofer and Doya (2003) proposed a meta-learning method based on the mid-term and the long-term running averages of the reward, and Kobayashi et al. (2009) proposed a metalearning method based on the absolute values of the TD-errors. Both methods require the setting of meta-meta-parameters, which is a non-trivial task. Proposed approaches for adapting individual meta-parameters include the incremental delta bar delta method (Sutton, 1992) to tune α, a method based on variance of the action value function (Ishii et al., 2002) to tune the inverse temperature β in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive λ Least-Squares Temporal Difference Learning method (Mann et al., 2016) to tune λ. François-Lavet et al. (2015) demonstrated that the performance of DQN could be improved in some Atari 2600 games by a rather ad-hoc tuning scheme that increase γ (γ ← min(0.02+0.98γ, 0.99)) and decrease α (α ← 0.98α).\nWe validate the OMPAC method in stochastic SZ-Tetris and in standard Tetris with a smaller, 10×10, board, and in the Atari 2600 domain. To be able to directly compare the learning performance with and without the OMPAC method, we use the same experimental setups as in our earlier study (Elfwing et al., 2017), where we proposed the sigmoid-weighted linear (SiL) unit and its derivative function (dSiL) as activation functions for neural network function approximation in reinforcement learning. In both SZ-Tetris and 10×10 Tetris, we train shallow neural network agents with dSiL units in the hidden layer and improve the\nstate-of-the-art scores by 31% and 84%, respectively. In three Atari 2600 games, we train deep neural network agents with SiL units in the convolutional layers and dSiL units in the hidden fully-connected layer and improve the performance by 62% or more. Finally, we demonstrate the ability of the OMPAC method to achieve efficient learning even if the initial values of the meta-parameters are not suitable for the task at hand."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 TD(λ) and Sarsa(λ)",
      "text" : "In this study, we use two reinforcement learning algorithms: TD(λ) (Sutton, 1988) and Sarsa(λ) (Rummery and Niranjan, 1994; Sutton, 1996). TD(λ) learns an estimate of the state-value function, V π , and Sarsa(λ) learns an estimate of the action-value function, Q π\n, while the agent follows policy π. If the approximated value functions, Vt ≈ V π and Qt ≈ Q π\n, are parameterized by the parameter vector θt, then the gradient-descent learning update of the parameters is computed by\nθt+1 = θt + αδtet, (1)\nwhere the TD-error, δt, is δt = rt + γVt(st+1)− Vt(st) (2)\nfor TD(λ) and δt = rt + γQt(st+1, at+1)−Qt(st, at) (3)\nfor Sarsa(λ). The eligibility trace vector, et, is\net = γλet−1 +∇θtVt(st), e0 = 0, (4)\nfor TD(λ) and et = γλet−1 +∇θtQt(st, at), e0 = 0, (5)\nfor Sarsa(λ). Here, st is the state at time t, at is the action selected at time t, rt is the reward for taking action at in state st, α is the learning rate, γ is the discount factor of future rewards, λ is the trace-decay rate, and ∇θtVt and ∇θtQt are the vectors of partial derivatives of the function approximators with respect to each component of θt."
    }, {
      "heading" : "2.2 Sigmoid-weighted Linear Units",
      "text" : "We recently proposed (Elfwing et al., 2017) the sigmoid-weighted linear (SiL) unit and its derivative function (dSiL) as activation functions for neural network function approximation in reinforcement learning. The activation ak of SiL unit k for an input vector s is computed by the sigmoid function, σ(·), multiplied by its input, zk:\nak(s) = zkσ(zk), (6) zk(s) = ∑\ni\nwiksi + bk, (7)\nσ(x) = 1\n1 + e−x . (8)\nHere, wik is the weight connecting state si and hidden unit k, bk is the bias weight for hidden unit k. The activation of the dSiL unit is computed by:\nak(s) = σ(zk) (1 + zk(1− σ(zk))) . (9)\nThe derivative of the activation function of the SiL unit, used for gradient-descent learning updates of the neural network weight parameters (see Equations 4 and 5), is given by\n∇wikak(s) = σ(zk) (1 + zk(1− σ(zk))) si, (10)\nand the derivative of the activation function of the dSiL unit is given by\n∇wikak(s) = σ(zk)(1 − σ(zk))(2 +\nzk(1− σ(zk))− zkσ(zk))si. (11)"
    }, {
      "heading" : "2.3 Action selection",
      "text" : "We use softmax action selection with a Boltzmann distribution in all experiments. For Sarsa(λ), the probability to select action a in state s is defined as\nπ(a|s) = exp(Q(s, a)/τ ) ∑\nb exp(Q(s, b)/τ ) . (12)\nFor the model-based TD(λ) algorithm, we select an action a in state s that leads to the next state s′ with a the probability defined as\nπ(a|s) = exp(V (f(s, a))/τ ) ∑\nb exp(V (f(s, b))/τ ) . (13)\nHere, f(s, a) returns the next state s′ according to the state transition dynamics and τ is the temperature that controls the trade-off between exploration and exploitation. We used hyperbolic discounting of the temperature and the temperature was decreased after every episode i:\nτ(i) = τ0\n1 + τki . (14)\nHere, τ0 is the initial temperature and τk controls the rate of discounting."
    }, {
      "heading" : "3 The OMPAC method",
      "text" : "In this section, we present the OMPAC (Online Meta-learning by Parallel Algorithm Competition) method. Algorithm 1 shows the pseudo-code for the OMPAC method with Sarsa(λ) and softmax action selection. Several, N , instances of the algorithm are run in parallel, with small differences in meta-parameter values. After a fixed number of episodes, the instances are selected for continued learning based on their performance in the task. The OMPAC\nAlgorithm 1 OMPAC with Sarsa(λ) and softmax action selection Initialize matrix of parameter vectors, Θ = [θ1, . . . ,θi, . . . ,θN ]. Initialize matrix of meta-parameter vectors, Ψ = [ψ1, . . . ,ψi, . . . ,ψN ]. for each generation do Initialize vector of scores F ← 0 parfor i = 1 to N do {Parallel for loop}\nfor each episode of the generation do Get initial state si and select action ai ∼ π(·|si). {Softmax action selection} ei ← 0 while si is not terminal do Take action ai, observe reward ri, score fi, and next state s′i. Fi ← Fi + fi. ei ← γiλiei +∇θiQ(si, ai|θi) if s′i is terminal then\nδi ← ri −Q(si, ai|θi) else\nSelect new action a′i ∼ π(·|s ′ i). δi ← ri + γiQ(s ′ i, a ′\ni|θi)−Q(si, ai|θi) ai ← a ′\ni\nend if\nθi ← θi + αiδiei si ← s ′\ni\nend while\nend for\nend for [Θ,Ψ] ← Selection(F ,Θ,Ψ) Ψ ← AddNoise(Ψ)\nend for\nmethod differentiates between the goal of the learning (i.e., maximize the expected accumulated discounted rewards, in the cases of Sarsa(λ)) and the overall goal of the task as measured by the score, Fi, for each instance i in each generation. In this study, the score is equal to the total number of points scored in the games we consider.\nWe use stochastic universal sampling (SUS; Baker, 1987) combined with elitism as the Selection() method. SUS is a fitness proportionate selection method with no bias and minimal variance. Instead of spinning an imaginary roulette wheel with one pointer N times, as in roulette wheel selection, SUS spins the wheel once with N equally-spaced pointers. We first select the best algorithm instance and it continues learning without adding noise to any of the meta-parameters (i.e., elitism). We then use SUS to select the remaining instances and we add Gaussian noise to the meta-parameters with a predefined probability pn. To ensure that changes in meta-parameter values are not too large and therefore disruptive to\nthe learning process, we use a AddNoise() method that adds noise, ǫ, to a meta-parameter, ψ, drawn from a normal distribution with standard deviation that depends on the magnitude of meta-parameter multiplied by predefined factor ηn. If 0 ≤ ψ ≤ 1 (α, γ, and λ in this study), then:\nǫ ∼\n\n\n\nN ( 0, (ψηn) 2 )\nifψ ≤ 0.5,\nN ( 0, ((1− ψ) ηn) 2 ) ifψ > 0.5, (15)\notherwise (τ0 and τk in this study):\nǫ ∼ N ( 0, (ψηn) 2 ) . (16)\nWe also use Equations 15 and 16 to initialize the meta-parameters by adding Gaussian noise to common starting values."
    }, {
      "heading" : "4 Experiments",
      "text" : "To be able to directly compare the learning performance with and without the OMPAC method, we used the same experimental setups as in our earlier study (Elfwing et al., 2017). In stochastic SZ-Tetris and standard Tetris with a smaller, 10×10, board size, we trained agents with shallow neural network function approximators with dSiL hidden units using TD(λ) and softmax action selection (hereafter, shallow dSiL agents). In the Atari 2600 domain, we trained agents with deep convolutional neural network function approximators with SiL hidden units in the convolutional layers and dSiL hidden units in the fully-connected layer using Sarsa(λ) and softmax action selection (hereafter, deep SiL agents). In all OMPAC experiments, the number of parallel algorithm instances N was set to 12, the number of learning episodes in each generation was set to 100, the probability of adding Gaussian noise to a meta-parameter pn was set to 0.1, and the factor controlling the magnitude of the Gaussian noise ηn was set to 0.05. The values were determined by preliminary experiments in stochastic SZ-Tetris."
    }, {
      "heading" : "4.1 Tetris",
      "text" : "Due to prohibitively long learning times (in the case of high performance), it is not feasible to apply value-based reinforcement learning to standard Tetris with a board height of 20. The current state-of-the-art result for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013), is a mean score of 51 million cleared lines. We instead consider stochastic SZ-Tetris (Burgiel, 1997; Szita and Szepesvári, 2010), which only uses the S-shaped and the Z-shaped tetrominos, and standard Tetris with a smaller, 10×10, board.\nIn both versions of Tetris, in each time step, a randomly selected tetromino appears above the board. The agent selects a rotation and a horizontal position, and the tetromino drops down the board, stopping when it hits another tetromino or the bottom of the board.\nIf a row is completed, then it disappears. An episode ends when a tetromino does not fit within the board. The agent gets a score equal to the number of cleared lines, with a maximum of 2 points in SZ-Tetris and of 4 points in 10×10 Tetris (only achievable by the stick-shaped tetromino). The possible number of actions is 17 for the S-shaped and the Zshaped tetrominos. For the additional five tetrominos in 10×10 Tetris, the possible numbers of actions are 9 for the block-shaped tetromino, 17 for stick-shaped tetromino, and 34 for the J-, L- and T-shaped tetrominos.\nWe recently achieved the current state-of-the-art results (Elfwing et al., 2017) using shallow dSiL agents. In SZ-Tetris, a shallow dSiL agent with 50 hidden nodes achieved a final (over 1,000 episodes) mean score of 263 points when averaged over 10 separate runs and of 320 points for the best run. In 10×10 Tetris, a shallow dSiL agent with 250 hidden nodes achieved a final (over 10,000 episodes) mean score of 4,900 points when averaged over 5 separate runs of and 5,300 points for the best run, which improved the average score of 4,200 points and the best score of 5,000 points achieved by the CBMPI algorithm (Gabillon et al., 2013).\nUsing the OMPAC method we trained shallow dSiL agents with 50 hidden units in SZ-Tetris and 250 hidden units in 10×10 Tetris. The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences. The binary state vectors were of length 460 in SZ-Tetris and 260 in 10×10 Tetris. We used the following reward function proposed by Faußer and Schwenker (2013):\nr(s) = e−(number of holes in s)/z . (17)\nHere, z was set to 33 for SZ-Tetris and 33/2 for 10×10 Tetris. We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: α: 0.001, γ: 0.99, λ: 0.55, τ0: 0.5, and τk: 0.00025. The experiments ran for 2,000 generations (i.e., 200,000 episodes of learning in total) in SZ-Tetris and 2,500 generations (250,000 episodes) in 10×10 Tetris. The score used for selection was the total number of points received by an algorithm instance in one generation.\nFigure 1 shows the average learning curves for the OMPAC method in SZ-Tetris and 10×10 Tetris. The scores were computed as mean scores over every 10 generations (1,000 episodes) in SZ-Tetris and every 100 generations (10,000 episodes) in 10×10 Tetris. The final scores are large and significant improvements of the previous state-of-the-art results achieved by shallow dSiL agents without OMPAC adaptation. The final average score of 345 points is a 82 points or 31% improvement in SZ-Tetris, and the final average score of 9,000 points is a 4,100 points or 84% improvement in 10×10 Tetris.\nFigure 2 shows the average values of the meta-parameters computed over the 12 algorithm instances. The adaptations of α and λ were similar in both games. The values of α decreased by about an order of magnitude, from 1.0×10−3 to 1.25×10−4 and 6.57×10−5, and the values of λ decreased from 0.55 to 0.188 and 0.125, in SZ-Tetris and in 10×10 Tetris, respectively. More interestingly, while the value of γ in SZ-Tetris was relatively stable around 0.99 (final average value of 0.9908), the value of γ in 10×10 Tetris first decreased to about 0.98 and\nthen increased back towards 0.99 (final average value of 0.9886). The value of the inverse temperature β = 1/τ in SZ-Tetris increased, in tandem with the improvement in mean score, to over 300 during first ∼125,000 episodes. During the last ∼75,000 episodes, there was only a very small (∼5 points) improvement in mean score and the value of β decreased slowly, reaching a final average value of 232. In 10×10 Tetris, the mean score improved continuously and the value of β increased over the whole learning process, reaching a final average value of 2670 (i.e., approximately greedy action selection)."
    }, {
      "heading" : "4.2 Atari 2600 games",
      "text" : "We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). We followed the experimental setup in our earlier study (Elfwing et al., 2017). The raw 210×160 Atari 2600 RGB frames were pre-processed by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105×80. The deep convolutional neural network used by the deep SiL agent consisted of two convolutional layers with SiL units (16 filters of size 8×8 with a stride of 4 and 32 filters of size 4×4 with a stride of 2), each followed by a max-pooling layer (pooling windows of size 3×3 with a stride of 2), a fully-connected hidden layer with 512 dSiL units, and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We used frame skipping where actions were selected every fourth frame and repeated for the next four frames. The input to the network was a 105×80×2 image consisting of the current and the fourth previous pre-processed frame. As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between −1 and +1, but we did not clip the values of the TD-errors. An episode started with up to 30 ’do nothing’ actions (no-op condition) and it was played until the end of the game or for a maximum of 18,000 frames (i.e., 5 minutes).\nWe used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: α: 0.001, γ: 0.99, λ: 0.8, τ0: 0.5, and τk: 0.0005. The experiments ran for 2,000 generations (i.e., 200,000 episodes of learning in total). The score used for selection was the total raw score received by an algorithm instance in one generation.\nThe deep SiL agent in Elfwing et al. (2017) outperformed DQN, the Gorila implementation of DQN (Nair et al., 2015) and double DQN (van Hasselt et al., 2015) when tested on the 12 games played by DQN that started with the letters ’A’ and ’B’. In this study, we tested the OMPAC method in three Atari 2600 games: Alien, Amidar, and Assault. We choose those three because they are games where the deep SiL agents were outperformed by one or more of the other agents, and it should therefore be room for significant improvements in performance.\nFigure 3 shows the average learning curves over the 12 algorithm instances for the three games. Table 1 summarizes the results as the average scores in the final generation and the best mean scores of individual instances in any generation. The table also shows the final and best mean scores achieved by deep SiL agents without OMPAC adaptation, and the reported best mean scores achieved by DQN, the Gorila implementation of DQN, and double DQN. OMPAC adaptation of the meta-parameters significantly improved the performance of the deep SiL agents, between 83% and 177% when measured by average final scores and between 62% and 82% when measured by best mean scores. The OMPAC scores are also higher than the reported results for the other three methods, except for the double DQN score in the Assault game. However, the learned behavior in the Assault was still good. We tested the best performing agent in last generation using the final values of the meta-parameters, and it survived for the full 5 minutes in 88 out of 100 episodes. A video of a typical episode lasting for 5\nminutes can be found at http://www.cns.atr.jp/~elfwing/videos/assault_OMPAC.mov.\nFigure 4 shows average values of the meta-parameters over the learning process in the three Atari 2600 games. Similar to the two Tetris games, the values of α decreased, by about an order of magnitude of more, over the learning process. In contrast to the two Tetris games where the values of λ decreased (from a smaller starting value of 0.55), the values of λ either increased to about 0.9 (Amidar and Assault) or reached about 0.75 after first decreasing to 0.6 (Alien). The trajectories of the values of γ and β were different in the three games: 1) in Assault, both meta-parameters increased over the whole learning process (final average values of 0.9973 and 1556, for γ and β, respectively); 2) in Amidar, the value of γ was relatively stable but the value of β increased, but slower than in Assault, over the whole learning process (0.9880 and 599); and 3) in Alien, the value of γ, similar to 10×10 Tetris, decreased during the first ∼50,000 episodes before it started to increase, and the value of β, similar to SZ-Tetris, increased during the first ∼120,000 episodes when learning performance was increasing and it then slowly decreased when the learning performance\nplateaued (0.9953 and 206)."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we investigate the ability of the OMPAC method to learn when starting from bad settings of the meta-parameters. The experiments in the two Tetris and the Atari 2600 domains showed that OMPAC adaptation of the meta-parameters can significantly improve the learning performance when using suitable starting values of the meta-parameters. However, when, for example, encountering a new task, it would be valuable to be able to use OMPAC either as a method for achieving high performance even when the initial values of the meta-parameters are not suitable for the task at hand or as a method for finding suitable values of the meta-parameters for future experiments.\nWe chose to limit our investigation to different settings of γ and τk. We trained shallow dSiL agents in SZ-Tetris with three settings of the starting value of γ ∈ {0.8, 0.9, 0.99} and three settings of the starting value of λ ∈ {2.5×10−3, 2.5×10−4, 2.5×10−5}. We used the same values of the three other meta-parameters as in the earlier SZ-Tetris experiment. For each of the nine settings of γ and τk, we also trained agents without OMPAC adaptation for 10 separate runs.\nFigure 5 shows average learning curves with and without OMPAC adaptation of the meta-parameters (top panel), and the average scores over the final 1000 episodes with standard deviations (bottom panel). The results clearly show that the OMPAC method is able to overcome bad initial settings of meta-parameters. The two largest improvements in average final scores (267 and 183 points) were for the (γ, λ)-settings ((0.8, 2.5×10−5) and (0.9, 2.5×10−5)) that achieved the lowest average final scores (3 and 84 points) without OMPAC\nadaptation."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this study, we proposed the OMPAC method for online adaptation of the meta-parameters in reinforcement learning by competition between algorithm instances running in parallel. We validated the proposed method by significantly improving the state-of-the-art scores\nin stochastic SZ-Tetris and 10×10 Tetris, and by significantly improving the performance of deep Sarsa(λ) agents in three Atari 2600 games. The experiments also demonstrated the ability of the OMPAC method to adapt the meta-parameters according to the learning progress in different tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the project commissioned by the New Energy and Industrial Technology Development Organization (NEDO), JSPS KAKENHI grant 16K12504, and Okinawa Institute of Science and Technology Graduate University research support to KD."
    } ],
    "references" : [ {
      "title" : "Reducing bias and inefficiency in the selection algorithm",
      "author" : [ "J.E. Baker" ],
      "venue" : "Proceedings of the Second Genetic Algorithms on Genetic Algorithms and Their Application, pages 14–",
      "citeRegEx" : "Baker,? 1987",
      "shortCiteRegEx" : "Baker",
      "year" : 1987
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279.",
      "citeRegEx" : "Bellemare et al\\.,? 2013",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Temporal differences based policy iteration and applications in neuro-dynamic programming",
      "author" : [ "D.P. Bertsekas", "S. Ioffe" ],
      "venue" : "Technical Report LIDS-P-2349, MIT.",
      "citeRegEx" : "Bertsekas and Ioffe,? 1996",
      "shortCiteRegEx" : "Bertsekas and Ioffe",
      "year" : 1996
    }, {
      "title" : "How to lose at Tetris",
      "author" : [ "H. Burgiel" ],
      "venue" : "Mathematical Gazette, 81:194–200.",
      "citeRegEx" : "Burgiel,? 1997",
      "shortCiteRegEx" : "Burgiel",
      "year" : 1997
    }, {
      "title" : "Temporal difference bayesian model averaging: A bayesian perspective on adapting lambda",
      "author" : [ "C. Downey", "S. Sanner" ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML2010), pages 311–318.",
      "citeRegEx" : "Downey and Sanner,? 2010",
      "shortCiteRegEx" : "Downey and Sanner",
      "year" : 2010
    }, {
      "title" : "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
      "author" : [ "S. Elfwing", "E. Uchibe", "K. Doya" ],
      "venue" : "CoRR, abs/1702.03118.",
      "citeRegEx" : "Elfwing et al\\.,? 2017",
      "shortCiteRegEx" : "Elfwing et al\\.",
      "year" : 2017
    }, {
      "title" : "Co-evolution of shaping rewards and meta-parameters in reinforcement learning",
      "author" : [ "S. Elfwing", "E. Uchibe", "K. Doya", "H.I. Christensen" ],
      "venue" : "Adaptive Behavior, 16(6):400– 412.",
      "citeRegEx" : "Elfwing et al\\.,? 2008",
      "shortCiteRegEx" : "Elfwing et al\\.",
      "year" : 2008
    }, {
      "title" : "Darwinian embodied evolution of the learning ability for survival",
      "author" : [ "S. Elfwing", "E. Uchibe", "K. Doya", "H.I. Christensen" ],
      "venue" : "Adaptive Behavior, 19(2):101–120.",
      "citeRegEx" : "Elfwing et al\\.,? 2011",
      "shortCiteRegEx" : "Elfwing et al\\.",
      "year" : 2011
    }, {
      "title" : "Evolution of meta-parameters in reinforcement learning algorithm",
      "author" : [ "A. Eriksson", "G. Capi", "K. Doya" ],
      "venue" : "Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS2003), pages 412–417.",
      "citeRegEx" : "Eriksson et al\\.,? 2003",
      "shortCiteRegEx" : "Eriksson et al\\.",
      "year" : 2003
    }, {
      "title" : "Neural network ensembles in reinforcement learning",
      "author" : [ "S. Faußer", "F. Schwenker" ],
      "venue" : "Neural Processing Letters, pages 1–15.",
      "citeRegEx" : "Faußer and Schwenker,? 2013",
      "shortCiteRegEx" : "Faußer and Schwenker",
      "year" : 2013
    }, {
      "title" : "How to discount deep reinforcement learning: Towards new dynamic strategies",
      "author" : [ "V. François-Lavet", "R. Fonteneau", "D. Ernst" ],
      "venue" : "CoRR, abs/1512.02011.",
      "citeRegEx" : "François.Lavet et al\\.,? 2015",
      "shortCiteRegEx" : "François.Lavet et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate dynamic programming finally performs well in the game of Tetris",
      "author" : [ "V. Gabillon", "M. Ghavamzadeh", "B. Scherrer" ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems (NIPS2013), pages 1754–1762.",
      "citeRegEx" : "Gabillon et al\\.,? 2013",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2013
    }, {
      "title" : "Control of exploitationâĂŞexploration meta-parameter in reinforcement learning",
      "author" : [ "S. Ishii", "W. Yoshida", "J. Yoshimoto" ],
      "venue" : "Neural Networks, 15(4-6):665–687.",
      "citeRegEx" : "Ishii et al\\.,? 2002",
      "shortCiteRegEx" : "Ishii et al\\.",
      "year" : 2002
    }, {
      "title" : "A meta-learning method based on temporal difference error",
      "author" : [ "K. Kobayashi", "H. Mizoue", "T. Kuremoto", "M. Obayashi" ],
      "venue" : "Proceedings of International Conference on Neural Information Processing (ICONIP2009), pages 530–537.",
      "citeRegEx" : "Kobayashi et al\\.,? 2009",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2009
    }, {
      "title" : "Philosophie Zoologique",
      "author" : [ "J.B. Lamarck" ],
      "venue" : "Chez Dentu.",
      "citeRegEx" : "Lamarck,? 1809",
      "shortCiteRegEx" : "Lamarck",
      "year" : 1809
    }, {
      "title" : "Adaptive λ least-squares temporal difference learning",
      "author" : [ "T.A. Mann", "H. Penedones", "T. Hester" ],
      "venue" : "CoRR, abs/1612.09465.",
      "citeRegEx" : "Mann et al\\.,? 2016",
      "shortCiteRegEx" : "Mann et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533.",
      "citeRegEx" : "Mnih et al\\.,? 2015",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Massively parallel methods for deep reinforcement learning",
      "author" : [ "A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver" ],
      "venue" : "CoRR, abs/1507.04296.",
      "citeRegEx" : "Nair et al\\.,? 2015",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2015
    }, {
      "title" : "On-line Q-learning using connectionist systems",
      "author" : [ "G.A. Rummery", "M. Niranjan" ],
      "venue" : "Technical Report CUED/F-INFENG/TR 166, Cambridge University Engineering Department.",
      "citeRegEx" : "Rummery and Niranjan,? 1994",
      "shortCiteRegEx" : "Rummery and Niranjan",
      "year" : 1994
    }, {
      "title" : "Meta-learning in reinforcement learning",
      "author" : [ "N. Schweighofer", "K. Doya" ],
      "venue" : "Neural Networks, 16(1):5–9.",
      "citeRegEx" : "Schweighofer and Doya,? 2003",
      "shortCiteRegEx" : "Schweighofer and Doya",
      "year" : 2003
    }, {
      "title" : "Learning to predict by the method of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning, 3:9–44.",
      "citeRegEx" : "Sutton,? 1988",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Adapting bias by gradient descent: an incremental version of the delta-bar-delta",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence.",
      "citeRegEx" : "Sutton,? 1992",
      "shortCiteRegEx" : "Sutton",
      "year" : 1992
    }, {
      "title" : "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems (NIPS1996), pages 1038–1044. MIT Press.",
      "citeRegEx" : "Sutton,? 1996",
      "shortCiteRegEx" : "Sutton",
      "year" : 1996
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A. Barto" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "SZ-Tetris as a benchmark for studying key problems of reinforcement learning",
      "author" : [ "I. Szita", "C. Szepesvári" ],
      "venue" : "ICML 2010 workshop on machine learning and games.",
      "citeRegEx" : "Szita and Szepesvári,? 2010",
      "shortCiteRegEx" : "Szita and Szepesvári",
      "year" : 2010
    }, {
      "title" : "Differentiation of learning abilities – a case study on optimizing parameter values in qlearning by a genetic algorithm",
      "author" : [ "T. Unemi", "M. Nagaoyoshi", "N. Hirayama", "T. Nade", "K. Yano", "Y. Masujima" ],
      "venue" : "Proceedings of the International Workshop on the Synthesis and Simulation of Living Systems, pages 331–336.",
      "citeRegEx" : "Unemi et al\\.,? 1994",
      "shortCiteRegEx" : "Unemi et al\\.",
      "year" : 1994
    }, {
      "title" : "Deep reinforcement learning with double q-learning",
      "author" : [ "H. van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : null,
      "citeRegEx" : "Hasselt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The efficiency of reinforcement learning (Sutton and Barto, 1998) algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration for new knowledge and exploitation of existing knowledge.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "performance in many Atari 2600 video games (Mnih et al., 2015).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "The OMPAC method is similar to a Lamarckian (Lamarck, 1809) evolutionary process without the crossover operator, but with two main differences compared with standard applications of artificial evolution.",
      "startOffset" : 44,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : ", the learning started from scratch in each generation) to find appropriate fixed values of the meta-parameters (Unemi et al., 1994; Eriksson et al., 2003; Elfwing et al., 2008, 2011).",
      "startOffset" : 112,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : ", the learning started from scratch in each generation) to find appropriate fixed values of the meta-parameters (Unemi et al., 1994; Eriksson et al., 2003; Elfwing et al., 2008, 2011).",
      "startOffset" : 112,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "Proposed approaches for adapting individual meta-parameters include the incremental delta bar delta method (Sutton, 1992) to tune α, a method based on variance of the action value function (Ishii et al.",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Proposed approaches for adapting individual meta-parameters include the incremental delta bar delta method (Sutton, 1992) to tune α, a method based on variance of the action value function (Ishii et al., 2002) to tune the inverse temperature β in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive λ Least-Squares Temporal Difference Learning method (Mann et al.",
      "startOffset" : 189,
      "endOffset" : 209
    }, {
      "referenceID" : 4,
      "context" : ", 2002) to tune the inverse temperature β in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive λ Least-Squares Temporal Difference Learning method (Mann et al.",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : ", 2002) to tune the inverse temperature β in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive λ Least-Squares Temporal Difference Learning method (Mann et al., 2016) to tune λ.",
      "startOffset" : 206,
      "endOffset" : 225
    }, {
      "referenceID" : 5,
      "context" : "To be able to directly compare the learning performance with and without the OMPAC method, we use the same experimental setups as in our earlier study (Elfwing et al., 2017), where we proposed the sigmoid-weighted linear (SiL) unit and its derivative function (dSiL) as activation functions for neural network function approximation in reinforcement learning.",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : ", 2003; Elfwing et al., 2008, 2011). Schweighofer and Doya (2003) proposed a meta-learning method based on the mid-term and the long-term running averages of the reward, and Kobayashi et al.",
      "startOffset" : 8,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : ", 2003; Elfwing et al., 2008, 2011). Schweighofer and Doya (2003) proposed a meta-learning method based on the mid-term and the long-term running averages of the reward, and Kobayashi et al. (2009) proposed a metalearning method based on the absolute values of the TD-errors.",
      "startOffset" : 8,
      "endOffset" : 198
    }, {
      "referenceID" : 4,
      "context" : ", 2002) to tune the inverse temperature β in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive λ Least-Squares Temporal Difference Learning method (Mann et al., 2016) to tune λ. François-Lavet et al. (2015) demonstrated that the performance of DQN could be improved in some Atari 2600 games by a rather ad-hoc tuning scheme that increase γ (γ ← min(0.",
      "startOffset" : 112,
      "endOffset" : 266
    }, {
      "referenceID" : 20,
      "context" : "1 TD(λ) and Sarsa(λ) In this study, we use two reinforcement learning algorithms: TD(λ) (Sutton, 1988) and Sarsa(λ) (Rummery and Niranjan, 1994; Sutton, 1996).",
      "startOffset" : 88,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "1 TD(λ) and Sarsa(λ) In this study, we use two reinforcement learning algorithms: TD(λ) (Sutton, 1988) and Sarsa(λ) (Rummery and Niranjan, 1994; Sutton, 1996).",
      "startOffset" : 116,
      "endOffset" : 158
    }, {
      "referenceID" : 22,
      "context" : "1 TD(λ) and Sarsa(λ) In this study, we use two reinforcement learning algorithms: TD(λ) (Sutton, 1988) and Sarsa(λ) (Rummery and Niranjan, 1994; Sutton, 1996).",
      "startOffset" : 116,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "We recently proposed (Elfwing et al., 2017) the sigmoid-weighted linear (SiL) unit and its derivative function (dSiL) as activation functions for neural network function approximation in reinforcement learning.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "We use stochastic universal sampling (SUS; Baker, 1987) combined with elitism as the Selection() method.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "To be able to directly compare the learning performance with and without the OMPAC method, we used the same experimental setups as in our earlier study (Elfwing et al., 2017).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 11,
      "context" : "The current state-of-the-art result for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013), is a mean score of 51 million cleared lines.",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "We instead consider stochastic SZ-Tetris (Burgiel, 1997; Szita and Szepesvári, 2010), which only uses the S-shaped and the Z-shaped tetrominos, and standard Tetris with a smaller, 10×10, board.",
      "startOffset" : 41,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : "We instead consider stochastic SZ-Tetris (Burgiel, 1997; Szita and Szepesvári, 2010), which only uses the S-shaped and the Z-shaped tetrominos, and standard Tetris with a smaller, 10×10, board.",
      "startOffset" : 41,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "We recently achieved the current state-of-the-art results (Elfwing et al., 2017) using shallow dSiL agents.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "In 10×10 Tetris, a shallow dSiL agent with 250 hidden nodes achieved a final (over 10,000 episodes) mean score of 4,900 points when averaged over 5 separate runs of and 5,300 points for the best run, which improved the average score of 4,200 points and the best score of 5,000 points achieved by the CBMPI algorithm (Gabillon et al., 2013).",
      "startOffset" : 316,
      "endOffset" : 339
    }, {
      "referenceID" : 2,
      "context" : "The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences.",
      "startOffset" : 66,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences. The binary state vectors were of length 460 in SZ-Tetris and 260 in 10×10 Tetris. We used the following reward function proposed by Faußer and Schwenker (2013):",
      "startOffset" : 66,
      "endOffset" : 386
    }, {
      "referenceID" : 5,
      "context" : "We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: α: 0.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "2 Atari 2600 games We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013).",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "We followed the experimental setup in our earlier study (Elfwing et al., 2017).",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between −1 and +1, but we did not clip the values of the TD-errors.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "(2017) outperformed DQN, the Gorila implementation of DQN (Nair et al., 2015) and double DQN (van Hasselt et al.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "2 Atari 2600 games We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). We followed the experimental setup in our earlier study (Elfwing et al., 2017). The raw 210×160 Atari 2600 RGB frames were pre-processed by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105×80. The deep convolutional neural network used by the deep SiL agent consisted of two convolutional layers with SiL units (16 filters of size 8×8 with a stride of 4 and 32 filters of size 4×4 with a stride of 2), each followed by a max-pooling layer (pooling windows of size 3×3 with a stride of 2), a fully-connected hidden layer with 512 dSiL units, and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We used frame skipping where actions were selected every fourth frame and repeated for the next four frames. The input to the network was a 105×80×2 image consisting of the current and the fourth previous pre-processed frame. As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between −1 and +1, but we did not clip the values of the TD-errors. An episode started with up to 30 ’do nothing’ actions (no-op condition) and it was played until the end of the game or for a maximum of 18,000 frames (i.e., 5 minutes). We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: α: 0.",
      "startOffset" : 113,
      "endOffset" : 1537
    }, {
      "referenceID" : 1,
      "context" : "2 Atari 2600 games We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). We followed the experimental setup in our earlier study (Elfwing et al., 2017). The raw 210×160 Atari 2600 RGB frames were pre-processed by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105×80. The deep convolutional neural network used by the deep SiL agent consisted of two convolutional layers with SiL units (16 filters of size 8×8 with a stride of 4 and 32 filters of size 4×4 with a stride of 2), each followed by a max-pooling layer (pooling windows of size 3×3 with a stride of 2), a fully-connected hidden layer with 512 dSiL units, and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We used frame skipping where actions were selected every fourth frame and repeated for the next four frames. The input to the network was a 105×80×2 image consisting of the current and the fourth previous pre-processed frame. As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between −1 and +1, but we did not clip the values of the TD-errors. An episode started with up to 30 ’do nothing’ actions (no-op condition) and it was played until the end of the game or for a maximum of 18,000 frames (i.e., 5 minutes). We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: α: 0.001, γ: 0.99, λ: 0.8, τ0: 0.5, and τk: 0.0005. The experiments ran for 2,000 generations (i.e., 200,000 episodes of learning in total). The score used for selection was the total raw score received by an algorithm instance in one generation. The deep SiL agent in Elfwing et al. (2017) outperformed DQN, the Gorila implementation of DQN (Nair et al.",
      "startOffset" : 113,
      "endOffset" : 1915
    } ],
    "year" : 2017,
    "abstractText" : "The efficiency of reinforcement learning algorithms depends critically on a few metaparameters that modulates the learning updates and the trade-off between exploration and exploitation. The adaptation of the meta-parameters is an open question in reinforcement learning, which arguably has become more of an issue recently with the success of deep reinforcement learning in high-dimensional state spaces. The long learning times in domains such as Atari 2600 video games makes it not feasible to perform comprehensive searches of appropriate meta-parameter values. We propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. In the OMPAC method, several instances of a reinforcement learning algorithm are run in parallel with small differences in the initial values of the meta-parameters. After a fixed number of episodes, the instances are selected based on their performance in the task at hand. Before continuing the learning, Gaussian noise is added to the meta-parameters with a predefined probability. We validate the OMPAC method by improving the state-of-theart results in stochastic SZ-Tetris and in standard Tetris with a smaller, 10×10, board, by 31% and 84%, respectively, and by improving the results for deep Sarsa(λ) agents in three Atari 2600 games by 62% or more. The experiments also show the ability of the OMPAC method to adapt the meta-parameters according to the learning progress in different tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}