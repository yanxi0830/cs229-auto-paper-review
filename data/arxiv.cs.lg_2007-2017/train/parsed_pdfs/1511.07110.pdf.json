{
  "name" : "1511.07110.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization",
    "authors" : [ "Pengtao Xie", "Yuntian Deng", "Eric Xing" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Recently diversity-inducing regularization methods for latent variable models (LVMs), which encourage the components in LVMs to be diverse, have been studied to address several issues involved in latent variable modeling: (1) how to capture long-tail patterns underlying data; (2) how to reduce model complexity without sacrificing expressivity; (3) how to improve the interpretability of learned patterns. While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing. In this paper, we aim to bridge this gap and analyze how the mutual angular regularizer (MAR) affects the generalization performance of supervised LVMs. We use neural network (NN) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis."
    }, {
      "heading" : "1 Introduction",
      "text" : "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6]. Among the various ML models and algorithms designed for pattern discovery,\nlatent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].\nAlthough LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1]. (2) To cope with the rapidly growing complexity of patterns present in big data, ML practitioners typically increase the size and capacity of LVMs, which incurs great challenges for model training, inference, storage and maintenance [25]. How to reduce model complexity without compromising expressivity is a challenging issue. (3) There exist substantial redundancy and overlapping amongst patterns discovered by existing LVMs from massive data, making them hard to interpret [26].\nTo address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually ”different” from each other. First, concerning the long-tail phenomenon in extracting latent patterns (e.g., clusters, topics) from data: if the model components are biased to be far apart from each other, then one would expect that such components will tend to be less overlapping and less aggregated over dominant patterns (as one often experiences in standard clustering algorithms [27]), and therefore more likely to capture the long-tail patterns. Second, reducing\nar X\niv :1\n51 1.\n07 11\n0v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\n01 5\nmodel complexity without sacrificing expressivity: if the model components are preferred to be different from each other, then the patterns captured by different components are likely to have less redundancy and hence complementary to each other. Consequently, it is possible to use a small set of components to sufficiently capture a large proportion of patterns. Third, improving the interpretability of the learned components: if model components are encouraged to be distinct from each other and non-overlapping, then it would be cognitively easy for human to associate each component to an object or concept in the physical world. Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].\nWhile the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear. In this paper, we aim to bridge this gap and make the first attempt to formally understand why and how introducing diversity into LVMs can lead to better modeling effects. We focus on the mutual angular regularizer proposed in [1] and analyze how it affects the generalization performance of supervised latent variable models. Specifically, we choose neural network (NN) as a model instance to carry out the analysis while noting that the analysis could be extended to other LVMs such as Restricted Boltzmann Machine and Distance Metric Learning. The major insights distilled from the analysis are: as the diversity (which will be made precise later) of hidden units in NN increases, the estimation error of NN decreases while the approximation error increases; thereby the overall generalization error (which is the sum of estimation error and generalization error) reaches the minimum if an optimal diversity level is chosen. In addition to the theoretical study, we also conduct experiments to empirically show that with the mutual angular regularization, the performance of neural networks can be greatly improved. And the empirical results are consistent with the theoretical analysis.\nThe major contributions of this paper include:\n• We propose a diversified neural network with mutual angular regularization (MAR-NN).\n• We analyze the generalization performance of MAR-NN, and show that the mutual angular regularizer can help reduce generalization error.\n• Empirically, we show that mutual angular regularizer can greatly improve the performance of\nNNs and the experimental results are in accordance with the theoretical analysis.\nThe rest of the paper is organized as follows. Section 2 introduces mutual angle regularized neural networks (MAR-NNs). The estimation and approximation errors of MAR-NN are analyzed in Section 3. Section 4 presents empirical studies of MAR-NN. Section 5 reviews related works and Section 6 concludes the paper."
    }, {
      "heading" : "2 Diversify Neural Networks with Mutual Angular Regularizer",
      "text" : "In this section, we review diversity-regularized latent variable models and propose diversified neural networks with mutual angular regularization."
    }, {
      "heading" : "2.1 Diversity-Promoting Regularization of",
      "text" : "Latent Variable Models\nUncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6]. Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task. The knowledge and structures hidden behind data are usually composed of multiple patterns. For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education. Accordingly, latent variable models are parametrized by multiple components where each component aims to capture one pattern in the knowledge and is represented with a parameter vector. For instance, the components in Latent Dirichlet Allocation [10] are called topics and each topic is parametrized by a multinomial vector.\nTo address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:\nmaxA L(A) + λΩ(A) (1)\nwhere each column of A ∈ Rd×k is the parameter vector of a component, L(A) is the objective function of the original LVM, Ω(A) is a regularizer encouraging the components in A to be diverse and λ is a tradeoff parameter. Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].\nHere we present a detailed review of the mutual angular regularizer [1] as our theoretical analysis is based on it. This regularizer is defined with the rationale that if each pair of components are mutually different, then\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nthe set of components are diverse in general. They utilize the non-obtuse angle θij = arccos( |ai·aj | ‖a‖i‖a‖j ) to measure the dissimilarity between component ai and aj as angle is insensitive to geometry transformations of vectors such as scaling, translation, rotation, etc. Given a set of components, angles {θij} between each pair of components are computed and the MAR is defined as the mean of these angles minus their variance\nΩ(A) = 1K(K−1) ∑K i=1 ∑K j=1,j 6=i θij − γ\n1 K(K−1)∑K\ni=1 ∑K j=1,j 6=i(θij − 1 K(K−1) ∑K p=1 ∑K q=1,q 6=p θpq) 2\n(2) where γ > 0 is a tradeoff parameter between mean and variance. The mean term summarizes how these vectors are different from each on the whole. A larger mean indicates these vectors share a larger angle in general, hence are more diverse. The variance term is utilized to encourage the vectors to evenly spread out to different directions. A smaller variance indicates that these vectors are uniformly different from each other."
    }, {
      "heading" : "2.2 Neural Network with Mutual Angular Regularization",
      "text" : "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc. Neural networks are nonlinear models with large capacity and rich expressiveness. If trained properly, they can capture the complex patterns underlying data and achieve notable performance in many machine learning tasks. NNs are composed of multiple layers of computing units and units in adjacent layers are connected with weighted edges. NNs are a typical type of LVMs where each hidden unit is a component aiming to capture the latent features underlying data and is characterized by a vector of weights connecting to units in the lower layer.\nWe instantiate the general framework of diversityregularized LVM to neural network and utilize the mutual angular regularizer to encourage the hidden units (precisely their weight vectors) to be different from each other, which could lead to several benefits: (1) better capturing of long-tail latent features; (2) reducing the size of NN without compromising modeling power. Let L({Ai}l−1i=0) be the loss function of a neural network with l layers where Ai are the weights between layer i and layer i+ 1, and each column of Ai corresponds to a unit. A diversified NN with mutual angular regularization (MAR-NN) can be defined as\nmin{Ai}l−1i=0 L({Ai}l−1i=0)− λ ∑l−2 i=0 Ω(Ai) (3)\nwhere Ω(Ai) is the mutual angular regularizer and λ > 0 is a tradeoff parameter. Note that the regularizer is\nnot applied to Al−1 since in the last layer are output units which are not latent components."
    }, {
      "heading" : "3 Generalization Error Analysis",
      "text" : "In this section, we analyze how the mutual angular regularizer affects the generalization error of neural networks. Let L(f) = E(x,y)∼p∗ [`(x, y, f)] denote the generalization error of hypothesis f , where p∗ is the distribution of input-output pair (x, y) and `(·) is the loss function. Let f∗ ∈ argminf∈FL(f) be the expected risk minimizer. Let L̂(f) = 1n ∑n i=1 `(x (i), y(i), f) be the training error and f̂ ∈ argminf∈F L̂(f) be the empirical risk minimizer. We are interested in the generalization error L(f̂) of the empirical risk min-\nimizer f̂ , which can be decomposed into two parts L(f̂) = L(f̂) − L(f∗) + L(f∗), where L(f̂) − L(f∗) is the estimation error (or excess risk) and L(f∗) is the approximation error. The estimation error represents how well the algorithm is able to learn and usually depends on the complexity of the hypothesis and the number of training samples. A lower hypothesis complexity and a larger amount of training data incur lower estimation error bound. The approximation error indicates how expressive the hypothesis set is to effectively approximate the target function.\nOur analysis below shows that the mutual angular regularizer can reduce the generalization error of neural networks. We assume with high probability τ , the angle between each pair of hidden units is lower bounded by θ. θ is a formal characterization of diversity. The larger θ is, the more diverse these hidden units are. The analysis in the following sections suggests that θ incurs a tradeoff between estimation error and approximation error: the larger θ is, the smaller the estimation error bound is and the larger the approximation error bound is. Since the generalization error is the sum of estimation error and approximation error, θ has an optimal value to yield the minimal generalization error. In addition, we can show that under the same probability τ , increasing the mutual angular regularizer can increase θ. Given a set of hidden units A learned by the MAR-NN, we assume their pairwise angles {θij} are i.i.d samples drawn from a distribution p(X) where the expectation and variance of random variable X is µ and σ respectively. Lemma 1 states that θ is an increasing function of µ and decreasing function of σ. By the definition of MAR, it encourages larger mean and smaller variance. Thereby, the larger the MAR is, the larger θ is. Hence properly controlling the MAR can generate a desired θ that produces the lowest generalization error.\nLemma 1. With probability at least τ , we have X ≥ θ = µ− √ σ\n1−τ\nProof. According to Chebyshev inequality [31],\nσ t2 ≥ p(|X − µ| > t) ≥ p(X < µ− t) (4)\nLet θ = µ− t, then p(X < θ) ≤ σ(µ−θ)2 . Hence p(X ≥ θ) ≥ 1 − σ(µ−θ)2 . Let τ = 1 − σ (µ−θ)2 , then θ = µ −√\nσ 1−τ ."
    }, {
      "heading" : "3.1 Setup",
      "text" : "For the ease of presentation, we first consider a simple neural network whose setup is described below. Later on we extend the analysis to more complicated neural networks.\n• Network structure: one input layer, one hidden layer and one output layer\n• Activation function: Lipschitz continuous function h(t) with constant L. Example: rectified linear h(t) = max(0, t), L = 1; tanh h(t) = tanh(t), L = 1; sigmoid h(t) = sigmoid(t), L = 0.25.\n• Task: univariate regression\n• Let x ∈ Rd be the input vector with ‖x‖2 ≤ C1\n• Let y be the response value with |y| ≤ C2\n• Let wj ∈ Rd be the weights connecting to the jth hidden unit, j = 1, · · · ,m, with ‖wj‖2 ≤ C3. Further, we assume with high probability τ , the\nangle ρ(wi,wj) = arccos( |wi·wj|\n‖wi‖2‖wj‖2 ) between wi and wj is lower bounded by a constant θ for all i 6= j.\n• Let αj be the weight connecting the hidden unit j to the output with ‖α‖2 ≤ C4 • Hypothesis set: F = {f |f(x) = m∑ j=1 αjh(wj Tx)}\n• Loss function set: A = {`|`(x, y) = (f(x)− y)2}"
    }, {
      "heading" : "3.2 Estimation Error",
      "text" : "We first analyze the estimation error bound of MARNN and are interested in how the upper bound is related with the diversity (measured by θ) of the hidden units. The major result is presented in Theorem 1.\nTheorem 1. With probability at least (1− δ)τ\nL(f̂)− L(f∗) ≤ 8( √ J + C2)(2LC1C3C4 + C4|h(0)|) √ m√ n\n+( √ J + C2)2\n√ 2 log(2/δ)\nn\n(5)\nwhere J = mC24h2(0)+L2C21C23C24 ((m−1) cos θ+1)+ 2 √ mC1C3C 2 4L|h(0)| √ (m− 1) cos θ + 1.\nNote that the right hand side is a decreasing function w.r.t θ. A larger θ (denoting the hidden units are more diverse) would induce a lower estimation error bound."
    }, {
      "heading" : "3.2.1 Proof",
      "text" : "A well established result in learning theory is that the estimation error can be upper bounded by the Rademacher complexity. We start from the Rademacher complexity, seek a further upper bound of it and show how the diversity of the hidden units affects this upper bound. The Rademacher complexity Rn(A) of the loss function set A is defined as\nRn(A) = E[sup`∈A 1n ∑n i=1 σi`(f(x\n(i)), y(i))] (6)\nwhere σi is uniform over {−1, 1} and {(x(i), y(i))}ni=1 are i.i.d samples drawn from p∗. The Rademacher complexity can be utilized to upper bound the estimation error, as shown in Lemma 2.\nLemma 2. [32, 33, 34] With probability at least 1− δ L(f̂)− L(f∗) ≤ 4Rn(A) +B √ 2 log(2/δ)\nn (7)\nfor B ≥ supx,y,f |`(f(x), y)|\nOur analysis starts from this lemma and we seek further upper bound of Rn(A). The analysis needs an upper bound of the Rademacher complexity of the hypothesis set F , which is given in Lemma 3. Lemma 3. Let Rn(F) denote the Rademacher complexity of the hypothesis set F = {f |f(x) = m∑ j=1 αjh(wj Tx)}, then\nRn(F) ≤ 2LC1C3C4 √ m√\nn + C4|h(0)|\n√ m√\nn (8)\nProof. Rn(F) = E[supf∈F 1n ∑n i=1 σi ∑m j=1 αjh(wj Txi)]\n= E[supf∈F 1n ∑m j=1 αj ∑n i=1 σih(wj Txi)]\n(9) Let α = [α1, · · · , αm]T and h = [ ∑n i=1 σih(w1 Txi), · · · , ∑n i=1 σih(wm Txi)] T , the inner product α · h ≤ ‖α‖1‖x‖∞ as ‖ · ‖1 and ‖ · ‖∞ are dual norms. Therefore\nα · h ≤ ‖α‖1‖h‖∞ = ( ∑m j=1 |αj |)(maxj=1,··· ,m | ∑n i=1 σih(wj\nTxi)|) ≤ √ m‖α‖2 ·maxj=1,··· ,m | ∑n i=1 σih(wj\nTxi)| ≤ √ mC4 ·maxj=1,··· ,m | ∑n i=1 σih(wj\nTxi)| (10)\nSo Rn(F) ≤ √ mC4E[supf∈F 1n | ∑n i=1 σih(wj\nTxi)|]. Denote R||(F) = E[supf∈F | 2nσif(xi)|], which is another form of Rademacher complexity used in some\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nliterature such as [33]. Let F ′ = {f ′|f ′(x) = h(wTx)} where w,x satisfy the conditions specified in Section 3.1, then Rn(F) ≤ √ mC4 2 R||(F ′). Let G = {g|g(x) = wTx} where w,x satisfy the conditions specified in Section 3.1, thenR||(F ′) = R||(h◦g). Let h′(·) = h(·) − h(0), then h′(0) = 0 and h′ is also L-Lipschitz. Then\nR||(F ′) = R||(h ◦ g) = R||(h′ ◦ g + h(0)) ≤ R||(h′ ◦ g) + 2|h(0)|√n (Theorem 12 in [33]) ≤ 2LR||(g) + 2|h(0)|√n (Theorem 12 in [33]) (11)\nNow we bound R||(g):\nR||(g) = E[supg∈G | 2n ∑n i=1 σiw\nTxi|] ≤ 2nE[supg∈G ‖w‖2 · ‖ ∑n i=1 σixi‖]\n≤ 2C3n E[‖ ∑n i=1 σixi‖2]\n= 2C3n Ex[Eσ[‖ ∑n i=1 σixi‖2]]\n≤ 2C3n Ex[ √ Eσ[‖ ∑n i=1 σixi‖22]] (concavity of √ ·)\n= 2C3n Ex[ √ Eσ[ ∑n i=1 σ 2 i xi\n2]] (∀i 6= j σi ⊥⊥ σj) = 2C3n Ex[ √∑n i=1 xi\n2] ≤ 2C1C3√\nn\n(12) Putting Eq.(11) and Eq.(12) together, we have R||(F ′) ≤ 4LC1C3√n + 2|h(0)|√ n . Plugging into Rn(F) ≤ √ mC4 2 R||(F ′) completes the proof.\nIn addition, we need the following bound of |f(x)|.\nLemma 4. With probability at least τ\nsup x,f |f(x)| ≤\n√ J (13)\nwhere J = mC24h2(0)+L2C21C23C24 ((m−1) cos θ+1)+ 2 √ mC1C3C 2 4L|h(0)| √ (m− 1) cos θ + 1.\nProof. Let α = [α1, · · · , αm]T , W = [w1, · · · ,wm], h = [h(w1 Tx), · · · , h(wmTx)]T , then we have\nf2(x) = ( ∑m j=1 αjh(wj Tx))2 = (α · h)2 ≤ (‖α‖2‖h‖2)2 ≤ C24‖h‖22\n(14)\nNow we want to derive an upper bound for ‖h‖2. As h(t) is L-Lipschitz, |h(wjTx)| ≤ L|wjTx| + |h(0)|.\nTherefore\n‖h‖22 = ∑m j=1 h 2(wj Tx)\n≤ ∑m j=1(L|wjTx|+ |h(0)|)2\n= ∑m j=1 h 2(0) + L2(wj Tx)2 + 2L|h(0)||wjTx| = mh2(0) + L2‖WTx‖22 + 2L|h(0)||WTx|1 ≤ mh2(0) + L2‖WTx‖22 + 2 √ mL|h(0)|‖WTx‖2 ≤ mh2(0) + L2‖WT ‖2op‖x‖22 +2 √ mL|h(0)|‖WT ‖op‖x‖2 = mh2(0) + L2‖W‖2op‖x‖22 +2 √ mL|h(0)|‖W‖op‖x‖2 ≤ mh2(0) + L2C21‖W‖2op + 2 √ mC1L|h(0)|‖W‖op\n(15) where ‖ ·‖op denotes the operator norm. We can make use of the lower bound of ρ(wj,wk) for j 6= k to get a bound for ‖W‖op:\n‖W‖2op = sup‖u‖2=1 ‖Wu‖ 2 2 = sup‖u‖2=1(u TWTWu)\n= sup‖u‖2=2 ∑m j=1 ∑m k=1 ujukwj ·wk\n≤ sup‖u‖2=2 ∑m j=1 ∑m k=1\n|uj ||uk||wj||wk| cos(ρ(wj,wk)) ≤ C23 sup‖u‖2=2 ∑m j=1 ∑m k=1,k 6=j\n|uj ||uk| cos θ + ∑m j=1 |uj |2 (with probability at least τ)\n(16)\nDefine u′ = [|u1|, · · · , |ump |]T , Q ∈ Rm p×mp : Qjk = cos θ for j 6= k and Qjj = 1, then ‖u′‖2 = ‖u‖ and\n‖W‖2op ≤ C23 sup‖u‖2=2 u ′TQu′ ≤ C23 sup‖u‖2=2 λ1(Q)‖u ′‖22 ≤ C23λ1(Q)\n(17)\nwhere λ1(Q) is the largest eigenvalue of Q. By simple linear algebra we can get λ1(Q) = (m− 1) cos θ+ 1, so\n‖W‖2op ≤ ((m− 1) cos θ + 1)C23 (18)\nSubstitute to Eq.(15), we have\n‖h‖22 ≤ mh2(0) + L2C21C23 ((m− 1) cos θ + 1)+\n2 √ mC1C3L|h(0)| √ (m− 1) cos θ + 1 (19)\nSubstitute to Eq.(14):\nf2(x) ≤ mC24h2(0) + L2C21C23C24 ((m− 1) cos θ + 1)+\n2 √ mC1C3C 2 4L|h(0)| √ (m− 1) cos θ + 1\n(20) In order to simplify our notations, define\nJ = mC24h2(0) + L2C21C23C24 ((m− 1) cos θ + 1)+\n2 √ mC1C3C 2 4L|h(0)| √ (m− 1) cos θ + 1 (21)\nThen supx,f |f(x)| ≤ √ supx,f f 2(x) = √ J . Proof completes.\nGiven these lemmas, we proceed to prove Theorem 1. The Rademacher complexity Rn(A) of A is\nRn(A) = E[supf∈F 1n ∑n i=1 σi`(f(x), y)]\n(22)\n`(·, y) is Lipschitz continuous with respect to the first argument, and the constant L is supx,y,f |f(x)− y| ≤ 2 supx,y,f (|f(x)| + |y|) = 2( √ J + C2). Applying the composition property of Rademacher complexity, we have\nRn(A) ≤ 2( √ J + C2)Rn(F) (23)\nUsing Lemma 3, we have\nRn(A) ≤ 2( √ J +C2)( 2LC1C3C4 √ m√\nn + C4|h(0)|\n√ m√\nn )\n(24) Note that supx,y,f |`(f(x), y)| ≤ ( √ J+C2)2, and plugging Eq.(24) into Lemma 2 completes the proof."
    }, {
      "heading" : "3.2.2 Extensions",
      "text" : "In the above analysis, we consider a simple neural network described in Section 3.1. In this section, we present how to extend the analysis to more complicated cases, such as neural networks with multiple hidden layers, other loss functions and multiple outputs.\nMultiple Hidden Layers The analysis can be extended to multiple hidden layers by recursively applying the composition property of Rademacher complexity to the hypothesis set.\nWe define the hypothesis set FP for neural network with P hidden layers in a recursive manner:\nF0 = {f0|f0(x) = w0 · x} F1 = F = {f1|f1(x) = ∑m0 j=1 wj 1h(f0j (x)), f0j ∈ F0} Fp = {fp|fp(x) = ∑mp−1 j=1 wj ph(fp−1j (x)),\nfp−1j ∈ Fp−1}(l = 2, · · · , P ) (25)\nwhere we assume there are mp units in hidden layer p and wpj is the connecting weight from the j-th unit in hidden layer p− 1 to p. (we index hidden layers from 0, w0 is the connecting weight from input to hidden layer 0). When P = 1 the above definition recovers the one-hidden-layer case in Section 3.1 if we treat w1 as α. We make similar assumptions as Section 3.1: h(·) is L-Lipschitz, ‖x‖2 ≤ C1, ‖wp‖2 ≤ Cp3 . We also assume that the pairwise angles of the connecting weights ρ(wpj ,w p k) for j 6= k are lower bounded by θp with probability at least τp. Under these assumptions, we have the following result:\nTheorem 2. For a neural network with P hidden layers, with probability at least (1− δ) ∏P−1 p=0 τ p\nL(f̂)− L(f∗) ≤ 8( √ J p + C2)( (2L) PC1C 0 3√\nn\n∏P−1 p=0 √ mpCp3\n+ |h(0)|√ n ∑P−1 p=0 (2L) P−1−p∏P−1 j=p √ mjCj3) +( √ J p + C2)2 √ 2 log(2/δ)\nn\n(26) where\nJ 0 = C21 ((m0 − 1) cos θ0 + 1) J p = (Cp3 )2((mp − 1) cos θp + 1)L2J p−1 +2(Cp3 ) 2L|h(0)| √ mp−1((mp − 1) cos θp + 1) √ J p−1+ (Cp3 ) 2((mp − 1) cos θp + 1)mp−1h2(0)(p = 1, · · · , P )\n(27)\nWhen P = 1, Eq.(26) reduces to the estimation error bound of neural network with one hidden layer. Note that the right hand side is a decreasing function w.r.t θp, hence making the hidden units in each hidden layer to be diverse can reduce the estimation error bound of neural networks with multiple hidden layers.\nIn order to prove Theorem 2, we first bound the Rademacher complexity of the hypothesis set FP : Lemma 5. Let Rn(FP ) denote the Rademacher complexity of the hypothesis set FP , then\nRn(FP ) ≤ (2L)PC1C 0 3√\nn\nP−1∏ p=0 √ mpCp3\n+ |h(0)|√\nn P−1∑ p=0 (2L)P−1−p P−1∏ j=p √ mjCj3 (28)\nProof. Notice that Rn(FP )) ≤ 12R||(F P ):\nRn(FP ) = E[supf∈FP 1n ∑n i=1 σif(xi)]\n≤ E[supf∈FP | 1n ∑n i=1 σif(xi)|] = 12R||(F P )\n(29)\nSo we can bound Rn(FP )) by bounding 12R||(F P ). We bound 12R||(F p) recursively: ∀p = 1, · · · , P , we have\nR||(Fp) = E[supf∈Fp | 2n ∑n i=1 σif(xi)|] = E[supfj∈Fp−1 | 2 n ∑n i=1 σi ∑ml−1 j=1 wj\nlh(fj(xi))|] ≤ √ mp−1Cp3E[supfj∈Fp−1 | 2 n ∑n i=1 σih(fj(xi))|]\n≤ √ mp−1Cp3 (2LR||(Fp−1) + 2|h(0)|√ n )\n(30) where the last two steps are similar to the proof of Lemma 3. Applying the inequality in Eq.(30) recursively, and noting from the proof of Lemma 3 that\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nR||(F0) ≤ 2C1C\n0 3√\nn we have\nR||(FP ) ≤ 2(2L)PC1C 0 3√\nn\n∏P−1 p=0 √ mpCp3\n+ 2|h(0)|√ n ∑P−1 p=0 (2L) P−1−p∏P−1 j=p √ mjCj3\n(31) Plugging into Eq.(29) completes the proof.\nIn addition, we need the following bound. Lemma 6. With probability at least ∏P−1 p=0 τ p, supx,fP∈Fp |fP (x)| ≤ √ J P , where\nJ 0 = C21 ((m0 − 1) cos θ0 + 1) J p = (Cp3 )2((mp − 1) cos θp + 1)L2J p−1 +2(Cp3 ) 2L|h(0)| √ mp−1((mp − 1) cos θp + 1) √ J p−1 +(Cp3 ) 2((mp − 1) cos θp + 1)mp−1h2(0)(1, · · · , P )\n(32)\nProof. For a given neural network, we denote the outputs of the p-th hidden layer before applying the activation function as vp:\nv0 = [w01 T x, · · · ,w0m0x]T vp = [ ∑mp−1 j=1 w p j,1h(v\np−1 j ), · · · ,∑mp−1\nj=1 w p j,mph(v p−1 j )] T (p = 1, · · · , P ) (33)\nwhere wpj,i is the connecting weight from the j-th unit of the hidden layer p− 1 to the i-th unit of the hidden layer p.\nTo facilitate the derivation of bounds, we also denote\nwpi = [w p 1,i, · · · ,w p mp−1,i] T (34)\nand\nhp = [h(vp−11 ), · · · , h(v p−1 mp−1)] T (35)\nwhere vp−1i is the i-th element of v p−1.\nUsing the above notations, we can write vp as\nvp = [wp1 · hp, · · · ,w p mp · hp]T (36)\nHence we can bound the L2 norm of v p recursively:\n‖vp‖22 = ∑mp i=1(w p i · hp)2 (37)\nDenote W = [wp1 , · · · ,w p mp ], then\n‖vp‖22 = ‖WThp‖22 ≤ ‖WT ‖2op‖hp‖22 = ‖W‖2op‖hp‖22\n(38)\nwhere ‖ · ‖op denotes the operator norm.\nWe can make use of the lower bound of ρ(wpj ,w p k) for j 6= k to get a bound for ‖W‖op:\n‖W‖2op = sup‖u‖2=1 ‖Wu‖ 2 2 = sup‖u‖2=1(u TWTWu)\n= sup‖u‖2=2 ∑mp j=1 ∑mp k=1 ujukw p j ·w p k\n≤ sup‖u‖2=2 ∑mp j=1 ∑mp k=1\n|uj ||uk||wpj ||w p k | cos(ρ(w p j ,w p k)) ≤ (Cp3 )2 sup‖u‖2=2 ∑mp j=1 ∑mp k=1,k 6=j\n|uj ||uk| cos θp + ∑mp j=1 |uj |2\n(with probability at least ∏P−1 p=0 τ p)\n(39)\nDefine u′ = [|u1|, · · · , |ump |]T , Q ∈ Rm p×mp : Qjk = cos θp for j 6= k and Qjj = 1, then ‖u′‖2 = ‖u‖ and\n‖W‖2op ≤ (Cp3 )2 sup‖u‖2=2 u ′TQu′ ≤ (Cp3 )2 sup‖u‖2=2 λ1(Q)‖u ′‖22 ≤ (Cp3 )2λ1(Q)\n(40)\nwhere λ1(Q) is the largest eigenvalue of Q. By simple linear algebra we can get λ1(Q) = (m\np − 1) cos θp + 1, so\n‖W‖2op ≤ ((mp − 1) cos θp + 1)(C p 3 ) 2 (41)\nSubstituting Eq.(41) back to Eq.(38), we have\n‖vp‖22 ≤ (C p 3 ) 2((mp − 1) cos θp + 1)‖hp‖22 (42)\nThen we make use of the Lipschitz-continuous property of h(t) to further bound ‖hp‖22:\n‖hp‖22 = ∑mp−1 j=1 h 2(vp−1j )\n≤ ∑mp−1 j=1 (|h(0)|+ L|v p−1 j |)2\n= ∑mp−1 j=1 h 2(0) + L2(vp−1j ) 2 + 2L|h(0)||vp−1j | = mp−1h2(0) + L2‖vp−1‖22 + 2L|h(0)|‖vp−1‖1 ≤ mp−1h2(0) + L2‖vp−1j ‖22 + 2L|h(0)| √ mp−1‖vp−1‖2\n(43) Substituting Eq.(43) to Eq.(42), we have\n‖vp‖22 ≤ (C p 3 ) 2((mp − 1) cos θp + 1)L2‖vp−1j ‖ 2 2\n+ 2(Cp3 ) 2L|h(0)| √ mp−1((mp − 1) cos θp + 1)‖vp−1‖2\n+ (Cp3 ) 2((mp − 1) cos θp + 1)mp−1h2(0) (44)\nAnd noticing that ‖v0‖22 ≤ ((m0−1) cos θ0 +1)‖x‖22 ≤ C21 ((m\n0−1) cos θ0 +1), we can bound ‖vp‖ recursively now. Denote\nJ 0 = C21 ((m0 − 1) cos θ0 + 1) J p = (Cp3 )2((mp − 1) cos θp + 1)L2J p−1 +2(Cp3 ) 2L|h(0)| √ mp−1((mp − 1) cos θp + 1) √ J p−1+ (Cp3 ) 2((mp − 1) cos θp + 1)mp−1h2(0)(p = 1, · · · , P )\n(45)\nthen ‖vp‖22 ≤ J p and J p decreases when θi(i = 0, · · · , p) increases.\nNow we are ready to bound supx,fP∈FP |fP (x)|:\nsupx,fP∈FP |fP (x)| = supx,fP∈FP |vP | ≤ √ J P\n(46)\nGiven these lemmas, we proceed to prove Theorem 2. The Rademacher complexity Rn(A) of A is\nRn(A) = E[supf∈F 1n ∑n i=1 σi`(f(xi), y)] (47)\n`(·, y) is Lipschitz continuous with respect to the first argument, and the constant L is supx,y,f |f(x)− y| ≤ 2 supx,y,f (|f(x)| + |y|) = 2( √ J + C2). Applying the composition property of Rademacher complexity, we have\nRn(A) ≤ 2( √ J + C2)Rn(F) (48)\nUsing Lemma 5, we have\nRn(A) ≤ 2( √ J + C2)( (2L)PC1C 0 3√\nn\nP−1∏ p=0 √ mpCp3\n+ |h(0)|√\nn P−1∑ p=0 (2L)P−1−p P−1∏ j=p √ mjCj3) (49)\nNote that supx,y,f |`(f(x), y)| ≤ ( √ J+C2)2, and plugging Eq.(49) into Lemma 2 completes the proof.\nOther Loss Functions Other than regression, a more popular application of neural network is classification. For binary classification, the most widely used loss functions are logistic loss and hinge loss. Estimation error bounds similar to that in Theorem 1 can also be derived for these two loss functions.\nLemma 7. Let the loss function `(f(x), y) = log(1 + exp(−yf(x))) be the logistic loss where y ∈ {−1, 1}, then with probability at least (1− δ)τ\nL(f̂)− L(f∗) ≤ 4\n1+exp(− √ J ) (2LC1C3C4 + C4|h(0)|) √ m√ n + log(1 + exp( √ J)) √ 2 log(2/δ)\nn\n(50)\nProof.\n|∂l(f(x), y) ∂f | = exp(−yf(x)) 1 + exp(−yf(x)) = 1\n1 + exp(yf(x)) (51)\nAs | 11+exp(yf(x)) | ≤ 1 1+exp(− supf,x |f(x)|) = 1 1+exp(− √ J ) , we have proved that the Lipschitz constant L of `(·, y) can be bounded by 1\n1+exp(− √ J ) .\nAnd the loss function `(f(x), y) can be bounded by\n|`(f(x), y)| ≤ log(1 + exp( √ J)) (52)\nSimilar to the proof of Theorem 1, we can finish the proof by applying the composition property of Rademacher complexity, Lemma 3 and Lemma 2.\nLemma 8. Let `(f(x), y) = max(0, 1 − yf(x)) be the hinge loss where y ∈ {−1, 1}, then with probability at least (1− δ)τ\nL(f̂)− L(f∗) ≤ 4(2LC1C3C4 + C4|h(0)|) √ m√ n\n+(1 + √ J)\n√ 2 log(2/δ)\nn\n(53)\nProof. Given y, `(·, y) is Lipschitz with constant 1. And the loss function can be bounded by\n|`(f(x), y)| ≤ 1 + √ J (54)\nThe proof can be completed using similar proof of Lemma 7.\nMultiple Outputs The analysis can be also extended to neural networks with multiple outputs, provided the loss function factorizes over the dimensions of the output vector. Let y ∈ RK denote the target output vector, x be the input feature vector and `(f(x),y) be the loss function. If `(f(x),y) factorizes\nover k, i.e., `(f(x),y) = ∑K k=1 `\n′(f(x)k, yk), then we can perform the analysis for each `′(f(x)k, yk) as that in Section 3.2.1 separately and sums the estimation error bounds up to get the error bound for `(f(x),y). Here we present two examples. For multivariate regression, the loss function `(f(x),y) is a squared loss: `(f(x),y) = ‖f(x) − y‖22, where f(·) is the prediction function. This squared loss can be factorized as ‖f(x)− y‖22 = ∑K k=1(f(x)k − yk)2. We can obtain an estimation error bound for each (f(x)k − yk)2 according to Theorem 1, then sum these bounds together to get the bound for ‖f(x)− y‖22.\nFor multiclass classification, the commonly used loss function is cross-entropy loss: `(f(x),y) =\n− ∑K k=1 yk log ak, where ak = exp(f(x)k)∑K j=1 exp(f(x)j) . We can also derive error bounds similar to that in Theorem 1 by using the composition property of Rademacher complexity. First we need to find the Lipschitz constant:\nLemma 9. Let `(x,y, f) be the cross-entropy loss,\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nthen for any f , f ′\n|`(f(x), y)− `(f ′(x), y)| ≤\nK − 1 K − 1 + exp(−2 √ J ) K∑ k=1 |f(x)k − f ′(x)k| (55)\nProof. Note that y is a 1-of-K coding vector where exactly one element is 1 and all others are 0. Without loss of generality, we assume yk′ = 1 and yk = 0 for k 6= k′. Then\n`(f(x),y) = − log exp(f(x)k ′)∑K\nj=1 exp(f(x)j) (56)\nHence for k 6= k′ we have\n|∂l(f(x),y)∂f(x)k | = 11+ ∑ j 6=k′ exp(f(x)j) ≤ 1 1+(K−1) exp(−2 √ J )\n(57)\nand for k′ we have\n|∂l(f(x),y)∂f(x)k′ | = ∑ j 6=k′ exp(f(x)j)\n1+ ∑ j 6=k′ exp(f(x)j)\n≤ K−1 K−1+exp(−2 √ J )\n(58)\nAs K−1 K−1+exp(−2 √ J ) ≥ 1 1+(K−1) exp(−2 √ J ) , we have proved that for any k, |∂l(f(x),y)∂f(x)k | ≤ K−1 K−1+exp(−2 √ J ) . Therefore\n‖∇f(x)`(f(x), y)‖∞ ≤ K − 1\nK − 1 + exp(−2 √ J ) (59)\nUsing mean value theorem, for any f , f ′, ∃ξ such that\n|`(f(x), y)− `(f ′(x), y)| = ∇ξ`(ξ, y) · (f(x)− f ′(x)) ≤ ‖∇f(x)`(f(x), y)‖∞‖f(x)− f ′(x)‖1 ≤ K−1\nK−1+exp(−2 √ J ) ∑K k=1 |f(x)k − f ′(x)k| (60)\nWith Lemma 9, we can get the Rademacher complexity of cross entropy loss by performing the Rademacher complexity analysis for each f(x)k as that in Section 3.2.1 separately, and multiplying the sum of them by K−1 K−1+exp(−2 √ J ) to get the Rademacher complexity of `(f(x), y). And as the loss function can be bounded by\n|`(f(x), y)| ≤ log(1 + (K − 1) exp(2 √ J )) (61)\nwe can use similar proof techniques as in Theorem 1 to get the estimation error bound."
    }, {
      "heading" : "3.3 Approximation Error",
      "text" : "Now we proceed to investigate how the diversity of weight vectors affects the approximation error bound. For the ease of analysis, following [35], we assume the target function g belongs to a function class with smoothness expressed in the first moment of its Fourier representation: we define function class ΓC as the set of functions g satisfying∫\n‖x‖2≤C1 |w||g̃(w)|dw ≤ C (62)\nwhere g̃(w) is the Fourier representation of g(x) and we assume ‖x‖2 ≤ C1 throughout this paper. We use function f in F = {f |f(x) = ∑m j=1 αjh(w T j x)} which is the NN function class defined in Section 3.1, to approximate g ∈ ΓC . Recall the following conditions of F :\n∀j ∈ {1, · · · ,m}, ‖wj‖2 ≤ C3 (63) ‖α‖2 ≤ C4 (64) ∀j 6= k, ρ(wj , wk) ≥ θ(with probability at least τ)\n(65)\nwhere the activation function h(t) is the sigmoid function and we assume ‖x‖2 ≤ C1. The following theorem states the approximation error.\nTheorem 3. Given C > 0, for every function g ∈ ΓC with g(0) = 0, for any measure P , if\nC1C3 ≥ 1 (66) C4 ≥ 2 √ mC (67) m ≤ 2(b π 2 − θ θ c+ 1) (68)\nthen with probability at least τ , there is a function f ∈ F such that\n‖g−f‖L ≤ 2C( 1√ n + 1 + 2 lnC1C3 C1C3 )+4mCC1C3 sin(\nθ′ 2 )\n(69) where ‖f‖L = √∫ x f2(x)dP (x), θ′ = min(3mθ, π).\nNote that the approximation error bound in Eq.(69) is an increasing function of θ. Hence increasing the diversity of hidden units would hurt the approximation capability of neural networks."
    }, {
      "heading" : "3.4 Proof",
      "text" : "Before proving Theorem 3, we need the following lemma:\nLemma 10. For any three nonzero vectors u1, u2, u3, let θ12 = arccos(\nu1·u2 ‖u1‖2‖u2‖2 ), θ23 = arccos( u2·u3 ‖u2‖2‖u3‖2 ),\nθ13 = arccos( u1·u3 ‖u1‖2‖u3‖2 ). We have θ13 ≤ θ12 + θ23.\nProof. Without loss of generality, assume ‖u1‖2 = ‖u2‖2 = ‖u3‖2 = 1. Decompose u1 as u1 = u1// +u1⊥ where u1// = c12u2 for some c12 ∈ R and u1⊥ ⊥ u2. As u1 · u2 = cos θ12, we have c12 = cos θ12 and ‖u1⊥‖2 = sin θ12.\nSimilarly, decompose u3 as u3 = u3// + u3⊥ where u3// = c32u2 for some c32 ∈ R and u3⊥ ⊥ u2. We have c23 = cos θ23 and ‖u3⊥‖2 = sin θ23.\nSo we have\ncos θ13 = u1 · u3 = (u1// + u1⊥) · (u3// + u3⊥) = u1// · u3// + u1⊥ · u3⊥ = cos θ12 cos θ23 + u1⊥ · u3⊥ ≥ cos θ12 cos θ23 − sin θ12 ∼ θ23 = cos(θ12 + θ23)\n(70)\nIf θ12 + θ23 ≤ π, arccos(cos(θ12 + θ23)) = θ12 + θ23. As arccos(·) is monotonously decreasing, we have θ13 ≤ θ12 + θ23. Otherwise, θ13 ≤ π ≤ θ12 + θ23.\nIn order to approximate the function class ΓC , we first remove the constraints ρ(wj , wk) ≥ θ and obtain an approximation error:\nLemma 11. Let F ′ = {f |f(x) = ∑m j=1 αjh(w T j x)} be the function class satisfying the following constraints:\n• |αj | ≤ 2C\n• ‖wj‖2 ≤ C3\nThen for every g ∈ ΓC with g(0) = 0, ∃f ′ ∈ F ′ such that\n‖g(x)− f ′(x)‖L ≤ 2C( 1√ n + 1 + 2 lnC1C3 C1C3 ) (71)\nProof. Please refer to Theorem 3 in [35] for the proof. Note that the τ used in their paper is C1C3 here. Furthermore, we omit the bias term b as we can always add a dummy feature 1 to the input x to avoid using the bias term.\nWe also need the following lemma: Lemma 12. For any 0 < θ < π2 , m ≤ 2(b π 2−θ θ c+ 1), (w′j) m j=1, ∃(wj)mj=1 such that\n∀j 6= k ∈ {1, · · · ,m}, ρ(wj , wk) ≥ θ (72) ∀j ∈ {1, · · · ,m}, ‖wj‖2 = ‖w′j‖2 (73) ∀j ∈ {1, · · · ,m}, arccos( wj · w′j\n‖wj‖2‖w′j‖2 ) ≤ θ′ (74)\nwhere θ′ = min(3mθ, π).\nProof. To simplify our notations, let φ(a, b) = arccos( a·b‖a‖2‖b‖2 ). We begin our proof by considering a 2-dimensional case: Let\nk = b π 2 − θ θ c (75)\nLet index set I = {−(k+ 1),−k, · · · ,−1, 1, 2, · · · , k+ 1}. We define a set of vectors (ei)i∈I : ei = (sin θi, cos θi), where θi ∈ (−π2 , π 2 ) is defined as follows:\nθi = sgn(i)( θ\n2 + (|i| − 1)θ) (76)\nFrom the definition we can verify the following conclusions:\n∀i 6= j ∈ I, ρ(ei, ej) ≥ θ (77)\n− π 2 + θ 2 ≤ θ−(k+1) < − π 2 + 3 2 θ (78) π\n2 − 3 2 θ < θk+1 ≤ π 2 − θ 2 (79)\n(80)\nAnd we can further verify that ∀i ∈ I, there exists different i1, · · · , i2k+1 ∈ I\\i such that φ(ei, eij ) ≤ jθ.\nFor any e = (sinβ, cosβ) with β ∈ [−π2 , π 2 ], we can find i ∈ I such that φ(ei, e) ≤ 32θ:\n• if β ≥ θk+1, take i = k + 1, we have φ(ei, e) ≤ π 2 − θk+1 < 3 2θ.\n• if β ≤ θ−(k+1), take i = −(k + 1), we also have φ(ei, e) ≤ 32θ\n• otherwise, take i = sgn(β)dβ− θ 2\nθ e, we also have φ(ei, e) ≤ θ < 32θ.\nRecall that for any i, there exists different i1, · · · , i2k+1 ∈ I\\i such that φ(ei, eij ) ≤ jθ, and use Lemma 10, we can draw the conclusion that for any e = (sinβ, cosβ) with β ∈ [−π2 , π 2 ], there exists different i1, · · · , i2k+2 such that φ(ei, eij ) ≤ 32θ+ (j− 1)θ = (j + 12 )θ. For any (w′j) m j=1, assume w ′ j = ‖w′j‖2(sinβj , cosβj), and we assume βj ∈ [−π2 , π 2 ]. Using the above conclusion, for w′1, we can find some r1 such that φ(w ′ 1, er1) ≤ 3 2θ. For w ′ 2, we can find different i1, i2 such that φ(w′2, ei1) ≤ 32θ < ( 3 2 + 1)θ and φ(w ′ 2, ei2) ≤ ( 32 + 1)θ. So we can find r2 6= r1 such that φ(w′2, er2) ≤ ( 32 +1)θ. Following this scheme, we can find rj /∈ {r1, · · · , rj−1} and φ(w′j , erj ) ≤ (j + 12 )θ < 3mθ for j = 1, · · · ,m, as we have assumed that m ≤ 2(k + 1). Let wj = ‖w′j‖2erj , then we have constructed (wj)mj=1 such that\n∀j ∈ {1, · · · ,m}, φ(w′j , wj) ≤ 3mθ (81) ∀j ∈ {1, · · · ,m}, ‖w′j‖2 = ‖wj‖2 (82) ∀j 6= k, ρ(wj , wk) ≥ θ (83)\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nNote that we have assumed that ∀j = 1, · · · ,m, βj ∈ [−π2 , π 2 ]. In order to show that the conclusion holds for general w′j , we need to consider the case where βj ∈ [− 32π,− π 2 ]. For that case, we can let β ′ j = βj +π, then β′j ∈ [−π2 , π 2 ]. Let w ′′ j = ‖w′j‖2(sinβ′j , cosβ′j), we can find the erj such that φ(w ′′ j , erj ) ≤ mθ following the same procedure. Let wj = −‖w′j‖2erj , then φ(w′j , wj) = φ(w ′′ j , erj ) ≤ 2mθ and as ρ(−erj , ek) = ρ(erj , ek), the ρ(wj , wk) ≥ θ condition is still satisfied. Also note that φ(a, b) ≤ π, the proof for 2-dimensional case is completed.\nNow we consider a general d-dimensional case. Similar to the 2-dimensional one, we construct a set of vectors with unit l2 norm such that the pairwise angles ρ(wj , wk) ≥ θ for j 6= k. We do the construction in two phases:\nIn the first phase, we construct a sequence of unit vector sets indexed by I = {−(k + 1), · · · ,−1, 1, · · · , k + 1}:\n∀i ∈ I, Ei = {e ∈ Rd|‖e‖2 = 1, e · (1, 0, · · · , 0) = cos θi} (84) where θi = sgn(i)( θ 2 + (|i| − 1)θ) is defined the same as we did in Eq.(76). It can be shown that ∀i 6= j, ∀ei ∈ Ei, ej ∈ Ej ,\nρ(ei, ej) ≥ θ (85)\nThe proof is as follows. First, we write ei as ei = (cos θi, 0, · · · , 0)+ ri, where ‖ri‖2 = | sin θi|. Similarly, ej = (cos θj , 0, · · · , 0) + rj , where ‖rj‖2 = | sin θj |. Hence we have\nei · ej = cos θi cos θj + ri · rj (86)\nHence\ncos(ρ(ei, ej)) = |ei · ej | ≤ cos θi cos θj + | sin θi sin θj | = max(cos(θi + θj), cos(θi − θj))\n(87)\nWe have shown in the 2-dimensional case that cos(θi+ θj) ≥ cos θ and cos(θi − θj) ≥ cos θ, hence ρ(ei, ej) ≥ θ. In other words, we have proved that for any two vectors from Ei and Ej , their pairwise angle is lower bounded by θ. Now we proceed to construct a set of vectors for each Ei such that the pairwise angles can also be lower bounded by θ. The construction is as follows.\nFirst, we claim that for any Ei, if W ⊂ E satisfies\n∀wj 6= wk ∈ W, φ(wj , wk) ≥ θ (88)\nthen |W | is finite. In order to prove that, we first define B(x, r) = {y ∈ Rn : ‖y − x‖2 < r}. Then\nEi ⊂ ∪e∈EiB(e, 1−cos θ2 1+cos θ2\n). From the definition of Ei, it is a compact set, so the open cover has a finite subcover. Therefore we have ∃V ⊂ Ei with |V | being finite and\nEi ⊂ ∪v∈VB(v, 1− cos θ2 1 + cos θ2 ) (89)\nFurthermore, we can verify that ∀v ∈ V,∀e1, e2 ∈ B(v,\n1−cos θ2 1+cos θ2\n), φ(e1, e2) ≤ θ. So if W ⊂ Ei satisfies ∀wj 6= wk ∈ W, φ(wj , wk) ≥ θ, then for each v, |B(v, 1−cos θ 2\n1+cos θ2 ) ∩W| = 1. As W ⊂ Ei, we have\n|W | = |W ∩ Ei| = |W ∩ (∪v∈VB(v,\n1−cos θ2 1+cos θ2 ))|\n= | ∪v∈V W ∩B(v, 1−cos θ2 1+cos θ2 )| ≤ ∑ v∈V |W ∩B(v,\n1−cos θ2 1+cos θ2\n)| ≤ ∑ v∈V 1 = |V |\n(90)\nTherefore, we have proved that |W | is finite. Using that conclusion, we can construct a sequence of vectors wj ∈ Ei(j = 1, · · · , l) in the following way:\n1. Let w1 ∈ Ei be any vector in Ei.\n2. For j = 2, · · · , let wj ∈ Ei be any vector satisfying\n∀k = 1, · · · , j − 1, φ(wj , wk) ≥ θ (91) ∃k ∈ {, · · · , j − 1}, φ(wj , wk) = θ (92)\nuntil we cannot find such vectors any more.\n3. As we have proved that |W | is finite, the above process will end in finite steps. Assume that the last vector we found is indexed by l.\nWe can verify that such constructed vectors satisfy\n∀j 6= k ∈ {1, · · · , l}, ρ(wj , wk) ≥ θ (93)\nNote that due to the construction, φ(wj , wk) ≥ θ, as ρ(wj , wk) = min(φ(wj , wk), π − φ(wj , wk)), we only need to show that π − φ(wj , wk) ≥ θ. To show that, we use the definition of Ei to write wj as wj = (cos θi, 0, · · · , 0)+rj , where ‖rj‖2 = | sin θi|. Similarly, wk = (cos θi, 0, · · · , 0) + rk, where ‖rk‖2 = | sin θi|. Therefore cos(φ(wj , wk)) = wj ·wk ≥ cos2 θi−sin2 θi = cos(2θi) ≥ cos(π−θ), where the last inequality follows from the construction of θi. So π− φ(wj , wk) ≥ θ, the proof for ρ(wj , wk) ≥ θ is completed.\nNow we will show that ∀e ∈ Ei, we can find j ∈ {1, · · · , l} such that φ(e, wj) ≤ θ. We prove it by contradiction: assume that there exists e such that\nminj∈{1,··· ,l} φ(e, wj) > θ, then as Ej is a connected set, there is a path q : t ∈ [0, 1] → Ej connecting e to w1, and when t = 0, the path starts at q(0) = e; when t = 1, the path ends at q(1) = w1. We define functions rj(t) = φ(q(t), wj) for t ∈ [0, 1] and j = 1, · · · , l. It is straightforward to see that rj(t) is continuous, hence minj(rj(t)) is also continuous. As minj(rj(0)) > θ and minj(rj(0)) = 0 < θ, there exists t\n∗ ∈ (0, 1) such that minj(rj(0)) = θ. Then q(t\n∗) satisfies Condition 91, which contradicts the construction in W as the construction only ends when we cannot find such vectors. Hence we have proved that\n∀e ∈ Ei,∃j ∈ {1, · · · , l}, φ(e, wj) ≤ θ (94)\nNow we can proceed to prove the main lemma. For each i ∈ I, we use Condition 91 to construct a sequence of vectors wij . Then such constructed vectors wij have pairwise angles greater than or equal to θ. Then for any e ∈ Rd with ‖e‖2 = 1, we write e in sphere coordinates as e = (cos r1, sin r1 cos r2, · · · , ∏d j=1 sin rj). Use the same method as we did for the 2-dimensional case, we can find θi such that |θi − r| ≤ 32θ. Then e ′ =\n(cos θi, sin θi cos r2, · · · , sin θi ∏d j=2 sin rj) ∈ Ei. It is easy to verify that φ(e, e′) = |θi − r| ≤ 32θ. As e′ ∈ Ei, there exists wij as we constructed such that φ(e′, wij) ≤ θ. So φ(e, wij) ≤ φ(e, e′) + φ(e′, wij) ≤ 5 2θ < 3θ. So we have proved that for any e ∈ R\nd with ‖e‖2 = 1, we can find wij such that φ(e, wij) < 3θ.\nFor any wij , assume i + 1 ∈ I, we first project wij onto w∗ ∈ Ei+1 with φ(wij , w∗) ≤ 32θ, then we find wi+1,j′ ∈ Ei+1 such that φ(wi+1,j′ , w∗) ≤ θ. So we have found wi+1,j′ such that φ(wij , wi+1,j′) ≤ 52θ < 3θ. We can use similar scheme to prove that ∀wij , there exists different wi1,j1 · · · , wi2k+1,j2k+1 such that (ir, jr) 6= (i, j) and φ(wij , wir,jr ) ≤ 3rθ. Following the same proof as the 2-dimensional case, we can prove that if m ≤ 2k + 1, then we can find a set of vectors (wj) m j=1 such that\n∀j ∈ {1, · · · ,m}, φ(w′j , wj) ≤ min(3mθ, π) (95) ∀j ∈ {1, · · · ,m}, ‖w′j‖2 = ‖wj‖2 (96) ∀j 6= k, ρ(wj , wk) ≥ θ (97)\nThe proof completes.\nLemma 13. For any f ′ ∈ F ′, ∃f ∈ F ′′ such that\n‖f ′ − f‖L ≤ 4mCC1C3 sin( θ′\n2 ) (98)\nwhere θ′ = min(3mθ, π).\nProof. According to the definition of F ′, ∀f ′ ∈ F ′, there exists (α′j) m j=1, (w ′ j) m j=1 such that\nf ′ = m∑ j=1 α′jh(w ′T j x) (99) ∀j ∈ {1, · · · ,m}, |α′j | ≤ 2C (100) ∀j ∈ {1, · · · ,m}, ‖w′j‖2 ≤ C4 (101)\nAccording to Lemma 12, there exists (wj) m j=1 such that\n∀j 6= k ∈ {1, · · · ,m}, ρ(wj , wk) ≥ θ (102) ∀j ∈ {1, · · · ,m}, ‖wj‖2 = ‖w′j‖2 (103) ∀j ∈ {1, · · · ,m}, arccos( wj · w′j\n‖wj‖2‖w′j‖2 ) ≤ θ′ (104)\nwhere θ′ = min(mθ, π2 ). Let f = ∑m j=1 αjh(w ′T j x),\nthen ‖α‖2 ≤ √ ‖α‖1‖α‖∞ ≤ 2 √ mC ≤ C4. Hence f ∈ F . Then all we need to do is to bound ‖f − f ′‖L:\n‖f − f ′‖2L = ∫ ‖x‖2≤C1(f(x)− f ′(x))2dP (x)\n= ∫ ‖x‖2≤C1( ∑ j αjh(w T j x)− ∑ j αjh(w ′T j x)) 2dP (x)\n= ∫ ‖x‖2≤C1( ∑ j αj(h(w T j x)− h(w′Tj x)))2dP (x)\n≤ ∫ ‖x‖2≤C1( ∑ j |αj ||wTj x− w′Tj x|)2dP (x)\n≤ C21 ∫ ‖x‖2≤C1( ∑ j |αj |‖wj − w′j‖2)2dP (x)\n(105)\nAs arccos( wj ·w′j\n‖wj‖2‖w′j‖2 ) ≤ θ′, we have wj · w′j ≥\n‖wj‖22 cos θ′. Hence\n‖wj − w′j‖22 = 2‖wj‖22 − 2wj · w′j ≤ 2‖wj‖22 − 2‖wj‖22 cos θ′ ≤ 4C23 sin2( θ ′ 2 )\n(106)\nSubstituting back to Eq.(105), we have\n‖f − f ′‖2L ≤ C21 ∫ ‖x‖2≤C1( ∑ j |αj |2C3 sin( θ′ 2 )) 2dP (x) ≤ 16m2C2C21C23 sin2( θ ′\n2 ) (107)\nWith this lemma, we can proceed to prove Theorem 3. For every g ∈ ΓC with g(0) = 0, according to Lemma 11, ∃f ′ ∈ F ′ such that\n‖g − f ′‖L ≤ 2C( 1√ n + 1 + 2 lnC1C4 C1C4 ) (108)\nAccording to Lemma 13, we can find f ∈ F such that\n‖f − f ′‖L ≤ 4mCC1C3 sin( θ′\n2 ) (109)\nThe proof is completed by noting\n‖g − f‖L ≤ ‖g − f ′‖L + ‖f ′ − f‖L (110)"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we present the experimental results on MAR-NN. Specifically, we are interested in how the performance of neural networks varies as the tradeoff parameter λ in MAR-NN increases. A larger λ would induce a stronger regularization, which generates a larger angle lower bound θ. We apply MAR-NN for phoneme classification [36] on the TIMIT1 speech dataset. The inputs are MFCC features extracted with context windows and the outputs are class labels generated by the HMM-GMM model through forced alignment [36]. The feature dimension is 360 and the number of classes is 2001. There are 1.1 million data instances in total. We use 70% data for training and 30% for testing. The activation function is sigmoid and loss function is cross-entropy. The networks are trained with stochastic gradient descent and the minibatch size is 100.\nFigure 1 shows the testing accuracy versus the tradeoff parameter λ achieved by four neural networks with one hidden layer. The number of hidden units varies in {50, 100, 200, 300}. As can be seen from these figures, under various network architectures, the best accuracy is achieved under a properly chosen λ. For example, for the neural network with 100 hidden units, the best accuracy is achieved when λ = 0.01. These empirical observations are aligned with our theoretical analysis that the best generalization performance is achieved under a proper diversity level. Adding this regularizer greatly improves the performance of neural networks, compared with unregularized NNs. For example, in a NN with 200 hidden units, the mutual angular regularizer improves the accuracy from ∼0.415 (without regularization) to 0.45."
    }, {
      "heading" : "5 Related Works",
      "text" : ""
    }, {
      "heading" : "5.1 Diversity-Promoting Regularization",
      "text" : "Diversity-promoting regularization approaches, which encourage the parameter vectors in machine learning\n1https://catalog.ldc.upenn.edu/LDC93S1\nmodels to be different from each other, have been widely studied and found many applications. Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity. Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity. In a multi-class classification problem, [44] proposed to use the determinant of the covariance matrix to encourage classifiers to be different from each other. Our work focuses on the theoretical analysis of diversity regularized latent variable models, using neural network as an instance to study how the mutual angular regularizer affects the generalization error."
    }, {
      "heading" : "5.2 Regularization of Neural Networks",
      "text" : "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data. Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48]. In this paper, we study a new type of regularization approach of NN: diversity-promoting regularization, which bears new properties and functionalities complementary to the existing regularizers."
    }, {
      "heading" : "5.3 Generalization Performance of Neural Networks",
      "text" : "The generalization performance of neural networks, in particular the approximation error and estimation error, has been widely studied in the past several decades. For the approximation error, [49] demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function. [50] showed that neural networks with a single hidden layer, sufficiently many hidden units and arbitrary bounded and nonconstant activation function are universal approximators. [51] proved that multi-\nlayer feedforward networks with a non-polynomial activation function can approximate any function. Various error rates have also been derived based on different assumptions of the target function. [52] showed that if the target function is in the hypothesis set formed by neural networks with one hidden layer of m units, then the approximation error rate is O(1/ √ m). [35] showed that neural networks with one layer of m hidden units and sigmoid activation function can achieve approximation error of order O(1/ √ m), where the target function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. [53] proved that if the target function is of the form f(x) = ∫ Q c(w, b)h(wTx + b)dµ, where c(·, ·) ∈ L∞(Q,µ), then neural networks with one layer of m hidden units can approximate it with an error rate of n−1/2−1/(2d) √ log n, where d is the dimension of input x. As for the estimation error, please refer to [32] for an extensive review, which introduces various estimation error bounds based on VC-dimension, flat-shattering dimension, pseudo dimension and so on."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we provide theoretical analysis regarding why the diversity-promoting regularizers can lead to better latent variable modeling. Using neural network as an instance, we analyze how the generalization performance of supervised latent variable models is affected by the mutual angular regularizer. Our analysis shows that increasing the diversity of hidden units leads to the decrease of estimation error bound and increase of approximation error bound. Overall, if the diversity level is set appropriately, a low generalization error can be achieved. The empirical experiments demonstrate that with mutual angular regularization, the performance of neural networks can be greatly improved and the empirical observations are consistent with the theoretical implications."
    } ],
    "references" : [ {
      "title" : "Diversifying restricted boltzmann machine for document modeling",
      "author" : [ "Pengtao Xie", "Yuntian Deng", "Eric P. Xing" ],
      "venue" : "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Pattern recognition and machine learning",
      "author" : [ "Christopher M Bishop" ],
      "venue" : "springer,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Data mining: concepts and techniques: concepts and techniques",
      "author" : [ "Jiawei Han", "Micheline Kamber", "Jian Pei" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Introduction to statistical pattern recognition",
      "author" : [ "Keinosuke Fukunaga" ],
      "venue" : "Academic press,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Machine learning: Trends, perspectives, and prospects",
      "author" : [ "MI Jordan", "TM Mitchell" ],
      "venue" : "Science, 349(6245):255–260,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "A tutorial on hidden markov models and selected applications in speech recognition",
      "author" : [ "Lawrence R Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "Latent variable models. In Learning in graphical models, pages 371–403",
      "author" : [ "Christopher M Bishop" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Latent variable models and factor analysis",
      "author" : [ "Martin Knott", "David J Bartholomew" ],
      "venue" : "Number 7. Edward Arnold,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan" ],
      "venue" : "Journal of machine Learning research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Mixed membership stochastic blockmodels",
      "author" : [ "Edoardo M Airoldi", "David M Blei", "Stephen E Fienberg", "Eric P Xing" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Build, compute, critique, repeat: Data analysis with latent variable models",
      "author" : [ "David M Blei" ],
      "venue" : "Annual Review of Statistics and Its Application,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Learning internal representations by error propagation",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1985
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott C. Deerwester", "Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman" ],
      "venue" : "Journal of the American Society for Information Science,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1990
    }, {
      "title" : "Sparse coding with an overcomplete basis set: A strategy employed by v1",
      "author" : [ "Bruno A Olshausen", "David J Field" ],
      "venue" : "Vision research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "Daniel D Lee", "H Sebastian Seung" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1999
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "Eric P Xing", "Michael I Jordan", "Stuart Russell", "Andrew Y Ng" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Bayesian haplotype inference via the dirichlet process",
      "author" : [ "Eric P Xing", "Michael I Jordan", "Roded Sharan" ],
      "venue" : "Journal of Computational Biology,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Keller: estimating time-varying interactions between genes",
      "author" : [ "Le Song", "Mladen Kolar", "Eric P Xing" ],
      "venue" : "Bioinformatics, 25(12):i128–i136,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Tied boltzmann machines for cold start recommendations",
      "author" : [ "Asela Gunawardana", "Christopher Meek" ],
      "venue" : "In Proceedings of the 2008 ACM conference on Recommender systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Yehuda Koren", "Robert Bell", "Chris Volinsky" ],
      "venue" : "IEEE Computer,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Peacock: Learning longtail topic features for industrial applications",
      "author" : [ "Yi Wang", "Xuemin Zhao", "Zhenlong Sun", "Hao Yan", "Lifeng Wang", "Zhihui Jin", "Liubin Wang", "Yang Gao", "Ching Law", "Jia Zeng" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Learning compact and effective distance metrics with diversity regularization",
      "author" : [ "Pengtao Xie" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Rubik: Knowledge guided tensor factorization and completion for health data analytics",
      "author" : [ "Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Priors for diversity in generative latent variable models",
      "author" : [ "James Y. Zou", "Ryan P. Adams" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Probabilistic latent semantic analysis",
      "author" : [ "Thomas Hofmann" ],
      "venue" : "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1999
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "All of statistics: a concise course in statistical inference",
      "author" : [ "Larry Wasserman" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Neural network learning: Theoretical foundations",
      "author" : [ "Martin Anthony", "Peter L Bartlett" ],
      "venue" : "cambridge university press,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1999
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "Lecture notes of statistical learning theory",
      "author" : [ "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Universal approximation bounds for superpositions of a sigmoidal function",
      "author" : [ "Andrew R Barron" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1993
    }, {
      "title" : "Deep belief networks using discriminative features for phone recognition",
      "author" : [ "Abdel-rahman Mohamed", "Tara N Sainath", "George Dahl", "Bhuvana Ramabhadran", "Geoffrey E Hinton", "Michael Picheny" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "Neural network ensembles, cross validation, and active learning",
      "author" : [ "Anders Krogh", "Jesper Vedelsby" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1995
    }, {
      "title" : "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy",
      "author" : [ "Ludmila I Kuncheva", "Christopher J Whitaker" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2003
    }, {
      "title" : "Diversity creation methods: a survey and categorisation",
      "author" : [ "Gavin Brown", "Jeremy Wyatt", "Rachel Harris", "Xin Yao" ],
      "venue" : "Information Fusion,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2005
    }, {
      "title" : "Ensemble diversity measures and their application to thinning",
      "author" : [ "Robert E Banfield", "Lawrence O Hall", "Kevin W Bowyer", "W Philip Kegelmeyer" ],
      "venue" : "Information Fusion,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2005
    }, {
      "title" : "An analysis of diversity measures",
      "author" : [ "E Ke Tang", "Ponnuthurai N Suganthan", "Xin Yao" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2006
    }, {
      "title" : "Focused ensemble selection: A diversitybased method for greedy ensemble selection",
      "author" : [ "Ioannis Partalas", "Grigorios Tsoumakas", "Ioannis P Vlahavas" ],
      "venue" : "In European Conference on Artificial Intelligence,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2008
    }, {
      "title" : "Diversity regularized machine",
      "author" : [ "Yang Yu", "Yu-Feng Li", "Zhi-Hua Zhou" ],
      "venue" : "In IJCAI Proceedings- International Joint Conference on Artificial Intelligence. Citeseer,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2011
    }, {
      "title" : "Ratio semi-definite classifiers",
      "author" : [ "Jonathan Malkin", "Jeff Bilmes" ],
      "venue" : "In Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2008
    }, {
      "title" : "Generalization performance of regularized neural network models",
      "author" : [ "Jan Larsen", "Lars Kai Hansen" ],
      "venue" : "In Neural Networks for Signal Processing",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1994
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2012
    }, {
      "title" : "Breaking the curse of dimensionality with convex neural networks",
      "author" : [ "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1412.8690,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2014
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2013
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "George Cybenko" ],
      "venue" : "Mathematics of control, signals and systems,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1989
    }, {
      "title" : "Approximation capabilities of multilayer feedforward networks",
      "author" : [ "Kurt Hornik" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1991
    }, {
      "title" : "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
      "author" : [ "Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1993
    }, {
      "title" : "A simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training",
      "author" : [ "Lee K Jones" ],
      "venue" : "The annals of Statistics,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 1992
    }, {
      "title" : "Uniform approximation by neural networks",
      "author" : [ "Y Makovoz" ],
      "venue" : "Journal of Approximation Theory,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].",
      "startOffset" : 95,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].",
      "startOffset" : 95,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].",
      "startOffset" : 95,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].",
      "startOffset" : 184,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 14,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 15,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 333,
      "endOffset" : 341
    }, {
      "referenceID" : 8,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 333,
      "endOffset" : 341
    }, {
      "referenceID" : 14,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 359,
      "endOffset" : 367
    }, {
      "referenceID" : 15,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 359,
      "endOffset" : 367
    }, {
      "referenceID" : 5,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 388,
      "endOffset" : 395
    }, {
      "referenceID" : 17,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 388,
      "endOffset" : 395
    }, {
      "referenceID" : 18,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 419,
      "endOffset" : 427
    }, {
      "referenceID" : 19,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 419,
      "endOffset" : 427
    }, {
      "referenceID" : 20,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 452,
      "endOffset" : 460
    }, {
      "referenceID" : 21,
      "context" : "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].",
      "startOffset" : 452,
      "endOffset" : 460
    }, {
      "referenceID" : 22,
      "context" : "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].",
      "startOffset" : 357,
      "endOffset" : 364
    }, {
      "referenceID" : 0,
      "context" : "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].",
      "startOffset" : 357,
      "endOffset" : 364
    }, {
      "referenceID" : 22,
      "context" : "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].",
      "startOffset" : 474,
      "endOffset" : 481
    }, {
      "referenceID" : 0,
      "context" : "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].",
      "startOffset" : 474,
      "endOffset" : 481
    }, {
      "referenceID" : 23,
      "context" : "(2) To cope with the rapidly growing complexity of patterns present in big data, ML practitioners typically increase the size and capacity of LVMs, which incurs great challenges for model training, inference, storage and maintenance [25].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 24,
      "context" : "(3) There exist substantial redundancy and overlapping amongst patterns discovered by existing LVMs from massive data, making them hard to interpret [26].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually ”different” from each other.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually ”different” from each other.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually ”different” from each other.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 25,
      "context" : ", clusters, topics) from data: if the model components are biased to be far apart from each other, then one would expect that such components will tend to be less overlapping and less aggregated over dominant patterns (as one often experiences in standard clustering algorithms [27]), and therefore more likely to capture the long-tail patterns.",
      "startOffset" : 278,
      "endOffset" : 282
    }, {
      "referenceID" : 25,
      "context" : "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 25,
      "context" : "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 0,
      "context" : "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].",
      "startOffset" : 289,
      "endOffset" : 292
    }, {
      "referenceID" : 0,
      "context" : "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].",
      "startOffset" : 319,
      "endOffset" : 322
    }, {
      "referenceID" : 25,
      "context" : "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.",
      "startOffset" : 94,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.",
      "startOffset" : 94,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.",
      "startOffset" : 94,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "We focus on the mutual angular regularizer proposed in [1] and analyze how it affects the generalization performance of supervised latent variable models.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.",
      "startOffset" : 23,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "For instance, the components in Latent Dirichlet Allocation [10] are called topics and each topic is parametrized by a multinomial vector.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:",
      "startOffset" : 243,
      "endOffset" : 254
    }, {
      "referenceID" : 0,
      "context" : "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:",
      "startOffset" : 243,
      "endOffset" : 254
    }, {
      "referenceID" : 23,
      "context" : "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:",
      "startOffset" : 243,
      "endOffset" : 254
    }, {
      "referenceID" : 25,
      "context" : "Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "Here we present a detailed review of the mutual angular regularizer [1] as our theoretical analysis is based on it.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 27,
      "context" : "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 29,
      "context" : "According to Chebyshev inequality [31],",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "[32, 33, 34] With probability at least 1− δ",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 31,
      "context" : "[32, 33, 34] With probability at least 1− δ",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 32,
      "context" : "[32, 33, 34] With probability at least 1− δ",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 31,
      "context" : "literature such as [33].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 31,
      "context" : "R||(F ′) = R||(h ◦ g) = R||(h ◦ g + h(0)) ≤ R||(h ◦ g) + 2|h(0)| n (Theorem 12 in [33]) ≤ 2LR||(g) + 2|h(0)| n (Theorem 12 in [33]) (11)",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "R||(F ′) = R||(h ◦ g) = R||(h ◦ g + h(0)) ≤ R||(h ◦ g) + 2|h(0)| n (Theorem 12 in [33]) ≤ 2LR||(g) + 2|h(0)| n (Theorem 12 in [33]) (11)",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "For the ease of analysis, following [35], we assume the target function g belongs to a function class with smoothness expressed in the first moment of its Fourier representation: we define function class ΓC as the set of functions g satisfying ∫",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 33,
      "context" : "Please refer to Theorem 3 in [35] for the proof.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "minj∈{1,··· ,l} φ(e, wj) > θ, then as Ej is a connected set, there is a path q : t ∈ [0, 1] → Ej connecting e to w1, and when t = 0, the path starts at q(0) = e; when t = 1, the path ends at q(1) = w1.",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "We define functions rj(t) = φ(q(t), wj) for t ∈ [0, 1] and j = 1, · · · , l.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : "We apply MAR-NN for phoneme classification [36] on the TIMIT speech dataset.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 34,
      "context" : "The inputs are MFCC features extracted with context windows and the outputs are class labels generated by the HMM-GMM model through forced alignment [36].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 35,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 36,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 37,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 38,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 39,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 40,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 23,
      "context" : "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 42,
      "context" : "In a multi-class classification problem, [44] proposed to use the determinant of the covariance matrix to encourage classifiers to be different from each other.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 43,
      "context" : "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data.",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 44,
      "context" : "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data.",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 45,
      "context" : "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 43,
      "context" : "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 44,
      "context" : "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 46,
      "context" : "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 47,
      "context" : "For the approximation error, [49] demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 48,
      "context" : "[50] showed that neural networks with a single hidden layer, sufficiently many hidden units and arbitrary bounded and nonconstant activation function are universal approximators.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 49,
      "context" : "[51] proved that multi-",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 50,
      "context" : "[52] showed that if the target function is in the hypothesis set formed by neural networks with one hidden layer of m units, then the approximation error rate is O(1/ √ m).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[35] showed that neural networks with one layer of m hidden units and sigmoid activation function can achieve approximation error of order O(1/ √ m), where the target function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 51,
      "context" : "[53] proved that if the target function is of the form f(x) = ∫",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "As for the estimation error, please refer to [32] for an extensive review, which introduces various estimation error bounds based on VC-dimension, flat-shattering dimension, pseudo dimension and so on.",
      "startOffset" : 45,
      "endOffset" : 49
    } ],
    "year" : 2015,
    "abstractText" : "Recently diversity-inducing regularization methods for latent variable models (LVMs), which encourage the components in LVMs to be diverse, have been studied to address several issues involved in latent variable modeling: (1) how to capture long-tail patterns underlying data; (2) how to reduce model complexity without sacrificing expressivity; (3) how to improve the interpretability of learned patterns. While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing. In this paper, we aim to bridge this gap and analyze how the mutual angular regularizer (MAR) affects the generalization performance of supervised LVMs. We use neural network (NN) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}