{
  "name" : "1609.08397.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalization Error Bounds for Optimization Algorithms via Stability",
    "authors" : [ "Qi Meng", "Yue Wang", "Wei Chen", "Taifeng Wang", "Zhi-Ming Ma", "Tie-Yan Liu" ],
    "emails" : [ "qimeng13@pku.edu.cn", "11271012@bjtu.edu.cn", "tie-yan.liu}@microsoft.com", "mazm@amt.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "( log 1/δ√\nn + ρ(T )\n) with\nprobability 1− δ). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM). Specifically, given a training dataset, the goal of R-ERM is to learn a model from a hypothesis space by minimizing the regularized empirical risk defined as the average loss on the training data plus a regularization term.\nIn most cases, it is hard to achieve an exact minimization of the objective function since the problem might be too complex to have a closed-form solution. Alternatively, we seek an approximate minimization by using some optimization algorithms. Widely used optimization algorithms include the first-order methods such as gradient descent (GD),\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nstochastic gradient descent (SGD), stochastic variance reduction (SVRG) (Johnson and Zhang 2013), and the secondorder methods such as Newton’s methods (Nocedal and Wright 2006) and quasi-Newton’s methods (Nocedal and Wright 2006). In this paper, for ease of analysis and without loss of generality, we will take GD, SGD and SVRG as examples. GD calculates the gradient of the objective function at each iteration and updates the model towards the direction of negative gradient by a constant step size. It has been proved that, if the step size is not very large, GD can achieve a linear convergence rate (Nesterov 2013). SGD exploits the additive nature of the objective function in R-ERM, and randomly samples an instance at each iteration to calculate the gradient. Due to the variance introduced by stochastic sampling, SGD has to adopt a decreasing step size in order to guarantee the convergence, and the corresponding convergence rate is sublinear in expectation (Rakhlin, Shamir, and Sridharan 2011). In order to reduce the variance in SGD, SVRG divides the optimization process into multiple stages and updates the model towards a direction of the gradient at a randomly sampled instance regularized by a full gradient over all the instances. In this way, SVRG can achieve linear convergence rate in expectation with a constant step size (Johnson and Zhang 2013).\nWhile the aforementioned convergence analysis can characterize the behaviors of the optimization algorithms in the training process, what the machine learning community cares more is the generalization performance of the learned model on unseen test data. 1 As we know, the generalization error of a machine learning algorithm can be decomposed into three parts, the approximation error, the estimation error, and the optimization error. The approximation error is caused by the limited representation power of the hypothesis space F ; the estimation error (which measures the difference between the empirical risk and the expected risk) is caused by the limited amount of training data (Vapnik and Kotz 1982)(Bousquet and Elisseeff 2002); and the optimization error (which measures the difference between expected risks of the model obtained by the optimization algorithm af-\n1Under a related but different setting, i.e., the data instances are successively generated from the underlying distribution, people have proven regret bounds for algorithms like SGD (Kakade and Tewari 2009; Cesa-Bianchi, Conconi, and Gentile 2004) and SVRG (Frostig et al. 2015).\nar X\niv :1\n60 9.\n08 39\n7v 1\n[ st\nat .M\nL ]\n2 7\nSe p\n20 16\nter T iterations and the true optimum of the regularized empirical risk) is caused by the limited computational power. In (Bousquet and Bottou 2008), Bottou and Bousquet proved generalization error bounds for GD and SGD based on VCdimension (Kearns and Ron 1999), which unavoidably are very loose in their nature.2 The goal of our paper is to develop more general and tighter generalization error bounds for the widely used optimization algorithms in R-ERM.\nTo this end, we leverage stability (Bousquet and Elisseeff 2002) as a tool and obtain the following results:\n(1) For convex objective functions, we prove that, the generalization error of an optimization algorithm can be upper bounded by a quantity related to its stability plus its convergence rate in expectation. Specifically, the generalization error bound is in the order of O(1/n + Eρ(T )), where ρ(T ) is the optimization convergence error and T is the number of iterations. This indicates that along with the optimization process on the training data, the generalization error will decrease, which is consistent with our intuition.\n(2) For convex objective functions, we can also obtain a high probability bound for the generalization error. In particular, the bound is in the order of O ( log 1/δ√\nn + ρ(T ) ) with probability at least 1 − δ. That is, if an algorithm has a high-probability convergence bound, we can get a highprobability generalization error bound too, and our bound is sharper than those derived in the previous literature.\n(3) Based on our theorems, we analyze the time for different optimization algorithms to achieve the same generalization error, given the same amount of training data. We find that SVRG outperforms GD and SGD in most cases, and although SGD can quickly reduce the test error at the beginning of the training process, it slows down due to the decreasing step size and can hardly obtain the same test error as GD and SVRG when n is large.\n(4) Some of our theoretical results can be extended to the nonconvex objective functions, with some additional assumptions on the distance between the global minimizer and the stationary local minimizers.\nWe have conducted experiments on linear regression, logistic regression, and fully-connected neural networks to verify our theoretical findings. The experimental results are consistent with our theory: (1) when the training process goes on, the test error decreases; (2) in most cases, SVRG has better generalization performance than GD and SGD."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section, we briefly introduce the R-ERM problem, and popular optimization algorithms to solve it."
    }, {
      "heading" : "2.1 R-ERM and its Stability",
      "text" : "Suppose that we have a training set S = {z1 = (x1, y1), ..., zn = (xn, yn)} with n instances that are i.i.d. sampled from Z = X × Y according to an unknown distribution P . The goal is to learn a good prediction model\n2In (Hardt, Recht, and Singer 2015), Hardt et.al studied convex risk minimization via stability, but they did not consider the influence of hypothesis space and the tradeoff between approximation error and estimation error.\nf ∈ F : X → Y , whose prediction accuracy at instance (x, y) is measured by a loss function l(y, f(x)) = l(f, z). Different learning tasks may use different loss functions, such as the least square loss (f(x) − y)2 for regression, and the logistic loss log (1 + e−yf(x)) for classification. We learn the prediction model from the training set S, and will use this model to give predictions for unseen test data.\nR-ERM is a very common way to achieve the above goal. Given loss function l(f, z), we aim to learn a model f∗ that minimizes the expected risk\nR(f) = Ez∼P l(f, z).\nBecause the underlying distribution P is unknown, in practice, we learn the prediction model by minimizing the regularized empirical risk over the training instances, which is defined as below,\nRrS(f) = 1\nn n∑ i=1 l(f, zi) + λN(f). (1)\nHere, the regularization term λN(f) helps to restrict the capacity of the hypothesis space F to avoid overfitting. In this paper, we consider N(f) as a norm in a reproducing kernel Hilbert space (RKHS): N(f) = ‖f‖2k where k refers to the kernel (Wahba 2000).\nAs aforementioned, our goal is expected risk minimization but what we can do in practice is empirical risk minimization instead. The gap between these two goals is measured by the so-called estimation error, which is usually expressed in the following way: the expected risk is upper bounded by the empirical risk plus a quantity related to the capacity of the hypothesis space (Vapnik and Kotz 1982)(Bousquet and Bottou 2008). One can choose different ways to measure the capacity of the hypothesis space, and stability is one of them, which is proved to be able to produce tighter estimation error bound than VC dimension (Kearns and Ron 1999). There has been a venerable line of research on estimation error analysis based on stability, dated back more than thirty years ago (Bousquet and Elisseeff 2002; Devroye and Wagner 1979; Kearns and Ron 1999; Mukherjee et al. 2006; Shalev-Shwartz et al. 2010). The landmark work by Bousquet and Elisseeff (Bousquet and Elisseeff 2002) introduced the following definitions of uniform loss stability and output stability. Definition 2.1 (Uniform Loss Stability) An algorithmA has uniform stability β0 with respect to loss function l if the following holds ∀S ∈ Zn,∀j ∈ {1, · · · , n},\n|EA [l(AS , ·)]− EA [l(AS\\j , ·)]| ≤ β0, (2)\nwhere AS , AS\\j are the outputs of algorithm A based on S and S\\j = {z1, · · · , zj−1, zj+1, · · · , zn}, respectively. Definition 2.2 (Output Stability) An algorithm has output stability β1 if the following holds ∀S ∈ Zn,∀j ∈ {1, · · · , n},\n‖AS −AS\\j‖Fc ≤ β1, (3) where ‖ · ‖Fc denotes the norm in hypothesis space Fc.\nFrom the above definitions, we can see that stability measures the change of the loss function or the produced model of a given learning algorithm if one instance in the training\nset is changed. For example, if the loss function is convex and L-Lipschitz w.r.t. f , the corresponding R-ERM algorithm with regularization term N(f) = ‖f‖2k has stability β0 ≤ L 2K2 2λn and β1 ≤ LK 2λn , where K is the upper bound of the kernel norm (Bousquet and Elisseeff 2002)."
    }, {
      "heading" : "2.2 Optimization Algorithms",
      "text" : "Many optimization methods can be used to solve the R-ERM problem, including the first-order methods such as Gradient Descent (GD) (Nesterov 2013), Stochastic Gradient Descent (SGD) (Rakhlin, Shamir, and Sridharan 2011), and Stochastic Variance Reduction (SVRG) (Johnson and Zhang 2013), as well as the second-order methods such as Newton’s methods (Nocedal and Wright 2006) and quasi-Newton’s methods (Byrd et al. 2016). We will take the first-order methods as examples in this paper, although many of our analysis can be easily extended to other optimization algorithms.\nLet us consider model f parameterized by w. The update rules of GD, SGD, and SVRG are summarized as follows. Gradient Descent (GD)\nwt+1 = wt − η∇RrS(wt). (4)\nStochastic Gradient Descent (SGD)\nwt+1 = wt − ηtg(wt). (5)\nStochastic Variance Reduced Gradient (SVRG)\nvts = g(w t s)−∇RrS(wts) +∇RrS(w̃t−1) (6) wts+1 = w t s − ηvts. (7)\nwhere g(·) is the gradient of ∇RrS(·) at randomly sampled training instances, wts is the output parameter at the s-th iteration in the t-th stage, and w̃t−1 is the final output in stage t− 1.\nWhen the loss function is strongly convex and smooth with respect to the model parameters, GD can achieve linear convergence rate; SGD can only achieve sublinear convergence rate due to the variance introduced by stochastic sampling (but in each iteration, it only needs to compute the gradient over one instance and thus can be much faster in speed); SVRG can achieve linear convergence rate by reducing the variance and in most iterations it only needs to compute the gradient over one instance. 3 When the loss functions are nonconvex w.r.t. the model parameters (e.g., neural networks), GD (Nesterov 2013), SGD (Ghadimi and Lan 2013), and SVRG (Reddi et al. 2016) still have convergence properties (although regarding a different measure of convergence). For ease of reference, we summarize the convergence rates of the aforementioned optimization algorithms in both convex and nonconvex cases in Table 1."
    }, {
      "heading" : "3 Generalization Analysis",
      "text" : "In this section, we will analyze the generalization error for optimization algorithms by using stability as a tool.\n3The second-order methods can get quadratic convergence rate (Nocedal and Wright 2006). However, as compared with the firstorder methods, the computation complexity of the second-order methods could be much higher due to the calculation of the secondorder information.\nFirstly, we introduce the definition of generalization error and its decomposition. Then, we prove the generalization error bounds of optimization algorithms in both convex and nonconvex cases. The proof details of all the lemmas and theorems are placed in the supplementary materials due to space limitation."
    }, {
      "heading" : "3.1 Generalization Error and its Decomposition",
      "text" : "As we mentioned in Section 2, R-ERM minimizes the regularized empirical risk, i.e.,\nf∗S,r := argminf∈FR r S(f) (8)\nas an approximation of the expected risk minimization:\nf∗ := argminfR(f). (9)\nDenote the empirical risk RS(f) = 1n ∑n i=1 l(f, zi). It is clear that, the minimization of RrS(f) in F is equivalent to the minimization of RS(f) in Fc = {f ∈ F , N(f) ≤ c} for some constant c. That is,\nf∗S,r = f ∗ S,Fc := argminf∈FcRS(f). (10)\nDenote the minimizer of the expected risk R(f) in the hypothesis space Fc as f∗Fc , i.e.,\nf∗Fc := argminf∈FcR(f). (11)\nIn many practical cases, neither f∗S,r nor f ∗ S,Fc has a closed\nform. What people do is to implement an iterative optimization algorithmA to produce the prediction model. We denote the output model of algorithm A at iteration T over n training instances as fT (A,n,Fc). We use generalization error to denote the difference between the expected risk of this learnt model and the optimal expected risk, as follows,\nE(A,n,Fc, T ) = R(fT (A,n,Fc))−R(f∗). (12) As known, the generalization error can be decomposed\ninto the three components,\nE(A,n,Fc, T ) (13) = R(fT )−R(f∗S,Fc) +R(f ∗ S,Fc)−R(f ∗ Fc) (14)\n+R(f∗Fc)−R(f ∗)\n:= Eopt(A,n,Fc, T ) + Eest(n,Fc) + Eapp(Fc). (15)\nThe item Eapp(Fc) := R(f∗Fc) − R(f ∗), is called approximation error, which is caused by the limited representation power of the hypothesis spaceFc. With the hypothesis space increasing, (i.e., c is increasing), the approximation error will decrease. The item Eest(n,Fc) := R(f∗S,Fc) − R(f ∗ Fc), is called estimation error, which is caused by the limited amount of the training data (which leads to the gap between the empirical risk and the expected risk). It will decrease with the increasing training data size n, and the decreasing capacity of the hypothesis space Fc. The item Eopt(A,n,Fc, T ) := R(fT )−R(f∗S,Fc), is called optimization error, which measures the sub-optimality of the optimization algorithms in terms of the expected risk. It is caused by the limited computational resources. 4\n4For simplicity, we sometimes denote fT (A,n,Fc), E(A,n,Fc, T ), Eapp(Fc), Eest(n,Fc), Eopt(A,n,Fc, T ) as fT , E , Eapp, Eest, Eopt, respectively.\nPlease note that, the optimization error under our study differs from the target in the conventional convergence analysis of optimization algorithms. In the optimization community, the following two objectives\nρ0(T ) = RS(fT )−RS(f∗S,Fc); ρ1(T ) = ‖fT − f ∗ S,Fc‖ 2 Fc (16)\nare commonly used in convex cases, and\nρ2(T ) = ‖∇RrS(fT )‖2 (17)\nis commonly used in nonconvex cases. To avoid confusion, we call them convergence error and their corresponding upper bounds convergence error bounds. Please note although convergence error is different from optimization error, having a convergence error bound plays an important role in guaranteeing a generalization error bound. In the following subsections, we will prove the generalization error bound for typical optimization algorithms, by using the stability techniques, based on their convergence error bounds."
    }, {
      "heading" : "3.2 Expected Generalization Bounds for Convex Case",
      "text" : "The following theorem gives an expected generalization error bounds in the convex case.\nTheorem 3.1 Consider an R-ERM problem, if the loss function is L-Lipschitz continuous, γ-smooth, and convex with respect to the prediction output vector, we have\nES,AE ≤ Eapp + 2β0 + ES,Aρ0(T ) + γES,Aρ1(T )\n2\n+ √ ES,Aρ1(T ) ( L2\n2n + 6Lγβ1\n) , (18)\nwhere β0, β1 are the uniform stability and output stability of the R-ERM process as defined in 2.1 and 2.2, ρ0(T ) and ρ1(T ) are the convergence errors defined in Eqn 16.\nFrom Theorem 3.1, we can see that the generalization error can be upper bounded by the stability β0 and β1, the convergence errors of the optimization algorithms ρ0(T ) and ρ1(T ), and the well-studied approximation error (Vapnik and Vapnik 1998). As the training process goes on, both Eρ0(T ) and Eρ1(T ) will decrease. Therefore, the expected generalization error will decrease too. This is consistent with our intuition. Better optimizations will lead to better expected generalization performance.\nIn order to prove Theorem 3.1, we need the following two lemmas, whose proofs are placed in the supplementary materials due to space restrictions.\nLemma 3.2 For R-ERM problems, we have ∀j ∈ {1, · · · , n}:\nES [ R(f∗S,Fc)−RS(f ∗ S,Fc) ] = ES [ l(f∗S,Fc , z ′ j)− l(f∗Sj ,Fc , z ′ j) ]\n(19) and\nES [∇R(f∗S,Fc)−∇RS(f ∗ S,Fc)] = ES [∇f l(f ∗ S,Fc , z ′ j)−∇f l(f∗Sj ,Fc , z ′ j)],\n(20) where Sj = {z1, · · · , zj−1, z′j , zj+1, · · · , zn}, and f∗Sj ,Fc is the minimizer of RSj (f) in Fc . Lemma 3.3 Assume that the loss function is L-Lipschitz and γ-smooth w.r.t. the prediction output vector, we have\nES [∇R(f∗S,Fc)−∇RS(f ∗ S,Fc)]\n2 ≤ L 2\n2n + 6Lγβ1. (21)\nProof Sketch of Theorem 3.1: Step 1: Since the loss function is convex and γ-smooth w.r.t. f , we can get that R(f) is γ-smooth and RS(f) is convex w.r.t f . We decompose Eopt as below:\nEopt ≤ ( ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) )T (fT − f∗S,Fc)\n+RS(fT )−RS(f∗S,Fc) + γ 2 ‖fT − f∗S,Fc‖ 2 Fc ,\nWe can use ρ0(T ), ρ1(T ) and Lemma 3.3, to get an upper bound of ES,AEopt.\nStep 2: Since RS(f∗S,Fc) ≤ RS(f ∗ Fc), we have\nEest ≤ [ R(f∗S,Fc)−RS(f ∗ S,Fc) ] + [RS(f ∗ Fc)−R(f ∗ Fc)] .\nWe have ES [ RS(f ∗ Fc)−R(f ∗ Fc) ] = 0. By using Lemma 3.2,\nwe can bound ESEest. By combining the upper bounds of ES,AEopt and ESEopt, we can get the results.\nAfter proving the general theorem, we consider a special case - an R-ERM problem with kernel regularization term λ‖f‖2k. In this case, we can derive the concrete expressions of the stability and convergence error. In particular, β0 = O(1/λn), β1 = O(1/λn) and ρ1(T ) is equivalent to ‖wT − w∗S,r‖2. If the loss function is convex and smooth w.r.t. parameter w, RrS(w) with N(f) = ‖f‖2k is strongly convex and smooth w.r.t w. In this case, ρ0(T ) dominates ρ1(T ), i.e., ρ0(T ) is larger than ρ1(T ) w.r.t the order of T . Therefore, we can obtain the following corollary.\nCorollary 3.4 For an R-ERM problem with a regularization term λ‖f‖2k, under the same assumptions in Theorem 3.1, and further assuming that the loss function is convex and smooth w.r.t parameter w, we have\nES,AE ≤ Eapp +O ( 1\nλn + ES,Aρ0(T )\n) . (22)"
    }, {
      "heading" : "3.3 High-Probability Generalization Bounds for Convex Case",
      "text" : "The following theorem gives a high-probability bound of E in the convex case. Due to space limitation, we put the proof in the supplementary materials. Theorem 3.5 For an R-ERM problem, if the loss function is L-Lipschitz continuous, γ-smooth and convex with respect to the prediction output vector, and 0 ≤ l(f∗S,Fc , z) ≤ M for arbitrary z ∈ Z and S ∈ Zn, then with probability at least 1− δ, we have\nE ≤ Eapp + 2β0 + ρ0(T ) + γ\n2 ρ1(T ) + 2γβ1\n√ ρ1(T )\n+ ( 4nβ0 + 2M + (4nγβ1 + L) √ ρ1(T ) )√ ln 4/δ 2n .\nThe high-probability bound is consistent with the expected bound given in the previous subsection. That is, the highprobability generalization bound will also decrease along with the training process. In addition, we can also get a corollary for the special case of R-ERM with kernel regularization.\nCorollary 3.6 For an R-ERM problem with kernel regularization term λ‖f‖2k, under the same assumptions in Theorem 3.5, and further assuming that the loss function is convex and smooth w.r.t parameter w, we have, with probability at least 1− δ,\nE ≤ Eapp +O\n(√ log 1/δ\nn + ρ0(T )\n) .\nRakhlin et.al. (Rakhlin, Shamir, and Sridharan 2011) proved a high-probability convergence rate for SGD. For GD, the training process is deterministic. By plugging the order of β0 and β1 in SGD and GD, we have the following corollary.\nCorollary 3.7 For an R-ERM problem with kernel regularization, under the assumptions in Corollary 3.6, with probability at least 1−δ, the generalization error of SGD and GD can be upper bounded as follows,\nESGD ≤ Eapp +O\n(√ ln 1/δ\nn\n) +O ( κ2 log(log(T )/δ)\nT\n) ;\nEGD ≤ Eapp +O\n(√ ln 1/δ\nn\n) +O ( e−κT ) ,\nwhere κ is the condition number."
    }, {
      "heading" : "3.4 Expected Generalization Bounds for Nonconvex Case",
      "text" : "In this subsection, we consider the case in which the loss function is convex w.r.t. the prediction output vector, but non-convex w.r.t. the model parameter. This case can cover deep neural networks, which are state-of-the-art AI techniques nowadays.\nFor the non-convex case, the definition of convergence error is a little different, as shown by Eq. (17). It measures whether the solution is close to a critical point, which is defined and further categorized as follows.\nDefinition 3.8 Consider the objectiveRrS and parameterw. If ∇RrS(w) = 0, we say w is a critical point of RrS; if ∇RrS(w) has at least one strictly negative eigenvalue, we say w is a strict saddle point. If each critical point w is either a local minimum or a strict saddle point, we say that RrS satisfies the strict saddle property.\nThe following theorem gives the expected generalization error bound for non-convex cases under the widely used assumptions (Lian et al. 2015; Reddi et al. 2016; Lee et al. 2016). Theorem 3.9 If RrS is µ-strongly convex in the 0- neighborhood of arbitrary local minimum wloc, satisfies strict saddle point property, L- Lipschitz continuous, γ-smooth and continuously twice differential w.r.t the model parameter w, and the loss function is convex w.r.t f , then we have\nES,AE ≤ Eapp+2β0+R(wloc)−R(w∗S,Fc)+ L\nµ\n√ min\nt=1,··· ,T ES,Aρ2(t),\nwhere T ≥ T1 and T1 is the number of iterations to achieve mint=1,··· ,T1 ES,A [ρ2(t)] ≤ γ2 20.\nSimilarly to the convex case, from the above theorem we can see that with the training process going on, the generalization error in the nonconvex case will also decrease. In addition, we can also derive specific bound for the R-ERM with kernel regularization."
    }, {
      "heading" : "4 Sufficient Training and Optimal Generalization Error",
      "text" : "In this section, we make further discussions on the generalization bound. In particular, we will explore the sufficient training iterations, and the optimal generalization error given the training data size.\nAs shown in Section 3, the generalization error bounds consist of an estimation error related to the training data size n and an optimization error related to the training iteration T . Given a machine learning task with fixed training size n, at the early stage of the training process (i.e., T is relatively small), the optimization error will dominate the generalization error; when T becomes larger than a threshold, the optimization error will decrease to be smaller than the estimation error (i.e. O(1/n)), and then the estimation error will dominate the generalization error. We call this threshold sufficient training iteration and the corresponding training time sufficient training time. The generalization error with the optimization algorithm sufficiently trained is called optimal generalization error. Given the generalization error bound, we can derive the sufficient training iteration/time. For ease of analysis, we list the sufficient training iteration/time of GD, SGD, and SVRG for both convex and nonconvex cases in Table 2.\nFrom Table 2, we have the following observations. For the convex case, when the condition number κ is much smaller than n, GD, SGD and SVRG have no big differences from each other in their sufficient training iterations; when κ is comparable with n, e.g., κ = O( √ n), 5 the sufficient train-\n5In some cases, κ is related to the regularization coefficient λ and λ is determined by the data size n (Vapnik and Vapnik 1998)(Shamir, Srebro, and Zhang 2014).\ning time for GD, SGD and SVRG is O(n √ nd lnn), O(n2d), O(nd lnn), respectively. That is, SVRG corresponds to a shorter sufficient training time than GD and SVRG. For the non-convex case, if 0 ≤ O(1/n), which is more likely to happen for small data size n, the first term in the sufficient training time dominates, and it is fine to terminate the training process at T = T1. SVRG requires shorter training time than GD and SGD by at least an order of O(n1/3) and O(n4/3), respectively. If 0 is larger than O(1/n), which is more likely to happen for large data size n, the sufficient training time for GD, SGD, and SVRG is O(n3), O(n4), and O(n8/3), respectively. In this case, SVRG requires shorter training time than GD and SGD by an order of O(n1/3) and O(n4/3), respectively."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we report experimental results to validate our theoretical findings. We conducted experiments on three tasks: linear regression, logistic regression, and fully connected neural networks, whose objective functions are least square loss, logistic loss, and cross-entropy loss respectively, plus an L2 regularization term with λ = 1/ √ n. The first two tasks are used to verify our results for convex problems, and the third task is used to verify our theory on nonconvex problems. For each task, we report three figures. The horizontal axis of each figure corresponds to the number of data passes and the vertical axis corresponds to the training loss, test loss, and log-scaled test loss, respectively. For linear regression, we independently sample data instances with size n = 40000 from a 100−dimension Gaussian distribution. We use half of them as the training data and the other as the test data. We set the step size for GD, SGD, SVRG as 0.032, 0.01/t and 0.005, respectively, according to the smoothness and strong-convexity coefficients. For our simulated data, the condition number κ ≈ 116. The results are shown in Fig.1(c)1(a)1(b). For logistic regression, we conduct binary classification on benchmark dataset rcv1. We set the step sizes for GD, SGD, SVRG as 400, 200/t and 1, respectively. The results are shown in Fig. 1(f)1(e)1(d). For neural networks, we work on a model with one fully connected hidden layer of 100 nodes, ten softmax output nodes, and sigmoid activation (Johnson and Zhang 2013). We tune the step size for GD, SGD, SVRG and eventually choose 0.03, 0.25/ √ t and 0.001, respectively, which correspond to the best performances in our experiments. The inner loop size for SVRG for convex problems is set as 2n and that for nonconvex problem is set as 5n. The results are shown in Fig.1(i)1(g)1(h).\nFrom the results for all the three tasks, we have the following observations. (1) As training error decreases, the test er-\nror also decreases. (2) According to Fig.1(c), SVRG is faster than GD by a factor ofO(κ) and faster than SGD by a factor of more than O(κ). (3) According to Fig. 1(c)1(f)1(i), SGD is the slowest although it is fast in the beginning, which is consistent with our discussions in Section 4.\nBy comparing the results of logistic regression and linear regression, we have the following observations. (1) The test error for logistic regression converges after fewer rounds of data passes than linear regression. This is because the condition number κ for logistic regression is smaller than linear regression. (2) SVRG is faster than GD and SGD but the differences between them are less significant for logistic regression, due to a smaller κ. As compared to the results for logistic regression and linear regression, we have the following observations on the results of neural networks. (1) The convergence rate is slower and the accuracy is lower. This is because of the nonconvexity and the gap between global optimum and local optimum. (2) SVRG is faster than GD and SGD but the differences between them are not as significant as in the convex cases, which is consistent with our discussions in Section 4 by considering the data size of CIFAR 10."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have studied the generalization error bounds for optimization algorithms to solve R-ERM prob-\nlems, by using stability as a tool. For convex problems, we have obtained both expected bounds and high-probability bounds. Some of our results can be extended to the nonconvex case. Roughly speaking, our theoretical analysis has shown: (1) Along with the training process, the generalization error will decrease; (2) SVRG outperforms GD and SGD in most cases. We have verified the theoretical findings by using experiments on linear regression, logistic regression and fully connected neural networks. In the future, we plan to study the stability of R-ERM with other regularization terms, e.g., the L1 regularizer, which is usually associated with non-smooth optimization methods."
    }, {
      "heading" : "7 Appendices",
      "text" : ""
    }, {
      "heading" : "7.1 Proofs of Lemma 3.2, Lemma 3.3 and",
      "text" : "Theorem 3.1 Lemma 3.2: For R-ERM problems, we have ∀j ∈ {1, · · · , n}: ES [ R(f∗S,Fc)−RS(f ∗ S,Fc) ] = ES [ l(f∗S,Fc , z ′ j)− l(f∗Sj ,Fc , z ′ j) ]\n(23) and\nES [∇R(f∗S,Fc)−∇RS(f ∗ S,Fc)]\n= ES [∇f l(f∗S,Fc , z ′ j)−∇f l(f∗Sj ,Fc , z ′ j)], (24)\nwhere Sj = {z1, · · · , zj−1, z′j , zj+1, · · · , zn}, and f∗Sj ,r is the minimizer of RrSj .\nProof: The proofs of Eq.(23) and Eq.(24) are very similar, we only prove Eq.(24). ES [∇RS(f∗S,Fc)] (25)\n= 1\nn n∑ j=1 ES [∇f l(f∗S,Fc , zj)] (26)\n= 1\nn n∑ j=1 ES,z′j [∇f l(f ∗ S,Fc , zj)] (27)\n= 1\nn n∑ j=1 ES,z′j [∇f l(f ∗ Sj ,Fc , z ′ j)] (28)\nUsing the definition of R, we can get ES∇R(f∗S,Fc) = ES,z∇f l(f ∗ S,Fc , z) = ES,z′j∇f l(f ∗ S,Fc , z ′ j). (29) By combining Eq.(28) and (29), we can get the results.\nLemma 3.3 : Assume that the loss function is L-Lipschitz and γ-smooth w.r.t. the prediction output vector, we have\nES [∇R(f∗S,Fc)−∇RS(f ∗ S,Fc)]\n2 ≤ L 2\n2n + 6Lγβ1. (30)\nProof: The proof is following Lemma 9 and Lemma 25 in (Bousquet and Elisseeff 2002). We just need to relpalce M which is the upper bound of the loss function by the upper bound of the derivative of loss function ∇f l(f, z), and replace the lip By the assumption that the loss function isL−Lipschitz continous and γ−smooth w.r.t. the prediction output vector, we can get that ∇f l(f, z) ≤ L and ∇f l(f, z) is γ− Lipschitz continous. Following Lemma 9 in (Bousquet and Elisseeff 2002), we have\nES [∇R(f∗S,Fc)−∇RS(f ∗ S,Fc)] 2 (31)\n≤ L 2\n2n + 3LES,z′j [|∇l(f\n∗ S,Fc , zj)−∇l(f ∗ Sj ,Fc , zj)|](32)\n≤ L 2\n2n + 3LγES,z′j [‖f\n∗ S,Fc − f ∗ Sj ,Fc‖Fc ] (33)\n≤ L 2\n2n + 3LγES,z′j [‖f\n∗ S,Fc − f ∗ S\\j ,Fc‖Fc ] (34)\n+3LγESj ,z′j [‖f ∗ S,Fc − f ∗ S\\j ,Fc‖Fc ] (35)\n≤ L 2\n2n + 6Lγβ1. (36)\nTheorem 3.1 : Consider an R-ERM problem, if the loss function is L-Lipschitz continuous, γ-smooth, and convex with respect to the prediction output vector, we have\nES,AE ≤ Eapp + 2β0 + ES,Aρ0(T ) + γES,Aρ1(T )\n2\n+ √ ES,Aρ1(T ) ( L2\n2n + 6Lγβ1\n) , (37)\nwhere β0, β1 are the uniform stability and output stability of the R-ERM process as defined in Def.2.1 and Def.2.2, ρ0(T ) and ρ1(T ) are the convergence errors defined in Eqn.(16).\nProof: If l(f, z) is convex and γ-smooth w.r.t. f , we can get that R(f) is γ-smooth and RS(f) is convex. First, we decompose Eopt as follows:\nEopt (38) = R(fT )−R(f∗S,Fc) (39) ≤ ∇R(f∗S,Fc) T (fT − f∗S,Fc) + γ\n2 ‖fT − f∗S,Fc‖ 2 Fc (40)\n= ( ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) )T (fT − f∗S,Fc) (41)\n+∇RS(f∗S,Fc) T (fT − f∗S,Fc) +\nγ 2 ‖fT − f∗S,Fc‖ 2 Fc(42)\n≤ ( ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) )T (fT − f∗S,Fc) (43)\n+RS(fT )−RS(f∗S,Fc) + γ 2 ‖fT − f∗S,Fc‖ 2 Fc ,\nwhere the first inequality is established by using the γsmoothness condition and the third inequality is established by using the convexity condition. Taking expectation w.r.t. S and the optimization algorithm A, we can get\nES,A γ 2 ‖fT − f∗S,Fc‖ 2 = γ 2 ES,Aρ1(T ) (44)\nES,A[RS(fT )−RS(f∗S,Fc)] = ES,Aρ0(T ) (45)\nFor the term ( ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) )T (fT − f∗S,Fc), by\nusing Cauthy-Schwarz inequality, we can get:\nES,A ( ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) )T (fT − f∗S,Fc)\n≤ √ ES,A‖fT − f∗S,Fc‖2ES ( ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) )2 ≤ √ ES,Aρ1(T ) ( L2\n2n + 6Lγβ1\n) (46)\nwhere the second inequality holds according to Lemma 3.3. Next we decompose Eest as follows:\nEest (47) = [ R(f∗S,Fc)−RS(f ∗ S,Fc) ] + [ RS(f ∗ S,Fc)−RS(f ∗ Fc) ]\n+ [RS(f ∗ Fc)−R(f ∗ Fc)] (48) ≤ [ R(f∗S,Fc)−RS(f ∗ S,Fc) ] + [RS(f ∗ Fc)−R(f ∗ Fc)] , (49)\nwhere the second inequality is established because f∗S,Fc is the minimizer of RS restricted to the hypothesis space Fc.\nSince f∗Fc is independent of S, we have ES [ RS(f ∗ Fc)−R(f ∗ Fc) ] = 0. Then by using Eq.(19)\nand the definition of uniform stability, we can get ESEest ≤ ES [ R(f∗S,Fc)−RS(f ∗ S,Fc) ] ≤ ES,z′j [|l(f ∗ S,Fc , z ′ j)− l(f∗Sj ,Fc , z ′ j)|]\n≤ ES,z′j [|l(f ∗ S,Fc , z ′ j)− l(f∗S\\j ,Fc , z ′ j)|]\n+ES,z′j [|l(f ∗ S\\j ,Fc , z ′ j)− l(f∗Sj ,Fc , z ′ j)|]\n≤ 2β0. (50)\nBy combining Ineq.(46) and Ineq.(50), we can get the result in the theorem."
    }, {
      "heading" : "7.2 Proof of Theorem 3.5",
      "text" : "Theorem 3.5: For an R-ERM problem, if the loss function is L-Lipschitz continuous, γ-smooth and convex with respect to the prediction output vector, and 0 ≤ l(f∗S,Fc , z) ≤M for arbitrary z ∈ Z and S ∈ Zn, then with probability at least 1− δ, we have\nE ≤ Eapp + 2β0 + ρ0(T ) + γ\n2 ρ1(T ) + 2γβ1\n√ ρ1(T )\n+ ( 4nβ0 + 2M + (4nγβ1 + L) √ ρ1(T ) )√ ln 4/δ 2n .\nIn order to prove Theorem 3.5, we need to use the following theorem which is proposed by McDiarmid.\nTheorem (McDiarmid,1989): Let S and Si are two data sets which are different at only one point j. Let F : Zn → R be any measurable function which there exists constants cj(j = 1, · · · , n) such that supS∈Zn,z′j∈Z |F (S) − F (S j)| ≤ cj , then PS(F (S)− ESF (S) ≥ ) ≤ e−2 2/ ∑n j=1 c 2 j .\nProof: Firstly, we give the high probability bound for estimation error Eest. As we have the decomposition Ineq. (49), we need to analyze R(f∗S,Fc)−RS(f ∗ S,Fc) and RS(f ∗ Fc)−R(f ∗ Fc). By using Theorem 12 in (Bousquet and Elisseeff 2002), we have with probability at least 1− δ,\nR(f∗S,Fc)−RS(f ∗ S,Fc) ≤ 2β0 + (4mβ0 +M)\n√ ln 1/δ\n2n . (51)\nFor RS(f∗Fc)−R(f ∗ Fc), by using Hoeffding’s inequality, we\ncan get with probability at least 1− δ,\nRS(f ∗ Fc)−R(f ∗ Fc) ≤M\n√ ln 1/δ\n2n (52)\nWe can use Hoeffding’s bound since f∗Fc is independent with the training set S. By combining Ineq.(51) and Ineq.(52), we have with probability at least 1− 2δ,\nEest ≤ 2β0 + (4mβ0 + 2M) √ ln 1/δ\n2n . (53)\nSecondly, we give high probability bound for the term Eopt by using Theorem (McDiarmid, 1989). According to Theorem (McDiarmid, 1989), we need to calculate ES [∇R(f∗S,Fc) − ∇RS(f ∗ S,Fc)], and cj = |∇R(f ∗ S,Fc) −\n∇RS(f∗S,Fc)− ( ∇R(f∗Sj ,Fc)−∇RS(f ∗ Sj ,Fc) ) |.\nFor ∇R(f∗S,Fc)−∇RS(f ∗ S,Fc), we have\nES [∇R(f∗S,Fc)−∇RS(f ∗ S,Fc)] (54) = ES [ ∇f l(f∗S,Fc , z ′ j)−∇f l(f∗Sj ,Fc , z ′ j) ] ≤ 2γβ1.(55)\nWe also have\n|∇R(f∗S,Fc)−∇R(f ∗ S\\j,r )| (56) ≤ ES [ |∇f l(f∗S,Fc , z ′ j)−∇f l(f∗Sj ,Fc , z ′ j)| ] ≤ γ|f∗S,Fc(x)− f ∗ S\\j ,Fc(x)| ≤ γβ1 (57)\nand |∇RS(f∗S,Fc)−∇RS(f ∗ S\\j ,Fc )| ≤ γβ1+ Ln , which yields\n|∇R(f∗S,Fc)−∇R(f ∗ Sj ,Fc)| ≤ 2γβ1 (58)\n|∇RS(f∗S,Fc)−∇RS(f ∗ Sj ,Fc)| ≤ 2γβ1 +\nL n . (59)\nThus we can get cj = 4γβ1+ Ln . By using Theorem (McDiarmid, 1989), we can get that with probability at least 1− δ\n∇R(f∗S,Fc)−∇RS(f ∗ S,Fc) ≤ 2γβ1 + (4nγβ1 + L)\n√ ln 1/δ\n2n .\n(60) By putting Ineq.(60), ρ0(T ) and ρ1(T ) in Ineq.(43), we\nhave with probability at least 1− 2δ,\nEopt ≤ ( 2γβ1 + (4nγβ1 + L) √ ln 1/δ\n2n\n)√ ρ1(T )\n+ρ0(T ) + (γ 2 ) ρ1(T ) (61)\nBy combining Ineq.(53) and Ineq.(61), we have with probability 1− 4δ,\nE ≤ Eapp + 2β0 + (4nβ0 + 2M) √ ln 1/δ\n2n + ρ0(T )\n+ (γ 2 ) ρ1(T ) + ( 2γβ1 + (4nγβ1 + L) √ ln 1/δ 2n )√ ρ1(T )\n≤ Eapp + 2β0 + ρ0(T ) + (γ 2 ) ρ1(T ) + 2γβ1 √ ρ1(T )\n+ ( 4nβ0 + 2M + (4nγβ1 + L) √ ρ1(T ) )√ ln 1/δ 2n\nBy using δ/4 to replace δ, we can get the result."
    }, {
      "heading" : "7.3 Proof of Theorem 3.9",
      "text" : "Theorem 3.9: If RrS is µ-strongly convex in the 0- neighborhood of arbitrary local minimum wloc, satisfies strict saddle point property, L- Lipschitz continuous, γ-smooth and continuously twice differential w.r.t the model parameter w, and the loss function is convex w.r.t f , then we have\nES,AE ≤ Eapp+2β0+R(wloc)−R(w∗S,Fc)+ L\nµ\n√ min\nt=1,··· ,T ES,Aρ2(t),\n(62) where T ≥ T1 and T1 is the number of iterations to achieve mint=1,··· ,T1 ES,A [ρ2(t)] ≤ γ2 20.\nProof: The upper bound for expected estimation error is the same as convex cases since the loss function is convex w.r.t f , i.e., ESEest ≤ 2β0.\nReferring to a recent work of Lee et.al (Lee et al. 2016), GD with a random initialization and sufficiently small constant step size converges to a local minimizer almost surely under the assumptions in Theorem 3.9. Thus, the assumption that RrS is µ-strongly convex in the 0-neighborhood of arbitrary local minimum wloc is easily to be satisfied in sense of ”almost surely”. We decompose mint=1,··· ,T EEopt as\nmin t=1,··· ,T\nE[R(wt)−R(wloc)] + E[R(wloc)−R(w∗S,Fc)]. (63)\nBy the L-Lipschitz condition, we have R(wt) − R(wloc) ≤ L‖wt − wloc‖. Firstly, We need to calculate how many iterations are needed to guarantee that\nmin t=1,··· ,T1\nE‖wt − wloc‖ ≤ 0. (64)\nBy the γ-smooth assumption, we have γ‖wt − wloc‖2 ≥ 〈∇RrS(wt), wt − wloc〉. Thus for wt ∈ B(wloc, 0), we have ‖∇RrS(wt)‖ ≤ γ‖wt − wloc‖ ≤ γ 0. By the continuously twice differential assumption, we can assume that ‖∇RrS(wt)‖ ≤ γ 0 for wt ∈ B(wloc, 0) and ‖∇RrS(wt)‖ ≥ γ 0 for wt /∈ B(wloc, 0) without loss of generality. 6 Therefore mint=1,··· ,T1 E‖∇RrS(wt)‖2 ≤ γ2 20 is a sufficient condition for mint=1,··· ,T1 E‖wt − wloc‖ ≤ 0.\nIf T ≥ T1, by the µ-strongly convex assumption, we have ‖wt−wloc‖2 ≤ 1µ 〈∇R r S(wt), wT−wloc〉 ≤ 1µ‖∇R r S(wt)‖‖wt− wloc‖ for wt ∈ B(wloc, 0), which yields ‖wt − wloc‖ ≤ 1 µ ‖∇RrS(wt)‖. Based on the above discussions, we can get\nmin t=1,··· ,T\nEEopt\n= min t=1,··· ,T E[R(wt)−R(wloc)] + E[R(wloc)−R(wS,F∗c )]\n≤ L min t=1,··· ,T E‖wt − wloc‖+ E[R(wloc)−R(wS,F∗c )]\n≤ L µ min t=1,··· ,T E‖∇RrS(wt)‖+ E[R(wloc)−R(wS,F∗c )]\n≤ L µ\n√ min\nt=1,··· ,T E‖∇RrS(wt)‖2 + E[R(wloc)−R(wS,F∗c )]\n= L\nµ\n√ min\nt=1,··· ,T Eρ2(t) + E[R(wloc)−R(wS,F∗c )],\nwhere T ≥ T1.\n6Otherwise, we can choose 0 small enough to make it satisfied."
    } ],
    "references" : [ {
      "title" : "and Bottou",
      "author" : [ "O. Bousquet" ],
      "venue" : "L.",
      "citeRegEx" : "Bousquet and Bottou 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "and Elisseeff",
      "author" : [ "O. Bousquet" ],
      "venue" : "A.",
      "citeRegEx" : "Bousquet and Elisseeff 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "R",
      "author" : [ "Byrd" ],
      "venue" : "H.; Hansen, S.; Nocedal, J.; and Singer, Y.",
      "citeRegEx" : "Byrd et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "Conconi Cesa-Bianchi", "N. Gentile 2004] Cesa-Bianchi", "A. Conconi", "C. Gentile" ],
      "venue" : "IEEE Transactions on Information Theory 50(9):2050–2057",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2004
    }, {
      "title" : "and Wagner",
      "author" : [ "L. Devroye" ],
      "venue" : "T.",
      "citeRegEx" : "Devroye and Wagner 1979",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "S",
      "author" : [ "R. Frostig", "R. Ge", "Kakade" ],
      "venue" : "M.; and Sidford, A.",
      "citeRegEx" : "Frostig et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Lan",
      "author" : [ "S. Ghadimi" ],
      "venue" : "G.",
      "citeRegEx" : "Ghadimi and Lan 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "Recht Hardt", "M. Singer 2015] Hardt", "B. Recht", "Y. Singer" ],
      "venue" : "arXiv preprint arXiv:1509.01240",
      "citeRegEx" : "Hardt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2015
    }, {
      "title" : "and Zhang",
      "author" : [ "R. Johnson" ],
      "venue" : "T.",
      "citeRegEx" : "Johnson and Zhang 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Tewari",
      "author" : [ "S.M. Kakade" ],
      "venue" : "A.",
      "citeRegEx" : "Kakade and Tewari 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Ron",
      "author" : [ "M. Kearns" ],
      "venue" : "D.",
      "citeRegEx" : "Kearns and Ron 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Gradient descent converges to minimizers",
      "author" : [ "B. Recht" ],
      "venue" : "University of California,",
      "citeRegEx" : "Recht,? \\Q2016\\E",
      "shortCiteRegEx" : "Recht",
      "year" : 2016
    }, {
      "title" : "Asynchronous parallel stochastic gradient for nonconvex optimization",
      "author" : [ "Lian" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lian,? \\Q2015\\E",
      "shortCiteRegEx" : "Lian",
      "year" : 2015
    }, {
      "title" : "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization",
      "author" : [ "Mukherjee" ],
      "venue" : null,
      "citeRegEx" : "Mukherjee,? \\Q2006\\E",
      "shortCiteRegEx" : "Mukherjee",
      "year" : 2006
    }, {
      "title" : "and Wright",
      "author" : [ "J. Nocedal" ],
      "venue" : "S.",
      "citeRegEx" : "Nocedal and Wright 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647",
      "author" : [ "Shamir Rakhlin", "A. Sridharan 2011] Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : null,
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2011
    }, {
      "title" : "S",
      "author" : [ "Reddi" ],
      "venue" : "J.; Hefny, A.; Sra, S.; Póczós, B.; and Smola, A.",
      "citeRegEx" : "Reddi et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shalev-Shwartz" ],
      "venue" : "Journal of Machine Learning Research 11(Oct):2635–2670",
      "citeRegEx" : "Shalev.Shwartz,? \\Q2010\\E",
      "shortCiteRegEx" : "Shalev.Shwartz",
      "year" : 2010
    }, {
      "title" : "Communication-efficient distributed optimization using an approximate newton-type method",
      "author" : [ "Srebro Shamir", "O. Zhang 2014] Shamir", "N. Srebro", "T. Zhang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "and Kotz",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "S.",
      "citeRegEx" : "Vapnik and Kotz 1982",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "and Vapnik",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "V.",
      "citeRegEx" : "Vapnik and Vapnik 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "G",
      "author" : [ "Wahba" ],
      "venue" : "2000. An introduction to model building with reproducing kernel hilbert spaces. Statistics Department TR",
      "citeRegEx" : "Wahba 2000",
      "shortCiteRegEx" : null,
      "year" : 1020
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data. In this paper, we investigate on this issue, by using stability as a tool. In particular, we decompose the generalization error for R-ERM, and derive its upper bound for both convex and non-convex cases. In convex cases, we prove that the generalization error can be bounded by the convergence rate of the optimization algorithm and the stability of the R-ERM process, both in expectation (in the order of O(1/n) + Eρ(T )), where ρ(T ) is the convergence error and T is the number of iterations) and in high probability (in the order of O ( log 1/δ √ n + ρ(T ) ) with probability 1− δ). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings.",
    "creator" : "LaTeX with hyperref package"
  }
}