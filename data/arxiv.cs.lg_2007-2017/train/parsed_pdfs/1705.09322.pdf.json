{
  "name" : "1705.09322.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convergent Tree-Backup and Retrace with Function Approximation",
    "authors" : [ "Ahmed Touati" ],
    "emails" : [ "ahmed.touati@umontreal.ca", "pbacon@cs.mcgill.ca", "dprecup@cs.mcgill.ca", "vincentp@iro.umontreal.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms, compatible with accumulating or Dutch traces, using a novel methodology based on proximal methods. In addition to convergence proofs, we provide sample-complexity bounds."
    }, {
      "heading" : "1 Introduction",
      "text" : "Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner). Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al. (2011). Therefore, the design and analysis of off-policy algorithms using all the features of reinforcement learning, e.g. bootstrapping, multi-step updates (eligibility traces), and function approximation has been explored extensively over three decades. While off-policy learning and function approximation have been understood in isolation, their combination with multi-steps bootstrapping produces a so-called deadly triad (Sutton and Barto, 2017), i.e., many algorithms in this category are unstable.\nA convergent approach to this triad is provided by importance sampling, which “bends\" the behavior policy distribution onto the target one (Precup, 2000; Precup et al., 2001). However, as the length of the trajectories increases, the variance of importance sampling corrections tends to become very large. An alternative approach which was developed for tabular representations of the value function is the tree backup algorithm (Precup, 2000) which, remarkably, does not rely on importance sampling directly. Tree Backup has recently been revisited by (Munos et al., 2016), who used its intuitions to develop the Retrace(λ) algorithm.\nar X\niv :1\n70 5.\n09 32\n2v 1\n[ cs\n.L G\n] 2\n5 M\nBoth Tree Backup and Retrace(λ) were only shown to converge with a tabular value function representation, and whether they would also converge with function approximation was an open question, which we tackle in this paper.\nFirst, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(λ), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample). Insights gained from this analysis allow us to derive a new gradient-based algorithm which provably converges to the right solution. Instead of adapting the blueprint from gradient-based temporal difference learning Sutton et al. (2009b), we rely on the primal-dual saddle point formulation of Liu et al. (2015), which also allows us to provide sample complexity bounds. Our algorithm can be implemented with both classical accumulating traces (Sutton and Barto, 1998) as well as Dutch traces (van Hasselt et al., 2014). We also provide empirical evidence of its good performance."
    }, {
      "heading" : "2 Background and notation",
      "text" : "In reinforcement learning, an agent interacts with its environment, assumed to be a discounted Markov Decision Process (S,A, γ, P, r) with state space S , action spaceA, discount factor γ ∈ [0, 1), transition probabilities P : S×A → (S → [0, 1]) mapping state-action pairs to distributions over next states, and reward function r : (S×A)→ R. For simplicity, we assume the state and action space are finite, but our analysis can be extended to the countable or continuous case. We denote by π(a | s) the probability of choosing action a in state s under the policy π : S → (A → [0, 1]). The action-value function for policy π, denoted Qπ : S ×A → R, represents the expected sum of discounted rewards along the trajectories induced by the MDP and π: Qπ(s, a) = E [ ∑∞ t=0 γ\ntrt | (s0, a0) = (s, a), π]. Qπ can be obtained as the fixed point of the Bellman operator on action-value functions T πQ = r + γPπQ where r is the expected immediate reward and Pπ is defined as:\n(PπQ)(s, a) := ∑ s′∈S ∑ a′∈A P (s′ | s, a)π(a′ | s′)Q(s′, a′) .\nIn this paper, we are concerned with the policy evaluation problem (Sutton and Barto, 1998) under model-free off-policy learning. That is, we will evaluate a target policy π using sampled trajectories (i.e. sequences of states, actions and rewards) drawn by following a different behavior policy µ.\nIn order to obtain generalization between different state-action pairs, Qπ should be represented in a functional form. We focus on linear function approximation of the form:\nQ(s, a) := θ>φ(s, a) ,\nwhere θ ∈ Θ ⊂ Rd is a weight vector and φ : S ×A → Rd is a feature map from a state-action pairs to a given d-dimensional feature space.\nOff-policy learning Munos et al. (2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(λ)π (Harutyunyan et al., 2016) and Tree-backup (TB(λ)) (Precup, 2000). It was shown that all these methods in fact share the following general form of the λ-return (Sutton and Barto, 2017) for some coefficients κi:\nGλk := Q(sk, ak) + ∞∑ t=k (λγ)t−k\n( t∏\ni=k+1\nκi ) (rt + γEπQ(st+1, ·)−Q(st, at))\n= Q(sk, ak) + ∞∑ t=k (λγ)t−k\n( t∏\ni=k+1\nκi ) δt ,\nwhere EπQ(st+1, .) := ∑ a∈A π(a | st+1)Q(st+1, a) and δt = rt + γEπQ(st+1, .) − Q(st, at) is the temporal-difference (TD) error. The coefficients κi determine how the TD errors would be scaled in order to correct for the discrepancy between target and behavior policies. From this unified representation, Munos et al. (2016) derived the Retrace(λ) algorithm. Both TB(λ) and Retrace(λ) consider this form of return, but set κi differently. The TB(λ) updates correspond to the choice κi = π(ai | si) while Retrace(λ) sets κi = min ( 1, π(ai | si)µ(ai | si) ) , which is intended to allow learning from full returns when the target and behavior policies are very close. The importance sampling\napproach (Precup, 2000) converges in the tabular case, as it warps the behavior data distribution to the distribution that would be induced by the target policy π, but it also suffers from high variance. As for Q(λ)π , the behavior and target policies must be sufficiently close to guarantee convergence in the tabular case.\nThe analysis provided in this paper concerns TB(λ) and Retrace(λ), which are convergent in the tabular case, but have not been analyzed in the function approximation case. We start by noting that the Bellman operator 1 R underlying these these algorithms can be written in the following form:\n(RQ)(s, a) := Q(s, a) + Eµ [ ∞∑ t=0 (λγ)t ( t∏ i=1 κi ) (rt + γEπQ(st+1, ·)−Q(st, at)) ] = Q(s, a) + (I − λγPκµ)−1(T πQ−Q)(s, a) ,\nwhere Eµ is the expectation over the behavior policy and MDP transition probabilities and Pκµ is the operator defined by:\n(PκµQ)(s, a) := ∑ s′∈S ∑ a′∈A P (s′ | s, a)µ(a′ | s′)κ(s′, a′)Q(s′, a′) .\nIn the tabular case, these operators can be shown to yield contraction mappings with respect to the max norm (Precup, 2000; Munos et al., 2016). In this paper, we focus on what happens to these operators when combined with linear function approximation."
    }, {
      "heading" : "3 Off-policy instability with function approximation",
      "text" : "When combined with function approximation, the temporal difference updates corresponding to the λ-return Gλk are given by\nθk+1 = θk + αk ( Gλk −Q(sk, ak) ) ∇θQ(sk, ak)\n= θk + αk ( ∞∑ t=k (λγ)t−k ( t∏ i=k+1 κi ) δkt ) φ(sk, ak) (1)\nwhere δkt = rt + γθ > k Eπφ(st+1, ·)− θ>k φ(st, at) and αk are positive non-increasing step sizes. The updates (1) implies off-line updating as Gλk is a quantity which depends on future rewards. This will be addressed later using eligibility traces: a mechanism to transform the off-line updates into efficient on-line ones. Since (1) describes stochastic updates, the following standard assumption is necessary: Assumption 1. The Markov chain induced by the behavior policy µ is ergodic and admits a unique stationary distribution, denoted by ξ, over state-action pairs. We write Ξ for the diagonal matrix whose diagonal entries are (ξ(s, a))s∈S,a∈A.\nOur first proposition establishes the expected behavior of the parameters in the limit. Proposition 1. If the behavior policy satisfies Assumption 1 and (θk)k≤0 is the Markov process defined by (1) then: E[θk+1 | θ0] = (I + αkA)E[θk | θ0] + b , where matrix A and vector b are defined as follows:\nA := Φ>Ξ(I − λγPκµ)−1(γPπ − I)Φ. b := Φ>Ξ(I − λγPκµ)−1r .\nSketch of Proof (The full proof is in the appendix).\nθk+1 = θk + αk ( ∞∑ t=k (λγ)t−k ( t∏ i=k+1 κi ) φ(sk, ak) ( [γEπφ(xt+1, ·)− φ(xt, at)]>θk + rt )) = θk + αk (Akθk + bk)\nSo, E[θk+1 | θk] = (I + αkA)θk + b where A = E[Ak] and b = E[bk] 1We overload our notation over linear operators and their corresponding matrix representation.\nThe ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997). In particular, we use Proposition 4.8 in (Bertsekas and Tsitsiklis (1995)), which states that under some conditions, θk converges to the unique solution θ∗ of the system Aθ∗ + b = 0. This crucially relies on the matrix A being negative definite i.e y>Ay < 0,∀y 6= 0. In the on-policy case, when µ = π, we rely on the fact that the stationary distribution is invariant by the the transition matrix Pπ i.e d>Pπ = d> (Tsitsiklis et al., 1997; Sutton et al., 2015). However, this is no longer true for off-policy learning with arbitrary target/behavior policies and the matrix A may not be negative definite: the series θk may then diverge. We will now see that the same phenomenon may occur with TB(λ) and Retrace(λ).\nCounterexample: We extend the two-states MDP of Tsitsiklis et al. (1997), originally proposed to show the divergence of off-policy TD(0), to function approximation over state-action pairs. This environment has only two states, as shown in Figure 1, and two actions: left or right.\nIn this particular case, both TB(λ) and Retrace(λ) share the same matrix Pκµ:\nPπ = 0 1 0 00 1 0 01 0 0 0 1 0 0 0  , Pκµ = 0.5Pπ, (Pπ)n = 0 1 0 00 1 0 00 1 0 0 0 1 0 0  ∀n ≥ 2 . If we set β := 0.5γλ, we then have:\n(I − λγPκµ)−1 =  1 β1−β 0 0 0 11−β 0 0 β β 2\n1−β 1 0\nβ β 2\n1−β 0 1\n , A = ( 6γ−β−5 1−β 0\n3(γβ−β2−β−γ) 1−β −5\n) .\nTherefore, ∀ γ ∈ ( 56 , 1) and ∀λ ∈ [0,min(1, 12γ−10 γ )), the first eigenvalue e1 = 6γ−β−5\n1−β of A is positive. The basis vectors (1, 0)> and (0, 1)> are eigenvectors of A associated with e1 and -5, then if θ0 = (η1, η2)>, we obtain E[θk | θ0] = (η1 ∏k−1 i=0 (1 + αie1), η2 ∏k−1 i=0 (1− 5αi))> implying that\n||E[θk | θ0]|| ≥ |η1| ∏k−1 i=0 (1 + αie1). Hence, as ∑ k αk →∞, ||E[θk | θ0]|| → ∞ if η1 6= 0."
    }, {
      "heading" : "4 Convergent gradient off-policy algorithms",
      "text" : "If A were negative definite, Retrace(λ) or TB(λ) with function approximation would converge to θ∗ = −A−1b. It is known (Bertsekas, 2011) that Φθ∗ is the fixed point of the projected Bellman operator : Φθ∗ = ΠµR(Φθ∗) , where Πµ = Φ(Φ>ΞΦ)−1Φ>Ξ is the orthogonal projection onto the space S = {Φθ|θ ∈ Rd} with respect to the weighted Euclidean norm ||.||Ξ. Rather than computing the sequence of iterates given by the projected Bellman operator, another approach for finding θ∗ is to directly minimize (Sutton et al., 2009a; Liu et al., 2015) the Mean Squared Projected Bellman Error (MSPBE):\nMSPBE(θ) = 1\n2 ||ΠµR(Φθ)− Φθ||2Ξ .\nThis is the route that we take in this paper to derive convergent forms of TB(λ) and Retrace(λ). To do so, we first define our objective function in terms of A and b which we introduced in Proposition 1. Proposition 2. Let M := Φ>ΞΦ = E[ΦΦ>] be the covariance matrix of features. We have:\nMSPBE(θ) = 1\n2 ||Aθ + b||2M−1\n(The proof is provided in the appendix.)\nIn order to derive parameter updates, we could compute gradients of the above expression explicitly as in Sutton et al. (2009b), but we would then obtain a gradient that is a product of expectations. The implied double sampling makes it not straightforward to obtain an unbiased estimator of the gradient. Sutton et al. (2009b) addressed this problem with a two-timescale stochastic approximations. However, the algorithm obtained in this way is no longer a true stochastic gradient method with respect to the original objective. Liu et al. (2015) suggested an alternative which converts the original minimization problem into a primal-dual saddle point problem. This is the approach that we chose in this paper.\nThe convex conjugate of a real-valued function f is defined as: f∗(y) = sup\nx∈X (〈y, x〉 − f(x)) , (2)\nand f is convex, we have f∗∗ = f . Also, if f(x) = 12 ||x||M−1 , then f ∗(x) = 12 ||x||M . Note that by going to the convex conjugate, we do not need to invert matrix M . We now go back to the original minimization problem:\nmin θ∈Θ MSPBE(θ)⇔ min θ∈Θ\n1 2 ||Aθ + b||2M−1 ⇔ min θ∈Θ max ω∈Ω\n( 〈Aθ + b, ω〉 − 1\n2 ||ω||2M ) We now apply the projected gradient updates for saddle-point problems (ascent in ω and descent in θ)\nωk+1 = ΠΩ (ωk + αk(Aθk + b−Mωk)) , θk+1 = ΠΘ ( θk − αkA>ωk ) . (3)\nwhere ΠΘ and ΠΩ are the orthogonal projections respectively on Θ and Ω. As the A, b and M are all expectations, we could derive stochastic updates by drawing samples, which would yield unbiased estimates of the gradient.\nOn-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011). Proposition 3. Let ek b the eligibility traces vector, defined as e−1 = 0 and\nek = λγκ(sk, ak)ek−1 + φ(sk, ak) ∀k ≥ 0 We define: Âk = ek(γEπ[φ(sk+1, .)] − φ(sk, ak)])>, b̂k = r(sk, ak)ek, M̂k = φ(sk, ak)φ(sk, ak)\n>. Then, we have E[Âk] = A, E[b̂k] = b and E[M̂k] = M . (The proof is provided in the appendix.)\nThis proposition allows us to replace the expectations in Eq. (3) by corresponding unbiased estimates. The resulting detailed procedure is provided in Algorithm 1\nTrue on-line equivalence: In van Hasselt et al. (2014), the authors derived a true on-line update for GTD(λ) that empirically performed better than GTD(λ) with eligibility traces. Based on this work, we derive true on-line updates for our algorithm. The gradient off-policy algorithm was derived by turning the expected forward view into an expected backward view which can be sampled. In order to derive a true on-line update, we sample instead the forward view and then we turn the sampled forward view to an exact backward view using Theorem 1 in van Hasselt et al. (2014). If k denotes the time horizon, we consider the sampled truncated interim forward return:\n∀t < k, Y kt = k−1∑ i=t (λγ)i−t  i∏ j=t+1 κj  δi where δi = ri + θ>t Eπφ(st+1, ·)− θ>t φ(st, at), which gives us the sampled forward update of ω:\n∀k < t, ωkt+1 = ωkt + αt(Y kt − φ(xt, at)>ωkt )φ(xt, at) (4)\nAlgorithm 1 Gradient Off-policy with eligibility traces\nGiven: target policy π, behavior policy µ Initialize θ0 and ω0 Σθ = α0θ0, Σα = α0 for n = 0 . . . do\nset e0 = 0 for k = 0 . . . end of episode do\nObserve sk, ak, rk, sk+1 according to µ Update traces ek = λγκ(sk, ak)ek−1 + φ(sk, ak) Update parameters δk = rk + γθ > k Eπφ(sk+1, .)− θ>k φ(sk, ak)\nωk+1 = ΠΩ ( ωk + αk ( δkek − w>k φ(sk, ak)φ(sk, ak) )) θk+1 = ΠΘ ( θk − αkw>k ek (γEπφ(sk+1, .)− φ(sk, ak))\n) Σθ = Σθ + αkθk, Σα = Σα + αk\nend for end for Output: Polyak-average : θ̄ = Σθ/Σα\nProposition 4. For any k, the parameter ωkk defined by the forward view (4) is equal to ωk defined by the following backward view:\neω−1 = 0\neωk = λγκke ω k−1 + αk(1− λγκkφ(sk, ak)>eωk−1)φ(sk, ak) ∀k ≥ 0\nωk+1 = ωk + δke ω k − αtφ(sk, ak)>ωkφ(sk, ak)\nSketch of Proof (the full proof is in the appendix). We show that the temporal differences Y k+1t −Y kt are related through: ∀t < k, Y k+1t − Y kt = λγκt+1 ( Y k+1t+1 − Y kt+1 ) and then we apply Theorem 1 in van Hasselt et al. (2014).\nThe resulting detailed procedure is provided in Algorithm 2.\nNote that when λ is equal to zero, the Algorithm 1 and 2 both reduce to the same update: ωk+1 = ΠΩ ( ωk + αk(δk − φ(sk, ak)>ωk)φ(sk, ak) ) θk+1 = ΠΘ ( θk − αkφ(sk, ak)>wk(γEπ[φ(sk+1, .)]− φ(sk, ak)])\n) Convergence analysis: Our algorithm is an instance of the mirror stochastic approximation described in Nemirovski et al. (2009). We need the following assumptions to prove convergence.\nAssumption 2. Matrices A and M are non singular. This implies that the unconstrained saddle point problem admits a unique solution (θ∗, ω∗) = (−A−1b, 0). Assumption 3. The features and reward functions are uniformly bounded. Assumption 4. The feasible sets Θ and Ω are bounded closed convex sets and (θ∗, ω∗) ∈ Θ× Ω. Proposition 5. We consider the Polyak average θ̄n = ∑n i=0 αiθi/ ∑n i=0 αi where n is the total number of updates and θi are iterates of Algorithm (1) or (2). If assumptions 2, 3 and 4 are satisfied, there exists a constant B > 0, such that if αi = B/ √ n ∀i = 0 . . . n then:\nE [ MSPBE(θ̄n) ] ≤ O(1/ √ n)\nSketch of Proof (the full proof is provided in the appendix). The proof is similar to the one in Liu et al. (2015) that gives a bound with high probability for GTD(0)/GTD2(0) algorithm. Our proof however provides a bound in expectation as we do not want to use more restrictive assumptions on the distributions of the Âk, b̂k and M̂k estimates.\nAlgorithm 2 Gradient Off-policy with eligibility/Dutch traces\nGiven: target policy π, behavior policy µ Initialize θ0 and ω0 Σθ = α0θ0, Σα = α0 for n = 0 . . . do\nset eθ−1 = e ω −1 = 0 for k = 0 . . . end of episode do Observe sk, ak, rk, sk+1 according to µ Update traces ek = λγκ(sk, ak)ek−1 + φ(sk, ak) Update Dutch traces eωk = λγκke ω k−1 + αk ( 1− λγκkφ(sk, ak)>eωk−1 ) φ(sk, ak)\nUpdate parameters δk = rk + γθ > k Eπφ(sk+1, .)− θ>k φ(sk, ak) ωk+1 = ΠΩ ( ωk + δke ω k − αkφ(sk, ak)>ωkφ(sk, ak) ) θk+1 = ΠΘ ( θk − αkw>k ek (γEπ[φ(sk+1, .)]− φ(sk, ak))\n) Σθ = Σθ + αkθk, Σα = Σα + αk\nend for end for Output: Polyak-average : θ̄ = Σθ/Σα"
    }, {
      "heading" : "5 Experimental Results",
      "text" : "To validate our theoretical results about instability, we test the TB(λ), Retrace(λ) and their gradient versions GTB(λ) and GRetrace(λ) in two environments. The first one is the 2-state counterexample that we detailed in the third section and the second is the 7-state versions of Baird’s counterexample (Baird et al. (1995)). Figures 2 and 3 show the MSBPE (averaged over 20 runs) as a function of the number of iterations. We can see that the gradient methods converge in these two counterexamples whereas TB(λ) and Retrace(λ) diverge.\nWe also evaluated the efficiency and robustness of Retrace for higher values of λ in a grid world environment with 4 × 4 cells and two terminal states. The state-action features were of the form: φ(s, a) = (0 . . . φ(s) . . . 0)> with φ(s) located in the ath position. We represented state by its Cartesian coordinates. Figure 4 shows the RMSE (averaged over 20 runs) as a function of the step size, with each line corresponding to a different value of λ. We notice that for higher value of λ, GRetrace(λ) and true on-line GRtrace(λ) can reach a lower value of RMSE but their performance deteriorates faster for higher steps sizes in comparison with GTB(λ) and true on-line GTB(λ). The true on-line versions diverge only slightly slower which might be due to the fact that the updates are not fully true on-line as they combine accumulating and Dutch traces."
    }, {
      "heading" : "6 Discussion",
      "text" : "Our analysis highlighted for the first time the difficulties of combining the Tree Backup and Retrace algorithms with function approximation. We addressed these issues by formulating gradient-based algorithm versions of these algorithms which minimize the mean-square projected Bellman error. Using a saddle-point formulation, we were also able to provide convergence guarantees and characterize the convergence rate of our algorithms. Furthermore, we provided versions of these algorithms in both the true on-line (van Hasselt et al., 2014) setting with Dutch traces as well as for classical accumulating traces.\nMahmood et al. (2017) has recently introduced the ABQ(ζ) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(ζ) which is related to Retrace(λ). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation. In contrast, our approach uses the saddle point formulation, avoiding the need for double sampling or different learning rates. Another benefit of this formulation is that it allows us to provide a bound of the convergence rate (proposition 5) whereas Mahmood et al. (2017) is restricted to a more general two-timescale asymptotic result from Yu (2015). The saddle-point formulation also provides a rich literature on acceleration methods which could be incorporated in our algorithms. In particular, the stochastic extra-gradient method or proximal methods (Balamurugan and Bach, 2016) seem to be promising future directions."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "We compute E[Ak] and E[bk] where expectation are over trajectories drawn by executing the behavior policy: sk, ak, rk, ak+1, . . . st, at, rt, st+1 . . . where sk, ak ∼ d, rt = r(st, at), st+1 ∼ p(· | st, at). We note that under stationarity of d, E[Ak] = E[A0] and E[bk] = E[b0]. Let θ, θ′ ∈ Rd and let Q = Φθ and Q′ = Φθ′ their respective Q-functions.\nθ′>E[Ak]θ = E [ ∞∑ t=0 (λγ)t ( t∏ i=1 κi ) Q′(s0, a0)[γEπQ(st+1, .)−Q(st, at)]> ]\n= ∞∑ t=0 (λγ)tEs0:t+1 a0:t\n[ Q′(s0, a0) ( t∏ i=1 κi ) [γEπQ(st+1, .)−Q(st, at)]> ]\n= ∞∑ t=0 (λγ)tEs0:t a0:t\n[ Q′(s0, a0) ( t∏ i=1 κi ) (γEst+1 [EπQ(st+1, .)|st, at]−Q(st, at)) ]\n= ∞∑ t=0 (λγ)tEs0:t a0:t\n[ Q′(s0, a0) ( t∏ i=1 κi ) (γ ∑ s′∈S ∑ a′∈A p(s′|st, at)π(a′|s′)Q(s′, a′)−Q(st, at)) ]\n= ∞∑ t=0 (λγ)tEs0:t a0:t\n[ Q′(s0, a0) ( t∏ i=1 κi ) (γPπQ(st, at)−Q(st, at)) ]\n= ∞∑ t=0 (λγ)tEs0:t−1 a0:t−1\n[ Q′(s0, a0) ( t−1∏ i=1 κi )∑ s′∈S ∑ a′∈A p(s′|st−1, at−1)κ(a′, s′)µ(a′|s′)(γPπQ(s′, a′)−Q(s′, a′)) ]\n= ∞∑ t=0 (λγ)tEs0:t−1 a0:t−1\n[ Q′(s0, a0) ( t−1∏ i=1 κi ) Pκµ(γPπ − I)Q(st−1, at−1) ]\n= Es0,a0 [ Q′(s0, a0) ∞∑ t=0 (λγ)t(Pκµ)t(γPπ − I)Q(x0, a0) ] = Es0,a0 [ Q′(s0, a0)(I − λγPκµ)−1(γPπ − I)Q(s0, a0)\n] = ∑ s∈S ∑ a∈A ξ(s, a)Q′(s, a)(I − λγPκµ)−1(γPπ − I)Q(s, a)\n= Q′Ξ(I − λγPκµ)−1(γPπ − I)Q So, θ′>E[Ak]θ = θ′>Φ>Ξ(I − λγPκµ)−1(γPπ − I)Φθ ∀θ, θ′ ∈ Rd, which implies that:\nE[Ak] = Φ>Ξ(I − λγPκµ)−1(Pπ − I)Φ\nθ>E[bk] = E[ ∞∑ t=0 (λγ)t ( t∏ i=1 κi ) rtQ(s0, a0)] = ∞∑ t=0 (λγ)tEs0:t a0:t [ Q(s0, a0) ( t∏ i=1 κi ) r(st, at) ]\n= ∞∑ t=0 (λγ)tEs0:t−1 a0:t−1\n[ Q(s0, a0) ( t−1∏ i=1 κi )∑ s′∈S ∑ a′∈A p(s′|st−1, at−1)κ(a′, s′)µ(a′|s′)r(s′, a′) ]\n= ∞∑ t=0 (λγ)tEs0:t−1 a0:t−1\n[ Q(s0, a0) ( t−1∏ i=1 κi ) Pκµr(s′, a′) ] = Es0,a0 [ Q(s0, a0)(I − λγPκµ)−1r(s0, s0) ] = ∑ s∈S ∑ a∈A ξ(s, a)Q(s, a)(I − λγPκµ)−1r(s, a) = QΞ(I − λγPκµ)−1r\nSo, θ>E[bk] = θ>Ξ(I − λγPκµ)−1r ∀θ ∈ Rd, which implies that:\nE[bk] = Ξ(I − λγPκµ)−1r"
    }, {
      "heading" : "B Proof of Proposition 2",
      "text" : "MSPBE(θ) = 1\n2 ||ΠµR(Φθ)− Φθ||2Ξ =\n1 2 ||Πµ (R(Φθ)− Φθ) ||2Ξ\n= 1 2 (Πµ (R(Φθ)− Φθ))> Ξ (Πµ (R(Φθ)− Φθ))\n= 1\n2\n( Φ>Ξ (R(Φθ)− Φθ) )> (Φ>ΞΦ)−1Φ>Ξ ( Φ(Φ>ΞΦ)−1Φ>Ξ(R(Φθ)− Φθ) ) = 1\n2 ||Φ>Ξ (R(Φθ)− Φθ) ||2M−1\n= 1\n2 ||Φ>Ξ\n( (I − λγPµπ)−1(T π − λγPµπ)Φθ − Φθ ) ||2M−1\n= 1\n2 ||Φ>Ξ(I − λγPµπ)−1(γPπ − I)Φθ + Φ>Ξ(I − λγPµπ)−1r||2M−1\n= 1\n2 ||Aθ + b||2M−1"
    }, {
      "heading" : "C Proof of Proposition 3",
      "text" : "Let’s show that E[Âk] = A. Let’s ∆t denotes [γEπφ(st+1, .)> − φ(st, at)>]\nA = E [ ∞∑ t=k (λγ)t−k ( t∏ i=k+1 κi ) φ(sk, ak)∆t ]\n= E [ φ(sk, ak)∆k +\n∞∑ t=k+1 (λγ)t−k\n( t∏\ni=k+1\nκi ) φ(sk, ak)∆t ]\n= E [ φ(sk, ak)∆k +\n∞∑ t=k (λγ)t−k+1 ( t+1∏ i=k+1 κi ) φ(sk, ak)∆t+1 ]\n= E [ φ(sk, ak)∆k + λγκ(sk+1, ak+1)φ(sk, ak)∆k+1 +\n∞∑ t=k+1 (λγ)t−k+1 ( t+1∏ i=k+1 κi ) φ(sk, ak)∆t+1 ]\n(?) = E [ φ(sk, ak)∆k + λγκ(sk, ak)φ(sk−1, ak−1)∆k +\n∞∑ t=k+1 (λγ)t−k+1 ( t+1∏ i=k+1 κi ) φ(sk, ak)∆t+1 ] = E [ ∆k(φ(sk, ak) + λγκ(sk, ak)φ(sk−1, ak−1) + (λγ) 2κ(sk, ak)κ(sk−1, ak−1)φ(sk−2, ak−2) + ...) ]\n= E ∆k  k∑ i=0 (λγ)k−i  k∏ j=i+1 κj φ(xi, ai) \n= E[∆kek] = E[Âk]\nwe have used in the line (?) the fact that E[κ(sk+1, ak+1)φ(skak)∆k+1] = E[κ(sk, ak)φ(sk−1ak−1)∆k] thanks to the stationarity of the distribution d.\nwe have also denote by ek the following vector:\nek = k∑ i=0 (λγ)k−i  k∏ j=i+1 κj φ(si, ai) = λγκk\nk−1∑ i=0 (λγ)k−1−i  k−1∏ j=i+1 κj φ(si, ai) + φ(sk, ak)\n= λγκkek−1 + φ(sk, ak)\nVector ek corresponds to the eligibility traces defined in the proposition. Similarly, we could show that Eµ[b̂k] = b."
    }, {
      "heading" : "D Proof of Proposition 4",
      "text" : "The return’s temporal difference Y k+1t − Y kt are related through:\n∀t < k, Y k+1t − Y kt = k∑ i=t (λγ)i−t( i∏ j=t+1 κj)δi − k−1∑ i=t (λγ)i−t( i∏ j=t+1 κj)δi\n= (λγ)k−t  k∏ j=t+1 κj  δk = λγκk+1\n(λγ)k−(t+1)  k∏ j=t+2 κj  δk \n= λγκk+1 ( Y k+1t+1 − Y kt+1 ) We could then apply Theorem 1 of van Hasselt et al. (2014) that give us the following backward view:\ne0 = α0φ(x0, a0)\net = λγκtet−1 + αt(1− λγκkφ(st, at)>et−1)φ(st, at) ∀t > 0 ωt+1 = ωt + (Y t+1 t − Y tt )et + αt(Y tt − φ(st, at)>ωt)φ(st, at)\n(?) = ωt + δtet − αtφ(st, at)>ωtφ(st, at)\nWe used in the line (?) that Y t+1t = δt and Y t t = 0"
    }, {
      "heading" : "E Proof of Proposition 5",
      "text" : "We prove here the convergence rate of the gradient algorithm with eligibility traces (1). The proof for the algorithm (2) with dutch traces is similar. The algorithm (1) is an instance of the Mirror Stochastic Approximation algorithm described in Nemirovski et al. (2009). For clarity, we propose to rephrase here the result of the article with simpler notation. Proposition 6. We consider a convex-concave function f defined on Θ× Ω ⊂ Rd × Rd, where Θ and Ω are two bounded closed convex sets whose diameters are upper bounded by Dθ, Dω > 0. we assume that we have an increasing sequence of σ-fields {Fk} such that, θ0, ω0 are F0 measurable and such that for k ≥ 1,\nθk = ΠΘ(θk−1 − αkgθk)) (5) ωk = ΠΩ (ωk−1 + αkg ω k )) (6)\noutput : θ̄K = ∑K k=0 αkθk∑K k=0 αk , ω̄K = ∑K k=0 αkωk∑K k=0 αk\nwhere\n• ΠΘ and ΠΩ are orthogonal projection respectively on Θ and Ω.\n• E(gθk|Fk−1) = ∂xf(θk−1, ωk−1) and E(gωk |Fk−1) = ∂yf(θk−1, ωk−1)\n• Its exists Gθ, Gω ≥ 0 such that, E(||gθk||2) ≤ G2θ and E(||gωk ||2) ≤ G2ω\nIf ∀k, αk = 2G√5K where G 2 = 2D2θG 2 θ + 2D 2 ωG 2 ω , then we have:\nmax ω∈Ω f(θ, ω)−min θ∈Θ\nf(θ, ω) ≤ 2G √ 5\nK (7)\nIn our case,\n• f(θ, ω) = 〈Aθ + b, ω〉 − 12 ||ω|| 2 M .\n• gωk = Âkθ + b̂k − M̂kω • gθk = Â>k ω\nAs our sub/super gradient estimates are unbiased as shown in Proposition (3), we need only to prove that they have bounded variance. We denote by Rmax the uniform bound on rewards and by B the uniform bound on features and L = max(maxθ∈Θ ‖θ‖,maxω∈Ω ‖ω‖). ‖ · ‖ denotes the Euclidean norm and we recall that ‖ · ‖ ≤ √ d‖ · ‖∞.\n‖ek‖2 = ‖ k∑ i=0 (λγ)k−i( k∏ j=i+1 κj)φ(si, ai)‖2 ≤ B2d 1− (λγ)2\nE [ ‖gωk ‖2 ] = E [ ‖ek (γEπ[φ(sk+1, ·)]− φ(sk, ak))> θ + ekrk − φ(sk, ak)>ωφ(sk, ak)‖2 ] ≤ E [ ‖ek‖2| (γEπ[φ(sk+1, ·)]− φ(sk, ak))> θ|2 + ‖ek‖2|rk|2 + ‖φ(sk, ak)‖2|φ(sk, ak)>ω|2\n] (?)\n≤ B 2d\n1− (λγ)2 (1 + γ)2B2dL2 +\nB2d\n1− (λγ)2 R2max +B 4d2L2 := G2ω.\nwhere we used Cauchy-Schwarz inequality in (?). Similarly, we prove that:\nE [ ‖gθk‖2 ] ≤ B 2d\n1− (λγ)2 (1 + γ)2B2dL2 := G2θ.\nWe get then the inequality (7). Now, we have that:\nmin θ∈Θ f(θ, ω) ≤ max ω∈Ω min θ∈Θ f(θ, ω) ≤ min θ∈Θ max ω∈Ω f(θ, ω) = MSPBE(θ?) = 0\nimplying that max ω∈Ω f(θ, ω)−min θ∈Θ f(θ, ω) ≥ max ω∈Ω f(θ, ω)\n= max ω∈Ω 〈Aθ + b, ω〉 − 1 2 ||ω||2M = 1 2 ‖Aθ + b‖2M−1\n= MSPBE(θ)\nwhich gives the desired inequality in Proposition (5)"
    }, {
      "heading" : "F Additional experimental results",
      "text" : "The random walk problem is a unidimensional Markov chain with five states plus two absorbing terminal states as each end. The state-action features are in the from: φ(s, a) = (0 . . . φ(s)︸︷︷︸\nathposition\n. . . 0)>.\nWe used for φ(s) two representations: the tabular representation (Figure 5) and the dependant representations (Figure 6). In the dependant representations, we have φ(1) = (1, 0, 0)>, φ(2) = ( 1√\n2 , 1√ 2 , 0)>, φ(3) =\n( 1√ 3 , 1√ 3 , 1√ 3 )>, φ(4) = (0, 1√ 2 , 1√ 2 )>and φ(5) = (0, 0, 1)>."
    } ],
    "references" : [ {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L Baird" ],
      "venue" : "Proceedings of the twelfth international conference on machine learning, pages 30–37.",
      "citeRegEx" : "Baird,? 1995",
      "shortCiteRegEx" : "Baird",
      "year" : 1995
    }, {
      "title" : "Stochastic variance reduction methods for saddle-point problems",
      "author" : [ "P. Balamurugan", "F. Bach" ],
      "venue" : "arXiv preprint arXiv:1605.06398.",
      "citeRegEx" : "Balamurugan and Bach,? 2016",
      "shortCiteRegEx" : "Balamurugan and Bach",
      "year" : 2016
    }, {
      "title" : "Temporal difference methods for general projected equations",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "IEEE Transactions on Automatic Control, 56(9):2128–2139.",
      "citeRegEx" : "Bertsekas,? 2011",
      "shortCiteRegEx" : "Bertsekas",
      "year" : 2011
    }, {
      "title" : "Neuro-dynamic programming: an overview",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, volume 1, pages 560–564. IEEE.",
      "citeRegEx" : "Bertsekas and Tsitsiklis,? 1995",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis",
      "year" : 1995
    }, {
      "title" : "The o.d.e. method for convergence of stochastic approximation and reinforcement learning",
      "author" : [ "V.S. Borkar", "S.P. Meyn" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Borkar and Meyn,? \\Q2000\\E",
      "shortCiteRegEx" : "Borkar and Meyn",
      "year" : 2000
    }, {
      "title" : "Q(λ) with off-policy corrections",
      "author" : [ "A. Harutyunyan", "M.G. Bellemare", "T. Stepleton", "R. Munos" ],
      "venue" : "CoRR, abs/1602.04951.",
      "citeRegEx" : "Harutyunyan et al\\.,? 2016",
      "shortCiteRegEx" : "Harutyunyan et al\\.",
      "year" : 2016
    }, {
      "title" : "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "author" : [ "Lin", "L.-J." ],
      "venue" : "Machine Learning, 8(3-4):293–321.",
      "citeRegEx" : "Lin and L..J.,? 1992",
      "shortCiteRegEx" : "Lin and L..J.",
      "year" : 1992
    }, {
      "title" : "Finite-sample analysis of proximal gradient td algorithms",
      "author" : [ "B. Liu", "J. Liu", "M. Ghavamzadeh", "S. Mahadevan", "M. Petrik" ],
      "venue" : "UAI, pages 504–513. Citeseer.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradient temporal-difference learning algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : null,
      "citeRegEx" : "Maei,? \\Q2011\\E",
      "shortCiteRegEx" : "Maei",
      "year" : 2011
    }, {
      "title" : "Multi-step off-policy learning without importance sampling ratios",
      "author" : [ "A.R. Mahmood", "H. Yu", "R.S. Sutton" ],
      "venue" : "arXiv preprint arXiv:1702.03006.",
      "citeRegEx" : "Mahmood et al\\.,? 2017",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2017
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "Balcan, M. F. and Weinberger, K. Q., editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1928–1937, New York, New York, USA. PMLR.",
      "citeRegEx" : "Mnih et al\\.,? 2016",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Safe and efficient off-policy reinforcement learning",
      "author" : [ "R. Munos", "T. Stepleton", "A. Harutyunyan", "M. Bellemare" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1046–1054.",
      "citeRegEx" : "Munos et al\\.,? 2016",
      "shortCiteRegEx" : "Munos et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on optimization, 19(4):1574–1609.",
      "citeRegEx" : "Nemirovski et al\\.,? 2009",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Eligibility traces for off-policy policy evaluation",
      "author" : [ "D. Precup" ],
      "venue" : "Computer Science Department Faculty Publication Series, page 80.",
      "citeRegEx" : "Precup,? 2000",
      "shortCiteRegEx" : "Precup",
      "year" : 2000
    }, {
      "title" : "Off-policy temporal difference learning with function approximation",
      "author" : [ "D. Precup", "R.S. Sutton", "S. Dasgupta" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 417–424, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Precup et al\\.,? 2001",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2001
    }, {
      "title" : "Introduction to reinforcement learning, volume 135",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press Cambridge.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "Second Edition. MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 2017",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 2017
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvári", "E. Wiewiora" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning - ICML. ACM Press.",
      "citeRegEx" : "Sutton et al\\.,? 2009a",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "A convergent o(n) temporal-difference algorithm for off-policy learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "C. Szepesvári" ],
      "venue" : "Advances in neural information processing systems, pages 1609–1616.",
      "citeRegEx" : "Sutton et al\\.,? 2009b",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "An emphatic approach to the problem of off-policy temporal-difference learning",
      "author" : [ "R.S. Sutton", "A.R. Mahmood", "M. White" ],
      "venue" : "The Journal of Machine Learning Research, 17:1–29.",
      "citeRegEx" : "Sutton et al\\.,? 2015",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2015
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup" ],
      "venue" : "The 10th International Conference on Autonomous Agents and Multiagent Systems Volume 2, AAMAS ’11, pages 761–768, Richland, SC. International Foundation for Autonomous Agents and Multiagent Systems.",
      "citeRegEx" : "Sutton et al\\.,? 2011",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "R.S. Sutton", "D. Precup", "S. Singh" ],
      "venue" : "Artificial Intelligence, 112(1-2):181–211.",
      "citeRegEx" : "Sutton et al\\.,? 1999",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Temporal-difference networks",
      "author" : [ "R.S. Sutton", "B. Tanner" ],
      "venue" : "Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 1377–1384.",
      "citeRegEx" : "Sutton and Tanner,? 2004",
      "shortCiteRegEx" : "Sutton and Tanner",
      "year" : 2004
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B Van Roy" ],
      "venue" : "IEEE transactions on automatic control,",
      "citeRegEx" : "Tsitsiklis and Roy,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy",
      "year" : 1997
    }, {
      "title" : "Off-policy td (λ) with a true online equivalence",
      "author" : [ "H. van Hasselt", "A.R. Mahmood", "R.S. Sutton" ],
      "venue" : "In Proceedings of the 30th Conference on Uncertainty",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2014
    }, {
      "title" : "Sample efficient actor-critic with experience",
      "author" : [ "Z. Wang", "V. Bapst", "N. Heess", "V. Mnih", "R. Munos", "K. Kavukcuoglu", "N. de Freitas" ],
      "venue" : "replay. CoRR,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "On convergence of emphatic temporal-difference learning",
      "author" : [ "H. Yu" ],
      "venue" : "Grünwald, P., Hazan, E., and Kale, S., editors, Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 1724–1751, Paris, France. PMLR.",
      "citeRegEx" : "Yu,? 2015",
      "shortCiteRegEx" : "Yu",
      "year" : 2015
    }, {
      "title" : "For clarity, we propose to rephrase here the result of the article with simpler notation",
      "author" : [ "Nemirovski" ],
      "venue" : "Proposition 6. We consider a convex-concave function f defined on Θ× Ω ⊂ R × R, where Θ and Ω are two bounded closed convex sets whose diameters are upper bounded by Dθ, Dω > 0. we assume that we have an increasing sequence of σ-fields",
      "citeRegEx" : "Nemirovski,? 2009",
      "shortCiteRegEx" : "Nemirovski",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al.",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "While off-policy learning and function approximation have been understood in isolation, their combination with multi-steps bootstrapping produces a so-called deadly triad (Sutton and Barto, 2017), i.",
      "startOffset" : 171,
      "endOffset" : 195
    }, {
      "referenceID" : 13,
      "context" : "A convergent approach to this triad is provided by importance sampling, which “bends\" the behavior policy distribution onto the target one (Precup, 2000; Precup et al., 2001).",
      "startOffset" : 139,
      "endOffset" : 174
    }, {
      "referenceID" : 14,
      "context" : "A convergent approach to this triad is provided by importance sampling, which “bends\" the behavior policy distribution onto the target one (Precup, 2000; Precup et al., 2001).",
      "startOffset" : 139,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "An alternative approach which was developed for tabular representations of the value function is the tree backup algorithm (Precup, 2000) which, remarkably, does not rely on importance sampling directly.",
      "startOffset" : 123,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "Tree Backup has recently been revisited by (Munos et al., 2016), who used its intuitions to develop the Retrace(λ) algorithm.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al.",
      "startOffset" : 264,
      "endOffset" : 283
    }, {
      "referenceID" : 10,
      "context" : "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner).",
      "startOffset" : 264,
      "endOffset" : 303
    }, {
      "referenceID" : 10,
      "context" : "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner). Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al.",
      "startOffset" : 264,
      "endOffset" : 862
    }, {
      "referenceID" : 10,
      "context" : "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner). Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al. (2011). Therefore, the design and analysis of off-policy algorithms using all the features of reinforcement learning, e.",
      "startOffset" : 264,
      "endOffset" : 884
    }, {
      "referenceID" : 4,
      "context" : "First, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(λ), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample).",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Our algorithm can be implemented with both classical accumulating traces (Sutton and Barto, 1998) as well as Dutch traces (van Hasselt et al.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "First, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(λ), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample). Insights gained from this analysis allow us to derive a new gradient-based algorithm which provably converges to the right solution. Instead of adapting the blueprint from gradient-based temporal difference learning Sutton et al. (2009b), we rely on the primal-dual saddle point formulation of Liu et al.",
      "startOffset" : 61,
      "endOffset" : 510
    }, {
      "referenceID" : 4,
      "context" : "First, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(λ), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample). Insights gained from this analysis allow us to derive a new gradient-based algorithm which provably converges to the right solution. Instead of adapting the blueprint from gradient-based temporal difference learning Sutton et al. (2009b), we rely on the primal-dual saddle point formulation of Liu et al. (2015), which also allows us to provide sample complexity bounds.",
      "startOffset" : 61,
      "endOffset" : 584
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we are concerned with the policy evaluation problem (Sutton and Barto, 1998) under model-free off-policy learning.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "(2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(λ) (Harutyunyan et al.",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "(2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(λ) (Harutyunyan et al., 2016) and Tree-backup (TB(λ)) (Precup, 2000).",
      "startOffset" : 141,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : ", 2016) and Tree-backup (TB(λ)) (Precup, 2000).",
      "startOffset" : 32,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "It was shown that all these methods in fact share the following general form of the λ-return (Sutton and Barto, 2017) for some coefficients κi: Gk := Q(sk, ak) + ∞ ∑",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "Off-policy learning Munos et al. (2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(λ) (Harutyunyan et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "From this unified representation, Munos et al. (2016) derived the Retrace(λ) algorithm.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "approach (Precup, 2000) converges in the tabular case, as it warps the behavior data distribution to the distribution that would be induced by the target policy π, but it also suffers from high variance.",
      "startOffset" : 9,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "In the tabular case, these operators can be shown to yield contraction mappings with respect to the max norm (Precup, 2000; Munos et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "In the tabular case, these operators can be shown to yield contraction mappings with respect to the max norm (Precup, 2000; Munos et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997).",
      "startOffset" : 152,
      "endOffset" : 209
    }, {
      "referenceID" : 19,
      "context" : "e d>Pπ = d> (Tsitsiklis et al., 1997; Sutton et al., 2015).",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997). In particular, we use Proposition 4.8 in (Bertsekas and Tsitsiklis (1995)), which states that under some conditions, θk converges to the unique solution θ∗ of the system Aθ∗ + b = 0.",
      "startOffset" : 153,
      "endOffset" : 285
    }, {
      "referenceID" : 2,
      "context" : "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997). In particular, we use Proposition 4.8 in (Bertsekas and Tsitsiklis (1995)), which states that under some conditions, θk converges to the unique solution θ∗ of the system Aθ∗ + b = 0. This crucially relies on the matrix A being negative definite i.e y>Ay < 0,∀y 6= 0. In the on-policy case, when μ = π, we rely on the fact that the stationary distribution is invariant by the the transition matrix P i.e d>Pπ = d> (Tsitsiklis et al., 1997; Sutton et al., 2015). However, this is no longer true for off-policy learning with arbitrary target/behavior policies and the matrix A may not be negative definite: the series θk may then diverge. We will now see that the same phenomenon may occur with TB(λ) and Retrace(λ). Counterexample: We extend the two-states MDP of Tsitsiklis et al. (1997), originally proposed to show the divergence of off-policy TD(0), to function approximation over state-action pairs.",
      "startOffset" : 153,
      "endOffset" : 998
    }, {
      "referenceID" : 2,
      "context" : "It is known (Bertsekas, 2011) that Φθ∗ is the fixed point of the projected Bellman operator : Φθ∗ = ΠμR(Φθ∗) , where Π = Φ(Φ>ΞΦ)−1Φ>Ξ is the orthogonal projection onto the space S = {Φθ|θ ∈ R} with respect to the weighted Euclidean norm ||.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "Rather than computing the sequence of iterates given by the projected Bellman operator, another approach for finding θ∗ is to directly minimize (Sutton et al., 2009a; Liu et al., 2015) the Mean Squared Projected Bellman Error (MSPBE): MSPBE(θ) = 1 2 ||ΠR(Φθ)− Φθ||Ξ .",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "Rather than computing the sequence of iterates given by the projected Bellman operator, another approach for finding θ∗ is to directly minimize (Sutton et al., 2009a; Liu et al., 2015) the Mean Squared Projected Bellman Error (MSPBE): MSPBE(θ) = 1 2 ||ΠR(Φθ)− Φθ||Ξ .",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : ") In order to derive parameter updates, we could compute gradients of the above expression explicitly as in Sutton et al. (2009b), but we would then obtain a gradient that is a product of expectations.",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : ") In order to derive parameter updates, we could compute gradients of the above expression explicitly as in Sutton et al. (2009b), but we would then obtain a gradient that is a product of expectations. The implied double sampling makes it not straightforward to obtain an unbiased estimator of the gradient. Sutton et al. (2009b) addressed this problem with a two-timescale stochastic approximations.",
      "startOffset" : 108,
      "endOffset" : 330
    }, {
      "referenceID" : 7,
      "context" : "Liu et al. (2015) suggested an alternative which converts the original minimization problem into a primal-dual saddle point problem.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "On-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011).",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "On-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011). Proposition 3. Let ek b the eligibility traces vector, defined as e−1 = 0 and ek = λγκ(sk, ak)ek−1 + φ(sk, ak) ∀k ≥ 0 We define: Âk = ek(γEπ[φ(sk+1, .)] − φ(sk, ak)]), b̂k = r(sk, ak)ek, M̂k = φ(sk, ak)φ(sk, ak) >. Then, we have E[Âk] = A, E[b̂k] = b and E[M̂k] = M . (The proof is provided in the appendix.) This proposition allows us to replace the expectations in Eq. (3) by corresponding unbiased estimates. The resulting detailed procedure is provided in Algorithm 1 True on-line equivalence: In van Hasselt et al. (2014), the authors derived a true on-line update for GTD(λ) that empirically performed better than GTD(λ) with eligibility traces.",
      "startOffset" : 143,
      "endOffset" : 683
    }, {
      "referenceID" : 8,
      "context" : "On-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011). Proposition 3. Let ek b the eligibility traces vector, defined as e−1 = 0 and ek = λγκ(sk, ak)ek−1 + φ(sk, ak) ∀k ≥ 0 We define: Âk = ek(γEπ[φ(sk+1, .)] − φ(sk, ak)]), b̂k = r(sk, ak)ek, M̂k = φ(sk, ak)φ(sk, ak) >. Then, we have E[Âk] = A, E[b̂k] = b and E[M̂k] = M . (The proof is provided in the appendix.) This proposition allows us to replace the expectations in Eq. (3) by corresponding unbiased estimates. The resulting detailed procedure is provided in Algorithm 1 True on-line equivalence: In van Hasselt et al. (2014), the authors derived a true on-line update for GTD(λ) that empirically performed better than GTD(λ) with eligibility traces. Based on this work, we derive true on-line updates for our algorithm. The gradient off-policy algorithm was derived by turning the expected forward view into an expected backward view which can be sampled. In order to derive a true on-line update, we sample instead the forward view and then we turn the sampled forward view to an exact backward view using Theorem 1 in van Hasselt et al. (2014). If k denotes the time horizon, we consider the sampled truncated interim forward return:",
      "startOffset" : 143,
      "endOffset" : 1204
    }, {
      "referenceID" : 24,
      "context" : "We show that the temporal differences Y k+1 t −Y k t are related through: ∀t < k, Y k+1 t − Y k t = λγκt+1 ( Y k+1 t+1 − Y k t+1 ) and then we apply Theorem 1 in van Hasselt et al. (2014). The resulting detailed procedure is provided in Algorithm 2.",
      "startOffset" : 166,
      "endOffset" : 188
    }, {
      "referenceID" : 12,
      "context" : "Convergence analysis: Our algorithm is an instance of the mirror stochastic approximation described in Nemirovski et al. (2009). We need the following assumptions to prove convergence.",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "The proof is similar to the one in Liu et al. (2015) that gives a bound with high probability for GTD(0)/GTD2(0) algorithm.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "The first one is the 2-state counterexample that we detailed in the third section and the second is the 7-state versions of Baird’s counterexample (Baird et al. (1995)).",
      "startOffset" : 124,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "In particular, the stochastic extra-gradient method or proximal methods (Balamurugan and Bach, 2016) seem to be promising future directions.",
      "startOffset" : 72,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "Mahmood et al. (2017) has recently introduced the ABQ(ζ) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Mahmood et al. (2017) has recently introduced the ABQ(ζ) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(ζ) which is related to Retrace(λ). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation.",
      "startOffset" : 0,
      "endOffset" : 417
    }, {
      "referenceID" : 8,
      "context" : "Mahmood et al. (2017) has recently introduced the ABQ(ζ) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(ζ) which is related to Retrace(λ). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation. In contrast, our approach uses the saddle point formulation, avoiding the need for double sampling or different learning rates. Another benefit of this formulation is that it allows us to provide a bound of the convergence rate (proposition 5) whereas Mahmood et al. (2017) is restricted to a more general two-timescale asymptotic result from Yu (2015).",
      "startOffset" : 0,
      "endOffset" : 722
    }, {
      "referenceID" : 8,
      "context" : "Mahmood et al. (2017) has recently introduced the ABQ(ζ) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(ζ) which is related to Retrace(λ). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation. In contrast, our approach uses the saddle point formulation, avoiding the need for double sampling or different learning rates. Another benefit of this formulation is that it allows us to provide a bound of the convergence rate (proposition 5) whereas Mahmood et al. (2017) is restricted to a more general two-timescale asymptotic result from Yu (2015). The saddle-point formulation also provides a rich literature on acceleration methods which could be incorporated in our algorithms.",
      "startOffset" : 0,
      "endOffset" : 801
    } ],
    "year" : 2017,
    "abstractText" : "Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms, compatible with accumulating or Dutch traces, using a novel methodology based on proximal methods. In addition to convergence proofs, we provide sample-complexity bounds.",
    "creator" : "LaTeX with hyperref package"
  }
}