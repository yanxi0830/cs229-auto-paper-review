{
  "name" : "1703.00066.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Power of Learning from k-Wise Queries",
    "authors" : [ "Vitaly Feldman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n00 06\n6v 1\n[ cs\n.L G\n] 2\n8 Fe\nBlum, Kalai, Wasserman [BKW03] showed that for any weak PAC learning problem over a fixed distribution, the complexity of learning with k-wise SQs is smaller than the (unary) SQ complexity by a factor of at most 2k. We show that for more general problems over distributions the picture is substantially richer. For every k, the complexity of distribution-independent PAC learning with k-wise queries can be exponentially larger than learning with (k + 1)-wise queries. We then give two approaches for simulating a k-wise query using unary queries. The first approach exploits the structure of the problem that needs to be solved. It generalizes and strengthens (exponentially) the results of Blum et al. [BKW03]. It allows us to derive strong lower bounds for learning DNF formulas and stochastic constraint satisfaction problems that hold against algorithms using k-wise queries. The second approach exploits the k-party communication complexity of the k-wise query function.\n∗Work done while at IBM Research - Almaden.\nContents"
    }, {
      "heading" : "1 Introduction 3",
      "text" : "1.1 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
    }, {
      "heading" : "2 Preliminaries 6",
      "text" : "3 Separation of (k + 1)-wise from k-wise queries 7 3.1 Upper bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Lower bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2.2 Proof of Lemma 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14"
    }, {
      "heading" : "4 Reduction for flat distributions 16",
      "text" : "4.1 Decision problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.2 General problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.3 Applications to solving CSPs and learning DNF . . . . . . . . . . . . . . . . . . . 21"
    }, {
      "heading" : "5 Reduction for low-communication queries 22",
      "text" : ""
    }, {
      "heading" : "6 Corollaries for other models 24",
      "text" : "6.1 k-local differential privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 6.2 k-wise b-bit sampling model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25"
    }, {
      "heading" : "A Omitted proofs 28",
      "text" : "A.1 Proof of Lemma 3.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 A.2 Proof of Proposition 3.14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 A.3 Proof of Proposition 3.15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 A.4 Proof of Proposition 3.16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we consider several well-studied models of learning from i.i.d. samples that restrict the algorithm’s access to samples to evaluation of functions of an individual sample. The primary model of interest is the statistical query model introduced by Kearns [Kea98] as a restriction of Valiant’s PAC learning model [Val84]. The SQ model allows the learning algorithm to access the data only via statistical queries, which are estimates of the expectation of any function of labeled examples with respect to the input distribution D. More precisely, if the domain of the functions is Z, then a statistical query is specified by a function φ : Z × {±1} → [−1, 1] and by a tolerance parameter τ . Given φ and τ , the statistical query oracle returns a value v which satisfies |v − E(z,b)∼D[φ(z, b)]| ≤ τ .\nThe SQ model is known to be closely-related to several other models and concepts: linear statistical functionals [Was13], learning with a distance oracle [BIK90], approximate counting (or linear) queries extensively studied in differential privacy (e.g., [DN03, BDMN05, DMNS06, RR10]), local differential privacy [KLN+11], evolvability [Val09, Fel08], and algorithms that extract a small amount of information from each sample [BD98, FGR+12, FPV13, SVW16]. This allows to easily extend the discussion in the context of the SQ model to these related models and we will formally state several such corollaries.\nMost standard algorithmic approaches used in learning theory are known to be implementable using SQs (e.g., [BFKV98, DV04, BDMN05, CKL+07, FPV13, BF15, FGV15]) leading to numerous theoretical (e.g., [BBFM12, DDS15, DFH+15b]) and practical (e.g., [CKL+07, RSK+10, SLB+11, DFH+15a]) applications. SQ algorithms have also been recently studied outside the context of learning theory [FGR+12, FPV13, FGV15]. In this case we denote the domain of data samples by X .\nAnother reason for the study of SQ algorithms is that it is possible to prove informationtheoretic lower bounds on the complexity of any SQ algorithm that solves a given problem. Given that a large number of algorithmic approaches to problems defined over data sampled i.i.d. from some distribution can be implemented using statistical queries, this provides a strong and unconditional evidence of the problem’s hardness. For a number of central problems in learning theory and complexity theory, unconditional lower bounds for SQ algorithms are known that closely match the known computational complexity upper bounds for those problems (e.g. [BFJ+94, FGR+12, FPV13, DSFT+15, DKS16]).\nA natural strengthening of the SQ model (and other related models) is to allow function over k-tuples of samples instead of a single sample. That is, for a k-ary query function φ : Xk → [−1, 1], the algorithm can obtain an estimate of Ex1,...,xk∼D[φ(x1, . . . , xk)]. It can be seen as interpolating between the power of algorithms that can see all the samples at once and those that process a single sample at a time. While most algorithms can be implemented using standard unary queries, some algorithms are known to require such more powerful queries. The most well-known example is Gaussian elimination over Fn2 that is used for learning parity functions. Standard hardness amplification techniques rely on mapping examples of a function f(z) to examples of a function g(f(z1), . . . , f(zk)) (for example [BL93, FLS11]). Implementing such reduction requires k-wise queries and, consequently, to obtain a lower bound for solving an amplified problem with unary queries one needs a lower bound against solving the original problem with k-wise queries. A simple example of 2-wise statistical query is collision probability Prx1,x2∼D[x1 = x2] that is used in several distribution property testing algorithms."
    }, {
      "heading" : "1.1 Previous work",
      "text" : "Blum, Kalai and Wasserman [BKW03] introduced and studied the power of k-wise SQs in the context of weak distribution-specific PAC learning: that is the learning algorithm observes pairs (z, b), where z is chosen randomly from some fixed and known distribution P over Z and b = f(z) for some unknown function f from a class of functions C. They showed that if a class of functions\nC can be learned with error 1/2− λ relative to distribution P using q k-wise SQs of tolerance τ then it can be learned with error max{1/2−λ, 1/2−τ/2k} using O(q ·2k) unary SQs of tolerance τ/2k.\nMore recently, Steinhardt et al. [SVW16] considered k-wise queries in the b-bit sampling model in which for any query function φ : Xk → {0, 1}b an algorithm get the value φ(x1, . . . , xk) for x1, . . . , xk drawn randomly and independently from D (it is referred to as one-way communication model in their work). They give a general technique for proving lower bounds on the number of such queries that are required to solve a given problem."
    }, {
      "heading" : "1.2 Our results",
      "text" : "In this work, we study the relationship between the power of k-wise queries and unary queries for arbitrary problems in which the input is determined by some unknown input distribution D that belongs a (known) family of distributions D over domain X .\nSeparation for distribution-independent learning: We first demonstrate that for distribution-independent PAC learning (k + 1)-wise queries are exponentially stronger than kwise queries. We say that the k-wise SQ complexity of a certain problem is m if m is the smallest such that there exists an algorithm that solves the problem using m k-wise SQs of tolerance 1/m.\nTheorem 1.1. (Informal) For every positive integer k and any prime number p, there is a concept class C of Boolean functions defined over a domain of size pk+1 such that the (k + 1)- wise SQ complexity of distribution-independent PAC learning C with is Ok(log p) whereas the k-wise SQ complexity of distribution-independent PAC learning of C is Ωk(p1/4).\nThe class of functions we use consists of all indicator functions of k-dimensional affine subspaces of Fk+1p . Our lower bound is a generalization of the lower bound for unary SQs in [Fel16b] (that corresponds to k = 1 case of the lower bound). A simple but important observation that allows us to easily adapt the techniques from earlier works on SQs to the k-wise case is that a k-wise SQ for an input distribution D ∈ D are equivalent to unary SQ for a product distribution Dk.\nThe upper bound relies on the ability to find the affine subspace given k+1 positively labeled and linearly independent points in Fk+1p . Unfortunately, for general distributions the probability of observing such a set of points can be arbitrarily small. Nevertheless, we argue that there will exist a unique lower-dimensional affine subspace that contains enough probability mass of all the positive points in this case. This upper bound essentially implies that given k-wise queries one can solve problems that require Gaussian elimination over a system of k equations.\nReduction for flat D: The separation in Theorem 1.1 relies on using an unrestricted class of distributions D. We now prove that if D is “flat” relative to some “central” distribution D̄ then one can upper bound the power of k-wise queries in terms of unary queries.\nDefinition 1.2 (Flat class of distributions). Let D be a set of distributions over X, and D̄ a distribution over X. For γ ≥ 1 we say that D is γ-flat if there exists some distribution D̄ over X such that for all D ∈ D and all measurable subsets E ⊆ X, we have that Prx∼D[x ∈ E] ≤ γ · Prx∼D̄[x ∈ E].\nWe now state our upper bound for flat classes of distributions, where we use STAT (k) D (τ) to\nrefer to the oracle that answers k-wise SQs for D with tolerance τ .\nTheorem 1.3. Let γ ≥ 1, τ > 0 and k be any positive integer. Let X be a domain and D a γ-flat class of distributions over X. There exists a randomized algorithm that given any δ > 0\nand a k-ary function φ : Xk → [−1, 1] estimates Dk[φ] within τ for every (unknown) D ∈ D with success probability at least 1− δ using\nÕ\n( γk−1 · k3\nτ3 · log(1/δ)\n)\nqueries to STAT (1) D (τ/(6 · k)).\nTo prove this result, we use a recent general characterization of SQ complexity [Fel16b]. This characterization reduces the problem of estimating Dk[φ] to the problem of distinguishing between Dk and Dk1 for every D ∈ D and some fixed D1. We show that when solving this problem, any k-wise query can be replaced by a randomly chosen set of unary queries. Finding these queries requires drawing samples from Dk−1. As we do not know D, we use D̄ instead incurring the γk−1 overhead in sampling. In Section 4 we show that weaker notions of “flatness” based on different notions of divergence between distributions can also be used in this reduction.\nIt is easy to see that, when PAC learning C with respect to a fixed distribution P over Z, the set of input distributions is 2-flat (relative to the distribution that is equal to P on Z and gives equal weight 1/2 to each label). Therefore, our result generalizes the results in [BKW03]. More importantly, the tolerance in our upper bound scales linearly with k rather than exponentially (namely, τ/2k).\nThis result can be used to obtain lower bounds against k-wise SQs algorithms from lower bounds against unary SQ algorithms. In particular, it can be used to rule out reductions that require looking at k points of the original problem instance to obtain each point of the new problem instance. As an application, we obtain exponential lower bounds for solving constraint stochastic satisfaction problems and DNF learning by k-wise SQ algorithm with k = n1−α for any constant α > 0 from lower bounds for CSPs given in [FPV13]. We state the result for learning DNF here. Definitions and the lower bound for CSPs can be found in Section 4.3.\nTheorem 1.4. For any constant α > 0 (independent of n), there exists a constant β > 0 such that any algorithm that learns DNF formulas of size n with error < 1/2− n−β logn and success probability at least 2/3 requires at least 2n 1−α\ncalls to STAT (n1−α) D (n −β logn).\nThis lower bound is based on a simple and direct reduction from solving the stochastic CSP that arises in Goldreich’s proposed PRG [Gol00] to learning DNF that is of independent interest (see Lemma 4.18). For comparison, the standard SQ lower bound for learning polynomial size DNF [BFJ+94] relies on hardness of learning parities of size logn over the uniform distribution. Yet, parities of size logn can be easily learned from (log2 n)-wise statistical queries (since solving a system of log2 n linear equations will uniquely identify a logn-sparse parity function). Hence our lower bound holds against qualitatively stronger algorithms. Our lower bound is also exponential in the number of queries whereas the known argument implies only a quasipolynomial lower bound1.\nReduction for low-communication queries: Finally, we point out that k-wise queries that require little information about each of the inputs can also be simulated using unary queries. This result is a simple corollary of the recent work of Steinhardt et al. [SVW16] who show that any computation that extracts at most b bits from each of the samples (not necessarily at once) can be simulated using unary SQs.\nTheorem 1.5. Let φ : Xk → {±1} be a function, and assume that φ has k-party public-coin randomized communication complexity of b bits per party with success probability 2/3. Then, there exists a randomized algorithm that, with probability at least 1 − δ, estimates Ex∼Dk [φ(x)] within τ using O(b · k · log(1/δ)/τ2) queries to STAT(1)D (τ ′) for some τ ′ = τO(b)/k.\n1We remark that an exponential lower bound on the number of queries has not been previously stated even for\nunary SQs. The unary version can be derived from known results as explained in Section 4.3.\nAs a simple application of Theorem 1.5, we show a unary SQ algorithm that estimates the\ncollision probability of an unknown distribution D within τ using 1/τ2 queries STAT (1) D (τ O(1)). The details appear in Section 5.\nCorollaries for related models: Our separation result and reductions imply similar results for k-wise versions of two well-studied learning models: local differential privacy and the b-bit sampling model.\nLocal differentially private algorithms [KLN+11] (also referred to as randomized response) are differentially private algorithms in which each sample goes through a differentially private transformation chosen by the analyst. This model is the focus of recent privacy preserving industrial applications by Google [EPK14] and Apple. We define a k-wise version of this model in which analyst’s differentially private transformations are applied to k-tuples of samples. This model interpolates naturally between the usual (or global) differential privacy and the local model.\nKasiviswanathan et al. [KLN+11] showed that a concept class is learnable by a local differentially private algorithm if and only if it is learnable in the SQ model. Hence up to polynomial factors the models are equivalent (naturally, such polynomial factors are important for applications but here we focus only on the high-level relationships between the models). This result also implies that k-local differentially private algorithms (formally defined in Section 6.1) are equivalent to k-wise SQ algorithms (up to a polynomial blow-up in the complexity). Theorem 1.1 then implies an exponential separation between k-wise and (k + 1)-wise local differentially private algorithms (see Corollary 6.6 for details). It can be seen as a substantial strengthening of a separation between the local model and the global one also given in [KLN+11]. The reductions in Theorem 1.3 and Theorem 1.5 imply two approaches for simulating k-local differentially private algorithms using 1-local algorithms.\nThe SQ model is also known to be equivalent (up to a factor polynomial in 2b) to the b-bit sampling model introduced by Ben-David and Dichterman [BD98] and studied more recently in [FGR+12, FPV13, ZDJW13, SD15, SVW16]. Lower bounds for the k-wise version of this model are given in [ZDJW13, SVW16]. Our results can be easily translated to this model as well. We provide additional details in Section 6."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "For any distribution D over a domain X and any positive integer k, we denote by Dk the distribution over Xk obtained by drawing k i.i.d. samples from D. For a distribution D over a domain X and a function φ : X → R, we denote D[φ] .= Ex∼D[φ(x)].\nNext, we formally define the k-wise SQ oracle.\nDefinition 2.1. Let D be a distribution over a domain X and τ > 0. A k-wise statistical query oracle STAT (k) D (τ) is an oracle that given as input any function φ : X\nk → [−1,+1], returns some value v such that |v − Ex∼Dk [φ(x)]| ≤ τ .\nWe say that a k-wise SQ algorithm is given access to STAT(k)(τ), if for every when the algorithm is given access to STAT (k) D (τ), where D is the input distribution. We note that for k = 1, Definition 2.1 reduces to the usual definition of an SQ oracle that was first introduced by Kearns [Kea98]. The k-wise SQ complexity of solving a problem with access to STAT(k)(τ) is the minimum number of queries q for which exists a k-wise SQ algorithm with access to STAT(k)(τ) that solves the problem using at most q queries. Our discussion and results can also be easily extended to the stronger VSTAT oracle defined in [FGR+12] and to more general real-valued queries using the reductions in [Fel16a].\nThe PAC learning [Val84] is defined as follows.\nDefinition 2.2. For a class C of Boolean-valued functions over a domain Z, a PAC learning algorithm for C is an algorithm that for every P distribution over Z and f ∈ C, given an error parameter ǫ > 0, failure probability δ > 0 and access to i.i.d. labeled examples of the form (x, f(x)) where x ∼ P , outputs a hypothesis function h that, with probability at least 1 − δ, satisfies Prx∼P [h(x) 6= f(x)] ≤ ǫ.\nWe next define one-vs-many decision problems, which will be used in the proofs in our Section 3 and Section 4.\nDefinition 2.3 (Decision problem B(D, D0)). Let D be a set of distributions and D0 a reference distribution over a set X. We denote by B(D, D0) the decision problem where we are given access to a distribution D ∈ D ∪ {D0} and wish to distinguish whether D ∈ D or D = D0.\n3 Separation of (k + 1)-wise from k-wise queries\nWe start by describing the concept class C that we use to prove Theorem 1.1. Let ℓ and k be positive integers with ℓ ≥ k + 1. The domain will be Fℓp. For every a = (a1, . . . , aℓ) ∈ Fℓp, we consider the hyperplane\nHypa . = {z = (z1, . . . , zℓ) ∈ Fℓp : zℓ = a1z1 + · · ·+ aℓ−1zℓ−1 + aℓ}.\nWe then define the Boolean-valued function fa : F ℓ p → {±1} to be the indicator function of the subset Hypa ⊆ Fℓp, i.e., for every z ∈ Fℓp,\nfa(z) = { +1 if z ∈ Hypa, −1 otherwise.\nThen, we will consider the concept classes Cℓ .= {fa : a ∈ Fℓp}. We denote C . = Ck+1. We start by stating our upper bound on the (k + 1)-wise SQ complexity of the distribution-independent PAC learning of Ck+1. Lemma 3.1 ((k + 1)-wise upper bound). Let p be a prime number and k be a positive integer. There exists a distribution-independent PAC learning algorithm for Ck+1 that makes at most t · log(1/ǫ) queries to STAT(k+1)(ǫ/t), for some t = Ok(log p).\nWe next state our lower bound on the k-wise SQ complexity of the same tasks considered in Lemma 3.1.\nLemma 3.2 (k-wise lower bound). Let p be a prime number and ℓ, k be positive integers with ℓ ≥ k + 1 and k = O(p). There exists t = Ω ( p(ℓ−k)/4 ) such that any distribution-independent PAC learning alogrithm for Cℓ with error at most 1/2− 2/t that is given access to STAT(k)(1/t) needs at least t queries.\nNote that Lemma 3.1 and Lemma 3.2 imply Theorem 1.1."
    }, {
      "heading" : "3.1 Upper bound",
      "text" : "Notation We first introduce some notation that will be useful in the description of our algorithm. For any matrix M with entries in the finite field Fp, we denote by rk(M) the rank of M over Fp. Let (a1, . . . , ak+1) ∈ Fk+1p be the unknown vector that defines fa and P be the unknown distribution over tuples (z1, . . . , zk+1) ∈ Fk+1p .\nNote that Hypa is an affine subspace of F k+1 p . To simplify our treatment of affine subspaces, we embed the points of Fk+1p into F k+2 p by mapping each z ∈ Fk+1p to (z, 1). This embedding maps every affine subspace V of Fk+1p to a linear subspace W of F k+2 p , namely the span of the\nimage of V under our embedding. Note that this mapping is one-to-one and allows us to easily recover V from W as V = {z ∈ Fk+1p | (z, 1) ∈ W}. Hence given k + 1 examples\n( (z1,1, . . . , z1,k+1), b1 ) , ( (z2,1, . . . , z2,k+1), b2 ) , . . . , ( (zk+1,1, . . . , zk+1,k+1), bk+1 )\nwe define the matrix:\nZ . =   z1,1 z1,2 · z1,k+1 1 z2,1 z2,2 · z2,k+1 1 · · · · · · · · · ·\nzk+1,1 zk+1,2 · zk+1,k+1 1\n  . (1)\nFor ℓ ∈ [k + 1] we also denote by Zℓ the matrix that consists of the top ℓ rows of Z. Further, for a (k + 1)-wise query function φ ( (z1, b1), . . . , (zk+1, bk+1) ) , we use Z to refer to the matrix obtained from the inputs to the function. Let Q be the distribution defined by sampling a random example ( (z1, . . . , zk+1), b ) , conditioning on the event that b = 1 and outputting (z1, . . . , zk+1, 1). Note that if the examples from which Z is built are positively labeled i.i.d. examples then each row of Z is sampled i.i.d. from Q and hence Zℓ is distributed according to Q\nℓ. We denote by 1k+1 the all +1’s vector of length k + 1.\nLearning algorithm We start by explaining the main ideas behind the algorithm. On a high level, in order to be able to use (k + 1)-wise SQs to learn the unknown subspace, we need to make sure that there exists an affine subspace that contains most of the probability mass of the positively-labeled points and that is spanned by k + 1 random positively-labeled points with noticeable probability. Here, the probability is with respect to the unknown distribution over labeled examples. Thus, for positively labeled tuples (z1,1, . . . , z1,k+1), (z2,1, . . . , z2,k+1), . . . , (zk+1,1, . . . , zk+1,k+1), we consider the (k + 1)× (k + 2) matrix Z defined in Equation (1). If W is the row-span of Z, then the desired (unknown) affine subspace is the set V of all points (z1, . . . , zk+1) such that (z1, . . . , zk+1, 1) ∈ W .\nIf the (unknown) distribution over labeled examples is such that with noticeable probability, k + 1 random positively-labeled points form a full-rank linear system (i.e., the matrix Z has full-rank with noticeable probability conditioned on (b1, . . . , bk+1) = 1\nk+1), we can use (k + 1)- wise SQs to find, one bit at a time, the (k + 1)-dimensional row-span W of Z, and we can then output the set V of all points (z1, . . . , zk+1) such that (z1, . . . , zk+1, 1) ∈ W as the desired affine subspace (below, we refer to this step as the Recovery Procedure).\nWe now turn to the (more challenging) case where the system is not full-rank with noticeable probability (i.e., the matrix Z is rank-deficient with high probability conditioned on (b1, . . . , bk+1) = 1\nk+1). Then, the system has rank at most i with high probability, for some i < k + 1. There is a large number of possible i-dimensional subspaces and therefore it is no longer clear that there exists a single i-dimensional subspace that contains most of the mass of the positively-labeled points. However, we demonstrate that for every i, if the rank of Z is at most i with sufficiently high probability, then there exists a fixed subspace W of dimension at most i that contains a large fraction of the probability under the row-distribution of Z (it turns out that if this subspace has rank equal to i, then it should be unique). We can then use (k+1)-wise SQs to output the affine subspace V consisting of all points (z1, . . . , zk+1) such that (z1, . . . , zk+1, 1) ∈ W (via the Recovery Procedure).\nThe general description of the algorithm is given in Algorithm 1, and the Recovery Procedure (allowing the reconstruction of the affine subspace V ) is separately described in Algorithm 2. We denote the indicator function of event E by 1(E). Note that the statistical query corresponding to the event 1(E) gives an estimate of the probability of E.\nAlgorithm 1 (k + 1)-wise SQ Algorithm Inputs. k ∈ N, error probability ǫ > 0. Output. Function f : Fk+1p → {±1}. 1: Set tolerance of each SQ to τ = (ǫ/2c·(k+2))(k+1) k+3\n, where c > 0 is a large enough absolute constant.\n2: Define the threshold τi = 2 c·(k+2−i) · k · τ1/(k+1)k+2−i for every i ∈ [k + 1]. 3: Ask the SQ φ(z, b) . = 1(b = 1) and let w be the response. 4: if w ≤ ǫ− τ then 5: Output the all −1’s function. 6: end if 7: Let φ̃ ( (z1, b1), . . . , (zk+1, bk+1) ) . = 1((b1, . . . , bk+1) = 1 k+1).\n8: Ask the SQ φ̃ and let v be the response. 9: for i = k + 1 down to 1 do\n10: Let φi ( (z1, b1), . . . , (zk+1, bk+1) ) . = 1((b1, . . . , bk+1) = 1\nk+1 and rk(Z) = i). 11: Ask the SQ φi and let vi be the response. 12: if vi/v ≥ τi then 13: Run Recovery Algorithm on input (i, vi) and let V̂ be the subspace of F k+1 p it outputs. 14: Define function f : Fk+1p → {−1, 1} by: 15: f(z1, . . . , zk+1) = +1 if (z1, . . . , zk+1) ∈ V̂ . 16: f(z1, . . . , zk+1) = −1 otherwise. 17: Return f . 18: end if\n19: end for\nAlgorithm 2 Recovery Procedure Input. Integer i ∈ [k + 1]. Output. Subspace V̂ of Fk+1p of dimension i.\n1: Let mi = (k + 2) · i · ⌈log p⌉ 2: for each bit j ≤ mi do 3: Define event Ej(Z) = 1(bit j of row span of Z is 1). 4: Let φi,j ( (z1, b1), . . . , (zk+1, bk+1) ) . = 1(Ej(Z) and (b1, . . . , bk+1) = 1\nk+1 and rk(Z) = i). 5: Ask the SQ φi,j and let ui,j be the response. 6: if ui,j/vi ≥ (9/10) then 7: Set bit j in binary representation of Ŵ to 1. 8: else\n9: Set bit j in binary representation of Ŵ to 0. 10: end if\n11: end for 12: Let V̂ be the set all points (z1, . . . , zk+1) such that (z1, . . . , zk+1, 1) ∈ Ŵ .\nAnalysis We now turn to the analysis of Algorithm 1 and the proof of Lemma 3.1. We will need the following lemma, which shows that if the rank of Z is at most i with high probability, then there is a fixed subspace of dimension at most i containing most of the probability mass\nunder the row-distribution of Z.\nLemma 3.3. Let i ∈ [k + 1]. If PrQk+1 [rk(Z) ≤ i] ≥ 1 − ξ, then there exists a subspace W of F k+2 p of dimension at most i such that Prz∼Q[z /∈ W ] ≤ ξ1/k.\nRemark 3.4. We point out that the exponential dependence on 1/k in the probability upper bound in Lemma 3.3 is tight. To see this, let p = 2, and {e1, . . . , ek} be the standard basis in Fk2. Consider the base distribution P on Fk2 that puts probability mass 1 − α on e1, and probability mass α/(k − 1) on each of e2, e3, . . . , ek. Then, a Chernoff bound implies that if we draw k i.i.d. samples from P , then the dimension of their span is at most 2 · α · k with probability at least 1 − exp(−k). On the other hand, for any subspace W of Fk2 of dimension 2 · α · k, the probability that a random sample from P lies inside W is only 1−Θ(α).\nTo prove Lemma 3.3, we will use the following proposition.\nProposition 3.5. Let ℓ ∈ [k+1], i ∈ [ℓ−1] and η > 0. If PrQℓ [rk(Zℓ) ≤ i] ≥ 1−η, then for every ν ∈ (0, 1], either there exists a subspace W of Fk+2p of dimension i such that Prz∼Q[z /∈ W ] ≤ ν or PrQi [rk(Zi) ≤ i− 1] ≥ 1− η/ν.\nProof. Let p . = PrQi [rk(Zi) ≤ i− 1]. For every (fixed) matrix Ai ∈ Fi×(k+2)p , define\nµ(Ai) . = Pr\nQℓ [rk(Zℓ) ≤ i | Zi = Ai].\nThen,\nPr Qℓ [rk(Zℓ) ≤ i] = p+ (1− p) · Pr Qℓ [rk(Zℓ) ≤ i | rk(Zi) = i]\n= p+ (1− p) · EQi [ µ(Zi) ∣∣∣∣ rk(Zi) = i ] .\nSince PrQℓ [rk(Zℓ) ≤ i] ≥ 1− η, we have that\nEQi [ µ(Zi) ∣∣∣∣ rk(Zi) = i ] ≥ 1− η/(1− p).\nHence, there exists a setting Ai ∈ Fi×(k+2)p of Zi such that rk(Ai) = i and\nPr[rk(Zℓ) ≤ i | Zi = Ai] ≥ 1− η/(1− p).\nWe let W be the Fp-span of the rows of Ai. Note that the dimension of W is equal to i and that Prz∼Q[z /∈ W ] ≤ η/(1− p). Thus, we conclude that for every ν ∈ (0, 1], either p ≥ 1− η/ν or Prz∼Q[z /∈ W ] ≤ ν, as desired.\nWe now complete the proof of Lemma 3.3.\nProof of Lemma 3.3. Starting with ℓ = k + 1 and η = ξ, we inductively apply Proposition 3.5 with ν = ξ1/k until we either get the desired subspace W or we get to the case where i = 1. In this case, we have that PrQℓ [rk(Zℓ) ≤ 1] ≥ 1− ξ1/k for ℓ ≥ 2. Since the last column of Zℓ is the all 1’s vector, we conclude that there exists z∗ ∈ Fk+1p such that Prz∼Q[z 6= (z∗, 1)] ≤ ξ1/k. We can then set our subspace W to be the Fp-span of the vector (z ∗, 1).\nFor the proof of Lemma 3.1 we will also need the following lemma, which states sufficient conditions under which the Recovery Procedure (Algorithm 2) succeeds.\nLemma 3.6. Let i ∈ [k + 1]. Assume that in Algorithm 1, v > ǫk+1/2 and vi/v ≥ τi. If there exists a subspace W of Fk+2p of dimension equal to i such that\nPr z∼Q [z /∈ W ] < τi 4 · (k + 1) , (2)\nthen the affine subspace V̂ output by Algorithm 2 (i.e., the Recovery Procedure) consists of all points (z1, . . . , zk+1) such that (z1, . . . , zk+1, 1) ∈ W .\nWe note that Lemma 3.6 would still hold under quantitatively weaker assumptions on v, vi/v and Prz∼Q[z /∈ W ] in Equation (2). In order to keep the expressions simple, we however choose to state the above version which will be sufficient to prove Lemma 3.1. The proof of Lemma 3.6 appears in Appendix A.1. We are now ready to complete the proof of Lemma 3.1.\nProof of Lemma 3.1. If Algorithm 1 terminates at Step 5, then the error of the output hypothesis is at most ǫ, as desired. Henceforth, we assume that Algorithm 1 does not terminate at Step 5. Then, we have that Pr[b = 1] > ǫ, and hence Pr[(b1, . . . , bk+1) = 1\nk+1] > ǫk+1. Thus, the value v obtained in Step 8 of Algorithm 1 satisfies v > ǫk+1 − τ ≥ ǫk+1/2, where the last inequality follows from the setting of τ . Let i∗ be the first (i.e., largest) value of i ∈ [k + 1] for which vi/v ≥ τi. To prove that such an i∗ exists, we proceed by contradiction, and assume that for all i ∈ [k+1], it is the case that vi/v < τi. Note that Z has an all 1’s column, so it has rank at least 1. Moreover, it has rank at most k + 1. Therefore, we have that\n1 = Pr[1 ≤ rk(Z) ≤ k + 1 | (b1, . . . , bk+1) = 1k+1]\n=\nk+1∑\ni=1\nPr[rk(Z) = i | (b1, . . . , bk+1) = 1k+1]\n≤ k+1∑\ni=1\nvi + τ v − τ\n≤ 2 · k+1∑\ni=1\nvi + τ\nv\n≤ 2 · k+1∑\ni=1\n( vi v + 2τ ǫk+1 )\n< 2 · k+1∑\ni=1\nτi + 4 · (k + 1) · τ\nǫk+1 .\nUsing the fact that τi is monotonically non-increasing in i and the settings of τ1 and τ , the last inequality gives\n1 ≤ 2 · (k + 1) · τ1 + 4 · (k + 1) · τ\nǫk+1 < 1,\na contradiction.\nWe now fix i∗ as above. We have that\nPr[rk(Z) ≤ i∗ | (b1, . . . , bk+1) = 1k+1] = 1− k+1∑\ni=i∗+1\nPr[rk(Z) = i | (b1, . . . , bk+1) = 1k+1]\n≥ 1− k+1∑\ni=i∗+1\nvi + τ v − τ\n≥ 1− 2 · k+1∑\ni=i∗+1\n( vi v + 2τ ǫk+1 )\n> 1− 2 · k+1∑\ni=i∗+1\n(τi + 2 · τ\nǫk+1 )\n≥ 1− 4 · k+1∑\ni=i∗+1\nτi\n≥ 1− 4 · k · τi∗+1.\nBy Lemma 3.3, there exists a subspace W of Fk+2p of dimension at most i ∗ such that\nPr z∼Q\n[z /∈ W ] ≤ (4 · k)1/k · τ1/ki∗+1. (3)\nProposition 3.7. For every i ∈ [k], we have that (k + 1) · (4 · k)1/k · τ1/ki+1 ≤ τi/4. We note that Proposition 3.7 follows immediately from the definitions of τi and τ (and by letting c by a sufficiently large positive absolute constant). Moreover, Proposition 3.7 (applied with i = i∗) along with Equation (3) imply that Prz∼Q[z /∈ W ] is at most τi∗/(4(k + 1)).\nBy a union bound, we get that with probability at least\n1− (k + 1) · Pr z∼Q [z /∈ W ] ≥ 1− τi∗ 4 , (4)\nall the rows of Z belong to W . Since vi∗/v ≥ τi∗, we also have that:\nPr[rk(Z) = i∗ | (b1, . . . , bk+1) = 1k+1] ≥ vi∗ − τ v + τ\n≥ 1 2 · (vi∗ − τ) v ≥ 1 2 · (τi∗ − 2 · τ ǫk+1 ) ≥ τi∗ 3\n(5)\nCombining Equation (4) and Equation (5), we get that the rank of W is equal to i∗. Let V be the affine subspace consisting of all points (z1, . . . , zk+1) such that (z1, . . . , zk+1, 1) ∈ W . By Lemma 3.6, we get that Algorithm 2 (and hence Algorithm 1) correctly recovers the affine subspace V .\nWe note that the function f output by Algorithm 1 is the ±1 indicator of a subspace of the true hyperplane Hypa. To see this, note that f is the ±1 indicator function of the subspace V , and by Equations (3) and (5), we have that with probability at least τi∗/12 over Z ∼ Qk+1, all the columns of Z belong to W and rk(Z) = i∗. Since the dimension of W is equal to i∗ and since we are conditioning on (b1, . . . , bk+1) = 1 k+1, this implies that the correct label of all the\npoints in V is +1. Hence, f only possibly errs on positively-labeled points (by wrongly giving them the label −1). Moreover, Algorithm 1 ensures that the output function f gives the label +1 to every (z1, . . . , zk+1) ∈ Fk+1p for which (z1, . . . , zk+1, 1) ∈ W . Therefore, the function f that is output by Algorithm 1 (when it does not terminate at Step 5) has error at most the right hand side of (3). So to upper-bound the error probability, it suffices for us to verify that the right-hand side of (3) is at most ǫ. This is obtained by applying the next proposition with i = i∗ + 1.\nProposition 3.8. For every i ∈ [k + 1], we have that (4 · k)1/k · τ1/ki ≤ ǫk. The proof of Proposition 3.8 follows immediately from the definitions of τi and τ and by letting c be a sufficiently large positive absolute constant. The number of queries performed by the (k+1)-wise algorithm is at most O(k2 · log p), and their tolerance is τ ≥ (ǫ/2c·(k+2))(k+1)k+3 , where c is a positive absolute constant. Finally, we remark that the dependence of the SQ complexity of the above algorithm on the error parameter ǫ is ǫ−k O(k)\n. It can be improved to a linear dependence on 1/ǫ by learning with error 1/3 and then using boosting in the standard way (boosting in the SQ model works essentially as in the regular PAC model [AD93])."
    }, {
      "heading" : "3.2 Lower bound",
      "text" : "Our proof of lower bound is a generalization of the lower bound in [Fel16b] (for ℓ = 2 and k = 1). It relies on a notion of combined randomized statistical dimension (“combined” refers to the fact that it examines a single parameter that lower bounds both the number of queries and the inverse of the tolerance). In order to apply this approach we need to extend it to k-wise queries. This extension follows immediately from a simple observation. If we define the domain to be X ′ . = Xk and the input distribution to be D′ . = Dk then asking a k-wise query φ : Xk → [−1, 1] to STAT (k) D (τ) is equivalent to asking a unary query φ : X\n′ → [−1, 1] to STAT(k)D′ (τ). Using this observation we define the k-wise versions of the notions from [Fel16b] and give their properties that are needed for the proof of Lemma 3.2."
    }, {
      "heading" : "3.2.1 Preliminaries",
      "text" : "Combined randomized statistical dimension is based on the following notion of average discrimination.\nDefinition 3.9 (k-wise average κ1-discrimination). Let k be any positive integer. Let µ be a probability measure over distributions over X and D0 be a reference distribution over X. Then,\nκ̄ (k) 1 (µ,D0) . = sup\nφ:Xk→[−1,+1]\n{ ED∼µ[|Dk[φ]−Dk0 [φ]|] } .\nWe denote the problem of PAC learning a concept class C of Boolean functions up to error ǫ by LPAC(C, ǫ). Let Z be the domain of the Boolean functions in C. For any distribution D0 over labeled examples (i.e., over Z × {±1}), we define the Bayes error rate of D0 to be\nerr(D0) = ∑\nz∈Z\nmin{D0(z, 1), D0(z,−1)} = min h:Z→{±1} Pr (z,b)∼D0 [h(z) 6= b].\nDefinition 3.10 (k-wise combined randomized statistical dimension). Let k be any positive integer. Let D be a set of distributions and D0 a reference distribution over X. The k-wise combined randomized statistical dimension of the decision problem B(D, D0) is then defined as\ncRSD (k) κ̄1 (B(D, D0)) . = sup\nµ∈SD (κ̄\n(k) 1 (µ,D0)) −1,\nwhere SD denotes the set of all probability distributions over D. Further, for any concept class C of Boolean functions over a domain Z, and for any ǫ > 0, the k-wise combined randomized statistical dimension of LPAC(C, ǫ) is defined as\ncRSD (k) κ̄1 (LPAC(C, ǫ)) . = sup\nD0∈SZ×{±1}:err(D0)>ǫ\ncRSD (k) κ̄1 (B(DC , D0)),\nwhere DC .= {P f : P ∈ SZ , f ∈ C} with P f denoting the distribution on labeled examples (x, f(x)) with x ∼ P .\nThe next theorem lower bounds the randomized k-wise SQ complexity of PAC learning a concept class in terms of its k-wise combined randomized statistical dimension.\nTheorem 3.11 ([Fel16b]). Let C be a concept class of Boolean functions over a domain Z, k be a positive integer and ǫ, δ > 0. Let d . = cRSD\n(k) κ̄1 (LPAC(C, ǫ)). Then, the randomized k-wise SQ\ncomplexity of solving LPAC(C, ǫ − 1/ √ d) with access to STAT(k)(1/ √ d) and success probability\n1− δ is at least (1− δ) · √ d− 1.\nTo lower bound the statistical dimension we will use the following “average correlation” parameter introduced in [FGR+12].\nDefinition 3.12 (k-wise average correlation). Let k be any positive integer. Let D be a set of distributions and D0 a reference distribution over X. Assume that the support of every distribution D ∈ D is a subset of the support of D0. Then, for every x ∈ Xk, define D̂(x) .= Dk(x) Dk0 (x) − 1. Then, the k-wise average correlation is defined as\nρ(k)(D, D0) .= 1 |D|2 · ∑\nD,D′∈D\n|Dk0 [D̂ · D̂′]|.\nLemma 3.13 relates the average correlation to the average discrimination (from Definition 3.9).\nLemma 3.13 ([Fel16b]). Let k be any positive integer. Let D be a set of distributions and D0 a reference distribution over X. Let µ be the uniform distribution over D. Then,\nκ̄ (k) 1 (µ,D0) ≤ 4 · √ ρ(k)(D, D0)."
    }, {
      "heading" : "3.2.2 Proof of Lemma 3.2",
      "text" : "Denote X . = Fℓp × {±1}. Let D be the set of all distributions over Xk that are obtained by sampling from any given distribution over (Fℓp) k and labeling the k samples according to any given hyperplane indicator function fa. Let D0 be the uniform distribution over X k. We now\nshow that cRSDκ̄1(B(D, D0)) = Ω ( p(ℓ−k)/2 ) . By definition,\ncRSDκ̄1(B(D, D0)) . = sup\nµ∈SD (κ̄1(µ,D0))\n−1.\nWe now choose the distribution µ. For a ∈ Fℓp, we define Pa to be the distribution over Fℓp that has density α = 1/(2(pℓ − pℓ−1)) on each of the pℓ − pℓ−1 points outside Hypa, and density β = 1/pℓ−1 −αp+α = 1/(2pℓ−1) on each of the pℓ−1 points inside Hypa. We then define Da to be the distribution obtained by sampling k i.i.d. random examples of Hypa, the marginal of each over Fℓp being Pa. Let D′ . = {Da | a ∈ Fℓp}, and let µ be the uniform distribution over D′. By\nLemma 3.13, we have that κ̄1(µ,D0) ≤ 4 · √ ρ(D, D0), so it is enough to upper bound ρ(D, D0).\nWe first note that for a, a′ ∈ Fℓp, we have\nD0[D̂a · D̂a′ ] = E(z,b)∼D0 [D̂a(z, b) · D̂a′(z, b)]\n= E(z,b)∼D0\n[( Da(z, b)\nD0(z, b) − 1\n) · ( Da′(z, b)\nD0(z, b) − 1\n)]\n= E(z,b)∼D0\n[ Da(z, b) ·Da′(z, b)\nD20(z, b) − Da(z, b) D0(z, b) − Da′(z, b) D0(z, b) + 1\n]\n= E(z,b)∼D0\n[ Da(z, b) ·Da′(z, b)\nD20(z, b)\n] − 2 · E(z,b)∼D0 [ Da(z, b)\nD0(z, b)\n] + 1\n= 22k · p2kℓ · E(z,b)∼D0 [Da(z, b) ·Da′(z, b)]− 2k+1 · pkℓ · E(z,b)∼D0 [Da(z, b)] + 1 We now compute each of the two expectations that appear in the last equation above.\nProposition 3.14. For every a ∈ Fℓp,\nE(z,b)∼D0 [Da(z, b)] = 1 2k · ( 1 p · β + ( 1− 1 p ) · α )k =\n1\n2k · pk·ℓ .\nThe proof of Proposition 3.14 appears in the appendix.\nProposition 3.15. For every a, a′ ∈ Fℓp,\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] =    1 2k · ( 1p · β2 + (1− 1p ) · α2)k if Hypa = Hypa′ , 1 2k · (α2 · (1− 2p ))k if Hypa ∩ Hypa′ = ∅, 1 2k · ( β2 p2 + α 2 · (1− 2p + 1p2 ))k otherwise.\nThe proof of Proposition 3.15 appears in the appendix. Using Proposition 3.14 and Proposition 3.15, we now compute D0[D̂a · D̂a′ ]. Proposition 3.16. For every a, a′ ∈ Fℓp,\nD0[D̂a · D̂a′ ] =    (p+ 1− 1p−1 )k − 1 if Hypa = Hypa′ , 1 2k · (1− 2 p )k (1− 1 p )2k − 1 if Hypa ∩ Hypa′ = ∅,\n0 otherwise.\nThe proof of Proposition 3.16 appears in the appendix. When computing ρ(D, D0), we will also use the following simple proposition.\nProposition 3.17. 1. The number of pairs (a, a′) ∈ (Fℓp)2 such that Hypa = Hypa′ is equal to pℓ.\n2. The number of pairs (a, a′) ∈ (Fℓp)2 such that Hypa and Hypa′ are distinct and parallel is equal to pℓ · (p− 1). 3. The number of pairs (a, a′) ∈ (Fℓp)2 such that Hypa and Hypa′ are distinct and intersecting is equal to p2·ℓ − pℓ+1.\nUsing Proposition 3.16 and Proposition 3.17, we are now ready to compute ρ(D, D0) as follows\nρ(D, D0) ≤ 1 p2·ℓ · [ pℓ · (p+ 1− 1 p− 1) k + pℓ · (p− 1) + p2·ℓ · 0 ]\n≤ O ( 1\npℓ−k\n) + 1\npℓ−1\n= O\n( 1\npℓ−k\n) ,\nwhere we used above the assumption that k = O(p). We deduce that κ̄1(µ,D0) = O\n( 1/p(ℓ−k)/2 ) ,\nand hence cRSDκ̄1(B(D, D0)) = Ω ( p(ℓ−k)/2 ) . This lower bound on cRSDκ̄1(B(D, D0)), along\nwith Definition 3.10, Theorem 3.11 and the fact that D0 has Bayes error rate equal to 1/2, imply Lemma 3.2."
    }, {
      "heading" : "4 Reduction for flat distributions",
      "text" : "To prove Theorem 1.3 we use the characterization of the SQ complexity of the problem of estimating Dk[φ] for D ∈ D using a notion of statistical dimension from [Fel16b]. Specifically, we use the characterization of the complexity of solving this problem using unary SQs and also the generalization of this characterization that characterizes the complexity of solving a problem using k-wise SQs. The latter is equal to 1 (since a single k-wise SQ suffices to estimate Dk[φ]). Hence the k-wise statistical dimension is also equal to 1. We then upper bound the unary statistical dimension by the k-wise statistical dimension. The characterization then implies that an upper bound on the unary statistical dimension gives an upper bound on the SQ complexity of estimating Dk[φ].\nWe also give a slightly different way to define flatness that makes it easier to extend our results to other notions of divergence.\nDefinition 4.1. Let D be a set of distributions over X. Define R∞(D) .= inf\nD̄∈SX sup D∈D\nD∞(D‖D̄),\nwhere SX denotes the set of all probability distributions over X and\nD∞(D‖D̄) .= sup y∈X ln Prx∼D[x = y] Prx∼D̄[x = y]\ndenotes the max-divergence. We say that D is γ-flat if R∞(D) ≤ ln γ. For simplicity, we will start by relating the k-wise SQ complexity to unary SQ complexity for decision problems. The statistical dimension for this type of problems is substantially simpler than for the general problems but is sufficient to demonstrate the reduction. We then build on the results for decision problems to obtain the proof of Theorem 1.3."
    }, {
      "heading" : "4.1 Decision problems",
      "text" : "The k-wise generalization of the statistical dimension for decision problems from [Fel16b] is defined as follows.\nDefinition 4.2. Let k be any positive integer. Consider a set of distributions D and a reference distribution D0 over X. Let µ be a probability measure over D and let τ > 0. The k-wise maximum covered µ-fraction is defined as\nκ1-frac (k)(µ,D0, τ) . = sup\nφ:Xk→[−1,+1]\n{ Pr\nD∼µ [|Dk[φ]−Dk0 [φ]| > τ ]\n} .\nDefinition 4.3 (k-wise randomized statistical dimension of decision problems). Let k be any positive integer. For any set of distributions D, a reference distribution D0 over X and τ > 0, we define\nRSD(k)κ1 (B(D, D0), τ) . = sup\nµ∈SD (κ1-frac\n(k)(µ,D0, τ)) −1,\nwhere SD denotes the set of all probability distributions over D.\nAs shown in [Fel16b], RSD tightly characterizes the randomized statistical query complexity of solving the problem using k-wise queries. As observed before, the k-wise versions below are implied by the unary version in [Fel16b] simply by defining the domain to be X ′ . = Xk and the set of input distributions to be D′ .= {Dk | D ∈ D}. Theorem 4.4 ([Fel16b]). Let B(D, D0) be a decision problem, τ > 0, δ ∈ (0, 1/2), k ∈ N and d = RSD(k)κ1 (B(D, D0), τ). Then there exists a randomized algorithm that solves B(D, D0) with success probability ≥ 1− δ using d · ln(1/δ) queries to STAT(k)D (τ/2). Conversely, any algorithm that solves B(D, D0) with success probability ≥ 1 − δ requires at least d · (1 − 2δ) queries to STAT\n(k) D (τ).\nWe will also need the following dual formulation of the statistical dimension given in Theorem 4.3.\nLemma 4.5 ([Fel16b]). Let k be any positive integer. For any set of distributions D, a reference distribution D0 over X and τ > 0, the statistical dimension RSD (k) κ1 (B(D, D0), τ) is equal to the smallest d for which there exists a distribution P over functions from Xk to [−1,+1] such that for every D ∈ D,\nPr φ∼P\n[|Dk[φ]−Dk0 [φ]| > τ ] ≥ 1\nd .\nWe can now state the relationship between RSD(k)κ1 and RSD (1) κ1 for any γ-flat D.\nLemma 4.6. Let γ ≥ 1, τ > 0 and k ∈ N. Let X be a domain, D be a γ-flat class of distributions over X and D0 be any distribution over X. Then\nRSD(1)κ1 (B(D, D0), τ/(2k)) ≤ 4k · γk−1\nτ · RSD(k)κ1 (B(D, D0), τ).\nProof. Let d . = RSD(k)κ1 (B(D, D0), τ). Fact 4.5 implies the existence of a distribution P over k-wise functions such that for every D ∈ D,\nPr φ∼P\n[|Dk[φ]−Dk0 [φ]| > τ ] ≥ 1\nd .\nWe now fix D and let φ be such that |Dk[φ]−Dk0 [φ]| > τ . By the standard hybrid argument,\nEj∼[k] [∣∣∣DjDk−j0 [φ]−Dj−1Dk−j+10 [φ] ∣∣∣ ] > τ\nk , (6)\nwhere j ∼ [k] denotes a random and uniform choice of j from [k]. This implies that\nEj∼[k]Ex<j∼Dj−1Ex>j∼D k−j 0\n[∣∣∣∣D[φ(x<j , ·, x>j)]−D0[φ(x<j , ·, x>j)] ∣∣∣∣ ] > τ\nk .\nBy an averaging argument (and using the fact that φ takes values between −1 and +1), we get that with probability at least τ/(4 · k) over the choice of j ∼ [k], x<j ∼ Dj−1 and x>j ∼ Dk−j0 , we have that ∣∣∣∣D[φ(x<j , ·, x>j)]−D0[φ(x<j , ·, x>j)] ∣∣∣∣ > τ\n2 · k .\nSince D is a γ-flat class of distributions, there exists a (fixed) distribution D̄ over X such that for every measurable event E ⊂ X , Prx∼D[x ∈ E] ≤ γ · Prx∼D̄[x ∈ E]. Thus, we can replace the unknown input distribution D by the distribution D̄ and get that, with probability at least τ/(4 · k · γk−1) over the choice of j ∼ [k], x<j ∼ D̄j−1 and x>j ∼ Dk−j0 , we have\n∣∣∣∣D[φ(x<j , ·, x>j)]−D0[φ(x<j , ·, x>j)] ∣∣∣∣ > τ\n2 · k . (7)\nWe now consider the following distribution P ′ over unary SQ functions (i.e., over [−1,+1]X): Independently sample φ from P , j uniformly from [k], x<j ∼ D̄j−1 and x>j ∼ Dk−j0 , and output the (unary) function φ′(x) = φ(x<j , x, x>j). Then, for every D ∈ D, we have that with probability at least 1d · τ4k · 1γk−1 over the choice of φ′ from P ′, we have that |D[φ′]−D0[φ′]| > τ/(2 · k). Thus, by Fact 4.5\nRSD(1)κ1 ( B(D, D0), τ 2 · k ) ≤ 4d · γ k−1 · k τ .\nLemma 4.6 together with the characterization in Theorem 4.4 imply the following upper bound on the SQ complexity of a decision problem in terms of its k-wise SQ complexity.\nTheorem 4.7. Let γ ≥ 1, τ > 0 and k ∈ N. Let X be a domain, D be a γ-flat class of distributions over X and D0 be any distribution over X. If there exists an algorithm that, with probability at least 2/3 solves B(D, D0) using t queries to STAT(k)D (τ), then for every δ > 0, there exists an algorithm that, with probability at least 1−δ solves B(D, D0) using t·12k·γk−1·ln(1/δ)/τ queries to STAT\n(1) D (τ/(4k))."
    }, {
      "heading" : "4.2 General problems",
      "text" : "We now define the general class of problems over sets of distributions and a notion of statistical dimension for these types of problems.\nDefinition 4.8 (Search problems). A search problem Z over a class D of distributions and a set F of solutions is a mapping Z : D → 2F \\ {∅}, where 2F denotes the set of all subsets of F . Specifically, for every distribution D ∈ D, Z(D) ⊆ F is the (non-empty) set of valid solutions for D. For a solution f ∈ F , we denote by Zf the set of all distributions for which f is a valid solution.\nDefinition 4.9 (Statistical dimension for search problems [Fel16b]). For τ > 0, k ∈ N, a domain X and a search problem Z over a class of distributions D over X and a set of solutions F , we define the k-wise statistical dimension with κ1-discrimination τ of Z as\nSD(k)κ1 (Z, τ) . = sup\nD0∈SX inf f∈F\nRSD(k)κ1 (B(D \\ Zf , D0), τ),\nwhere SX denotes the set of all probability distributions over X.\nLemma 4.10 lower-bounds the deterministic k-wise SQ complexity of a search problem in terms of its (k-wise) statistical dimension.\nTheorem 4.10 ([Fel16b]). Let Z be a search problem, τ > 0 and k ∈ N. The deterministic k-wise SQ complexity of solving Z with access to STAT(k)(τ) is at least SD(k)κ1 (Z, τ).\nThe following theorem from [Fel16b] gives an upper bound on the SQ complexity of a search problem in terms of its statistical dimension. It relies on the multiplicative weights update method to reconstruct the unknown distribution sufficiently well for solving the problem. The use of this algorithm introduces dependence on KL-radius of D. Namely, we define\nRKL(D) .= inf D̄∈SX sup D∈D KL(D‖D̄),\nwhere KL(·‖·) denotes the KL-divergence.\nTheorem 4.11 ([Fel16b]). Let Z be a search problem, τ, δ > 0 and k ∈ N. There is a randomized k-wise SQ algorithm that solves Z with success probability 1− δ using\nO ( SD(k)κ1 (Z, τ) · RKL(D) τ2 · log ( RKL(D) τ · δ ))\nqueries to STAT(k)(τ/3).\nNote that KL-divergence between two distributions is upper-bounded (and is usually much smaller) than the max-divergence we used in the definition of γ-flatness. Specifically, if D is γ-flat then RKL(D) ≤ ln γ. We are now ready to prove Theorem 1.3 which we restate here for convenience.\nTheorem 1.3 (restated). Let γ ≥ 1, τ > 0 and k be any positive integer. Let X be a domain and D be a γ-flat class of distributions over X. There exists a randomized algorithm that given any δ > 0 and a k-ary function φ : Xk → [−1, 1], estimates Dk[φ] within τ for every (unknown) D ∈ D with success probability at least 1− δ using\nÕ\n( γk−1 · k3\nτ3 · log(1/δ)\n)\nqueries to STAT (1) D (τ/(6 · k)).\nProof. We first observe that the task of estimating Dk[φ] up to additive τ can be viewed as a search problem Z over the set D of distributions and over the class F of solutions that corresponds to the interval [−1,+1]. Next, observe that one can easily estimate Dk[φ] up to additive τ using a single query to STAT\n(k) D (τ). Lemma 4.10 implies that SD (k) κ1 (Z, τ) = 1. By\nDefinition 4.9, for every D1 ∈ SX , there exists f ∈ F , such that RSD(k)κ1 (B(D \\ Zf , D1), τ) = 1. By Lemma 4.6,\nRSD(1)κ1 ( B(D \\ Zf , D1), τ 2 · k ) ≤ 4 · γ k−1 · k τ .\nThus, Fact 4.5 and Definition 4.9 imply that\nSD(1)κ1 (Z, τ 2 · k ) ≤ 4 · γk−1 · k τ .\nApplying Lemma 4.11, we conclude that there exists a randomized unary SQ algorithm that solves Z with probability at least 1− δ using at most\nO ( γk−1 · k3 · RKL(D)\nτ3 · log\n( k ·RKL(D)\nτ · δ\n))\nqueries to STAT(1)(τ/(6 · k)). This – along with the fact that RKL(D) ≤ ln(γ) whenever D is a γ-flat set of distributions – concludes the proof of Theorem 1.3.\nOther divergences: While the max-divergence that we used for measuring flatness suffices for the applications we give in this paper (and is relatively simple), it might be too conservative in other problems. For example, such divergence is infinite even for two Gaussian distributions with the same standard deviation but different means. A simple way to obtain a more robust version of our reduction is to use approximate max-divergence. For δ ∈ [0, 1) it is defined as:\nDδ∞(D‖D̄) . = ln sup\nE⊆X Prx∼D[x ∈ E]− δ Prx∼D̄[x ∈ E] .\nNote that D0∞(D‖D̄) = D∞(D‖D̄). Similarly, we can define a radius of D in this divergence\nRδ∞(D) . = inf\nD̄∈SX sup D∈D\nDδ∞(D‖D̄).\nNow, it is easy to see that, if Dδ∞(D‖D̄) ≤ r then Dkδ∞(Dk‖D̄k) ≤ kr. This means that if in the proof of Lemma 4.6 we use the condition R τ/(8k2) ∞ (D) ≤ ln γ instead of γ-flatness then we will obtain that the event in Equation (7) holds with probability at least\n( τ 4k − (k − 1) · τ 8k2 ) /γk−1 ≥ τ γk−1 · 8k\nover the same random choices. This implies the following generalization of Theorem 1.3.\nTheorem 4.12. Let τ > 0 and k be any positive integer. Let D be a class of distributions over a domain X and γ = exp(R τ/(8k2) ∞ (D)). There exists a randomized algorithm that given any δ > 0 and a k-ary function φ : Xk → [−1, 1], estimates Dk[φ] within τ for every (unknown) D ∈ D with success probability at least 1− δ using\nÕ\n( γk−1 · k3 · RKL(D)\nτ3 · log(1/δ)\n)\nqueries to STAT (1) D (τ/(6 · k)).\nAn alternative approach is to use Renyi divergence of order α > 1 defined as follows:\nDα(D‖D̄) .= 1 1− α · ln ( Ey∼D [( Prx∼D[x = y]\nPrx∼D̄[x = y]\n)α−1]) .\nThe corresponding radius is defined as\nRα(D) .= inf D̄∈SX sup D∈D Dα(D‖D̄).\nTo use it in our application we need the standard property of the Renyi divergence for product distributions Dα(D\nk‖D̄k) = k · Dα(D‖D̄) and also the following simple lemma from [MMR09, Lemma 1]:\nLemma 4.13. For α > 1, any two distributions D, D̄ over X and an event E ⊆ X:\nPr x∼D\n[x ∈ E] ≤ ( exp(Dα(D‖D̄)) · Pr\nx∼D̄ [x ∈ E]\n)α−1 α\n.\nWe will need the inverted version of this lemma:\nPr x∼D̄\n[x ∈ E] ≥ (Prx∼D[x ∈ E]) α α−1\nexp(Dα(D‖D̄)) .\nApplying this in the proof of Lemma 4.6 for γ = exp(Rα(D)), we obtain that the event in Equation (7) holds with probability at least\n( τ 4k ) α α−1 /γk−1.\nThis gives the following generalization of Theorem 1.3.\nTheorem 4.14. Let τ > 0, α > 1 and k be any positive integer. Let D be a class of distributions over a domain X and γ = exp(Rα(D)). There exists a randomized algorithm that given any δ > 0 and a k-ary function φ : Xk → [−1, 1], estimates Dk[φ] within τ for every (unknown) D ∈ D with success probability at least 1− δ using\nÕ ( γk−1 · ( k\nτ\n)2+ α α−1 · log(1/δ) )\nqueries to STAT (1) D (τ/(6 · k))."
    }, {
      "heading" : "4.3 Applications to solving CSPs and learning DNF",
      "text" : "We now give some examples of the application of our reduction to obtain lower bounds against k-wise SQ algorithms. Our applications for stochastic constraint satisfaction problems (CSPs) and DNF learning. We start with the definition of a stochastic CSP with a planted solution which is a pseudo-random generator based on Goldreich’s proposed one-way function [Gol00].\nDefinition 4.15. Let t ∈ N and P : {±1}t → {±1} be a fixed predicate. We are given access to samples from a distribution Pσ, corresponding to a (“planted”) assignment σ ∈ {±1}n. A sample from this distribution is a uniform-random t-tuple (i1, . . . , it) of distinct variable indices along with the value P (σi1 , . . . , σit). The goal is to recover the assignment σ when given m independent samples from Pσ. A (potentially) easier problem is to distinguish any such planted distribution from the distribution Ut in which the value is an independent uniform-random coin flip (instead of P (σi1 , . . . , σit)).\nWe say that a predicate P : {±1}t → {±1} has complexity r if r is the degree of the lowestdegree non-zero Fourier coefficient of P . It can be as large as t (for the parity function). A lower bound on the (unary) SQ complexity of solving such CSPs was shown by [FPV13] (their result is for the stronger VSTAT oracle but here we state the version for the STAT oracle).\nTheorem 4.16 ([FPV13]). Let t, q ∈ N and P : {±1}t → {±1} be a fixed predicate of complexity r. Then for any q > 0, any algorithm that, given access to a distribution D ∈ {Pσ | σ ∈ {±1}n} ∪ {Ut} decides correctly whether D = Pσ or D = Ut with probability at least 2/3 needs q/2O(t) queries to STAT\n(1) D (( log q n )r/2) .\nThe set of input distributions in this problem is 2-flat relative to Ut and it is one-to-many decision problem. Hence Theorem 4.7 implies2 the following lower bound for k-wise SQ algorithms.\nTheorem 4.17. Let t ∈ N and P : {±1}t → {±1} be a fixed predicate of complexity r. Then for any α > 0, any algorithm that, given access to a distribution D ∈ {Pσ | σ ∈ {±1}n} ∪ {Ut} decides correctly whether D = Pσ or D = Ut with probability at least 2/3 needs 2 n1−α−O(t) queries to STAT (n1−α) D ( (2/nα)r/2 · n1−α/4 ) .\nProof. Let A be a k-wise SQ algorithm using q′ queries to STAT(n 1−α)\nD\n( (2/nα)r/2 · n1−α/6 )\nwhich solves the problem with success probability 2/3. We let k = n1−α and apply Theorem 4.7 to obtain an algorithm that uses unary SQs and solves the problem with success probability 2/3. This algorithm uses q0 = q ′ · 2n1−α · nO(r) queries to STAT(1)D ( (2/nα)r/2 ) . Now choosing q = 22n 1−α we get that (\nlog q n )r/2 ≤ (2/nα)r/2. This means that q0 ≥ q/2O(t) = 22n 1−α−O(t).\nHence q′ = 22n 1−α−O(t)−n1−α−O(r) = 2n 1−α−O(t).\n2We can also get essentially the same result by applying the simulation of a k-wise SQ using unary SQs from\nTheorem 1.3.\nSimilar lower bounds can be obtained for other problems considered in [FPV13], namely, planted satisfiability and t-SAT refutation.\nTo obtain a lower bound for learning DNF formulas we can use a simple reduction from the Goldreich’s PRG defined above to learning DNF formulas of polynomial size. It is based on ideas implicit in the reduction from t-SAT refutation to DNF learning from [DS16].\nLemma 4.18. P : {±1}t → {±1} be a fixed predicate. There exists a mapping M from t-tuples of indices in [n] to {0, 1}tn such that for every σ ∈ {±1}n there exists a DNF formula fσ of size 2t satisfying P (σi1 , . . . , σit) = fσ(M(i1, . . . , it)).\nProof. The mapping M maps (i1, . . . , it) to the concatenation of the indicator vectors of each of the indices. Namely, for j ∈ [t] and ℓ ∈ [n], M(i1, . . . , it)j,ℓ = 1 if and only if ij = ℓ, where we use the double index j, ℓ to refer to element n(j − 1) + ℓ of the vector. Let vj,ℓ denote the variable with the index j, ℓ. Let σ be any assignment and we denote by zσj the j-th variable of our predicate P when the assignment is equal to σ. We first observe that zσj ≡ ∧ ℓ∈[n],σℓ=0\nv̄j,ℓ. This is true since, by definition, the value of the j-th variable of our predicate is σij . This value is 1 if and only if ij 6∈ {ℓ ∈ [n] | σℓ = 0}. This is equivalent to vj,ℓ being equal to 0 for all ℓ ∈ [n] such that σℓ = 0. Analogously, z̄ σ j ≡ ∧ ℓ∈[n],σℓ=1\nv̄j,ℓ. This implies that any conjunction of variables zσ1 , z̄ σ 1 , . . . , z σ t , z̄ σ t can be expressed as a conjunction over variables v̄j,ℓ. Any predicate P can be expressed as a disjunction of at most 2t conjunctions and hence there exists a DNF formula fσ of size at most 2 t whose value on M(i1, . . . , it) is equal to P (σi1 , . . . , σit)\nThis reduction implies that by converting a sample ((i1, . . . , it), b) to a sample (M(i1, . . . , it), b) we can transform the Goldreich’s PRG problem into a problem in which our goal is to distinguish examples of some DNF formula fσ from randomly labeled examples. Naturally, an algorithm that can learn DNF formulas can output a hypothesis which predicts the label (with some nontrivial accuracy), whereas such hypothesis cannot exist for predicting random labels. Hence known SQ lower bounds on planted CSPs [FPV13] immediately imply lower bounds for learning DNF. Further, by applying Lemma 4.18 together with Thm. 4.17 for t = r = logn we obtain the first lower bounds for learning DNF against n1−α-wise SQ algorithms.\nTheorem 4.19. For any constant (independent of n) α > 0, there exists a constant β > 0 such that any algorithm that PAC learns DNF formulas of size n with error < 1/2 − n−β log n and success probability at least 2/3 needs at least 2n 1−α\nqueries to STAT (n1−α) D (n −β logn).\nWe remark that this is a lower bound for PAC learning polynomial size DNF formulas with respect to some fixed (albeit non-uniform) distribution over {0, 1}n. The approach for relating k-wise SQ complexity to unary SQ complexity given in [BKW03] applies to this setting. Yet, in their proof the tolerance needed for the unary SQ algorithm is τ/2k and therefore it would not give a non-trivial lower bounds beyond k = O(log n)."
    }, {
      "heading" : "5 Reduction for low-communication queries",
      "text" : "In this section, we prove Theorem 1.5 using a recent result of Steinhardt, Valiant and Wager [SVW16]. Their result can be seen giving a SQ algorithm that simulates a communication protocol between n parties. Each party is holding a sample drawn i.i.d. from distribution D and broadcasts at most b bits about its sample (to all the other parties). The bits can be sent over multiple rounds. This is essentially the standard model of multi-party communication complexity (e.g. [KN97]) but with the goal of solving some problem about the unknown distribution D rather than computing a specific function of the inputs. Alternatively, one can also see this model as a single algorithm that extracts at most b-bits of information about each random sample from D and is allowed to extract the bits in an arbitrary order (generalizing the b-bit sampling model that we discuss in Section 6.2 and in which b-bits are extracted from each\nsample at once). We refer to this model simply as algorithms that extract at most b bits per sample.\nTheorem 5.1 ([SVW16]). Let A be an algorithm that uses n samples drawn i.i.d. from a distribution D and extracts at most b bits per sample. Then, for every β > 0, there is an algorithm B that makes at most 2·b·n queries to STAT(1)D (β/(2b+1·k)) and the output distributions of A and B are within total variation distance β.\nWe will use this simulation to estimate the expectation of k-wise functions that have low communication complexity. Specifically, we recall the following standard model of public-coin randomized k-party communication complexity.\nDefinition 5.2. For a function φ : Xk → {±1} we say that φ has a k-party public-coin randomized communication complexity of at most b bits per party with success probability 1− δ if there exist a protocol satisfying the following conditions. Each of the parties is given xi ∈ X and access to shared random bits. In each round one of the parties can compute one or more bits using its input, random bits and all the previous communication and then broadcast it to all the other parties. In the last round one of the parties computes a bit that is the output of the protocol. Each of the parties communicates at most b bits in total. For every x1, . . . , xk ∈ X, with probability at least 1 − δ over the choice of the random bits the output of the protocol is equal to φ(x1, . . . , xk).\nWe are now ready to prove Theorem 1.5 which we restate here for convenience.\nTheorem 1.5 (restated). Let φ : Xk → {±1} be a function, and assume that φ has k-party public-coin randomized communication complexity of b bits per party with success probability 2/3. Then, there exists a randomized algorithm that, with probability at least 1 − δ, estimates Ex∼Dk [φ(x)] within τ using O(b · k · log(1/δ)/τ2) queries to STAT(1)D (τ ′) for some τ ′ = τO(b)/k. Proof. We first amplify the success probability of the protocol for computing φ to δ′ . = τ/8 using the majority vote of O(log(1/δ′)) repetitions. By Yao’s minimax theorem [Yao77] there exists a deterministic protocol Π′ that succeeds with probability at least 1− δ′ for (x1, . . . , xk) ∼ Dk. Applying Theorem 5.1, we obtain a unary SQ algorithmA whose output is within total variation distance at most β . = τ/8 from Π′(x1, . . . , xk) (and we can assume that the output of A is in {±1}). Therefore:\n|E[A]−Dk[φ]| ≤ |E[A] − EDk [Π′(x1, . . . , xk)]|+ |EDk [Π′(x1, . . . , xk)]−Dk[φ]| ≤ 2τ\n8 +\n2τ\n8 =\nτ 2 .\nRepeating A O(log(1/δ)/τ2) times and taking the mean, we get an estimate of Dk[φ] within τ with probability at least 1− δ. This algorithm uses O(b · k · log(1/δ)/τ2) queries to STAT(1)D (τ ′) for τ ′ = τ8/(2 O(log(8/τ)·b) · k) = τO(b)/k.\nThe collision probability for a distribution D is defined as Pr(x1,x2)∼D2 [x1 = x2]. This corresponds to φ(x1, x2) being the Equality function which, as is well-known, has randomized 2-party communication complexity of O(1) bits per party with success probability 2/3 (see, e.g., [KN97]). Applying Theorem 1.5 with k = 2 we get the following corollary.\nCorollary 5.3. For any τ, δ > 0, there is a SQ algorithm that estimates the collision probability of an unknown distribution D within τ with success probability 1−δ using O(log(1/δ)/τ2) queries to STAT\n(1) D (τ O(1))."
    }, {
      "heading" : "6 Corollaries for other models",
      "text" : "6.1 k-local differential privacy\nWe start by formally defining the k-wise version of the local differentially privacy model from [KLN+11].\nDefinition 6.1 (k-local randomizer). A k-local ǫ-differentially private (DP) randomizer is a randomized map R : Xk → W such that for all u, u′ ∈ Xk and all w ∈ W , we have that Pr[R(u) = w] ≤ eǫ · Pr[R(u′) = w] where the probabilities are taken over the coins of R.\nThe following definition gives a k-wise generalization of the local randomizer (LR) oracle which was used in [KLN+11].\nDefinition 6.2 (k-local Randomizer Oracle). Let z = (z1, . . . , zn) ∈ Xn be a database. A k-LR oracle LRz(·, ·) gets a k-tuple of indices ī ∈ [n]k and a k-local ǫ-DP randomizer as inputs, and outputs an element w ∈ W which is sampled from the distribution R(zi1 , . . . , zik).\nWe are now ready to give the definition of k-local differential privacy.\nDefinition 6.3 (k-local differentially private algorithm). A k-local ǫ-differentially private algorithm is an algorithm that accesses a database z ∈ Xn via a k-LR oracle LRz with the restriction that for all i ∈ [n], if LRz (̄i1, R1), . . . , LRz (̄it, Rt) are the algorithm’s invocations of LRz on ktuples of indices that include index i, where for each j ∈ [t] Rj is a k-local ǫj-DP randomizer, then ǫ1 + · · ·+ ǫt ≤ ǫ.\nThe following two theorems – which follow from Theorem 5.7 and Lemma 5.8 of [KLN+11] – show that k-local differentially private algorithms are equivalent (up to polynomial factors) to k-wise statistical query algorithms.\nTheorem 6.4. Let ASQ be a k-wise SQ algorithm that makes at most t queries to STAT(k)D (τ). Then, for every β > 0, there exists a k-local ǫ-DP algorithm ADP such that if the database z has n ≥ n0 = O(k · t · log(t/β)/(ǫ2 ·τ2)) entries sampled i.i.d. from the distribution D, then ADP makes n0/k queries and the total variation between ADP ’s and ASQ’s output distributions is at most β.\nTheorem 6.5. Let z ∈ Xn be a database with entries drawn i.i.d. from a distribution D. For every k-local ǫ-DP algorithm ADP making t queries to LRz and β > 0, there exists a k-wise statistical query algorithm ASQ that in expectation makes O(t · eǫ) queries to STAT(k)D (τ) for τ = Θ(β/(e2ǫ · t)) such that the total variation between ASQ’s and ADP ’s output distributions is at most β.\nBy combining Theorem 1.1, Theorem 6.4 and Theorem 6.5 we then obtain the following corollary.\nCorollary 6.6. For every positive integer k and any prime number p, there is a concept class C of Boolean functions defined over a domain of size pk+1 for which there exists a (k + 1)-local 1-DP distribution-independent PAC learning algorithm using a database consisting of Õk(log p) i.i.d. samples, whereas any k-local 1-DP distribution-independent PAC learning algorithm requires Ωk(p 1/4) samples.\nThe reduction in Theorem 1.3 then implies that for γ-flat classes of distributions a k-local DP algorithm can be simulated by a 1-local DP algorithm with an overhead that is linear in γk−1 and polynomial in other parameters.\nTheorem 6.7. Let γ ≥ 1, k be any positive integer. Let X be a domain and D a γ-flat class of distributions over X. Let z ∈ Xn be a database with entries drawn i.i.d. from a distribution D ∈ D. For every k-local ǫ-DP algorithm A making t queries to a k-LR oracle LRz and β > 0,\nthere exists a 1-local ǫ-DP algorithm B such that if n ≥ n0 = Õ ( γk−1·t6·k6·e11ǫ\nβ3ǫ2\n) then for every\nD ∈ D, B makes n0/k queries to 1-LR oracle LR′z and the total variation distance between B’s and A’s output distributions is at most β.\nThe reduction from Theorem 1.5 can be translated to this model analogously.\n6.2 k-wise b-bit sampling model\nFor an integer b > 0, a b-bit sampling oracle BSD(b) is defined as follows: Given any function φ : X → {0, 1}b, BSD(b) returns φ(x) for x drawn randomly and independently from D, where D is the unknown input distribution. This oracle was first studied by Ben-David and Dichterman [BD98] as a weak Restricted Focus of Attention model. They showed that algorithms in this model can be simulated efficiently using statistical queries and vice versa. Lower bounds against algorithms that use such an oracle have been studied in [FGR+12, FPV13]. More recently, motivated by communication constraints in distributed systems, the sample complexity of several basic problems in statistical estimation has been studied in this and related models [ZDJW13, SD15, SVW16]. These works also study the natural k-wise generalization of this model. Specifically, BS (k) D (b) is the oracle that given any function φ : X\nk → {0, 1}b, returns φ(x) for x drawn randomly and independently from Dk.\nThe following two theorems – which follow from Theorem 5.2 in [BD98] and Proposition 3 in [SVW16] (that strengthens a similar result in [BD98]) – show that k-wise algorithms in the b-bit sampling model are equivalent (up to polynomial and 2b factors) to k-wise statistical query algorithms.\nTheorem 6.8. Let ASQ be a k-wise SQ algorithm that makes at most t Boolean queries to STAT\n(k) D (τ). Then, for every β > 0, there exists a k-wise 1-bit sampling algorithm A1-bit that\nuses O( tτ2 · log(t/β)) queries to BS (k) D (b) and the total variation distance between ASQ’s and A1-bit’s output distributions is at most β. Theorem 6.9. Let Ab-bit be a k-wise b-bit sampling algorithm that makes at most t queries to BS\n(k) D (b). Then, for every β > 0, there exists a k-wise SQ algorithm ASQ that makes 2bt\nqueries to STAT (k) D (β/(2 b+1t)) and the total variation distance between ASQ’s and Ab-bit’s output distributions is at most β.\nFeldman et al. [FGR+12] give a tighter correspondence between the BS oracle and the slightly stronger VSTAT oracle. Their simulations can be extended to the k-wise case in a similar way.\nThe following corollary now follows by combining Theorem 1.1, Theorem 6.8 and Theorem 6.9.\nCorollary 6.10. Let b = O(1). For every positive integer k and any prime number p, there is a concept class C of Boolean functions defined over a domain of size pk+1 for which there exists a (k+1)-wise b-bit sampling distribution-independent PAC learning algorithm making Õk(log p) queries, whereas any k-wise b-bit sampling distribution-independent PAC learning algorithm requires Ω̃k(p 1/12) queries.\nThe reduction in Theorem 1.3 then implies that for γ-flat classes of distributions a k-wise 1-bit sampling algorithm can be simulated by a 1-wise 1-bit sampling algorithm.\nTheorem 6.11. Let γ ≥ 1, k be any positive integer. Let X be a domain and D a γ-flat class of distributions over X. For every algorithm A making t queries to BS(k)D (1) and every β > 0, there exists a 1-bit sampling algorithm B that for every D ∈ D, uses Õ ( γk−1·t6·k5\nβ3\n) queries to\nBSD(1) and the total variation distance between B’s and A’s output distributions is at most β."
    }, {
      "heading" : "A Omitted proofs",
      "text" : "A.1 Proof of Lemma 3.6\nIn the following, we denote by oc(·) and ωc(·) asymptotic functions obtained by taking the limit as the parameter c goes to infinity. In particular, oc(1) can be made arbitrarily close to 0 by letting c be large enough.\nLet W be as in the statement of Lemma 3.6. To prove the lemma, it suffices to show that each bit j in the binary representation of the subspace Ŵ constructed by Algorithm 2 is equal to the corresponding bit of W . Henceforth, we fix j. We consider the two cases where bit j of W is equal to 1, and where it is equal to 0.\nFirst, we assume that bit j ofW is equal to 1, and prove that in the execution of Algorithm 2, it will be the case that ui,j/vi ≥ 1 − oc(1). We can then set c to be sufficiently large to ensure that ui,j/vi ≥ (9/10). Note that for any positive real numbers N , D and τ such that τ = o(N) and τ = o(D), we have that\nN − τ D + τ ≥ N D · (1− o(1)).\nThus, it is enough to show that the next three statements hold:\n(i) τ = oc(vi),\n(ii) if bit j of W is 1, then (ui,j/vi) ≥ 1− oc(1), (iii) if bit j of W is 1, then τ = oc(ui,j),\nwhere ui,j , E[φi,j ] and vi , E[φi]. To show (i) above, note that\nvi = Pr [ (b1, . . . , bk+1) = 1 k+1 and rk(Z) = i ]\n≥ vi − τ ≥ v · τi − τ ≥ ωc(τ),\nwhere the first inequality follows from the definition of vi and the SQ guarantee, the second inequality follows from the given assumption (in the statement of Lemma 3.6) that (vi/v) ≥ τi, and the last inequality follows from the fact that since v > ǫk+1/2, for every i ∈ [k+1], we have that\nτ = oc ( (v · τi − τ) · (1− τi/4) ) .\n. Recall the definition of the event Ej(Z) from the description of Algorithm 2. To show (ii) above, note that\nui,j vi = Pr\n[ Ej(Z) | (b1, . . . , bk+1) = 1k+1 and rk(Z) = i ]\n≥ Pr [ all rows of Z belong to W | (b1, . . . , bk+1) = 1k+1 and rk(Z) = i ] = 1− Pr [ ∃ a row of Z that /∈ W | (b1, . . . , bk+1) = 1k+1 and rk(Z) = i ]\n≥ 1− (k + 1) · Pr z∼Q [z /∈ W ] ≥ 1− τi 4 ≥ 1− oc(1),\nwhere the first inequality uses the assumption that bit j in the binary representation of W is 1 and the facts that the dimension of W is equal to i and that we are conditioning on rk[Z] = i. The second inequality follows from the union bound, the third inequality follows from the assumption given in Equation (2), and the last inequality follows from the fact that for every i ∈ [k + 1], we have that τi = oc(1).\nTo show (iii) above, note that\nui,j = vi · ui,j vi\n≥ ωc(τ) · (1− oc(1)) ≥ ωc(τ),\nwhere the first inequality follows from (i) and (ii) above. We now turn to the (slightly different) case where bit j of W is equal to 0, and prove that in the execution of Algorithm 2, we will have that ui,j/vi = oc(1). Note that for any positive\nreal numbers N , D and τ such that τ = o(D), we have that\nN + τ D − τ ≤ N D · (1 + o(1)) + o(1).\nThus, it is enough to use the fact that τ = oc(vi) (proven in (i) above) and to show the next statement:\n(iv) if bit j of W is 0, then (ui,j/vi) = oc(1).\nTo prove (iv), note that since bit j of W is 0, we have that\nui,j vi\n≤ Pr [ ∃ a row of Z that /∈ W | (b1, . . . , bk+1) = 1k+1 and rk(Z) = i ]\n≤ τi 4 ≤ oc(1),\nwhere the first inequality above follows from the assumption that bit j in the binary representation of W is 0 and the facts that the dimension of W is equal to i and that we are conditioning on rk[Z] = i. The second inequality above follows from the union bound and the assumption given in Equation (2), and the last inequality follows from the fact that for every i ∈ [k + 1], we have that τi = oc(1). As before, we choose c to be sufficiently large to ensure that this last probability is smaller than (1/10).\nA.2 Proof of Proposition 3.14\nLet a ∈ Fℓp. We have that:\nE(z,b)∼D0 [Da(z, b)] = E(z,b)∼D0\n[ k∏\ni=1\nE(zi,bi)∼D0 [Da(zi, bi)\n]\n=\nk∏\ni=1\nE(zi,bi)∼D0\n[ Da(zi, bi) ]\n=\nk∏\ni=1\nE(zi,bi)∼D0\n[ Da(zi) · 1(bi = fa(zi)) ]\n= k∏\ni=1\nEzi∼D0 [ Da(zi) · Ebi∈R{±1}[1(bi = fa(zi))] ]\n= 1\n2k ·\nk∏\ni=1\nEzi∼D0 [ Da(zi) ]\n= 1 2k · ( 1 p · β + ( 1− 1 p ) · α )k .\nA.3 Proof of Proposition 3.15\nLet a, a′ ∈ Fℓp. First, assume that Hypa = Hypa′ , i.e., that a = a′. Then,\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = E(z,b)∼D0 [Da(z, b)2]\n= E(z,b)∼D0\n[ k∏\ni=1\nDa(zi, bi) 2\n]\n=\nk∏\ni=1\nE(zi,bi)∼D0 [Da(zi, bi) 2]\n=\nk∏\ni=1\nE(zi,bi)∼D0 [Da(zi) 2 · 1(bi = fa(zi))]\n=\nk∏\ni=1\nEzi [ Da(zi) 2 · Ebi [1(bi = fa(zi))] ]\nThus,\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = 1\n2k ·\nk∏\ni=1\nEzi [Da(zi) 2]\n= 1\n2k ·\nk∏\ni=1\n( 1\np · β2 +\n( 1− 1\np\n) · α2 )\n= 1 2k · ( 1 p · β2 + ( 1− 1 p ) · α2 )k .\nNow we assume that Hypa ∩ Hypa′ = ∅. Then,\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = E(z,b)∼D0 [ k∏\ni=1\nDa(zi, bi) ·Da′(zi, bi) ]\n=\nk∏\ni=1\nE(zi,bi)∼D0 [Da(zi, bi) ·Da′(zi, bi)]\n=\nk∏\ni=1\nE(zi,bi)∼D0 [Da(zi) · 1(bi = fa(zi)) ·Da′(zi) · 1(bi = fa′(zi))]\n=\nk∏\ni=1\nEzi [ Da(zi) ·Da′(zi) · 1(fa(zi) = fa′(zi)) · Ebi [1(bi = fa(zi))] ]\n= 1\n2k ·\nk∏\ni=1\nEzi [ Da(zi) ·Da′(zi) · 1(fa(zi) = fa′(zi)) ]\n= 1\n2k ·\nk∏\ni=1\n( α2 · ( 1− 2\np\n))\n= 1 2k · ( α2 · ( 1− 2 p ))k .\nFinally, we assume that Hypa 6= Hypa′ and Hypa ∩ Hypa′ 6= ∅. Then,\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = 1\n2k ·\nk∏\ni=1\nEzi [ Da(zi) ·Da′(zi) · 1(fa(zi) = fa′(zi)) ]\n= 1\n2k ·\nk∏\ni=1\n( β2 p2 + α2 · (1− 2 p + 1 p2 ))\n= 1 2k · (β\n2\np2 + α2 · (1− 2 p + 1 p2 ))k.\nA.4 Proof of Proposition 3.16\nFirst, we assume that a, a′ ∈ Fℓp are such that Hypa = Hypa′ , i.e., a = a′. Then, by Proposition 3.15 and by our settings of α and β, we have that\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = 1 2k · (1 p · β2 + (1− 1 p ) · α2)k\n= 1 22k · p(2ℓ−1)·k · (1 + 1 p− 1) k.\nHence, D0[D̂a · D̂a′ ] = (p+ 1− 1p−1 )k − 1, as desired. Next, we assume that a, a′ ∈ Fℓp are such that Hypa ∩ Hypa′ = ∅. Then, by Proposition 3.15 and by our setting of α, we have that\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = 1 2k · (α2 · (1− 2 p ))k\n= 1 23k · p2kℓ · (1− 2p )k\n(1 − 1p )2k .\nHence, D0[D̂a · D̂a′ ] = 12k · (1− 2 p )k\n(1− 1 p )2k − 1, as desired. Finally, we assume that a, a′ ∈ Fℓp are such that Hypa 6= Hypa′ and Hypa ∩Hypa′ 6= ∅. Then,\nby Proposition 3.15 and by our settings of α and β, we have that\nE(z,b)∼D0 [Da(z, b) ·Da′(z, b)] = 1 2k · (β\n2\np2 + α2 · (1− 2 p + 1 p2 ))k\n= 1\n22k · p2kℓ .\nHence, D0[D̂a · D̂a′ ] = 0, as desired."
    } ],
    "references" : [ {
      "title" : "General bounds on statistical query learning and pac learning with noise via hypothesis boosting",
      "author" : [ "Javed A Aslam", "Scott E Decatur" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Aslam and Decatur.,? \\Q1993\\E",
      "shortCiteRegEx" : "Aslam and Decatur.",
      "year" : 1993
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour" ],
      "venue" : "In COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning with restricted focus of attention",
      "author" : [ "Shai Ben-David", "Eli Dichterman" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Ben.David and Dichterman.,? \\Q1998\\E",
      "shortCiteRegEx" : "Ben.David and Dichterman.",
      "year" : 1998
    }, {
      "title" : "Practical privacy: the sulq framework",
      "author" : [ "Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim" ],
      "venue" : "In Proceedings of the Twenty-fourth ACM SIGACTSIGMOD-SIGART Symposium on Principles of Database Systems, June 13-15,",
      "citeRegEx" : "Blum et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 2005
    }, {
      "title" : "Statistical active learning algorithms for noise tolerance and differential privacy",
      "author" : [ "Maria-Florina Balcan", "Vitaly Feldman" ],
      "venue" : null,
      "citeRegEx" : "Balcan and Feldman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Balcan and Feldman.",
      "year" : 2015
    }, {
      "title" : "Weakly learning DNF and characterizing statistical query learning using Fourier analysis",
      "author" : [ "A. Blum", "M. Furst", "J. Jackson", "M. Kearns", "Y. Mansour", "S. Rudich" ],
      "venue" : "In Proceedings of STOC,",
      "citeRegEx" : "Blum et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 1994
    }, {
      "title" : "A polynomial-time algorithm for learning noisy linear threshold functions",
      "author" : [ "Avrim Blum", "Alan Frieze", "Ravi Kannan", "Santosh Vempala" ],
      "venue" : null,
      "citeRegEx" : "Blum et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning by distances",
      "author" : [ "Shai Ben-David", "Alon Itai", "Eyal Kushilevitz" ],
      "venue" : "In Proceedings of the Third Annual Workshop on Computational Learning Theory, COLT 1990,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 1990
    }, {
      "title" : "Noise-tolerant learning, the parity problem, and the statistical query model",
      "author" : [ "Avrim Blum", "Adam Kalai", "Hal Wasserman" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Blum et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 2003
    }, {
      "title" : "Amplification of weak learning over the uniform distribution",
      "author" : [ "D. Boneh", "R. Lipton" ],
      "venue" : "In Proceedings of the Sixth Annual Workshop on Computational Learning Theory,",
      "citeRegEx" : "Boneh and Lipton.,? \\Q1993\\E",
      "shortCiteRegEx" : "Boneh and Lipton.",
      "year" : 1993
    }, {
      "title" : "Map-reduce for machine learning on multicore",
      "author" : [ "Cheng Chu", "Sang Kyun Kim", "Yi-An Lin", "YuanYuan Yu", "Gary Bradski", "Andrew Y Ng", "Kunle Olukotun" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Chu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning from satisfying assignments",
      "author" : [ "Anindya De", "Ilias Diakonikolas", "Rocco A Servedio" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "De et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "De et al\\.",
      "year" : 2015
    }, {
      "title" : "Generalization in adaptive data analysis and holdout reuse",
      "author" : [ "Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toni Pitassi", "Omer Reingold", "Aaron Roth" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2015
    }, {
      "title" : "Preserving statistical validity in adaptive data analysis",
      "author" : [ "Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Leon Roth" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures",
      "author" : [ "Ilias Diakonikolas", "Daniel M. Kane", "Alistair Stewart" ],
      "venue" : "CoRR, abs/1611.03473,",
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2016
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam D. Smith" ],
      "venue" : "In Theory of Cryptography, Third Theory of Cryptography Conference,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Revealing information while preserving privacy",
      "author" : [ "Irit Dinur", "Kobbi Nissim" ],
      "venue" : "In Proceedings of the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, June 9-12,",
      "citeRegEx" : "Dinur and Nissim.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dinur and Nissim.",
      "year" : 2003
    }, {
      "title" : "Complexity theoretic limitations on learning dnf’s",
      "author" : [ "Amit Daniely", "Shai Shalev-Shwartz" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Daniely and Shalev.Shwartz.,? \\Q2016\\E",
      "shortCiteRegEx" : "Daniely and Shalev.Shwartz.",
      "year" : 2016
    }, {
      "title" : "Approximate resilience, monotonicity, and the complexity of agnostic learning",
      "author" : [ "Dana Dachman-Soled", "Vitaly Feldman", "Li-Yang Tan", "Andrew Wan", "Karl Wimmer" ],
      "venue" : "In Proceedings of SODA,",
      "citeRegEx" : "Dachman.Soled et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dachman.Soled et al\\.",
      "year" : 2015
    }, {
      "title" : "A simple polynomial-time rescaling algorithm for solving linear programs",
      "author" : [ "John Dunagan", "Santosh Vempala" ],
      "venue" : "In Proceedings of the 36th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Dunagan and Vempala.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dunagan and Vempala.",
      "year" : 2004
    }, {
      "title" : "RAPPOR: randomized aggregatable privacy-preserving ordinal response",
      "author" : [ "Úlfar Erlingsson", "Vasyl Pihur", "Aleksandra Korolova" ],
      "venue" : "In ACM SIGSAC Conference on Computer and Communications Security,",
      "citeRegEx" : "Erlingsson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Erlingsson et al\\.",
      "year" : 2014
    }, {
      "title" : "Evolvability from learning algorithms",
      "author" : [ "Vitaly Feldman" ],
      "venue" : "In Proceedings of the fortieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Feldman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Feldman.",
      "year" : 2008
    }, {
      "title" : "Dealing with range anxiety in mean estimation via statistical queries",
      "author" : [ "Vitaly Feldman" ],
      "venue" : "arXiv, abs/1611.06475,",
      "citeRegEx" : "Feldman.,? \\Q2016\\E",
      "shortCiteRegEx" : "Feldman.",
      "year" : 2016
    }, {
      "title" : "A general characterization of the statistical query complexity",
      "author" : [ "Vitaly Feldman" ],
      "venue" : "CoRR, abs/1608.02198,",
      "citeRegEx" : "Feldman.,? \\Q2016\\E",
      "shortCiteRegEx" : "Feldman.",
      "year" : 2016
    }, {
      "title" : "Statistical algorithms and a lower bound for detecting planted cliques",
      "author" : [ "Vitaly Feldman", "Elena Grigorescu", "Lev Reyzin", "Santosh Vempala", "Ying Xiao" ],
      "venue" : "arXiv, CoRR,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2012
    }, {
      "title" : "Statistical query algorithms for mean vector estimation and stochastic convex optimization",
      "author" : [ "Vitaly Feldman", "Cristobal Guzman", "Santosh Vempala" ],
      "venue" : "CoRR, abs/1512.09170,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2015
    }, {
      "title" : "Lower bounds and hardness amplification for learning shallow monotone formulas",
      "author" : [ "V. Feldman", "H. Lee", "R. Servedio" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2011
    }, {
      "title" : "On the complexity of random satisfiability problems with planted solutions",
      "author" : [ "Vitaly Feldman", "Will Perkins", "Santosh Vempala" ],
      "venue" : "CoRR, abs/1311.4821,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2013
    }, {
      "title" : "Candidate one-way functions based on expander graphs",
      "author" : [ "Oded Goldreich" ],
      "venue" : "IACR Cryptology ePrint Archive,",
      "citeRegEx" : "Goldreich.,? \\Q2000\\E",
      "shortCiteRegEx" : "Goldreich.",
      "year" : 2000
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "Michael Kearns" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Kearns.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kearns.",
      "year" : 1998
    }, {
      "title" : "What can we learn privately",
      "author" : [ "Shiva Prasad Kasiviswanathan", "Homin K Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Kasiviswanathan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kasiviswanathan et al\\.",
      "year" : 2011
    }, {
      "title" : "Communication complexity",
      "author" : [ "Eyal Kushilevitz", "Noam Nisan" ],
      "venue" : null,
      "citeRegEx" : "Kushilevitz and Nisan.,? \\Q1997\\E",
      "shortCiteRegEx" : "Kushilevitz and Nisan.",
      "year" : 1997
    }, {
      "title" : "Multiple source adaptation and the rényi divergence",
      "author" : [ "Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Mansour et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mansour et al\\.",
      "year" : 2009
    }, {
      "title" : "Interactive privacy via the median mechanism",
      "author" : [ "Aaron Roth", "Tim Roughgarden" ],
      "venue" : "In Proceedings of the forty-second ACM symposium on Theory of computing,",
      "citeRegEx" : "Roth and Roughgarden.,? \\Q2010\\E",
      "shortCiteRegEx" : "Roth and Roughgarden.",
      "year" : 2010
    }, {
      "title" : "Airavat: Security and privacy for mapreduce",
      "author" : [ "Indrajit Roy", "Srinath TV Setty", "Ann Kilzer", "Vitaly Shmatikov", "Emmett Witchel" ],
      "venue" : "In NSDI,",
      "citeRegEx" : "Roy et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2010
    }, {
      "title" : "Minimax rates for memory-bounded sparse linear regression",
      "author" : [ "Jacob Steinhardt", "John C. Duchi" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Steinhardt and Duchi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Steinhardt and Duchi.",
      "year" : 2015
    }, {
      "title" : "Optiml: an implicitly parallel domain-specific language for machine learning",
      "author" : [ "Arvind Sujeeth", "HyoukJoong Lee", "Kevin Brown", "Tiark Rompf", "Hassan Chafi", "Michael Wu", "Anand Atreya", "Martin Odersky", "Kunle Olukotun" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Sujeeth et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sujeeth et al\\.",
      "year" : 2011
    }, {
      "title" : "Memory, communication, and statistical queries",
      "author" : [ "J. Steinhardt", "G. Valiant", "S. Wager" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Steinhardt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Steinhardt et al\\.",
      "year" : 2016
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "Leslie G Valiant" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Valiant.,? \\Q1984\\E",
      "shortCiteRegEx" : "Valiant.",
      "year" : 1984
    }, {
      "title" : "All of statistics: a concise course in statistical inference",
      "author" : [ "Larry Wasserman" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Wasserman.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wasserman.",
      "year" : 2013
    }, {
      "title" : "Probabilistic computations: Toward a unified measure of complexity",
      "author" : [ "Andrew Yao" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Yao.,? \\Q1977\\E",
      "shortCiteRegEx" : "Yao.",
      "year" : 1977
    }, {
      "title" : "Information-theoretic lower bounds for distributed statistical estimation with communication constraints",
      "author" : [ "Yuchen Zhang", "John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Several well-studied models of access to data samples, including statistical queries, local differential privacy and low-communication algorithms rely on queries that provide information about a function of a single sample. (For example, a statistical query (SQ) gives an estimate of Ex∼D[q(x)] for any choice of the query function q : X → R, where D is an unknown data distribution.) Yet some data analysis algorithms rely on properties of functions that depend on multiple samples. Such algorithms would be naturally implemented using k-wise queries each of which is specified by a function q : X → R. Hence it is natural to ask whether algorithms using k-wise queries can solve learning problems more efficiently and by how much. Blum, Kalai, Wasserman [BKW03] showed that for any weak PAC learning problem over a fixed distribution, the complexity of learning with k-wise SQs is smaller than the (unary) SQ complexity by a factor of at most 2. We show that for more general problems over distributions the picture is substantially richer. For every k, the complexity of distribution-independent PAC learning with k-wise queries can be exponentially larger than learning with (k + 1)-wise queries. We then give two approaches for simulating a k-wise query using unary queries. The first approach exploits the structure of the problem that needs to be solved. It generalizes and strengthens (exponentially) the results of Blum et al. [BKW03]. It allows us to derive strong lower bounds for learning DNF formulas and stochastic constraint satisfaction problems that hold against algorithms using k-wise queries. The second approach exploits the k-party communication complexity of the k-wise query function. ∗Work done while at IBM Research Almaden.",
    "creator" : "LaTeX with hyperref package"
  }
}