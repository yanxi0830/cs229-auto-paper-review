{
  "name" : "1405.3167.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "bneyshabur@ttic.edu", "yury@ttic.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n31 67\nv1 [\ncs .L\nG ]\n1 3\nM ay\n2 01\n4\nKeywords: Clustering, Hamming Embedding, LSH, Max Norm"
    }, {
      "heading" : "1 Introduction",
      "text" : "Convex relaxations play an important role in designing efficient learning and recovery algorithms, as well as in statistical learning and online optimization. It is thus desirable to understand the convex hull of hypothesis sets, to obtain tractable relaxation to these convex hulls, and to understand the tightness of such relaxations.\nIn this paper we consider convex relaxations of two important problems, namely clustering and hamming embedding, and study the convex hulls of the corresponding hypothesis classes: of cluster incidence matrices and of similarity measures with a short hamming embedding. In section 2 we introduce these classes formally, and understand the relationship between them, showing how hamming embedding can be seen as a generalization of clustering. In section 3 we discuss their convex hull and its relationship to notion of Locality Sensitive Hashing (LSH) as studied by [6]. There has been several studies on different aspects of LSH (e.g.[16,10,7]).\nMore specifically, we focus on the asymmetric versions of these classes, which correspond to co-clustering (e.g. [11,3]) and asymmetric hamming embedding as recently introduced by [15]. We define the corresponding notion of an Asymmetric LSH, and show how it could be much more powerful then standard (symmetric) LSH (section 4).\nOur main conclusion is that the convex hull of asymmetric clustering and hamming embedding is tightly captured by a shift-invariant modification of the max-norm—a tractable SDP-representable relaxation (Theorem 2 in section 5). We contrast this with the symmetric case, in which the corresponding SDP relaxation is not tight, highlighting an important distinction between symmetric and asymmetric clustering, embedding and LSH."
    }, {
      "heading" : "2 Clustering and Hamming Embedding",
      "text" : "In this section we introduce the problems of clustering and hamming embedding, providing a unified view of both problems, with hamming embedding being viewed as a direct generalization of clustering. Our starting point, in any case, is a given similarity function sim : S × S → [−1,+1] over a (possibly infinite) set of objects S. “Clustering”, as we think of it here, is the problem of partitioning the elements of S into disjoint clusters so that items in the same cluster are similar while items in different clusters are not similar. “Hamming Embedding” is the problem of embedding S into some hamming space such that the similarity between objects is captured by the hamming distance between their mappings."
    }, {
      "heading" : "2.1 Clustering",
      "text" : "We represent a clustering of S as a mapping h : S → Γ , where Γ is a discrete alphabet. We can think of h as a function that assigns a cluster identity to each element, where the meaning of the different identities is arbitrary. The alphabet Γ might have a fixed finite cardinality |Γ | = k, if we would like to have a clustering with a specific number of clusters. E.g., a binary alphabet corresponds to standard graph partitioning into two clusters. If |Γ | = k, we can assume that Γ = [k]. The alphabet Γ might be infinitely countable (e.g. Γ = N), in which case we are not constraining the number of clusters.\nThe cluster incidence function κh : S×S → {±1} associated with a clustering h is defined as κh(x, y) = 1 if h(x) = h(y) and κh(x, y) = −1 otherwise. For a finite space S of cardinality n = |S| we can think of κh ∈ {±1}n×n as a permuted block-diagonal matrix. We denote the set of all valid cluster incidence functions over S with an alphabet of size k (i.e. with at most k clusters) as MS,k = {κh | h : S → [k]}, where k = ∞ is allowed.\nWith this notion in hand, we can think of clustering as a problem of finding a cluster incidence function κh that approximates a given similarity sim, as quantified by objectives minEx,y[|κh(x, y)− sim(x, y)|] or maxEx,y[sim(x, y)κh(x, y)] (this is essentially the correlation clustering objective). Since objectives themselves are convex in κ, but the constraint that κ is a valid cluster incidence function is not a convex constraint, a possible approach is to relax the constraint that κ is a valid cluster incidence function, or in the finite case, a cluster incidence matrix. This is the approach taken by, e.g. [12,13], who relax the constraint to a trace-norm and max-norm constraint respectively. One of the questions we will be exploring here is whether this is the tightest relaxation possible, or whether there is a significantly tighter relaxation."
    }, {
      "heading" : "2.2 Hamming Embedding and Binary Matrix Factorization",
      "text" : "In the problem of binary hamming embedding (also known as binary hashing), we want to find a mapping from each object x ∈ S to binary string b(x) ∈ {±1}d\nsuch that similarity between strings is approximated by the hamming distance between their images:\nsim(x, y) ≈ 1− 2δHam(b(x), b(y)) d\n(1)\nCalculating the hamming distance of two binary hashes is an extremely fast operation, and so such a hash is useful for very fast computation of similarities between massive collections of objects. Furthermore, hash tables can be used to further speed up retrieval of similar objects.\nBinary hamming embedding can be seen as a generalization of clustering as follows: For each position i = 1, . . . , d in the hash, we can think of bi(x) as a clustering into two clusters (i.e. with Γ = {±1}). The hamming distance is then an average of the d cluster incidence functions:\n1− 2δHam(b(x), b(y)) d = 1 d\nd ∑\ni=1\nκbi(x, y).\nOur goal then is to approximate a similarity function by an average of d binary clusterings. For d = 1 this is exactly a binary clustering. For d > 1, we are averaging multiple binary clusterings.\nSince we have 〈b(x), b(y)〉 = d − 2δHam(b(x), b(y)), we can formulate the binary hashing problem as a binary matrix factorization where the goal is to approximate the similarity matrix by a matrix of the form RR⊤, where R is a d-dimensional binary matrix:\nmin R\n∑\nij\nerr(sim(i, j), X(i, j))\ns.t X = RR⊤ R ∈ {±1}n×d (2)\nwhere err(x, y) is some error function such as err(x, y) = |x− y|. Going beyond binary clustering and binary embedding, we can consider hamming embeddings over larger alphabets. That is, we can consider mappings b : S → Γ d, where we aim to approximate the similarity as in (1), recalling that the hamming distance always counts the number of positions in which the strings disagree. Again, we have that the length d hamming embeddings over a (finite or infinitely countable) alphabet Γ correspond to averages of d cluster incidence matrices over the same alphabet Γ ."
    }, {
      "heading" : "3 Locality Sensitive Hashing Schemes",
      "text" : "Moving on from a finite average of clusterings, with a fixed number of components, as in hamming embedding, to an infinite average, we arrive at the notion of LSH as studied by [6].\nGiven a collection S of objects, an alphabet Γ and a similarity function sim : S × S → [−1, 1] such that for any x ∈ S we have sim(x, x) = 1,a locality sensitive hashing scheme (LSH) is a probability distribution on the family of clustering functions (hash functions) H = {h : S → Γ} such that [6]:\nEh∈H[κh(x, y)] = sim(x, y). (3)\n[6] discuss similarity functions sim : S × S → [0, 1] as so require\nPh∈H[h(x) = h(y)] = sim(x, y).\nThe definition (3) is equivalent, except it applies to the transformed similarity function 2sim(x, y)− 1.\nThe set of all locality sensitive hashing schemes with an alphabet of size k is nothing but the convex hull of the set MS,k of cluster incidence matrices.\nThe importance of an LSH, as an object in its own right as studied by [6], is that a hamming embedding can be obtained from an LSH by randomly generating a finite number of hash functions from the distribution over the family H. In particular, if we draw h1, . . . , hd i.i.d. from an LSH, then the length-d hamming embedding b(x) = [h1(x), . . . , hd(x)] has expected square error\nE[(sim(x, y)− 1 d ∑ κhd(x, y)) 2] ≤ 1 d , (4)\nwhere the expectation is w.r.t. the sampling, and this holds for all x, y, and so also for any average over them."
    }, {
      "heading" : "3.1 α-LSH",
      "text" : "If the goal is to obtain an low-error embedding, the requirement (3) might be too harsh. If we are willing to tolerate a fixed offset between our embedding and the target similarity, we can instead require that\nαEh∈H[κh(x, y)]− θ = sim(x, y). (5)\nwhere α, θ ∈ R, α > 0. A distribution over h that obeys (5) is called an α-LSH. We can now verify that, for h1, . . . , hd drawn i.i.d. from an α-LSH, and any x, y ∈ S:\nE\n[\n( sim(x, y)− (α d ∑ κhd(x, y)− θ) )2\n]\n≤ α 2\nd . (6)\nThe length of the LSH required to acheive accurate approximation of a similarity function thus scales quadartically with α, and it is therefor desireable to obtain an α-LSH with as low an α as possible (note that sim(x, x) = 1, implies θ = α−1, and so we must allow a shift if we want to allow α 6= 1).\nUnfortunately, even the requirement (5) of an α-LSH is quite limiting and difficult to obey, as captured by the following theorem, which is based on lemmas 2 and 3 of [6]:\nClaim 1. For any finite or countable alphabet Γ , k = |Γ | ≥ 2, a similarity function sim has an α-LSH over Γ for some α if and only if D(x, y) = 1−sim(x,y)2 is embeddable to hamming space with no distortion.\nProof. Given metric spaces (X, d) and (X, d′) any map f : X → X ′ is called a metric embedding. The distortion of such an embedding is defined as:\nβ = max x,y∈X\nd(x, y)\nd′(f(x), f(y)) . max x,y∈X\nd′(f(x), f(y))\nd(x, y)\nWe first show that if there exist an α-LSH for function sim(x, y) then 1−sim(x,y)2 is embeddable to hamming space with no distortion. An α-LSH for function sim(x, y) corresponds to an LSH for function 1 − 1−sim(x,y)α . Using lemma 3 in [6], we can say that 1−sim(x,y)α can be isometrically embedded in the Hamming cube which means 1 − sim(x, y) can be embedded in Hamming cube with no distortion.\nEh∼DH [κh(x, y)] = 2Ph∼DH [h(x) = h(y)]− 1 = 1− 2Ph∼DH [h(x) 6= h(y)] = 1− 2dH(x, y)\n= 1− 2d(x, y) β\n= 2 β ( β 2 − d(x, y))\n= 2 β (1− d(x, y) + β 2 − 1)\n= sim(x, y) + θ\nθ + 1\nAs a result of Claim 1, it can be shown that given any large enough set of low dimensional unit vectors, there is no α-LSH for the Euclidian inner product.\nClaim 2. Let {x(1), . . . , x(n)} be an arbitrary set of unit vectors in the unit sphere. Let Zij = 〈x(i), x(j)〉 for 1 ≤ i, j ≤ n. If d < log2 n, then there is no α-LSH for Z.\nProof. According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle. Equivalently, there exist three vectors x, y and z in any set of n different d-dimensional unit vectors such that:\n〈z − x, z − y〉 < 0\nWe rewrite the above inequality as:\n(1− 〈z, x〉) + (1− 〈z, y〉) < (1− 〈x, y〉)\nThe above inequality implies that the distance measure ∆ij = (1−Zij)/2 is not a metric. Consequently, according to Claim 1 since ∆ij = (1 − Zij)/2 is not a metric, there is no α-LSH for the matrix Z.\nAs noted by [6] (and stated in claim 2), we can therefore unfortunately conclude that there is no α-LSH for several important similarity measures such as the Euclidian inner product, Overlap coefficient and Dice’s coefficient. Note that based on Claim 1, even a finite positive semidefinite similarity matrix is not necessarily α-LSHable."
    }, {
      "heading" : "3.2 Generalized α-LSH",
      "text" : "In the following section, we will see how to break the barrier imposed by Claim 1 by allowing asymmetry, highlighting the extra power asymmetry affords us. But before doing so, let us consider a different attempt at relaxing the definition of an α-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift θ from the scaling α, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.e. on the diagonal of sim).\nWe say that a probability distribution over H = {h : S → Γ} is a Generalized α-LSH, for α > 0 if there exist θ, γ ∈ R such that for all x, y:\nαEh∈H[κh(x, y))] = sim(x, y) + θ + γ1x=y\nWith this definition, then any symmetric similarity function, at least over a finite domain, admits a Generalized α-LSH, with a sufficiently large α:\nClaim 3. For a finite set S, |S| = n, for any symmetric sim : S × S → [−1, 1] with sim(x, x) = 1, there exists a Generalized α-LSH over a binary alphabet Γ (|Γ | = 2) where α = O((1−λmin) logn)-LSH, and λmin is the smallest eigenvalue of the matrix sim.\nProof. We observe that sim−λminI is a positive semidefinite matrix. According to [5], if a matrix Z with unit diagonal is positive semidefinite, then there is a probability distribution over a family H of hash functions such that for any x 6= y:\nEh∈H[h(x)h(y)] = Z(x, y)\nC logn .\nWe let Z(x, y) = (sim(x, y) − λmin1x=y)/(1 − λmin). Matrix Z is positive semidefinite and has unit diagonal. Hence, there is a probability distribution over a family H of hash functions such that\nEh∈H[h(x)h(y)] = sim(x, y)− λmin1x=y C(1− λmin) logn ,\nequivalently\n(C(1 − λmin) log n) · Eh∈H[κh(x, y))] = sim(x, y)− λmin1x=y.\nIt is important to note that λmin could be negative, and as low as λmin = −Ω(n). The required α might therefor be as large as Ω(n), yielding a terrible LSH."
    }, {
      "heading" : "4 Asymmetry",
      "text" : "In order to allow for greater power, we now turn to Asymmetric variants of clustering, hamming embedding, and LSH.\nGiven two collections of objects S, T , which might or might not be identical, and an alphabet Γ , an asymmetric clustering (or co-clustering [11]) is specified by pair of mappings f : S → Γ and g : T → Γ and is captured by the asymmetric cluster incidence matrix κf,g(x, y) where κf,g(x, y) = 1 if f(x) = g(y) and κf,g(x, y) = −1 otherwise. We denote the set of all valid asymmetric cluster incidence functions over S, T with an alphabet of size k as M(S,T ),k = {κf,g | f : S → [k], g : T → [k]}, where we again also allow k = ∞ to correspond to a countable alphabet Γ = N.\nLikewise, an asymmetric binary embedding of S, T with alphabet Γ consists of a pair of functions f : S → Γ d, g : T → Γ d, where we approximate a similarity as:\nsim(x, y) ≈ 1− 2δHam(f(x), g(y)) d = 1 d\nd ∑\ni=1\nκfi,gi(x, y). (7)\nThat is, in asymmetric hamming embedding, we approximate a similarity as an average of d asymmetric cluster incidence matrices from M(S,T ),k.\nIn a recent work, [15] showed that even when S = T and the similarity function sim is a well-behaved symmetric similarity function, asymmetric binary embedding could be much more powerful in approximating the similarity, using shorter lengths d, both theoretically and empirically on data sets of interest. That is, these concepts are relevant and useful not only in an a-priory asymmetric case where S 6= T or sim is not symmetric, but also when the target similarity is symmetric, but we allow an asymmetric embedding. We will soon see such gaps also when considering the convex hulls of MS,k and M(S,T ),k, i.e. when considering LSHs. Let us first formally define an asymmetric α-LSH.\nGiven two collections of objects S and T , an alphabet Γ , a similarity function sim : S×T → [−1, 1], and α > 0, we say that an α-ALSH is a distribution over pairs of functions f : S → Γ , g : T → Γ , or equivalently over M(S,T ),|Γ |, such that for some θ ∈ R and all x ∈ S, y ∈ T :\nαE(f,g)∈F×G [κf,g(x, y))]− θ = sim(x, y). (8)\nTo understand the power of asymmetric LSH, recall that many symmetric similarity functions do not have an α-LSH for any α. On the other hand, any similarity function over finite domains necessarily has an α-ALSH:\nClaim 4. For any similarity function sim : S × T → [−1, 1] over finite S, T , there exists an α-ALSH with α ≤ min{|S|, |T |}\nThis is corollary of Theorem 2 that will be proved later in section 5. The proof follows from Theorem 2 the following upper bound on the max-norm:\n‖Z‖max ≤ rank(Z).‖Z‖2∞\nwhere ‖Z‖2∞ = maxx,y |Z(x, y)|. In section 3, we saw that similarity functions that do not admit an α-LSH, still admit Generalized α-LSH. However, the gap between the α required for a Generalized α-LSH and that required for an α-ALSH might be as large as Ω(|S|):\nTheorem 1. For any even n, there exists a set S of n objects and a similarity Z : S × S → R such that\n– there is a binary 3KR-ALSH for Z, where KR ≈ 1.79 is Krivine’s constant; – there is no Generalized α-LSH for any α < n− 1.\nProof. Let S = [n] and Z be the following similarity matrix:\nZ = 2In×n +\n[\n−1n 2 ×n 2 1n 2 ×n 2\n1n 2 ×n 2 −1n 2 ×n 2\n]\nNow we use Theorem 2, which we will prove later (our proof of Theorem 2 does not rely on the proof of this theorem). Using triangle inequality property of the norm, we have ‖Z‖max ≤ ‖Z− 2In×n‖max+ ‖2In×n‖max = 3; and by Theorem 2 there is a 3KR-ALSH for Z. Looking at the decomposition of Z, it is not difficult to see that the smallest eigenvalue of Z is 2− n. So in order to have a positive semidefinite similarity matrix, we need γ to be at least n−2 and θ to be at least −1 (otherwise the sum of elements of Z + θ+(n− 2)I will be less than zero and so Z + θ + (n − 2)I will not be positive semidefinite). So α = θ + γ is at least n− 1."
    }, {
      "heading" : "5 Convex Relaxations, α-LSH and Max-norm",
      "text" : "We now turn to two questions which are really the same: can we get a tight convex relaxation of the set M(S,T ),k of (asymmetric) clustering incidence functions, and can we characterize the values of α for which we can get an α-ALSH for a particular similarity measure.\nFor notational simplicity, we will now fix S and T and use Mk to denote M(S,T ),k."
    }, {
      "heading" : "5.1 The Ratio Function",
      "text" : "The tightest possible convex relaxation of Mk is simply its convex hull convMk. Assuming P 6= NP, convMk is not polynomially tractable. What we ask here is\nwhether he have a tractable tight relaxation of convMk. To measure tightness of some convex B ⊇ Mk, for each Z ∈ B, we will bound its cluster ratio:\nρk(Z) = min{r|Z ∈ r convMk} = min{r|Z/r ∈ convMk}.\nThat is, by how much to we have to inflate Mk so that includes Z ∈ B. The supremum ρk(B) = supZ∈B ρk(Z) is then the maximal inflation ratio between convMk and B, i.e. such that convMk ⊆ B ⊆ ρk convMk. Similarly, we define the centralized cluster ratio as:\nρ̂k(Z) = min θ∈R\nmin{r|Z − θ ∈ r convMk}.\nThis is nothing but the lowest α for which we have an α-ALSH:\nClaim 5. For any similarity function sim(x, y), ρ̂k(sim) is equal to the smallest α s.t. there exists an α-ALSH for sim over alphabet of cardinality k.\nProof. We write the problem of minimizing α in α-ALSH as:\nmin θ∈R,α∈R+\nα\ns.t sim(x, y) = αE(f,g)∈F×G [κf,g(x, y)]− θ (9)\nWe know that:\nE(f,g)∈F×G [κf,g(x, y)] = ∑\nf∈MS,k\n∑\ng∈MT,k\nκf,g(x, y)p(f, g)\nwhere p(f, g) is the joint probability of hash functions f and g. Define µ(f, g) = αp(f, g) and write:\nα = α ∑\nf∈MS,k\n∑\ng∈MT,k\np(f, g) = ∑\nf∈MS,k\n∑\ng∈MT,k\nαp(f, g) = ∑\nf∈MS,k\n∑\ng∈MT,k\nµ(f, g)\nWe have:\nα ∑\nf∈MS,k\n∑\ng∈MT,k κf,g(x, y)p(f, g)− θ =\n∑\nf∈MS,k\n∑\ng∈MT,k κf,g(x, y)µ(f, g)− θ\nSubstituting the last two equalities into formulation 9 gives us the formulation for centralized cluster ratio.\nOur main goal in this section is to obtain tight bounds on ρk(Z) and ρ̂k(Z).\nThe Ratio Function and Cluster Norm The convex hull convMk is related to the cut-norm, and its generalization the cluster-norm, and although the two are not identical, its worth understanding the relationship.\nFor k = 2, the ratio function is a norm, and is in fact the dual of a modified cut-norm:\nρ∗2(W ) = ‖W‖C,2 = max u:S→{±1},v:S→{±1}\n∑\nx∈S,y∈T W (x, y)u(x)v(y) (10)\nThe norm ‖W‖C,2 is a variant of the cut-norm, and is always within a factor of four from the cut-norm as defined in, e.g. [1]. The set convM2 in this case is the unit ball of the modified cut-norm.\nFor k > 2, the ratio function is not a norm, since Mk, for k > 2, is not symmetric about the origin: we might have Z ∈ Mk but −Z 6∈ Mk and so ρk(Z) 6= ρk(−Z). A ratio function defined with respect the symmetric convex hull of conv(Mk ∪ −Mk), is a norm, and is dual to the following cluster norm, which is a generalization of the modified cut-norm:\n‖W‖C,k = max u:S→Γ,v:S→Γ\n∑\nx∈S,y∈T W (x, y)κu,v(x, y) (11)"
    }, {
      "heading" : "5.2 A Tight Convex Relaxation using the Max-Norm",
      "text" : "Recall that the max-norm (also known as the γ2 : ℓ1 → ℓ∞ norm) of a matrix is defined as [18]:\n‖Z‖max = min UV ⊤ max(‖U‖22,∞, ‖V ‖22,∞)\nwhere ‖U‖2,∞ is the maximum ℓ2 norm of rows of the matrix U . The maxnorm is SDP representable and thus tractable [17]. Even when S and T are not finite, and thus sim is not a finite matrix, the max-norm can be defined as above, where now U and V can be thought of as mappings from S and T respectively into a Hilbert space, with sim(x, y) = (UV ⊤)(x, y) = 〈U(x), V (y)〉 and ‖U‖2,∞ = supx ‖U(x)‖.\nWe also define the centralized max-norm, which, even though it is not a norm, we denote as:\n‖Z‖m̂ax = min θ ‖Z − θ‖max\nThe centralized max-norm is also SDP-representable. Our main result is that the max-norm provides a tight bound on the ratio function:\nTheorem 2. For any similarity function sim : S × T → R we have that: 1\n2 ‖sim‖m̂ax ≤\n1 2 ρ̂2(sim) ≤ ρ̂(sim) ≤ ρ̂k(sim) ≤ ρ̂2(sim) ≤ K‖sim‖m̂ax\nand also\n1 3 ‖sim‖max ≤ ρ(sim) ≤ ρk(sim) ≤ ρ2(sim) ≤ K‖sim‖max\nwhere all inequalities are tight and we have 1.67 ≤ KG ≤ K ≤ KR ≤ 1.79 (KG is Grothendieck’s constant and KR is Krivine’s constant).\nConsidering the dual view of ρ(sim), the theorem can also be viewed in two ways: First, we see that the centralized max-norm provides a tight characterization (up to a small constant factor) of the smallest α for which we can obtain an α-ALSH. In particular, since for domains (i.e. finite matrices) the max-norm is always finite, this establishes that we always have an α-ALSH, as claimed in Claim 4. We also used it in Theorem 1 to establish the existence of an α-ALSH for a specific, small, α.\nSecond, bounding the ratio function establishes that the max-norm ball is a tight tractable relaxation of convMk:\n{Z ‖ ‖Z‖max ≤ 1/K} ⊆ convMk ⊆ {Z ‖ ‖Z‖max ≤ 3} (12)\nThird, we see the effect of the alphabet size k (number of clusters) on the convex hull is very limited.\nThe Symmetric Case It is not difficult to show that the lower bounds for αLSH are the same as for α-ALSH and the inequalities are tight. However, there are no upper bounds for α-LSH similar to those for α-ALSH. Specifically, let α̂ and α̂g be the smallest values of α such that there is an α-LSH for sim and there is a generalized α-LSH for sim, respectively. Note that for some similarity functions sim there is no α-LSH at all; that is, α̂ = ∞ and ‖sim‖max < ∞. Also, as Theorem 1 shows, there is a similarity function sim such that\n‖sim‖max = O(1) but α̂g ≥ n− 1.\nMoreover, it follows from the result of [2] that there is no efficiently computable upper bound β for α̂g such that\nβ\nlogc n ≤ α̂g ≤ β\n(under a standard complexity assumption that NP 6⊆ DTIME(nlog3 n)). That is, neither the max-norm nor any other efficiently computable norm of sim gives a constant factor approximation for α̂g.\nIn the remainder of this section we prove a series of lemmas corresponding to the inequalities in Theorem 2."
    }, {
      "heading" : "5.3 Proofs",
      "text" : "Lemma 1. For any two sets S and T of objects and any function sim : S×T → R, we have that ρ̂2(sim) ≤ 2ρ̂(sim) and the inequality is tight.\nProof. Using Claim 5, all we need to do is to prove that given the function sim, if there exist an α-ALSH with arbitrary cardinality, then we can find a binary 2α−ALSH . In order to do so, we assume that there exists an α-ALSH for family F and G of hash functions such that:\nαE(f,g)∈F×G [κf,g(x, y)] = sim(x, y) + θ\nwhere f : S → Γ and g : T → Γ are hash functions. Now let H be a family of pairwise independent hash functions of the form Γ → {±1} such that each element γ ∈ Γ , has the equal chance of being mapped into -1 or 1. Now, we have that:\n2αEh∈H,(f,g)∈F×G[κhof,hog(x, y)] = 2αEh∈H,(f,g)∈F×G[κhof,hog(x, y)]\n= 2αEh∈H,(f,g)∈F×G[h(f(x))h(g(y))] = 2α(2Ph∈H,(f,g)∈F×G[h(f(x)) = h(g(y))]− 1) = 2αP(f,g)∈F×G [f(x) = g(y)]\n= sim(x, y) + θ + α\n= sim(x, y) + θ̃\nThe tightness can be demonstrated by the example sim(x, y) = 2x=y − 1 when S is not finite.\nLemma 2. For any two sets S and T of objects and any function sim : S×T → R, we have that ‖sim‖max ≤ ρ2(sim) and the inequality is tight. Proof. Without loss of generality, we assume that Γ = {±1}. We want to solve the following optimization problem:\nρ2(sim) = min µ:MS,2×MT,2→R+\n∑\nf∈MS,2\n∑\ng∈MT,2 µ(f, g)\ns.t. sim(x, y) = ∑\nf∈MS,2\n∑\ng∈MT,2 κf,g(x, y)µ(f, g)\nFor any x ∈ S and y ∈ T , we define two new function variables ℓx : MS,2 × MT,2 → R and ry : MS,2 ×MT,2 → R:\nℓx(f, g) = √ µ(f, g)f(x) ry(f, g) = √ µ(f, g)g(y)\nSince cluster incidence matrix can be written as κf,g(x, y) = f(x)g(y), we have sim(x, y) = 〈ℓx, ry〉 and ‖ℓx‖22 = ∑ f∈MS,2 ∑\ng∈MT,2 µ(f, g). Therefore, we rewrite the optimization problem as:\nρ2(sim) = min t,ℓ,r,µ:MS,2×MT,2→R+ t\ns.t. 〈lx, ry〉 = sim(x, y) ‖ℓx‖22 ≤ t ‖ry‖22 ≤ t ℓx(f, g) = √ µ(f, g)f(x)\nry(f, g) = √ µ(f, g)g(y)\nFinally, we relax the above problem by removing the last two constraints:\n‖sim‖max = min t,ℓ,r t\ns.t. 〈lx, ry〉 = sim(x, y) ‖ℓx‖22 ≤ t (13) ‖rx‖22 ≤ t\nThe above problem is a max-norm problem and the solution is ‖sim‖max. Therefore, ‖sim‖max ≤ ρ2(sim). Taking the function sim(x, y) to be a binary cluster incidence function will indicate the tightness of the inequality.\nLemma 3. (Krivine’s lemma [14]) For any two sets of unit vectors {ui} and {vj} in a Hilbert space H, there are two sets of unit vectors {u′i} and {v′j} in a Hilbert space H ′ such that for any ui and vj, sin(c〈ui, vj〉) = 〈u′i, v′j〉 where c = sinh−1(1).\nLemma 4. For any two sets S and T of objects and any function sim : S×T → R, we have that ρ2(sim) ≤ K‖sim‖max where 1.67 ≤ KG ≤ K ≤ KR ≤ 1.79 (KG is Grothendieck’s constant and KR is Krivine’s constant).\nProof. A part of the proof is similar to [1]. Let ℓx and ry be the solution to the max-norm formulation 13. If we use Lemma 3 on the normalized ℓx/‖ℓx‖2 and ry/‖ry‖2 in Hilbert space H and we call the new vectors ℓ′x and r′y in Hilbert space H ′, we have that:\nsin\n(\nc.Z(x, y)\n‖ℓx‖2‖rx‖2\n)\n= 〈ℓ′x, r′y〉\nIf z is a random vector chosen uniformly from H ′, by Lemma 3, we have:\nE([sign(〈ℓ′x, z〉)].[sign(〈r′y , z〉)]) = 2 π arcsin(〈ℓ′x, r′y〉)) =\n2c\nπ‖ℓx‖2‖ry‖2 sim(x, y)\nNow if we set the hashing function f(x) = s(x).[sign(〈ℓ′x, z〉)] where s(x) = 1 with probability 12 + ‖ℓx‖2 2 √ t and s(x) = −1 with probability 12 − ‖ℓx‖2 2 √ t we have that:\nE[f(x).sign(〈r′y , z〉)] = ( 1\n2 + ‖ℓx‖2 2 √ t\n)\n2c\nπ‖ℓx‖2‖ry‖2 sim(x, y)\n− ( 1 2 − ‖ℓx‖2 2 √ t )\n2c\nπ‖ℓx‖2‖ry‖2 sim(x, y)\n= 2c\nπ √ t‖ry‖2 sim(x, y)\nIf we do the same procedure on g(y) = s′(x).[sign(〈r′y , z〉)], we will have:\nE[f(x).g(y)] = 2c\ntπ sim(x, y)\nBy setting µ(f, g) = π‖sim‖max2c p(f, g) where p(f, g) is the probability distribution over the defined f and g, we can see that such µ(f, g) is a feasible solution for the formulation of cluster ratio and we have:\nρ2(sim) ≤ ∑\nf∈MS,2\n∑\ng∈MT,2 µ(f, g) =\nπ 2c ‖sim‖max = KR‖sim‖max\nThe inequality KG ≤ K is known due to [1]."
    }, {
      "heading" : "A Random Matrices",
      "text" : "In this section we investigate the locality sensitive hashing schemes on random p.s.d matrices. We generate a random n×n positive semidefinite matrix Z of rank at most d by choosing n d-dimensional unit vectors x(i) uniformly at random from the unit ball and set Zij = 〈x(i), x(j)〉. Since we are generating the data randomly and E[Zij ] = 0, we don’t expect to observe major changes by thresholding the matrix. So our analysis is limited to the LSH without thresholding, i.e. θ = 0.\nSince based on Theorem 2, we already know given any set of unit vectors x(1), . . . , x(n) and Zij = 〈x(i), x(j)〉, there is an KR-ALSH for the matrix Z, we are just interested in investigating the symmetric LSH for these random vectors.\nA.1 LSH\nFor the symmetric LSH, we only have two possibilities: either having LSH with α = 1 or not having any LSH. We also know from Claim 2 that there is no α LSH if d < log2 n because in that case D(x\n(i), x(j)) = 1 − Zij is not metric. So we want to know the conditions under which the distance will be a metric and also the conditions for having α-LSH with high probability.\nLemma 5. [9] If x is a d-dimensional unit vector and x̃ is its projection onto another unit vector that is sampled uniformly at random from the unit sphere, then for any t > 1, we have E[‖x̃‖22] = 1d and moreover, P(‖x̃‖22 ≥ td ) ≤ e 1−t+log t 2 .\nLemma 6. Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere and for any 1 ≤ i, j ≤ n let Zij = 〈x(i), x(j)〉. If d ≥ 72 loge n + loge 1δ , then the distance measure ∆ij = 1 − Zij is metric with probability at least 1− δ.\nProof. The distance measure ∆ij = 1− 〈x(i), x(j)〉 is not a metric if and only if there exist i, j and k such that\n(1− 〈x(i), x(j)〉) + (1− 〈x(i), x(k)〉) < (1− 〈x(j), x(k)〉)\nA simple reordering of the above inequality gives us:\nCijk = 〈x(i), x(j)〉+ 〈x(i), x(k)〉 − 〈x(j), x(k)〉) > 1\nFor this inequality to hold, the absolute value of at least one of the inner products 〈x(i), x(j)〉, 〈x(i), x(k)〉, 〈x(j), x(k)〉 must be at least 13 . Now we have:\nP(∆ is not a metric) = P(∃ijk : Cijk > 1) ≤ P(∃ij : |〈x(i), x(j)〉| > 1/3)\n≤ n 2\n2 P(|〈x(1), x(2)〉| > 1/3)\nSince both x(1) and x(2) are random vectors, the probability P(|〈x(1), x(2)〉| > 1/3) is equal to the probability that the projection of a random d-dimensional vector onto a 1-dimensional subspace is at least 1/3 in absolute value. By Lemma 5, we have:\nP(∆ is not a metric) ≤ n 2\n2 P(|〈x(1), x(2)〉| > 1/3)\n≤ n 2\n2 P(〈x(1), x(2)〉2 > 1/9)\n≤ n 2\n2 e\n1+log(d/9)−(d/9) 2\n≤ n2e− d36 ≤ δ\nLemma 7. ([19], Theorem 5.39) Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere and t ∈ (0, 1). Let Zij = 〈x(i), x(j)〉 for 1 ≤ i, j ≤ n. If d ≥ C1n/t2 then with probability at least 1 − 2e−C2t\n2N , we have |λi − 1| ≤ t for all eigenvalues λi of Z. Here, C1 > 0 and C2 > 0 are some absolute constants.\nTheorem 3. Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere. Let Zij = 〈x(i), x(j)〉 for 1 ≤ i, j ≤ n. If d ≥ Cn log2 n then with probability at least 1 − eC′n/ log2 n, there is an LSH for Z. Here, C > 0 and C′ > 0 are some absolute constants.\nProof. Apply Lemma 7 with t = 1C0 logn (where C0 is a sufficiently large constant). We get that if d ≥ (C20C1)n log2 n then with probability at least 1−e−(C2/C21)N/ log2 n the smallest eigenvalue is greater than or equal to 1− 1C logn . Therefore, matrix Y = C logn (Z − (1− 1C logn )I) is a positive semidefinite matrix with unit diagonal. Now according to [5], there exists a distribution over a family H of hash functions such that for any i 6= j, Eh∈H[hihj ] = YijC log n . We have,\nEh∈H[hihj ] = Yij\nC logn = Zij −\n(\n1− 1 C logn\n)\nIij = Zij\nMoreover, for every i, we have Eh∈H[hihi] = 1 = Zii.\nA.2 Generalized LSH\nIn this section, we try to investigate the conditions to have Generalized LSH with high probability.\nLemma 8. Let {x(1), . . . , x(n)} be a set of unit vectors. Let Zij = 〈x(i), x(j)〉 for 1 ≤ i, j ≤ n. There is a generalized α-LSH for matrix Z with α = O(log n).\nProof. Matrix Z is positive semi-definite and thus its smallest eigenvalue λmin is non-negative. Applying Claim 3, we get the statement of the lemma.\nTheorem 4. Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere, let 0 < α < O(log n). Let Zij = 〈x(i), x(j)〉 for 1 ≤ i, j ≤ n. If d ≥ Cn log2 n/α2 then with probability at least 1− eC′nα2/ log2 n, there is a generalized α-LSH for Z. Here C > 0 and C′ > 0 are some absolute constants.\nProof. The proof is a straightforward generalization of Theorem 3."
    } ],
    "references" : [ {
      "title" : "Approximating the cut-norm via grothendieck’s inequality",
      "author" : [ "N. Alon", "A. Naor" ],
      "venue" : "SIAM Journal on Computing, 35(4):787–803,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "On nonapproximability for quadratic programs",
      "author" : [ "Sanjeev Arora", "Eli Berger", "Hazan Elad", "Guy Kindler", "Muli Safra" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "A generalized maximum entropy approach to bregman co-clustering and matrix approximation",
      "author" : [ "A. Banerjee", "I. Dhillon", "J. Ghosh", "S. Merugu", "D. S D.S. Modha" ],
      "venue" : "SIGKDD, pages 509–514,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Two new approaches to obtaining estimates in the danzergrunbaum problem",
      "author" : [ "L.V. Buchok" ],
      "venue" : "Mathematical Notes, 87(4):489–496,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Maximizing quadratic programs: Extending grothendieck’s inequality",
      "author" : [ "M. Charikar", "A. Wirth" ],
      "venue" : "In FOCS, pages 54–60,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Similarity estimation techniques from rounding algorithms",
      "author" : [ "M.S. Charikar" ],
      "venue" : "STOC,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Lsh-preserving functions and their applications",
      "author" : [ "F. Chierichetti", "R. Kumar" ],
      "venue" : "SODA,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Über zwei probleme bezüglich konvexer körper von p",
      "author" : [ "L Danzer", "B Grünbaum" ],
      "venue" : "erdös und von vl klee. Mathematische Zeitschrift, 79(1):95–99,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "An elementary proof of a theorem of johnson and lindenstrauss",
      "author" : [ "S. Dasgupta", "A. Gupta" ],
      "venue" : "Random Structures & Algorithms, 22(1):60–65,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Locality-sensitive hashing scheme based on p-stable distributions",
      "author" : [ "M. Datar", "N. Immorlica", "P. Indyk", "S.V. Mirrokni" ],
      "venue" : "In Proc. 20th SoCG, pages 253–262,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Information-theoretic co-clustering",
      "author" : [ "I.S. Dhillon", "M. Subramanyam", "S.M. Dharmendra" ],
      "venue" : "SIGKDD,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Clustering partially observed graphs via convex optimization",
      "author" : [ "A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xuo" ],
      "venue" : "ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Clustering using max-norm constrained optimization",
      "author" : [ "A. Jalali", "N. Srebro" ],
      "venue" : "ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sur la constante de grothendieck",
      "author" : [ "J.L. Krivine" ],
      "venue" : "C. R. Acad. Sci. Paris Ser. A-B 284, pages 445–446,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "The power of asymmetry in binary hashing",
      "author" : [ "B. Neyshabur", "P. Yadollahpour", "Y. Makarychev", "R. Salakhutdinov", "N. Srebro" ],
      "venue" : "NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Approximate nearest neighbors: towards removing the curse of dimensionality",
      "author" : [ "Rajeev Motwani Piotr Indyk" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "Maximum margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T. Jaakkola" ],
      "venue" : "NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Rank, trace-norm and max-norm",
      "author" : [ "N. Srebro", "A. Shraibman" ],
      "venue" : "COLT,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by [6] and to the max-norm ball, and the differences between their symmetric and asymmetric versions.",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "In section 3 we discuss their convex hull and its relationship to notion of Locality Sensitive Hashing (LSH) as studied by [6].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "[16,10,7]).",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 9,
      "context" : "[16,10,7]).",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "[16,10,7]).",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "[11,3]) and asymmetric hamming embedding as recently introduced by [15].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "[11,3]) and asymmetric hamming embedding as recently introduced by [15].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 14,
      "context" : "[11,3]) and asymmetric hamming embedding as recently introduced by [15].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "[12,13], who relax the constraint to a trace-norm and max-norm constraint respectively.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "[12,13], who relax the constraint to a trace-norm and max-norm constraint respectively.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "Moving on from a finite average of clusterings, with a fixed number of components, as in hamming embedding, to an infinite average, we arrive at the notion of LSH as studied by [6].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Given a collection S of objects, an alphabet Γ and a similarity function sim : S × S → [−1, 1] such that for any x ∈ S we have sim(x, x) = 1,a locality sensitive hashing scheme (LSH) is a probability distribution on the family of clustering functions (hash functions) H = {h : S → Γ} such that [6]: Eh∈H[κh(x, y)] = sim(x, y).",
      "startOffset" : 294,
      "endOffset" : 297
    }, {
      "referenceID" : 5,
      "context" : "(3) [6] discuss similarity functions sim : S × S → [0, 1] as so require Ph∈H[h(x) = h(y)] = sim(x, y).",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "(3) [6] discuss similarity functions sim : S × S → [0, 1] as so require Ph∈H[h(x) = h(y)] = sim(x, y).",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "The importance of an LSH, as an object in its own right as studied by [6], is that a hamming embedding can be obtained from an LSH by randomly generating a finite number of hash functions from the distribution over the family H.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Unfortunately, even the requirement (5) of an α-LSH is quite limiting and difficult to obey, as captured by the following theorem, which is based on lemmas 2 and 3 of [6]:",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "Using lemma 3 in [6], we can say that 1−sim(x,y) α can be isometrically embedded in the Hamming cube which means 1 − sim(x, y) can be embedded in Hamming cube with no distortion.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "As noted by [6] (and stated in claim 2), we can therefore unfortunately conclude that there is no α-LSH for several important similarity measures such as the Euclidian inner product, Overlap coefficient and Dice’s coefficient.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "But before doing so, let us consider a different attempt at relaxing the definition of an α-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift θ from the scaling α, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "But before doing so, let us consider a different attempt at relaxing the definition of an α-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift θ from the scaling α, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "According to [5], if a matrix Z with unit diagonal is positive semidefinite, then there is a probability distribution over a family H of hash functions such that for any x 6= y: Eh∈H[h(x)h(y)] = Z(x, y) C logn .",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "Given two collections of objects S, T , which might or might not be identical, and an alphabet Γ , an asymmetric clustering (or co-clustering [11]) is specified by pair of mappings f : S → Γ and g : T → Γ and is captured by the asymmetric cluster incidence matrix κf,g(x, y) where κf,g(x, y) = 1 if f(x) = g(y) and κf,g(x, y) = −1 otherwise.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "In a recent work, [15] showed that even when S = T and the similarity function sim is a well-behaved symmetric similarity function, asymmetric binary embedding could be much more powerful in approximating the similarity, using shorter lengths d, both theoretically and empirically on data sets of interest.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "2 A Tight Convex Relaxation using the Max-Norm Recall that the max-norm (also known as the γ2 : l1 → l∞ norm) of a matrix is defined as [18]: ‖Z‖max = min UV ⊤ max(‖U‖2,∞, ‖V ‖2,∞) where ‖U‖2,∞ is the maximum l2 norm of rows of the matrix U .",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "The maxnorm is SDP representable and thus tractable [17].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Moreover, it follows from the result of [2] that there is no efficiently computable upper bound β for α̂g such that β log n ≤ α̂g ≤ β (under a standard complexity assumption that NP 6⊆ DTIME(nlog3 )).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "(Krivine’s lemma [14]) For any two sets of unit vectors {ui} and {vj} in a Hilbert space H, there are two sets of unit vectors {ui} and {v′ j} in a Hilbert space H ′ such that for any ui and vj, sin(c〈ui, vj〉) = 〈ui, v′ j〉 where c = sinh−1(1).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "A part of the proof is similar to [1].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "g∈MT,2 μ(f, g) = π 2c ‖sim‖max = KR‖sim‖max The inequality KG ≤ K is known due to [1].",
      "startOffset" : 82,
      "endOffset" : 85
    } ],
    "year" : 2014,
    "abstractText" : "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by [6] and to the max-norm ball, and the differences between their symmetric and asymmetric versions.",
    "creator" : "LaTeX with hyperref package"
  }
}